{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id='top'></a>\n<h1 style=\"text-align:center;font-size:200%;;\">Real or Not? NLP with Disaster Tweets</h1>\n![](https://st.depositphotos.com/1032753/4674/v/950/depositphotos_46741417-stock-illustration-twitter-and-social-media-concept.jpg)"},{"metadata":{},"cell_type":"markdown","source":"# About competition: <br>\n* Twitter has become an important communication channel in times of emergency.The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies). <br>\n* In this competition, we are challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.<br>"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content:</h3>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Target Visualization\" role=\"tab\" aria-controls=\"profile\">Part one: Target Visualization<span class=\"badge badge-primary badge-pill\">1</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#word Embeddings\" role=\"tab\" aria-controls=\"messages\">Part Two: Introduction to word Embeddings<span class=\"badge badge-primary badge-pill\">2</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#models\" role=\"tab\" aria-controls=\"settings\">Part three: Building basic models and text preprocessing to improve score<span class=\"badge badge-primary badge-pill\">3</span></a>\n  "},{"metadata":{},"cell_type":"markdown","source":"#### Credits and refrences: <br>\nI have learned these techniques and implemented in this competitions from following kernels: <br>\n1. [Target Visualization - T-SNE and Doc2Vec](https://www.kaggle.com/arthurtok/target-visualization-t-sne-and-doc2vec) <br>\n2. [A Detailed Explanation of Keras Embedding Layer](https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer) <br>\n3. [A look at different embeddings.!](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings)<br>\n4. [Improve your Score with Text Preprocessing -- V2](https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2)<br>\nThanks to authors of the above kernels :)"},{"metadata":{},"cell_type":"markdown","source":"<a id='Target Visualization'></a>\n# <font color='red'> Part one: Target Visualization</font> <br>"},{"metadata":{},"cell_type":"markdown","source":"In this part one will be see an exploration into the target variable and how it is distributed across the structure of the training data to see if any potential information or patterns can be gleaned going forward. Since classical treatments of text data normally comes with the challenges of high dimensionality (using term frequencies or term frequency inverse document frequencies), the plan therefore in this kernel is to visually explore the target variable in some lower dimensional space."},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing necessary modules."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer \nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import stopwords\nfrom string import punctuation\n\nimport re\nfrom functools import reduce\n\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.plotting import figure, show, output_notebook, reset_output\nfrom bokeh.palettes import d3\nimport bokeh.models as bmo\nfrom bokeh.io import save, output_file\n\n# init_notebook_mode(connected = True)\n# color = sns.color_palette(\"Set2\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\npd.options.display.max_rows = 999\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Importing dataframes."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Columns description\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Resampling the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_size = 3271 \n\n# Rebalancing the training set\ntrain_rebal = train_df[train_df.target == 1].sample(sample_size).append(train_df[train_df.target == 0].sample(sample_size)).reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Text processing.<br>\n**In this section, we will do some pre-processing of the text contained within the training data. The processing applied here are some of the standard NLP steps that one would implement in a text based problem, consisting of:**\n\n* Tokenization\n* Stemming or Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(words):\n    \"\"\"\n    Function to remove stopwords from the text\n    \"\"\"\n    stop_words = set(stopwords.words(\"english\"))\n    return [word for word in words if word not in stop_words]\n\ndef remove_punctuation(text):\n    \"\"\"\n    Function to remove punctuation from the text\n    \"\"\"\n    return re.sub(r'[^\\w\\s]', '', text)\n\ndef lemmatize_text(words):\n    \"\"\"\n    Function to lemmatize the text\n    \"\"\"\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef stem_text(words):\n    \"\"\"\n    Function to stem th question text\n    \"\"\"\n    ps = PorterStemmer()\n    return [ps.stem(word) for word in words]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"puncts=['☹', 'Ź', 'Ż', 'ἰ', 'ή', 'Š', '＞', 'ξ','ฉ', 'ั', 'น', 'จ', 'ะ', 'ท', 'ำ', 'ใ', 'ห', '้', 'ด', 'ี', '่', 'ส', 'ุ', 'Π', 'प', 'ऊ', 'Ö', 'خ', 'ب', 'ஜ', 'ோ', 'ட', '「', 'ẽ', '½', '△', 'É', 'ķ', 'ï', '¿', 'ł', '북', '한', '¼', '∆', '≥', '⇒', '¬', '∨', 'č', 'š', '∫', 'ḥ', 'ā', 'ī', 'Ñ', 'à', '▾', 'Ω', '＾', 'ý', 'µ', '?', '!', '.', ',', '\"', '#', '$', '%', '\\\\', \"'\", '(', ')', '*', '+', '-', '/', ':', ';', '<', '=', '>', '@', '[', ']', '^', '_', '`', '{', '|', '}', '~', '“', '”', '’', 'é', 'á', '′', '…', 'ɾ', '̃', 'ɖ', 'ö', '–', '‘', 'ऋ', 'ॠ', 'ऌ', 'ॡ', 'ò', 'è', 'ù', 'â', 'ğ', 'म', 'ि', 'ल', 'ग', 'ई', 'क', 'े', 'ज', 'ो', 'ठ', 'ं', 'ड', 'Ž', 'ž', 'ó', '®', 'ê', 'ạ', 'ệ', '°', 'ص', 'و', 'ر', 'ü', '²', '₹', 'ú', '√', 'α', '→', 'ū', '—', '£', 'ä', '️', 'ø', '´', '×', 'í', 'ō', 'π', '÷', 'ʿ', '€', 'ñ', 'ç', 'へ', 'の', 'と', 'も', '↑', '∞', 'ʻ', '℅''ι', '•', 'ì', '−', 'л', 'я', 'д', 'ل', 'ك', 'م', 'ق', 'ا', '∈', '∩', '⊆', 'ã', 'अ', 'न', 'ु', 'स', '्', 'व', 'ा', 'र', 'त', '§', '℃', 'θ', '±', '≤', 'उ', 'द', 'य', 'ब', 'ट', '͡', '͜', 'ʖ', '⁴', '™', 'ć', 'ô', 'с', 'п', 'и', 'б', 'о', 'г', '≠', '∂', 'आ', 'ह', 'भ', 'ी', '³', 'च', '...', '⌚', '⟨', '⟩', '∖', '˂', 'ⁿ', '⅔', 'న', 'ీ', 'క', 'ె', 'ం', 'ద', 'ు', 'ా', 'గ', 'ర', 'ి', 'చ', 'র', 'ড়', 'ঢ়', 'સ', 'ં', 'ઘ', 'ર', 'ા', 'જ', '્', 'ય', 'ε', 'ν', 'τ', 'σ', 'ş', 'ś', 'س', 'ت', 'ط', 'ي', 'ع', 'ة', 'د', 'Å', '☺', 'ℇ', '❤', '♨', '✌', 'ﬁ', 'て', '„', 'Ā', 'ត', 'ើ', 'ប', 'ង', '្', 'អ', 'ូ', 'ន', 'ម', 'ា', 'ធ', 'យ', 'វ', 'ី', 'ខ', 'ល', 'ះ', 'ដ', 'រ', 'ក', 'ឃ', 'ញ', 'ឯ', 'ស', 'ំ', 'ព', 'ិ', 'ៃ', 'ទ', 'គ', '¢', 'つ', 'や', 'ค', 'ณ', 'ก', 'ล', 'ง', 'อ', 'ไ', 'ร', 'į', 'ی', 'ю', 'ʌ', 'ʊ', 'י', 'ה', 'ו', 'ד', 'ת', 'ᠠ', 'ᡳ', 'ᠰ', 'ᠨ', 'ᡤ', 'ᡠ', 'ᡵ', 'ṭ', 'ế', 'ध', 'ड़', 'ß', '¸', 'ч',  'ễ', 'ộ', 'फ', 'μ', '⧼', '⧽', 'ম', 'হ', 'া', 'ব', 'ি', 'শ', '্', 'প', 'ত', 'ন', 'য়', 'স', 'চ', 'ছ', 'ে', 'ষ', 'য', '়', 'ট', 'উ', 'থ', 'ক', 'ῥ', 'ζ', 'ὤ', 'Ü', 'Δ', '내', '제', 'ʃ', 'ɸ', 'ợ', 'ĺ', 'º', 'ष', '♭', '़', '✅', '✓', 'ě', '∘', '¨', '″', 'İ', '⃗', '̂', 'æ', 'ɔ', '∑', '¾', 'Я', 'х', 'О', 'з', 'ف', 'ن', 'ḵ', 'Č', 'П', 'ь', 'В', 'Φ', 'ỵ', 'ɦ', 'ʏ', 'ɨ', 'ɛ', 'ʀ', 'ċ', 'օ', 'ʍ', 'ռ', 'ք', 'ʋ', '兰', 'ϵ', 'δ', 'Ľ', 'ɒ', 'î', 'Ἀ', 'χ', 'ῆ', 'ύ', 'ኤ', 'ል', 'ሮ', 'ኢ', 'የ', 'ኝ', 'ን', 'አ', 'ሁ', '≅', 'ϕ', '‑', 'ả', '￼', 'ֿ', 'か', 'く', 'れ', 'ő', '－', 'ș', 'ן', 'Γ', '∪', 'φ', 'ψ', '⊨', 'β', '∠', 'Ó', '«', '»', 'Í', 'க', 'வ', 'ா', 'ம', '≈', '⁰', '⁷', 'ấ', 'ũ', '눈', '치', 'ụ', 'å', '،', '＝', '（', '）', 'ə', 'ਨ', 'ਾ', 'ਮ', 'ੁ', '︠', '︡', 'ɑ', 'ː', 'λ', '∧', '∀', 'Ō', 'ㅜ', 'Ο', 'ς', 'ο', 'η', 'Σ', 'ण']\nodd_chars=[ '大','能', '化', '生', '水', '谷', '精', '微', 'ル', 'ー', 'ジ', 'ュ', '支', '那', '¹', 'マ', 'リ', '仲', '直', 'り', 'し', 'た', '主', '席', '血', '⅓', '漢', '髪', '金', '茶', '訓', '読', '黒', 'ř', 'あ', 'わ', 'る', '胡', '南', '수', '능', '广', '电', '总', 'ί', '서', '로', '가', '를', '행', '복', '하', '게', '기', '乡', '故', '爾', '汝', '言', '得', '理', '让', '骂', '野', '比', 'び', '太', '後', '宮', '甄', '嬛', '傳', '做', '莫', '你', '酱', '紫', '甲', '骨', '陳', '宗', '陈', '什', '么', '说', '伊', '藤', '長', 'ﷺ', '僕', 'だ', 'け', 'が', '街', '◦', '火', '团', '表',  '看', '他', '顺', '眼', '中', '華', '民', '國', '許', '自', '東', '儿', '臣', '惶', '恐', 'っ', '木', 'ホ', 'ج', '教', '官', '국', '고', '등', '학', '교', '는', '몇', '시', '간', '업', '니', '本', '語', '上', '手', 'で', 'ね', '台', '湾', '最', '美', '风', '景', 'Î', '≡', '皎', '滢', '杨', '∛', '簡', '訊', '短', '送', '發', 'お', '早', 'う', '朝', 'ش', 'ه', '饭', '乱', '吃', '话', '讲', '男', '女', '授', '受', '亲', '好', '心', '没', '报', '攻', '克', '禮', '儀', '統', '已', '經', '失', '存', '٨', '八', '‛', '字', '：', '别', '高', '兴', '还', '几', '个', '条', '件', '呢', '觀', '《', '》', '記', '宋', '楚', '瑜', '孫', '瀛', '枚', '无', '挑', '剔', '聖', '部', '頭', '合', '約', 'ρ', '油', '腻', '邋', '遢', 'ٌ', 'Ä', '射', '籍', '贯', '老', '常', '谈', '族', '伟', '复', '平', '天', '下', '悠', '堵', '阻', '愛', '过', '会', '俄', '罗', '斯', '茹', '西', '亚', '싱', '관', '없', '어', '나', '이', '키', '夢', '彩', '蛋', '鰹', '節', '狐', '狸', '鳳', '凰', '露', '王', '晓', '菲', '恋', 'に', '落', 'ち', 'ら', 'よ', '悲', '反', '清', '復', '明', '肉', '希', '望', '沒', '公', '病', '配', '信', '開', '始', '日', '商', '品', '発', '売', '分', '子', '创', '意', '梦', '工', '坊', 'ک', 'پ', 'ڤ', '蘭', '花', '羡', '慕', '和', '嫉', '妒', '是', '样', 'ご', 'め', 'な', 'さ', 'い', 'す', 'み', 'ま', 'せ', 'ん', '音', '红', '宝', '书', '封', '柏', '荣', '江', '青', '鸡', '汤', '文', '粵', '拼', '寧', '可', '錯', '殺', '千', '絕', '放', '過', '」', '之', '勢', '请', '国', '知', '识', '产', '权', '局', '標', '點', '符', '號', '新', '年', '快', '乐', '学', '业', '进', '步', '身', '体', '健', '康', '们', '读', '我', '的', '翻', '译', '篇', '章', '欢', '迎', '入', '坑', '有', '毒', '黎', '氏', '玉', '英', '啧', '您', '这', '口', '味', '奇', '特', '也', '就', '罢', '了', '非', '要', '以', '此', '为', '依', '据', '对', '人', '家', '批', '判', '一', '番', '不', '地', '道', '啊', '谢', '六', '佬']\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",  \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_numbers(x):\n    x = re.sub('[0-9]{5,}', ' ##### ', x)\n    x = re.sub('[0-9]{4}', ' #### ', x)\n    x = re.sub('[0-9]{3}', ' ### ', x)\n    x = re.sub('[0-9]{2}', ' ## ', x)\n    return x\n\ndef punct_add_space(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x  \n\ndef odd_add_space(x):\n    x = str(x)\n    for odd in odd_chars:\n        x = x.replace(odd, f' {odd} ')\n    return x \n\ndef clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rebal[\"text\"] = train_rebal[\"text\"].apply(lambda x: clean_numbers(x))\ntrain_rebal[\"text\"] = train_rebal[\"text\"].apply(lambda x: punct_add_space(x))\ntrain_rebal[\"text\"] = train_rebal[\"text\"].apply(lambda x: odd_add_space(x))\ntrain_rebal[\"text\"] = train_rebal[\"text\"].apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rebal[\"text\"] = train_rebal[\"text\"].apply(lambda x: word_tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rebal.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. T-SNE applied to Latent Semantic (LSA) space\n* To start off we look at the sparse representation of text documents via the Term frequency Inverse document frequency method. What this does is create a matrix representation that upweights locally prevalent but globally rare terms - therefore accounting for the occurence bias when using just term frequencies."},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_idf_vec = TfidfVectorizer(min_df=3,\n                             max_features = None, \n                             analyzer=\"word\",\n                             ngram_range=(1,3), # (1,6)\n                             stop_words=\"english\")\ntf_idf = tf_idf_vec.fit_transform(list(train_rebal[\"text\"].map(lambda tokens: \" \".join(tokens))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Having obtained our tf-idf matrix - a sparse matrix object, we now apply the TruncatedSVD method to first reduce the dimensionality of the Tf-idf matrix to a decomposed feature space, referred to in the community as the LSA (Latent Semantic Analysis) method.\n\n* LSA has been one of the classical methods in text that have existed for a while allowing \"concept\" searching of words whereby words which are semantically similar to each other (i.e. have more context) are closer to each other in this space and vice-versa."},{"metadata":{},"cell_type":"markdown","source":"# 6.Scatter plots of the Latent Semantic Space"},{"metadata":{},"cell_type":"markdown","source":"## 6.1 First 3 dimensions of the Latent Semantic Space"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=50, random_state=2020)\nsvd_tfidf = svd.fit_transform(tf_idf)\nprint(\"Dimensionality of LSA space: {}\".format(svd_tfidf.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(16,12))\n\n# Plot models:\nax = Axes3D(fig) \nax.scatter(svd_tfidf[:,0],\n           svd_tfidf[:,1],\n           svd_tfidf[:,2],\n           c=train_rebal.target.values,\n           cmap=plt.cm.winter_r,\n           s=20,\n           edgecolor='none',\n           marker='o')\nplt.title(\"Semantic Tf-Idf-SVD reduced plot of real-Not real data distribution\")\nplt.xlabel(\"First dimension\")\nplt.ylabel(\"Second dimension\")\nplt.legend()\nplt.xlim(0.0, 0.4)\nplt.ylim(-0.2,0.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.2 Random 3 dimensions of the Latent Semantic Space"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,12))\n\n# Plot models:\nax = Axes3D(fig) \nax.scatter(svd_tfidf[:,20],\n           svd_tfidf[:,21],\n           svd_tfidf[:,22],\n           c=train_rebal.target.values,\n           cmap=plt.cm.winter_r,\n           s=20,\n           edgecolor='none',\n           marker='o')\nplt.title(\"Semantic Tf-Idf-SVD reduced plot of real-Not real data distribution\")\nplt.xlabel(\"First dimension\")\nplt.ylabel(\"Second dimension\")\nplt.legend()\nplt.xlim(-0.4, 0.4)\nplt.ylim(-0.3,0.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.3 Last 3 dimensions of the Latent Semantic Space"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,12))\n\n# Plot models:\nax = Axes3D(fig) \nax.scatter(svd_tfidf[:,47],\n           svd_tfidf[:,48],\n           svd_tfidf[:,49],\n           c=train_rebal.target.values,\n           cmap=plt.cm.winter_r,\n           s=20,\n           edgecolor='none',\n           marker='x')\nplt.title(\"Semantic Tf-Idf-SVD reduced plot of real-Not real data distribution\")\nplt.xlabel(\"First dimension\")\nplt.ylabel(\"Second dimension\")\nplt.legend()\nplt.xlim(-0.2, 0.6)\nplt.ylim(-0.2,0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations: <br>\n* From the above scatter plots, It is apparent that real disaster tweets and not real disaster tweets overlap quite significantly in the LSA semantic space. <br>\n* Also,there does not seem to be any clear or obvious pattern in segregating the class labels. <br>"},{"metadata":{},"cell_type":"markdown","source":"# 7. Applying T-SNE(non-linear method) to LSA reduced space"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install MulticoreTSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from MulticoreTSNE import MulticoreTSNE as TSNE\ntsne_model = TSNE(n_jobs=4,\n                  early_exaggeration=4, # Trying out exaggeration trick\n                  n_components=2,\n                  verbose=1,\n                  random_state=2020,\n                  n_iter=500)\ntsne_tfidf = tsne_model.fit_transform(svd_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_tfidf_df = pd.DataFrame(data=tsne_tfidf, columns=[\"x\", \"y\"])\ntsne_tfidf_df[\"id\"] = train_rebal[\"id\"].values\ntsne_tfidf_df[\"text\"] = train_rebal[\"text\"].values\ntsne_tfidf_df[\"target\"] = train_rebal[\"target\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_notebook()\nplot_tfidf = bp.figure(plot_width = 600, plot_height = 600, \n                       title = \"T-SNE applied to Tfidf_SVD space\",\n                       tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                       x_axis_type = None, y_axis_type = None, min_border = 1)\n\n# colormap = np.array([\"#6d8dca\", \"#d07d3c\"])\ncolormap = np.array([\"darkblue\", \"red\"])\n\n# palette = d3[\"Category10\"][len(tsne_tfidf_df[\"asset_name\"].unique())]\nsource = ColumnDataSource(data = dict(x = tsne_tfidf_df[\"x\"], \n                                      y = tsne_tfidf_df[\"y\"],\n                                      color = colormap[tsne_tfidf_df[\"target\"]],\n                                      text = tsne_tfidf_df[\"text\"],\n                                      id = tsne_tfidf_df[\"id\"],\n                                      target = tsne_tfidf_df[\"target\"]))\n\nplot_tfidf.scatter(x = \"x\", \n                   y = \"y\", \n                   color=\"color\",\n                   legend = \"target\",\n                   source = source,\n                   alpha = 1)\nhover = plot_tfidf.select(dict(type = HoverTool))\nhover.tooltips = {\"id\": \"@id\", \n                  \"text\": \"@text\", \n                  \"target\":\"@target\"}\n\nshow(plot_tfidf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations:<br>\n* It seems that the distribution of the real disaster tweets and not real disaster tweets overlap in certain regions of the T-SNE plots in concept space, which does not allow easy visual discernment between the two classes. <br>\n* This raises a question of how easy therefore, is it to a human to distinguish between an real disaster tweets and not real disaster tweets, when we see data from both class labels overlapping quite heavily across each other. <br>"},{"metadata":{},"cell_type":"markdown","source":"# 8. Visualising T-SNE applied to LSA reduced space by changing Perplexity."},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_model_5 = TSNE(n_jobs=4, \n                    early_exaggeration=4,\n                  perplexity=5,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2020,\n                  n_iter=500)\ntsne_tfidf_5 = tsne_model_5.fit_transform(svd_tfidf[:6542,:])\n# Creating a Dataframe for Perplexity=5\ntsne_tfidf_df_5 = pd.DataFrame(data=tsne_tfidf_5, columns=[\"x5\", \"y5\"])\ntsne_tfidf_df_5[\"target\"] = train_rebal[\"target\"][:6542].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_model_25 = TSNE(n_jobs=4, \n                     early_exaggeration=4,\n                  perplexity=25,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2020,\n                  n_iter=500)\ntsne_tfidf_25 = tsne_model_25.fit_transform(svd_tfidf[:6542,:])\n# Creating a Dataframe for Perplexity=5\ntsne_tfidf_df_25 = pd.DataFrame(data=tsne_tfidf_25, \n                             columns=[\"x25\", \"y25\"])\ntsne_tfidf_df_25[\"target\"] = train_rebal[\"target\"][:6542].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_model_50 = TSNE(n_jobs=4, \n                     early_exaggeration=4,\n                  perplexity=50,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2020,\n                  n_iter=500)\ntsne_tfidf_50 = tsne_model_50.fit_transform(svd_tfidf[:6542,:])\n# Creating a Dataframe for Perplexity=50\ntsne_tfidf_df_50 = pd.DataFrame(data=tsne_tfidf_50, \n                                columns=[\"x50\", \"y50\"])\ntsne_tfidf_df_50[\"target\"] = train_rebal[\"target\"][:6542].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nplt.scatter(tsne_tfidf_df_5.x5, \n            tsne_tfidf_df_5.y5, \n            alpha=0.75,\n            c=tsne_tfidf_df_5.target,\n            cmap=plt.cm.coolwarm)\nplt.title(\"T-SNE plot in SVD space (perplexity=5)\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nplt.scatter(tsne_tfidf_df_25.x25, \n            tsne_tfidf_df_25.y25, \n            c=tsne_tfidf_df_25.target,\n            cmap=plt.cm.coolwarm)\nplt.title(\"T-SNE plot in SVD space (perplexity=25)\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nplt.scatter(tsne_tfidf_df_50.x50, \n            tsne_tfidf_df_50.y50, \n            c=tsne_tfidf_df_50.target,\n            cmap=plt.cm.coolwarm)\nplt.title(\"T-SNE plot in SVD space (perplexity=50)\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations: <br>\n*  Here also the distribution of the real disaster tweets and not real disaster tweets overlap in certain regions of the T-SNE plots in concept space, which does not allow easy visual discernment between the two classes."},{"metadata":{},"cell_type":"markdown","source":"# 9. T-SNE applied on Doc2Vec embedding<br>\n* Moving forward with our T-SNE visual explorations, we next move away from semantic matrices into the realm of embeddings. Here we will use the Doc2Vec algorithm and much like its very well known counterpart Word2vec involves unsupervised learning of continuous representations for text. Unlike Word2vec which involves finding the representations for words (i.e. word embeddings), Doc2vec modifies the former method and extends it to sentences and even documents.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\ntexts = list(train_rebal[\"text\"])\n\n# Creating a list of terms and a list of labels to go with it\ndocuments = [TaggedDocument(doc, tags=[str(i)]) for i, doc in enumerate(texts)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_epochs = 100\nalpha=0.025\nmodel = Doc2Vec(documents,\n                size=10, \n                min_alpha=0.00025,\n                alpha=alpha,\n                min_count=1,\n                workers=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting a T-SNE model to the dense embeddings and overlaying that with the target visuals, we get:<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_model = TSNE(n_jobs=4,\n                  early_exaggeration=4,\n                  n_components=2,\n                  verbose=1,\n                  random_state=2020,\n                  n_iter=300)\ntsne_d2v = tsne_model.fit_transform(model.docvecs.vectors_docs)\n\n# Putting the tsne information into sq\ntsne_d2v_df = pd.DataFrame(data=tsne_d2v, columns=[\"x\", \"y\"])\ntsne_d2v_df[\"id\"] = train_rebal[\"id\"].values\ntsne_d2v_df[\"text\"] = train_rebal[\"text\"].values\ntsne_d2v_df[\"target\"] = train_rebal[\"target\"].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_notebook()\nplot_d2v = bp.figure(plot_width = 500, plot_height = 500, \n                       title = \"T-SNE applied to Doc2vec document embeddings\",\n                       tools = \"pan, wheel_zoom, box_zoom, reset, hover, previewsave\",\n                       x_axis_type = None, y_axis_type = None, min_border = 1)\n\ncolormap = np.array([\"darkblue\", \"cyan\"])\n\nsource = ColumnDataSource(data = dict(x = tsne_d2v_df[\"x\"], \n                                      y = tsne_d2v_df[\"y\"],\n                                      color = colormap[tsne_d2v_df[\"target\"]],\n                                      text = tsne_d2v_df[\"text\"],\n                                      id = tsne_d2v_df[\"id\"],\n                                      target = tsne_d2v_df[\"target\"]))\n\nplot_d2v.scatter(x = \"x\", \n                   y = \"y\", \n                   color=\"color\",\n                   legend = \"target\",\n                   source = source,\n                   alpha = 1.0)\nhover = plot_d2v.select(dict(type = HoverTool))\nhover.tooltips = {\"id\": \"@id\", \n                  \"text\": \"@text\", \n                  \"target\":\"@target\"}\n\nshow(plot_d2v)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Observations: <br>\n* The visual overlap between real disaster tweets and not real disaster tweets are pretty high in some regions and not that much high in some regions in the Doc2Vec plots.<br>\n* But,we cannot saggregate the labels using our eye ball. <br>"},{"metadata":{},"cell_type":"markdown","source":"<a id='word Embeddings'></a>\n# <font color='blue'>Part Two: Introduction to word Embeddings</font>"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 What are word embeddings?<br>\n* A word embedding is a learned representation for text where words that have the same meaning have a similar representation.<br>\n* In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. <br>"},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Why do we need word embeddings?\n* As it turns out, many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. And with the huge amount of data that is present in the text format, it is imperative to extract knowledge out of it and build applications.<br>"},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Different types of word embeddings <br>\n* The different types of word embeddings can be broadly classified into two categories- <br>\n\n1. Frequency based Embedding <br>\n  1.1 Count Vector <br>\n  1.2 Tf-IDF Vector <br>\n  1.3 Co-Occurance Vector <br>\n  \n2. Prediction based Embedding <br>\n#### [Read more](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)"},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Word Embedding Algorithms <br>\n* Word embedding methods learn a real-valued vector representation for a predefined fixed sized vocabulary from a corpus of text.The learning process is either joint with the neural network model on some task. <br>"},{"metadata":{},"cell_type":"markdown","source":"### 2.4.1 Word Embedding layer <br>\n*  A word embedding layer that is learned jointly with a neural network model on a specific natural language processing task classification,text generation etc..,\n* It requires that text be cleaned and prepared such that each word is one-hot encoded. The size of the vector space is specified as part of the model, such as 50, 100, or 300 dimensions. The vectors are initialized with small random numbers. The embedding layer is used on the front end of a neural network and is fit in a supervised way using the Backpropagation algorithm. <br>\n* The one-hot encoded words are mapped to the word vectors. If a multilayer Perceptron model is used, then the word vectors are concatenated before being fed as input to the model. If a recurrent neural network is used, then each word may be taken as one input in a sequence. <br>\n* [Read more here](https://machinelearningmastery.com/what-are-word-embeddings/)"},{"metadata":{},"cell_type":"markdown","source":"## 2.5 Training our own embedding layer in keras <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n\n#stop-words\nfrom nltk.corpus import stopwords\nstop_words=set(nltk.corpus.stopwords.words('english'))\n\n# tokenizing\nfrom nltk import word_tokenize,sent_tokenize\n\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n\n#keras\nimport keras\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import one_hot,Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense , Flatten ,Embedding,Input\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, Conv1D,Lambda\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom tensorflow.keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\nfrom tensorflow.keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom keras.engine.topology import Layer\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.callbacks import *\n#custome function for f1 score\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    \n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Taking our sample text corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_text_1=\"Kaggle, a subsidiary of Google LLC, is an online community of Data scientists and machine learning practitioners\"\nsample_text_2=\"Data science is a multi-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data\"\ncorp=[sample_text_1,sample_text_2]\nno_docs=len(corp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### INTEGER ENCODING ALL THE DOCUMENTS\n* After this all the unique words will be reprsented by an integer. For this we are using one_hot function from the Keras. Note that the vocab_size is specified large enough so as to ensure unique integer encoding for each and every word.\n\n* Note one important thing that the integer encoding for the word remains same in different docs. eg 'Data' is denoted by 21 in each and every document."},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size=50 \nencod_corp=[]\nfor i,doc in enumerate(corp):\n    encod_corp.append(one_hot(doc,50))\n    print(\"The encoding for document\",i+1,\" is : \",one_hot(doc,50))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### PADDING THE DOCS (to make very doc of same length): <br>\n* The Keras Embedding layer requires all individual documents to be of same length. Hence we wil pad the shorter documents with 0 for now. Therefore now in Keras Embedding layer the 'input_length' will be equal to the length (ie no of words) of the document with maximum length or maximum number of words.\n\n* To pad the shorter documents I am using pad_sequences functon from the Keras library."},{"metadata":{"trusted":true},"cell_type":"code","source":"maxlen=-1\nfor doc in corp:\n    tokens=nltk.word_tokenize(doc)\n    if(maxlen<len(tokens)):\n        maxlen=len(tokens)\nprint(\"The maximum number of words in any document is : \",maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.\npad_corp=pad_sequences(encod_corp,maxlen=maxlen,padding='post',value=0.0)\nprint(\"No of padded documents: \",len(pad_corp))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,doc in enumerate(pad_corp):\n     print(\"The padded encoding for document\",i+1,\" is : \",doc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CREATING THE EMBEDDINGS USING KERAS EMBEDDING LAYER <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"input=Input(shape=(no_docs,maxlen),dtype='float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_input=Input(shape=(maxlen,),dtype='float64')  \n\n# creating the embedding\nword_embedding=Embedding(input_dim=vocab_size,output_dim=8,input_length=maxlen)(word_input)\n\nword_vec=Flatten()(word_embedding) # flatten\nembed_model =Model([word_input],word_vec) # combining all into a Keras model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_model.compile(optimizer=Adam(lr=1e-3),loss='binary_crossentropy',metrics=['acc']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(word_embedding))\nprint(word_embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(embed_model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings=embed_model.predict(pad_corp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of embeddings : \",embeddings.shape)\nprint(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings=embeddings.reshape(-1,maxlen,8)\nprint(\"Shape of embeddings : \",embeddings.shape) \nprint(embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The resulting shape is (2,34,8).\n\n2---> no of documents\n\n34---> each document is made of 34 words which was our maximum length of any document.\n\n& 8---> each word is 8 dimensional."},{"metadata":{},"cell_type":"markdown","source":"### GETTING ENCODING FOR A PARTICULAR WORD IN A SPECIFIC TEXT <br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,doc in enumerate(embeddings):\n    for j,word in enumerate(doc):\n        print(\"The encoding for \",j+1,\"th word\",\"in\",i+1,\"th document is : \\n\\n\",word)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='models'></a>\n# <font color='orange'> Part three: Building basic models and text preprocessing to improve score</font>  "},{"metadata":{},"cell_type":"markdown","source":"* In this part we will build some basic models with simple architectures.Also,we will explore text processing techniques when we are using word embeddings.<br>\n* Also we will compare results of the model build on text processed text and text which was not processed and cleaned."},{"metadata":{},"cell_type":"markdown","source":"### Preparing data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2020)\n\n## some config values \nembed_size = 128 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\ntrain_X = train_df[\"text\"].values\nval_X = val_df[\"text\"].values\ntest_X = test_df[\"text\"].values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building model without pretrained embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size)(inp)\nx = Bidirectional(GRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='Adamax', metrics=['accuracy',f1])\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_X, train_y, batch_size=512, epochs=10, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ndef threshold_search(y_true, y_proba):\n#reference: https://www.kaggle.com/hung96ad/pytorch-starter\n    best_threshold = 0\n    best_score = 0\n    for threshold in tqdm([i * 0.001 for i in range(1000)]):\n        score = metrics.f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result\nsearch_result = threshold_search(val_y, pred_noemb_val_y)\nsearch_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### We can see the f1 score was 0.76 at threshold value of 0.159 "},{"metadata":{},"cell_type":"markdown","source":"### Building model using Glove Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_df.drop('target',axis = 1)\ndf = pd.concat([train ,test_df])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embed(file):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n    if file == '/kaggle/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n    return embeddings_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove = '/kaggle/input/embeddings/glove-840B-300d.txt'\nprint(\"Extracting GloVe embedding\")\nembed_glove = load_embed(glove)\nprint('Loaded glove embeddings sucessfully...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vocabulary and Coverage functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator \ndef build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(df['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Added {count} words to embedding\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)\nadd_lower(embed_glove, vocab)\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Contractions:"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"- Known Contractions -\")\nprint(\"   Glove :\")\nprint(known_contractions(embed_glove))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['treated_text'] = df['text'].apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(df['treated_text'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Punctuations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unknown_punct(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Glove :\")\nprint(unknown_punct(embed_glove, punct))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['treated_text'] = df['treated_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(df['treated_text'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correcting spellings:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['treated_text'] = df['treated_text'].apply(lambda x: correct_spelling(x, mispell_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(df['treated_text'])\nprint(\"Glove : \")\noov_glove = check_coverage(vocab, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correcting the spellings had not done any trick for us. As we can observe that found embeddings percentage was decreased."},{"metadata":{},"cell_type":"markdown","source":"### Preparing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['treated_text'] = train['text'].apply(lambda x: x.lower())\n# Contractions\ntrain['treated_text'] = train['text'].apply(lambda x: clean_contractions(x, contraction_mapping))\n# Special characters\ntrain['treated_text'] = train['text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_data(X):\n    t = Tokenizer(num_words=max_features)\n    t.fit_on_texts(X)\n    X = t.texts_to_sequences(X)\n    X = pad_sequences(X, maxlen=maxlen)\n    return X, t.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, word_index = make_data(train['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_treated_data(X):\n    t = Tokenizer(num_words=max_features, filters='')\n    t.fit_on_texts(X)\n    X = t.texts_to_sequences(X)\n    X = pad_sequences(X, maxlen=maxlen)\n    return X, t.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_treated, word_index_treated = make_treated_data(train['treated_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, train_df['target'].values, test_size=0.1, random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_t_train, X_t_val, _, _ = train_test_split(X_treated, train_df['target'].values, test_size=0.1, random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_embed_matrix(embeddings_index, word_index, len_voc):\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    word_index = word_index\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n    \n    for word, i in word_index.items():\n        if i >= len_voc:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding = make_embed_matrix(embed_glove, word_index, max_features)\ndel word_index\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_treated = make_embed_matrix(embed_glove, word_index_treated, max_features)\ndel word_index_treated\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def modelling(embe_matrix):\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, 300, weights=[embe_matrix])(inp)\n    x = Bidirectional(GRU(64, return_sequences=True))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(16, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy',f1])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = modelling(embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_treated = modelling(embedding_treated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, batch_size=512, epochs=10, \n                    validation_data=[val_X, val_y])\npred_val = model.predict(X_val, batch_size=512, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_treated.fit(X_t_train, y_train, batch_size=512, epochs=10, \n                            validation_data=[X_t_val, y_val])\npred_t_val = model_treated.predict(X_t_val, batch_size=512, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"search_result = threshold_search(y_val, pred_val)\nsearch_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### We can observe that f1-score without text preprocessing was 0.799"},{"metadata":{"trusted":true},"cell_type":"code","source":"search_result = threshold_search(y_val, pred_t_val)\nsearch_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### We can observe that f1-score with text preprocessing was 0.82"},{"metadata":{},"cell_type":"markdown","source":"## Therefore we can conclude that preprocessing the text with according to the embeddings we use,will help us to increase the score."},{"metadata":{},"cell_type":"markdown","source":"# Thats all for now.<font color='red'>Please consider Upvoting this kernel</font>.Suggestions are much appreciated to improve this kernel further. <br>\n# <font color='blue'>Happy learning :)</font>"},{"metadata":{},"cell_type":"markdown","source":"<a href=\"#top\" class=\"btn btn-primary btn-lg active\" role=\"button\" aria-pressed=\"true\">Go to TOP</a>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}