{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenization\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Loading","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Missing Values\nIt so happens that only NaN values in the dataset are present in `keyword` and `location` columns. The former one having **0.81%** of NaN values and the later one having shy of **50%** NaN values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of the training dataset: {}.\".format(train_data.shape))\nprint(\"Shape of the testing dataset: {}\".format(test_data.shape))\nfor col in train_data.columns:\n    nan_vals = train_data[col].isna().sum()\n    pcent = (train_data[col].isna().sum() / train_data[col].count()) * 100\n    print(\"Total NaN values in column '{}' are: {}, which is {:.2f}% of the data in that column\".format(col, nan_vals, pcent))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 NaN (missing) values visualization ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's plot NaN value distribution\nfig = sns.barplot(\n    x=train_data[['keyword', 'location']].isna().sum().index,\n    y=train_data[['keyword', 'location']].isna().sum().values,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Visualizing Target Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vals = [len(train_data[train_data['target']==1]['target']), len(train_data[train_data['target']==0]['target'])]\n\nplt.pie(vals, labels=[\"Non-Disaster\", \"Disaster\"])\nplt.axis('equal')\nplt.title(\"Target Value Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Keyword Frequency Count","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 70), dpi=100)\nsns.countplot(y=train_data['keyword'].sort_values(), hue=train_data['target'])\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5 Character Count","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_twt = train_data[train_data['target']==1]['text'].str.len()\nnon_dis_twt = train_data[train_data['target']==0]['text'].str.len()\n\nsns.distplot([dis_twt, non_dis_twts_twt])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.6 Word Count Distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_cnt = train_data[train_data['target']==1]['text'].str.split().map(lambda x: len(x))\nndis_cnt = train_data[train_data['target']==0]['text'].str.split().map(lambda x: len(x))\n\nfig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(\n    go.Histogram(x=list(dis_cnt), name='Disaster Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(ndis_cnt), name='Non Disaster Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.update_layout(height=500, width=950, title_text=\"Words Count\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.7 Average Word length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_avg = train_data[train_data['target']==1]['text'].str.split().map(lambda x: [len(j) for j in x]).map(lambda x: np.mean(x)).to_list()\nndis_avg = train_data[train_data['target']==0]['text'].str.split().map(lambda x: [len(j) for j in x]).map(lambda x: np.mean(x)).to_list()\n\nfig = ff.create_distplot([dis_avg, ndis_avg], ['Disaster', 'Non Disaster'])\nfig.update_layout(height=500, width=950, title_text=\"Average Word Length Distribution\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.8 Unique Word Count Distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_uvc = train_data[train_data['target']==1]['text'].apply(lambda x: len(set(str(x).split()))).to_list()\nndis_uvc = train_data[train_data['target']==0]['text'].apply(lambda x: len(set(str(x).split()))).to_list()\n\nfig = ff.create_distplot([dis_uvc, ndis_uvc], ['Disaster', 'Non Disaster'])\nfig.update_layout(height=500, width=950, title_text=\"Unique Word Count Distribution\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.9 URL Count","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_uc = train_data[train_data['target']==1]['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w])).to_list()\nndis_uc = train_data[train_data['target']==0]['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w])).to_list()\n\nfig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(\n    go.Histogram(x=dis_uc, name='Disaster Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=ndis_uc, name='Non Disaster Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.update_layout(height=500, width=950, title_text=\"URL Count\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.10 WordCloud for Disaster and Non-Disaster Tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dis_snt = train_data[train_data['target']==1]['text'].to_list()\ndis_snt = \" \".join(dis_snt)\n\ndis_wc = WordCloud(width=256, height=256, collocations=False).generate(dis_snt)\nplt.figure(figsize = (7,7))\nplt.imshow(dis_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ndis_snt = train_data[train_data['target']==0]['text'].to_list() \nndis_snt = \" \".join(ndis_snt)\n\nndis_wc = WordCloud(width=256, height=256, collocations=False).generate(ndis_snt)\nplt.figure(figsize = (7,7))\nplt.imshow(ndis_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Models\nFor training the model, we'll fine-tune BERT","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Tokenizer function\nBelow is a function defined to tokenize the data and also adds the `CLS` and `SEP` tokens at the start & end as required by BERT ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens, all_masks, all_segments = [], [], []\n    \n    for text in tqdm(texts):\n        # Tokenize the current text\n        text = tokenizer.tokenize(text)\n        # Select text only till \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Get the Model from TFHub\nGet the model from TFHub and the vocab file with it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nurl = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the vocab file (for tokenizing) and tokenizer itself\nvocab_fl = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nlower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_fl, lower_case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Encode the data\nNow, let's encode the training and testing data into","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntrain_input = bert_encode(train_data['text'].values, tokenizer, max_len=160)\ntest_input = bert_encode(test_data['text'].values, tokenizer, max_len=160)\ntrain_labels = train_data['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 3.4 Building the Model\nWe'll make a function for fine-tuning BERT","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    # Naming your keras ops is very important 😉\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_word_ids')\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name='segment_ids')\n    # Get the sequence output\n    _, seq_op = transformer([input_word_ids, input_mask, segment_ids])\n    # Get the respective class token from that sequence output\n    class_tkn = seq_op[:, 0, :]\n    # Final Neuron (for Classification)\n    op = Dense(1, activation='sigmoid')(class_tkn)\n    # Bind the inputs and outputs together into a Model\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=op)\n    \n    model.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.5 Training the model\nFinally! Let's train our model on GPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the model\nmodel = build_model(bert_layer, max_len=160)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.1,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 3.6 Testing the model\nLet's test our model and submit predictions!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(test_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Submission CSV file\nsub_fl = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsub_fl['target'] = preds.round().astype(int)\nsub_fl.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}