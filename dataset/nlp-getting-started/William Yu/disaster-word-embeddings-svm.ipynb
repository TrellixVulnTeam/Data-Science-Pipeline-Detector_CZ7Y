{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Real or Not? NLP with Disaster Tweets Kaggle Challenge\nWilliam Yu\n\nI would call myself a beginner in NLP so any feedback is greatly welcome!\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Introduction\nThe purpose of this challenge is, given a set of Tweets, are you able to identify whether a Tweet is tweeting about a real disaster? The premise is simple, but what lies in the core of this challenge are basic Natural Language Processing (NLP) and binary classification techniques. \n\nLet's get started."},{"metadata":{},"cell_type":"markdown","source":"## Loading the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import necessary packages\nimport pandas as pd \nimport numpy as np\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import train and test data\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that we have 5 columns, `id`, `keyword`,`location`, `text`, and the `target` label. The labels are encoded with 0 and 1's to indicate if a disaster took place, with 1 being the tweet is about a real disaster. \n\nWe'll be mainly focusing on the `text` column and deriving meaning on the actual tweet itself to aid us in our analysis."},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing and Vizualization"},{"metadata":{},"cell_type":"markdown","source":"Some things we want to do prior to modeling:\n- Remove html tags, urls and special characters\n- Visualize word frequencies of all tweets\n- Lemmatize words and construct vocabulary"},{"metadata":{},"cell_type":"markdown","source":"In order to do visualze the word frequencies we'll be using a **Bag of Words (BOW)** model, which will preserve every word in the given text and its multiplicity but discard the order of the words. This way, we can see how many times a given word has appeared in our corpus, or our collection of tweets. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to remove html tags in text\ndef htmlremove(text):\n    return re.sub('<[^<]+?>', '', text)\n\ntrain['text'] = train['text'].apply(htmlremove)\ntest['text'] = test['text'].apply(htmlremove)     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to remove any sort of url's or username mentions as they have little to no influence on the prediction:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove url and @'s\ndef urlremove(text):\n    return re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n\ntrain['text'] = train['text'].apply(urlremove)\ntest['text'] = test['text'].apply(urlremove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove emoji's, contractions, special characters and numbers if there are any:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove emojis\n# https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        u\"(\\ud83d[\\ude00-\\ude4f])|\"  # emoticons\n        u\"(\\ud83c[\\udf00-\\uffff])|\"  # symbols & pictographs (1 of 2)\n        u\"(\\ud83d[\\u0000-\\uddff])|\"  # symbols & pictographs (2 of 2)\n        u\"(\\ud83d[\\ude80-\\udeff])|\"  # transport & map symbols\n        u\"(\\ud83c[\\udde0-\\uddff])\"  # flags (iOS)\n        \"+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntrain['text'] = train['text'].apply(remove_emoji)\ntest['text'] = test['text'].apply(remove_emoji)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove contractions\n# taken from: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ndef decontracted(text):\n    if '’' in text:\n        text = text.replace(\"’\", \"'\")\n    # specific\n    text = re.sub(r\"won\\'t\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n\n    # general\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    return text\n\ntrain['text'] = train['text'].apply(decontracted)\ntest['text'] = test['text'].apply(decontracted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove special characters\ndef characterremove(text):\n    return re.sub('\\W+|_', ' ', text)\n\ntrain.text = train.text.apply(characterremove)\ntest.text = test.text.apply(characterremove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove numbers in text\ndef remove_numbers(text):\n    # define the pattern to keep\n    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' \n    return re.sub(pattern, '', text)\n\ntrain.text = train.text.apply(remove_numbers)\n# lower-case text and remove unnecessary whitespaces\ntrain.text = train.text.str.lower()\ntrain.text = train.text.str.strip()\n\ntest.text = test.text.apply(remove_numbers)\ntest.text = test.text.str.lower()\ntest.text = test.text.str.strip()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We finished the processing of our data and now can move to a short vizualization using a BOW approach."},{"metadata":{},"cell_type":"markdown","source":"To get our BOW, we import `CountVectorizer` from sklearn's `feature_extraction.text` package and fit it on our data to get the word occurences.\n\nWhat `CountVectorizer` is doing is tokenizing each word in a document, and then building a vocabulary consisting of all these words. We also specified a list of stopwords, which will remove certain words that are present in the document from our vocabulary. `min_df` corresponds to the minimum number of documents a word has to appear in to be put into the vocabulary, which will remove infrequent words.\n\nUsing this vocabulary, we can create a group representation as a sparse matrix, where each row corresponds to a document (a tweet in this case). If an entry in a given row is '0', that indicates the word corresponding to that entry in the feature vector (use `CountVectorizer.get_feature_names()` to get the feature vector) does not appear in that document. Otherwise, any other number in the row equates to the word being in a given document and its associated counts. So most of these entries in the matrix will be 0, hence the sparse matrix notation.\n\nThis corresponds to the line in the code above: `X = init_vec.fit_transform(new_corpus)` where X is the sparse matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\n\nstopwords = nltk.corpus.stopwords.words('english')\n\ndef get_top_20(ngram_range, new_corpus, stopwords, min_df):\n    init_vec = CountVectorizer(ngram_range=ngram_range, stop_words = stopwords, min_df=min_df)\n    # get sparse matrix from vocabulary\n    X = init_vec.fit_transform(new_corpus)\n    count_list = X.toarray().sum(axis=0) \n    word_list = init_vec.get_feature_names()\n    counts = dict(zip(word_list,count_list))\n    top_20 = sorted(counts.items(), key=lambda x:x[1], reverse=True)[:20]\n    return top_20, counts\n\ntop_20, counts = get_top_20((1,1), train.text, stopwords, 4)\ntop_20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the word \"like\" has the most occurences with 348, followed by \"amp\" with 344. \"amp\" is actually a character reference in HTML that stands for \"&\", which is commonly called an \"ampersand\". This won't have any meaning towards our prediction, so we can add it to our stopwords when modeling later."},{"metadata":{"trusted":true},"cell_type":"code","source":"# wordcloud image of above (with more words) \nfrom PIL import Image\nfrom wordcloud import WordCloud\n\ndef generate_wc_from_counts(counts):\n    wc = WordCloud(background_color=\"white\", width=1000, height=600, contour_width=3,\n               contour_color='steelblue', max_words=300, \n               relative_scaling=0.5,normalize_plurals=False, random_state=0).generate_from_frequencies(counts)\n    return wc\n\nwc = generate_wc_from_counts(counts)\nwc.to_image()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After vizualizing top words, it seems like we may need to **lemmatize** our words; in essence, lemmatization seeks to remove inflectional endings of words and reduce to a common base word. For instance, if the words 'burn', 'burns', and 'burning' appear in our corpus, lemmatization would reduce these words to their common base word 'burn'. This makes it much easier to conduct text analysis as we no longer have to take into account the various inflections in a word."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lemmatize words in text\n# lemmatize with nltk before spacy seems to give higher acc?\nfrom nltk.stem import WordNetLemmatizer\n\nlemmer = WordNetLemmatizer()\ntrain.text = [' '.join([lemmer.lemmatize(word) for word in text.split(' ')]) for text in train.text]\n\ntest.text = [' '.join([lemmer.lemmatize(word) for word in text.split(' ')]) for text in test.text]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Embeddings and Modeling"},{"metadata":{},"cell_type":"markdown","source":"Instead of BOW, we will be using **word embeddings**. There are certain limitations to BOW, such as large feature space and an inability to capture similarity and semantics, so we will opt for word embeddings, which seeks to capture similarities betweeen words numerically. A short description of word embeddings is that they map words and phrases to vectors of numerical values. Although we won't be looking at semantics here, it is good habit to adopt some of these principles.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into X and y\nX_text = train.text\ny_text = train.target\nX_test_text = test.text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can use spacy to load some pre-trained word vectors and apply them to our corpus. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_lg\")\nnlp.vocab[\"amp\"].is_stop = True\n\ndocs = [nlp(d).vector for d in X_text]\nX_text = np.vstack(docs)\n\ndocs_test = [nlp(d).vector for d in X_test_text]\nX_test_text = np.vstack(docs_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_text, y_text, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For binary classification, we have a multitude of models to choose from. Some of these include Logistic Regression, Support Vector Machines (SVM), Decision Trees, and ensemble methods such as Random Forests. We will be using SVM's due to its simplicity and interpretability, and it achieves quite a decent accuracy on the data as well."},{"metadata":{},"cell_type":"markdown","source":"**SVM** is a margin-based classifier that seeks to maximize the margin, or hyperplane, between the two classes. The model learns by support vectors, which are data points that are relatively close to or at the margin, and these support vectors influence the position and orientation of the hyperplane. \n\nWe will grid search the kernel to be used for SVM as well as the regularization parameter \"C\", which is inversely proportional to regularization. So the bigger C is, the less regularization there is, and the smaller the margin, which will in turn have less support vectors. Conversely, the smaller C, the more regularization there is, and the model seeks to fit a larger margin even if there are misclassified points in your training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# gridsearch svm hyperparameters \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\n\nparam_grid = {'kernel': ['rbf','linear'], 'C': [0.001, 0.01, 0.1, 1, 1.5, 2, 3, 10]}\n\ngrid = GridSearchCV(SVC(), param_grid, cv=10)\ngrid.fit(X_train, y_train)\nprint(grid.best_params_)\ngrid.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From grid searching, we can see that a regularization parameter C = 1.5 with a rfbf kernel achieves the best results. \n\nWe achieve an accuracy of 81.09% on the test set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# get predictions for test data and export\ny_test_text = grid.predict(X_test_text)\ntest['target'] = y_test_text\ntest_export = test.loc[:, ['id', 'target']]\ntest_export.to_csv('disaster_test_word2vec.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}