{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The problem\n--------------------------------\nTwitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it’s not always clear whether a person’s words are actually announcing a disaster.\n\nThe objective\nPredicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.","metadata":{"id":"WsomRoKCF7-Z"}},{"cell_type":"markdown","source":"# Loading Libraries and Data","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow-text","metadata":{"id":"HdZzBKpKXG1z","execution":{"iopub.status.busy":"2021-08-22T10:21:01.901798Z","iopub.execute_input":"2021-08-22T10:21:01.902247Z","iopub.status.idle":"2021-08-22T10:22:08.593074Z","shell.execute_reply.started":"2021-08-22T10:21:01.902163Z","shell.execute_reply":"2021-08-22T10:22:08.591761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport nltk\nimport spacy\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"id":"qcwXvhr78uks","outputId":"b37caf3a-7673-428e-84e4-265102183fae","execution":{"iopub.status.busy":"2021-08-22T10:22:08.597651Z","iopub.execute_input":"2021-08-22T10:22:08.598073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LOADING THE DATA**","metadata":{"id":"uqtugyNv811A"}},{"cell_type":"code","source":"df = pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"id":"7mH2pWt5804w","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"Nzv2TUK18zcR","outputId":"923bbf1e-6c29-4ef4-a868-7ec52b6bcb72","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning the data","metadata":{"id":"75VRTgHYRYpH"}},{"cell_type":"markdown","source":"Filling Nan Values","metadata":{"id":"qZpjgk89RurQ"}},{"cell_type":"code","source":"df.info()","metadata":{"id":"OYONfOE3-F6u","outputId":"72b11be4-107c-4e0d-b2f4-a42044f64bff","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fillna(df, column, fillwith):\n  df[column] = df[column].fillna(fillwith)\n  return df","metadata":{"id":"D9jNkp6y-Jx0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in ['keyword','location']:\n  df = fillna(df,i,'a')","metadata":{"id":"cyGzfLj9-lcw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"S6Q6QX94-zKn","outputId":"892837ff-becc-4754-da8a-6e1b91337822","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"pDw-JbL7-0rX","outputId":"429e3f7d-517c-4ab3-d739-b617b2602453","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['space'] = ' '\ndf['full_text'] = df['keyword'] + df['space'] + df['location']+ df['space'] + df['text']\ndf.drop('keyword', axis=1, inplace=True)\ndf.drop('location', axis=1, inplace=True)\ndf.drop('text', axis=1, inplace=True)\ndf.drop('space', axis=1, inplace=True)\ndf.drop('id', axis=1, inplace=True)","metadata":{"id":"xV58CUtL-2TE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"6_kSRAXy_Ttn","outputId":"8b708f14-7ef4-40a9-bfed-3c9d7120891f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing punctuation and stopwords.","metadata":{"id":"ymzcXUKqR-_y"}},{"cell_type":"code","source":"import string\n\n\ndef remove_punct(text):\n    table = str.maketrans(\"\", \"\", string.punctuation)\n    return text.translate(table)\ndf[\"full_text\"] = df.full_text.map(lambda x: remove_punct(x))\n\nfrom nltk.corpus import stopwords\n\nstop = set(stopwords.words(\"english\"))\n\n\ndef remove_stopwords(text):\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n\n    return \" \".join(text)\n\ndf[\"full_text\"] = df[\"full_text\"].map(remove_stopwords)","metadata":{"id":"P3jQCznD_Uw-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"i5aPD2MaBUmy","outputId":"de399aec-ad85-45d8-e0bb-ea658bd6b470","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualising Named Entity\n\n---\nVisualising these can help us understand the features data might reply upon the most\n","metadata":{"id":"_FiM193VjYsI"}},{"cell_type":"code","source":"df_dis = df[df['target']==1]\ndf_no_dis = df[df['target']==0]","metadata":{"id":"TzkeN7SzjfXK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Func to represent randomly\nnlp = spacy.load('en_core_web_sm')\ndef namedRandom(df):\n  random = [np.random.randint(0,len(df)-1) for i in range(0,5)]\n  for index in random:\n    text = df.full_text.iloc[index]\n    doc = nlp(text)\n    spacy.displacy.render(doc, style=\"ent\", jupyter=True)","metadata":{"id":"voaWwJZri3GW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"namedRandom(df_dis)","metadata":{"id":"aJxQXSkai4Oe","outputId":"2e0c6ea7-3149-4690-e6d2-b3a906a77737","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"namedRandom(df_no_dis)","metadata":{"id":"8g8gYJjvjnjy","outputId":"49d6b8a3-de17-4f9a-9019-59a81d666124","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LOOKING AT THE WORD CLOUD**","metadata":{"id":"kjn4mGEZj_qs"}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\ndef cloudRandom(df, type):\n  random = [np.random.randint(0,len(df)-1) for i in range(0,5)]\n  text = ''\n  for index in random:\n    text = text + df.full_text.iloc[index]\n  wc = WordCloud(background_color=\"white\", \n               max_words=350, \n               width=1000, \n               height=600, \n               random_state=1).generate(text)\n  \n  plt.figure(figsize=(15,15))\n  plt.imshow(wc)\n  plt.axis(\"off\")\n  plt.title('Word Cloud for '+ (type) +' tweets')","metadata":{"id":"7VNKLKxzkfoc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cloudRandom(df_dis, 'disaster')","metadata":{"id":"nXatUC5skpzt","outputId":"eb373190-2b1b-46e5-a820-5d65c9ed2a9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cloudRandom(df_no_dis, 'non - disaster')","metadata":{"id":"UYdkIRjxmbNB","outputId":"b4b4b177-f539-4e27-83d9-a85d00fd24d3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lemmetizing and Stemming the text","metadata":{"id":"O7f9FJi2SGvg"}},{"cell_type":"code","source":"def getLemmText(text):\n tokens=word_tokenize(text)\n lemmatizer = WordNetLemmatizer()\n tokens=[lemmatizer.lemmatize(word) for word in tokens]\n return ' '.join(tokens)\ndf['full_text'] = list(map(getLemmText,df['full_text']))\n\ndef getStemmText(text):\n tokens=word_tokenize(text)\n ps = PorterStemmer()\n tokens=[ps.stem(word) for word in tokens]\n return ' '.join(tokens)\ndf['full_text'] = list(map(getStemmText,df['full_text']))","metadata":{"id":"aNfqsW5QBjAQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory data analysis after cleaning","metadata":{"id":"WV14O1IuSQ6c"}},{"cell_type":"code","source":"#defining visualisation params\nfig_dims = (10, 8)","metadata":{"id":"-5K_3bOmS4iI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking at class distribution to see if it is balanced\nclasses = df['target']\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.countplot(x = classes, ax=ax, palette='Oranges')\nplt.title('Class Distribution')\nplt.xlabel('0: No disaster tweet           1: Disaster tweet')\nplt.ylabel('Count of tweets')\nplt.show()","metadata":{"id":"H72AadJESUq-","outputId":"554fa7a4-ea7d-4968-91ae-2ab08c58623a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks well balanced","metadata":{"id":"P9s-TjBaT-Hd"}},{"cell_type":"code","source":"# Removing # and @ from the tweets.\ndef removetags(text):\n  tags = ['@','#']\n  tokens = word_tokenize(text)\n  tokens = [word for word in tokens if word not in tags]\n  return ' '.join(tokens)\ndf['full_text'] = list(map(removetags,df['full_text']))","metadata":{"id":"AtcBlKVddVOZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for Big Numbers if they can be considered as features or not.","metadata":{"id":"TUghKhR5YpbS"}},{"cell_type":"code","source":"#Logic to find big numbers\nfor i in df.full_text[3].split():\n  print(i)","metadata":{"id":"dSoT48r3VgkR","outputId":"af26d0d1-f245-432d-a297-7bd40a3e23ad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to check bigNumbers Count\ndef bigNumCount(df):\n  bigNum_Count = []\n  truth_val = []\n  for text in df.full_text:\n    for token in text.split():\n      if token.isnumeric():\n        try:\n          if int(token)>10000:\n            bigNum_Count.append(text)\n            break\n        except:\n          pass\n  return bigNum_Count","metadata":{"id":"MnlOemKDWhzs","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking at the Big number analysis\n#looking at BigNumbers in entire dataset\ndf_entire_BigNum = bigNumCount(df)\n# If there are more than 300 bigNum tweets we'll consider big numbers as features\nprint('There are '+ str(len(df_entire_BigNum)) +' big Numbers in the entire dataset.')","metadata":{"id":"IrBZ7JUkUA91","outputId":"886b6111-3791-4ee9-fd2c-e8b2e0b060d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking at the Big number analysis\n#looking at BigNumbers in disaster dataset\ndf_dis = df[df['target']==1]\ndf_dis_BigNum = bigNumCount(df_dis)\n# If there are more than 100 bigNum tweets we'll consider big numbers as features\nprint('There are '+ str(len(df_dis_BigNum)) +' big Numbers in the disaster dataset.')","metadata":{"id":"pQnn8ilsVbvY","outputId":"9d249fb6-78ac-4b2f-94c5-364dbdcd30fc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining a function to replace \ndef replaceNumbers(text):\n  tokens = word_tokenize(text)\n  tokens = [word if word.isalpha() else 'bignumber' for word in tokens]\n  return ' '.join(tokens)\nprint('Example:')\nprint('text : 13000 one guy')\nprint(replaceNumbers('result : 13000 one guy'))\ndf['full_text'] = list(map(replaceNumbers,df['full_text']))","metadata":{"id":"o1XYIjWDZI1P","outputId":"865ea1cc-0930-4fbf-f5e8-1b65a954ed98","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Code to remove numbers If you decide otherwise.\n# def removeNumbers(text):\n#   tokens = word_tokenize(text)\n#   tokens = [word for word in tokens if word.isalpha()]\n#   return ' '.join(tokens)\n\n# df['full_text'] = list(map(removeNumbers,df['full_text']))","metadata":{"id":"ep_317HKkLZD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"rYAKQ4I4k0tx","outputId":"1caa8f7f-bf3b-46c4-971e-08410d2fdfac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training The model using BERT","metadata":{"id":"S0RdtFMmm0A-"}},{"cell_type":"code","source":"labels = df.target","metadata":{"id":"8vG_qFLpDWwm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(df.full_text.values, labels, \n random_state=42, test_size=0.3, shuffle=True, stratify = labels)","metadata":{"id":"Rwa0v5VrCML4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Choosing BERT model for training","metadata":{"id":"8aO3U0NQpYi_"}},{"cell_type":"code","source":"bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_base/2',\n    'electra_small':\n        'https://tfhub.dev/google/electra_small/2',\n    'electra_base':\n        'https://tfhub.dev/google/electra_base/2',\n    'experts_pubmed':\n        'https://tfhub.dev/google/experts/bert/pubmed/2',\n    'experts_wiki_books':\n        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'electra_small':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'electra_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_pubmed':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_wiki_books':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')","metadata":{"id":"ajbQeawHdgES","outputId":"6aa48094-15db-48d1-db7e-3bb071eeb853","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_preprocess = hub.KerasLayer(tfhub_handle_preprocess)\nbert_encoder = hub.KerasLayer(tfhub_handle_encoder)","metadata":{"id":"4iI-jCD6WROz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bert layers\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\n\n# Neural network layers\nl = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\nl = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n\n# Use inputs and outputs to construct a final model\nmodel = tf.keras.Model(inputs=[text_input], outputs = [l])","metadata":{"id":"S7Jq1z7lfAuA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with tpu_strategy.scope(): creating the model in the TPUStrategy scope means we will train the model on the TPU\n# model = create_model()","metadata":{"id":"iupE93sbWPVL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","metadata":{"id":"wQQTR3ZNYmKJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(xtrain, ytrain, epochs=10, validation_data=(xtest, ytest))","metadata":{"id":"e_a4bQLNffJi","outputId":"69e8e2db-732d-49a2-fc79-b56dbda941a9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result Evaluation","metadata":{"id":"egkFZBFCpelb"}},{"cell_type":"code","source":"def graph_plots(history, string):\n plt.plot(history.history[string])\n plt.plot(history.history['val_'+string])\n plt.xlabel('Epochs')\n plt.ylabel(string)\n plt.legend([string, 'val_'+string])\n plt.show()\n \ngraph_plots(model.history, 'accuracy')\ngraph_plots(model.history, 'loss')","metadata":{"id":"Ocg9QsEAfluX","outputId":"ca37b2c1-1d0b-4031-f143-c24ce51fd93c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\npredicted = model.predict(xtest)","metadata":{"id":"558aOV209uHz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(predicted)","metadata":{"id":"hYvNM8P1_bMZ","outputId":"848aa47a-9c15-498f-9093-9f7e01b2125f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = []\nfor i in predicted:\n  if i>0.5:\n    results.append(1)\n  else:\n    results.append(0)","metadata":{"id":"3oQvTqXOAalL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real = np.array(ytest)\npredicted = results","metadata":{"id":"B3xeN7ZEA0VN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(real,predicted))","metadata":{"id":"BJWEfJ4L-f6Q","outputId":"de4a2d76-0e1b-4e50-bd03-2cd4aaf5c610","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"id":"sZNo9gKiBPcF"}},{"cell_type":"code","source":"subtest = pd.read_csv('../input/nlp-getting-started/test.csv')\nsubtest.head()","metadata":{"id":"oOZnq1TjCfde","outputId":"d96cf6a7-2871-4e8f-fcde-5a97f7052021","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Pre-processing on Test data","metadata":{"id":"O3U_B_f3D1R1"}},{"cell_type":"code","source":"for i in ['keyword','location']:\n  subtest = fillna(subtest,i,'a')\n\nsubtest['space'] = ' '\nsubtest['full_text'] = subtest['keyword'] + subtest['space'] + subtest['location']+ subtest['space'] + subtest['text']\nsubtest.drop('keyword', axis=1, inplace=True)\nsubtest.drop('location', axis=1, inplace=True)\nsubtest.drop('text', axis=1, inplace=True)\nsubtest.drop('space', axis=1, inplace=True)\nsubtest.drop('id', axis=1, inplace=True)\n\nsubtest[\"full_text\"] = subtest.full_text.map(lambda x: remove_punct(x))\nsubtest[\"full_text\"] = subtest[\"full_text\"].map(remove_stopwords)\n\nsubtest['full_text'] = list(map(getLemmText,subtest['full_text']))\nsubtest['full_text'] = list(map(getStemmText,subtest['full_text']))\n\nsubtest['full_text'] = list(map(removetags,subtest['full_text']))\nsubtest['full_text'] = list(map(replaceNumbers,subtest['full_text']))","metadata":{"id":"Lgyw90qeC2lp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subtest.head()","metadata":{"id":"WYPvhMYzD-bt","outputId":"b9266077-fe64-4502-a85b-d5c643bd6048","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred = model.predict(subtest)\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('sub.csv', index=False)","metadata":{"id":"5knh6fOfA-Qf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}