{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:37:10.922612Z","iopub.execute_input":"2021-10-25T07:37:10.922983Z","iopub.status.idle":"2021-10-25T07:37:10.951638Z","shell.execute_reply.started":"2021-10-25T07:37:10.922889Z","shell.execute_reply":"2021-10-25T07:37:10.951038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import some of the important libraries.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy.sparse import csr_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n\n#import accuracy\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:37:22.502686Z","iopub.execute_input":"2021-10-25T07:37:22.503243Z","iopub.status.idle":"2021-10-25T07:37:24.202131Z","shell.execute_reply.started":"2021-10-25T07:37:22.503207Z","shell.execute_reply":"2021-10-25T07:37:24.201329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:37:24.896232Z","iopub.execute_input":"2021-10-25T07:37:24.897206Z","iopub.status.idle":"2021-10-25T07:37:24.988965Z","shell.execute_reply.started":"2021-10-25T07:37:24.897165Z","shell.execute_reply":"2021-10-25T07:37:24.988141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make a copy , Trying not to make any changes in original dataset","metadata":{}},{"cell_type":"code","source":"tr = train.copy()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:37:31.392858Z","iopub.execute_input":"2021-10-25T07:37:31.393811Z","iopub.status.idle":"2021-10-25T07:37:31.398924Z","shell.execute_reply.started":"2021-10-25T07:37:31.393755Z","shell.execute_reply":"2021-10-25T07:37:31.398318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:37:43.302274Z","iopub.execute_input":"2021-10-25T07:37:43.30253Z","iopub.status.idle":"2021-10-25T07:37:43.310541Z","shell.execute_reply.started":"2021-10-25T07:37:43.302504Z","shell.execute_reply":"2021-10-25T07:37:43.309681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:37:46.711644Z","iopub.execute_input":"2021-10-25T07:37:46.712061Z","iopub.status.idle":"2021-10-25T07:37:46.730789Z","shell.execute_reply.started":"2021-10-25T07:37:46.712031Z","shell.execute_reply":"2021-10-25T07:37:46.729907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets Drop 'id' column as it is not usefull for analysis","metadata":{}},{"cell_type":"code","source":"tr.drop('id', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:40:13.583084Z","iopub.execute_input":"2021-10-25T07:40:13.583346Z","iopub.status.idle":"2021-10-25T07:40:13.594804Z","shell.execute_reply.started":"2021-10-25T07:40:13.583319Z","shell.execute_reply":"2021-10-25T07:40:13.593836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store Test_id , it will be require at the time of submission\n\ntest_id = test['id']","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:40:20.452526Z","iopub.execute_input":"2021-10-25T07:40:20.45318Z","iopub.status.idle":"2021-10-25T07:40:20.459318Z","shell.execute_reply.started":"2021-10-25T07:40:20.453131Z","shell.execute_reply":"2021-10-25T07:40:20.458441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check for duplicate records","metadata":{}},{"cell_type":"code","source":"duplicates_record = tr[tr.duplicated(['text'], keep=False)]\nduplicates_record","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:41:07.322673Z","iopub.execute_input":"2021-10-25T07:41:07.322958Z","iopub.status.idle":"2021-10-25T07:41:07.346046Z","shell.execute_reply.started":"2021-10-25T07:41:07.322927Z","shell.execute_reply":"2021-10-25T07:41:07.345139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicates_record.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:42:33.064762Z","iopub.execute_input":"2021-10-25T07:42:33.065732Z","iopub.status.idle":"2021-10-25T07:42:33.071824Z","shell.execute_reply.started":"2021-10-25T07:42:33.065683Z","shell.execute_reply":"2021-10-25T07:42:33.071007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Around 179 duplicate records are found. Now lets consider (Text , target) and only keep the first record of duplicate record and remove remaining.","metadata":{}},{"cell_type":"code","source":"tr.drop_duplicates(subset = ['text','target'], keep = 'first', inplace = True, ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:46:10.012114Z","iopub.execute_input":"2021-10-25T07:46:10.012677Z","iopub.status.idle":"2021-10-25T07:46:10.02356Z","shell.execute_reply.started":"2021-10-25T07:46:10.012624Z","shell.execute_reply":"2021-10-25T07:46:10.022885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets check again","metadata":{}},{"cell_type":"code","source":"duplicates_record = tr[tr.duplicated(['text'], keep=False)]\nduplicates_record.head(6)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:50:16.688485Z","iopub.execute_input":"2021-10-25T07:50:16.689438Z","iopub.status.idle":"2021-10-25T07:50:16.705368Z","shell.execute_reply.started":"2021-10-25T07:50:16.689394Z","shell.execute_reply":"2021-10-25T07:50:16.704509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Still we see there are various duplicate records are available but with one difference, this time for same tweet , one time target is 0 and for another it is 1. This seems to be corrupted records and ML model may get confuse, so its better to remove these records.","metadata":{}},{"cell_type":"code","source":"tr.drop_duplicates(subset = ['text'], keep = False, inplace = True, ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:50:33.90912Z","iopub.execute_input":"2021-10-25T07:50:33.909389Z","iopub.status.idle":"2021-10-25T07:50:33.918354Z","shell.execute_reply.started":"2021-10-25T07:50:33.909362Z","shell.execute_reply":"2021-10-25T07:50:33.917664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now Lets Check if there is any missing values.","metadata":{}},{"cell_type":"code","source":"tr.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:50:52.01268Z","iopub.execute_input":"2021-10-25T07:50:52.013226Z","iopub.status.idle":"2021-10-25T07:50:52.022289Z","shell.execute_reply.started":"2021-10-25T07:50:52.013189Z","shell.execute_reply":"2021-10-25T07:50:52.021741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keyword has 56 missing value , we will deal with those in sometime, and Location has many missing values also for this analysis , i am considering Location is not an important feature, So lets move on.","metadata":{}},{"cell_type":"code","source":"tr['keyword'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:53:43.778416Z","iopub.execute_input":"2021-10-25T07:53:43.77873Z","iopub.status.idle":"2021-10-25T07:53:43.792624Z","shell.execute_reply.started":"2021-10-25T07:53:43.778696Z","shell.execute_reply":"2021-10-25T07:53:43.791686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr['location'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:53:51.628228Z","iopub.execute_input":"2021-10-25T07:53:51.628518Z","iopub.status.idle":"2021-10-25T07:53:51.635549Z","shell.execute_reply.started":"2021-10-25T07:53:51.628478Z","shell.execute_reply":"2021-10-25T07:53:51.634763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr[~tr['location'].isna()]","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:53:59.233573Z","iopub.execute_input":"2021-10-25T07:53:59.234032Z","iopub.status.idle":"2021-10-25T07:53:59.248549Z","shell.execute_reply.started":"2021-10-25T07:53:59.234Z","shell.execute_reply":"2021-10-25T07:53:59.247655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets look at target variable.","metadata":{}},{"cell_type":"code","source":"tr['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:54:53.066754Z","iopub.execute_input":"2021-10-25T07:54:53.067013Z","iopub.status.idle":"2021-10-25T07:54:53.074758Z","shell.execute_reply.started":"2021-10-25T07:54:53.066986Z","shell.execute_reply":"2021-10-25T07:54:53.073927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(tr['target']);","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:56:02.198055Z","iopub.execute_input":"2021-10-25T07:56:02.198511Z","iopub.status.idle":"2021-10-25T07:56:02.305248Z","shell.execute_reply.started":"2021-10-25T07:56:02.198471Z","shell.execute_reply":"2021-10-25T07:56:02.304725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems to be very much balanced dataset.","metadata":{}},{"cell_type":"markdown","source":"Lets seprate out target variable.","metadata":{}},{"cell_type":"code","source":"Y = tr['target']\ntr.drop('target', axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:56:40.55757Z","iopub.execute_input":"2021-10-25T07:56:40.557858Z","iopub.status.idle":"2021-10-25T07:56:40.563598Z","shell.execute_reply.started":"2021-10-25T07:56:40.55783Z","shell.execute_reply":"2021-10-25T07:56:40.562773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:57:14.114091Z","iopub.execute_input":"2021-10-25T07:57:14.114473Z","iopub.status.idle":"2021-10-25T07:57:14.121572Z","shell.execute_reply.started":"2021-10-25T07:57:14.114432Z","shell.execute_reply":"2021-10-25T07:57:14.120441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:58:51.968282Z","iopub.execute_input":"2021-10-25T07:58:51.968621Z","iopub.status.idle":"2021-10-25T07:58:51.975495Z","shell.execute_reply.started":"2021-10-25T07:58:51.968572Z","shell.execute_reply":"2021-10-25T07:58:51.97419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T07:59:22.389065Z","iopub.execute_input":"2021-10-25T07:59:22.390486Z","iopub.status.idle":"2021-10-25T07:59:22.403214Z","shell.execute_reply.started":"2021-10-25T07:59:22.39042Z","shell.execute_reply":"2021-10-25T07:59:22.401887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, i am combining the train and test dataset, before converting word to vectors.\nThe reason is if we do it separately then there is a good chance that the vector we will get in train set and test set will not be same and while predicting the test tweet the train and test features should be same.","metadata":{}},{"cell_type":"code","source":"tr = pd.concat([tr,test], axis = 0)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:04:31.244111Z","iopub.execute_input":"2021-10-25T08:04:31.244456Z","iopub.status.idle":"2021-10-25T08:04:31.254003Z","shell.execute_reply.started":"2021-10-25T08:04:31.244423Z","shell.execute_reply":"2021-10-25T08:04:31.253108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:04:40.1684Z","iopub.execute_input":"2021-10-25T08:04:40.168867Z","iopub.status.idle":"2021-10-25T08:04:40.175942Z","shell.execute_reply.started":"2021-10-25T08:04:40.16883Z","shell.execute_reply":"2021-10-25T08:04:40.174952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets do some text cleaning","metadata":{}},{"cell_type":"code","source":"alpha = [' ','a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:05:51.109022Z","iopub.execute_input":"2021-10-25T08:05:51.109672Z","iopub.status.idle":"2021-10-25T08:05:51.115707Z","shell.execute_reply.started":"2021-10-25T08:05:51.109621Z","shell.execute_reply":"2021-10-25T08:05:51.114969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is the 2 method (Stemmer and Lemmatizer) to bring the word to its root word.","metadata":{}},{"cell_type":"code","source":"ps = nltk.PorterStemmer()\nwn = nltk.WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:05:56.517658Z","iopub.execute_input":"2021-10-25T08:05:56.518141Z","iopub.status.idle":"2021-10-25T08:05:56.523093Z","shell.execute_reply.started":"2021-10-25T08:05:56.518102Z","shell.execute_reply":"2021-10-25T08:05:56.52208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr['keyword'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:07:23.607359Z","iopub.execute_input":"2021-10-25T08:07:23.608008Z","iopub.status.idle":"2021-10-25T08:07:23.61593Z","shell.execute_reply.started":"2021-10-25T08:07:23.607957Z","shell.execute_reply":"2021-10-25T08:07:23.615278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets not remove missing value present in Keyword, instead we can replace the NaN with Text 'missing'. in order to avoid data loss.","metadata":{}},{"cell_type":"code","source":"tr['keyword'] = np.where(tr['keyword'].isna(),'missing', tr['keyword'])","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:08:38.908784Z","iopub.execute_input":"2021-10-25T08:08:38.90915Z","iopub.status.idle":"2021-10-25T08:08:38.919569Z","shell.execute_reply.started":"2021-10-25T08:08:38.909109Z","shell.execute_reply":"2021-10-25T08:08:38.918789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr['keyword'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:09:28.273663Z","iopub.execute_input":"2021-10-25T08:09:28.274116Z","iopub.status.idle":"2021-10-25T08:09:28.282262Z","shell.execute_reply.started":"2021-10-25T08:09:28.274084Z","shell.execute_reply":"2021-10-25T08:09:28.281196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets clean the keyword, as we see in some of the keyword '%20' is added not sure why and how, but lets remove those.","metadata":{}},{"cell_type":"code","source":"def cleanKeyword(text):\n    \n    text = text.lower()   # to convert to all lower case text.\n    text = text.replace('%20',' ')   # Remove '%20' if present.\n    text = ' '.join([ps.stem(word) for word in text.split(' ')]) # To bring the word to its root word.\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:11:43.028261Z","iopub.execute_input":"2021-10-25T08:11:43.028527Z","iopub.status.idle":"2021-10-25T08:11:43.034273Z","shell.execute_reply.started":"2021-10-25T08:11:43.0285Z","shell.execute_reply":"2021-10-25T08:11:43.033404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr['clean_keyword'] = tr['keyword'].apply(cleanKeyword)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:11:51.449426Z","iopub.execute_input":"2021-10-25T08:11:51.44976Z","iopub.status.idle":"2021-10-25T08:11:51.714247Z","shell.execute_reply.started":"2021-10-25T08:11:51.449727Z","shell.execute_reply":"2021-10-25T08:11:51.713115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr['clean_keyword'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:12:31.063172Z","iopub.execute_input":"2021-10-25T08:12:31.063443Z","iopub.status.idle":"2021-10-25T08:12:31.071022Z","shell.execute_reply.started":"2021-10-25T08:12:31.063414Z","shell.execute_reply":"2021-10-25T08:12:31.070127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr['clean_keyword'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:12:52.597707Z","iopub.execute_input":"2021-10-25T08:12:52.598455Z","iopub.status.idle":"2021-10-25T08:12:52.605018Z","shell.execute_reply.started":"2021-10-25T08:12:52.598418Z","shell.execute_reply":"2021-10-25T08:12:52.604412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Much cleaner and shorter, Earlier there were total 221 unique values and now after cleaning it reduced to 167.","metadata":{}},{"cell_type":"code","source":"stopWords = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:13:45.048631Z","iopub.execute_input":"2021-10-25T08:13:45.04888Z","iopub.status.idle":"2021-10-25T08:13:45.059765Z","shell.execute_reply.started":"2021-10-25T08:13:45.048855Z","shell.execute_reply":"2021-10-25T08:13:45.058704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punct = string.punctuation\npunct","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:13:49.477255Z","iopub.execute_input":"2021-10-25T08:13:49.47755Z","iopub.status.idle":"2021-10-25T08:13:49.482232Z","shell.execute_reply.started":"2021-10-25T08:13:49.477519Z","shell.execute_reply":"2021-10-25T08:13:49.48156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#stopWords","metadata":{"execution":{"iopub.status.busy":"2021-10-24T19:15:28.779117Z","iopub.execute_input":"2021-10-24T19:15:28.779844Z","iopub.status.idle":"2021-10-24T19:15:28.786768Z","shell.execute_reply.started":"2021-10-24T19:15:28.779813Z","shell.execute_reply":"2021-10-24T19:15:28.785945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets clean the Text variable.","metadata":{}},{"cell_type":"code","source":"def cleanText(text):\n    \n    text = text.lower() # change to lower case\n    text = ''.join([char for char in text if char in alpha]) # remove anything that is not an alphabet.\n    text = ' '.join([ps.stem(word) for word in text.split(' ') if ((word not in stopWords) & (len(word)>1))])  # Bring the word to its root word.\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:16:01.758035Z","iopub.execute_input":"2021-10-25T08:16:01.758807Z","iopub.status.idle":"2021-10-25T08:16:01.763667Z","shell.execute_reply.started":"2021-10-25T08:16:01.758762Z","shell.execute_reply":"2021-10-25T08:16:01.762735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr['text_clean'] = tr['text'].apply(cleanText)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:16:05.866842Z","iopub.execute_input":"2021-10-25T08:16:05.867273Z","iopub.status.idle":"2021-10-25T08:16:08.360545Z","shell.execute_reply.started":"2021-10-25T08:16:05.86723Z","shell.execute_reply":"2021-10-25T08:16:08.359937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:16:08.362005Z","iopub.execute_input":"2021-10-25T08:16:08.362468Z","iopub.status.idle":"2021-10-25T08:16:08.3759Z","shell.execute_reply.started":"2021-10-25T08:16:08.362427Z","shell.execute_reply":"2021-10-25T08:16:08.374885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'clean_keyword' feature alone do not contribute much in analysis, so lets combine it with 'text_clean' and create a brand new feature 'clean_tweet'.","metadata":{}},{"cell_type":"code","source":"tr['clean_tweet'] = tr['text_clean'] + ' ' + tr['clean_keyword']","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:17:59.084344Z","iopub.execute_input":"2021-10-25T08:17:59.084636Z","iopub.status.idle":"2021-10-25T08:17:59.094112Z","shell.execute_reply.started":"2021-10-25T08:17:59.084581Z","shell.execute_reply":"2021-10-25T08:17:59.093218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets apply  word to vector method.","metadata":{}},{"cell_type":"code","source":"vector = TfidfVectorizer(sublinear_tf=True, max_features=2700)\nX = vector.fit_transform(tr['clean_tweet'].values)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:19:15.918314Z","iopub.execute_input":"2021-10-25T08:19:15.918576Z","iopub.status.idle":"2021-10-25T08:19:16.116007Z","shell.execute_reply.started":"2021-10-25T08:19:15.918548Z","shell.execute_reply":"2021-10-25T08:19:16.115222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, for max_features i am taking value as 2700, As i tried with several values from 500 to 3000, and 2700 was giving best accuracy.\n\nAlso it is important to select some value for max_features as if it is not selected then the method (TfidfVectorizer) will create feature for each word and you will end up in getting very very very large number of features (in this case it was more tha 25000) and while creating model you may get memory error.\n\nAnd by giving some values (lets say 2700) to max_features, it will select 2700 most important features.","metadata":{}},{"cell_type":"code","source":"X_col = vector.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:21:07.128126Z","iopub.execute_input":"2021-10-25T08:21:07.128722Z","iopub.status.idle":"2021-10-25T08:21:07.136172Z","shell.execute_reply.started":"2021-10-25T08:21:07.128674Z","shell.execute_reply":"2021-10-25T08:21:07.135407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"X_col will give the all the 2700 feature selected.","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame.sparse.from_spmatrix(X, columns = X_col)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:26:25.167384Z","iopub.execute_input":"2021-10-25T08:26:25.167816Z","iopub.status.idle":"2021-10-25T08:26:25.213701Z","shell.execute_reply.started":"2021-10-25T08:26:25.167784Z","shell.execute_reply":"2021-10-25T08:26:25.212836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:26:33.56856Z","iopub.execute_input":"2021-10-25T08:26:33.568904Z","iopub.status.idle":"2021-10-25T08:26:34.092632Z","shell.execute_reply.started":"2021-10-25T08:26:33.568869Z","shell.execute_reply":"2021-10-25T08:26:34.091805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:26:43.40321Z","iopub.execute_input":"2021-10-25T08:26:43.403545Z","iopub.status.idle":"2021-10-25T08:26:43.409646Z","shell.execute_reply.started":"2021-10-25T08:26:43.403507Z","shell.execute_reply":"2021-10-25T08:26:43.408691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets separate out Test and train data.","metadata":{}},{"cell_type":"code","source":"test = df.iloc[7485:]","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:28:20.669006Z","iopub.execute_input":"2021-10-25T08:28:20.669772Z","iopub.status.idle":"2021-10-25T08:28:21.204939Z","shell.execute_reply.started":"2021-10-25T08:28:20.669734Z","shell.execute_reply":"2021-10-25T08:28:21.203967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.reset_index(drop = True , inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:28:26.396966Z","iopub.execute_input":"2021-10-25T08:28:26.397222Z","iopub.status.idle":"2021-10-25T08:28:26.401729Z","shell.execute_reply.started":"2021-10-25T08:28:26.397195Z","shell.execute_reply":"2021-10-25T08:28:26.40083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:28:28.10722Z","iopub.execute_input":"2021-10-25T08:28:28.107487Z","iopub.status.idle":"2021-10-25T08:28:28.455127Z","shell.execute_reply.started":"2021-10-25T08:28:28.107455Z","shell.execute_reply":"2021-10-25T08:28:28.454314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = df.iloc[:7485]","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:28:35.388673Z","iopub.execute_input":"2021-10-25T08:28:35.388954Z","iopub.status.idle":"2021-10-25T08:28:36.162121Z","shell.execute_reply.started":"2021-10-25T08:28:35.388916Z","shell.execute_reply":"2021-10-25T08:28:36.161148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:28:37.577413Z","iopub.execute_input":"2021-10-25T08:28:37.577692Z","iopub.status.idle":"2021-10-25T08:28:37.583771Z","shell.execute_reply.started":"2021-10-25T08:28:37.577665Z","shell.execute_reply":"2021-10-25T08:28:37.582997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All looks good, lets create Model","metadata":{}},{"cell_type":"code","source":"\ncv = KFold(n_splits=10, random_state=1, shuffle=True)\n# create model\nmodel = LogisticRegression()\n# evaluate model\nscores = cross_val_score(model,train_df, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: ',((scores).mean()))","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:29:30.728682Z","iopub.execute_input":"2021-10-25T08:29:30.728968Z","iopub.status.idle":"2021-10-25T08:29:45.358451Z","shell.execute_reply.started":"2021-10-25T08:29:30.728938Z","shell.execute_reply":"2021-10-25T08:29:45.357713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"80.4 % accuracy not that bad.....","metadata":{}},{"cell_type":"code","source":"model.fit(train_df, Y)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:30:29.928774Z","iopub.execute_input":"2021-10-25T08:30:29.929561Z","iopub.status.idle":"2021-10-25T08:30:30.15845Z","shell.execute_reply.started":"2021-10-25T08:30:29.929523Z","shell.execute_reply":"2021-10-25T08:30:30.157646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets Use Count Vectorizor and see if it is better than Tfidf","metadata":{}},{"cell_type":"code","source":"count_vector = CountVectorizer(encoding='utf-8', max_features=2500)\nX_count = count_vector.fit_transform(tr['clean_tweet'].values)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:31:36.458543Z","iopub.execute_input":"2021-10-25T08:31:36.45884Z","iopub.status.idle":"2021-10-25T08:31:36.644094Z","shell.execute_reply.started":"2021-10-25T08:31:36.458812Z","shell.execute_reply":"2021-10-25T08:31:36.64303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_count_col = count_vector.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:31:37.167788Z","iopub.execute_input":"2021-10-25T08:31:37.168076Z","iopub.status.idle":"2021-10-25T08:31:37.174309Z","shell.execute_reply.started":"2021-10-25T08:31:37.168045Z","shell.execute_reply":"2021-10-25T08:31:37.173385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_mat = pd.DataFrame.sparse.from_spmatrix(X_count, columns = X_count_col)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:31:38.627308Z","iopub.execute_input":"2021-10-25T08:31:38.627883Z","iopub.status.idle":"2021-10-25T08:31:38.827546Z","shell.execute_reply.started":"2021-10-25T08:31:38.627837Z","shell.execute_reply":"2021-10-25T08:31:38.826673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_mat.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:31:39.037625Z","iopub.execute_input":"2021-10-25T08:31:39.037885Z","iopub.status.idle":"2021-10-25T08:31:39.358159Z","shell.execute_reply.started":"2021-10-25T08:31:39.037859Z","shell.execute_reply":"2021-10-25T08:31:39.357282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = KFold(n_splits=10, random_state=1, shuffle=True)\n# create model\nmodel = LogisticRegression()\n# evaluate model\nscores = cross_val_score(model,train_df, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n# report performance\nprint('Accuracy: ',((scores).mean()))","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:31:39.597026Z","iopub.execute_input":"2021-10-25T08:31:39.59729Z","iopub.status.idle":"2021-10-25T08:31:51.483445Z","shell.execute_reply.started":"2021-10-25T08:31:39.597264Z","shell.execute_reply":"2021-10-25T08:31:51.482345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exactly same......","metadata":{}},{"cell_type":"code","source":"model.fit(train_df, Y)","metadata":{"execution":{"iopub.status.busy":"2021-10-25T08:31:51.485207Z","iopub.execute_input":"2021-10-25T08:31:51.485513Z","iopub.status.idle":"2021-10-25T08:31:51.618221Z","shell.execute_reply.started":"2021-10-25T08:31:51.485473Z","shell.execute_reply":"2021-10-25T08:31:51.617378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Submission**","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T19:16:41.436841Z","iopub.execute_input":"2021-10-24T19:16:41.437741Z","iopub.status.idle":"2021-10-24T19:16:41.618522Z","shell.execute_reply.started":"2021-10-24T19:16:41.43769Z","shell.execute_reply":"2021-10-24T19:16:41.617586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T19:16:41.620289Z","iopub.execute_input":"2021-10-24T19:16:41.620693Z","iopub.status.idle":"2021-10-24T19:16:41.628057Z","shell.execute_reply.started":"2021-10-24T19:16:41.620649Z","shell.execute_reply":"2021-10-24T19:16:41.627194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a submisison dataframe and append the relevant columns\n\nsubmit=pd.DataFrame()\nsubmit['id'] = test_id\nsubmit['target'] = y_pred # our model predictions on the test dataset\nsubmit.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T19:16:41.629841Z","iopub.execute_input":"2021-10-24T19:16:41.630108Z","iopub.status.idle":"2021-10-24T19:16:41.648169Z","shell.execute_reply.started":"2021-10-24T19:16:41.630079Z","shell.execute_reply":"2021-10-24T19:16:41.647171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(submit) == len(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-24T19:16:41.651156Z","iopub.execute_input":"2021-10-24T19:16:41.652263Z","iopub.status.idle":"2021-10-24T19:16:41.660769Z","shell.execute_reply.started":"2021-10-24T19:16:41.652214Z","shell.execute_reply":"2021-10-24T19:16:41.659843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert submisison dataframe to csv for submission to csv \n# for Kaggle submisison\nsubmit.to_csv('../Disaster tweet.csv', index=False)\nprint('Submission CSV is ready!')","metadata":{"execution":{"iopub.status.busy":"2021-10-24T19:16:41.662015Z","iopub.execute_input":"2021-10-24T19:16:41.663354Z","iopub.status.idle":"2021-10-24T19:16:41.679991Z","shell.execute_reply.started":"2021-10-24T19:16:41.66331Z","shell.execute_reply":"2021-10-24T19:16:41.679367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissions_check = pd.read_csv(\"../Disaster tweet.csv\")\nsubmissions_check.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-24T19:16:41.681429Z","iopub.execute_input":"2021-10-24T19:16:41.681881Z","iopub.status.idle":"2021-10-24T19:16:41.696123Z","shell.execute_reply.started":"2021-10-24T19:16:41.681847Z","shell.execute_reply":"2021-10-24T19:16:41.695366Z"},"trusted":true},"execution_count":null,"outputs":[]}]}