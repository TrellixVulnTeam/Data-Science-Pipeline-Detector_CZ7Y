{"cells":[{"metadata":{},"cell_type":"markdown","source":"I used GPT-2 model in HuggingFace Library. <br>\nI refered to following links, [ðŸŽ»Fine-tune Transformers in PyTorch using ðŸ¤— Transformers](https://gmihaila.medium.com/fine-tune-transformers-in-pytorch-using-transformers-57b40450635)."},{"metadata":{},"cell_type":"markdown","source":"### 1. Model and Tokenizer\n\nIn ðŸ¤—, they prepared GPT2 model for classification in advance. Very Thankful! <br>\nHere's link: https://huggingface.co/transformers/model_doc/gpt2.html#transformers.GPT2ForSequenceClassification"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import set_seed, GPT2Config, GPT2Tokenizer, GPT2ForSequenceClassification\n\nset_seed(731) # My Birthday!, you should get train_loss: 0.773, train_acc: 0.567 in epoch 0.\n\nmodel_config = GPT2Config.from_pretrained('gpt2', num_labels=2) # Binary Classification\nmodel = GPT2ForSequenceClassification.from_pretrained('gpt2', config=model_config)\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.padding_side = \"left\" # Very Important\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.config.pad_token_id = model.config.eos_token_id","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Build Dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nfrom torch.utils.data import Dataset\n\nclass TweetDataset(Dataset):\n    def __init__(self, train=True):\n        super().__init__()\n        self.train = train\n        self.data = pd.read_csv(os.path.join('/kaggle/input/nlp-getting-started/', 'train.csv' if train else 'test.csv'))\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        record = self.data.iloc[index]\n        text = record['text']\n        if self.train:\n            return {'text': text, 'label': record['target']}\n        else:\n            return {'text': text, 'label': '0'}\n\ntrain_dataset = TweetDataset(train=True)\ntest_dataset = TweetDataset(train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    print(train_dataset.__getitem__(i)['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Data Collator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Gpt2ClassificationCollator(object):\n    def __init__(self, tokenizer, max_seq_len=None):\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n        \n        return\n    \n    def __call__(self, sequences):\n        texts = [sequence['text'] for sequence in sequences]\n        labels = [int(sequence['label']) for sequence in sequences]\n        inputs = self.tokenizer(text=texts,\n                                return_tensors='pt',\n                                padding=True,\n                                truncation=True,\n                                max_length=self.max_seq_len)\n        inputs.update({'labels': torch.tensor(labels)})\n        \n        return inputs\n\ngpt2classificationcollator = Gpt2ClassificationCollator(tokenizer=tokenizer,\n                                                        max_seq_len=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. DataLoader"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\n\ntrain_size = int(len(train_dataset) * 0.8)\nval_size = len(train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\ntrain_dataloader = DataLoader(dataset=train_dataset,\n                              batch_size=32,\n                              shuffle=True,\n                              collate_fn=gpt2classificationcollator)\nval_dataloader = DataLoader(dataset=val_dataset,\n                            batch_size=32,\n                            shuffle=False,\n                            collate_fn=gpt2classificationcollator)\ntest_dataloader = DataLoader(dataset=test_dataset,\n                             batch_size=32,\n                             shuffle=False,\n                             collate_fn=gpt2classificationcollator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Optimizer & Lr Scheduler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AdamW, get_cosine_schedule_with_warmup\n\ntotal_epochs = 10\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\noptimizer = AdamW(optimizer_grouped_parameters,\n                  lr=1e-5,\n                  eps=1e-8)\n\nnum_train_steps = len(train_dataloader) * total_epochs\nnum_warmup_steps = int(num_train_steps * 0.1) \n\nlr_scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                               num_warmup_steps=num_warmup_steps,\n                                               num_training_steps = num_train_steps)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Train & Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\ndef train(dataloader, optimizer, scheduler, device_):\n    global model\n    model.train()\n    \n    prediction_labels = []\n    true_labels = []\n    \n    total_loss = []\n    \n    for batch in dataloader:\n        true_labels += batch['labels'].numpy().flatten().tolist()\n        batch = {k:v.type(torch.long).to(device_) for k, v in batch.items()}\n        \n        \n        outputs = model(**batch)\n        loss, logits = outputs[:2]\n        logits = logits.detach().cpu().numpy()\n        total_loss.append(loss.item())\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # prevent exploding gradient\n\n        optimizer.step()\n        scheduler.step()\n        \n        prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n    \n    return true_labels, prediction_labels, total_loss\n\ndef validation(dataloader, device_):\n    global model\n    model.eval()\n    \n    prediction_labels = []\n    true_labels = []\n    \n    total_loss = []\n    \n    for batch in dataloader:\n        true_labels += batch['labels'].numpy().flatten().tolist()\n        batch = {k:v.type(torch.long).to(device_) for k, v in batch.items()}\n        \n        with torch.no_grad():\n            outputs = model(**batch)\n            loss, logits = outputs[:2]\n            logits = logits.detach().cpu().numpy()\n            total_loss.append(loss.item())\n\n            prediction_labels += logits.argmax(axis=-1).flatten().tolist()\n        \n    return true_labels, prediction_labels, total_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Run!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\n\nall_loss = {'train_loss': [], 'val_loss': []}\nall_acc = {'train_acc': [], 'val_acc': []}\n\nfor epoch in range(total_epochs):\n    y, y_pred, train_loss = train(train_dataloader, optimizer, lr_scheduler, device)\n    train_acc = accuracy_score(y, y_pred)\n    \n    y, y_pred, val_loss = validation(val_dataloader, device)\n    val_acc = accuracy_score(y, y_pred)\n    \n    all_loss['train_loss'] += train_loss\n    all_loss['val_loss'] += val_loss\n    \n    all_acc['train_acc'].append(train_acc)\n    all_acc['val_acc'].append(val_acc)\n    \n    print(f'Epoch: {epoch}, train_loss: {torch.tensor(train_loss).mean():.3f}, train_acc: {train_acc:.3f}, val_loss: {torch.tensor(val_loss).mean():.3f}, val_acc: {val_acc:.3f}') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.1. Check Loss with Graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfig = plt.figure(figsize=(20,20))\na = fig.add_subplot(4, 1, 1)\nb = fig.add_subplot(4, 1, 2)\nc = fig.add_subplot(2, 1, 2)\na.plot(all_loss['train_loss'])\nb.plot(all_loss['val_loss'])\nc.plot(all_acc['train_acc'])\nc.plot(all_acc['val_acc'])\nc.set(xlabel='epoch', ylabel='accuracy')\nc.legend(['train', 'val'])\n\npass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. Run on Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"_, y_pred, _ = validation(test_dataloader, device)\n\nsubmit = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsubmit['target'] = y_pred\n\nsubmit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}