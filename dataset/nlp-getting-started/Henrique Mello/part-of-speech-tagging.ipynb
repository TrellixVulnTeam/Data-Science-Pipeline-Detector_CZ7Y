{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"![](https://d33wubrfki0l68.cloudfront.net/d5cbc4b0e14c20f877366b69b9171649afe11fda/d96a8/assets/images/bigram-hmm/pos-title.jpg)\n\n# Introduction\n\nWhen creating machine learning models using NLP, people generally tend to use techniques that deal directly with the words in the text, such as bag of words. However, one could wonder, is there a difference between the frequency of adjectives, verbs and substantives in disasters tweets when compared to common tweets? This is a good technique when dealing with long texts (above 100 words usually), but maybe it's worth trying it here. It has also been tested to detect fake news with a very good accuracy [by the authors of this github repo](https://github.com/roneysco/Fake.br-Corpus).\n\nQuoting wikipedia,\n> In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its contextâ€”i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc. \n\nBasically, what we want to do is to use NLTK library to pos-tag a sentence and count the number of each class on them. \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\nimport os\nprint(\"File Folders:\")\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ntrain[\"isTrain\"] = True\ntest[\"isTrain\"] = False\n\nfull = pd.concat([train, test])\nfull","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering\n\nThe number of links and tags (like @Someone) seem to be important factors, as we tend to share links to the actual news when talking about disasters. Hashtags also seem to be important, along with the their number on a specific tweet, so we'll track those as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_at(row):\n    return re.findall(\"@[\\w]+\", row[\"text\"])\n\ndef get_http(row):\n    return re.findall(\"http[\\:\\/\\.\\w]+\", row[\"text\"])\n\ndef get_hashtags(row):\n    return re.findall(\"#[\\w]+\", row[\"text\"])\n\ndef number_of_tags(row):\n    return len(row[\"tags\"])\n\ndef number_of_links(row):\n    return len(row[\"links\"])\n\ndef number_of_hashs(row):\n    return len(row[\"hashtags\"])\n\ndef clean_text(row):\n    clean = row[\"text\"]\n    \n    if len(row[\"tags\"]) != 0:\n        for word in row[\"tags\"]:\n            clean = clean.replace(word, \"\")\n    \n    if len(row[\"links\"]) != 0:\n        for word in row[\"links\"]:\n            clean = clean.replace(word, \"\")\n    \n    #only remove the # symbol\n    clean = clean.replace(\"#\", \"\").replace(\"/\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n    \n    return clean.strip()\n\nfull[\"tags\"] = full.apply(lambda row: get_at(row), axis = 1)\nfull[\"links\"] = full.apply(lambda row: get_http(row), axis = 1)\nfull[\"hashtags\"] = full.apply(lambda row: get_hashtags(row), axis = 1)\n\nfull[\"number_of_tags\"] = full.apply(lambda row: number_of_tags(row), axis = 1)\nfull[\"number_of_links\"] = full.apply(lambda row: number_of_links(row), axis = 1)\nfull[\"number_of_hashs\"] = full.apply(lambda row: number_of_hashs(row), axis = 1)\n\nfull[\"clean_text\"] = full.apply(lambda row: clean_text(row), axis = 1)\nfull.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have cleaned our texts and stored links, hashtags and tags, it's time for the real deal. We'll first tokenize our texts and use them to get our grammatical classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\ndef get_tokens(row):\n    return word_tokenize(row[\"clean_text\"].lower())\n\nfull[\"tokens\"] = full.apply(lambda row: get_tokens(row), axis = 1)\nfull.sample(5, random_state = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = [\"screams\", \"in\", \"the\", \"distance\"]\n\ndef get_postags(row):\n    \n    postags = nltk.pos_tag(row[\"tokens\"])\n    list_classes = list()\n    for  word in postags:\n        list_classes.append(word[1])\n    \n    return list_classes\n\nfull[\"postags\"] = full.apply(lambda row: get_postags(row), axis = 1)\nfull.sample(5, random_state = 4)\n# nltk.help.upenn_tagset('NNS')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have the POS tags for every text. There are lots of categories and I'll focus only in a few of them. Here are the meanings:\n\n- NN: noun (there are other categories that can fit within this one for our purposes, such as NNS, NNP, NNPS, which all belong to nouns, containing plurals and proper names)\n- RB: adverb\n- VB: verb (and similar categories indicating tense: VBP, VBG, VBS..)\n- JJ: adjective or numeral\n\n\n**Note:** *If you want to get more information about the classes that pos_tag can identify, use the command `nltk.help.upenn_tagset(\"NNS\")` for instance. If you want to see all the similar categories, you can just write the first letter of the class: `nltk.help.upenn_tagset(\"N\")`.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_no_class(count, class_name = \"\"):\n    total = 0\n    for key in count.keys():\n        if key.startswith(class_name):\n            total += count[key]\n            \n            \n    return total\n\ndef get_classes(row, grammatical_class = \"\"):\n    count = Counter(row[\"postags\"])\n    return find_no_class(count, class_name = grammatical_class)/len(row[\"postags\"])\n\nfull[\"freqAdverbs\"] = full.apply(lambda row: get_classes(row, \"RB\"), axis = 1)\nfull[\"freqVerbs\"] = full.apply(lambda row: get_classes(row, \"VB\"), axis = 1)\nfull[\"freqAdjectives\"] = full.apply(lambda row: get_classes(row, \"JJ\"), axis = 1)\nfull[\"freqNouns\"] = full.apply(lambda row: get_classes(row, \"NN\"), axis = 1)\n\nfull.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"training = full.loc[full[\"isTrain\"] == True, :].copy()\ntesting = full.loc[full[\"isTrain\"] == False, :].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## A few visualizations of the POS for disaster and non-disaster tweets\n\nNote: non-disaster tweets are shown in blue\n\n### Noun frequency"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"training.loc[training[\"target\"] == 0.0, \"freqNouns\"].hist(alpha = 0.5);\ntraining.loc[training[\"target\"] == 1.0, \"freqNouns\"].hist(alpha = 0.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Verb Frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"training.loc[training[\"target\"] == 0.0, \"freqVerbs\"].hist(alpha = 0.5);\ntraining.loc[training[\"target\"] == 1.0, \"freqVerbs\"].hist(alpha = 0.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adjective frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"training.loc[training[\"target\"] == 0.0, \"freqAdjectives\"].hist(alpha = 0.5);\ntraining.loc[training[\"target\"] == 1.0, \"freqAdjectives\"].hist(alpha = 0.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of frequencies for the three grammatical classes are about the same for disaster and non-disaster tweets.  The most prominent difference being the number of instances. In that case, we have more non-disaster than disaster tweets in the training dataset. This distribution study is very important to do because there are cases (such as fake news detection mentioned earlier) in which both distributions have statistically different distributions, and this fact may help in detection.\n\nWe are ready to make a first model out of the variables we have. Let's create a simple train-test split."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nx = training.loc[:, [\"number_of_tags\", \"number_of_links\", \"freqAdverbs\", \"freqVerbs\", \"freqAdjectives\", \"freqNouns\"]]\ny = training.loc[:, \"target\"]\n\nskf = StratifiedKFold(n_splits=5)\nskf.get_n_splits(x, y)\n\nfor train_index, test_index in skf.split(x, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    x_train, x_test = x.loc[train_index], x.loc[test_index]\n    y_train, y_test = y.loc[train_index], y.loc[test_index]\n    \n    clf = GradientBoostingClassifier(learning_rate=0.1, max_depth= 5, max_features = 5,random_state = 42)\n#     clf = RandomForestClassifier(random_state = 42)\n    \n    clf.fit(x_train, y_train)\n    preds = clf.predict(x_test)\n    \n    print(accuracy_score(y_test, preds))\n    \n    print(confusion_matrix(y_test, preds))\n\n\ntotal_preds = clf.predict(x)\nprint(\"Confusion Matrix:\")\nconfusion_matrix(y,total_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the feature importance is"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.figure(figsize=(20,15))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, x.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After a few tests, it seems that our simple model using only these engineered variables gave us approximately 60% accuracy. It is also evident that the number of links on a page plays a big role in detecting disaster tweets. Maybe this could be a variable used in other kernels to improve scores. Before ending the notebook, let's check the distribution of number of links for both disaster and non-disaster tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"training.loc[training[\"target\"] == 0.0, \"number_of_links\"].hist(alpha = 0.5);\ntraining.loc[training[\"target\"] == 1.0, \"number_of_links\"].hist(alpha = 0.5);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indeed, most non-disaster tweets have no links at all, while most disaster ones have at least one.\n\nNow we'll just score the test dataset and make the submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = clf.predict(testing.loc[:, [\"number_of_tags\", \"number_of_links\", \"freqAdverbs\", \"freqVerbs\", \"freqAdjectives\", \"freqNouns\"]])\ntesting[\"prediction\"] = preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission[\"target\"] = preds.astype(int)\nsubmission.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}