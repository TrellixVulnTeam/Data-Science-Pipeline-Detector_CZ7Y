{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-29T15:09:11.723344Z","iopub.execute_input":"2022-04-29T15:09:11.723658Z","iopub.status.idle":"2022-04-29T15:09:11.734709Z","shell.execute_reply.started":"2022-04-29T15:09:11.723616Z","shell.execute_reply":"2022-04-29T15:09:11.733935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch \n\nfrom torch.utils.data import TensorDataset, random_split\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom transformers import BertForSequenceClassification,  BertConfig, AdamW\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer, AutoModel\n\nimport time\nimport re","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:11.736437Z","iopub.execute_input":"2022-04-29T15:09:11.738876Z","iopub.status.idle":"2022-04-29T15:09:11.744316Z","shell.execute_reply.started":"2022-04-29T15:09:11.738838Z","shell.execute_reply":"2022-04-29T15:09:11.743524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n# Set the seed value all over the place to make this reproducible.  \ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:11.745561Z","iopub.execute_input":"2022-04-29T15:09:11.746003Z","iopub.status.idle":"2022-04-29T15:09:11.753564Z","shell.execute_reply.started":"2022-04-29T15:09:11.745967Z","shell.execute_reply":"2022-04-29T15:09:11.75269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get data","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/nlp-getting-started'\ndf = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv' )\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv' )","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:11.754881Z","iopub.execute_input":"2022-04-29T15:09:11.755355Z","iopub.status.idle":"2022-04-29T15:09:11.789113Z","shell.execute_reply.started":"2022-04-29T15:09:11.755318Z","shell.execute_reply":"2022-04-29T15:09:11.788432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelname = 'bert-base-uncased' #'google/electra-base-discriminator'# \nbatchsize =40\ntokenizer = AutoTokenizer.from_pretrained(modelname) ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:11.79034Z","iopub.execute_input":"2022-04-29T15:09:11.790736Z","iopub.status.idle":"2022-04-29T15:09:14.939393Z","shell.execute_reply.started":"2022-04-29T15:09:11.790694Z","shell.execute_reply":"2022-04-29T15:09:14.938535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"def clean(text):\n    text = text.lower()\n    text=text.replace(r'&amp;?',r'and')\n    text=text.replace(r'&lt;',r'<')\n    text=text.replace(r'&gt;',r'>')\n    \n    res = re.sub(r'http(s)?:\\/\\/([\\w\\.\\/])*' ,' ',text) # clean url http://x.x.x.x/xxx\n    res = re.sub('[0-9]+', '', res) #  clean numbers\n    res = re.sub(r'[!\"#$%&()*+,-./:;=?@\\\\^_`\"~\\t\\n\\<\\>\\[\\]\\{\\}]',' ',res) # clean special chars。\n    res = re.sub(r'  +',' ',res) # convert multiple continues blank char to one blank char。\n    return res.strip()\n\ndf['text'] = df['text'].apply(clean)\ntest_df['text'] =test_df['text'].apply(clean)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:14.94099Z","iopub.execute_input":"2022-04-29T15:09:14.941237Z","iopub.status.idle":"2022-04-29T15:09:15.098752Z","shell.execute_reply.started":"2022-04-29T15:09:14.941204Z","shell.execute_reply":"2022-04-29T15:09:15.097972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef plot_sentence_embeddings_length(text_list, tokenizer):\n    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t), text_list))\n    tokenized_texts_len = list(map(lambda t: len(t), tokenized_texts))\n    fig, ax = plt.subplots(figsize=(8, 5));\n    ax.hist(tokenized_texts_len, bins=40);\n    ax.set_xlabel(\"Length of Comment Embeddings\");\n    ax.set_ylabel(\"Number of Comments\");\n    return max(tokenized_texts_len)\n\nmaxlen1 = plot_sentence_embeddings_length(df['text'].values, tokenizer)\nmaxlen2 = plot_sentence_embeddings_length(test_df['text'].values, tokenizer)\nmaxlen = max(maxlen1,maxlen2)\nprint(maxlen)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:15.100175Z","iopub.execute_input":"2022-04-29T15:09:15.100439Z","iopub.status.idle":"2022-04-29T15:09:16.999227Z","shell.execute_reply.started":"2022-04-29T15:09:15.100406Z","shell.execute_reply":"2022-04-29T15:09:16.998523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split dataset","metadata":{}},{"cell_type":"code","source":"df = df.sample(frac=1.0)\ntrain_df = df[:6500] \nvalid_df = df[6500:] \nprint(train_df.shape,valid_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:17.006073Z","iopub.execute_input":"2022-04-29T15:09:17.00882Z","iopub.status.idle":"2022-04-29T15:09:17.022274Z","shell.execute_reply.started":"2022-04-29T15:09:17.00878Z","shell.execute_reply":"2022-04-29T15:09:17.019634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train element","metadata":{}},{"cell_type":"code","source":"maxlen = 50\ndef prepare_input(text, keyword):\n    keystr = str(keyword)\n    inputs = tokenizer(text, #keystr, \n                           add_special_tokens=True,\n                           max_length= maxlen,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:17.030329Z","iopub.execute_input":"2022-04-29T15:09:17.030601Z","iopub.status.idle":"2022-04-29T15:09:17.044328Z","shell.execute_reply.started":"2022-04-29T15:09:17.030564Z","shell.execute_reply":"2022-04-29T15:09:17.043024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Package DataSet","metadata":{}},{"cell_type":"code","source":"class TrainDataset(Dataset):\n    def __init__(self,vdf):\n        self.target = vdf['target'].values\n        self.text = vdf['text'].values\n        self.keyword = vdf['keyword'].values\n        self.location = vdf['location'].values\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        inputs = prepare_input( \n                               self.text[item], \n                               self.keyword[item])\n        label = torch.tensor(self.target[item], dtype=torch.long)\n        \n        return (inputs['input_ids'],inputs['token_type_ids'],inputs['attention_mask'] ), label\n\n# Combine the training inputs into a TensorDataset.\n# dataset = TensorDataset(input_ids, attention_masks, labels)\ntrain_ds = TrainDataset(train_df) \nvalid_ds = TrainDataset(valid_df) ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:17.049396Z","iopub.execute_input":"2022-04-29T15:09:17.049691Z","iopub.status.idle":"2022-04-29T15:09:17.066378Z","shell.execute_reply.started":"2022-04-29T15:09:17.049641Z","shell.execute_reply":"2022-04-29T15:09:17.065608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_ds,\n                              batch_size=batchsize,\n                              shuffle=True,\n                              pin_memory=True, drop_last=False)\nvalid_loader = DataLoader(valid_ds,\n                              batch_size=batchsize,\n                              shuffle=True,\n                              pin_memory=True, drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:17.070347Z","iopub.execute_input":"2022-04-29T15:09:17.071548Z","iopub.status.idle":"2022-04-29T15:09:17.08411Z","shell.execute_reply.started":"2022-04-29T15:09:17.071508Z","shell.execute_reply":"2022-04-29T15:09:17.08294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\n    modelname, # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:17.091045Z","iopub.execute_input":"2022-04-29T15:09:17.093926Z","iopub.status.idle":"2022-04-29T15:09:18.95729Z","shell.execute_reply.started":"2022-04-29T15:09:17.093768Z","shell.execute_reply":"2022-04-29T15:09:18.956577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tell pytorch to run this model on the GPU.\nmodel.cuda()\nprint('the model is initialized')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:18.958553Z","iopub.execute_input":"2022-04-29T15:09:18.959407Z","iopub.status.idle":"2022-04-29T15:09:19.076713Z","shell.execute_reply.started":"2022-04-29T15:09:18.959351Z","shell.execute_reply":"2022-04-29T15:09:19.075826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n# I believe the 'W' stands for 'Weight Decay fix\"\noptimizer = AdamW(model.parameters(),\n                  lr = 2.48e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:19.078265Z","iopub.execute_input":"2022-04-29T15:09:19.078566Z","iopub.status.idle":"2022-04-29T15:09:19.088102Z","shell.execute_reply.started":"2022-04-29T15:09:19.078525Z","shell.execute_reply":"2022-04-29T15:09:19.087332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs. The BERT authors recommend between 2 and 4. \n# We chose to run for 4, but we'll see later that this may be over-fitting the\n# training data.\nepochs = 3 # 4\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_loader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:19.089139Z","iopub.execute_input":"2022-04-29T15:09:19.089381Z","iopub.status.idle":"2022-04-29T15:09:19.099364Z","shell.execute_reply.started":"2022-04-29T15:09:19.089349Z","shell.execute_reply":"2022-04-29T15:09:19.098387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:19.101164Z","iopub.execute_input":"2022-04-29T15:09:19.101695Z","iopub.status.idle":"2022-04-29T15:09:19.108444Z","shell.execute_reply.started":"2022-04-29T15:09:19.101643Z","shell.execute_reply":"2022-04-29T15:09:19.10764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:19.109355Z","iopub.execute_input":"2022-04-29T15:09:19.110808Z","iopub.status.idle":"2022-04-29T15:09:19.117377Z","shell.execute_reply.started":"2022-04-29T15:09:19.110771Z","shell.execute_reply":"2022-04-29T15:09:19.116608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:19.118516Z","iopub.execute_input":"2022-04-29T15:09:19.1194Z","iopub.status.idle":"2022-04-29T15:09:19.126055Z","shell.execute_reply.started":"2022-04-29T15:09:19.119348Z","shell.execute_reply":"2022-04-29T15:09:19.125346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Process","metadata":{}},{"cell_type":"code","source":"# We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    for step, (inputs, labels) in enumerate(train_loader):\n\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_loader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = inputs[0] .to(device)\n        b_input_mask = inputs[1].to(device)\n        b_labels = labels.to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # The documentation for this `model` function is here: \n        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n        # It returns different numbers of parameters depending on what arguments\n        # arge given and what flags are set. For our useage here, it returns\n        # the loss (because we provided labels) and the \"logits\"--the model\n        # outputs prior to activation.\n        \n        output= model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n        loss = output[0]\n        logits =output[1]\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_loader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch,(inputs, labels) in enumerate(valid_loader):\n        \n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using \n        # the `to` method.\n        #\n        b_input_ids = inputs[0] .to(device)\n        b_input_mask = inputs[2].to(device)\n        b_labels = labels.to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n            # Get the \"logits\" output by the model. The \"logits\" are the output\n            # values prior to applying an activation function like the softmax.\n            output = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n            loss = output[0]\n            logits =output[1]\n            \n            \n        # Accumulate the validation loss.\n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n\n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy / len(valid_loader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(valid_loader)\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:09:19.127549Z","iopub.execute_input":"2022-04-29T15:09:19.128027Z","iopub.status.idle":"2022-04-29T15:11:04.485923Z","shell.execute_reply.started":"2022-04-29T15:09:19.127992Z","shell.execute_reply":"2022-04-29T15:11:04.485088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\")\nprint(\"Training complete!\")\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:11:04.487321Z","iopub.execute_input":"2022-04-29T15:11:04.487784Z","iopub.status.idle":"2022-04-29T15:11:04.499772Z","shell.execute_reply.started":"2022-04-29T15:11:04.487746Z","shell.execute_reply":"2022-04-29T15:11:04.498767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Prediction","metadata":{}},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self,vdf):\n        self.text = vdf['text'].values\n        self.keyword = vdf['keyword'].values\n        self.location = vdf['location'].values\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, item):\n        inputs = prepare_input( \n                               self.text[item], \n                               self.keyword[item])        \n        \n        return (inputs['input_ids'],inputs['token_type_ids'],inputs['attention_mask']) ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:11:04.500968Z","iopub.execute_input":"2022-04-29T15:11:04.502261Z","iopub.status.idle":"2022-04-29T15:11:04.511577Z","shell.execute_reply.started":"2022-04-29T15:11:04.502117Z","shell.execute_reply":"2022-04-29T15:11:04.510598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = TestDataset(test_df)\ntest_loader = DataLoader(test_ds,\n                              batch_size=batchsize,\n                              shuffle=False,\n                              pin_memory=True, drop_last=False)\npredictions =np.array([]) ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:11:04.513209Z","iopub.execute_input":"2022-04-29T15:11:04.513893Z","iopub.status.idle":"2022-04-29T15:11:04.524989Z","shell.execute_reply.started":"2022-04-29T15:11:04.513854Z","shell.execute_reply":"2022-04-29T15:11:04.524112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nfor batch,inputs in enumerate(test_loader):\n        \n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using \n        # the `to` method.\n        #\n        b_input_ids = inputs[0] .to(device)\n        b_input_mask = inputs[2].to(device)\n\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():   \n            output = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask\n                                   )\n            logits =output[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        # y_pred_label = logits.argmax(dim=1)\n        pred_flat = np.argmax(logits, axis=1).flatten()\n        predictions = np.concatenate([predictions,pred_flat])","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:11:04.526553Z","iopub.execute_input":"2022-04-29T15:11:04.527148Z","iopub.status.idle":"2022-04-29T15:11:10.279762Z","shell.execute_reply.started":"2022-04-29T15:11:04.527111Z","shell.execute_reply":"2022-04-29T15:11:10.279018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = {\"id\":test_df['id'],\n                 \"target\":predictions.astype(int)}\nsubmission = pd.DataFrame(submission_df)\nsubmission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T15:11:10.280881Z","iopub.execute_input":"2022-04-29T15:11:10.28334Z","iopub.status.idle":"2022-04-29T15:11:10.295453Z","shell.execute_reply.started":"2022-04-29T15:11:10.283302Z","shell.execute_reply":"2022-04-29T15:11:10.294721Z"},"trusted":true},"execution_count":null,"outputs":[]}]}