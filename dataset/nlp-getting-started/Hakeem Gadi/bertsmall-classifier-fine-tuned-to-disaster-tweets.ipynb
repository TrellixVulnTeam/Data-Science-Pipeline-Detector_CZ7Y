{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine tuning a BERT model to classify *natural disaster* tweets to literal and figurative\n\nThe language describing natural disaster is commonly used in figurative way to describe other situation s. The sentence \"The scientific community unleashed a tsunami of tsunami of articles criticising the paper\" illustrates this linguistic phenomenon. In this notebook, you will see how to use the proven capabilities of fine tuning a pre-trained BERT model to distinguish between tweets meant figuratively and tweets intended to describe real natural disasters, and thus achieve transfer learning. Pretrained models are a promising area of application because it brings the value of state-of-the-art models built and optimised by the likes of Google (BERT is Google's creation) to the hands of every machine learning practitioner. ","metadata":{}},{"cell_type":"markdown","source":"# Loading the data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-14T02:51:08.181985Z","iopub.execute_input":"2022-06-14T02:51:08.182356Z","iopub.status.idle":"2022-06-14T02:51:08.190406Z","shell.execute_reply.started":"2022-06-14T02:51:08.182326Z","shell.execute_reply":"2022-06-14T02:51:08.189351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntrain_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:51:08.237411Z","iopub.execute_input":"2022-06-14T02:51:08.23777Z","iopub.status.idle":"2022-06-14T02:51:08.295487Z","shell.execute_reply.started":"2022-06-14T02:51:08.237745Z","shell.execute_reply":"2022-06-14T02:51:08.294804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nprint(test_data.size)\ntest_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:51:08.297913Z","iopub.execute_input":"2022-06-14T02:51:08.298295Z","iopub.status.idle":"2022-06-14T02:51:08.329777Z","shell.execute_reply.started":"2022-06-14T02:51:08.298258Z","shell.execute_reply":"2022-06-14T02:51:08.328912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring the data\n\nThe first thing we notice from the partially displayed dataframes above is that the columns: `keyword` and `location` have many `NaN` values in them; something that we need to find a way to remedy later.","metadata":{}},{"cell_type":"markdown","source":"## A peak into tweet lengths\n\nThe histogram below shows the number of tweets per length","metadata":{}},{"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = [15, 5.50]\nplt.rcParams[\"figure.autolayout\"] = True\ntweet_lengths=map(lambda x:len(x),train_data['text'])\nplt.hist(list(tweet_lengths),50)\n                  \nplt.grid(True)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:51:08.353229Z","iopub.execute_input":"2022-06-14T02:51:08.353482Z","iopub.status.idle":"2022-06-14T02:51:08.785745Z","shell.execute_reply.started":"2022-06-14T02:51:08.353459Z","shell.execute_reply":"2022-06-14T02:51:08.784979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The keyword column\nLet us, first explore the `keyword` column for usefulness for the current task","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:50:22.298097Z","iopub.execute_input":"2022-06-12T20:50:22.298442Z","iopub.status.idle":"2022-06-12T20:50:22.308936Z","shell.execute_reply.started":"2022-06-12T20:50:22.298412Z","shell.execute_reply":"2022-06-12T20:50:22.307897Z"}}},{"cell_type":"code","source":"number_of_unique_keywords=train_data['keyword'].unique().size\nprint(f'Number of unique keywords: {number_of_unique_keywords}')","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:51:08.787185Z","iopub.execute_input":"2022-06-14T02:51:08.787464Z","iopub.status.idle":"2022-06-14T02:51:08.794885Z","shell.execute_reply.started":"2022-06-14T02:51:08.787438Z","shell.execute_reply":"2022-06-14T02:51:08.793829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of unique values in `keyword` is 222. Such a reduction of number from the total number of samples (more than 7000) can be promising in the classification task. But let's keep exploring. Below is a plot of each keyword's count.","metadata":{}},{"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = [15, 30]\nplt.rcParams[\"figure.autolayout\"] = True\ntrain_data['keyword'].value_counts().plot(kind='barh')\nplt.grid(True)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:51:08.796494Z","iopub.execute_input":"2022-06-14T02:51:08.797003Z","iopub.status.idle":"2022-06-14T02:51:11.877327Z","shell.execute_reply.started":"2022-06-14T02:51:08.796965Z","shell.execute_reply":"2022-06-14T02:51:11.876558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below is a table showing all the keywords in this columns.","metadata":{}},{"cell_type":"code","source":"utd=train_data['keyword'].unique()\n#Convert NaN to 'NaN'\nuutd=['NaN' if e is np.nan else e for e in utd]\nuutd=np.array(uutd)\n#Extend list to have a perfect square size\nextension=15*15-uutd.size\nxutd=np.hstack((uutd,\n                    np.full((extension,),\n                            'None')))\nxutd=np.reshape(xutd,(-1,15))\npd.DataFrame(xutd)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:51:11.879032Z","iopub.execute_input":"2022-06-14T02:51:11.879407Z","iopub.status.idle":"2022-06-14T02:51:11.909774Z","shell.execute_reply.started":"2022-06-14T02:51:11.879374Z","shell.execute_reply":"2022-06-14T02:51:11.909126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While there are ways for utilizing the `keyword` column, in this notebook we will drop it because it requires rethinking our model's architecture. A second reason is that if we manage to train a model based on tweets alone we will have a more general model for sorting out tweets about natural disaster based only on the tweet text. We will also drop the `location` column.","metadata":{}},{"cell_type":"markdown","source":"# Preparing the training and the test sets","metadata":{}},{"cell_type":"code","source":"drop_columns = ['location','keyword']\ntarget = train_data.pop('target')\ntrain_data=train_data.drop(columns=drop_columns)\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:51:11.911095Z","iopub.execute_input":"2022-06-14T02:51:11.911472Z","iopub.status.idle":"2022-06-14T02:51:11.926696Z","shell.execute_reply.started":"2022-06-14T02:51:11.911437Z","shell.execute_reply":"2022-06-14T02:51:11.925952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data=test_data.drop(columns=drop_columns)\ntest_data","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:51:11.927735Z","iopub.execute_input":"2022-06-14T02:51:11.929361Z","iopub.status.idle":"2022-06-14T02:51:11.941725Z","shell.execute_reply.started":"2022-06-14T02:51:11.929326Z","shell.execute_reply":"2022-06-14T02:51:11.94096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You may have noticed from the keyword table above that some keywords had `%20` between words. This is a space character. It is better to convert it to a normal space so that our model handles it as a space. This is vary crucial to do, since spaces are a prominant feature of the English script -and many scripts for that matter. From transformer-based model point of view, such as BERT the pretrained attention heads encode high attention to spaces, since they signal changes in sequences of letters. So as a first step, we need to check whether the text column has any of these spaces.","metadata":{}},{"cell_type":"code","source":"tweet_substring=map(lambda x:'%20' in x,train_data['text'])\nnum_spaces = sum(list(tweet_substring))\nprint(f'{num_spaces} wierd spaces were found')","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:51:11.943075Z","iopub.execute_input":"2022-06-14T02:51:11.943622Z","iopub.status.idle":"2022-06-14T02:51:11.951812Z","shell.execute_reply.started":"2022-06-14T02:51:11.943589Z","shell.execute_reply":"2022-06-14T02:51:11.950841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_substring=map(lambda x:'%20' in x,test_data['text'])\nnum_spaces = sum(list(tweet_substring))\nprint(f'{num_spaces} wierd spaces were found')","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:51:11.953208Z","iopub.execute_input":"2022-06-14T02:51:11.953948Z","iopub.status.idle":"2022-06-14T02:51:11.962102Z","shell.execute_reply.started":"2022-06-14T02:51:11.95391Z","shell.execute_reply":"2022-06-14T02:51:11.961055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No wierd spaces were found in either the training set or the test set. Indeed there are numerous other aspects of the data set we can investigate. For example, tweet texts have many short urls that do not seem to contain any information in their cryptic format, their presence in the tweet could be informative, it is a good idea to try and unify them to one string just to signal their presence without the unifromative variation they have. But, for the time being, let us limit ourselves to the preprocessing we've done above and delve right into building and our BERT model.","metadata":{}},{"cell_type":"markdown","source":"# Before using BERT\n\n","metadata":{}},{"cell_type":"markdown","source":"BERT uses a preprocessor unit as the first stage to convert the text to the proper embedding space that the encoder model understands. The package tensorflow-text is required for the preprocessor to work. We will also install tf-models-official to make use of the AdamW optimiser, which is much better optimiser for transformers than Adam. If you're running this notbook without GPU you the first install line below will produce several errors, please ignore them along with the warnings, as they should not affect the code execution below.","metadata":{}},{"cell_type":"code","source":"#Required for the preprocessor to work\n!pip install -q -U \"tensorflow-text==2.8.*\"","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:51:11.963502Z","iopub.execute_input":"2022-06-14T02:51:11.963961Z","iopub.status.idle":"2022-06-14T02:52:30.921322Z","shell.execute_reply.started":"2022-06-14T02:51:11.963926Z","shell.execute_reply":"2022-06-14T02:52:30.920117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Required for AdamW\n!pip install -q tf-models-official==2.7.0","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:52:30.925185Z","iopub.execute_input":"2022-06-14T02:52:30.925504Z","iopub.status.idle":"2022-06-14T02:53:23.761707Z","shell.execute_reply.started":"2022-06-14T02:52:30.925475Z","shell.execute_reply":"2022-06-14T02:53:23.760576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\nimport tensorflow as tf\nimport tensorflow_hub as hub \nimport tensorflow_text as text\nfrom official.nlp import optimization\n\ntf.get_logger().setLevel('ERROR')","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:53:23.764528Z","iopub.execute_input":"2022-06-14T02:53:23.765282Z","iopub.status.idle":"2022-06-14T02:53:27.662604Z","shell.execute_reply.started":"2022-06-14T02:53:23.765241Z","shell.execute_reply":"2022-06-14T02:53:27.66075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing a TensorFlow dataset","metadata":{}},{"cell_type":"markdown","source":"Prepare the training set `disaster_ds` with a batch size of 32, and then check that two values look Ok for a good measure. TensorFlow datasets encapsulate many functionalities essential to data sets such as batch size and prefetching to speed up excecution, sample shuffling to mitigate sample correlation effect, setting training/validation splits and many others. We will only use batch size in this notebook to keep things simple. ","metadata":{"execution":{"iopub.status.busy":"2022-06-12T20:52:58.064477Z","iopub.execute_input":"2022-06-12T20:52:58.065623Z","iopub.status.idle":"2022-06-12T20:52:58.069746Z","shell.execute_reply.started":"2022-06-12T20:52:58.065585Z","shell.execute_reply":"2022-06-12T20:52:58.068943Z"}}},{"cell_type":"code","source":"batch_size = 32\ndisaster_ds = tf.data.Dataset.from_tensor_slices((train_data['text'], target)).batch(batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:53:27.663868Z","iopub.execute_input":"2022-06-14T02:53:27.664593Z","iopub.status.idle":"2022-06-14T02:53:28.89831Z","shell.execute_reply.started":"2022-06-14T02:53:27.664554Z","shell.execute_reply":"2022-06-14T02:53:28.897412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test of the training set contains the desired data\nfor row in disaster_ds.take(2):\n    print(row)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:53:28.899576Z","iopub.execute_input":"2022-06-14T02:53:28.899997Z","iopub.status.idle":"2022-06-14T02:53:28.91689Z","shell.execute_reply.started":"2022-06-14T02:53:28.899955Z","shell.execute_reply":"2022-06-14T02:53:28.914979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The BERT with a thousand faces","metadata":{}},{"cell_type":"markdown","source":"BERT comes in many flavours, versions and sizes. You need to match the model to the compatible preprocessing module. The one we chose for this task is the English uncased (all letters are lower-cased) with 4 transformer layers (L), 128 characters maximum input length, and output embeding dimension of 512 per tocken (H). Each model and its corresponding preprocessor can be retrieved from TensorFlow Hub using the URL associated with it. `KerasLayer` uses URL as a handle to wrap the preprocessor and the encoder as Keras Layers.","metadata":{}},{"cell_type":"code","source":"encoder_url ='https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\npreprocessor_url='https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:53:28.918353Z","iopub.execute_input":"2022-06-14T02:53:28.918751Z","iopub.status.idle":"2022-06-14T02:53:28.925204Z","shell.execute_reply.started":"2022-06-14T02:53:28.918713Z","shell.execute_reply":"2022-06-14T02:53:28.924194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor = hub.KerasLayer(preprocessor_url)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:53:28.927015Z","iopub.execute_input":"2022-06-14T02:53:28.927522Z","iopub.status.idle":"2022-06-14T02:53:31.633845Z","shell.execute_reply.started":"2022-06-14T02:53:28.927482Z","shell.execute_reply":"2022-06-14T02:53:31.633027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test the preprocessor model on a text string to get a feel of what the preprocessor does.","metadata":{}},{"cell_type":"code","source":"test_txt = ['His haircut is an absolute disaster']\ntext_out = preprocessor(test_txt)\ntext_out","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:53:31.635259Z","iopub.execute_input":"2022-06-14T02:53:31.635615Z","iopub.status.idle":"2022-06-14T02:53:31.820949Z","shell.execute_reply.started":"2022-06-14T02:53:31.635581Z","shell.execute_reply":"2022-06-14T02:53:31.819992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the model","metadata":{}},{"cell_type":"markdown","source":"- The first layer of the model is the `Input` layer, whose purpose is to produce a Keras tensor object based on the `shape` and `dtype`, as well as other parameters. The tensor contains enough information to build the model and connect the layers automatically to the next layer.\n- The preprocessing layer follows the the `Input` layer and is the first layer a string goes through. This layer remains frozen, as its purpose is to recast a string into the model's embedded space.\n- The pre-trained encoder comes next, and here we need to set `trainable` to `True` since the encoder weights will be optimised based on the `disaster_ds` data set.\n- The encoder, by design, makes available the outputs of each transformer to allow for more flexibility in reusing BERT as part of other architectures. But the output of the model for sequential tasks such as language modeling tasks is all the outputs of the last transformer. In the case of classification tasks (like the one in hand) only the first vector of the 128 output vectors is normally used (The [CLS] vector, which is called the pooled output)\n- The final layer is constituted of only one neuron, and has linear activation (default). Therefore, it is equivalent to linear regression. The ouput is simply: $\\sum_{i=1}^{512} w_ix_i+b_i$","metadata":{}},{"cell_type":"code","source":"def build_bert_classifier():\n  tweet = tf.keras.layers.Input(shape=(), dtype=tf.string, name='tweets')\n  preprocessing_layer = hub.KerasLayer(preprocessor_url, name='preprocessing')\n  encoder_inputs = preprocessing_layer(tweet)\n#Make sure to make the encoder is trainable\n  encoder = hub.KerasLayer(encoder_url, trainable=True, name='BERT_encoder')\n  outputs = encoder(encoder_inputs)\n  net = outputs['pooled_output']\n  net = tf.keras.layers.Dropout(0.1)(net)\n  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n  return tf.keras.Model(tweet, net)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:53:31.822417Z","iopub.execute_input":"2022-06-14T02:53:31.822856Z","iopub.status.idle":"2022-06-14T02:53:31.829835Z","shell.execute_reply.started":"2022-06-14T02:53:31.82282Z","shell.execute_reply":"2022-06-14T02:53:31.829084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test that the model works properly.","metadata":{}},{"cell_type":"code","source":"#Test if the model works on sample text\nbert_tweet_classifier = build_bert_classifier()\nno_training_result = bert_tweet_classifier(tf.constant([\"No one expected a fire of this scale\"]))\nprint(tf.sigmoid(no_training_result))","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:53:31.831087Z","iopub.execute_input":"2022-06-14T02:53:31.831642Z","iopub.status.idle":"2022-06-14T02:53:43.174873Z","shell.execute_reply.started":"2022-06-14T02:53:31.831602Z","shell.execute_reply":"2022-06-14T02:53:43.173844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the model to make sure that everything is in place.","metadata":{}},{"cell_type":"code","source":"tf.keras.utils.plot_model(bert_tweet_classifier,show_shapes=True,rankdir='LR')","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:53:43.176365Z","iopub.execute_input":"2022-06-14T02:53:43.176767Z","iopub.status.idle":"2022-06-14T02:53:44.169943Z","shell.execute_reply.started":"2022-06-14T02:53:43.176726Z","shell.execute_reply":"2022-06-14T02:53:44.169023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the loss function we use binary cross entropy, and set from_logits to True because we are not using sigmoid activation for the output layer.","metadata":{}},{"cell_type":"code","source":"loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nmetrics = tf.metrics.BinaryAccuracy()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:53:44.171901Z","iopub.execute_input":"2022-06-14T02:53:44.172351Z","iopub.status.idle":"2022-06-14T02:53:44.185506Z","shell.execute_reply.started":"2022-06-14T02:53:44.172308Z","shell.execute_reply":"2022-06-14T02:53:44.184657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training run settings\nepochs = 7\n#Get the cardinality of the dataset (Simply put, cardinality is the total number of batches in a dataset)\nsteps_per_epoch = tf.data.experimental.cardinality(disaster_ds).numpy()\nnum_train_steps = steps_per_epoch * epochs\n#Do a warm-up for number of steps = 10% of the total number of steps\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:53:44.187088Z","iopub.execute_input":"2022-06-14T02:53:44.187703Z","iopub.status.idle":"2022-06-14T02:53:44.195061Z","shell.execute_reply.started":"2022-06-14T02:53:44.187655Z","shell.execute_reply":"2022-06-14T02:53:44.194367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compile the model\nbert_tweet_classifier.compile(optimizer=optimizer,\n                         loss=loss,\n                         metrics=metrics)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:54:22.251544Z","iopub.execute_input":"2022-06-14T02:54:22.251901Z","iopub.status.idle":"2022-06-14T02:54:22.268404Z","shell.execute_reply.started":"2022-06-14T02:54:22.251873Z","shell.execute_reply":"2022-06-14T02:54:22.267629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the model summary\nbert_tweet_classifier.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:54:28.294174Z","iopub.execute_input":"2022-06-14T02:54:28.295057Z","iopub.status.idle":"2022-06-14T02:54:28.326769Z","shell.execute_reply.started":"2022-06-14T02:54:28.295018Z","shell.execute_reply":"2022-06-14T02:54:28.326037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the model\nprint(f'Training model with {encoder_url}')\nhistory = bert_tweet_classifier.fit(x=disaster_ds,\n                               epochs=epochs)","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:54:34.536704Z","iopub.execute_input":"2022-06-14T02:54:34.53708Z","iopub.status.idle":"2022-06-14T02:58:45.228515Z","shell.execute_reply.started":"2022-06-14T02:54:34.537051Z","shell.execute_reply":"2022-06-14T02:58:45.227614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting state of test data. We just the test dataset as input to our model \npreds = bert_tweet_classifier(test_data['text'])\n\npreds","metadata":{"execution":{"iopub.status.busy":"2022-06-14T02:58:45.230332Z","iopub.execute_input":"2022-06-14T02:58:45.230782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#It is always a good idea to plot a histogram to check if anything is off\nplt.rcParams[\"figure.figsize\"] = [15, 5.50]\nplt.rcParams[\"figure.autolayout\"] = True\nplt.hist(preds.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-06-13T04:09:29.479954Z","iopub.execute_input":"2022-06-13T04:09:29.480577Z","iopub.status.idle":"2022-06-13T04:09:29.735238Z","shell.execute_reply.started":"2022-06-13T04:09:29.480538Z","shell.execute_reply":"2022-06-13T04:09:29.734364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Get the sigmoid ranges from the data\nsigpreds= tf.sigmoid(preds)\nsigpreds","metadata":{"execution":{"iopub.status.busy":"2022-06-13T04:09:29.736652Z","iopub.execute_input":"2022-06-13T04:09:29.737173Z","iopub.status.idle":"2022-06-13T04:09:29.745387Z","shell.execute_reply.started":"2022-06-13T04:09:29.737132Z","shell.execute_reply":"2022-06-13T04:09:29.744513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot a histogram to check if anything is off\n\nplt.hist(sigpreds.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-06-13T04:09:29.746833Z","iopub.execute_input":"2022-06-13T04:09:29.747408Z","iopub.status.idle":"2022-06-13T04:09:29.990509Z","shell.execute_reply.started":"2022-06-13T04:09:29.747355Z","shell.execute_reply":"2022-06-13T04:09:29.989766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Recast to 0 and 1\nsigpreds = sigpreds*2\nsigpreds=np.floor(sigpreds).astype(int)\nsigpreds","metadata":{"execution":{"iopub.status.busy":"2022-06-13T04:09:29.991665Z","iopub.execute_input":"2022-06-13T04:09:29.992549Z","iopub.status.idle":"2022-06-13T04:09:30.000904Z","shell.execute_reply.started":"2022-06-13T04:09:29.992507Z","shell.execute_reply":"2022-06-13T04:09:29.999989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot a histogram to check if anything is off\nplt.hist(sigpreds)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T04:09:30.002453Z","iopub.execute_input":"2022-06-13T04:09:30.002956Z","iopub.status.idle":"2022-06-13T04:09:30.253769Z","shell.execute_reply.started":"2022-06-13T04:09:30.002918Z","shell.execute_reply":"2022-06-13T04:09:30.252872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sigpreds=sigpreds.reshape(-1) #Flatten array\nsigpreds","metadata":{"execution":{"iopub.status.busy":"2022-06-13T04:09:30.255105Z","iopub.execute_input":"2022-06-13T04:09:30.2561Z","iopub.status.idle":"2022-06-13T04:09:30.262063Z","shell.execute_reply.started":"2022-06-13T04:09:30.256064Z","shell.execute_reply":"2022-06-13T04:09:30.261321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot a histogram to check if anything is off\nplt.hist(sigpreds)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T04:09:30.26347Z","iopub.execute_input":"2022-06-13T04:09:30.264086Z","iopub.status.idle":"2022-06-13T04:09:30.52333Z","shell.execute_reply.started":"2022-06-13T04:09:30.264046Z","shell.execute_reply":"2022-06-13T04:09:30.522565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Save predictions\ndfsub = pd.DataFrame({'id' : test_data['id'].to_list(),'target': sigpreds})\ndfsub.to_csv('submission.csv', index=False)\ndfsub = pd.read_csv('submission.csv')\ndfsub\n","metadata":{"execution":{"iopub.status.busy":"2022-06-13T04:09:30.524468Z","iopub.execute_input":"2022-06-13T04:09:30.524994Z","iopub.status.idle":"2022-06-13T04:09:30.551752Z","shell.execute_reply.started":"2022-06-13T04:09:30.524953Z","shell.execute_reply":"2022-06-13T04:09:30.550885Z"},"trusted":true},"execution_count":null,"outputs":[]}]}