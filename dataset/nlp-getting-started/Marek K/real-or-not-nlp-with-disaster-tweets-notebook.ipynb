{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport nltk\nfrom sklearn import feature_extraction, linear_model, model_selection\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NLP with disaster tweets\n\n\nThis notebook is an attempt to create a submision to *Real or not? NLP with disaster tweets* competition.\n\nAs I have little experience with natural language processing, this will be more less trial and error thing. \n\n"},{"metadata":{},"cell_type":"markdown","source":"1. Loading the data\n2. Data overview\n3. Data cleanup\n4. Running the model\n"},{"metadata":{},"cell_type":"markdown","source":"# 1. Loading the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Training dataset overview"},{"metadata":{},"cell_type":"markdown","source":"So, as we now have both datasets loaded, we can take a closer look at the training data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have to handle the missing values in 'keyword' and 'location' columns. For both of them a good way to do this is to just put \"no data\" in the empty cells."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.fillna(value = 'No data')\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the dataset is looking well, we can take a closer look on it."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.target.value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_freq_location = train_data.location.value_counts()[:10].sort_values(ascending = False)\nmost_freq_location = most_freq_location.drop('No data')\nmost_freq_location","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we have countries and cities mixed. I don't want to clean this up yet as we might need the most exact location there is available, but it looks like the vast majority of tweets is from the US."},{"metadata":{},"cell_type":"markdown","source":"... to be expanded"},{"metadata":{},"cell_type":"markdown","source":"# 3. Processing tweet text for prediction\n"},{"metadata":{},"cell_type":"markdown","source":"Before working on tweet text, we're going to clean up the text: remove stop words, punctuation and urls."},{"metadata":{"trusted":true},"cell_type":"code","source":"#from nltk.corpus import stopwords\n#stop_words = stopwords.words('english')\n\n#It seems that removing stopwords makes the model less efficient. I'm going to comment this part out for now.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data['text'] = train_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))#\ntrain_data['text'] = train_data['text'].str.replace('[^\\w\\s]','')\ntrain_data['text'] = train_data['text'].str.replace(\"https?://[A-Za-z0-9./]*\", \"\")\nprint(train_data.text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to use a Logistic Regression model we have to vectorize our tweets first. We're going to do that using scikitlearn's CountVectorizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = feature_extraction.text.CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_vectors = count_vectorizer.fit_transform(train_data[\"text\"])\n\ntest_vectors = count_vectorizer.transform(test_data[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = train_data.target\ntrain_x = train_vectors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Model fitting and prediction "},{"metadata":{},"cell_type":"markdown","source":"Now, that we've established our training subsets, we can set up our model. Because our dependent variable is binary, I am going to try and evaluate logistic regression model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = model_selection.train_test_split(train_x, train_y)\nclf = linear_model.LogisticRegression(random_state=0, max_iter = 150).fit(x_train, y_train)\nclf.score(x_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, the validation score (mean accuracy on our test subset 'x_val' and labels 'y_val') is pretty high. Now, to the prediction:"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = clf.predict(test_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'id': test_data.id,\n                       'target': predictions})\noutput.to_csv('submission.csv', index = False)\nprint('submission saved!')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}