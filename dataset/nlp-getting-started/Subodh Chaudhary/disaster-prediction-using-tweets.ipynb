{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport string\nimport re\nimport nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom wordcloud import WordCloud,STOPWORDS\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-02-13T10:16:35.635817Z","iopub.execute_input":"2022-02-13T10:16:35.636195Z","iopub.status.idle":"2022-02-13T10:16:35.652118Z","shell.execute_reply.started":"2022-02-13T10:16:35.636147Z","shell.execute_reply":"2022-02-13T10:16:35.651331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_values = [\"na\",\"n/a\",\"-\",\"NaN\"] #dataset may contains null values in these forms\ndf_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv',na_values=missing_values)\ndf_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv',na_values=missing_values)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:16:35.653623Z","iopub.execute_input":"2022-02-13T10:16:35.65463Z","iopub.status.idle":"2022-02-13T10:16:35.698056Z","shell.execute_reply.started":"2022-02-13T10:16:35.654577Z","shell.execute_reply":"2022-02-13T10:16:35.696898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:16:35.699743Z","iopub.execute_input":"2022-02-13T10:16:35.700019Z","iopub.status.idle":"2022-02-13T10:16:35.712582Z","shell.execute_reply.started":"2022-02-13T10:16:35.699979Z","shell.execute_reply":"2022-02-13T10:16:35.711555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef text_process(tweet):\n    tweet = re.sub(r'http\\S+', '', tweet)\n    stopword = set(stopwords.words('english'))\n    lem = WordNetLemmatizer()\n    word_tokens = word_tokenize(tweet)\n    word_tokens_temp = []\n    for word in word_tokens:\n        word = ''.join(i for i in word if not i.isdigit())\n        word_tokens_temp.append(word)\n    \n    filtered_words = [lem.lemmatize(w) for w in word_tokens_temp if w not in stopword and w not in string.punctuation]\n    new_sentence = ' '.join(filtered_words)\n    return new_sentence","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:16:35.714952Z","iopub.execute_input":"2022-02-13T10:16:35.715411Z","iopub.status.idle":"2022-02-13T10:16:35.724395Z","shell.execute_reply.started":"2022-02-13T10:16:35.71538Z","shell.execute_reply":"2022-02-13T10:16:35.723478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['text']  = df_train['text'].apply(text_process)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:16:35.725884Z","iopub.execute_input":"2022-02-13T10:16:35.726094Z","iopub.status.idle":"2022-02-13T10:16:40.092328Z","shell.execute_reply.started":"2022-02-13T10:16:35.726069Z","shell.execute_reply":"2022-02-13T10:16:40.091247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word CLoud","metadata":{}},{"cell_type":"code","source":"stopwordSet = set(STOPWORDS)\ntweet_words = ''\nfor tweet in df_train['text']:\n    tweet = str(tweet)\n    tokens = tweet.split()\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n        tweet_words += ' '.join(tokens) + ' '\n        \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwordSet,\n                min_font_size = 10).generate(tweet_words)\n# plot the WordCloud image                      \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:16:40.093629Z","iopub.execute_input":"2022-02-13T10:16:40.093856Z","iopub.status.idle":"2022-02-13T10:16:45.28482Z","shell.execute_reply.started":"2022-02-13T10:16:40.093828Z","shell.execute_reply":"2022-02-13T10:16:45.284072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:16:45.286321Z","iopub.execute_input":"2022-02-13T10:16:45.286779Z","iopub.status.idle":"2022-02-13T10:16:45.300746Z","shell.execute_reply.started":"2022-02-13T10:16:45.286736Z","shell.execute_reply":"2022-02-13T10:16:45.299717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['text'] = df_test['text'].apply(text_process)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:16:45.302267Z","iopub.execute_input":"2022-02-13T10:16:45.302566Z","iopub.status.idle":"2022-02-13T10:16:47.181686Z","shell.execute_reply.started":"2022-02-13T10:16:45.302527Z","shell.execute_reply":"2022-02-13T10:16:47.180643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def multiclass_logloss(actual, predicted, eps=1e-15):\n\n#     # Convert 'actual' to a binary array if it's not already:\n#     if len(actual.shape) == 1:\n#         actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n#         for i, val in enumerate(actual):\n#             actual2[i, val] = 1\n#         actual = actual2\n\n#     clip = np.clip(predicted, eps, 1 - eps)\n#     rows = actual.shape[0]\n#     vsota = np.sum(actual * np.log(clip))\n#     return -1.0 / rows * vsota\n\n# def pred(prediction):\n#     predict = []\n#     for i in prediction[:,1]:\n#         if i >= 0.5:\n#             predict.append(1)\n#         else:\n#             predict.append(0)\n#     return predict","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:16:47.185378Z","iopub.execute_input":"2022-02-13T10:16:47.185745Z","iopub.status.idle":"2022-02-13T10:16:47.19075Z","shell.execute_reply.started":"2022-02-13T10:16:47.185701Z","shell.execute_reply":"2022-02-13T10:16:47.18971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df_train['target']\nx = df_train['text'] + ' ' + df_train['keyword']\nxtrain, xvalid, ytrain, yvalid = train_test_split(x.values.astype(str), y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:16:47.192463Z","iopub.execute_input":"2022-02-13T10:16:47.192778Z","iopub.status.idle":"2022-02-13T10:16:47.225848Z","shell.execute_reply.started":"2022-02-13T10:16:47.192737Z","shell.execute_reply":"2022-02-13T10:16:47.225098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (xtrain.shape)\nprint (xvalid.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:16:47.22752Z","iopub.execute_input":"2022-02-13T10:16:47.228025Z","iopub.status.idle":"2022-02-13T10:16:47.23447Z","shell.execute_reply.started":"2022-02-13T10:16:47.227982Z","shell.execute_reply":"2022-02-13T10:16:47.233308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Basic Models\nLet's start building our very first model.\n\nOur very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression.","metadata":{}},{"cell_type":"code","source":"tfv = TfidfVectorizer(max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)\nxtest_tfv = tfv.transform(df_test['text'])","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:23:04.553637Z","iopub.execute_input":"2022-02-13T10:23:04.553909Z","iopub.status.idle":"2022-02-13T10:23:04.975742Z","shell.execute_reply.started":"2022-02-13T10:23:04.553882Z","shell.execute_reply":"2022-02-13T10:23:04.974872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=1.0).fit(xtrain_tfv,ytrain)\n#clf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict(xvalid_tfv)\n\n# print (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))\nprint(\"Logistic Regression Score: \",clf.score(xvalid_tfv,yvalid))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:23:10.587601Z","iopub.execute_input":"2022-02-13T10:23:10.588013Z","iopub.status.idle":"2022-02-13T10:23:10.97782Z","shell.execute_reply.started":"2022-02-13T10:23:10.587978Z","shell.execute_reply":"2022-02-13T10:23:10.976907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\noutput = pd.DataFrame({'id': df_test.id, 'target': clf.predict(xtest_tfv)})\noutput.to_csv('submissionLogReg.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:23:13.086536Z","iopub.execute_input":"2022-02-13T10:23:13.086816Z","iopub.status.idle":"2022-02-13T10:23:13.10075Z","shell.execute_reply.started":"2022-02-13T10:23:13.086788Z","shell.execute_reply":"2022-02-13T10:23:13.100037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of using TF-IDF, we can also use word counts as features. This can be done easily using CountVectorizer from scikit-learn.","metadata":{}},{"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 2), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)\nxtest_ctv = ctv.transform(df_test.text)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:23:33.62211Z","iopub.execute_input":"2022-02-13T10:23:33.622397Z","iopub.status.idle":"2022-02-13T10:23:34.375404Z","shell.execute_reply.started":"2022-02-13T10:23:33.622366Z","shell.execute_reply":"2022-02-13T10:23:34.374335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\n\nprint(\"Logistic Regression count Score: \",clf.score(xvalid_ctv,yvalid))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:23:34.377377Z","iopub.execute_input":"2022-02-13T10:23:34.377621Z","iopub.status.idle":"2022-02-13T10:23:35.828807Z","shell.execute_reply.started":"2022-02-13T10:23:34.377592Z","shell.execute_reply":"2022-02-13T10:23:35.827879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\noutput = pd.DataFrame({'id':df_test.id,'target':clf.predict(xtest_ctv)})\noutput.to_csv('submissionLogRegCtv.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:23:35.834408Z","iopub.execute_input":"2022-02-13T10:23:35.834935Z","iopub.status.idle":"2022-02-13T10:23:35.867469Z","shell.execute_reply.started":"2022-02-13T10:23:35.834885Z","shell.execute_reply":"2022-02-13T10:23:35.86629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(xtrain_ctv, ytrain)\nprint(\"Logistic Regression Score: \",clf.score(xvalid_ctv,yvalid))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:23:35.873407Z","iopub.execute_input":"2022-02-13T10:23:35.877768Z","iopub.status.idle":"2022-02-13T10:23:35.90111Z","shell.execute_reply.started":"2022-02-13T10:23:35.877704Z","shell.execute_reply":"2022-02-13T10:23:35.900201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'id':df_test.id,'target':clf.predict(xtest_ctv)})\noutput.to_csv('submissionNaiveBayes.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:23:36.060523Z","iopub.execute_input":"2022-02-13T10:23:36.06083Z","iopub.status.idle":"2022-02-13T10:23:36.077055Z","shell.execute_reply.started":"2022-02-13T10:23:36.0608Z","shell.execute_reply":"2022-02-13T10:23:36.075855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\n\nsvd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_ctv)\nxtrain_svd = svd.transform(xtrain_ctv)\nxvalid_svd = svd.transform(xvalid_ctv)\nxtest_svd = svd.transform(xtest_ctv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)\nxtest_svd_scl = scl.transform(xtest_svd)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:23:36.413664Z","iopub.execute_input":"2022-02-13T10:23:36.413989Z","iopub.status.idle":"2022-02-13T10:23:41.093343Z","shell.execute_reply.started":"2022-02-13T10:23:36.413957Z","shell.execute_reply":"2022-02-13T10:23:41.092488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting a simple SVM\n\nclf = SVC(C=1.0, probability=True) # since we need probabilities\nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict_proba(xvalid_svd_scl)\n\nprint(\"SVM score: \",clf.score(xvalid_svd_scl,yvalid))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:23:41.095407Z","iopub.execute_input":"2022-02-13T10:23:41.095686Z","iopub.status.idle":"2022-02-13T10:24:15.01452Z","shell.execute_reply.started":"2022-02-13T10:23:41.095648Z","shell.execute_reply":"2022-02-13T10:24:15.013409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'id':df_test.id,'target':clf.predict(xtest_svd_scl)})\noutput.to_csv('submissionSVM.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:24:15.015983Z","iopub.execute_input":"2022-02-13T10:24:15.016254Z","iopub.status.idle":"2022-02-13T10:24:17.431669Z","shell.execute_reply.started":"2022-02-13T10:24:15.016209Z","shell.execute_reply":"2022-02-13T10:24:17.431078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nRF_clf = RandomForestClassifier().fit(xtrain_ctv,ytrain)\n\nprint(\"RandomForest score: \",RF_clf.score(xvalid_ctv,yvalid))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:24:17.433383Z","iopub.execute_input":"2022-02-13T10:24:17.433791Z","iopub.status.idle":"2022-02-13T10:25:16.713582Z","shell.execute_reply.started":"2022-02-13T10:24:17.433762Z","shell.execute_reply":"2022-02-13T10:25:16.712962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'id':df_test.id,'target':RF_clf.predict(xtest_ctv)})\noutput.to_csv('submissionRandForest.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:25:16.714683Z","iopub.execute_input":"2022-02-13T10:25:16.715013Z","iopub.status.idle":"2022-02-13T10:25:17.354852Z","shell.execute_reply.started":"2022-02-13T10:25:16.714986Z","shell.execute_reply":"2022-02-13T10:25:17.353757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}