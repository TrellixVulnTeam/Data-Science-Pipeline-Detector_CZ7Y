{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Import modules to attach and read the dataset**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Read the dataset using pandas**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"target\"].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%', figsize=(6,6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x='target',data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df.keyword!='NaN'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=train_df.drop([\"location\",\"keyword\",\"id\"], axis=1)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Read the test dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=test_df.drop([\"location\",\"keyword\"], axis=1)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Import the required NLTK modules**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import tokenize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Lower case the text data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.text=train_df.text.apply(lambda x: x.lower())\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Expanding the contracted and abbreviated text data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install contractions\nimport contractions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def con(data):\n  expand=contractions.fix(data)\n  return expand\n\ntrain_df.text=train_df.text.apply(con)\ntrain_df['text'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Removing the punctuations and special characters**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef remove_sp(data):\n  pattern=r'[^A-Za-z0-9\\s]'\n  data=re.sub(pattern,'',data)\n  return data\n\ntrain_df.text=train_df.text.apply(remove_sp)\ntrain_df.text[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import string\n#punctuations=list(string.punctuation)\n#train_df.text=train_df.text.apply(lambda x : \" \".join(x for x in x.split() if x not in punctuations))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Removing Stopwords**"},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nstopword_list=stopwords.words('english')\nstopword_list.remove('no')\nstopword_list.remove('not')\n\ntrain_df.text=train_df.text.apply(lambda x : \" \".join(x for x in x.split() if x not in stopword_list))\ntrain_df['text'][5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Tokenization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('punkt')\ntrain_df['text']=train_df.text.apply(word_tokenize)\ntrain_df['text'][0]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Lemmatization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('wordnet')\nlemmatizer=WordNetLemmatizer()\ntrain_df['text']=train_df.text.apply(lambda x:[lemmatizer.lemmatize(word) for word in x])\ntrain_df.text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.text= train_df.text.astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ** Creating the features and the target variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train_df.text\nY=train_df.target\nX_test=test_df.text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#from sklearn.feature_extraction.text import TfidfVectorizer\n#from sklearn.pipeline import Pipeline\n#from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Applying TFIDF (Term Frequency Inverse Document Frequency) Vectorizer to convert categorical features into numbers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer()\nx_train_tfidf = tfidf.fit_transform(X)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Applying Support Vector Machine Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\nfrom sklearn.svm import SVC\nsvc_clf=SVC()\nsvc_clf.fit(x_train_tfidf,Y)\nsvc_clf.score(x_train_tfidf,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"np.random.seed(42)\nfrom sklearn.ensemble import RandomForestClassifier\nrf_clf=RandomForestClassifier()\nrf_clf.fit(x_train_tfidf,Y)\nrf_clf.score(x_train_tfidf,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from mlxtend.classifier import StackingCVClassifier\nscv=StackingCVClassifier(classifiers=[svc_clf,rf_clf],meta_classifier= rf_clf)\nscv.fit(x_train_tfidf,Y)\nscv.score(x_train_tfidf,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\nfrom sklearn import linear_model\nrd_clf = linear_model.RidgeClassifier()\nrd_clf.fit(x_train_tfidf,Y)\nrd_clf.score(x_train_tfidf,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_test=X_test.apply(lambda x: x.lower())\nX_test=X_test.apply(con)\nX_test=X_test.apply(remove_sp)\n#test_df.text=test_df.text.apply(lambda x : \" \".join(x for x in x.split() if x not in punctuations))\nX_test=X_test.apply(lambda x : \" \".join(x for x in x.split() if x not in stopword_list))\nX_test=X_test.apply(word_tokenize)\nX_test=X_test.apply(lambda x:[lemmatizer.lemmatize(word) for word in x])\nX_test= X_test.astype(str)\nx_test_tfidf = tfidf.transform(X_test)\nX_test\nx_test_tfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=rd_clf.predict(x_test_tfidf)\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'Id': test_df.id, 'Target': predictions})\noutput.to_csv('my_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}