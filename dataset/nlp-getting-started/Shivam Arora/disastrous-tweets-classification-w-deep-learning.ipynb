{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hi!\n\nThis notebook will implement a RNN model to classify a given tweet as disastrous or not with some EDA.\n\n## Problem Statement\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:\n\n##### Sentence 1: Forest fire near la ronge sask.\n##### Sentence 2: This video was fire!\n\nYou'll notice that both these sentences have fire in it but only one of them is about an actual disaster. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine (or wrong?). \n\nIn this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which ones aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified.","metadata":{}},{"cell_type":"markdown","source":"Importing Dependencies","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\n\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom keras.initializers import Constant\nimport keras.metrics as metrics\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n\nimport warnings\nimport string\nimport re\nwarnings.filterwarnings('ignore')\nsns.set()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:27.728366Z","iopub.execute_input":"2021-05-20T19:52:27.728713Z","iopub.status.idle":"2021-05-20T19:52:27.738353Z","shell.execute_reply.started":"2021-05-20T19:52:27.728685Z","shell.execute_reply":"2021-05-20T19:52:27.737222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:27.783382Z","iopub.execute_input":"2021-05-20T19:52:27.783721Z","iopub.status.idle":"2021-05-20T19:52:27.85604Z","shell.execute_reply.started":"2021-05-20T19:52:27.783691Z","shell.execute_reply":"2021-05-20T19:52:27.854986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:27.858033Z","iopub.execute_input":"2021-05-20T19:52:27.858354Z","iopub.status.idle":"2021-05-20T19:52:27.888084Z","shell.execute_reply.started":"2021-05-20T19:52:27.858325Z","shell.execute_reply":"2021-05-20T19:52:27.886995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding the data","metadata":{}},{"cell_type":"code","source":"train.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:27.889489Z","iopub.execute_input":"2021-05-20T19:52:27.889885Z","iopub.status.idle":"2021-05-20T19:52:27.895563Z","shell.execute_reply.started":"2021-05-20T19:52:27.88985Z","shell.execute_reply":"2021-05-20T19:52:27.894609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training dataset has 7613 rows and 5 columns, and testing dataset has 3263 rows and 4 columns. The missing column is of course the target variable which we want to predict. ","metadata":{}},{"cell_type":"code","source":"train.isnull().sum().sum(), test.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:27.896945Z","iopub.execute_input":"2021-05-20T19:52:27.897553Z","iopub.status.idle":"2021-05-20T19:52:27.916259Z","shell.execute_reply.started":"2021-05-20T19:52:27.897509Z","shell.execute_reply":"2021-05-20T19:52:27.91515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training dataset has 2594 null values, and test dataset has 1131 null values. Many algorithms cannot deal with missing values, hence we would need to either drop these values or impute them. ","metadata":{}},{"cell_type":"code","source":"train.id.isin(test.id).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:27.920486Z","iopub.execute_input":"2021-05-20T19:52:27.920917Z","iopub.status.idle":"2021-05-20T19:52:27.932711Z","shell.execute_reply.started":"2021-05-20T19:52:27.920873Z","shell.execute_reply":"2021-05-20T19:52:27.931274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no id's from training dataset in the testing dataset.","metadata":{}},{"cell_type":"code","source":"train.text.isin(test.text).value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:27.93647Z","iopub.execute_input":"2021-05-20T19:52:27.93676Z","iopub.status.idle":"2021-05-20T19:52:27.947156Z","shell.execute_reply.started":"2021-05-20T19:52:27.936733Z","shell.execute_reply":"2021-05-20T19:52:27.946427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 127 texts from training dataset in the testing dataset.","metadata":{}},{"cell_type":"code","source":"disaster_tweets = pd.concat([train, test], axis=0)\ndisaster_tweets.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:27.94849Z","iopub.execute_input":"2021-05-20T19:52:27.949051Z","iopub.status.idle":"2021-05-20T19:52:27.971851Z","shell.execute_reply.started":"2021-05-20T19:52:27.949007Z","shell.execute_reply":"2021-05-20T19:52:27.971155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.countplot(disaster_tweets.target, palette='Blues')\nplt.title(\"Distribution of Target Counts\", size=25, weight='bold')\nplt.xlabel(\"Target Label\", size=14)\nplt.ylabel(\"Frequency\", size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:27.972847Z","iopub.execute_input":"2021-05-20T19:52:27.973231Z","iopub.status.idle":"2021-05-20T19:52:28.186811Z","shell.execute_reply.started":"2021-05-20T19:52:27.973202Z","shell.execute_reply":"2021-05-20T19:52:28.185865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As displayed, there is an imbalance towards the negative class (not a disaster) in our dataset. This is important as it can have significant effect on the classifier. Hence, accuracy is a misleading metric here for these types of problems as the classifier will be more inclined towards the dominant class for evaluating our models. In this case, the imbalance is not severe hence we will not perform any oversampling and undersampling techniques to tweak this but it still worthy of note-taking. ","metadata":{}},{"cell_type":"code","source":"missing_values = pd.DataFrame(dict(\n    round(100 * disaster_tweets.drop('target', axis=1).isnull().sum() /\n          len(disaster_tweets), 2)), index=[0])\nmissing_values = missing_values.melt(var_name = 'Features', value_name = 'Percentage')\n\nplt.figure(figsize=(20,10))\nsns.barplot('Features', 'Percentage', data=missing_values, palette='Blues')\nplt.title(\"Percentage of Missing Values\", size=25, weight='bold')\nplt.xlabel(\"Features\", size=14)\nplt.ylabel(\"Percentage\", size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:28.188332Z","iopub.execute_input":"2021-05-20T19:52:28.188761Z","iopub.status.idle":"2021-05-20T19:52:28.575001Z","shell.execute_reply.started":"2021-05-20T19:52:28.188728Z","shell.execute_reply":"2021-05-20T19:52:28.574114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are about 0.8% of missing values in feature 'keyword' and about 33.45% in feature 'location'. Since these are locations that users can opt out of disclosing when tweeting.","metadata":{}},{"cell_type":"code","source":"unique_values = pd.DataFrame(dict(disaster_tweets.drop('target', axis=1).nunique()),\n             index=[0]).melt(var_name = 'Features', value_name = 'Frequency')\n\nplt.figure(figsize=(20,10))\nsns.barplot('Features', 'Frequency', data=unique_values, palette='Blues')\nplt.title(\"Count of Unique Values\", size=25, weight='bold')\nplt.xticks(size=18)\nplt.xlabel(\"Features\", size=18)\nplt.ylabel(\"Frequency\", size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:28.576369Z","iopub.execute_input":"2021-05-20T19:52:28.576649Z","iopub.status.idle":"2021-05-20T19:52:28.787497Z","shell.execute_reply.started":"2021-05-20T19:52:28.576622Z","shell.execute_reply":"2021-05-20T19:52:28.786374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of Duplicated Tweets in Train and Test: {}\".format(\n    len(disaster_tweets[disaster_tweets.text.duplicated()])))\nprint(\"Number of Duplicated Tweets in Train: {}\".format(\n    (disaster_tweets[disaster_tweets.text.duplicated()].target.notna().sum())))\nprint(\"Number of Duplicated Tweets in Test: {}\".format(\n    (disaster_tweets[disaster_tweets.text.duplicated()].target.isna().sum())))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:28.789008Z","iopub.execute_input":"2021-05-20T19:52:28.789313Z","iopub.status.idle":"2021-05-20T19:52:28.803132Z","shell.execute_reply.started":"2021-05-20T19:52:28.789281Z","shell.execute_reply":"2021-05-20T19:52:28.802177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is important to consider because if there are duplicate tweets in both the datasets, since this dataset was hand engineered there could be possibilities of human error in labelling the target variables differently (for the same tweets of course). ","metadata":{}},{"cell_type":"code","source":"duplicate_values = train[train.text.duplicated()][['text','target']]\nduplicate_values[duplicate_values.text=='To fight bioterrorism sir.']","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:28.80471Z","iopub.execute_input":"2021-05-20T19:52:28.805094Z","iopub.status.idle":"2021-05-20T19:52:28.818734Z","shell.execute_reply.started":"2021-05-20T19:52:28.805011Z","shell.execute_reply":"2021-05-20T19:52:28.817529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I took one example from the training dataset and found out that there are multiple labels for the same tweet. Hence this will confuse our model and result in inconsistant results.","metadata":{}},{"cell_type":"code","source":"different_target_tweets_indexes = []\nfor index, target in enumerate(duplicate_values.groupby(['text']).agg(list).target):\n    if len(list(set(target)))>1:\n        different_target_tweets_indexes.append(index)\n        \nduplicate_values.groupby('text').agg(list).iloc[different_target_tweets_indexes]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:28.820094Z","iopub.execute_input":"2021-05-20T19:52:28.820436Z","iopub.status.idle":"2021-05-20T19:52:28.858571Z","shell.execute_reply.started":"2021-05-20T19:52:28.820401Z","shell.execute_reply":"2021-05-20T19:52:28.857496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hence it is possible for the test dataset that Kaggle will use to evaluate might also have incorrect labels like this in the training dataset. However, I cannot be totally sure.","metadata":{}},{"cell_type":"code","source":"location_values = pd.DataFrame(dict(disaster_tweets.location.value_counts()), \n             index=[0]).melt(var_name=\"Country\", value_name=\"#Unique\")\nlocation_values[:10]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:28.859815Z","iopub.execute_input":"2021-05-20T19:52:28.860122Z","iopub.status.idle":"2021-05-20T19:52:29.195254Z","shell.execute_reply.started":"2021-05-20T19:52:28.86009Z","shell.execute_reply":"2021-05-20T19:52:29.194375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be understood that the feature 'location' has different names for the same location. For eg, USA and United States. Another thing to notice here is that location can contain names of cities as well. For eg, Los Angeles, CA and London. This is important to note and clean but rather we can just drop this column as it has about 38% null values as well. Although, I would replace some values just for the sake of exploring. ","metadata":{}},{"cell_type":"code","source":"replace_locations= {\n    \"United States\": \"USA\",\n    \"London\": \"UK\",\n    \"New York\": \"USA\",\n    \"Los Angeles, CA\": \"USA\",\n    \"Washington, DC\": \"USA\",\n    \"Mumbai\": \"India\"\n}\n\ncountries = disaster_tweets[\"location\"].replace(replace_locations).value_counts()\nlocation_values = pd.DataFrame(dict(zip(countries.index, countries.values)), \n                               index=[0]).melt(var_name=\"Country\", value_name=\"#Unique\")\n\ndata = {\n    \"locations\": location_values.loc[:10, \"Country\"],\n    \"locationmode\": \"country names\",\n    \"z\": location_values.loc[:10, \"#Unique\"],\n    \"colorscale\": \"blugrn\",\n    \"text\": location_values.loc[:10, \"Country\"],\n    \"type\": \"choropleth\",\n    \"colorbar\": {\"title\": \"#Unique\", \"len\": 200, \"lenmode\":\"pixels\"}\n}\n\nlayout = go.Layout(title_text=\"<b>Top 10 Countries based on # Tweets Location</b>\",\n                   geo=dict(scope=\"world\"))\n\nfig = go.Figure(data=[data], layout=layout)\nfig.update_layout(title_x=0.5)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:29.196951Z","iopub.execute_input":"2021-05-20T19:52:29.197268Z","iopub.status.idle":"2021-05-20T19:52:29.706026Z","shell.execute_reply.started":"2021-05-20T19:52:29.197238Z","shell.execute_reply":"2021-05-20T19:52:29.705238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = disaster_tweets.replace(replace_locations)\ndf = df[df.location.isin(location_values.Country.head(9))]\ndf = df.sort_values(by='location')\n\nplt.figure(figsize=(20,10))\nseaborn_plot = sns.countplot(df.location, hue=df.target, palette='Blues')\nfor p in seaborn_plot.patches:\n    seaborn_plot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center',\n                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\nplt.title(\"# Tweets for Top10 Countries\", size=25, weight='bold')\nplt.xticks(size=18)\nplt.xlabel(\"Countries\", size=25, weight='bold')\nplt.ylabel(\"Frequency\", size=14)\nplt.legend((\"Not Disastrous\", \"Disastrous\"))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:29.707185Z","iopub.execute_input":"2021-05-20T19:52:29.707648Z","iopub.status.idle":"2021-05-20T19:52:30.121342Z","shell.execute_reply.started":"2021-05-20T19:52:29.707602Z","shell.execute_reply":"2021-05-20T19:52:30.120408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since I only replaced only a few locations these counts may appear wrong but you get the gist I hope. ","metadata":{}},{"cell_type":"code","source":"all_keywords = \" \".join(keyword for keyword in disaster_tweets[disaster_tweets.target==1].keyword.dropna())\nword_cloud= WordCloud(width=1250, height=625, max_font_size=350, \n                      random_state=42).generate(all_keywords)\nplt.figure(figsize=(20, 10))\nplt.title(\"Words used for Disastrous Tweets\", size=20, weight=\"bold\")\nplt.imshow(word_cloud)\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:30.123082Z","iopub.execute_input":"2021-05-20T19:52:30.123415Z","iopub.status.idle":"2021-05-20T19:52:32.108403Z","shell.execute_reply.started":"2021-05-20T19:52:30.123384Z","shell.execute_reply":"2021-05-20T19:52:32.107594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_keywords = \" \".join(keyword for keyword in disaster_tweets[disaster_tweets.target==0].keyword.dropna())\nword_cloud= WordCloud(width=1250, height=625, max_font_size=350, \n                      random_state=42).generate(all_keywords)\nplt.figure(figsize=(20, 10))\nplt.title(\"Words used for Non-Disastrous Tweets\", size=20, weight=\"bold\")\nplt.imshow(word_cloud)\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:32.109537Z","iopub.execute_input":"2021-05-20T19:52:32.109954Z","iopub.status.idle":"2021-05-20T19:52:34.240329Z","shell.execute_reply.started":"2021-05-20T19:52:32.109911Z","shell.execute_reply":"2021-05-20T19:52:34.239506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be derived that tweets which are disastrous have words like 'outbreak', 'decrailment' and 'wreckage' more frequent and for tweets which are non-disastrous have words like 'body', 'armageddon' and 'bags'.","metadata":{}},{"cell_type":"code","source":"stats_values = disaster_tweets.copy()\nSTOPWORDS = set(stopwords.words(\"english\"))\n\nstats_values['char_len'] = stats_values.text.apply(lambda x: len(x))\nstats_values['word_len'] = stats_values.text.apply(lambda x: len(x.split()))\nstats_values['count_stopwords'] = stats_values.text.apply(\n    lambda x: len(([w for w in str(x).lower().split() if w in STOPWORDS])))\nstats_values['count_punctuation'] = stats_values.text.apply(\n    lambda x: len([c for c in str(x) if c in string.punctuation]))\n\nstats_values.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:34.241472Z","iopub.execute_input":"2021-05-20T19:52:34.241871Z","iopub.status.idle":"2021-05-20T19:52:34.428929Z","shell.execute_reply.started":"2021-05-20T19:52:34.241833Z","shell.execute_reply":"2021-05-20T19:52:34.42789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\nax = sns.distplot(stats_values[stats_values.target==0].char_len, \n                  ax = axes[0], bins=40, color='green')\nax.set_title(\"Non-Disastrous Tweets\", size=25)\nax.set_xlabel('Character Length', size=18)\nax = sns.distplot(stats_values[stats_values.target==1].char_len, \n                  ax = axes[1], bins=40, color='red')\nax.set_title(\"Disastrous Tweets\", size=25)\nax.set_xlabel('Character Length', size=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:34.430121Z","iopub.execute_input":"2021-05-20T19:52:34.430412Z","iopub.status.idle":"2021-05-20T19:52:35.105565Z","shell.execute_reply.started":"2021-05-20T19:52:34.430377Z","shell.execute_reply":"2021-05-20T19:52:35.104502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\nax = sns.distplot(stats_values[stats_values.target==0].word_len, \n                  ax=axes[0], color='green')\nax.set_title(\"Non-Disastrous Tweets\", size=25)\nax.set_xlabel('Word Length', size=18)\nax = sns.distplot(stats_values[stats_values.target==1].word_len, \n                  ax=axes[1], color='red')\nax.set_title(\"Disastrous Tweets\", size=25)\nax.set_xlabel('Word Length', size=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:35.10672Z","iopub.execute_input":"2021-05-20T19:52:35.107049Z","iopub.status.idle":"2021-05-20T19:52:35.681791Z","shell.execute_reply.started":"2021-05-20T19:52:35.107014Z","shell.execute_reply":"2021-05-20T19:52:35.680754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\nax = sns.distplot(stats_values[stats_values.target==0].count_stopwords, \n                  ax=axes[0], color='green', bins=15)\nax.set_title(\"Non-Disastrous Tweets\", size=25)\nax.set_xlabel('Count of StopWords', size=18)\nax = sns.distplot(stats_values[stats_values.target==1].count_stopwords, \n                  ax=axes[1], color='red', bins=15)\nax.set_title(\"Disastrous Tweets\", size=25)\nax.set_xlabel('Count of StopWords', size=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:35.683003Z","iopub.execute_input":"2021-05-20T19:52:35.683284Z","iopub.status.idle":"2021-05-20T19:52:36.199753Z","shell.execute_reply.started":"2021-05-20T19:52:35.683256Z","shell.execute_reply":"2021-05-20T19:52:36.198817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,10))\nax = sns.distplot(stats_values[stats_values.target==0].count_punctuation, \n                  ax=axes[0], color='green')\nax.set_title(\"Non-Disastrous Tweets\", size=25)\nax.set_xlabel('Count of Punctuation', size=18)\nax = sns.distplot(stats_values[stats_values.target==1].count_punctuation, \n                  ax=axes[1], color='red')\nax.set_title(\"Disastrous Tweets\", size=25)\nax.set_xlabel('Count of Punctuation', size=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:36.201127Z","iopub.execute_input":"2021-05-20T19:52:36.201412Z","iopub.status.idle":"2021-05-20T19:52:36.860333Z","shell.execute_reply.started":"2021-05-20T19:52:36.201385Z","shell.execute_reply":"2021-05-20T19:52:36.859489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Realistically, I think this dataset is small to infer real-world behaviour although here is a summary of all my findings with this dataset:\n\n1. Tweets which are disastrous have words like 'outbreak', 'decrailment' and 'wreckage' more frequent and for tweets which are non-disastrous have words like 'body', 'armageddon' and 'bags'.\n\n2. People generally tend to type more characters when tweeting about a disaster.\n\n3. People generally type more words when tweeting about a disaster.\n\n4. People generally use more StopWords when tweeting about a disaster.\n\n5. People generally use more punctuation when tweeting about a disaster.\n\nPretty strange right?!","metadata":{}},{"cell_type":"markdown","source":"<center> <img src=\"https://tenor.com/view/president-donald-trump-tweets-mad-gif-8149740.gif\"> </center>","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"train = disaster_tweets.iloc[:len(train), :]\ntest = disaster_tweets.iloc[len(train):, :].drop('target', axis=1)\n\ntrain.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:36.861623Z","iopub.execute_input":"2021-05-20T19:52:36.861912Z","iopub.status.idle":"2021-05-20T19:52:36.870231Z","shell.execute_reply.started":"2021-05-20T19:52:36.861882Z","shell.execute_reply":"2021-05-20T19:52:36.869456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(tweet):\n    tweet = tweet.lower() #text to lowercase\n    tweet = re.sub(r'\\$\\w*', '', str(tweet)) #remove stock market symbols\n    tweet = re.sub(r'^RT[\\s]+', '', str(tweet)) #remove RT or Retweet symbols\n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', str(tweet)) #remove links\n    tweet = re.sub(r'#', '', str(tweet)) #remove # or Hashtag symbols\n    return tweet\n\ndef clean_punctuation(tweet):\n    tweet = \"\".join(word for word in tweet if word not in set(string.punctuation))\n    return tweet\n\ndef clean_stopwords(tweet):\n    STOPWORDS = set(stopwords.words(\"english\"))\n    tweet = \" \".join(word for word in tweet.split() if word not in STOPWORDS)\n    return tweet\n\ndef clean_tweet(tweet):\n    tweet = clean_text(tweet)\n    tweet = clean_punctuation(tweet)\n    tweet = clean_stopwords(tweet)\n    return tweet\n\ndef manual_label_target(df):\n    df.loc[df[\"text\"]==\n           \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\",\n           \"target\"]= 0\n    df.loc[df[\"text\"]==\n          \"#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption\",\n          \"target\"] = 0\n    df.loc[df[\"text\"]==\n          \".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4\",\n          \"target\"] = 0\n    df.loc[df[\"text\"]==\n          \"CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring\",\n          \"target\"] = 1\n    df.loc[df[\"text\"]==\n          \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\",\n          \"target\"] = 0\n    df.loc[df[\"text\"]==\n          \"Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife\",\n          \"target\"] = 0\n    df.loc[df[\"text\"]==\n          \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\",\n          \"target\"] = 0\n    df.loc[df[\"text\"]==\"To fight bioterrorism sir.\", \"target\"] = 0\n    df.loc[df[\"text\"]==\n          \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\",\n          \"target\"] = 0\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:36.871515Z","iopub.execute_input":"2021-05-20T19:52:36.871794Z","iopub.status.idle":"2021-05-20T19:52:36.884953Z","shell.execute_reply.started":"2021-05-20T19:52:36.871766Z","shell.execute_reply":"2021-05-20T19:52:36.883915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = manual_label_target(train)\n\ntrain['clean_tweet'] = train.text.apply(clean_tweet)\ntest['clean_tweet'] = test.text.apply(clean_tweet)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:36.887221Z","iopub.execute_input":"2021-05-20T19:52:36.887646Z","iopub.status.idle":"2021-05-20T19:52:39.510651Z","shell.execute_reply.started":"2021-05-20T19:52:36.887602Z","shell.execute_reply":"2021-05-20T19:52:39.509629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:39.512226Z","iopub.execute_input":"2021-05-20T19:52:39.512655Z","iopub.status.idle":"2021-05-20T19:52:39.528892Z","shell.execute_reply.started":"2021-05-20T19:52:39.512613Z","shell.execute_reply":"2021-05-20T19:52:39.527692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train.text == 'To fight bioterrorism sir.']","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:39.53054Z","iopub.execute_input":"2021-05-20T19:52:39.530923Z","iopub.status.idle":"2021-05-20T19:52:39.547704Z","shell.execute_reply.started":"2021-05-20T19:52:39.530881Z","shell.execute_reply":"2021-05-20T19:52:39.546531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(['id', 'keyword', 'location', 'text'], axis=1, inplace=True)\ntest.drop(['id', 'keyword', 'location', 'text'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:39.548942Z","iopub.execute_input":"2021-05-20T19:52:39.549333Z","iopub.status.idle":"2021-05-20T19:52:39.557822Z","shell.execute_reply.started":"2021-05-20T19:52:39.5493Z","shell.execute_reply":"2021-05-20T19:52:39.556521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I cleaned the dataset by converting to lowercase, removing stock-market symbols, RTs, links and hashtags. I removed punctuation marks and stopwords and finally manually labelled duplicated tweets as found earlier. ","metadata":{}},{"cell_type":"code","source":"train.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:39.55923Z","iopub.execute_input":"2021-05-20T19:52:39.559566Z","iopub.status.idle":"2021-05-20T19:52:39.573089Z","shell.execute_reply.started":"2021-05-20T19:52:39.559536Z","shell.execute_reply":"2021-05-20T19:52:39.572069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:39.574506Z","iopub.execute_input":"2021-05-20T19:52:39.574785Z","iopub.status.idle":"2021-05-20T19:52:39.59082Z","shell.execute_reply.started":"2021-05-20T19:52:39.574758Z","shell.execute_reply":"2021-05-20T19:52:39.589839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\n\nI will use GLoVe Embeddings as they represent words using the context and combine matrix factorization algorithms and window based algorithms. If you are not familier with GLoVe Embeddings, I have made another notebook which is beginner friendly which explains why GLoVe Embeddings are useful and how to implement them using Deep Learning models such as RNNs. \n\nLink: https://www.kaggle.com/shivam017arora/imdb-sentiment-analysis","metadata":{}},{"cell_type":"code","source":"corpus = []\nfor text in train['clean_tweet']:\n    words = [word.lower() for word in word_tokenize(text)] \n    corpus.append(words)\nnum_words = len(corpus)\nprint(num_words)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:39.596038Z","iopub.execute_input":"2021-05-20T19:52:39.596347Z","iopub.status.idle":"2021-05-20T19:52:40.890918Z","shell.execute_reply.started":"2021-05-20T19:52:39.596311Z","shell.execute_reply":"2021-05-20T19:52:40.889925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train['clean_tweet']\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:40.89214Z","iopub.execute_input":"2021-05-20T19:52:40.892402Z","iopub.status.idle":"2021-05-20T19:52:40.901479Z","shell.execute_reply.started":"2021-05-20T19:52:40.892376Z","shell.execute_reply":"2021-05-20T19:52:40.900294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 32\ntokenizer = Tokenizer(num_words)\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train, maxlen=max_len, truncating='post', padding='post')\nX_test = tokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, maxlen=max_len, truncating='post', padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:40.902924Z","iopub.execute_input":"2021-05-20T19:52:40.903387Z","iopub.status.idle":"2021-05-20T19:52:41.181415Z","shell.execute_reply.started":"2021-05-20T19:52:40.90334Z","shell.execute_reply":"2021-05-20T19:52:41.180277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index\nprint(\"Number of unique words: {}\".format(len(word_index)))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:41.18281Z","iopub.execute_input":"2021-05-20T19:52:41.183251Z","iopub.status.idle":"2021-05-20T19:52:41.188815Z","shell.execute_reply.started":"2021-05-20T19:52:41.183209Z","shell.execute_reply":"2021-05-20T19:52:41.187735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding = {}\nwith open(\"/kaggle/input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt\") as file:\n    for line in file:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding[word] = vectors\nfile.close()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:52:41.190397Z","iopub.execute_input":"2021-05-20T19:52:41.190829Z","iopub.status.idle":"2021-05-20T19:53:35.838325Z","shell.execute_reply.started":"2021-05-20T19:52:41.190775Z","shell.execute_reply":"2021-05-20T19:53:35.8373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((num_words+1, 100))\nfor i, word in tokenizer.index_word.items():\n    if i < (num_words+1):\n        vector = embedding.get(word)\n        if vector is not None:\n            embedding_matrix[i] = vector","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:53:35.839486Z","iopub.execute_input":"2021-05-20T19:53:35.839739Z","iopub.status.idle":"2021-05-20T19:53:35.866772Z","shell.execute_reply.started":"2021-05-20T19:53:35.839714Z","shell.execute_reply":"2021-05-20T19:53:35.865909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\n\nEvaluation Rules: \nSubmissions are evaluated using F1 score between the predicted and expected answers.\n\nIn order to evaluate my model, I have split my training data with 30% testing data.","metadata":{}},{"cell_type":"code","source":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:53:35.868374Z","iopub.execute_input":"2021-05-20T19:53:35.869129Z","iopub.status.idle":"2021-05-20T19:53:35.875487Z","shell.execute_reply.started":"2021-05-20T19:53:35.86908Z","shell.execute_reply":"2021-05-20T19:53:35.874496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initalizing Model","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Embedding(input_dim=num_words+1, output_dim=100, \n                    embeddings_initializer=Constant(embedding_matrix), \n                    input_length=32, trainable=False))\nmodel.add(LSTM(100, dropout=0.1))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:53:35.877221Z","iopub.execute_input":"2021-05-20T19:53:35.877616Z","iopub.status.idle":"2021-05-20T19:53:36.478215Z","shell.execute_reply.started":"2021-05-20T19:53:35.877575Z","shell.execute_reply":"2021-05-20T19:53:36.477307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:53:36.482036Z","iopub.execute_input":"2021-05-20T19:53:36.482342Z","iopub.status.idle":"2021-05-20T19:53:36.489264Z","shell.execute_reply.started":"2021-05-20T19:53:36.482311Z","shell.execute_reply":"2021-05-20T19:53:36.488083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training Model","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=5, batch_size=256, \n                    validation_data=(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:53:36.490416Z","iopub.execute_input":"2021-05-20T19:53:36.490693Z","iopub.status.idle":"2021-05-20T19:53:50.753488Z","shell.execute_reply.started":"2021-05-20T19:53:36.490666Z","shell.execute_reply":"2021-05-20T19:53:50.752557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting loss while training","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nepochs = range(1, len(history.history['accuracy'])+1)\nplt.plot(epochs, history.history['loss'], 'b', label='Training Loss', color='red')\nplt.plot(epochs, history.history['val_loss'], 'b', label='Validation Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:53:50.755069Z","iopub.execute_input":"2021-05-20T19:53:50.755349Z","iopub.status.idle":"2021-05-20T19:53:51.025107Z","shell.execute_reply.started":"2021-05-20T19:53:50.75532Z","shell.execute_reply":"2021-05-20T19:53:51.024244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting accuracy while training","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,5))\nepochs = range(1, len(history.history['accuracy'])+1)\nplt.plot(epochs, history.history['accuracy'], 'b', label='Training Accuracy', color='red')\nplt.plot(epochs, history.history['val_accuracy'], 'b', label='Validation Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:53:51.026563Z","iopub.execute_input":"2021-05-20T19:53:51.026946Z","iopub.status.idle":"2021-05-20T19:53:51.284174Z","shell.execute_reply.started":"2021-05-20T19:53:51.026901Z","shell.execute_reply":"2021-05-20T19:53:51.283333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict_classes(X_test)\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:53:51.285557Z","iopub.execute_input":"2021-05-20T19:53:51.285828Z","iopub.status.idle":"2021-05-20T19:53:52.465497Z","shell.execute_reply.started":"2021-05-20T19:53:51.285802Z","shell.execute_reply":"2021-05-20T19:53:52.464232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels= [\"Negative\", \"Positive\"]\ncm = confusion_matrix(y_test, y_pred)\n\nfig, ax = plt.subplots(figsize=(20, 10))\nsns.heatmap(cm, annot=True, fmt=\"g\", ax=ax)\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"Actual Labels\")\nplt.title(\"Confusion Matrix\", size=20, weight=\"bold\")\nplt.yticks(ticks=[0.5, 1.5], labels=labels)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:53:52.467158Z","iopub.execute_input":"2021-05-20T19:53:52.467557Z","iopub.status.idle":"2021-05-20T19:53:52.765779Z","shell.execute_reply.started":"2021-05-20T19:53:52.467514Z","shell.execute_reply":"2021-05-20T19:53:52.764644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nConsidering this dataset is small, and only using the 'text' feature I got a good f1 score on predicting the negative class (dominant class) but only got a decent score for predicting the positive class. The confusion matrix also indicates there were more false predictions for the positive class i.e 265 and 175 for the negative class. In the future, I plan to perform Synthetic Minority Oversampling Technique and upload a new version with improvements (hopefully!) and try evaluating more models for better understanding. \n\nIf you enjoyed and liked my work, please follow up and upvote and comment your feedback as I am a beginner and open to learn.","metadata":{}}]}