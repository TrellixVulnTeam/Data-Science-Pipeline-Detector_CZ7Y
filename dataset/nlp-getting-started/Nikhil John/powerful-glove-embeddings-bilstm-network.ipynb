{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Import necessary libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, precision_recall_curve\n\nimport re\nimport string\nimport tqdm\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nlemma = WordNetLemmatizer()\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization,SpatialDropout1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} rows and {} columns in train set'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in test et'.format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val = pd.DataFrame(train.isnull().sum())\nmissing_val = missing_val.reset_index()\nmissing_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data cleaning and preprocessing\n\nBefore doing the data analysis, it would be better if we could clean-up our data to remove html tags, emojis and other punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions\n\ntrain['clean_text'] = train['text'].apply(lambda x: remove_URL(x))\ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_emoji(x))\ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_html(x))\ntrain['clean_text'] = train['clean_text'].apply(lambda x: remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizing the cleaned texts.\n\ntrain['tokenized'] = train['clean_text'].apply(word_tokenize)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['lower'] = train['tokenized'].apply(\n    lambda x: [word.lower() for word in x])\n\ntrain['no_stopwords'] = train['lower'].apply(\n    lambda x: [word for word in x if word not in set(nltk.corpus.stopwords.words('english'))])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['no_stopwords'] = [' '.join(map(str, l)) for l in train['no_stopwords']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['clean_text'] = test['text'].apply(lambda x: remove_URL(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_emoji(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_html(x))\ntest['clean_text'] = test['clean_text'].apply(lambda x: remove_punct(x))\n\ntest['tokenized'] = test['clean_text'].apply(word_tokenize)\n\ntest['lower'] = test['tokenized'].apply(\n    lambda x: [word.lower() for word in x])\n\ntest['no_stopwords'] = test['lower'].apply(\n    lambda x: [word for word in x if word not in set(nltk.corpus.stopwords.words('english'))])\n\ntest['no_stopwords'] = [' '.join(map(str, l)) for l in test['no_stopwords']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = train['no_stopwords'].tolist() + test['no_stopwords'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(combined)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target distribution.\n\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\nsns.countplot(train['target'], ax=axes[0])\naxes[1].pie(train['target'].value_counts(),\n            labels=['Not Disaster', 'Disaster'],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nfig.suptitle('Distribution of the Tweets', fontsize=24)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word cloud for all disaster tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nstopwords = nltk.corpus.stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.no_stopwords[train['target']==1])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Word cloud for all disaster tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.no_stopwords[train['target']==0])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Words in a processed tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='yellow')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Words in a processed tweet')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average number of words in a processed tweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='purple')\nax1.set_title('disaster tweets')\nword=train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='orange')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Average word length in each processed tweet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating embedding of our tweets using GloVe embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load pretrained GloVe embeddings\n\nembeddings_index = dict()\nf = open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Embeddings for the tweets\n\nmax_len_tweet = 50\n\ntok = Tokenizer()\ntok.fit_on_texts(combined)\nvocab_size = len(tok.word_index) + 1\nencoded_tweet = tok.texts_to_sequences(combined)\npadded_tweet = pad_sequences(encoded_tweet, maxlen=max_len_tweet, padding='post')\n\nvocab_size = len(tok.word_index) + 1\n\ntweet_embedding_matrix = np.zeros((vocab_size, 100))\nfor word, i in tok.word_index.items():\n    t_embedding_vector = embeddings_index.get(word)\n    if t_embedding_vector is not None:\n        tweet_embedding_matrix[i] = t_embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define LSTM network"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 100, input_length=max_len_tweet, embeddings_initializer=Constant(tweet_embedding_matrix), trainable=False))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(128, dropout=0.3, recurrent_dropout=0.2))\nmodel.add(BatchNormalization())\n# model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n# model.add(BatchNormalization())\n# model.add(LSTM(20,dropout=0.2, recurrent_dropout=0.2))\n# model.add(Dropout(0.3))\n#model.add(BatchNormalization())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\noptimzer=Adam(learning_rate=1e-4)\nmodel.compile(loss='binary_crossentropy', optimizer=optimzer, metrics=['accuracy', 'mae'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(padded_tweet[:7613], train['target'].values, epochs = 11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(padded_tweet[7613:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pred=[1 if i>0.5 else 0 for i in preds]\npred = np.round(preds).astype(int).reshape(3263)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.DataFrame()\nsubmission['id']=test['id'].to_list()\nsubmission['target']=pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission4.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}