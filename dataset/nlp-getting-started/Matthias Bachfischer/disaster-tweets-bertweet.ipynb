{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"We first install the latest version of the `transformers` library (provides access to pre-trained language models like BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet etc.) ","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/huggingface/transformers.git\n!pip3 install --upgrade ./transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Install `emoji`","metadata":{}},{"cell_type":"code","source":"! pip install emoji","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport time\nimport datetime\nimport torch\nimport argparse\nimport numpy as np\nimport pandas as pd\nfrom torch.nn import functional as F\nfrom transformers import (get_linear_schedule_with_warmup,AdamW,AutoModel, AutoTokenizer,\n                            AutoModelForSequenceClassification)\nfrom torch.utils.data import (TensorDataset,DataLoader,\n                             RandomSampler, SequentialSampler, Dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define some helper functions to handle formatting:","metadata":{}},{"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We feed the text input to the BERTweet tokenizer. \n\nFor each input,we construct the following:\n* **input ids:** a sequence of integers identifying each input token to its index number in the PureBERT tokenizer vocabulary\n* **attention mask:** a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens","metadata":{}},{"cell_type":"code","source":"def bert_encode(df, tokenizer):\n    input_ids = []\n    attention_masks = []\n    for sent in df[[\"text\"]].values:\n        sent = sent.item()\n        encoded_dict = tokenizer.encode_plus(\n                            sent,                      \n                            add_special_tokens = True, \n                            max_length = 128,           \n                            pad_to_max_length = True,\n                            truncation = True,\n                            return_attention_mask = True,   \n                            return_tensors = 'pt',    \n                    )\n           \n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    inputs = {\n    'input_word_ids': input_ids,\n    'input_mask': attention_masks}\n\n    return inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We construct the PyTorch DataLoader for the training and test dataset. Note that we do not perform any pre-processing at all (the BERTweet tokenizer takes care of handling URLS, emojis etc.)!","metadata":{}},{"cell_type":"code","source":"def prepare_dataloaders(train_df,test_df,batch_size=8):\n    # Load the AutoTokenizer with a normalization mode if the input Tweet is raw\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n    \n    tweet_train = bert_encode(train_df, tokenizer)\n    tweet_train_labels = train_df.target.astype(int)\n    \n    tweet_test = bert_encode(test_df, tokenizer)\n\n    input_ids, attention_masks = tweet_train.values()\n    labels = torch.tensor(tweet_train_labels.values)\n    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n\n    \n    input_ids, attention_masks = tweet_test.values()\n    test_dataset = TensorDataset(input_ids, attention_masks)\n\n    \n    train_dataloader = DataLoader(\n                train_dataset,\n                sampler = RandomSampler(train_dataset), \n                batch_size = batch_size \n            )\n\n\n    test_dataloader = DataLoader(\n                test_dataset, \n                sampler = SequentialSampler(test_dataset), \n                batch_size = batch_size\n            )\n    return train_dataloader, test_dataloader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader,test_dataloader = prepare_dataloaders(train_df, test_df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Experiments","metadata":{}},{"cell_type":"markdown","source":"Below we study the tokenization process (i.e. mapping the input text from the tweets to input_ids from tokenizer) ","metadata":{}},{"cell_type":"code","source":"def test_encode(sentence):\n    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n\n    encoded_dict = tokenizer.encode_plus(\n                        sentence,                      \n                        add_special_tokens = True, \n                        max_length = 128,           \n                        pad_to_max_length = True,\n                        truncation = True,\n                        return_attention_mask = True,   \n                        return_tensors = 'pt',    \n                )\n           \n    return encoded_dict['input_ids']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_decode(tokens):\n    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n    return tokenizer.convert_ids_to_tokens(tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.text[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_test = train_df.text[0]\ntext_preprocessed = test_encode(text_test)\n\n\nprint(f'Shape      : {text_preprocessed.shape}')\nprint(f'Word Ids   : {text_preprocessed[0, :128]}')\nprint(test_decode(text_preprocessed[0, :128]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"Define the model using AdamW optimizer and an initial learning rate of 5e-5 and set epsilon to 1e-8.\n\nPassing a model to *model_to_load* allows you to load a pre-trained BERTweet model - the default BERTweet model is `vinai/bertweet-bas`).","metadata":{}},{"cell_type":"code","source":"def prepare_model(model_class=\"vinai/bertweet-base\",num_classes=2,model_to_load=None,total_steps=-1):\n\n\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_class,\n        num_labels = num_classes,  \n        output_attentions = False, \n        output_hidden_states = False,\n    )\n\n    optimizer = AdamW(model.parameters(),\n                    lr = 5e-5,\n                    eps = 1e-8\n                    )\n    scheduler = get_linear_schedule_with_warmup(optimizer, \n                                                num_warmup_steps = 0, \n                                                num_training_steps = total_steps)\n\n    if model_to_load is not None:\n        try:\n            model.roberta.load_state_dict(torch.load(model_to_load))\n            print(\"LOADED MODEL\")\n        except:\n            pass\n    return model, optimizer, scheduler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\ntotal_steps = len(train_dataloader) * epochs\n\nmodel, optimizer, scheduler = prepare_model(\"vinai/bertweet-base\" ,num_classes=2, model_to_load=None, total_steps = total_steps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the training loop - during training, we print out the loss.","metadata":{}},{"cell_type":"code","source":"def train(model,optimizer,scheduler,train_dataloader,epochs):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    training_stats = []\n    total_t0 = time.time()\n\n    for epoch_i in range(0, epochs):\n\n        print(\"\")\n        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n        \n        t0 = time.time()\n        total_train_loss = 0\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            if step % 40 == 0 and not step == 0:\n                elapsed = format_time(time.time() - t0)\n                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n            b_input_ids = batch[0].to(device)\n            b_input_mask = batch[1].to(device)\n            b_labels = batch[2].to(device)\n            model.zero_grad()        \n            outputs = model(b_input_ids, \n                                token_type_ids=None, \n                                attention_mask=b_input_mask, \n                                labels=b_labels)\n            loss = outputs.loss\n            logits = outputs.logits\n            total_train_loss += loss.item()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n        avg_train_loss = total_train_loss / len(train_dataloader)            \n        training_time = format_time(time.time() - t0)\n\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n\n    print(\"\")\n    print(\"Training complete!\")\n\n    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(model,optimizer,scheduler,train_dataloader, epochs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"markdown","source":"The `predict` method allows you to perform inference on the test set.","metadata":{}},{"cell_type":"code","source":"def predict(model,test_dataloader):\n    model.eval()\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model.to(device)\n    preds = []\n\n    for batch in test_dataloader:\n        \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        with torch.no_grad():        \n            outputs = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask)\n            logits = outputs.logits\n\n        logits = logits.detach().cpu().numpy()\n        for logit in logits:\n            preds.append(logit)\n\n    return preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = predict(model,test_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.special import softmax\n\npred_labels = np.argmax(result, axis = 1)\npred_scores = softmax(result, axis=1)[:, 1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'id':test_df.id,'target':pred_labels})\noutput","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}