{"cells":[{"metadata":{},"cell_type":"markdown","source":"# An introduction to Fastai for NLP Text Classification\n\nThis notebook will describe the basic steps to create a text classifier with the library Fastai. It is very simple and is based on the example/overview provided in the website of fastai. Some modifications have been included, but they are minor changes. I am not trying to get the best model, but with this guide you can improve the model performance applying some improvement (using some embeddings, optimizing the model, trying pretrained models,...) \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom fastai.text import * \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_path=\"../output\"\ntext_columns=['text']\nlabel_columns=['target']\nBATCH_SIZE=128","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest= pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = pd.concat([train[text_columns], test[text_columns]])\nprint(tweets.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train a text language model\nCreate a databunch for a text language model to get the data ready for training a language model. The text will be processed, tokenized and numericalized by a default processor, if you want to apply a customized tokenizer or vocab, you just need to create them."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lm = (TextList.from_df(tweets)\n           #Inputs: all the text files in path\n            .split_by_rand_pct(0.15)\n           #We randomly split and keep 10% for validation\n            .label_for_lm()           \n           #We want to do a language model so we label accordingly\n            .databunch(bs=BATCH_SIZE))\ndata_lm.save('tmp_lm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lm.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can create a language model based on the architecture AWD_LSTM (a detailed explanation can be found on the fastai website https://docs.fast.ai/text.models.html#AWD_LSTM):"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model Summary:')\nprint(learn.layer_groups)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets train our language model. First, we call lr_find to analyze and find an optimal learning rate for our problem, then we fit or train the model for a few epochs. Finally we unfreeze the model and runs it for a few more epochs. \nSo we have a encoder trained and ready to be used for our classifier and it is recorded on disk."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, 1e-2)\nlearn.save('lm_fit_head')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('lm_fit_head')\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save_encoder('ft_enc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building and training a text classifier"},{"metadata":{},"cell_type":"markdown","source":"Now that we have a language model trained, we can create a text classifier on it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clas = (TextList.from_df(train, cols=text_columns, vocab=data_lm.vocab)\n             .split_by_rand_pct(0.15)\n             .label_from_df('target')\n             .add_test(test[text_columns])\n             .databunch(bs=BATCH_SIZE))\n\ndata_clas.save('tmp_clas')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next step, to load the encoder previously trained (the language model)."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load_encoder('ft_enc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the training cycle is repeated: lr_find, freeze except last layer,..., unfreeze the model and saving the final trained model."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-1)\nlearn.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(10, 1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('stage1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('stage1')\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(5, slice(5e-3/2., 5e-3))\nlearn.save('stage2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('stage2')\nlearn.unfreeze()\nlearn.fit_one_cycle(5, slice(2e-3/100, 2e-3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.export()\nlearn.save('final')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some analysis and interpretation of the results\nThere are some functions to explore the behaviour of our model. For example we can explore the confusion matrix and show the top losses of our classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.vision import ClassificationInterpretation\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(6,6), dpi=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = TextClassificationInterpretation.from_learner(learn)\ninterp.show_top_losses(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting and creating a submission file\nNow we can predict on the test set and create the submission file requiered by the competition:"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.predict(test.loc[0,'text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in learn.data.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = get_preds_as_nparray(DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsample_submission['target'] = np.argmax(test_preds, axis=1)\nsample_submission.to_csv(\"predictions.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}