{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-16T21:17:39.707549Z","iopub.execute_input":"2021-11-16T21:17:39.707762Z","iopub.status.idle":"2021-11-16T21:17:39.718046Z","shell.execute_reply.started":"2021-11-16T21:17:39.70774Z","shell.execute_reply":"2021-11-16T21:17:39.717155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Part of the code is taken from https://www.kaggle.com/mohitsital/0-80777-simplest-model-naive-bayes/data.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:17:39.720098Z","iopub.execute_input":"2021-11-16T21:17:39.720763Z","iopub.status.idle":"2021-11-16T21:17:39.759816Z","shell.execute_reply.started":"2021-11-16T21:17:39.720723Z","shell.execute_reply":"2021-11-16T21:17:39.75935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:17:39.760672Z","iopub.execute_input":"2021-11-16T21:17:39.760925Z","iopub.status.idle":"2021-11-16T21:17:39.763763Z","shell.execute_reply.started":"2021-11-16T21:17:39.760903Z","shell.execute_reply":"2021-11-16T21:17:39.763371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is a function for transforming the tweet to a more accessible format.","metadata":{}},{"cell_type":"code","source":"import re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\ndef process_tweet(tweet):\n    tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n    tweet2 = re.sub('https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n    tweet2 = re.sub(r'#', '', tweet2)\n    tokenizer = TweetTokenizer(preserve_case = False, strip_handles = True, reduce_len = True)\n    tweet_tokens = tokenizer.tokenize(tweet2)\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords.words('english') and word not in string.punctuation):\n            tweets_clean.append(word)\n    stemmer = PorterStemmer()\n    tweets_stem = []\n    for word in tweets_clean:\n        stem_word = stemmer.stem(word)\n        tweets_stem.append(stem_word)\n    return \" \".join(tweets_stem)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:17:39.764646Z","iopub.execute_input":"2021-11-16T21:17:39.76504Z","iopub.status.idle":"2021-11-16T21:17:39.781016Z","shell.execute_reply.started":"2021-11-16T21:17:39.765013Z","shell.execute_reply":"2021-11-16T21:17:39.780093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(process_tweet('forest%20fire'))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:17:39.782802Z","iopub.execute_input":"2021-11-16T21:17:39.783363Z","iopub.status.idle":"2021-11-16T21:17:39.795871Z","shell.execute_reply.started":"2021-11-16T21:17:39.783315Z","shell.execute_reply":"2021-11-16T21:17:39.795166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And these functions deal with the keyword and location. I have surveyed these data fields to see what changes need to be made (eg relating 'US' to 'United States').","metadata":{}},{"cell_type":"code","source":"#Cleans keyword.\ndef process_keyword(keyword):\n    keyword_arr = []\n    keywords = keyword.split('%20')\n    stemmer = PorterStemmer()\n    for word in keywords:\n        word = word.lower()\n        if word not in stopwords.words('english') and word not in string.punctuation:\n            keyword_arr.append(stemmer.stem(word))\n    return \" \".join(keyword_arr)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:17:39.798552Z","iopub.execute_input":"2021-11-16T21:17:39.798773Z","iopub.status.idle":"2021-11-16T21:17:39.808151Z","shell.execute_reply.started":"2021-11-16T21:17:39.798744Z","shell.execute_reply":"2021-11-16T21:17:39.807612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(process_keyword('evacuation'))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:17:39.809002Z","iopub.execute_input":"2021-11-16T21:17:39.809706Z","iopub.status.idle":"2021-11-16T21:17:39.824164Z","shell.execute_reply.started":"2021-11-16T21:17:39.809678Z","shell.execute_reply":"2021-11-16T21:17:39.823666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cleans location.\ndef process_location(location):\n    #Replace short-hand\n    dictionary = {'United States': 'US', 'New York': 'NYC', 'Los Angeles': 'LA', 'D.C.': 'DC', \n                  'United Kingdom': 'UK', 'USA': 'US', 'Planet': '', 'California': 'CA', \n                  'New York City': 'NYC', 'Texas': 'TX', 'San Diego': 'SanDiego', 'South Africa': 'SouthAfrica', \n                  'Tennessee': 'TN', 'New Jersey': 'NJ'}\n    for i in dictionary.keys():\n        location = location.replace(i, dictionary[i])\n    locations = location.replace(',', '').split()\n    ret_arr = []\n    for loc in locations:\n        loc2 = loc.lower()\n        ret_arr.append(loc2)\n    return \" \".join(ret_arr)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:17:39.826022Z","iopub.execute_input":"2021-11-16T21:17:39.826766Z","iopub.status.idle":"2021-11-16T21:17:39.836648Z","shell.execute_reply.started":"2021-11-16T21:17:39.826734Z","shell.execute_reply":"2021-11-16T21:17:39.835834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(process_location('Los Angeles, CA'))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:17:39.837798Z","iopub.execute_input":"2021-11-16T21:17:39.838101Z","iopub.status.idle":"2021-11-16T21:17:39.850031Z","shell.execute_reply.started":"2021-11-16T21:17:39.838073Z","shell.execute_reply":"2021-11-16T21:17:39.849477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We fill the NA data and treat it as a keyword or location, since there are quite a lot of NAs out there.","metadata":{}},{"cell_type":"code","source":"train = train.fillna('nan')\ntest = test.fillna('nan')","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:17:39.851048Z","iopub.execute_input":"2021-11-16T21:17:39.851677Z","iopub.status.idle":"2021-11-16T21:17:39.867867Z","shell.execute_reply.started":"2021-11-16T21:17:39.851642Z","shell.execute_reply":"2021-11-16T21:17:39.867281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(train.shape[0]):\n    train.loc[i, 'keyword'] = process_keyword(train.loc[i, 'keyword'])\n    train.loc[i, 'location'] = process_location(train.loc[i, 'location'])\n    train.loc[i, 'text'] = process_tweet(train.loc[i, 'text'])","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:17:39.870238Z","iopub.execute_input":"2021-11-16T21:17:39.870505Z","iopub.status.idle":"2021-11-16T21:18:05.429821Z","shell.execute_reply.started":"2021-11-16T21:17:39.870475Z","shell.execute_reply":"2021-11-16T21:18:05.428629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(test.shape[0]):\n    test.loc[i, 'keyword'] = process_keyword(test.loc[i, 'keyword'])\n    test.loc[i, 'location'] = process_location(test.loc[i, 'location'])\n    test.loc[i, 'text'] = process_tweet(test.loc[i, 'text'])","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:05.43095Z","iopub.execute_input":"2021-11-16T21:18:05.431174Z","iopub.status.idle":"2021-11-16T21:18:19.153313Z","shell.execute_reply.started":"2021-11-16T21:18:05.431146Z","shell.execute_reply":"2021-11-16T21:18:19.152606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We first try tf_idf to vectorise the data.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\ntf_idf_vect = TfidfVectorizer(ngram_range=(1,3)) # one,two and three gram vectorization\ndf_train, df_val = train_test_split(train)\nfreqs_train = tf_idf_vect.fit_transform(df_train['text'].values)\nfreqs_val = tf_idf_vect.transform(df_val['text'].values)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:19.154148Z","iopub.execute_input":"2021-11-16T21:18:19.15436Z","iopub.status.idle":"2021-11-16T21:18:19.736342Z","shell.execute_reply.started":"2021-11-16T21:18:19.154315Z","shell.execute_reply":"2021-11-16T21:18:19.735558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf_key = TfidfVectorizer()\ntf_idf_loc = TfidfVectorizer()\nkey_train = tf_idf_key.fit_transform(df_train['keyword'].values)\nloc_train = tf_idf_loc.fit_transform(df_train['location'].values)\nkey_val = tf_idf_key.transform(df_val['keyword'].values)\nloc_val = tf_idf_loc.transform(df_val['location'].values)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:19.739156Z","iopub.execute_input":"2021-11-16T21:18:19.739402Z","iopub.status.idle":"2021-11-16T21:18:19.85254Z","shell.execute_reply.started":"2021-11-16T21:18:19.739376Z","shell.execute_reply":"2021-11-16T21:18:19.85168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(freqs_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:19.853628Z","iopub.execute_input":"2021-11-16T21:18:19.853824Z","iopub.status.idle":"2021-11-16T21:18:19.859573Z","shell.execute_reply.started":"2021-11-16T21:18:19.853796Z","shell.execute_reply":"2021-11-16T21:18:19.858849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we build a naive bayes classifier on each indicator: text, key and location.","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\ny_train = df_train['target'].values\nnb_text = MultinomialNB().fit(freqs_train, y_train)\nnb_key = MultinomialNB().fit(key_train, y_train)\nnb_loc = MultinomialNB().fit(loc_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:19.860509Z","iopub.execute_input":"2021-11-16T21:18:19.860703Z","iopub.status.idle":"2021-11-16T21:18:19.886423Z","shell.execute_reply.started":"2021-11-16T21:18:19.860676Z","shell.execute_reply":"2021-11-16T21:18:19.885651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\ndef printreport(exp, pred):\n    print(pd.crosstab(exp, pred, rownames=['Actual'], colnames=['Predicted']))\n    print('\\n \\n')\n    print(classification_report(exp, pred))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:19.887517Z","iopub.execute_input":"2021-11-16T21:18:19.887757Z","iopub.status.idle":"2021-11-16T21:18:19.893Z","shell.execute_reply.started":"2021-11-16T21:18:19.887722Z","shell.execute_reply":"2021-11-16T21:18:19.892139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Essentially we calculate the log probabilities for each classifier and try to combine them. We do a grid search to find the best parameters for each naive bayes classifier.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score\ny_val = df_val['target'].values\ntext_weighting_arr = [0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1, 1.1, 1.2, 1.3, 1.4, 1.5]\nkey_weighting_arr = [0, .05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8]\nresult = np.zeros((len(text_weighting_arr), len(key_weighting_arr)))\nfor i in range(len(text_weighting_arr)):\n    for j in range(len(key_weighting_arr)):\n        if (text_weighting_arr[i] + key_weighting_arr[j] <= 1):\n            predicted_proba = text_weighting_arr[i] * nb_text.predict_log_proba(freqs_val) + key_weighting_arr[j] * nb_key.predict_log_proba(key_val) + (1-text_weighting_arr[i]-key_weighting_arr[j]) * nb_loc.predict_log_proba(loc_val)\n            predicted = np.argmax(predicted_proba, axis = 1)\n            result[i,j] = f1_score(y_val, predicted)\n#print(result)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:19.894088Z","iopub.execute_input":"2021-11-16T21:18:19.89489Z","iopub.status.idle":"2021-11-16T21:18:20.443898Z","shell.execute_reply.started":"2021-11-16T21:18:19.894857Z","shell.execute_reply":"2021-11-16T21:18:20.443127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_proba = .6 * nb_text.predict_log_proba(freqs_val) + .25 * nb_key.predict_log_proba(key_val) + .15 * nb_loc.predict_log_proba(loc_val)\npredicted = np.argmax(predicted_proba, axis = 1)\ny_val = df_val['target']\n#printreport(y_val, predicted)\n#print(f1_score(y_val, predicted))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:20.445003Z","iopub.execute_input":"2021-11-16T21:18:20.446945Z","iopub.status.idle":"2021-11-16T21:18:20.476142Z","shell.execute_reply.started":"2021-11-16T21:18:20.44686Z","shell.execute_reply":"2021-11-16T21:18:20.475071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer(ngram_range=(1,3))\ndf_train_2, df_val_2 = train_test_split(train)\nfreqs_train_2 = count_vect.fit_transform(df_train_2['text'].values)\nfreqs_val_2 = count_vect.transform(df_val_2['text'].values)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:20.479055Z","iopub.execute_input":"2021-11-16T21:18:20.479259Z","iopub.status.idle":"2021-11-16T21:18:21.077716Z","shell.execute_reply.started":"2021-11-16T21:18:20.479232Z","shell.execute_reply":"2021-11-16T21:18:21.076907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\ny_train_2 = df_train_2['target'].values\nnb_2 = MultinomialNB().fit(freqs_train_2, y_train_2)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:21.07918Z","iopub.execute_input":"2021-11-16T21:18:21.07941Z","iopub.status.idle":"2021-11-16T21:18:21.094492Z","shell.execute_reply.started":"2021-11-16T21:18:21.079381Z","shell.execute_reply":"2021-11-16T21:18:21.093537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_2 = nb_2.predict(freqs_val_2)\ny_val_2 = df_val_2['target'].values\nprintreport(y_val_2, predicted_2)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:21.095823Z","iopub.execute_input":"2021-11-16T21:18:21.096159Z","iopub.status.idle":"2021-11-16T21:18:21.118025Z","shell.execute_reply.started":"2021-11-16T21:18:21.096128Z","shell.execute_reply":"2021-11-16T21:18:21.117456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we train the Naive Bayes model on the whole training set.","metadata":{}},{"cell_type":"code","source":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,3)) # one,two and three gram vectorization\ntf_idf_key = TfidfVectorizer()\ntf_idf_loc = TfidfVectorizer()\nfreqs_train = tf_idf_vect.fit_transform(train['text'].values)\nkey_train = tf_idf_key.fit_transform(train['keyword'].values)\nloc_train = tf_idf_loc.fit_transform(train['location'].values)\nfreqs_test = tf_idf_vect.transform(test['text'].values)\nkey_test = tf_idf_key.transform(test['keyword'].values)\nloc_test = tf_idf_loc.transform(test['location'].values)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:21.118998Z","iopub.execute_input":"2021-11-16T21:18:21.121455Z","iopub.status.idle":"2021-11-16T21:18:22.103862Z","shell.execute_reply.started":"2021-11-16T21:18:21.12143Z","shell.execute_reply":"2021-11-16T21:18:22.102884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train['target'].values\nnb_text = MultinomialNB().fit(freqs_train, y_train)\nnb_key = MultinomialNB().fit(key_train, y_train)\nnb_loc = MultinomialNB().fit(loc_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:22.105289Z","iopub.execute_input":"2021-11-16T21:18:22.105512Z","iopub.status.idle":"2021-11-16T21:18:22.121923Z","shell.execute_reply.started":"2021-11-16T21:18:22.105485Z","shell.execute_reply":"2021-11-16T21:18:22.12108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicted_proba = .6 * nb_text.predict_log_proba(freqs_test) + .25 * nb_key.predict_log_proba(key_test) + .15 * nb_loc.predict_log_proba(loc_test)\n#predicted = np.argmax(predicted_proba, axis = 1)\npredicted = nb_text.predict(freqs_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:22.122904Z","iopub.execute_input":"2021-11-16T21:18:22.123089Z","iopub.status.idle":"2021-11-16T21:18:22.132319Z","shell.execute_reply.started":"2021-11-16T21:18:22.123067Z","shell.execute_reply":"2021-11-16T21:18:22.13161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Make submission file\nsubmission_df = pd.DataFrame()\nsubmission_df['id'] = test['id']\nsubmission_df['target'] = predicted\nprint(submission_df.head())","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:22.1346Z","iopub.execute_input":"2021-11-16T21:18:22.134926Z","iopub.status.idle":"2021-11-16T21:18:22.142765Z","shell.execute_reply.started":"2021-11-16T21:18:22.134903Z","shell.execute_reply":"2021-11-16T21:18:22.142357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T21:18:22.143888Z","iopub.execute_input":"2021-11-16T21:18:22.144552Z","iopub.status.idle":"2021-11-16T21:18:22.16029Z","shell.execute_reply.started":"2021-11-16T21:18:22.144522Z","shell.execute_reply":"2021-11-16T21:18:22.159462Z"},"trusted":true},"execution_count":null,"outputs":[]}]}