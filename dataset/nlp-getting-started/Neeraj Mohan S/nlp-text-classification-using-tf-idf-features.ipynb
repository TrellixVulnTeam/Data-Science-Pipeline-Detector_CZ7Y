{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem statement\n\nThe aim is to develop an algorithm to predict whether a tweet is about a real disaster or not.","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport re\nimport string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer  \nstop_words = stopwords.words('english')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:37.748157Z","iopub.execute_input":"2022-05-11T05:52:37.748835Z","iopub.status.idle":"2022-05-11T05:52:39.854563Z","shell.execute_reply.started":"2022-05-11T05:52:37.748787Z","shell.execute_reply":"2022-05-11T05:52:39.853552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:39.857577Z","iopub.execute_input":"2022-05-11T05:52:39.858099Z","iopub.status.idle":"2022-05-11T05:52:39.932819Z","shell.execute_reply.started":"2022-05-11T05:52:39.858049Z","shell.execute_reply":"2022-05-11T05:52:39.931634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.countplot(x=\"target\", data=df)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:39.934708Z","iopub.execute_input":"2022-05-11T05:52:39.935173Z","iopub.status.idle":"2022-05-11T05:52:40.094115Z","shell.execute_reply.started":"2022-05-11T05:52:39.935125Z","shell.execute_reply":"2022-05-11T05:52:40.09295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no class imbalance in the distribution of target variable.","metadata":{}},{"cell_type":"markdown","source":"# Data Clensing\n\nWe are using custom functions to perform the following tasks. Cleaning up the data for modeling should be carried out carefully and with the help of subject matter experts, if possible. This cleaning is done completely based on observation, and can not be considered as a generic preprocessing step for all the NLP tasks. This preprocessing function ensures:\n\n* Removing urls from tweet\n* Removing html tags\n* Removing punctuations\n* Removing stopwords\n* Removing emoji\n* Lemmatization","metadata":{}},{"cell_type":"code","source":"sw = stopwords.words('english')\nlemmatizer = WordNetLemmatizer() \n\ndef clean_text(text):\n    \n    text = text.lower()\n    \n    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n    #text = re.sub(r\"http\", \"\",text)\n    \n    html=re.compile(r'<.*?>') \n    \n    text = html.sub(r'',text) #Removing html tags\n    \n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n    for p in punctuations:\n        text = text.replace(p,'') #Removing punctuations\n        \n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    \n    text = [lemmatizer.lemmatize(word) for word in text]\n    \n    text = \" \".join(text) #removing stopwords\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text) #Removing emojis\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:40.095937Z","iopub.execute_input":"2022-05-11T05:52:40.096724Z","iopub.status.idle":"2022-05-11T05:52:40.111409Z","shell.execute_reply.started":"2022-05-11T05:52:40.096669Z","shell.execute_reply":"2022-05-11T05:52:40.110111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: clean_text(x))\n \ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:40.114375Z","iopub.execute_input":"2022-05-11T05:52:40.114705Z","iopub.status.idle":"2022-05-11T05:52:43.448643Z","shell.execute_reply.started":"2022-05-11T05:52:40.114676Z","shell.execute_reply":"2022-05-11T05:52:43.447454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Words to vectors","metadata":{}},{"cell_type":"markdown","source":"Reference\n\nhttps://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\n\nhttps://towardsdatascience.com/hacking-scikit-learns-vectorizers-9ef26a7170af","metadata":{}},{"cell_type":"markdown","source":"We need to convert the words to numeric representations, to use them as input to our machine learning model. One way to do this is:\n\n## Count Vectorizer\nThe most basic and naive method of transforming words into vectors by counting occurrence of each word in each document. The output is a document-term matrix with each row representing a document and each column addressing a token (weight assigned to each token based on counting the occurence). \n\nFor example:","metadata":{}},{"cell_type":"code","source":"sample_corpora = df['text'].iloc[:2].values\nsample_corpora","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:43.45033Z","iopub.execute_input":"2022-05-11T05:52:43.450704Z","iopub.status.idle":"2022-05-11T05:52:43.45792Z","shell.execute_reply.started":"2022-05-11T05:52:43.450672Z","shell.execute_reply":"2022-05-11T05:52:43.456697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_vectorizer = CountVectorizer()\nwm = count_vectorizer.fit_transform(sample_corpora)\n\ndoc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\nfeat_names = count_vectorizer.get_feature_names()\n\n\nsample_df = pd.DataFrame(data=wm.toarray(), index=doc_names,columns=feat_names)\nsample_df","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:43.459645Z","iopub.execute_input":"2022-05-11T05:52:43.460143Z","iopub.status.idle":"2022-05-11T05:52:43.48679Z","shell.execute_reply.started":"2022-05-11T05:52:43.460094Z","shell.execute_reply":"2022-05-11T05:52:43.485686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF Vectorizer\n\nTaking only word count in document have certain drawbacks. The presence of frequent word vectors wont be sparse (espicially stopwords), though the word may not be important. Also, rare words looks very sparse, hence less importance. To tackle these problems, we use TF-IDF vectorization. \n\nThe first part is **TF**, called term frequency. This simply means the number of times the word occur in document divided by the total number of words in the document. \n\nThe second part is **IDF**, which stands for ‘inverse document frequency’, interpreted like inversed number of documents, in which the term we’re interested in occurs.","metadata":{}},{"cell_type":"markdown","source":"**$tf(t,d)$**  is the term frequency is the number of times the term appears in the document.\n\n**$$tf(t,d) = {n_{t,d} \\over \\sum_{{k \\in d}} n_{k,d}} $$**\n\n\nWhere, $t$ is term and $d$ document.\n\n\n**$idf(t,D)$** is the document frequency is the number of documents '**d**' that contain term '**t**'.\n\n**$$idf(t,D) = {log {N\\over{n_t}}}$$**\n\n**$$d \\in D, t \\in d $$**\n\n\nwhere **$N$** is the total number of documents and **$n_t$** is the number of documents containing the term $t$.\n\nThe **TF-IDF** vector,\n\n**$$tfidf(t,d,D) = tf(t,d) * idf(t,D)$$**\n\n\n### Smoothing\n\nAn idf is constant per corpus for a word. A smoothened version, adding 1 to denominator is applied to avoid division by 0 (occurs when term is not present in corpora). The idea of idf is to weight down the frequent terms while scale up the rare ones, assuming more frequent ones are not always important ones. So, smoothened idf equation is:\n\n**$$idf(t,D) = {log {N\\over{1 + n_t}}}$$**\n\nNB : smoothing is set True as default in scikit learn tfidf vectorizer","metadata":{}},{"cell_type":"markdown","source":"# TF-IDF Vectorization using scikit learn","metadata":{}},{"cell_type":"markdown","source":"We will split the data to train and test set. In production, we may face out of vocabulary (OOV) words, so we need to account for that too while evaluating the model in test set. Hence we use training data to built vocabulory for tfidf vectorization.\n\nTfidf vectorization can be implemented easily using scikit learn.\n\nRef - https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html","metadata":{}},{"cell_type":"code","source":"X_train, X_test , y_train, y_test = train_test_split(df['text'].values,df['target'].values,test_size=0.2,random_state=123,stratify=df['target'].values)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:43.488404Z","iopub.execute_input":"2022-05-11T05:52:43.489183Z","iopub.status.idle":"2022-05-11T05:52:43.509343Z","shell.execute_reply.started":"2022-05-11T05:52:43.489131Z","shell.execute_reply":"2022-05-11T05:52:43.508118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer() \n\ntfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n\ntfidf_test_vectors = tfidf_vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:43.510574Z","iopub.execute_input":"2022-05-11T05:52:43.510869Z","iopub.status.idle":"2022-05-11T05:52:43.731386Z","shell.execute_reply.started":"2022-05-11T05:52:43.51084Z","shell.execute_reply":"2022-05-11T05:52:43.730356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest classifier\n\nA basic random forest model is used for classification problem.","metadata":{}},{"cell_type":"code","source":"classifier = RandomForestClassifier()\n\nclassifier.fit(tfidf_train_vectors,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:43.732528Z","iopub.execute_input":"2022-05-11T05:52:43.732837Z","iopub.status.idle":"2022-05-11T05:52:50.885656Z","shell.execute_reply.started":"2022-05-11T05:52:43.732808Z","shell.execute_reply":"2022-05-11T05:52:50.884276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(tfidf_test_vectors)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:50.886818Z","iopub.execute_input":"2022-05-11T05:52:50.887342Z","iopub.status.idle":"2022-05-11T05:52:51.040188Z","shell.execute_reply.started":"2022-05-11T05:52:50.887294Z","shell.execute_reply":"2022-05-11T05:52:51.039093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:51.041472Z","iopub.execute_input":"2022-05-11T05:52:51.041798Z","iopub.status.idle":"2022-05-11T05:52:51.055527Z","shell.execute_reply.started":"2022-05-11T05:52:51.041767Z","shell.execute_reply":"2022-05-11T05:52:51.054053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test,y_pred)\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');","metadata":{"execution":{"iopub.status.busy":"2022-05-11T05:52:51.057273Z","iopub.execute_input":"2022-05-11T05:52:51.05819Z","iopub.status.idle":"2022-05-11T05:52:51.265459Z","shell.execute_reply.started":"2022-05-11T05:52:51.058148Z","shell.execute_reply":"2022-05-11T05:52:51.264286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# End Notes\n\nThis is a beginer level notebook for the scikit learn based implementation of tfidf vectorization. The accuracy can be improved by using dense vectors like Word2Vec, Glove etc. , that accounts the semantic relationship between the words. Also, the problem can be treated as a sequential learning problem, and can use deep learning techniques.\n\nLink of my work on the same is provided below:\n\nhttps://www.kaggle.com/neerajmohan/bidirectional-lstm-using-tensorflow-2-0\n","metadata":{}}]}