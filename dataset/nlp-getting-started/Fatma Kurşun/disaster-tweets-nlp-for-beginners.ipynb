{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP with Disaster Tweets\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency theyâ€™re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12,8\n#sns.color_palette(\"hls\", 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Id = test.id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape #there are 7613 rows and 5 columns in the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head() # 1 is real disaster tweets 0 is fake ones","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(train['id'])) # There are 7613 unique users. Each tweet was tweeted by unique users","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['target'])\nplt.show()\nprint(train['target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y = train.keyword,order = train['keyword'].value_counts().sort_values(ascending=False).iloc[0:20].index)\nplt.title(\"Count of Keywords\")\nplt.show() # It shows the most usage keywords ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count of keywords for real disaster;\ndisastered_tweet = train.groupby('keyword')['target'].mean().sort_values(ascending=False).head(15)\nnon_disasterd  = train.groupby('keyword')['target'].mean().sort_values().head(15)\n\nplt.figure(figsize=(8,5))\nsns.barplot(disastered_tweet, disastered_tweet.index, color='red')\nplt.title('Keywords with highest % of disaster tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count of eywords for Non-Disasters\nplt.figure(figsize=(8,5))\nsns.barplot(non_disasterd, non_disasterd.index, color='blue')\nplt.title('Keywords with lowest % of disaster tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Locations"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y = train.location,order = train['location'].value_counts().sort_values(ascending=False).iloc[0:15].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As you can see the data is not clean. We need to do data cleansing. For example Us,USA and United State are same location.We need to seperate them \n\n* Let's see which location has the most disaster tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_loc = train.location.value_counts()\ntop_loc_disaster = list(raw_loc[raw_loc>=10].index)\ntop_only_disaster = train[train.location.isin(top_loc_disaster)]\n\ntop_location = top_only_disaster.groupby('location')['target'].mean().sort_values(ascending=False)\nsns.barplot(x=top_location.index, y=top_location)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Mumbai and India have the most disaster tweets. But we can see, there lots of noise in the location feature. We need to fix that "},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to fill null values with None\nfor i in ['keyword','location']:\n    train[i] = train[i].fillna('None')\n    test[i] = test[i].fillna('None')\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info() #As we can see there is no null value now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(train['location'])) #There are 3342 unique location values. we are going to decrease of that numbers as using data cleaning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_location(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    elif x in top_location:\n        return x\n    else: \n        return 'Others'\n    \ntrain['location'] = train['location'].apply(lambda x: clean_location(str(x)))\ntest['location'] = test['location'].apply(lambda x: clean_location(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_location = train.groupby('location')['target'].mean().sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_location.index, y=top_location)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It looks, the data clean anymore. Now, Mumbai and Nigeria have the most disasters tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(train['location'])) # As we can see, the unique values decreased. It has 27 now.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at the rondom tweets. \ntrain['text'][0]\n\n# As wee can see there is a hashtag(#) in that tweet. We can split the hashtag and can use as a new feature\n# let's look at another random tweet ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'][789] \n\n# There is a tagged in that tweet. We can also split thatn and we can use as a new feature\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'][417] # and in that tweet. there is a link.  we are gonna fix all those tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\n# We are going to split the hashtag,link and tagged\ndef created_feature(train):\n    train['hashtags'] = train['text'].apply(lambda x: \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", x)]) or 'no_hashtag')\n    train['tagged'] = train['text'].apply(lambda x: \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", x)]) or 'no_tagged')\n    train['link'] = train['text'].apply(lambda x:\" \".join([match.group(0)[:] for match in re.finditer(r\"https?://\\S+\", x)]) or 'no_link')\n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = created_feature(train)\ntest = created_feature(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train # As we can see, we have new features now. Great!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['hashtags'].value_counts().sort_values(ascending=False).iloc[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['tagged'].value_counts().sort_values(ascending=False).iloc[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['link'].value_counts().sort_values(ascending=False).iloc[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(r'https?://\\S+', '', text) # remove links\n    text = re.sub(r'\\n',' ', text) #  remove breaks\n    text = re.sub('\\s+', ' ', text).strip() \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'][417] # Let's look at that sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text(train['text'][417]) # as we can see we cleaned the tweet\n# We are gonna use of that method for all text ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Text Mining\nimport nltk\n#nltk.download(\"stopwords\")\n#!pip install textblob\n#nltk.download(\"wordnet\")\n\n#Upper lower convert\ntrain['text'] = train['text'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\ntest['text'] = test['text'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n\n# punctuation marks\ntrain['text'] =train['text'].str.replace('[^\\w\\s]','')\ntest['text'] =test['text'].str.replace('[^\\w\\s]','')\n\n# numbers\ntrain['text'] = train['text'].str.replace('[\\d]','')\ntest['text'] = test['text'].str.replace('[\\d]','')\n\nfrom nltk.corpus import stopwords\nsw = stopwords.words('english')\ntrain['text'] =train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\ntest['text'] =test['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n\n#lemmi \nfrom textblob import Word\ntrain['text'] = train['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ntest['text'] = test['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\n\ntrain['text'] = train['text'].str.replace('rt','')\ntest['text'] = test['text'].str.replace('rt','')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text # we did some cleaning to text ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_df = train['text'].apply(lambda x:pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\nfreq_df.columns = ['words', 'frequences']\nfreq_df.sort_values('frequences',ascending=False) # It shows frequences of words ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most used words\ntop_freq = freq_df.sort_values('frequences',ascending=False)[0:15]\ntop_freq.set_index('words',inplace=True)\ntop_freq.plot.bar(color=(0.2, 0.4, 0.6, 0.6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Most used words dor disasters\nfreq_df = train[train['target']==1]['text'].apply(lambda x:pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\nfreq_df.columns = ['words', 'frequences']\nfreq_df.sort_values('frequences',ascending=False)\n\ntop_freq_disaster = freq_df.sort_values('frequences',ascending=False)[0:15]\ntop_freq_disaster.set_index('words',inplace=True)\ntop_freq_disaster.plot.bar(color ='g')\nplt.title(\"Disaster Tweets\")\nplt.show()  #Fire and news are most used words in the disasters tweets.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most used words for Non-Disaster tweets\nfreq_df = train[train['target']==0]['text'].apply(lambda x:pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\nfreq_df.columns = ['words', 'frequences']\nfreq_df.sort_values('frequences',ascending=False)\n\ntop_freq_non_disaster = freq_df.sort_values('frequences',ascending=False)[0:15]\ntop_freq_non_disaster.set_index('words',inplace=True)\ntop_freq_non_disaster.plot.bar(color ='orange')\nplt.title(\"Non-Disaster Tweets\")\nplt.show() #","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nimport numpy as np\nimport pandas as pd\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud , STOPWORDS, ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'm keeping the all tweets in ne text to do word cloud\n\ntext = \" \".join(i for i in train.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text[0:1000] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = WordCloud(background_color='white').generate(text)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\n* count vectors\n* TF-IDF vectors(words, chracters, n-grams)\n\n\nTF (t) = (Frequency of a term in a document) / (total number of terms in a document)\n\nIDF (t) = log_e (Total number of documents) / (number of documents with t terms in it)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Count Vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\n\n# Target encoding\nfeatures = ['keyword', 'location']\nencoder = ce.TargetEncoder(cols=features)\nencoder.fit(train[features],train['target'])\n\ntrain = train.join(encoder.transform(train[features]).add_suffix('_target'))\ntest = test.join(encoder.transform(test[features]).add_suffix('_target'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# CountVectorizer\n\n# Links\nvec_links = CountVectorizer(min_df = 5, analyzer = 'word', token_pattern = r'https?://\\S+') # Only include those >=5 occurrences\nlink_vec = vec_links.fit_transform(train['link'])\nlink_vec_test = vec_links.transform(test['link'])\nX_train_link = pd.DataFrame(link_vec.toarray(), columns=vec_links.get_feature_names())\nX_test_link = pd.DataFrame(link_vec_test.toarray(), columns=vec_links.get_feature_names())\n\n# Tagged\nvec_tag = CountVectorizer(min_df = 5)\ntag_vec = vec_tag.fit_transform(train['tagged'])\ntag_vec_test = vec_tag.transform(test['tagged'])\nX_train_tag = pd.DataFrame(tag_vec.toarray(), columns=vec_tag.get_feature_names())\nX_test_tag = pd.DataFrame(tag_vec_test.toarray(), columns=vec_tag.get_feature_names())\n\n# Hashtags\nvec_hash = CountVectorizer(min_df = 5)\nhash_vec = vec_hash.fit_transform(train['hashtags'])\nhash_vec_test = vec_hash.transform(test['hashtags'])\nX_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\nX_test_hash = pd.DataFrame(hash_vec_test.toarray(), columns=vec_hash.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tf-idf for text\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \ntext_vec = vec_text.fit_transform(train['text'])\ntext_vec_test = vec_text.transform(test['text'])\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())\nprint (X_train_text.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.join(X_train_link, rsuffix='_link')\ntrain = train.join(X_train_tag, rsuffix='_tagged')\ntrain = train.join(X_train_hash, rsuffix='_hashtag')\ntrain = train.join(X_train_text, rsuffix='_text')\n\ntest = test.join(X_test_link, rsuffix='_link')\ntest = test.join(X_test_tag, rsuffix='_mention')\ntest = test.join(X_test_hash, rsuffix='_hashtag')\ntest = test.join(X_test_text, rsuffix='_text')\n\nprint (train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head() # as we can see the data has 1708 feature now","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train-Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_x,test_x,train_y,test_y = train_test_split(train.drop(columns = ['id', 'keyword', 'location', 'text', \n                                                                       'target', 'hashtags', 'tagged','link']),\n                                                 train['target'],test_size = 0.3) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn import linear_model\nlog = linear_model.LogisticRegression(solver='liblinear', random_state=777)\nlog_model = log.fit(train_x, train_y)\nlog_pred = log_model.predict(test_x)\naccuracy = model_selection.cross_val_score(log_model,\n                                          test_x,\n                                          test_y,\n                                          cv=10).mean()\n\nprint('Accuracy of Logistic Regression: ', accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(test_y,log_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Navie-Bayes "},{"metadata":{"trusted":true},"cell_type":"code","source":"nb= naive_bayes.MultinomialNB()\nnb_model = nb.fit(train_x, train_y)\nnb_pred = nb_model.predict(test_x)\naccuracy = model_selection.cross_val_score(nb_model,\n                                          test_x,\n                                          test_y,\n                                          cv=10).mean()\nprint('Accuracy of Naive-Bayes: ', accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(test_y,nb_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = ensemble.RandomForestClassifier()\nrf_model = rf.fit(train_x,train_y)\nrf_pred = rf.predict(test_x)\naccuracy = model_selection.cross_val_score(rf_model,\n                                          test_x,\n                                          test_y,\n                                          cv=10).mean()\nprint('Accuracy of Random Forest: ', accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(test_y,rf_pred) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nxgb = xgboost.XGBClassifier()\nxgb_model = xgb.fit(train_x,train_y)\nxgb_pred = xgb_model.predict(test_x)\naccuracy = model_selection.cross_val_score(xgb_model,\n                                          test_x,\n                                          test_y,\n                                          cv=10).mean()\nprint('Accuracy of XGBoost: ', accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(test_y,xgb_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = train_x.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.reindex(columns = columns, fill_value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nb_model = nb.fit(train.drop(columns = ['id', 'keyword', 'location', 'text','target', 'hashtags', 'tagged','link']),train['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = nb_model.predict(test)\nsubmission = pd.DataFrame({\"id\": Id, \"target\": pred})\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\nIf you like it please vote !"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}