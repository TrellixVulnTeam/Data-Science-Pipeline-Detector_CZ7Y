{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# make sure latest version of fastai is installed\n#!conda install -c pytorch -c fastai fastai --yes\n#!conda install -c anaconda nltk --yes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyenchant\n# Enchant needs libenchant to be installed\n! apt-get update\n! apt-get install libenchant-dev -y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport time\nfrom datetime import datetime\n\nfrom sklearn.model_selection import train_test_split\n\nfrom google.cloud import storage\nfrom google.cloud import automl_v1beta1 as automl\n# from google.cloud import automl\n\nfrom automlwrapper import AutoMLWrapper\n\nfrom fastai.text import *\nimport spacy\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk import word_tokenize\nfrom tqdm import tqdm\nimport enchant\nfrom nltk.metrics import edit_distance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regex and Spelling functions\nSource: https://github.com/japerk/nltk3-cookbook"},{"metadata":{"trusted":true},"cell_type":"code","source":"replacement_patterns = [\n    (r'won\\'t', 'will not'),\n    (r'can\\'t', 'cannot'),\n    (r'i\\'m', 'i am'),\n    (r'ain\\'t', 'is not'),\n    (r'(\\w+)\\'ll', '\\g<1> will'),\n    (r'(\\w+)n\\'t', '\\g<1> not'),\n    (r'(\\w+)\\'ve', '\\g<1> have'),\n    (r'(\\w+)\\'s', '\\g<1> is'),\n    (r'(\\w+)\\'re', '\\g<1> are'),\n    (r'(\\w+)\\'d', '\\g<1> would'),\n]\n\nclass RegexpReplacer(object):\n    # Replaces regular expression in a text.\n    def __init__(self, patterns=replacement_patterns):\n        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n    \n    def replace(self, text):\n        s = text\n        \n        for (pattern, repl) in self.patterns:\n            s = re.sub(pattern, repl, s)\n        \n        return s\n\nclass SpellingReplacer(object):\n    \"\"\" Replaces misspelled words with a likely suggestion based on shortest\n    edit distance\n    \"\"\"\n    def __init__(self, dict_name='en', max_dist=2):\n        self.spell_dict = enchant.Dict(dict_name)\n        self.max_dist = max_dist\n    \n    def replace(self, word):\n        if self.spell_dict.check(word):\n            return word\n        \n        suggestions = self.spell_dict.suggest(word)\n        \n        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n            return suggestions[0]\n        else:\n            return word","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = '/kaggle/input/'\noutput_path = '/kaggle/working/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# relabel incorrect tweets\nids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain_orig = pd.read_csv(input_path+'nlp-getting-started/train.csv', encoding = 'latin-1')\nincorrect = train_orig[train_orig['id'].isin(ids_with_target_error)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(input_path+'disasters-on-social-media/socialmedia-disaster-tweets-DFE.csv', encoding = 'latin-1')\ndf_train = df_train[['_unit_id','keyword','location','text','choose_one']]\n#now grab ids that were identified earlier as mislabelled\nincorrect_ids = incorrect.merge(df_train,on=['text'])['_unit_id']\ndf_train.rename(columns = {'_unit_id': 'id'}, inplace = True)\ndf_train['target'] = df_train['choose_one'].map({'Relevant': 1, 'Not Relevant': 0})\ndf_train.drop(['choose_one'], axis = 1, inplace = True)\ndf_train = df_train[-df_train['target'].isna()]\ndf_train = df_train.astype({'target': 'int64'})\n#reassign incorrect labels\ndf_train.loc[df_train['id'].isin(incorrect_ids),'target'] = 0\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text'][1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(input_path+'nlp-getting-started/test.csv')\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional training data\ncols = ['target','id','date','flag','user','text']\ntrain_add = pd.read_csv(input_path+'disastertweetsinput/train_clean_add.csv',names=cols,encoding = 'latin-1', skiprows=1)\ntrain_add.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_add = df_train[['id','text','target']].append(train_add[['id','text','target']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning text data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_tweet(text) :\n    # remove urls\n    #text = df.apply(lambda x: re.sub(r'http\\S+', '', x))\n    text = re.sub(r'http\\S+', '', text)\n\n    # replace contractions\n    replacer = RegexpReplacer()\n    text = replacer.replace(text)\n\n    #split words on - and \\\n    text = re.sub(r'\\b', ' ', text)\n    text = re.sub(r'-', ' ', text)\n\n    # replace negations with antonyms\n\n    #nltk.download('punkt')\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    tokens = tokenizer.tokenize(text)\n\n    # spelling correction\n    replacer = SpellingReplacer()\n    tokens = [replacer.replace(t) for t in tokens]\n\n    # lemmatize/stemming\n    wnl = nltk.WordNetLemmatizer()\n    tokens = [wnl.lemmatize(t) for t in tokens]\n    porter = nltk.PorterStemmer()\n    tokens = [porter.stem(t) for t in tokens]\n    # filter insignificant words (using fastai)\n    # swap word phrases\n\n    text = ' '.join(tokens)\n    return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = df_train['text']\ntqdm.pandas(desc=\"Cleaning tweets\")\ntweets_cleaned = tweets.progress_apply(clean_tweet)\ndf_train['text_clean'] = tweets_cleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = df_test['text']\ntweets_cleaned = tweets.progress_apply(clean_tweet)\ndf_test['text_clean'] = tweets_cleaned","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_test[['text_clean']].append(df_train[['text_clean']])\ndf = df.drop_duplicates().reset_index()['text_clean']\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#append text_token to df_train and df_test\ntrain = []\ntokenizer = Tokenizer()\ntok = SpacyTokenizer('en')\nfor line in tqdm(df_train.text_clean):\n    lne = ' '.join(tokenizer.process_text(line, tok))\n    train.append(lne)\n\ndf_train['text_tokens'] = train\n    \ntest = []\ntokenizer = Tokenizer()\ntok = SpacyTokenizer('en')\nfor line in tqdm(df_test.text_clean):\n    lne = ' '.join(tokenizer.process_text(line, tok))\n    test.append(lne)\n    \ndf_test['text_tokens'] = test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AutoML"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set your own values for these. bucket_name should be the project_id + '-lcm'.\nPROJECT_ID = 'kaggle-tweets-0234'\nbucket_name = 'kaggle-tweets-0234-lcm'\n\nregion = 'us-central1' # Region must be us-central1\ndataset_display_name = 'disaster_tweets'\nmodel_display_name = 'disaster_tweets_model1'\nstorage_client = storage.Client(project=PROJECT_ID)\nclient = automl.AutoMlClient()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO(developer): Uncomment and set the following variables\n# project_id = 'YOUR_PROJECT_ID'\n\n# A resource that represents Google Cloud Platform location.\nproject_location = client.location_path(PROJECT_ID, 'us-central1')\nresponse = client.list_models(project_location, '')\n\nprint('List of models:')\nfor model in response:\n    # Display the model information.\n    if model.deployment_state == \\\n            automl.enums.Model.DeploymentState.DEPLOYED:\n        deployment_state = 'deployed'\n    else:\n        deployment_state = 'undeployed'\n\n    print(u'Model name: {}'.format(model.name))\n    print(u'Model id: {}'.format(model.name.split('/')[-1]))\n    print(u'Model display name: {}'.format(model.display_name))\n    print(u'Model create time:')\n    print(u'\\tseconds: {}'.format(model.create_time.seconds))\n    print(u'\\tnanos: {}'.format(model.create_time.nanos))\n    print(u'Model deployment state: {}'.format(deployment_state))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nlp_train_df = pd.read_csv(\"/kaggle/input/tokenized-disaster-tweets/train_token.csv\")\nnlp_train_df =  df_train\n#nlp_test_df = pd.read_csv(\"/kaggle/input/tokenized-disaster-tweets/test_token.csv\")\nnlp_test_df =  df_test\ndef callback(operation_future):\n    result = operation_future.result()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_train_df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GCS upload/download utilities\nThese functions make upload and download of files from the kernel to Google Cloud Storage easier. This is needed for AutoML"},{"metadata":{"trusted":true},"cell_type":"code","source":"def upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https://cloud.google.com/storage/docs/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}'.format(\n        source_file_name,\n        'gs://' + bucket_name + '/' + destination_blob_name))\n    \ndef download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bucket = storage.Bucket(storage_client, name=bucket_name)\nif not bucket.exists():\n    bucket.create(location=region)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Export to CSV and upload to GCS"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the text body and the target value, for sending to AutoML NL\nnlp_train_df[['text_tokens','target']].to_csv('train.csv', index=False, header=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_train_df[['id','text_tokens','target']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_gcs_path = 'uploads/kaggle_getstarted/full_train.csv'\n#upload_blob(bucket_name, 'train.csv', training_gcs_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create our class instance"},{"metadata":{"trusted":true},"cell_type":"code","source":"amw = AutoMLWrapper(client=client, \n                    project_id=PROJECT_ID, \n                    bucket_name=bucket_name, \n                    region='us-central1', \n                    dataset_display_name=dataset_display_name, \n                    model_display_name=model_display_name)     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create (or retreive) dataset\nCheck to see if this dataset already exists. If not, create it"},{"metadata":{"trusted":true},"cell_type":"code","source":"if not amw.get_dataset_by_display_name(dataset_display_name):\n    print('dataset not found')\n    amw.create_dataset()\n    amw.import_gcs_data(training_gcs_path)\n\namw.dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kick off the training for the model\nAnd retrieve the training info after completion. \nStart model deployment."},{"metadata":{"trusted":true},"cell_type":"code","source":"if not amw.get_model_by_display_name():\n    amw.train_model()\n#amw.deploy_model()\namw.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amw.model_full_path","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction\nNote that prediction will not run until deployment finishes, which takes a bit of time.\nHowever, once you have your model deployed, this notebook won't re-train the model, thanks to the various safeguards put in place. Instead, it will take the existing (trained) model and make predictions and generate the submission file."},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create client for prediction service.\nprediction_client = automl.PredictionServiceClient()\namw.set_prediction_client(prediction_client)\n\npredictions_df = amw.get_predictions(nlp_test_df, \n                                     input_col_name='text_tokens', \n#                                    ground_truth_col_name='target', # we don't have ground truth in our test set\n                                     limit=None, \n                                     threshold=0.5,\n                                     verbose=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (optional) Undeploy model\nUndeploy the model to stop charges"},{"metadata":{"trusted":true},"cell_type":"code","source":"#amw.undeploy_model()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create submission output"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.concat([nlp_test_df['id'], predictions_df['class']], axis=1)\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions_df['class'].iloc[:10]\n# nlp_test_df['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = submission_df.rename(columns={'class':'target'})\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit predictions to the competition!"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls -l submission.csv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}