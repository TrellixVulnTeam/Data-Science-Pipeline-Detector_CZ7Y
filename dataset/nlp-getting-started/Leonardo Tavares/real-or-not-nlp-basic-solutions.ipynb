{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.xoriant.com/blog/wp-content/uploads/2020/01/NPL_Blog.png)"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Installation of auxiliary libraries\n!pip install missingno","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport missingno as msno\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# graphics import\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Natural language tool kits\nimport nltk\nfrom nltk import FreqDist, ngrams\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n# download stopwords\nnltk.download('stopwords')\n\n\n# string operations\nimport string \nimport re\n\n# sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import f1_score\n\nfrom yellowbrick.classifier import ClassificationReport\nfrom yellowbrick.classifier import ROCAUC\nfrom yellowbrick.classifier import PrecisionRecallCurve\nfrom yellowbrick.model_selection import FeatureImportances\nfrom yellowbrick.classifier import ConfusionMatrix\nfrom yellowbrick.text import FreqDistVisualizer\nfrom yellowbrick.text import TSNEVisualizer\nfrom yellowbrick.contrib.classifier import DecisionViz\nfrom yellowbrick.classifier import DiscriminationThreshold\n\n\nfrom lime.lime_text import LimeTextExplainer\n\n# Changing the number of characters displayed in pandas \npd.options.display.max_colwidth = 150\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Problem definition\n\n\nGiven a set of tweets (location + keyword + text) predict whether there is real disaster information or not. \n\n## Special thanks:\n\nPart of the code is based on the following work:\nhttps://www.kaggle.com/parulpandey/getting-started-with-nlp-a-general-intro\n\n**Thanks Parul Pandey!** \n\nAnd some part of the code is based on the following work:\nhttps://www.kaggle.com/datafan07/disaster-tweets-nlp-eda-bert-with-transformers\n\n**Thanks ErtuÄŸrul Demir!**\n\n## Special ask: please upvote if you like this solution!"},{"metadata":{},"cell_type":"markdown","source":"# 2. Analyze data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import database\ndf = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n# submission file\ndf_sub = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n# Displaying database's sample \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Data sctructure\n\n### Question 1: How is the data structured?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Question 2: How about missing values?"},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.matrix(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 3: what is the length of the tweets?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text_len'] = df['text'].str.len()\n\nplt.figure(figsize=(15,5))\nax = sns.distplot(df['text_len'])\nax.set(xlabel='Text length', ylabel='Freq.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 3.1 what is the length of the reviews per outcome?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nax = sns.distplot(df[df['target'] == 1]['text_len'])\nax = sns.distplot(df[df['target'] == 0]['text_len'])\nax.set(xlabel='Text length', ylabel='Freq.')\nfig.legend(labels=['Real','Fake'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 3.2 how many words does each Tweet have??"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['word_count'] = df['text'].str.split().map(lambda x:len(x))\n\nplt.figure(figsize=(15,5))\nax = sns.distplot(df['word_count'])\nax.set(xlabel='Word count', ylabel='Freq.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 3.2 how many words does each Tweet have per outcome?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nax = sns.distplot(df[df['target'] == 1]['word_count'])\nax = sns.distplot(df[df['target'] == 0]['word_count'])\nax.set(xlabel='Text length', ylabel='Freq.')\nfig.legend(labels=['Real','Fake'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 4: What are the 10 most frequent words?"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\ndocs       = vectorizer.fit_transform(df['text'].tolist())\nfeatures   = vectorizer.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features, orient='h', n=10)\nvisualizer.fit(docs)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 4.1: and about bigrams?"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(ngram_range=(2, 2))\ndocs       = vectorizer.fit_transform(df['text'].tolist())\nfeatures   = vectorizer.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features, orient='h', n=10)\nvisualizer.fit(docs)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 5: Outcome distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(15, 8))\nsns.barplot(df['target'].value_counts().index,df['target'].value_counts(), ax=axes[0])\n\naxes[1].pie(df['target'].value_counts(),\n            autopct='%1.2f%%',\n            explode=(0.05, 0),\n            startangle=60)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question 6: Let's see the word cloud per outcome"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_real = df[df['target']==1]['text']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_real))\n\nplt.figure(1,figsize=(15, 15))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_fake = df[df['target']==0]['text']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_fake))\n\nplt.figure(1,figsize=(15, 15))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Prepare data"},{"metadata":{},"cell_type":"markdown","source":"## Step 1: I'll use just text and outcome..."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[['text', 'target']]\n# applying in submission dataset\ndf_sub = df_sub[['id','text']]\n\n\ndf.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: transform text to lowercase"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text_lw'] = df['text'].str.lower()\n# applying in submission dataset\ndf_sub['text_lw'] = df_sub['text'].str.lower()\n\n\ndf[['text','text_lw']].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: remove useless items from text (html, links, pontuation, numbers, stopwords, ...)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    # remove \n    text = re.sub('\\[.*?\\]', '', text)\n    # remove links\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    # remove tags\n    text = re.sub('<.*?>+', '', text)\n    # remove punctuation\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # remove breaklines\n    text = re.sub('\\n', '', text)\n    # remove numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    # transform text into token\n    text_token = nltk.word_tokenize(text)\n    \n    # remove stopwords\n    words = [w for w in text_token if w not in stopwords.words('english')]\n    \n    \n    return ' '.join(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text_cl'] = df['text_lw'].apply(clean_text)\n\n# applying in submission dataset\ndf_sub['text_cl'] = df_sub['text_lw'].apply(clean_text)\n\n\ndf[['text','text_lw','text_cl']].head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 4: Text normalization\n\nref: https://medium.com/@gaurav5430/using-nltk-for-lemmatizing-sentences-c1bfff963258"},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer() \n\n# function to convert nltk tag to wordnet tag\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:          \n        return None\n\ndef lemmatize_sentence(sentence):\n    #tokenize the sentence and find the POS tag for each token\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n    #tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n    lemmatized_sentence = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            #if there is no available tag, append the token as is\n            if len(word) > 2:\n                lemmatized_sentence.append(word)\n        else:        \n            #else use the tag to lemmatize the token\n            lemma = lemmatizer.lemmatize(word, tag)\n            if len(lemma) > 2:\n                lemmatized_sentence.append(lemma)\n    return \" \".join(lemmatized_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text_lm'] = df['text_cl'].apply(lemmatize_sentence)\n\n# applying in submission dataset\ndf_sub['text_lm'] = df_sub['text_cl'].apply(lemmatize_sentence)\n\n\ndf[['text','text_lw','text_cl', 'text_lm']].head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's see some charts again"},{"metadata":{},"cell_type":"markdown","source":"### text length"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\n\ndf['text_len'] = df['text'].str.len()\nplt.figure(figsize=(15,5))\naxes[0].set_title('Before transformation')\nax=sns.distplot(df['text_len'], ax=axes[0])\naxes[0].set(xlabel='Text length', ylabel='Freq.')\n\ndf['text_len'] = df['text_lm'].str.len()\nplt.figure(figsize=(15,5))\naxes[1].set_title('After transformation')\nsns.distplot(df['text_len'], ax=axes[1])\naxes[1].set(xlabel='Text length', ylabel='Freq.')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### length per outcome"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\ndf['text_len'] = df['text'].str.len()\naxes[0].set_title('Before transformation')\nax = sns.distplot(df[df['target'] == 1]['text_len'], ax=axes[0])\nax = sns.distplot(df[df['target'] == 0]['text_len'], ax=axes[0])\nax.set(xlabel='Text length', ylabel='Freq.')\n\ndf['text_len'] = df['text_lm'].str.len()\naxes[1].set_title('After transformation')\nax = sns.distplot(df[df['target'] == 1]['text_len'], ax=axes[1])\nax = sns.distplot(df[df['target'] == 0]['text_len'], ax=axes[1])\nax.set(xlabel='Text length', ylabel='Freq.')\n\n\nfig.legend(labels=['Real','Fake'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## how many words"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\ndf['word_count'] = df['text'].str.split().map(lambda x:len(x))\naxes[0].set_title('Before transformation')\nplt.figure(figsize=(15,5))\nax = sns.distplot(df['word_count'], ax=axes[0])\nax.set(xlabel='Word count', ylabel='Freq.')\n\ndf['word_count'] = df['text_lm'].str.split().map(lambda x:len(x))\naxes[1].set_title('After transformation')\nplt.figure(figsize=(15,5))\nax = sns.distplot(df['word_count'], ax=axes[1])\nax.set(xlabel='Word count', ylabel='Freq.')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## how many words per outcome"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n\ndf['word_count'] = df['text'].str.split().map(lambda x:len(x))\naxes[0].set_title('Before transformation')\nax = sns.distplot(df[df['target'] == 1]['word_count'], ax=axes[0])\nax = sns.distplot(df[df['target'] == 0]['word_count'], ax=axes[0])\nax.set(xlabel='Text length', ylabel='Freq.')\nfig.legend(labels=['Real','Fake'])\n\ndf['word_count'] = df['text_lm'].str.split().map(lambda x:len(x))\naxes[1].set_title('After transformation')\nax = sns.distplot(df[df['target'] == 1]['word_count'], ax=axes[1])\nax = sns.distplot(df[df['target'] == 0]['word_count'], ax=axes[1])\nax.set(xlabel='Text length', ylabel='Freq.')\nfig.legend(labels=['Real','Fake'])\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Most frequent words"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer()\ndocs       = vectorizer.fit_transform(df['text_lm'].tolist())\nfeatures   = vectorizer.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features, orient='h', n=10)\nvisualizer.fit(docs)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### And bigrams?"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(ngram_range=(2, 2))\ndocs       = vectorizer.fit_transform(df['text_lm'].tolist())\nfeatures   = vectorizer.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features, orient='h', n=10)\nvisualizer.fit(docs)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word cloud...\n### Real"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=[20, 10])\n\ndf_real = df[df['target']==1]['text']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_real))\n\naxes[0].imshow(wordcloud1)\naxes[0].axis('off')\naxes[0].set_title('Before transformation')\n\ndf_real = df[df['target']==1]['text_lm']\n\nwordcloud2 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_real))\n\naxes[1].imshow(wordcloud2)\naxes[1].axis('off')\naxes[1].set_title('After transformation')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fake"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=[20, 10])\n\ndf_real = df[df['target']==0]['text']\n\nwordcloud1 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_real))\n\naxes[0].imshow(wordcloud1)\naxes[0].axis('off')\naxes[0].set_title('Before transformation')\n\ndf_real = df[df['target']==0]['text_lm']\n\nwordcloud2 = WordCloud(stopwords=STOPWORDS,\n                      background_color='white',\n                      width=2500,\n                      height=2000\n                      ).generate(\" \".join(df_real))\n\naxes[1].imshow(wordcloud2)\naxes[1].axis('off')\naxes[1].set_title('After transformation')\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Evaluate algorithms"},{"metadata":{},"cell_type":"markdown","source":"### Step 0: Create train and test dataset (80% = train / 20% = test,stratified by the outcome)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df[['text_lm']], df['target'], test_size=0.20, random_state=42, stratify=df['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying TF-IDF vectoring"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nX_train_vec = vectorizer.fit_transform(X_train['text_lm'])\nX_test_vec = vectorizer.transform(X_test['text_lm'])\n\nX_sub = vectorizer.transform(df_sub['text_lm'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some tf-idf examples..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform into dataframe\nX_train_vec_df = pd.DataFrame(columns=vectorizer.get_feature_names(), data=X_train_vec.toarray())\nX_test_vec_df = pd.DataFrame(columns=vectorizer.get_feature_names(), data=X_test_vec.toarray())\n\nX_sub_vec_df = pd.DataFrame(columns=vectorizer.get_feature_names(), data=X_sub.toarray())\n\n# some sample\nX_train_vec_df[(X_train_vec_df['get']>0) | (X_train_vec_df['like'] > 0) | (X_train_vec_df['fire'] > 0)][['get', 'like', 'fire']].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How outcome are distributed..."},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNEVisualizer()\ntsne.fit(X_train_vec_df, y_train)\ntsne.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: create a dummy baseline model"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DummyClassifier().fit(X_train_vec_df, y_train)\ny_pred = clf.predict(X_test_vec_df)\nvisualizer = ConfusionMatrix(clf, percent=True)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()\n\nprint(f'Acc = {accuracy_score(y_test, y_pred)}')\nprint(f'F1 = {f1_score(y_test, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ROC-AUC curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ROCAUC(clf)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PR Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = PrecisionRecallCurve(clf)\nvisualizer.fit(X_test_vec_df, y_test)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ClassificationReport(clf)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 3: create a basic predictive model (naive bayes)"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = MultinomialNB().fit(X_train_vec_df, y_train)\nvisualizer = ConfusionMatrix(clf, percent=True)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()\n\ny_pred = clf.predict(X_test_vec_df)\nprint(f'Acc = {accuracy_score(y_test, y_pred)}')\nprint(f'F1 = {f1_score(y_test, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ROC-AUC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ROCAUC(clf)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PR Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = PrecisionRecallCurve(clf)\nvisualizer.fit(X_train_vec_df, y_train)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ClassificationReport(clf)\nvisualizer.score(X_test_vec_df, y_test)\nvisualizer.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explain predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"c = make_pipeline(vectorizer, clf)\nexplainer = LimeTextExplainer(class_names=['Fake', 'Real'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Real instance"},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 2\n\ns = X_test.iloc[idx]['text_lm']\nprint('Original:', df.loc[7515]['text'])\nprint('Treated:', s)\n\nprint(f'Outcome {y_test.iloc[idx]}')\n\n\nexp = explainer.explain_instance(s, c.predict_proba)\nexp.show_in_notebook()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fake instance"},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 3\n\ns = X_test.iloc[idx]['text_lm']\nprint('Original:', df.loc[1294]['text'])\nprint('Treated:', s)\n\nprint(f'Outcome {y_test.iloc[idx]}')\n\n\nexp = explainer.explain_instance(s, c.predict_proba)\nexp.show_in_notebook()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Discrimination Threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = DiscriminationThreshold(clf)\nvisualizer.fit(X_test_vec_df, y_test)\nvisualizer.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adjust threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"thr = 0.40\n\nbest_f1 = 0\nbest_thr = 0\n\ny_pred_proba = clf.predict_proba(X_test_vec_df)[:, 1]\ny_pred = [1 if y >= thr else 0 for y in y_pred_proba]\n\n\nfor t in np.linspace(0, 1, 101):\n    new_y_pred = [1 if y >= t else 0 for y in y_pred_proba]    \n    f1 = f1_score(y_test, new_y_pred)\n    if f1 > best_f1:\n        best_f1 = f1\n        best_thr = t\n\nnew_y_pred = [1 if y >= best_thr else 0 for y in y_pred_proba]\n        \nprint(f'Acc after= {accuracy_score(y_test, new_y_pred)}')\nprint(f'F1 after = {f1_score(y_test, new_y_pred)}')\nprint(f'Best thr = {best_thr}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Submit results"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_proba = clf.predict_proba(X_sub_vec_df)[:, 1]\ny_pred = [1 if y >= best_thr else 0 for y in y_pred_proba]\n\ndf_sub['target'] = y_pred\ndf_sub = df_sub[['id', 'target']]\ndf_sub.to_csv('submission.csv', index=False, header=True)\ndf_sub.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Special ask again: please upvote if you like this solution!\n\n![That's all folks!!!](https://i.pinimg.com/originals/2c/2e/ef/2c2eef8da1285d958914eef079f9b70c.jpg)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}