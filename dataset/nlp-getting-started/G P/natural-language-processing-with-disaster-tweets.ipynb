{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# keras embeddings technique","metadata":{}},{"cell_type":"markdown","source":"# Import packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:28:46.973331Z","iopub.execute_input":"2021-09-17T18:28:46.973713Z","iopub.status.idle":"2021-09-17T18:28:46.996132Z","shell.execute_reply.started":"2021-09-17T18:28:46.973623Z","shell.execute_reply":"2021-09-17T18:28:46.99508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import packages","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize, word_tokenize","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:28:46.997769Z","iopub.execute_input":"2021-09-17T18:28:46.998533Z","iopub.status.idle":"2021-09-17T18:28:48.651239Z","shell.execute_reply.started":"2021-09-17T18:28:46.998494Z","shell.execute_reply":"2021-09-17T18:28:48.650142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\ndf = pd.read_csv('../input/nlp-getting-started/train.csv',encoding=\"latin_1\")\ndf_test = pd.read_csv('../input/nlp-getting-started/test.csv',encoding=\"latin_1\")\ndata1=pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:28:48.652618Z","iopub.execute_input":"2021-09-17T18:28:48.652851Z","iopub.status.idle":"2021-09-17T18:28:48.757582Z","shell.execute_reply.started":"2021-09-17T18:28:48.652825Z","shell.execute_reply":"2021-09-17T18:28:48.756961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texts = df['text']\ntexts_test = df_test['text']\ny = df.pop('target')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:28:48.758676Z","iopub.execute_input":"2021-09-17T18:28:48.759081Z","iopub.status.idle":"2021-09-17T18:28:48.772493Z","shell.execute_reply.started":"2021-09-17T18:28:48.759007Z","shell.execute_reply":"2021-09-17T18:28:48.77154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocesing","metadata":{}},{"cell_type":"code","source":"def pre_process_data(text):\n    lemm = WordNetLemmatizer()\n    text  = re.sub(r\"[^0-9a-zA-Z]+\",' ',text)\n    tokenized = word_tokenize(text)\n    text = [lemm.lemmatize(i.lower()) for i in tokenized if not(i.lower() in stop_words) and i.isalpha()]\n    text = [i.replace('http','') for i in text]\n    text = [i.replace('co','') for i in text]\n    text = [i.replace('amp','') for i in text]\n    return ' '.join(text)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:28:48.775539Z","iopub.execute_input":"2021-09-17T18:28:48.776074Z","iopub.status.idle":"2021-09-17T18:28:48.787838Z","shell.execute_reply.started":"2021-09-17T18:28:48.775987Z","shell.execute_reply":"2021-09-17T18:28:48.787096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ntexts_test = [pre_process_data(i) for i in texts_test]\ntexts = [pre_process_data(i) for i in texts]","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:28:48.789074Z","iopub.execute_input":"2021-09-17T18:28:48.789464Z","iopub.status.idle":"2021-09-17T18:28:55.009893Z","shell.execute_reply.started":"2021-09-17T18:28:48.789409Z","shell.execute_reply":"2021-09-17T18:28:55.008927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels   = data1['target'].values.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:28:55.011185Z","iopub.execute_input":"2021-09-17T18:28:55.012073Z","iopub.status.idle":"2021-09-17T18:28:55.018612Z","shell.execute_reply.started":"2021-09-17T18:28:55.012011Z","shell.execute_reply":"2021-09-17T18:28:55.017501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras_preprocessing\nfrom keras_preprocessing.text import one_hot\nfrom keras_preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:28:55.020483Z","iopub.execute_input":"2021-09-17T18:28:55.020743Z","iopub.status.idle":"2021-09-17T18:28:55.039546Z","shell.execute_reply.started":"2021-09-17T18:28:55.020712Z","shell.execute_reply":"2021-09-17T18:28:55.038519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# integer encode the documents\nvocab_size = 10000\nencoded_docs = [one_hot(d, vocab_size) for d in texts ]\n","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:28:55.041002Z","iopub.execute_input":"2021-09-17T18:28:55.041352Z","iopub.status.idle":"2021-09-17T18:28:55.156054Z","shell.execute_reply.started":"2021-09-17T18:28:55.041321Z","shell.execute_reply":"2021-09-17T18:28:55.155136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 1000\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:28:55.157573Z","iopub.execute_input":"2021-09-17T18:28:55.157818Z","iopub.status.idle":"2021-09-17T18:28:55.238736Z","shell.execute_reply.started":"2021-09-17T18:28:55.157789Z","shell.execute_reply":"2021-09-17T18:28:55.237596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:28:55.240445Z","iopub.execute_input":"2021-09-17T18:28:55.240771Z","iopub.status.idle":"2021-09-17T18:29:01.578457Z","shell.execute_reply.started":"2021-09-17T18:28:55.240728Z","shell.execute_reply":"2021-09-17T18:29:01.57734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:01.579796Z","iopub.execute_input":"2021-09-17T18:29:01.580185Z","iopub.status.idle":"2021-09-17T18:29:01.585578Z","shell.execute_reply.started":"2021-09-17T18:29:01.58015Z","shell.execute_reply":"2021-09-17T18:29:01.584294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the model","metadata":{}},{"cell_type":"code","source":"# define the model\nmodel = tf.keras.Sequential()\nmodel.add(layers.Embedding(vocab_size, 10, input_length=max_length))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(1, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:01.586837Z","iopub.execute_input":"2021-09-17T18:29:01.587175Z","iopub.status.idle":"2021-09-17T18:29:01.714381Z","shell.execute_reply.started":"2021-09-17T18:29:01.58713Z","shell.execute_reply":"2021-09-17T18:29:01.713334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting with ndarray\nlabels=np.array(labels) \n#data=np.array(labels,dtype=float)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:01.717576Z","iopub.execute_input":"2021-09-17T18:29:01.717856Z","iopub.status.idle":"2021-09-17T18:29:01.728176Z","shell.execute_reply.started":"2021-09-17T18:29:01.717826Z","shell.execute_reply":"2021-09-17T18:29:01.727005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compile the model","metadata":{}},{"cell_type":"code","source":"# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# summarize the model\nmodel.summary()\n# fit the model\nbatch_size = 32\nepochs = 50\nhistory = model.fit(padded_docs, labels,batch_size=batch_size,epochs=epochs)\nhistory","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:01.72926Z","iopub.execute_input":"2021-09-17T18:29:01.729497Z","iopub.status.idle":"2021-09-17T18:29:48.091475Z","shell.execute_reply.started":"2021-09-17T18:29:01.72947Z","shell.execute_reply":"2021-09-17T18:29:48.090438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting Graphs for accuracy","metadata":{}},{"cell_type":"code","source":"plt.figure(0)\nplt.plot(history.history['accuracy'], label='training accuracy')\nplt.plot(history.history['loss'], label='Value Loss')\nplt.title('Training accuracy & Value Loss')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.092645Z","iopub.execute_input":"2021-09-17T18:29:48.093245Z","iopub.status.idle":"2021-09-17T18:29:48.312382Z","shell.execute_reply.started":"2021-09-17T18:29:48.093207Z","shell.execute_reply":"2021-09-17T18:29:48.311331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saveing model","metadata":{}},{"cell_type":"code","source":"model.save(\"Natural Language Processing with Disaster Tweets.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.313517Z","iopub.execute_input":"2021-09-17T18:29:48.31375Z","iopub.status.idle":"2021-09-17T18:29:48.348143Z","shell.execute_reply.started":"2021-09-17T18:29:48.313723Z","shell.execute_reply":"2021-09-17T18:29:48.347132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load model","metadata":{}},{"cell_type":"code","source":"#from keras.models import load_model\n#model = load_model(./Natural Language Processing with Disaster Tweets.h5')\nmodel = tf.keras.models.load_model(\"./Natural Language Processing with Disaster Tweets.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.349651Z","iopub.execute_input":"2021-09-17T18:29:48.349903Z","iopub.status.idle":"2021-09-17T18:29:48.417068Z","shell.execute_reply.started":"2021-09-17T18:29:48.349875Z","shell.execute_reply":"2021-09-17T18:29:48.415975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testinng Data","metadata":{}},{"cell_type":"code","source":"texts_test","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.418512Z","iopub.execute_input":"2021-09-17T18:29:48.419585Z","iopub.status.idle":"2021-09-17T18:29:48.450103Z","shell.execute_reply.started":"2021-09-17T18:29:48.419538Z","shell.execute_reply":"2021-09-17T18:29:48.449159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# integer encode the documents\nvocab_size = 10000\nencoded_docs_test = [one_hot(d, vocab_size) for d in texts_test ]","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.451322Z","iopub.execute_input":"2021-09-17T18:29:48.451561Z","iopub.status.idle":"2021-09-17T18:29:48.509101Z","shell.execute_reply.started":"2021-09-17T18:29:48.451533Z","shell.execute_reply":"2021-09-17T18:29:48.508209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 1000\npadded_docs = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.510461Z","iopub.execute_input":"2021-09-17T18:29:48.510776Z","iopub.status.idle":"2021-09-17T18:29:48.548247Z","shell.execute_reply.started":"2021-09-17T18:29:48.510734Z","shell.execute_reply":"2021-09-17T18:29:48.547181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padded_docs","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.549325Z","iopub.execute_input":"2021-09-17T18:29:48.54958Z","iopub.status.idle":"2021-09-17T18:29:48.556009Z","shell.execute_reply.started":"2021-09-17T18:29:48.549549Z","shell.execute_reply":"2021-09-17T18:29:48.555151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict_classes(padded_docs)","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.557116Z","iopub.execute_input":"2021-09-17T18:29:48.557337Z","iopub.status.idle":"2021-09-17T18:29:48.824128Z","shell.execute_reply.started":"2021-09-17T18:29:48.557311Z","shell.execute_reply":"2021-09-17T18:29:48.823182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.825355Z","iopub.execute_input":"2021-09-17T18:29:48.825599Z","iopub.status.idle":"2021-09-17T18:29:48.831512Z","shell.execute_reply.started":"2021-09-17T18:29:48.825573Z","shell.execute_reply":"2021-09-17T18:29:48.830542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# My submission","metadata":{}},{"cell_type":"code","source":"sample = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nmy_submission = pd.DataFrame()\nmy_submission['id'] = sample['id']\nmy_submission['target'] = pred\n\nmy_submission.to_csv('Submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.83283Z","iopub.execute_input":"2021-09-17T18:29:48.833175Z","iopub.status.idle":"2021-09-17T18:29:48.870451Z","shell.execute_reply.started":"2021-09-17T18:29:48.833134Z","shell.execute_reply":"2021-09-17T18:29:48.869466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing WordCloud","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, ImageColorGenerator\nfrom os import path, getcwd\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.871618Z","iopub.execute_input":"2021-09-17T18:29:48.871836Z","iopub.status.idle":"2021-09-17T18:29:48.910857Z","shell.execute_reply.started":"2021-09-17T18:29:48.871808Z","shell.execute_reply":"2021-09-17T18:29:48.909903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.91237Z","iopub.execute_input":"2021-09-17T18:29:48.912712Z","iopub.status.idle":"2021-09-17T18:29:48.941965Z","shell.execute_reply.started":"2021-09-17T18:29:48.912669Z","shell.execute_reply":"2021-09-17T18:29:48.94106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating the text variable for positve reviews\nneg=data.loc[data['target']==0].reset_index(drop=True)\nneg.head()\n\n# Adding Text to a Variable\ntext=neg['text'][5]\n# Creating the Word Cloud\nwordcloud = WordCloud().generate(text)\n# Plotting the Word Cloud\nplt.figure(figsize = (20,20))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:48.943137Z","iopub.execute_input":"2021-09-17T18:29:48.943379Z","iopub.status.idle":"2021-09-17T18:29:49.211262Z","shell.execute_reply.started":"2021-09-17T18:29:48.943349Z","shell.execute_reply":"2021-09-17T18:29:49.210357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating the text variable for positve reviews\npos=data.loc[data['target']==1].reset_index(drop=True)\npos.head()\n\n# Adding Text to a Variable\ntext=pos['text'][5]\n# Creating the Word Cloud\nwordcloud = WordCloud().generate(text)\n# Plotting the Word Cloud\nplt.figure(figsize = (20,20))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-17T18:29:49.212522Z","iopub.execute_input":"2021-09-17T18:29:49.212787Z","iopub.status.idle":"2021-09-17T18:29:49.542847Z","shell.execute_reply.started":"2021-09-17T18:29:49.212756Z","shell.execute_reply":"2021-09-17T18:29:49.54219Z"},"trusted":true},"execution_count":null,"outputs":[]}]}