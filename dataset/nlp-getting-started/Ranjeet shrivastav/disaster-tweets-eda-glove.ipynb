{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport re\nimport nltk\n\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:16:14.772682Z","iopub.execute_input":"2021-08-24T16:16:14.773554Z","iopub.status.idle":"2021-08-24T16:16:14.790947Z","shell.execute_reply.started":"2021-08-24T16:16:14.773507Z","shell.execute_reply":"2021-08-24T16:16:14.789838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(r'../input/nlp-getting-started/train.csv')\ntest = pd.read_csv(r'../input/nlp-getting-started/test.csv')\nsub = pd.read_csv(r'../input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:42.602107Z","iopub.execute_input":"2021-08-24T16:26:42.6025Z","iopub.status.idle":"2021-08-24T16:26:42.641457Z","shell.execute_reply.started":"2021-08-24T16:26:42.60246Z","shell.execute_reply":"2021-08-24T16:26:42.640764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth',200)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:43.884394Z","iopub.execute_input":"2021-08-24T16:26:43.884729Z","iopub.status.idle":"2021-08-24T16:26:43.888282Z","shell.execute_reply.started":"2021-08-24T16:26:43.884687Z","shell.execute_reply":"2021-08-24T16:26:43.88747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:44.962611Z","iopub.execute_input":"2021-08-24T16:26:44.962963Z","iopub.status.idle":"2021-08-24T16:26:44.973541Z","shell.execute_reply.started":"2021-08-24T16:26:44.962931Z","shell.execute_reply":"2021-08-24T16:26:44.972671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, test.shape, sub.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:45.97501Z","iopub.execute_input":"2021-08-24T16:26:45.975327Z","iopub.status.idle":"2021-08-24T16:26:45.982688Z","shell.execute_reply.started":"2021-08-24T16:26:45.9753Z","shell.execute_reply":"2021-08-24T16:26:45.981692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:46.903326Z","iopub.execute_input":"2021-08-24T16:26:46.90364Z","iopub.status.idle":"2021-08-24T16:26:46.915784Z","shell.execute_reply.started":"2021-08-24T16:26:46.903609Z","shell.execute_reply":"2021-08-24T16:26:46.9148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:47.623231Z","iopub.execute_input":"2021-08-24T16:26:47.62356Z","iopub.status.idle":"2021-08-24T16:26:47.631511Z","shell.execute_reply.started":"2021-08-24T16:26:47.623523Z","shell.execute_reply":"2021-08-24T16:26:47.630584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(['id','keyword','location'],axis=1,inplace=True)\ntest.drop(['id','keyword','location'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:48.392692Z","iopub.execute_input":"2021-08-24T16:26:48.393055Z","iopub.status.idle":"2021-08-24T16:26:48.399665Z","shell.execute_reply.started":"2021-08-24T16:26:48.393023Z","shell.execute_reply":"2021-08-24T16:26:48.398822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s check out a few disaster related text .","metadata":{}},{"cell_type":"code","source":"train[train['target']==1].head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:49.703496Z","iopub.execute_input":"2021-08-24T16:26:49.703841Z","iopub.status.idle":"2021-08-24T16:26:49.714731Z","shell.execute_reply.started":"2021-08-24T16:26:49.703809Z","shell.execute_reply":"2021-08-24T16:26:49.713809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let’s check out a few normal text.","metadata":{}},{"cell_type":"code","source":"train[train['target']==0].head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:50.812055Z","iopub.execute_input":"2021-08-24T16:26:50.812389Z","iopub.status.idle":"2021-08-24T16:26:50.826444Z","shell.execute_reply.started":"2021-08-24T16:26:50.812359Z","shell.execute_reply":"2021-08-24T16:26:50.825556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are quite a many words and characters which are not really required. So, we will try to keep only those words which are important and add value.","metadata":{}},{"cell_type":"markdown","source":"Let’s have a glimpse at target-distribution in the train dataset.","metadata":{}},{"cell_type":"code","source":"train['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:52.683149Z","iopub.execute_input":"2021-08-24T16:26:52.683456Z","iopub.status.idle":"2021-08-24T16:26:52.690441Z","shell.execute_reply.started":"2021-08-24T16:26:52.683429Z","shell.execute_reply":"2021-08-24T16:26:52.689492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will check the distribution of length of the tweets, in terms of words, in both train and test data.","metadata":{}},{"cell_type":"code","source":"length_train = train['text'].str.len()\nlength_test = test['text'].str.len()\nplt.hist(length_train,bins=20,label = 'train_text')\nplt.hist(length_test,bins=20,label='test_text')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:54.14203Z","iopub.execute_input":"2021-08-24T16:26:54.142355Z","iopub.status.idle":"2021-08-24T16:26:54.492279Z","shell.execute_reply.started":"2021-08-24T16:26:54.142324Z","shell.execute_reply":"2021-08-24T16:26:54.491454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Cleaning**\n\nIn any natural language processing task, cleaning raw text data is an important step. It helps in getting rid of the unwanted words and characters which helps in obtaining better features. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don’t carry much weightage in context to the text.","metadata":{}},{"cell_type":"markdown","source":"Given below is a user-defined function to remove unwanted text patterns from the text.","metadata":{}},{"cell_type":"code","source":"def remove_pattern(input_txt,pattern):\n    r=re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i,'',input_txt)\n    return input_txt","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:56.85216Z","iopub.execute_input":"2021-08-24T16:26:56.852478Z","iopub.status.idle":"2021-08-24T16:26:56.859544Z","shell.execute_reply.started":"2021-08-24T16:26:56.852447Z","shell.execute_reply":"2021-08-24T16:26:56.858793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing Twitter handles (@user)** ","metadata":{}},{"cell_type":"code","source":"train['text'] = np.vectorize(remove_pattern)(train['text'],'@[/w]*')\ntest['text'] = np.vectorize(remove_pattern)(test['text'],'@[/w]*')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:26:58.27253Z","iopub.execute_input":"2021-08-24T16:26:58.272863Z","iopub.status.idle":"2021-08-24T16:26:58.311504Z","shell.execute_reply.started":"2021-08-24T16:26:58.272833Z","shell.execute_reply":"2021-08-24T16:26:58.310791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing Punctuations, Numbers, and Special Characters**","metadata":{}},{"cell_type":"markdown","source":"Here we will replace everything except characters and hashtags with spaces. The regular expression “[^a-zA-Z#]” means anything except alphabets and ‘#’.","metadata":{}},{"cell_type":"code","source":"train['text'] = train['text'].str.replace('[^a-zA-Z#]',' ')\ntest['text'] = test['text'].str.replace('[^a-zA-Z#]',' ')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:00.783055Z","iopub.execute_input":"2021-08-24T16:27:00.783406Z","iopub.status.idle":"2021-08-24T16:27:00.887841Z","shell.execute_reply.started":"2021-08-24T16:27:00.783377Z","shell.execute_reply":"2021-08-24T16:27:00.887047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Removing Short Words**\n\nWe have to be a little careful here in selecting the length of the words which we want to remove. So, I have decided to remove all the words having length 2 or less. For example, terms like “hmm”, “oh” are of very little use. It is better to get rid of them.","metadata":{}},{"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: ' '.join([w for w in x.split()\n                                                                 if len(w)>2]))\ntest['text'] = test['text'].apply(lambda x: ' '.join([w for w in x.split()\n                                                                 if len(w)>2]))\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:03.492887Z","iopub.execute_input":"2021-08-24T16:27:03.493193Z","iopub.status.idle":"2021-08-24T16:27:03.547353Z","shell.execute_reply.started":"2021-08-24T16:27:03.493164Z","shell.execute_reply":"2021-08-24T16:27:03.546403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Understanding the common words used in the texts: WordCloud**\n\nNow I want to see how well the given sentiments are distributed across the train dataset. One way to accomplish this task is by understanding the common words by plotting wordclouds.\n\nA wordcloud is a visualization where in the most frequent words appear in large size and the less frequent words appear in smaller sizes.\n\nLet’s visualize all the words our data using the wordcloud plot.","metadata":{}},{"cell_type":"code","source":"all_words = ' '.join([text for text in train['text']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height = 500, max_font_size=110).generate(all_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:05.142825Z","iopub.execute_input":"2021-08-24T16:27:05.143944Z","iopub.status.idle":"2021-08-24T16:27:06.849397Z","shell.execute_reply.started":"2021-08-24T16:27:05.143893Z","shell.execute_reply":"2021-08-24T16:27:06.848385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"normal_words = ' '.join([text for text in train['text'][train['target']==0]])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height = 500, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:06.85125Z","iopub.execute_input":"2021-08-24T16:27:06.851792Z","iopub.status.idle":"2021-08-24T16:27:08.020587Z","shell.execute_reply.started":"2021-08-24T16:27:06.851755Z","shell.execute_reply":"2021-08-24T16:27:08.019602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_related_words = ' '.join([text for text in train['text'][train['target']==1]])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height = 500, max_font_size=110).generate(disaster_related_words)\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:08.022214Z","iopub.execute_input":"2021-08-24T16:27:08.022605Z","iopub.status.idle":"2021-08-24T16:27:09.139835Z","shell.execute_reply.started":"2021-08-24T16:27:08.022504Z","shell.execute_reply":"2021-08-24T16:27:09.138843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Understanding the impact of Hashtags on texts sentiment**\n\nHashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time. We should try to check whether these hashtags add any value in this task.","metadata":{}},{"cell_type":"code","source":"# function to collect hashtags\ndef hashtag_extract(x):   \n            hashtags = []    \n\n            # Loop over the words in the tweet    \n            for i in x:        \n                ht = re.findall(r\"#(\\w+)\", i)        \n                hashtags.append(ht)     \n            return hashtags\n\n# extracting hashtags from non racist/sexist tweets \nHT_regular = hashtag_extract(train['text'][train['target'] == 0]) \n\n# extracting hashtags from racist/sexist tweets \nHT_negative = hashtag_extract(train['text'][train['target'] == 1]) \n\n# unnesting list \nHT_regular = sum(HT_regular,[])\nHT_negative = sum(HT_negative,[])","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:09.142115Z","iopub.execute_input":"2021-08-24T16:27:09.142447Z","iopub.status.idle":"2021-08-24T16:27:09.181618Z","shell.execute_reply.started":"2021-08-24T16:27:09.142414Z","shell.execute_reply":"2021-08-24T16:27:09.180939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = nltk.FreqDist(HT_regular)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                 'Count': list(a.values())})\n\n#selecting top 20 most frequent hashtags\nd = d.nlargest(columns = 'Count',n=20)\nplt.figure(figsize=(16,5))\nax = sns.barplot(data = d, x='Hashtag', y='Count')\nax.set(ylabel = 'Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:09.183276Z","iopub.execute_input":"2021-08-24T16:27:09.18363Z","iopub.status.idle":"2021-08-24T16:27:09.445424Z","shell.execute_reply.started":"2021-08-24T16:27:09.183595Z","shell.execute_reply":"2021-08-24T16:27:09.44447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b = nltk.FreqDist(HT_negative)\ne = pd.DataFrame({'Hashtag': list(b.keys()),\n                 'Count': list(b.values())})\n\n#selecting top 20 most frequent hashtags\ne = e.nlargest(columns = 'Count',n=20)\nplt.figure(figsize=(16,5))\nax = sns.barplot(data = e, x='Hashtag', y='Count')\nax.set(ylabel = 'Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:09.447123Z","iopub.execute_input":"2021-08-24T16:27:09.447594Z","iopub.status.idle":"2021-08-24T16:27:09.707462Z","shell.execute_reply.started":"2021-08-24T16:27:09.447558Z","shell.execute_reply":"2021-08-24T16:27:09.706675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GloVe\n\nI will use GloVe pretrained corpus model to represent our words.","metadata":{}},{"cell_type":"code","source":"data = pd.concat([train,test])\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:10.172564Z","iopub.execute_input":"2021-08-24T16:27:10.172895Z","iopub.status.idle":"2021-08-24T16:27:10.182096Z","shell.execute_reply.started":"2021-08-24T16:27:10.172867Z","shell.execute_reply":"2021-08-24T16:27:10.180816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_corpus_new(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus   ","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:11.371068Z","iopub.execute_input":"2021-08-24T16:27:11.371451Z","iopub.status.idle":"2021-08-24T16:27:11.378871Z","shell.execute_reply.started":"2021-08-24T16:27:11.37142Z","shell.execute_reply":"2021-08-24T16:27:11.378124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = create_corpus_new(data)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:12.722539Z","iopub.execute_input":"2021-08-24T16:27:12.722868Z","iopub.status.idle":"2021-08-24T16:27:14.644634Z","shell.execute_reply.started":"2021-08-24T16:27:12.722838Z","shell.execute_reply":"2021-08-24T16:27:14.643767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:14.646726Z","iopub.execute_input":"2021-08-24T16:27:14.647104Z","iopub.status.idle":"2021-08-24T16:27:30.549749Z","shell.execute_reply.started":"2021-08-24T16:27:14.647064Z","shell.execute_reply":"2021-08-24T16:27:30.548744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:30.551658Z","iopub.execute_input":"2021-08-24T16:27:30.552031Z","iopub.status.idle":"2021-08-24T16:27:30.841823Z","shell.execute_reply.started":"2021-08-24T16:27:30.551992Z","shell.execute_reply":"2021-08-24T16:27:30.840838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:30.843762Z","iopub.execute_input":"2021-08-24T16:27:30.844127Z","iopub.status.idle":"2021-08-24T16:27:30.850242Z","shell.execute_reply.started":"2021-08-24T16:27:30.844091Z","shell.execute_reply":"2021-08-24T16:27:30.84932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec    ","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:30.851749Z","iopub.execute_input":"2021-08-24T16:27:30.852426Z","iopub.status.idle":"2021-08-24T16:27:30.925538Z","shell.execute_reply.started":"2021-08-24T16:27:30.852388Z","shell.execute_reply":"2021-08-24T16:27:30.924726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_pad[0][0:]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:30.927087Z","iopub.execute_input":"2021-08-24T16:27:30.927429Z","iopub.status.idle":"2021-08-24T16:27:30.934446Z","shell.execute_reply.started":"2021-08-24T16:27:30.927395Z","shell.execute_reply":"2021-08-24T16:27:30.933327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Building","metadata":{}},{"cell_type":"code","source":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:30.936007Z","iopub.execute_input":"2021-08-24T16:27:30.936438Z","iopub.status.idle":"2021-08-24T16:27:31.083756Z","shell.execute_reply.started":"2021-08-24T16:27:30.93638Z","shell.execute_reply":"2021-08-24T16:27:31.082681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_new = tweet_pad[:train.shape[0]]\ntest_new = tweet_pad[train.shape[0]:]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:31.085696Z","iopub.execute_input":"2021-08-24T16:27:31.08614Z","iopub.status.idle":"2021-08-24T16:27:31.090833Z","shell.execute_reply.started":"2021-08-24T16:27:31.086103Z","shell.execute_reply":"2021-08-24T16:27:31.089906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(train_new,train['target'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:31.092378Z","iopub.execute_input":"2021-08-24T16:27:31.092814Z","iopub.status.idle":"2021-08-24T16:27:31.103553Z","shell.execute_reply.started":"2021-08-24T16:27:31.092779Z","shell.execute_reply":"2021-08-24T16:27:31.102194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Recomended 10-20 epochs\nhistory=model.fit(X_train,y_train,batch_size=128,epochs=10,validation_data=(X_test,y_test),verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:27:54.264106Z","iopub.execute_input":"2021-08-24T16:27:54.264418Z","iopub.status.idle":"2021-08-24T16:29:29.404201Z","shell.execute_reply.started":"2021-08-24T16:27:54.264389Z","shell.execute_reply":"2021-08-24T16:29:29.403445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:41:56.273623Z","iopub.execute_input":"2021-08-24T16:41:56.274032Z","iopub.status.idle":"2021-08-24T16:41:56.523548Z","shell.execute_reply.started":"2021-08-24T16:41:56.273994Z","shell.execute_reply":"2021-08-24T16:41:56.522542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:42:51.353847Z","iopub.execute_input":"2021-08-24T16:42:51.35427Z","iopub.status.idle":"2021-08-24T16:42:51.555915Z","shell.execute_reply.started":"2021-08-24T16:42:51.354231Z","shell.execute_reply":"2021-08-24T16:42:51.554906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction","metadata":{}},{"cell_type":"code","source":"test_pred= model.predict(test_new)\ntest_pred_int = test_pred.round().astype('int')\nsub['target'] = test_pred_int\nsub.to_csv('pred.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:34:03.443959Z","iopub.execute_input":"2021-08-24T16:34:03.444361Z","iopub.status.idle":"2021-08-24T16:34:04.940786Z","shell.execute_reply.started":"2021-08-24T16:34:03.444326Z","shell.execute_reply":"2021-08-24T16:34:04.940054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2021-08-24T16:34:22.212745Z","iopub.execute_input":"2021-08-24T16:34:22.213069Z","iopub.status.idle":"2021-08-24T16:34:22.225753Z","shell.execute_reply.started":"2021-08-24T16:34:22.213039Z","shell.execute_reply":"2021-08-24T16:34:22.224739Z"},"trusted":true},"execution_count":null,"outputs":[]}]}