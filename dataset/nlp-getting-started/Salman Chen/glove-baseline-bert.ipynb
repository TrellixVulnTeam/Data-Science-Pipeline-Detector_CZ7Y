{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is forked from \nhttps://www.kaggle.com/shahules/basic-eda-cleaning-and-glove/notebook\n\nAnd transformer guide from https://www.kaggle.com/massinissaguendoul/nlp-disaster-tweet"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Quick look"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ndisplay(train.head())\nprint(len(train))\ndisplay(test.head())\nprint(len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train[\"target\"].value_counts()\nplt.grid()\nsns.barplot(x.index, x)\nplt.gca().set_ylabel(\"samples\")\nplt.title(\"distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.grid()\n\nplt.hist(train[train[\"target\"] == 1][\"text\"].str.len())\nplt.title(\"Disaster tweets length\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.grid()\n\nplt.hist(train[train[\"target\"] == 0][\"text\"].str.len(), color= 'r')\nplt.title(\"No disaster tweets length\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.grid()\n\nword1 = train[train[\"target\"] == 1][\"text\"].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)))\nplt.title(\"Disaster tweets length\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.grid()\n\nword1 = train[train[\"target\"] == 0][\"text\"].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)), color = 'r')\nplt.title(\"Disaster tweets length\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Create corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n    for x in train[train[\"target\"] == target][\"text\"].str.split():\n        print(x)\n        for i in x:\n            corpus.append(i)\n            \n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(0)\n\nstop = set(stopwords.words(\"english\"))\n\ndictionary = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dictionary[word] +=1\n        \ntop = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = zip(*top)\n\nplt.grid()\nplt.bar(x,y)\nplt.title(\"top words 0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(1)\n\nstop = set(stopwords.words(\"english\"))\n\ndictionary = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dictionary[word] +=1\n        \ntop = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = zip(*top)\n\nplt.grid()\nplt.bar(x,y, color = 'r')\nplt.title(\"top words 1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(1)\n\ndictionary = defaultdict(int)\n\nimport string\n\nspecial_char = string.punctuation\n\nfor i in corpus:\n    if i in special_char:\n        dictionary[i] +=1\n        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y = zip(*dictionary.items())\n\nplt.grid()\nplt.bar(x,y)\nplt.title(\"Punctuation disaster 1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(0)\n\ndictionary = defaultdict(int)\n\nimport string\n\nspecial_char = string.punctuation\n\nfor i in corpus:\n    if i in special_char:\n        dictionary[i] +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y = zip(*dictionary.items())\n\nplt.grid()\nplt.bar(x,y, color = 'r')\nplt.title(\"Punctuation disaster 0\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Common words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\n\nfor word, count in most[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title(\"most common words\")\nplt.grid()\nsns.barplot(x = y, y = x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## removing URLs"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_url(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_url(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## remove html tag"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove emoji"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n                               u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n                               u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n                               u\"\\U0001F1E0-\\U0001F1FF\" #flags\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"    \n                               \"]+\", flags = re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuation(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_punctuation(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spelling checker\n\nAdditional: spelling checker for indonesian dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspellchecker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spellchecker import SpellChecker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spell = SpellChecker()\n\ndef correct_spellings(text):\n    corrected_text = []\n    \n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df['text']=df['text'].apply(lambda x : correct_spellings(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Glove vectorization (word2vec)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df):\n    corpus = []\n    for tweet in tqdm(df[\"text\"]):\n        words = [word.lower() for word in word_tokenize(tweet) if \\\n        ((word.isalpha() == 1) & (word not in stop))]\n        corpus.append(words)\n        \n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict = {}\n\nwith open('../input/glove6b100dtxt/glove.6B.100d.txt','r') as glove:\n    for line in glove:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\n        \nglove.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Dropout, Input\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom keras import regularizers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\n\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences,\n                          maxlen = MAX_LEN, \n                         truncating = 'post', \n                         padding = 'post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer_obj.word_index\nprint('number of unique words: ', len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words,100))\n\n\nfor word, i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n        \n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_tweets(tokenizer, tweets, max_len):\n    nb_tweets = len(tweets)\n    tokens = np.ones((nb_tweets,max_len),dtype='int32')\n    masks = np.zeros((nb_tweets,max_len),dtype='int32')\n    segs = np.zeros((nb_tweets,max_len),dtype='int32')\n\n    for k in range(nb_tweets):        \n        # INPUT_IDS\n        tweet = tweets[k]\n        enc = tokenizer.encode(tweet)                   \n        if len(enc) < max_len-2:\n            tokens[k,:len(enc)+2] = [0] + enc + [2]\n            masks[k,:len(enc)+2] = 1\n        else:\n            tokens[k,:max_len] = [0] + enc[:max_len-2] + [2]\n            masks[k,:max_len] = 1 \n    return tokens,masks,segs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(max_len):\n    ids = Input((max_len,), dtype=tf.int32)\n    attention = Input((max_len,), dtype=tf.int32)\n    token = Input((max_len,), dtype=tf.int32)\n    \n    bertweet = TFAutoModel.from_pretrained(\"vinai/bertweet-base\")\n    x,_ = bertweet(ids,attention_mask=attention,token_type_ids=token)\n\n    out = Dense(1,activation='sigmoid')(x[:,0,:])\n    \n    model = Model(inputs=[ids, attention, token], outputs = out)\n    optimizer = Adam(learning_rate=1e-5)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    return model\n\n    \nmodel = build_model(MAX_LEN)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = Adam(learning_rate=1e-5)\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = [\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = tweet_pad[train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens, train_masks, train_segs = encode_tweets(tokenizer, train[\"text\"].to_list(), MAX_LEN)\ntrain_labels = train[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_segs[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"X_train, X_test, y_train, y_test = train_test_split(train_data, train[\"target\"].values, test_size = 0.15)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\n#CKPT = ModelCheckpoint('./ckpt.h5', save_best_only=True, monitor='val_loss', mode='min')\nES = EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit([train_tokens,train_masks,train_segs], train_labels, \n                 batch_size = 32, epochs = 50, \n                 validation_split = 0.1, callbacks= [ES])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tokens, test_masks, test_segs = encode_tweets(tokenizer,test[\"text\"].to_list(), MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"target\"] = model.predict([test_tokens, test_masks, test_segs]).round().astype(int)\nsubmission = test[[\"id\", \"target\"]]\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}