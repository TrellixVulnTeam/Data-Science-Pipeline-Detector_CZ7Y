{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook is forked from \nhttps://www.kaggle.com/shahules/basic-eda-cleaning-and-glove/notebook"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Quick look"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ndisplay(train.head())\nprint(len(train))\ndisplay(test.head())\nprint(len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train[\"target\"].value_counts()\nplt.grid()\nsns.barplot(x.index, x)\nplt.gca().set_ylabel(\"samples\")\nplt.title(\"distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.grid()\n\nplt.hist(train[train[\"target\"] == 1][\"text\"].str.len())\nplt.title(\"Disaster tweets length\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.grid()\n\nplt.hist(train[train[\"target\"] == 0][\"text\"].str.len(), color= 'r')\nplt.title(\"No disaster tweets length\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.grid()\n\nword1 = train[train[\"target\"] == 1][\"text\"].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)))\nplt.title(\"Disaster tweets length\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.grid()\n\nword1 = train[train[\"target\"] == 0][\"text\"].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)), color = 'r')\nplt.title(\"Disaster tweets length\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Create corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n    for x in train[train[\"target\"] == target][\"text\"].str.split():\n        print(x)\n        for i in x:\n            corpus.append(i)\n            \n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(0)\n\nstop = set(stopwords.words(\"english\"))\n\ndictionary = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dictionary[word] +=1\n        \ntop = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = zip(*top)\n\nplt.grid()\nplt.bar(x,y)\nplt.title(\"top words 0\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(1)\n\nstop = set(stopwords.words(\"english\"))\n\ndictionary = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dictionary[word] +=1\n        \ntop = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x, y = zip(*top)\n\nplt.grid()\nplt.bar(x,y, color = 'r')\nplt.title(\"top words 1\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(1)\n\ndictionary = defaultdict(int)\n\nimport string\n\nspecial_char = string.punctuation\n\nfor i in corpus:\n    if i in special_char:\n        dictionary[i] +=1\n        \n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y = zip(*dictionary.items())\n\nplt.grid()\nplt.bar(x,y)\nplt.title(\"Punctuation disaster 1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(0)\n\ndictionary = defaultdict(int)\n\nimport string\n\nspecial_char = string.punctuation\n\nfor i in corpus:\n    if i in special_char:\n        dictionary[i] +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y = zip(*dictionary.items())\n\nplt.grid()\nplt.bar(x,y, color = 'r')\nplt.title(\"Punctuation disaster 0\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Common words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\n\nfor word, count in most[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title(\"most common words\")\nplt.grid()\nsns.barplot(x = y, y = x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## removing URLs"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_url(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_url(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## remove html tag"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove emoji"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n                               u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n                               u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n                               u\"\\U0001F1E0-\\U0001F1FF\" #flags\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"    \n                               \"]+\", flags = re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuation(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_punctuation(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spelling checker\n\nAdditional: spelling checker for indonesian dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspellchecker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spellchecker import SpellChecker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spell = SpellChecker()\n\ndef correct_spellings(text):\n    corrected_text = []\n    \n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df['text']=df['text'].apply(lambda x : correct_spellings(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Glove vectorization (word2vec)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df):\n    corpus = []\n    for tweet in tqdm(df[\"text\"]):\n        words = [word.lower() for word in word_tokenize(tweet) if \\\n        ((word.isalpha() == 1) & (word not in stop))]\n        corpus.append(words)\n        \n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict = {}\n\nwith open('../input/glove6b100dtxt/glove.6B.100d.txt','r') as glove:\n    for line in glove:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\n        \nglove.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\n\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences,\n                          maxlen = MAX_LEN, \n                         truncating = 'post', \n                         padding = 'post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer_obj.word_index\nprint('number of unique words: ', len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words,100))\n\n\nfor word, i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n        \n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import regularizers\n\nmodel = Sequential()\n\nglove_embedding = Embedding(num_words, 100, embeddings_initializer = Constant(embedding_matrix), \n                     input_length = MAX_LEN, \n                     trainable = False)\n\nmodel.add(glove_embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2))\nmodel.add(Dense(128, activation = 'relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\nmodel.add(Dense(256, activation = 'relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\noptimizer = Adam(learning_rate=1e-5)\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = [\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = tweet_pad[:train.shape[0]]\ntest_data = tweet_pad[train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_data, train[\"target\"].values, test_size = 0.15)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = model.fit(X_train, y_train, batch_size = 64, epochs = 50, validation_data = (X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict = model.predict(test_data)\ny_predict = np.round(y_predict).astype(int).reshape(3263)\n\nsub=pd.DataFrame({'id':submit['id'].values.tolist(),'target': y_predict})\nsub.to_csv('submission.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}