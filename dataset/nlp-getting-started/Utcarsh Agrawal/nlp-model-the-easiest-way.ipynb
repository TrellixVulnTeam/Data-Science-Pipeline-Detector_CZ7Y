{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Introduction**\n<div class=\"column\">\n<img src=\"https://media-exp1.licdn.com/dms/image/C561BAQGEbzpXZ34-gQ/company-background_10000/0?e=2159024400&v=beta&t=o3vOn3Ye-qpqlDH64A1of1_aRAQ8TunahPQ4ZWuISRI\" style=\"width:650px;height:350px;\">\n    </div><br>\n<b>In this kernel we will go together into Disaster Tweets data and will do some data analysis, data cleaning and then create a simple NLP model to predict whether the tweet is about real disaster or not. I have tried to explain all the steps so that even if this is your first nlp problem, you will not get any confusion in any step.</b><br><br>\n\n##  **<font color=\"red\"> Please do an upvote if you find my kernel useful.</font>**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Table of Content**\n* [Importing necesseties](#1)\n* [Reading the data](#2)\n* [EDA](#3)\n* [Data Cleaning](#4)\n* [Model](#5)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = '1'></a>\n# **Importing necesseties**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport re\nimport string\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('seaborn')\nfrom plotly import graph_objs as go\nimport plotly.express as px\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\n\nimport keras\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Flatten, Dropout\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '2'></a>\n# **Reading the data**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df1 = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf2 = pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmit = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df1.shape)\nprint(df2.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So we have 7613 tweets in the train set and 3263 tweets in the test set**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '3'></a>\n# **EDA**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Let's see how many tweets are disaster and non-disaster tweets**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df1.groupby('target').count()['text'].reset_index()\ntemp['label'] = temp['target'].apply(lambda x : 'Disaster Tweet' if x==1 else 'Non Disaster Tweet')\ntemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nsns.countplot(x='target',data=df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(go.Funnelarea(\n    text = temp.label,\n    values = temp.text,\n    title = {\"position\" : \"top center\", \"text\" : \"Funnel Chart for target distribution\"}\n    ))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Target Distribution in Keywords**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['target_mean'] = df1.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 78), dpi=100)\n\nsns.countplot(y=df1.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=df1.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ndf1.drop(columns=['target_mean'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Number of words in a tweet**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntweet_len=df1[df1['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('Disaster Tweets')\ntweet_len=df1[df1['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='blue')\nax2.set_title('Non Disaster Tweets')\nfig.suptitle('No.of words in a tweet')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now let us observe the common words in the tweet. But first we will convert all the text in lowercase so that same words with different case are not counted differently.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = str(text).lower()\n    return text\n\ndf1['text_plot'] = df1['text'].apply(lambda x:clean_text(x))\n\ndf1['temp_list'] = df1['text_plot'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in df1['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(temp, x='count',y='Common_words',title='Common words in tweet',orientation='h',width=700,height=700,color='Common_words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will remove all the stopwords and then will observe the common words graphically.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(x):\n    return [y for y in x if y not in stopwords.words('english')]\ndf1['temp_list'] = df1['temp_list'].apply(lambda x : remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top = Counter([item for sublist in df1['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us also visualize the wordcloud**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text = df1['text'].values\ntwitter_logo = np.array(Image.open('../input/twitter-logo2/10wmt-articleLarge-v4.jpg'))\ncloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='white',\n                          mask = twitter_logo,\n                          max_words=200\n                         ).generate(\" \".join(text))\n\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '4'></a>\n# **Data Cleaning**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will apply data cleaning steps to both our training and test data. Here we are going to concat them so that we don't have to apply each steps separately. Then later on after applying data cleaning process we will separate them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del df1['text_plot']\ndel df1['temp_list']\n\ndf = pd.concat([df1,df2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**First we will fill all the null values with no_{column name}.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['keyword', 'location']:\n    df[col] = df[col].fillna(f'no_{col}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we observed from EDA that we have to remove many things like url, html tags, punctuation marks etc.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**So now we will remove all the all the urls and the HTML tags**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].str.replace('https?://\\S+|www\\.\\S+','').str.replace('<.*?>','')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Removing all the emojis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x : remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Removing punctuation marks**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x : remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Remove leading, trailing, and extra spaces**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    text = re.sub('\\s+', ' ', text).strip() \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x : clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now as we have applied all the cleaning steps so now its time to separate our data back.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = np.split(df, [len(df1)], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = dfs[0]\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = dfs[1]\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop('target',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = '5'></a>\n# Model\nAfter doing all the data analysis and applying data cleaning process, now its time to create our model. \n\nNatural Language Processing (NLP) is a field in machine learning with the ability of a computer to understand, analyze, manipulate, and potentially generate human language.Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.We will be using Keras for creating our own NLP model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Firstly we define vocabulary size as len(test). That means this system here will support len(test) different words.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(test)\ntext = train['text'].values\nlabel = train['target'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As each word is just a sequence of characters and, obviously, we cannot work with sequence of characters. Therefore, we will convert each word into an integer number, and this integer number is unique, as long as we don't exceed the vocabulary size. It's not the one-hot encoding. It's basically just the transformation from a list of the words into a list of integer values.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_docs = [one_hot(d,vocab_size) for d in text]\nfor x in range(5):\n    print(encoded_docs[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now here, we are actually padding that means, if the sentence is not long enough, we are just filling it with zeros.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = len(train['text'].max())\npad_docs = pad_sequences(encoded_docs,maxlen=max_len,padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will define our model.**\n\nHere we are creating a Sequential model. Then we have one Embedding layer with vocab_size=7613,dimension=100 and input_length=max_len, one Dropout layer, one Flatten layer. Then we add 2 Dense layers with 1024 parameters and activation function is relu and a Dropout layer after each Dense Layer. In the end we add one final Dense layer with output_class=1 and activation function is sigmoid.\n\nThen we will compile our model with Adam optimizer and binary_crossentropy as loss function.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(7613,100,input_length=max_len))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will train our model with our padded data pad_docs and label with 20 epochs and batch_size=32.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(pad_docs,label,epochs=20,batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's check our predictions**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(pad_docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['prediction'] = prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['prediction'] = train['prediction'].apply(lambda x : 0 if x<0.5 else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now we will apply our trained model to the test data. But before that we have to also convert text of test data to padded data like we did earlier.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text2 = test['text'].values\nencoded_docs2 = [one_hot(d,vocab_size) for d in text2]\npad_docs2 = pad_sequences(encoded_docs2,maxlen=max_len,padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction2 = model.predict(pad_docs2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['prediction'] = prediction2\ntest['prediction'] = test['prediction'].apply(lambda x : 0 if x<0.5 else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Submitting our predictions**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submit['target'] = test['prediction']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This is my first kaggle notebook and I hope I have tried to explain each and every step. I will be back with new ideas and models as I learn more about different machine learning models. Please if you want to give me any suggestion or any doubt in any step comment below.**\n\n#  **<font color=\"red\"> Please do an upvote if you liked my kernel!</font>**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}