{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![Logo](https://raw.githubusercontent.com/marcelo-campa/Analytica-Kaggle-NLP/main/img/logo_desafio.png)","metadata":{}},{"cell_type":"markdown","source":"# Natural Language Processing with Disaster Tweets Challenge","metadata":{}},{"cell_type":"markdown","source":"The Brazilian Portuguese version of this notebook can be found on [this link](https://github.com/marcelo-campa/Analytica-Kaggle-NLP) \n\nThe [challenge](https://www.kaggle.com/c/nlp-getting-started) consists of a text classification problem. More specifically, it deals with tweets classification, which can be related to real disasters or not. For this, we will use Natural Language Processing (NLP) tools, as well as Machine Learning.\n\nOn this notebook, we'll deal with\n\n- \"Tweet Tokenizer\" for tweets tokenization \n- \"Word Vectors\"\n- SVC and Logistic Regression as classifiers\n\nWe'll also explain future improvements at the end of this document. Some parts were inspired by [this notebook](https://www.kaggle.com/pranjalchatterjee/word-vectors-svc-on-nlp-with-disaster-tweets) from @pranjalchatterjee ","metadata":{}},{"cell_type":"code","source":"# Importing Libs\n\nimport pandas as pd\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom scipy.sparse import csr_matrix\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:56:19.104462Z","iopub.execute_input":"2021-09-28T02:56:19.104815Z","iopub.status.idle":"2021-09-28T02:56:20.780962Z","shell.execute_reply.started":"2021-09-28T02:56:19.104724Z","shell.execute_reply":"2021-09-28T02:56:20.780012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading train and Test datasets\n\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:16.095845Z","iopub.execute_input":"2021-09-28T02:59:16.096149Z","iopub.status.idle":"2021-09-28T02:59:16.175579Z","shell.execute_reply.started":"2021-09-28T02:59:16.096119Z","shell.execute_reply":"2021-09-28T02:59:16.174663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:18.151346Z","iopub.execute_input":"2021-09-28T02:59:18.151705Z","iopub.status.idle":"2021-09-28T02:59:18.174976Z","shell.execute_reply.started":"2021-09-28T02:59:18.151668Z","shell.execute_reply":"2021-09-28T02:59:18.173978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:18.384577Z","iopub.execute_input":"2021-09-28T02:59:18.385013Z","iopub.status.idle":"2021-09-28T02:59:18.396102Z","shell.execute_reply.started":"2021-09-28T02:59:18.384973Z","shell.execute_reply":"2021-09-28T02:59:18.395019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:18.770754Z","iopub.execute_input":"2021-09-28T02:59:18.771048Z","iopub.status.idle":"2021-09-28T02:59:18.802508Z","shell.execute_reply.started":"2021-09-28T02:59:18.77102Z","shell.execute_reply":"2021-09-28T02:59:18.801661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:19.339217Z","iopub.execute_input":"2021-09-28T02:59:19.3395Z","iopub.status.idle":"2021-09-28T02:59:19.35321Z","shell.execute_reply.started":"2021-09-28T02:59:19.339471Z","shell.execute_reply":"2021-09-28T02:59:19.352157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `'Keyword'` column has some potential, as it hasn't much missing values. However, we will start our analysis with solutions focused on the content of documents.","metadata":{}},{"cell_type":"markdown","source":"## Default Tokenizer","metadata":{}},{"cell_type":"markdown","source":"Tokenizers are tools that helps us transforming sentences into separate words. As an example, the sentence `'Three people died from the heat wave so far'`, picked from the train dataset, is transformed in a list, in which every word is a token. The result is\n\n```['Three', 'people', 'died', 'from', 'the', 'heat', 'wave', 'so', 'far']```\n","metadata":{}},{"cell_type":"code","source":"stop_words_nltk = list(stopwords.words('english'))\ncount_vectorizer = CountVectorizer(stop_words='english')\ncount_train = count_vectorizer.fit_transform(train['text'].values)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:21.820622Z","iopub.execute_input":"2021-09-28T02:59:21.820969Z","iopub.status.idle":"2021-09-28T02:59:22.184042Z","shell.execute_reply.started":"2021-09-28T02:59:21.820931Z","shell.execute_reply":"2021-09-28T02:59:22.183209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csr_matrix(count_train).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:23.774922Z","iopub.execute_input":"2021-09-28T02:59:23.775227Z","iopub.status.idle":"2021-09-28T02:59:23.9493Z","shell.execute_reply.started":"2021-09-28T02:59:23.775194Z","shell.execute_reply":"2021-09-28T02:59:23.948136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Picking a sentence to be tokenized\n\ntrain['text'].values[10]","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:23.978617Z","iopub.execute_input":"2021-09-28T02:59:23.978934Z","iopub.status.idle":"2021-09-28T02:59:23.985245Z","shell.execute_reply.started":"2021-09-28T02:59:23.978901Z","shell.execute_reply":"2021-09-28T02:59:23.984352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the tokenized sentence\n\nword_tokenize(train['text'][10])","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:25.263072Z","iopub.execute_input":"2021-09-28T02:59:25.263359Z","iopub.status.idle":"2021-09-28T02:59:25.284851Z","shell.execute_reply.started":"2021-09-28T02:59:25.263327Z","shell.execute_reply":"2021-09-28T02:59:25.284269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tweet tokenizer","metadata":{}},{"cell_type":"markdown","source":"The tokenizer mentioned above was made with common texts in mind. However, we know that tweets have their own characteristics, such as the use of emojis, hashtags and various abbreviations.\n\nWith that in mind, we opted to test the Tweet Tokenizer, a tokenizer in the NLTK library optimized for analyzing Twitter texts.","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import TweetTokenizer\n\ndef tweet_tokenize_column(df, column):\n    \"\"\"     \n        This function gets the Dataframe and the name of a column (String) containing texts (Strings) and returns\n        a list of lists containing the tokenized text. It also turns every token to it's lower form and excludes\n        stopwords.\n        \n        Essa funcao recebe o Dataframe e o nome de uma coluna (String) contendo textos (Strings), e retorna uma lista\n        de listas contendo o texto tokenizado. A funcao tambem transforma todas as letras maiusculas em minusculas e \n        exclui stopwords.\n        \n        Input: Pandas DataFrame, String\n        Return: Nested List\n    \"\"\"\n    \n    tweet_tokenizer = TweetTokenizer()\n    \n    # List of sentences / Lista de sentencas\n    list_sent = [tweet_tokenizer.tokenize(sent) for sent in df[column].values]\n    \n    # List of sentences excluding stopword tokens / Lista de sentencas excluindo stopwords\n    list_sent_no_stop = [[token.lower() \n                           for token in sent \n                           if token not in stopwords.words('english')] \n                           for sent in list_sent]\n    \n    return list_sent_no_stop","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:26.759789Z","iopub.execute_input":"2021-09-28T02:59:26.760612Z","iopub.status.idle":"2021-09-28T02:59:26.76781Z","shell.execute_reply.started":"2021-09-28T02:59:26.760573Z","shell.execute_reply":"2021-09-28T02:59:26.766847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the function on train and test datasets\n\ntokenized_sent_train = tweet_tokenize_column(train,'text')\ntokenized_sent_test = tweet_tokenize_column(test,'text')","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:27.421136Z","iopub.execute_input":"2021-09-28T02:59:27.421483Z","iopub.status.idle":"2021-09-28T02:59:51.812344Z","shell.execute_reply.started":"2021-09-28T02:59:27.421446Z","shell.execute_reply":"2021-09-28T02:59:51.811455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the difference of the tokenizers. On this one, we observe that the tokenization of hashtags were improved.","metadata":{}},{"cell_type":"code","source":"tokenized_sent_train[:2]","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:51.814027Z","iopub.execute_input":"2021-09-28T02:59:51.814742Z","iopub.status.idle":"2021-09-28T02:59:51.821549Z","shell.execute_reply.started":"2021-09-28T02:59:51.814704Z","shell.execute_reply":"2021-09-28T02:59:51.820525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_sent_test[:2]","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:51.822614Z","iopub.execute_input":"2021-09-28T02:59:51.822827Z","iopub.status.idle":"2021-09-28T02:59:51.835729Z","shell.execute_reply.started":"2021-09-28T02:59:51.822801Z","shell.execute_reply":"2021-09-28T02:59:51.834835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create a list of lists containing all tokenized tweets from both training and testing. This is a way to ensure an unified analysis by TF-IDF, which will be explained later.","metadata":{}},{"cell_type":"code","source":"tokenized_sent_all = tokenized_sent_train + tokenized_sent_test","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:51.837414Z","iopub.execute_input":"2021-09-28T02:59:51.837626Z","iopub.status.idle":"2021-09-28T02:59:51.84725Z","shell.execute_reply.started":"2021-09-28T02:59:51.837601Z","shell.execute_reply":"2021-09-28T02:59:51.846074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF-IDF\n\nThe TF-IDF (Term Frequency - Inverse Document Frequency) is a numerical statistic that gives us the importance of a given word in a set of documents (corpus). Intuitively, a word (token) has a high TF-IDF score if\n\n- It has a high number of occurrences on a single document\ne\n- It has a low number of ocurrences on all documents\n\nThis 'number' is a result of the product of the 'TF' and the 'IDF', which are calculated as follows\n\n\\begin{align}\n    \\operatorname{tf}(t, d)&=\\frac{f_{t, d}}{\\sum_{t^{\\prime} \\in d} f_{t^{\\prime}, d}}\\\\\\\\\n    \\operatorname{idf}(t, D)&=\\log \\frac{N}{|\\{d \\in D: t \\in d\\}|}\\\\\\\\\n    \\operatorname{tfidf}(t, d, D)&=\\operatorname{tf}(t, d) \\cdot \\operatorname{idf}(t, D)\n\\end{align}\n\nAll terms can be seen in depth on [this link](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n\nNow, let's apply the TF-IDF.","metadata":{}},{"cell_type":"code","source":"# Auxiliar function to bypass the tokenizer, as this step had already been done\n\ndef identity_tokenizer(text):\n    return text\n\ntfidf_all = TfidfVectorizer(tokenizer=identity_tokenizer, stop_words='english', lowercase=False)    \ntfidf_all_fit = tfidf_all.fit_transform(tokenized_sent_all)\n\ntfidf_all.get_feature_names()[1000:1002]\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:51.848662Z","iopub.execute_input":"2021-09-28T02:59:51.84923Z","iopub.status.idle":"2021-09-28T02:59:52.162107Z","shell.execute_reply.started":"2021-09-28T02:59:51.849192Z","shell.execute_reply":"2021-09-28T02:59:52.161203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that because TF-IDF was done on all documents both from the train and test datasets, we calculate the scores by treating this aggregated list as a new corpus. This is positive as we will have this score based on more data, tending to give us a more realistic information.\n","metadata":{}},{"cell_type":"code","source":"# Creating an unified dataframe. The firs 'n' lines there are data about the train dataset, and on the 'm' following\n# lines we have data from the test dataset. 'm' is the number of train documents and 'n' is the number of test \n# documents.\n\ntfidf_all_df = pd.DataFrame(tfidf_all_fit.toarray(), columns=tfidf_all.get_feature_names())","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:52.165298Z","iopub.execute_input":"2021-09-28T02:59:52.165547Z","iopub.status.idle":"2021-09-28T02:59:52.715381Z","shell.execute_reply.started":"2021-09-28T02:59:52.16552Z","shell.execute_reply":"2021-09-28T02:59:52.714626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_all_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:52.716559Z","iopub.execute_input":"2021-09-28T02:59:52.716763Z","iopub.status.idle":"2021-09-28T02:59:52.752483Z","shell.execute_reply.started":"2021-09-28T02:59:52.716738Z","shell.execute_reply":"2021-09-28T02:59:52.751971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the aggregated dataframe into train and test\n\ntfidf_train_df = tfidf_all_df[:len(train)]\n\ntfidf_test_df = tfidf_all_df[len(train):]\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:52.753484Z","iopub.execute_input":"2021-09-28T02:59:52.753787Z","iopub.status.idle":"2021-09-28T02:59:52.757001Z","shell.execute_reply.started":"2021-09-28T02:59:52.75376Z","shell.execute_reply":"2021-09-28T02:59:52.756454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Including target column on the TF-IDF train dataset\n\ntfidf_train_df[\"target_column\"] = train['target']","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:52.75796Z","iopub.execute_input":"2021-09-28T02:59:52.758285Z","iopub.status.idle":"2021-09-28T02:59:52.77435Z","shell.execute_reply.started":"2021-09-28T02:59:52.758259Z","shell.execute_reply":"2021-09-28T02:59:52.773618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classifiers","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nX = tfidf_train_df.drop(\"target_column\", axis=1)\ny = tfidf_train_df[\"target_column\"]\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=16)\n\nclf = LogisticRegression(random_state=16)\n\nscores_logistic = cross_val_score(clf, X, y, cv=5)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T02:59:52.777661Z","iopub.execute_input":"2021-09-28T02:59:52.778004Z","iopub.status.idle":"2021-09-28T03:00:39.307112Z","shell.execute_reply.started":"2021-09-28T02:59:52.777972Z","shell.execute_reply":"2021-09-28T03:00:39.305977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores_logistic.mean()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:00:39.309062Z","iopub.execute_input":"2021-09-28T03:00:39.309436Z","iopub.status.idle":"2021-09-28T03:00:39.32319Z","shell.execute_reply.started":"2021-09-28T03:00:39.309385Z","shell.execute_reply":"2021-09-28T03:00:39.321034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nclf.fit(X,y)\n\ny_pred = clf.predict(X)\n\nprint('Training accuracy is {}'.format(accuracy_score(y, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:00:39.325376Z","iopub.execute_input":"2021-09-28T03:00:39.325845Z","iopub.status.idle":"2021-09-28T03:00:47.72872Z","shell.execute_reply.started":"2021-09-28T03:00:39.325798Z","shell.execute_reply":"2021-09-28T03:00:47.727845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We noticed a considerable gap between the cross-validation scores and the accuracy score. We note that this may have to do with overfit, when we train on the entire training dataset, and underfit when we separate the training and testing, datasets as we train with much smaller data.\n\nThe submission to Kaggle gave us a public score of 78,547%","metadata":{}},{"cell_type":"code","source":"# Submission 0.78547\n\nsample_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n\ny_sub = clf.predict(tfidf_test_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:02:46.353587Z","iopub.execute_input":"2021-09-28T03:02:46.353869Z","iopub.status.idle":"2021-09-28T03:02:46.838877Z","shell.execute_reply.started":"2021-09-28T03:02:46.35384Z","shell.execute_reply":"2021-09-28T03:02:46.837898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = sample_submission.copy()\nsub['target'] = y_sub\nsub.set_index('id',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:02:48.405209Z","iopub.execute_input":"2021-09-28T03:02:48.40551Z","iopub.status.idle":"2021-09-28T03:02:48.412659Z","shell.execute_reply.started":"2021-09-28T03:02:48.405479Z","shell.execute_reply":"2021-09-28T03:02:48.411686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:02:48.597495Z","iopub.execute_input":"2021-09-28T03:02:48.59778Z","iopub.status.idle":"2021-09-28T03:02:48.606616Z","shell.execute_reply.started":"2021-09-28T03:02:48.597752Z","shell.execute_reply":"2021-09-28T03:02:48.605965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"./sub_01.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:03:10.507737Z","iopub.execute_input":"2021-09-28T03:03:10.508031Z","iopub.status.idle":"2021-09-28T03:03:10.521747Z","shell.execute_reply.started":"2021-09-28T03:03:10.508003Z","shell.execute_reply":"2021-09-28T03:03:10.520478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"markdown","source":"As a way to try to reduce the overfit and improve the performance of our classifier, we chose to use the $\\chi^{2}$ and Mutual Information scores in order to select the most \"important\" variables for our model, reducing also the dimensionality of the problem.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif, chi2\n\n# mi = mutual_info_classif(tfidf_train_df_int.drop(\"target_column\", axis=1), tfidf_train_df_int[\"target_column\"])\n# mi = pd.Series(mi)\n# mi.index = intersect_columns\n# mi.sort_values(ascending=False, inplace=True) \n\nchi = chi2(X,y)\nchi = pd.Series(chi[0])\nchi.index = X.columns\nchi.sort_values(ascending=False, inplace=True)    \n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:03:13.738689Z","iopub.execute_input":"2021-09-28T03:03:13.738995Z","iopub.status.idle":"2021-09-28T03:03:15.150827Z","shell.execute_reply.started":"2021-09-28T03:03:13.738964Z","shell.execute_reply":"2021-09-28T03:03:15.150143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chi[:5]","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:03:15.208702Z","iopub.execute_input":"2021-09-28T03:03:15.209472Z","iopub.status.idle":"2021-09-28T03:03:15.221316Z","shell.execute_reply.started":"2021-09-28T03:03:15.20943Z","shell.execute_reply":"2021-09-28T03:03:15.220397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chi.to_csv(\"./chi.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:03:19.549285Z","iopub.execute_input":"2021-09-28T03:03:19.549586Z","iopub.status.idle":"2021-09-28T03:03:19.64126Z","shell.execute_reply.started":"2021-09-28T03:03:19.549554Z","shell.execute_reply":"2021-09-28T03:03:19.640343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"atts = np.linspace(100,10000,100)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:03:23.181009Z","iopub.execute_input":"2021-09-28T03:03:23.181635Z","iopub.status.idle":"2021-09-28T03:03:23.748747Z","shell.execute_reply.started":"2021-09-28T03:03:23.18159Z","shell.execute_reply":"2021-09-28T03:03:23.747805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.fit(X_train[chi[:3800].index],y_train)\n\ny_pred = clf.predict(X_test[chi[:3800].index])\n\nacc = accuracy_score(y_test , y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:03:29.494599Z","iopub.execute_input":"2021-09-28T03:03:29.495476Z","iopub.status.idle":"2021-09-28T03:03:30.980474Z","shell.execute_reply.started":"2021-09-28T03:03:29.49542Z","shell.execute_reply":"2021-09-28T03:03:30.97926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sub_chi = clf.predict(tfidf_test_df[chi[:3800].index])","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:03:39.6137Z","iopub.execute_input":"2021-09-28T03:03:39.614016Z","iopub.status.idle":"2021-09-28T03:03:39.763497Z","shell.execute_reply.started":"2021-09-28T03:03:39.613967Z","shell.execute_reply":"2021-09-28T03:03:39.762498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_chi = sample_submission.copy()\n\nsub_chi['target'] = y_sub_chi\n\nsub_chi.set_index('id',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:03:40.090368Z","iopub.execute_input":"2021-09-28T03:03:40.090656Z","iopub.status.idle":"2021-09-28T03:03:40.097874Z","shell.execute_reply.started":"2021-09-28T03:03:40.090624Z","shell.execute_reply":"2021-09-28T03:03:40.096827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Chi^2 feature selection submission\n\nsub_chi.to_csv(\"./sub_chi.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:03:45.808132Z","iopub.execute_input":"2021-09-28T03:03:45.808411Z","iopub.status.idle":"2021-09-28T03:03:45.821128Z","shell.execute_reply.started":"2021-09-28T03:03:45.808383Z","shell.execute_reply":"2021-09-28T03:03:45.820303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Suppor Vector Classifier (SVC)","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nclf_svc = SVC()\nclf_svc.fit(X_train[chi[:3800].index],y_train)\ny_pred = clf_svc.predict(X_test[chi[:3800].index])\nacc = accuracy_score(y_test , y_pred)\n\nprint('Training accuracy is {}'.format(acc))","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:03:47.772319Z","iopub.execute_input":"2021-09-28T03:03:47.772578Z","iopub.status.idle":"2021-09-28T03:07:34.962378Z","shell.execute_reply.started":"2021-09-28T03:03:47.77255Z","shell.execute_reply":"2021-09-28T03:07:34.961611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_svc.fit(tfidf_train_df[chi[:3800].index],y)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:07:34.96393Z","iopub.execute_input":"2021-09-28T03:07:34.964182Z","iopub.status.idle":"2021-09-28T03:12:10.859689Z","shell.execute_reply.started":"2021-09-28T03:07:34.964155Z","shell.execute_reply":"2021-09-28T03:12:10.859007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sub_svc = clf_svc.predict(tfidf_test_df[chi[:3800].index])","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:12:10.860775Z","iopub.execute_input":"2021-09-28T03:12:10.861221Z","iopub.status.idle":"2021-09-28T03:14:04.899418Z","shell.execute_reply.started":"2021-09-28T03:12:10.861188Z","shell.execute_reply":"2021-09-28T03:14:04.898449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_svc = sample_submission.copy()\nsub_svc['target'] = y_sub_svc\nsub_svc.set_index('id',inplace=True)\n\nsub_svc.to_csv(\"./sub_svc_overfit.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:14:30.167338Z","iopub.execute_input":"2021-09-28T03:14:30.167657Z","iopub.status.idle":"2021-09-28T03:14:30.184178Z","shell.execute_reply.started":"2021-09-28T03:14:30.167623Z","shell.execute_reply":"2021-09-28T03:14:30.183176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# atts = [1000,3000,5000]\n# list_scores_svc = []\n\n# for att in tqdm(atts):\n#     clf_svc.fit(X_train[chi[:int(att)].index],y_train)\n#     y_pred = clf_svc.predict(X_test[chi[:int(att)].index])\n#     acc = accuracy_score(y_test , y_pred)\n    \n#     list_scores_svc.append(acc)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:14:30.976584Z","iopub.execute_input":"2021-09-28T03:14:30.976872Z","iopub.status.idle":"2021-09-28T03:14:30.981736Z","shell.execute_reply.started":"2021-09-28T03:14:30.97684Z","shell.execute_reply":"2021-09-28T03:14:30.980619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Vectors","metadata":{}},{"cell_type":"markdown","source":"Having in mind the solution through tokenization with TF-IDF, let's now move on to another type of abstraction, the Word Vectors. This abstraction allows us to map words as multidimensional vectors, indicating the mapping of that word not only in terms of quantity like TF-IDF, but also taking context into account.\n\nAs an example, the image below represents the two most important coordinates of a series of 300 dimension word vectors (which represent words in a given context).\n\n<img src=\"https://raw.githubusercontent.com/marcelo-campa/Analytica-Kaggle-NLP/main/img/word_vectors_map.png\" alt=\"Vetor de Palavras\" width=\"600\"/>\n\nCredits to [this website](https://dzone.com/articles/introduction-to-word-vectors).\n","metadata":{}},{"cell_type":"code","source":"import spacy \n\nnlp = spacy.load('en_core_web_lg')\n\nwith nlp.disable_pipes():\n    train_vecs = pd.DataFrame(np.array([nlp(text).vector for text in train.text])) # doc vectors for training set\n    test_vecs = pd.DataFrame(np.array([nlp(text).vector for text in test.text])) # doc vectors for testing set","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:14:33.69036Z","iopub.execute_input":"2021-09-28T03:14:33.690617Z","iopub.status.idle":"2021-09-28T03:16:45.608652Z","shell.execute_reply.started":"2021-09-28T03:14:33.690591Z","shell.execute_reply":"2021-09-28T03:16:45.608029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi = mutual_info_classif(train_vecs,train.target)\nmi = pd.Series(mi)\nmi.index = train_vecs.columns\nmi.sort_values(ascending=False, inplace=True)    ","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:16:45.610859Z","iopub.execute_input":"2021-09-28T03:16:45.611751Z","iopub.status.idle":"2021-09-28T03:17:10.513532Z","shell.execute_reply.started":"2021-09-28T03:16:45.611703Z","shell.execute_reply":"2021-09-28T03:17:10.512873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_word_vec_train, X_word_vec_test, y_word_vec_train, y_word_vec_test = train_test_split(train_vecs, train.target.values, test_size=0.33, random_state=16)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:17:10.514578Z","iopub.execute_input":"2021-09-28T03:17:10.515263Z","iopub.status.idle":"2021-09-28T03:17:10.524096Z","shell.execute_reply.started":"2021-09-28T03:17:10.51522Z","shell.execute_reply":"2021-09-28T03:17:10.523327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC()\n\natts = np.linspace(1, 299, 299)\nlist_scores_svc = []\n\nfor att in tqdm(atts):\n    svc.fit(X_word_vec_train[mi[:int(att)].index].values, y_word_vec_train)\n    y_pred = svc.predict(X_word_vec_test[mi[:int(att)].index].values)\n    acc = accuracy_score(y_word_vec_test , y_pred)\n    \n    list_scores_svc.append(acc)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:17:10.526098Z","iopub.execute_input":"2021-09-28T03:17:10.52696Z","iopub.status.idle":"2021-09-28T03:45:02.875551Z","shell.execute_reply.started":"2021-09-28T03:17:10.526914Z","shell.execute_reply":"2021-09-28T03:45:02.874711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nint_atts = [int(att) for att in atts]\n\nsns.set()\nplt.figure(figsize=(14,7))\nsns.lineplot(y=list_scores_svc, x=atts)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:45:02.87681Z","iopub.execute_input":"2021-09-28T03:45:02.877134Z","iopub.status.idle":"2021-09-28T03:45:03.378788Z","shell.execute_reply.started":"2021-09-28T03:45:02.877088Z","shell.execute_reply":"2021-09-28T03:45:03.37792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc.fit(train_vecs.values, train.target)\ny_pred = svc.predict(test_vecs.values)\n\nsub_svc = sample_submission.copy()\nsub_svc['target'] = y_pred\nsub_svc.set_index('id',inplace=True)\n\nsub_svc.to_csv(\"./sub_svc_word_vec.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-09-28T03:45:48.614988Z","iopub.execute_input":"2021-09-28T03:45:48.615365Z","iopub.status.idle":"2021-09-28T03:46:11.709765Z","shell.execute_reply.started":"2021-09-28T03:45:48.615326Z","shell.execute_reply":"2021-09-28T03:46:11.708938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## To-Do\n\n- Test other models ( NaiveBayes, RidgeClassifier, ...)\n- Use GridSearch to tune Hyperparameters\n","metadata":{}}]}