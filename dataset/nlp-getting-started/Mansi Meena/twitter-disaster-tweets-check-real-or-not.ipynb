{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Twitter Disaster Tweets check\n\nTwitter has become an important communication channel in times of emergency.The ubiquitousness of smartphones enables people to announce an emergency theyâ€™re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).  This notebook is a basic demonstration of the process used in classification of fake versus real disaster tweets."},{"metadata":{},"cell_type":"markdown","source":"**About Data:**\n\nFiles\n\n* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - a sample submission file in the correct format\n* Columns\n\nColumns\n\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n"},{"metadata":{},"cell_type":"markdown","source":"**Target** :\n\nTo Predict whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0."},{"metadata":{},"cell_type":"markdown","source":"# **Importing Libraries**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n#Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\nimport plotly.offline\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n#Natural Language Processing\n#Data Manipulation and Cleaning\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import Counter\nstop = set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\n#Modeling\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"Train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating Train copy to perform Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = Train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the data the target column shows 1 for real disaster tweet and 0 for fake disaster tweet."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(train.shape[0], train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis and Visualisation "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Category counts for type of tweets\nCategory_count=np.array(train['target'].value_counts())\nTweet_type=sorted(train['target'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[go.Pie(labels=Tweet_type, values=Category_count, hole=.3)])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Donut chart shows that 57 % of the Disaster Tweets are Fake. Let's check the actual counts."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the actual counts show that more than 4000+ Disasters tweets are fake and 3000+ disasters tweets are real. "},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"Analysis on character level, word level and sentence level."},{"metadata":{},"cell_type":"markdown","source":"# **Number of characters in tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding lenght column to dataset\ntrain['length']=train['text'].apply(len)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking length distribution\nimport plotly.express as px\nfig = px.histogram(train, x=\"length\", color=\"target\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Number of words in tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['word_count']=train['text'].str.split().map(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.histogram(train, x=\"word_count\", color=\"target\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tweet with max length\ntrain[train['length']==157]['text'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['length']==7]['text'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tweet with max word count\ntrain[train['word_count']==31]['text'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['word_count']==1]['text'].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Average word length in a tweet**"},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_word_length=train['text'].str.split().apply(lambda x : [len(i) for i in x])\ntrain['avg_word_length']=avg_word_length.map(lambda x: np.mean(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.histogram(train, x=\"avg_word_length\", color=\"target\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Tweet Corpus function\ndef create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n            \n    return corpus        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Common stopwords in tweets**"},{"metadata":{},"cell_type":"markdown","source":"First we will analyze stopwords in real tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(1)\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop = sorted(dic.items(), key = lambda x:x[1], reverse = True)[:10]        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y = zip(*top)\nplt.bar(x,y, color = 'pink')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyzing stopwords in Fake Tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(0)\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop = sorted(dic.items(), key = lambda x:x[1], reverse = True)[:10]        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x,y = zip(*top)\nplt.bar(x,y, color = 'pink')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Analyzing punctuations.**"},{"metadata":{},"cell_type":"markdown","source":"First let's check tweets indicating real disaster tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,5))\ncorpus = create_corpus(1)\n\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i] += 1\n        \nx, y = zip(*dic.items())\nplt.bar(x, y, color='purple')\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll look at punctuations of fake tweets "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,5))\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i] += 1\n        \nx, y = zip(*dic.items())\nplt.bar(x, y, color = 'purple')\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Analyzing Common words**"},{"metadata":{},"cell_type":"markdown","source":"**Real Disaster Tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(1)\ncounter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\nfor word, count in most[:40]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fake Disaster Tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(0)\ncounter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\nfor word, count in most[:40]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Ngram Analysis**"},{"metadata":{},"cell_type":"markdown","source":"we will do a bigram (n=2) analysis over the tweets. Let's check the most common bigrams in tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_tweet_bigrams(corpus, n = None):\n    vec = CountVectorizer(ngram_range = (2,2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0)\n    words_freq = [(word, sum_words[0, idx]) for word,idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return words_freq[:n]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,5))\ntop_tweet_bigrams = get_top_tweet_bigrams(train['text'])[:10]\nx,y = map(list, zip(*top_tweet_bigrams))\nsns.barplot(x=y, y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"As we know,twitter tweets always have to be cleaned before we go onto modelling.So we will do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.So let's start."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([Train,test])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Removing URLs**"},{"metadata":{"trusted":true},"cell_type":"code","source":"example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\nremove_URL(example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x : remove_URL(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing HTML tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"example = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing Emojis"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake ðŸ˜”ðŸ˜”\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x : remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    table = str.maketrans('','', string.punctuation)\n    return text.translate(table)\n\nexample = \"I am a #king\"\nprint(remove_punct(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Glove for Vectorization"},{"metadata":{},"cell_type":"markdown","source":"Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df):\n    corpus = []\n    for tweet in tqdm(df['text']):\n        words = [word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict = {}\nwith open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt', 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\nf.close()        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences, maxlen = MAX_LEN, truncating = 'post', padding = 'post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer_obj.word_index\nprint('Number of unique words:', len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words, 100))\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n        \n    emb_vec = embedding_dict.get(word)   \n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline Model using LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ = tweet_pad[:train.shape[0]]\ntest = tweet_pad[train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_,train['target'].values, test_size = 0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train,y_train, batch_size = 4, epochs =15, validation_data = (X_test, y_test), verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so, we have got 78% accuracy using LSTM Baseline Model"},{"metadata":{},"cell_type":"markdown","source":"# Making our submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pre=model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please upvote my work if it could help! Thank you!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}