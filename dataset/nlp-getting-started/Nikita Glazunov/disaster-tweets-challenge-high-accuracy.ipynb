{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libs","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport nltk\nimport string\nimport re\nimport emoji\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 40\n\ndef set_seed(seed=SEED):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nset_seed()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading files","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\nsample = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n\ntrain_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# need to clean duplicates\ntrain_df.drop_duplicates('text', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    text = re.sub('https?:\\/\\/t.co\\/[A-Za-z0-9]+', '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\d+', '', text)\n    text = re.sub(r'&amp;?', r'and', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub(r'[!]+', '!', text)\n    text = re.sub(r'[?]+', '?', text)\n    text = re.sub(r'[.]+', '.', text)\n    #delete emodzi\n    allchars = [c for c in text]\n    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI[\"en\"]]\n    text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n    text = nltk.word_tokenize(text)\n    text = [i.lower() for i in text if (i not in string.punctuation)]\n    text = [i for i in text if (i not in stopwords.words('english'))]\n#     in case you need cleaner data\n#     #stemming\n#     text = [PorterStemmer().stem(word) for word in text]\n#     #Lemmentization\n#     text = [WordNetLemmatizer().lemmatize(word) for word in text]\n    text = ' '.join(text)\n    text = text.strip()\n    \n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['tokenized'] = train_df.text.apply(preprocess_text)\ntest_df['tokenized'] = test_df.text.apply(preprocess_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok = Tokenizer()\n\ntrain_text = train_df.tokenized\ntrain_labels = train_df.target\ntest_text = test_df.tokenized\n\ntok.fit_on_texts(train_text)\n\ntrain_text = tok.texts_to_sequences(train_text)\ntest_text = tok.texts_to_sequences(test_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_list = [len(s) for s in train_text]\nMAX_SEQ_LEN = np.max(seq_list)\n\ntrain_text = tf.keras.preprocessing.sequence.pad_sequences(\n    train_text,\n    padding='post',\n    truncating='post',\n    maxlen=MAX_SEQ_LEN\n)\n\ntest_text = tf.keras.preprocessing.sequence.pad_sequences(\n    test_text,\n    padding='post',\n    truncating='post',\n    maxlen=MAX_SEQ_LEN\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"VOC_SIZE = len(tok.index_word) + 1\nEPOCHS = 100\nBATCH_SIZE = 512\nUNITS = 64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOC_SIZE, UNITS, input_length=MAX_SEQ_LEN, mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(UNITS, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(UNITS, return_sequences=False, dropout=0.5, recurrent_dropout=0.5)),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# very important callbacks\nearly_stopping = EarlyStopping(patience=13, verbose=1)\ncheckpoint = ModelCheckpoint('model.h5', save_best_only=True, verbose=1)\nlr_reduce = ReduceLROnPlateau(patience=5, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(amsgrad=True),\n    loss='binary_crossentropy',\n    metrics=['acc']\n    )\n\nhistory = model.fit(\n    train_text,\n    train_labels,\n    validation_split=0.2,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[early_stopping, checkpoint, lr_reduce]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict_classes(test_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample['target'] = pred\nsample.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feel free to comment my notebook and don't forget to hit the like button!","metadata":{}}]}