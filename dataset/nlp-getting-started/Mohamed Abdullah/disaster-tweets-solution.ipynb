{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Real or Not? NLP with Disaster Tweets\n<br>\n<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTbmtImbdYE8HEt_GzzxuWvTAXcNTzdk-vC0q3q5wtzbWniXvQG' alt='twitter' style='float:left' width=100%>\n<div style='clear:both'></div>\n<hr>\n**Welcome all ðŸ˜Š**<br>\n\nIn this kernel we will go together into **Disaster Tweets** data to learn how use basic natural language processing **NLP** techniques<br>\n\nThis kernel will be devided into the following parts<br>\n<ol>\n    <li><b>Data Exploration</b></li>\n    <li><b>Data Preprocessing</b></li>\n    <li><b>Basic NLP Techniques</b></li>\n    <li><b>Models Bulding</b></li>\n    <li><b>Models evaluation</b></li>\n</ol>\nNow we will import libraries and load our data."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Important libraries loaded successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Exploration"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\nprint(\"Data shape = \",data_train.shape)\ndata_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see in above table we have some missing values. So let's deal with it\n\n# 2. Data Preprocessing\nData Preprocessing one of important steps in any data science or machine learning project so let's start.\n## 2.1 Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get total count of data including missing data\ntotal = data_train.isnull().sum().sort_values(ascending=False)\n\n#get percent of missing data relevant to all data\npercent = (data_train.isnull().sum()/data_train.isnull().count()).sort_values(ascending=False)\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(data_train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see in above table almost 33% of **location** column is missing and very littel percentage of **keyword** column is missing.<br>\n\n## 2.2 How to Handle Missing Data ?\nOne of the most common problems we have faced in Data Analysis is handling the missing values.<br>\n\nI love put this image in my kernels because it give a roadmap to handle **missing data** \n<img src='https://miro.medium.com/max/1528/1*_RA3mCS30Pr0vUxbp25Yxw.png' width=\"550px\" style='float:left;'>\n<div style='clear:both'></div>\n<br>\n\nIn **Deletion** I will use **Deleting Columns** technique. Now we will drop **location** and **keyword** columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = data_train.drop(['location','keyword'], axis=1)\nprint(\"location and keyword columns droped successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We all know that **id** column isn't important to us, so we will drop it"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = data_train.drop('id', axis=1)\nprint(\"id column droped successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we only have text and target columns only. let's make sure"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Basic NLP Techniques\n<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcT6TkPpD8nWsbTVa9ExwfCQUnFmzkNE8zjZJ3uXSaBVd09ErhvZ' alt='text preprocessing' style='float:left' width=50% >\n<div style='clear:both'></div>\n<hr>\nBefore starting text preprocess steps we must we must know two terms **Corpus and Bag of word.**<br>\n\n**Corpus :** Is a large and structured set of texts, We can consider it as simplified version of our text data that contain clean and benefit data.<br>\n\n**Bag of word :** In practice, the Bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a \"bag of words\", we can calculate various measures to characterize the text [wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)<br>\n\nNow we will do the following steps to preprocess our text data\n<ol>\n    <li><b>Remove unwanted words</b></li>\n    <li><b>Transform words to lowercase</b></li>\n    <li><b>Remove stopwords</b></li>\n    <li><b>Stemming words</b></li>\n    <li><b>Create sparse matrix ( Bag of words )</b></li>\n</ol>  \nNow let's deal with our **text** column by exploar it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train[\"text\"].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Remove unwanted words\nAs we see our **text** column contain unwanted words as **#, =>, numbers, or ... etc** these letters will not be useful in our problem so we will get only pure text without any markings or numbers.<br>\n\nWe will do it by **specify** our pattern using **re** library.\n\n## 3.2 Transform words to lowercase\nWe must transform words to lowercase because each letter has own **ASCII Code** that represent text in computers, Uppercase letter has different ASCII Code than same letter in lowercase format. **so that** 'A' letter differ from 'a' letter in computer.\n\n## 3.3 Remove stopwords\n**Stop words :** are generally the most common words in a language, so we will remove it to prevent misleading problem in our model.\n\n## 3.4 Stemming words\n**stemming :** is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form [wikipedia](https://en.wikipedia.org/wiki/Stemming)<br>\nWe use stemming to reduce **bag of words** dimensionality."},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus  = []\npstem = PorterStemmer()\nfor i in range(data_train['text'].shape[0]):\n    #Remove unwanted words\n    tweet = re.sub(\"[^a-zA-Z]\", ' ', data_train['text'][i])\n    #Transform words to lowercase\n    tweet = tweet.lower()\n    tweet = tweet.split()\n    #Remove stopwords then Stemming it\n    tweet = [pstem.stem(word) for word in tweet if not word in set(stopwords.words('english'))]\n    tweet = ' '.join(tweet)\n    #Append cleaned tweet to corpus\n    corpus.append(tweet)\n    \nprint(\"Corpus created successfully\")    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's explore corpus, and discover the difference between raw and clean text data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.DataFrame(corpus)[0].head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rawTexData = data_train[\"text\"].head(10)\ncleanTexData = pd.DataFrame(corpus, columns=['text after cleaning']).head(10)\n\nframes = [rawTexData, cleanTexData]\nresult = pd.concat(frames, axis=1, sort=False)\nresult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we know that there some words that repeated so little in our tweets, so we must remove these words from our **Bag of words** to decrease dimensionality as possible.<br>\n\nWe will do it by create dictionary where **key** refer to **word** and **value** refer to **word frequents in all tweets**."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create our dictionary \nuniqueWordFrequents = {}\nfor tweet in corpus:\n    for word in tweet.split():\n        if(word in uniqueWordFrequents.keys()):\n            uniqueWordFrequents[word] += 1\n        else:\n            uniqueWordFrequents[word] = 1\n            \n#Convert dictionary to dataFrame\nuniqueWordFrequents = pd.DataFrame.from_dict(uniqueWordFrequents,orient='index',columns=['Word Frequent'])\nuniqueWordFrequents.sort_values(by=['Word Frequent'], inplace=True, ascending=False)\nuniqueWordFrequents.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uniqueWordFrequents['Word Frequent'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see some words repeated a lot and others repeated less, so we will get only words that repeated more than or equal 20 once."},{"metadata":{"trusted":true},"cell_type":"code","source":"uniqueWordFrequents = uniqueWordFrequents[uniqueWordFrequents['Word Frequent'] >= 20]\nprint(uniqueWordFrequents.shape)\nuniqueWordFrequents","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5 Create sparse matrix ( Bag of words )\n**Bag of word** contain only unique words in corpus."},{"metadata":{"trusted":true},"cell_type":"code","source":"counVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\nbagOfWords = counVec.fit_transform(corpus).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Models Bulding\nNow we will build our models, we will use following models\n* Decision Tree Model\n* Gradient Boosting Model\n* K - Nearest Neighbors Model\n* Logistic Regression Model\n* Stochastic Gradient Descent Model\n* Support Vector Machine Model\n* Bernoulli Naive Bayes Model\n* Gaussian Naive Bayes Model\n* Multinomial Naive Bayes Model\n* Voting Classifier Model\n\nBut before using it we will split our data to train and test set first."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = bagOfWords\ny = data_train['target']\nprint(\"X shape = \",X.shape)\nprint(\"y shape = \",y.shape)\n\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.20, random_state=55, shuffle =True)\nprint('data splitting successfully')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Decision Tree Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"decisionTreeModel = DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = None, \n                                           splitter='best', \n                                           random_state=55)\n\ndecisionTreeModel.fit(X_train,y_train)\n\nprint(\"decision Tree Classifier model run successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Gradient Boosting Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"gradientBoostingModel = GradientBoostingClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 100,\n                                                   max_depth = 30,\n                                                   random_state=55)\n\ngradientBoostingModel.fit(X_train,y_train)\n\nprint(\"gradient Boosting Classifier model run successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3 K - Nearest Neighbors Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"KNeighborsModel = KNeighborsClassifier(n_neighbors = 7,\n                                       weights = 'distance',\n                                      algorithm = 'brute')\n\nKNeighborsModel.fit(X_train,y_train)\n\nprint(\"KNeighbors Classifier model run successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4 Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"LogisticRegression = LogisticRegression(penalty='l2', \n                                        solver='saga', \n                                        random_state = 55)  \n\nLogisticRegression.fit(X_train,y_train)\n\nprint(\"LogisticRegression Classifier model run successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.5 Stochastic Gradient Descent Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"SGDClassifier = SGDClassifier(loss = 'hinge', \n                              penalty = 'l1',\n                              learning_rate = 'optimal',\n                              random_state = 55, \n                              max_iter=100)\n\nSGDClassifier.fit(X_train,y_train)\n\nprint(\"SGDClassifier Classifier model run successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.6 Support Vector Machine Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"SVClassifier = SVC(kernel= 'linear',\n                   degree=3,\n                   max_iter=10000,\n                   C=2, \n                   random_state = 55)\n\nSVClassifier.fit(X_train,y_train)\n\nprint(\"SVClassifier model run successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.7 Bernoulli Naive Bayes Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"bernoulliNBModel = BernoulliNB(alpha=0.1)\nbernoulliNBModel.fit(X_train,y_train)\n\nprint(\"bernoulliNB model run successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.8 Gaussian Naive Bayes Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"gaussianNBModel = GaussianNB()\ngaussianNBModel.fit(X_train,y_train)\n\nprint(\"gaussianNB model run successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.9 Multinomial Naive Bayes Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"multinomialNBModel = MultinomialNB(alpha=0.1)\nmultinomialNBModel.fit(X_train,y_train)\n\nprint(\"multinomialNB model run successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.10 Voting Classifier Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"modelsNames = [('LogisticRegression',LogisticRegression),\n               ('SGDClassifier',SGDClassifier),\n               ('SVClassifier',SVClassifier),\n               ('bernoulliNBModel',bernoulliNBModel),\n               ('multinomialNBModel',multinomialNBModel)]\n\nvotingClassifier = VotingClassifier(voting = 'hard',estimators= modelsNames)\nvotingClassifier.fit(X_train,y_train)\nprint(\"votingClassifier model run successfully\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5 Models evaluation\n\nNow we will evaluate our model using **f1_score** let's go. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#evaluation Details\nmodels = [decisionTreeModel, gradientBoostingModel, KNeighborsModel, LogisticRegression, \n          SGDClassifier, SVClassifier, bernoulliNBModel, gaussianNBModel, multinomialNBModel, votingClassifier]\n\nfor model in models:\n    print(type(model).__name__,' Train Score is   : ' ,model.score(X_train, y_train))\n    print(type(model).__name__,' Test Score is    : ' ,model.score(X_test, y_test))\n    \n    y_pred = model.predict(X_test)\n    print(type(model).__name__,' F1 Score is      : ' ,f1_score(y_test,y_pred))\n    print('--------------------------------------------------------------------------')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style='font-size:25px;font-weight:bold'>Please If you find this kernel useful, upvote it to help others see it ðŸ˜Š</p>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}