{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/input/nlp-getting-started')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('test.csv')\ntrain_data = pd.read_csv('train.csv')\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop(columns=['id', 'keyword', 'location'], inplace=True, axis = 1)\ntest_data.drop(columns=[ 'keyword', 'location'], inplace=True, axis = 1)\ntrain_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['len'] = 0\nj = 0\nfor i in train_data.text:\n    train_data.len[j] = len(i)\n    j +=1\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('maximum length of tweet is' , max(train_data.len))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.distplot(train_data.len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['len'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 10000\nmaxlen = 150\nembedding_size = 200","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntrain_data['text'] = train_data['text'].apply(lambda x: x.lower())\n    \nvocabSize = 10000 #Vocabulary size\ntokenizer = Tokenizer(num_words = vocabSize, split=' ')\ntokenizer.fit_on_texts(train_data['text'].values) #fitting tokenizer based on the text \nX = tokenizer.texts_to_sequences(train_data['text'].values) #converting text to sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pad_sequences(X, maxlen = maxlen)#making sequences of same size by padding with 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_data['target']\ny = np.array(y)\ny.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = len(tokenizer.word_index) + 1\nprint(num_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls ../glove6b200d/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE = '../glove6b200d/glove.6B.200d.txt'\n\nembeddings = {}\nfor o in open(EMBEDDING_FILE):\n    word = o.split(\" \")[0]\n    # print(word)\n    embd = o.split(\" \")[1:]\n    embd = np.asarray(embd, dtype='float32')\n    # print(embd)\n    embeddings[word] = embd\n\n# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((num_words, 200))\n\nfor word, i in tokenizer.word_index.items():\n\tembedding_vector = embeddings.get(word)\n\tif embedding_vector is not None:\n\t\tembedding_matrix[i] = embedding_vector\n\t#if i >= vocabSize - 1:\n\t\t#break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Input, Flatten\n\n\nmodel = Sequential()\nmodel.add(Input(shape=(maxlen,)))\nmodel.add(Embedding(input_dim=num_words, output_dim=200, weights=[embedding_matrix],  trainable=True,  input_length=maxlen))\nmodel.add(Bidirectional(LSTM(units=200)))\nmodel.add(Dense(100, activation=\"softmax\"))\nmodel.add(Dropout(0.15))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation=\"sigmoid\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X, y, batch_size=32, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = tokenizer.texts_to_sequences(test_data['text'].values)\nX_test= pad_sequences(X_test, maxlen = maxlen)\ny_test = (model.predict(X_test) > 0.5).astype(\"int32\")\ny_test=np.array(y_test)\ny_test_id=np.array(test_data['id'].values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = y_test.reshape(-1)\ny_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.DataFrame({'id': y_test_id, 'target': y_test}, columns=['id', 'target'])\ndataset.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.to_csv('/kaggle/working/submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pwd\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}