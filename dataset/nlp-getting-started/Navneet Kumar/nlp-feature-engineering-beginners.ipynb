{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Hello Friends,\n In this kernel my main aim is to make you guys familar with basic nlp techniques and feature engineering with codes and theory.**"},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement"},{"metadata":{},"cell_type":"markdown","source":"Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it’s not always clear whether a person’s words are actually announcing a disaster\n\n> In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t\n\nRead more about this here --> https://www.kaggle.com/c/nlp-getting-started"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\nprint(\"Shape of train dataset is\", train.shape)\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nprint(\"Shape of test dataset is\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# with below method we can display maximum number of rows and columns we want to display.\n\npd.set_option('display.max_rows', 10)\npd.set_option('display.max_columns', 10)\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check for target distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef bar_plot(feature):\n    sns.set(style=\"darkgrid\")\n    ax = sns.countplot(x=feature , data=train)\n    \nprint(\"Total number of different target categories is\", train.target.value_counts().count())\ncount_0 = train.target.value_counts()[0]\ncount_1 = train.target.value_counts()[1]\nprint(\"target with count 1 is {}\".format(count_1))\nprint(\"target with count 0 is {}\".format(count_0))\nbar_plot(\"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total different categories in keyword is :\", train.keyword.value_counts().count())\nprint(\"Total different categories in location is :\", train.location.value_counts().count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking for null values "},{"metadata":{},"cell_type":"markdown","source":"> it always recommended to check total number of null values\n\n> and then we have to decide where we should delete all the null rows or repalce by mean, median, mode or total number of counts"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> From the above we can see that  location contains 2533 null values followed by keywords with 61 null values"},{"metadata":{},"cell_type":"markdown","source":"# Before diving deep into text data lets explore categorical data"},{"metadata":{},"cell_type":"markdown","source":"> Checking not null locations, below I have limited it to 40, you can try will all"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[~train[\"location\"].isna()][\"location\"].tolist()[0:40]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extract country from location"},{"metadata":{},"cell_type":"markdown","source":"> here i am making an extra column named country using geopy,\n\n>  you can play with this geopy library to get latitude and longitude also..\n\n> Comment down how you want to use geopy for this competion ?"},{"metadata":{},"cell_type":"markdown","source":"**let's extract the country name from given location**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import geopy\nimport numpy as np\nimport pycountry\n\nfrom geopy.geocoders import Nominatim\ngeolocator = Nominatim(\"navneet\")\ndef get_location(region=None):\n \n    if region:\n        try:    \n            return geolocator.geocode(region)[0].split(\",\")[-1] \n        except:\n            return region\n    return None\n\ntrain[\"country\"] = train[\"location\"].apply(get_location)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[~train[\"country\"].isna()][\"country\"].tolist()[30:50]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[~train[\"country\"].isna()][\"country\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> there are 86 unique country in dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[~train[\"country\"].isna()][\"country\"].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's play with keyword"},{"metadata":{"trusted":true},"cell_type":"code","source":"set(train[~train[\"keyword\"].isna()][\"keyword\"].tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> In the keywords we can see that few of the words are concatenated with \"%20\". Let's seperate these words"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_keywords(keyword):\n    try:\n        return keyword.split(\"%20\")\n    except:\n        return [keyword]\n    \n\ntrain[\"keyword\"] = train[\"keyword\"].apply(split_keywords)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[~train[\"keyword\"].isna()][\"keyword\"].tolist()[100:110]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Function to check if keywords exist in text or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_keywords_in_text(keywords, text):\n    if not keywords[0]:\n        return 0\n    count = 0\n    for keyword in keywords:\n        each_keyword_count = text.count(str(keyword))\n        count = count + each_keyword_count\n    return count\n\ntrain[\"keyword_count_in_text\"] = train.apply(lambda row: count_keywords_in_text(row[\"keyword\"] , row['text']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**future pending work to be done below**"},{"metadata":{},"cell_type":"markdown","source":"# Let's start doing analysis on text data"},{"metadata":{},"cell_type":"markdown","source":"> Analysing first 100 rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"].tolist()[0:100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> form above we can see that we have #, ==>, ... and a lot of unnecessary words like to, is, are [stopwords], links that needs to be removed"},{"metadata":{},"cell_type":"markdown","source":"> In the below codes we are removing all the website links starting with http: or https:"},{"metadata":{},"cell_type":"markdown","source":" # Count number of #(hash) in a text"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_count_of_hash(text):\n    if not text:\n        return -1\n    return text.count(\"#\")\n\ntrain[\"count_#\"] = train[\"text\"].apply(get_count_of_hash)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Count number of @(at the rate) in a text"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_count_of_at_rate(text):\n    if not text:\n        return -1\n    return text.count(\"@\")\n\ntrain[\"count_@\"] = train[\"text\"].apply(get_count_of_at_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"count_@\"].to_list()[100:110]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Since this is twitter text, so counting number of hashes becomes more important"},{"metadata":{},"cell_type":"markdown","source":"**Remove website links**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nprint(\"Before---------\")\nprint(train[\"text\"].tolist()[31])\n\ntrain['text'] = train['text'].str.replace('http:\\S+', '', case=False)\ntrain['text'] = train['text'].str.replace('https:\\S+', '', case=False)\nprint(\"After----------\")\nprint(train[\"text\"].tolist()[31])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> this way we can remove all website links"},{"metadata":{},"cell_type":"markdown","source":"**Removing all punctuations except hash**"},{"metadata":{},"cell_type":"markdown","source":"> punctuations should be removed because it doesnot add much value "},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nexclude = set(string.punctuation)\nexclude_hash = {\"#\"}\nexclude = exclude - exclude_hash\nprint(\"Length of punctuations to be excluded :\",len(exclude))\n\nprint(\"Before---------\")\nprint(train[\"text\"].tolist()[0])\n\nfor punctuation in exclude:\n  train['text'] = train['text'].str.replace(punctuation, '', regex=True)\n\nprint(\"After----------\")\nprint(train[\"text\"].tolist()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing all the stop words"},{"metadata":{},"cell_type":"markdown","source":"> here i am adding stop words from two different package.\n\n> you can check all the stop words by running below code."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nfrom stop_words import get_stop_words\nfrom nltk.corpus import stopwords\n\nstop_words = list(get_stop_words('en'))         #About 900 stopwords\nnltk_words = list(stopwords.words('english')) #About 179 stopwords\nstop_words = sorted(set(stop_words).union(set(nltk_words)) - exclude_hash)  # removing hash from stop words\n\nprint(\"total stop words to be removed :\", len(stop_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Before--------\")\nprint(train[\"text\"].tolist()[0])\npreprocessed_text = []\n# tqdm is for printing the status bar\nfor sentance in train['text'].values:\n    sent = ' '.join(e for e in sentance.split() if e not in stop_words)\n    preprocessed_text.append(sent.lower().strip())\n\ntrain[\"text\"] = preprocessed_text\nprint(\"After----------\")\nprint(train[\"text\"].tolist()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> from above we can see that all the stop words like [are, of, this] has been removed."},{"metadata":{},"cell_type":"markdown","source":"# Since dataset is very less so lets create out own w2v embedding"},{"metadata":{},"cell_type":"markdown","source":"**Lemmatise the words with spacy**"},{"metadata":{},"cell_type":"markdown","source":"**Why lemmatisation ?**\n\n> lemmatisation is done on text data to get the lemma of that word.. Ex : stops -- > stop"},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\n# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\nprint(\"Before--------\")\nprint(train[\"text\"].tolist()[2])\n\nlemet_text = []\n# tqdm is for printing the status bar\nfor sentance in train['text'].values:\n    sent = \" \".join([token.lemma_ for token in nlp(sentance)])\n    lemet_text.append(sent.lower().strip())\n\ntrain[\"text\"] = lemet_text\n\ntrain[\"text\"] = lemet_text\nprint(\"After----------\")\nprint(train[\"text\"].tolist()[2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Toekenizing the data**"},{"metadata":{},"cell_type":"markdown","source":"> tokenization is needed for making w2v models.\n\n> \"my name is navneet\" --> after tokenization --> [\"my\", \"name\", \"is\", \"navneet\"]"},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('punkt')\n\ntrain['text'] = train.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\ntrain[\"text\"].tolist()[2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**converting the data into vector forms using Word2Vec with vector size of 300**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\n# train model\nmodel = Word2Vec(train.text.values, min_count=1, size = 300)\n\n# summarize vocabulary\nwords = list(model.wv.vocab)\n#print(words)\n\n# save model\nmodel.save('model.bin')\n# load model\nnew_model = Word2Vec.load('model.bin')\nprint(new_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.most_similar('disaster', topn = 20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Work in progress, please upvote this kernel if you like my work and comment if i made any mistake."},{"metadata":{},"cell_type":"markdown","source":"**Future work**\n\n> different types of text embedding like countvectorizer, tfidf etc.\n\n> some more feature engineering and cleaning\n\n> differnt types of models like naive bayes, logistic, lightgbm"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}