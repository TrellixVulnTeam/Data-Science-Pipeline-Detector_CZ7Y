{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tensorflow hub\nimport tensorflow_hub as hub\n# tensor flow module\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\n# matplotlib\nfrom matplotlib import colors\nfrom matplotlib import pyplot as plt\n\n# word vectorizor\n# first converts the text into a matrix of word counts\n# then transforms these counts by normalizing them based on the term frequency\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# used to create word encoders\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Background\nIn this competition we have a keyword(sometimes), location(sometimes), and text to represent a Tweet. Our goal is to predict whether the tweet is describing a real disaster (1) or not (0).\n\nIn this kernel I am going to focus on the text and keyword features to predict a target. First, I must convert the text into vectors to use in my model. I will use a multinominal Naive Bayes classifier to build my model given the text features. Then I will use Naive Bayes to get predictions based on keywords."},{"metadata":{},"cell_type":"markdown","source":"Tensorflow code is inspired by github repo:\nhttps://github.com/nicolov/naive_bayes_tensorflow/blob/master/tf_iris.py\n"},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{},"cell_type":"markdown","source":"read in data to pandas dataframes"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory\nI just want to get a general idea of the data before we predict the targets"},{"metadata":{},"cell_type":"markdown","source":"count how many of each target is found in the training set "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby(\"target\")[\"id\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is amazing! **Our training set has no null targets and our targets are fairly balanced as well!**"},{"metadata":{},"cell_type":"markdown","source":"How many unique locations are duplicated in the tweets of the tweets that contain locations?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test_df.loc[test_df[\"location\"].notnull(),\"location\"]))\nprint(len(test_df.loc[test_df[\"location\"].notnull(),\"location\"].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it looks like not even half match a previous tweet. We may eventually have to categorize locations using a different technique than string matching. For example, it may be useful to categorize cities verse suburban areas rather than just the city names."},{"metadata":{},"cell_type":"markdown","source":"What about the key words?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test_df.loc[test_df[\"keyword\"].notnull(),\"keyword\"]))\nprint(len(test_df.loc[test_df[\"keyword\"].notnull(),\"keyword\"].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These may actually be useful in a naive bayes model since each key word will have a conditional probability."},{"metadata":{},"cell_type":"markdown","source":"# Tweet Text: Sentence Embedding"},{"metadata":{},"cell_type":"markdown","source":"Next we can convert each of the texts into vectors using a TensorFlow universal-sentence-encoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/3\")\nX_train_embeddings = embed(train_df[\"text\"].values)\nX_test_embeddings = embed(test_df[\"text\"].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tensor Flow Naiver Bayes"},{"metadata":{},"cell_type":"markdown","source":"Lets create numpy matricies for the features in the training and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_matrix = X_train_embeddings['outputs'].numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_matrix = X_test_embeddings['outputs'].numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target is my y variable for my model."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = tf.constant(train_df[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First let's create a python object that can fit and predict a naive bayes classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TFNaiveBayesClassifier:\n    dist = None\n    \n    # X is the matrix containing the vectors for each sentence\n    # y is the list target values in the same order as the X matrix\n    def fit(self, X, y):\n        unique_y = np.unique(y) # unique target values: 0,1\n        print(unique_y)\n        # `points_by_class` is a numpy array the size of \n        # the number of unique targets.\n        # in each item of the list is another list that contains the vector\n        # of each sentence from the same target value\n        points_by_class = np.asarray([np.asarray(\n            [np.asarray(\n                X.iloc[x,:]) for x in range(0,len(y)) if y[x] == c]) for c in unique_y])\n        mean_list=[]\n        var_list=[]\n        for i in range(0, len(points_by_class)):\n            mean_var, var_var = tf.nn.moments(tf.constant(points_by_class[i]), axes=[0])\n            mean_list.append(mean_var)\n            var_list.append(var_var)\n        mean=tf.stack(mean_list, 0)\n        var=tf.stack(var_list, 0)\n        # Create a 3x2 univariate normal distribution with the \n        # known mean and variance\n        self.dist = tfp.distributions.Normal(loc=mean, scale=tf.sqrt(var))\n        \n    def predict(self, X):\n        assert self.dist is not None\n        nb_classes, nb_features = map(int, self.dist.scale.shape)\n\n        # uniform priors\n        priors = np.log(np.array([1. / nb_classes] * nb_classes))\n        \n        # Conditional probabilities log P(x|c)\n        # (nb_samples, nb_classes, nb_features)\n        all_log_probs = self.dist.log_prob(\n            tf.reshape(\n                tf.tile(X, [1, nb_classes]), [-1, nb_classes, nb_features]))\n        # (nb_samples, nb_classes)\n        cond_probs = tf.reduce_sum(all_log_probs, axis=2)\n        \n        # posterior log probability, log P(c) + log P(x|c)\n        joint_likelihood = tf.add(priors, cond_probs)\n\n        # normalize to get (log)-probabilities\n        norm_factor = tf.reduce_logsumexp(\n            joint_likelihood, axis=1, keepdims=True)\n        log_prob = joint_likelihood - norm_factor\n        # exp to get the actual probabilities\n        return tf.exp(log_prob)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we initialize our naive bayes model and fit it using the training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_nb = TFNaiveBayesClassifier()\ntf_nb.fit(pd.DataFrame(X_train_matrix),\n          y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"predict probability of each target values in the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = tf_nb.predict(X_test_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a dataframe containing the probability of each target given the text in each tweet."},{"metadata":{"trusted":true},"cell_type":"code","source":"predProbGivenText_df = pd.DataFrame(y_pred.numpy())\npredProbGivenText_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keyword Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"Let's look at our unique key words."},{"metadata":{"trusted":true},"cell_type":"code","source":"uniq_keywords = train_df[\"keyword\"].unique()[1:]\nprint(len(uniq_keywords))\nprint(uniq_keywords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I noticed that there are some keywords that are very similar to each other so I decided to manually make them the same word."},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_keywords(df_og):\n    df = df_og.copy()\n    df[\"keyword\"] = df[\"keyword\"].replace(\"ablaze\",\"blaze\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"blazing\",\"blaze\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"annihilated\",\"annihilation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"attacked\",\"attack\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bioterror\",\"bioterrorism\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"blown%20up\",\"blew%20up\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bloody\",\"blood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bleeding\",\"blood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"body%20bags\",\"body%20bag\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"body%20bagging\",\"body%20bag\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bombed\",\"bomb\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bombing\",\"bomb\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"burning%20buildings\",\"buildings%20burning\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"buildings%20on%20fire\",\"buildings%20burning\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"burned\",\"burning\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"casualties\",\"casualty\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"catastrophe\",\"catastrophic\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"collapse\",\"collapsed\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"collide\",\"collision\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"collided\",\"collision\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"crash\",\"crashed\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"crush\",\"crushed\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"dead\",\"death\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"deaths\",\"death\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"deluge\",\"deluged\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"demolished\",\"demolish\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"demolition\",\"demolish\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"derailment\",\"derail\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"derailed\",\"derail\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"desolation\",\"desolate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"destroyed\",\"destroy\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"destruction\",\"destroy\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"detonate\",\"detonation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"devastated\",\"devastation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"drowned\",\"drown\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"drowning\",\"drown\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"electrocute\",\"electrocuted\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"evacuated\",\"evacuate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"evacuation\",\"evacuate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"explode\",\"explosion\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"exploded\",\"explosion\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"fatality\",\"fatalities\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"floods\",\"flood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"flooding\",\"flood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bush%20fires\",\"forest%20fire\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"forest%20fires\",\"forest%20fire\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hailstorm\",\"hail\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hazardous\",\"hazard\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hijacking\",\"hijack\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hijacker\",\"hijack\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hostages\",\"hostage\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"injured\",\"injury\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"injures\",\"injury\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"inundated\",\"inundation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"mass%20murderer\",\"mass%20murder\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"obliterated\",\"obliterate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"obliteration\",\"obliterate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"panicking\",\"panic\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"quarantined\",\"quarantine\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"rescuers\",\"rescue\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"rescued\",\"rescue\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"rioting\",\"riot\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"dust%20storm\",\"sandstorm\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"screamed\",\"screams\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"screaming\",\"screams\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"sirens\",\"siren\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"suicide%20bomb\",\"suicide%20bomber\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"suicide%20bombing\",\"suicide%20bomber\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"survived\",\"survive\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"survivors\",\"survive\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"terrorism\",\"terrorist\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"thunderstorm\",\"thunder\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"traumatised\",\"trauma\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"twister\",\"tornado\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"typhoon\",\"hurricane\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"weapons\",\"weapon\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wild%20fires\",\"wildfire\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wounded\",\"wounds\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wrecked\",\"wreckage\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wreck\",\"wreckage\")\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = replace_keywords(train_df)\ntest_df = replace_keywords(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the updated set of keywords let's get the probability for each target given a specific key word."},{"metadata":{"trusted":true},"cell_type":"code","source":"uniq_keywords = train_df[\"keyword\"].unique()[1:]\nkword_resArr = []\nprint(len(uniq_keywords))\nfor kword in uniq_keywords:\n    kword_df = train_df.loc[train_df[\"keyword\"] == kword,: ]\n    total_kword = float(len(kword_df))\n    target0_n = float(len(kword_df.loc[kword_df[\"target\"]==0,:]))\n    target1_n = float(len(kword_df.loc[kword_df[\"target\"]==1,:]))\n    kword_prob_df = pd.DataFrame({'keyword':[kword],\n                                 \"keywordPred0\": [target0_n/total_kword],\n                                 \"keywordPred1\": [target1_n/total_kword]})\n    kword_resArr.append(kword_prob_df)\npredProbGivenKeyWord_df= pd.concat(kword_resArr)\npredProbGivenKeyWord_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create submission file"},{"metadata":{},"cell_type":"markdown","source":"get probabilities given the tweet text"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"textprob0\"]=predProbGivenText_df.loc[:,0].copy()\ntest_df[\"textprob1\"]=predProbGivenText_df.loc[:,1].copy()\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get the probabilities given the key words. Note that if there is no key word than the probability is 50% for both targets."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_df.merge(predProbGivenKeyWord_df, how='left', on=\"keyword\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"keywordPred0\"]=test_df[\"keywordPred0\"].fillna(0.5)\ntest_df[\"keywordPred1\"]=test_df[\"keywordPred1\"].fillna(0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's calculate the probability given the text and the keyword. Then let's choose our prediction based on which target has the higher probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"pred0\"]=test_df[\"textprob0\"]*test_df[\"keywordPred0\"]\ntest_df[\"pred1\"]=test_df[\"textprob1\"]*test_df[\"keywordPred1\"]\ntest_df[\"target\"]=test_df[\"pred1\"]>test_df[\"pred0\"]\ntest_df[\"target\"] = test_df[\"target\"].astype(np.int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = test_df.loc[:,[\"id\",\"target\"]]\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that in the code above I only look at the tweets themselves and the keywords, but many of the tweets do not have keywords. We could maybe create a model to infer keywords from tweets that do not have keywords. We also did not include location in the model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}