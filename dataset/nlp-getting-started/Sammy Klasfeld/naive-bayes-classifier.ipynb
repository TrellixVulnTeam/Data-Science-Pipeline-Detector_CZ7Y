{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word vectorizor\n# first converts the text into a matrix of word counts\n# then transforms these counts by normalizing them based on the term frequency\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Multinomial Naive Bayes Classifier\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Background\nIn this competition we are a keyword(sometimes), location(sometimes), and text to represent a Tweet. Our goal is to predict whether the tweet is describing a real disaster (1) or not (0).\n\nIn this kernel I am only going to focus on the text and keyword features to predict a target. First, I must convert the text into vectors to use in my model. I will use a Naive Bayes classifiers to build my model."},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{},"cell_type":"markdown","source":"read in data to pandas dataframes"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"count how many of each target is found in the training set "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby(\"target\")[\"id\"].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is amazing! Our training set has no null targets and our targets are fairly balanced as well!"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text: Multinominal Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_df[\"text\"].copy()\nX_test = test_df[\"text\"].copy()\ny_train = train_df[\"target\"].copy()\n\ntxt_vectorizer = TfidfVectorizer()\nX_train_tf = txt_vectorizer.fit_transform(X_train)\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\nX_test_tf = txt_vectorizer.transform(X_test)\n\n# Fit a Naive Base classifier to the training set\nmultinominalNB_clf = MultinomialNB().fit(X_train_tf, y_train)\n\n# get predicted values\ntest_df.loc[:,\"target\"] = multinominalNB_clf.predict(X_test_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predProbGivenText_df = pd.DataFrame(multinominalNB_clf.predict_proba(X_test_tf))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Keyword: Naive Bayes"},{"metadata":{},"cell_type":"markdown","source":"Let's look at our unique key words."},{"metadata":{"trusted":true},"cell_type":"code","source":"uniq_keywords = train_df[\"keyword\"].unique()[1:]\nprint(len(uniq_keywords))\nprint(uniq_keywords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I noticed that there are some keywords that are very similar to each other so I decided to manually make them the same word."},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_keywords(df_og):\n    df = df_og.copy()\n    df[\"keyword\"] = df[\"keyword\"].replace(\"ablaze\",\"blaze\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"blazing\",\"blaze\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"annihilated\",\"annihilation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"attacked\",\"attack\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bioterror\",\"bioterrorism\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"blown%20up\",\"blew%20up\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bloody\",\"blood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bleeding\",\"blood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"body%20bags\",\"body%20bag\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"body%20bagging\",\"body%20bag\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bombed\",\"bomb\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bombing\",\"bomb\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"burning%20buildings\",\"buildings%20burning\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"buildings%20on%20fire\",\"buildings%20burning\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"burned\",\"burning\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"casualties\",\"casualty\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"catastrophe\",\"catastrophic\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"collapse\",\"collapsed\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"collide\",\"collision\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"collided\",\"collision\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"crash\",\"crashed\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"crush\",\"crushed\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"dead\",\"death\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"deaths\",\"death\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"deluge\",\"deluged\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"demolished\",\"demolish\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"demolition\",\"demolish\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"derailment\",\"derail\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"derailed\",\"derail\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"desolation\",\"desolate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"destroyed\",\"destroy\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"destruction\",\"destroy\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"detonate\",\"detonation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"devastated\",\"devastation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"drowned\",\"drown\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"drowning\",\"drown\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"electrocute\",\"electrocuted\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"evacuated\",\"evacuate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"evacuation\",\"evacuate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"explode\",\"explosion\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"exploded\",\"explosion\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"fatality\",\"fatalities\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"floods\",\"flood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"flooding\",\"flood\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"bush%20fires\",\"forest%20fire\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"forest%20fires\",\"forest%20fire\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hailstorm\",\"hail\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hazardous\",\"hazard\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hijacking\",\"hijack\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hijacker\",\"hijack\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"hostages\",\"hostage\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"injured\",\"injury\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"injures\",\"injury\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"inundated\",\"inundation\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"mass%20murderer\",\"mass%20murder\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"obliterated\",\"obliterate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"obliteration\",\"obliterate\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"panicking\",\"panic\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"quarantined\",\"quarantine\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"rescuers\",\"rescue\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"rescued\",\"rescue\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"rioting\",\"riot\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"dust%20storm\",\"sandstorm\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"screamed\",\"screams\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"screaming\",\"screams\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"sirens\",\"siren\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"suicide%20bomb\",\"suicide%20bomber\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"suicide%20bombing\",\"suicide%20bomber\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"survived\",\"survive\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"survivors\",\"survive\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"terrorism\",\"terrorist\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"thunderstorm\",\"thunder\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"traumatised\",\"trauma\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"twister\",\"tornado\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"typhoon\",\"hurricane\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"weapons\",\"weapon\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wild%20fires\",\"wildfire\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wounded\",\"wounds\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wrecked\",\"wreckage\")\n    df[\"keyword\"] = df[\"keyword\"].replace(\"wreck\",\"wreckage\")\n    return(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = replace_keywords(train_df)\ntest_df = replace_keywords(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the updated set of keywords let's get the probability for each target given a specific key word."},{"metadata":{"trusted":true},"cell_type":"code","source":"uniq_keywords = train_df[\"keyword\"].unique()[1:]\nkword_resArr = []\nprint(len(uniq_keywords))\nfor kword in uniq_keywords:\n    kword_df = train_df.loc[train_df[\"keyword\"] == kword,: ]\n    total_kword = float(len(kword_df))\n    target0_n = float(len(kword_df.loc[kword_df[\"target\"]==0,:]))\n    target1_n = float(len(kword_df.loc[kword_df[\"target\"]==1,:]))\n    kword_prob_df = pd.DataFrame({'keyword':[kword],\n                                 \"keywordPred0\": [target0_n/total_kword],\n                                 \"keywordPred1\": [target1_n/total_kword]})\n    kword_resArr.append(kword_prob_df)\npredProbGivenKeyWord_df= pd.concat(kword_resArr)\npredProbGivenKeyWord_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"get probabilities given the tweet text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"textprob0\"]=predProbGivenText_df.loc[:,0].copy()\ntest_df[\"textprob1\"]=predProbGivenText_df.loc[:,1].copy()\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get the probabilities given the key words. Note that if there is no key word than the probability is 50% for both targets."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = test_df.merge(predProbGivenKeyWord_df, how='left', on=\"keyword\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"keywordPred0\"]=test_df[\"keywordPred0\"].fillna(0.5)\ntest_df[\"keywordPred1\"]=test_df[\"keywordPred1\"].fillna(0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's calculate the probability given the text and the keyword. Then let's choose our prediction based on which target has the higher probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[\"pred0\"]=test_df[\"textprob0\"]*test_df[\"keywordPred0\"]\ntest_df[\"pred1\"]=test_df[\"textprob1\"]*test_df[\"keywordPred1\"]\ntest_df[\"target\"]=test_df[\"pred1\"]>test_df[\"pred0\"]\ntest_df[\"target\"] = test_df[\"target\"].astype(np.int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = test_df.loc[:,[\"id\",\"target\"]].copy()\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that in the code above I only look at the tweets themselves and the keywords, but many of the tweets do not have keywords. We could maybe create a model to infer keywords from tweets that do not have keywords. We also did not include location in the model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}