{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-01T17:42:49.033125Z","iopub.execute_input":"2021-06-01T17:42:49.033505Z","iopub.status.idle":"2021-06-01T17:42:49.058112Z","shell.execute_reply.started":"2021-06-01T17:42:49.03347Z","shell.execute_reply":"2021-06-01T17:42:49.057117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import various libraries and modules","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib as mlt\nimport re\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nnltk.download('stopwords')\nfrom sklearn.model_selection import train_test_split\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:42:49.059719Z","iopub.execute_input":"2021-06-01T17:42:49.060024Z","iopub.status.idle":"2021-06-01T17:42:49.067506Z","shell.execute_reply.started":"2021-06-01T17:42:49.059995Z","shell.execute_reply":"2021-06-01T17:42:49.066703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train and test data is read using pandas","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:42:49.069531Z","iopub.execute_input":"2021-06-01T17:42:49.070392Z","iopub.status.idle":"2021-06-01T17:42:49.129524Z","shell.execute_reply.started":"2021-06-01T17:42:49.070349Z","shell.execute_reply":"2021-06-01T17:42:49.128514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:42:49.131114Z","iopub.execute_input":"2021-06-01T17:42:49.131419Z","iopub.status.idle":"2021-06-01T17:42:49.144505Z","shell.execute_reply.started":"2021-06-01T17:42:49.131389Z","shell.execute_reply":"2021-06-01T17:42:49.1433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:42:49.146601Z","iopub.execute_input":"2021-06-01T17:42:49.147318Z","iopub.status.idle":"2021-06-01T17:42:49.179224Z","shell.execute_reply.started":"2021-06-01T17:42:49.147252Z","shell.execute_reply":"2021-06-01T17:42:49.177994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"we could see from above data frame is that the tweet data is not much cleaned.\nWhat is meaning of cleaned data then?\nfor machine to convert each word as a vector it has to know the meaning of that word or we can say it will present is dictonary of library,So we will only takes those word which machines knows, not some random slangs.","metadata":{}},{"cell_type":"markdown","source":"here we will use regular expression to remove special symbols, also use NLTK stemming,lowercase words etc etc we will clean the tweet texts.","metadata":{}},{"cell_type":"code","source":"#text_cleaning(df):\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:42:49.180914Z","iopub.execute_input":"2021-06-01T17:42:49.181735Z","iopub.status.idle":"2021-06-01T17:42:49.194975Z","shell.execute_reply.started":"2021-06-01T17:42:49.181679Z","shell.execute_reply":"2021-06-01T17:42:49.194152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the function I used to cleaned the data","metadata":{}},{"cell_type":"code","source":"def cleaningfun(df):\n    corpus=[]\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    for sent in list(df['text']):\n    \n        sent=re.sub(\"\\w*\\@\\w*\", \"\", sent)\n        sent=emoji_pattern.sub(r'', sent)\n        sent=re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \",sent)\n    \n        sent=re.sub(r'<.*?>','',sent)\n        sent=re.sub(\"[^a-zA-Z]+\",\" \",sent)\n        sent=sent.lower()\n        sent=sent.split()\n        sent=[ ps.stem(word) for word in sent if not word in stopwords.words('english')]\n        sent = ' '.join(sent)\n \n    \n        corpus.append(sent)\n    return corpus\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:42:49.196023Z","iopub.execute_input":"2021-06-01T17:42:49.196454Z","iopub.status.idle":"2021-06-01T17:42:49.212973Z","shell.execute_reply.started":"2021-06-01T17:42:49.196423Z","shell.execute_reply":"2021-06-01T17:42:49.212162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Applied on Test and Train data","metadata":{}},{"cell_type":"code","source":"corpus_train=cleaningfun(train)\ncorpus_test=cleaningfun(test)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:42:49.214562Z","iopub.execute_input":"2021-06-01T17:42:49.214974Z","iopub.status.idle":"2021-06-01T17:43:11.897326Z","shell.execute_reply.started":"2021-06-01T17:42:49.214942Z","shell.execute_reply":"2021-06-01T17:43:11.896453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_train=pd.DataFrame(corpus_train,index=train.id,columns=['text'])\nhidden_test=pd.DataFrame(corpus_test,index=test.id,columns=['text'])\nhidden_train","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:43:11.89894Z","iopub.execute_input":"2021-06-01T17:43:11.899592Z","iopub.status.idle":"2021-06-01T17:43:11.916999Z","shell.execute_reply.started":"2021-06-01T17:43:11.899543Z","shell.execute_reply":"2021-06-01T17:43:11.915749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We cannot perform classification operation on directly on text data because all the operation use mathamatical concepts for which we required numbers or vectors to be precise. We use TFIDF vectoriser here imported from SKlearn","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_train = TfidfVectorizer(max_features = 2500)\nMat_train= vectorizer_train.fit_transform(corpus_train)\n#len(vectorizer.get_feature_names())\ntfidf_tokens_train = vectorizer_train.get_feature_names()\nMat_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:43:11.919253Z","iopub.execute_input":"2021-06-01T17:43:11.919685Z","iopub.status.idle":"2021-06-01T17:43:12.081625Z","shell.execute_reply.started":"2021-06-01T17:43:11.919649Z","shell.execute_reply":"2021-06-01T17:43:12.080537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"7613 rows and 2500 columns, these 7613 rows are the text sentences that we have given and the label of each columns is the word","metadata":{}},{"cell_type":"markdown","source":"The vectoriser is fit with only Train dataset and we will transform both training and testing data with it, do not perform fit operation on testing data, if performed fit on test data we will get different no. of columns from training data and then the classifier will give error.","metadata":{}},{"cell_type":"code","source":"\nMat_test= vectorizer_train.transform(corpus_test)\n#len(vectorizer.get_feature_names())\n#tfidf_tokens_test = vectorizer_test.get_feature_names()\nMat_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:43:12.083747Z","iopub.execute_input":"2021-06-01T17:43:12.084219Z","iopub.status.idle":"2021-06-01T17:43:12.143952Z","shell.execute_reply.started":"2021-06-01T17:43:12.084152Z","shell.execute_reply":"2021-06-01T17:43:12.14285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lets define the X and target y","metadata":{}},{"cell_type":"code","source":"X = pd.DataFrame(data = Mat_train.toarray(),index =train.id,columns = tfidf_tokens_train)\nX","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:43:12.145242Z","iopub.execute_input":"2021-06-01T17:43:12.145578Z","iopub.status.idle":"2021-06-01T17:43:12.368947Z","shell.execute_reply.started":"2021-06-01T17:43:12.14555Z","shell.execute_reply":"2021-06-01T17:43:12.368161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data=pd.DataFrame(data = Mat_test.toarray(),index =test.id,columns = tfidf_tokens_train)\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:43:12.370376Z","iopub.execute_input":"2021-06-01T17:43:12.370994Z","iopub.status.idle":"2021-06-01T17:43:12.470917Z","shell.execute_reply.started":"2021-06-01T17:43:12.370958Z","shell.execute_reply":"2021-06-01T17:43:12.470017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=train['target']","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:43:12.472085Z","iopub.execute_input":"2021-06-01T17:43:12.472467Z","iopub.status.idle":"2021-06-01T17:43:12.476502Z","shell.execute_reply.started":"2021-06-01T17:43:12.472421Z","shell.execute_reply":"2021-06-01T17:43:12.475698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will do train validation data split ","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=20)\nX_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:43:12.478636Z","iopub.execute_input":"2021-06-01T17:43:12.479233Z","iopub.status.idle":"2021-06-01T17:43:12.698788Z","shell.execute_reply.started":"2021-06-01T17:43:12.479168Z","shell.execute_reply":"2021-06-01T17:43:12.697562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have used here Naive bais as Classification algo. here and accuracy of model is predicted using validation data","metadata":{}},{"cell_type":"code","source":"# training the model on training set\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n  \n# making predictions on the testing set\ny_pred = gnb.predict(X_val)\n  \n# comparing actual response values (y_test) with predicted response values (y_pred)\nfrom sklearn import metrics\nprint(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_val, y_pred)*100)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:43:12.700238Z","iopub.execute_input":"2021-06-01T17:43:12.700732Z","iopub.status.idle":"2021-06-01T17:43:13.065657Z","shell.execute_reply.started":"2021-06-01T17:43:12.700689Z","shell.execute_reply":"2021-06-01T17:43:13.064626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trained model used to predict test data targets also and save them","metadata":{}},{"cell_type":"code","source":"y_test = gnb.predict(test_data)\ny_test","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:43:13.066974Z","iopub.execute_input":"2021-06-01T17:43:13.067293Z","iopub.status.idle":"2021-06-01T17:43:13.236015Z","shell.execute_reply.started":"2021-06-01T17:43:13.067259Z","shell.execute_reply":"2021-06-01T17:43:13.234857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\n    'id':test.id,\n    'target':y_test\n}).to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T17:43:13.237306Z","iopub.execute_input":"2021-06-01T17:43:13.237602Z","iopub.status.idle":"2021-06-01T17:43:13.250689Z","shell.execute_reply.started":"2021-06-01T17:43:13.237573Z","shell.execute_reply":"2021-06-01T17:43:13.249674Z"},"trusted":true},"execution_count":null,"outputs":[]}]}