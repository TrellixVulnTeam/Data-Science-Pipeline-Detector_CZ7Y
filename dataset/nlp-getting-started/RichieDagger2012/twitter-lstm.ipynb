{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Twitter Sentiment Analysis","metadata":{}},{"cell_type":"markdown","source":"## 1. Import necessary libraries:","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport tensorflow.keras as keras \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout \nprint('You are using TensorFlow version: ',tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:16.154688Z","iopub.execute_input":"2022-01-13T16:12:16.155365Z","iopub.status.idle":"2022-01-13T16:12:23.180233Z","shell.execute_reply.started":"2022-01-13T16:12:16.155325Z","shell.execute_reply":"2022-01-13T16:12:23.179346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use this to stretch the dataframe view\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.181826Z","iopub.execute_input":"2022-01-13T16:12:23.182097Z","iopub.status.idle":"2022-01-13T16:12:23.187936Z","shell.execute_reply.started":"2022-01-13T16:12:23.182065Z","shell.execute_reply":"2022-01-13T16:12:23.18726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Prepare the data:\n\nThe keyword and location columns are not needed, so we remove them","metadata":{}},{"cell_type":"code","source":"root_folder = '/kaggle/input/nlp-getting-started/'","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.189035Z","iopub.execute_input":"2022-01-13T16:12:23.189811Z","iopub.status.idle":"2022-01-13T16:12:23.219059Z","shell.execute_reply.started":"2022-01-13T16:12:23.189759Z","shell.execute_reply":"2022-01-13T16:12:23.218062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(root_folder+'train.csv', sep=',', encoding='ISO-8859-1')\ndf_test = pd.read_csv(root_folder+'test.csv', sep=',', encoding='ISO-8859-1')\ndf_train.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.220872Z","iopub.execute_input":"2022-01-13T16:12:23.22152Z","iopub.status.idle":"2022-01-13T16:12:23.318682Z","shell.execute_reply.started":"2022-01-13T16:12:23.22148Z","shell.execute_reply":"2022-01-13T16:12:23.317719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of empty entries: \",np.sum(df_train.isnull().any(axis=1)))","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.319851Z","iopub.execute_input":"2022-01-13T16:12:23.320456Z","iopub.status.idle":"2022-01-13T16:12:23.328599Z","shell.execute_reply.started":"2022-01-13T16:12:23.320418Z","shell.execute_reply":"2022-01-13T16:12:23.327745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're','s', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n\nSTOPWORDS = set(stopwordlist)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.329763Z","iopub.execute_input":"2022-01-13T16:12:23.330172Z","iopub.status.idle":"2022-01-13T16:12:23.33967Z","shell.execute_reply.started":"2022-01-13T16:12:23.330139Z","shell.execute_reply":"2022-01-13T16:12:23.338701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#clean data by removing special symbols (like # and others), URL's and stop-words:\n\ndef cleaning_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndef remove_urls(text):\n    new_text = re.sub(r'http?:\\/\\/.*[\\r\\n]*', \"\", text)\n    new_text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', \"\", new_text)\n    #new_text = re.sub(r'@[a-zA-Z0-9]', \"\", new_text)\n    new_text = ' '.join(x for x in new_text.split() if not x.startswith('@'))\n    return new_text.casefold().strip()\n\ndef remove_specials(text):\n    #new_text = re.sub(r\"[^a-zA-Z0-9\\s]\",'',text)\n    new_text = re.sub(r\"[^a-zA\\s]\",'',text)\n    new_text=new_text.replace(\"#\",\"\").strip()\n    return new_text.strip()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.341008Z","iopub.execute_input":"2022-01-13T16:12:23.341797Z","iopub.status.idle":"2022-01-13T16:12:23.360165Z","shell.execute_reply.started":"2022-01-13T16:12:23.341749Z","shell.execute_reply":"2022-01-13T16:12:23.359326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['text'] = df_train.text.apply(remove_urls).dropna()\ndf_test['text'] = df_test.text.apply(remove_urls)\ndf_train.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.361616Z","iopub.execute_input":"2022-01-13T16:12:23.362139Z","iopub.status.idle":"2022-01-13T16:12:23.470191Z","shell.execute_reply.started":"2022-01-13T16:12:23.362092Z","shell.execute_reply":"2022-01-13T16:12:23.469285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['text'] = df_train.text.apply(remove_specials)\ndf_test['text'] = df_test.text.apply(remove_specials)\ndf_train.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.471389Z","iopub.execute_input":"2022-01-13T16:12:23.471648Z","iopub.status.idle":"2022-01-13T16:12:23.529014Z","shell.execute_reply.started":"2022-01-13T16:12:23.471618Z","shell.execute_reply":"2022-01-13T16:12:23.527577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['text'] = df_train.text.apply(lambda x: cleaning_stopwords(x))\ndf_test['text'] = df_test.text.apply(lambda x: cleaning_stopwords(x))\ndf_train = df_train.drop(['keyword','location'], axis=1)\ndf_test = df_test.drop(['keyword','location'], axis=1)\ndf_train.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.532272Z","iopub.execute_input":"2022-01-13T16:12:23.532946Z","iopub.status.idle":"2022-01-13T16:12:23.593823Z","shell.execute_reply.started":"2022-01-13T16:12:23.532902Z","shell.execute_reply":"2022-01-13T16:12:23.592862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Setting the training and test variables:","metadata":{}},{"cell_type":"code","source":"x = df_train.sort_values(by= ['id'], ascending=True)\nx_test = df_test.sort_values(by= ['id'], ascending=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.595149Z","iopub.execute_input":"2022-01-13T16:12:23.596126Z","iopub.status.idle":"2022-01-13T16:12:23.606451Z","shell.execute_reply.started":"2022-01-13T16:12:23.596064Z","shell.execute_reply":"2022-01-13T16:12:23.605273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Retrieve the text from the dataframe as a numpy array\ntwitts = x.loc[:,x.columns=='text'].values.flatten()\ntwitts_test = x_test.loc[:,x_test.columns=='text'].values.flatten()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.608723Z","iopub.execute_input":"2022-01-13T16:12:23.609157Z","iopub.status.idle":"2022-01-13T16:12:23.626157Z","shell.execute_reply.started":"2022-01-13T16:12:23.609102Z","shell.execute_reply":"2022-01-13T16:12:23.625324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see a sample of the collected twitts:\nprint(twitts[:10])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.628441Z","iopub.execute_input":"2022-01-13T16:12:23.629049Z","iopub.status.idle":"2022-01-13T16:12:23.648473Z","shell.execute_reply.started":"2022-01-13T16:12:23.629002Z","shell.execute_reply":"2022-01-13T16:12:23.647489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Applying NLP:\nWe need to use the usual tokenization techniques in order to translate the text data to numerical vectors that we can feed to a neural network.","metadata":{}},{"cell_type":"code","source":"t = Tokenizer()\ndef preprocess(text):\n    seqs = t.fit_on_texts(text)\n    return seqs","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.650898Z","iopub.execute_input":"2022-01-13T16:12:23.651657Z","iopub.status.idle":"2022-01-13T16:12:23.659283Z","shell.execute_reply.started":"2022-01-13T16:12:23.651604Z","shell.execute_reply":"2022-01-13T16:12:23.658531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fit tokenizer on training set:\ntokens=preprocess(twitts)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.661388Z","iopub.execute_input":"2022-01-13T16:12:23.662157Z","iopub.status.idle":"2022-01-13T16:12:23.824491Z","shell.execute_reply.started":"2022-01-13T16:12:23.662104Z","shell.execute_reply":"2022-01-13T16:12:23.823544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#text to sequences\nX_train = t.texts_to_sequences(twitts)\nX_test = t.texts_to_sequences(twitts_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.826073Z","iopub.execute_input":"2022-01-13T16:12:23.826663Z","iopub.status.idle":"2022-01-13T16:12:23.992774Z","shell.execute_reply.started":"2022-01-13T16:12:23.826606Z","shell.execute_reply":"2022-01-13T16:12:23.991423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pad sequences\nX_train = np.array(X_train, dtype=object)\nX_test = np.array(X_test, dtype=object)\nX_train = keras.preprocessing.sequence.pad_sequences(X_train)\nX_test = keras.preprocessing.sequence.pad_sequences(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:23.994841Z","iopub.execute_input":"2022-01-13T16:12:23.995226Z","iopub.status.idle":"2022-01-13T16:12:24.064334Z","shell.execute_reply.started":"2022-01-13T16:12:23.995126Z","shell.execute_reply":"2022-01-13T16:12:24.063297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this is a single encoded twitt:\nprint(X_train[0])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:24.065688Z","iopub.execute_input":"2022-01-13T16:12:24.065922Z","iopub.status.idle":"2022-01-13T16:12:24.074346Z","shell.execute_reply.started":"2022-01-13T16:12:24.065889Z","shell.execute_reply":"2022-01-13T16:12:24.073072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#size of the vocabulary\nvocab_size = len(t.word_index)\nprint('Size of vocabulary:', vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:24.076034Z","iopub.execute_input":"2022-01-13T16:12:24.076334Z","iopub.status.idle":"2022-01-13T16:12:24.086314Z","shell.execute_reply.started":"2022-01-13T16:12:24.076302Z","shell.execute_reply":"2022-01-13T16:12:24.085418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target training variable:\ny_train = df_train.target.values","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:24.087594Z","iopub.execute_input":"2022-01-13T16:12:24.088391Z","iopub.status.idle":"2022-01-13T16:12:24.0979Z","shell.execute_reply.started":"2022-01-13T16:12:24.088352Z","shell.execute_reply":"2022-01-13T16:12:24.097189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the y variable represents the labels (1 for true disaster, 0 otherwise):\nprint(y_train[0])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:24.099188Z","iopub.execute_input":"2022-01-13T16:12:24.100012Z","iopub.status.idle":"2022-01-13T16:12:24.111639Z","shell.execute_reply.started":"2022-01-13T16:12:24.099966Z","shell.execute_reply":"2022-01-13T16:12:24.110774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation split\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\nprint(\"Training data shape: \", x_train.shape)\nprint(\"Validation data shape: \", x_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:24.113196Z","iopub.execute_input":"2022-01-13T16:12:24.113762Z","iopub.status.idle":"2022-01-13T16:12:24.859445Z","shell.execute_reply.started":"2022-01-13T16:12:24.113712Z","shell.execute_reply":"2022-01-13T16:12:24.858678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Training the model RNN:","metadata":{}},{"cell_type":"code","source":"#create the model (try experimenting with your own architectures )\n#create the model\nmodel = Sequential()\nmodel.add(Embedding(input_dim = vocab_size+2, output_dim = 128, name='Embedding'))\nmodel.add(tf.keras.layers.LSTM(128, return_sequences=True))\nmodel.add(tf.keras.layers.LSTM(128))\n#model.add(Dense(128, activation = 'relu'))\n#model.add(LSTM(64))\n#model.add(Dropout(0.3))\n#model.add(Dense(32, activation = 'relu'))\nmodel.add(Dense(1))\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(1e-4), metrics=[\"accuracy\"])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:42.248039Z","iopub.execute_input":"2022-01-13T16:12:42.248616Z","iopub.status.idle":"2022-01-13T16:12:42.982547Z","shell.execute_reply.started":"2022-01-13T16:12:42.248577Z","shell.execute_reply":"2022-01-13T16:12:42.981581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\nhistory = model.fit(x_train, y, validation_data=(x_val,y_val), shuffle=True, epochs=5, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:12:47.511845Z","iopub.execute_input":"2022-01-13T16:12:47.512137Z","iopub.status.idle":"2022-01-13T16:13:39.95153Z","shell.execute_reply.started":"2022-01-13T16:12:47.512103Z","shell.execute_reply":"2022-01-13T16:13:39.950627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_score = model.evaluate(x_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:13:39.954304Z","iopub.execute_input":"2022-01-13T16:13:39.954991Z","iopub.status.idle":"2022-01-13T16:13:40.92718Z","shell.execute_reply.started":"2022-01-13T16:13:39.954941Z","shell.execute_reply":"2022-01-13T16:13:40.926169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot history: Binary Cross-entropy\nplt.plot(history.history['loss'], label='BCE (training data)')\nplt.plot(history.history['val_loss'], label='BCE (validation data)')\nplt.title('BCE for disaster Twitts')\nplt.ylabel('BCE value')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:13:40.928722Z","iopub.execute_input":"2022-01-13T16:13:40.929023Z","iopub.status.idle":"2022-01-13T16:13:41.199111Z","shell.execute_reply.started":"2022-01-13T16:13:40.92894Z","shell.execute_reply":"2022-01-13T16:13:41.197727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot history: Accuracy\nplt.plot(history.history['accuracy'], label='acc (training data)')\nplt.plot(history.history['val_accuracy'], label='acc (validation data)')\nplt.title('Accuracy for disaster Twitts')\nplt.ylabel('Accuracy value')\nplt.xlabel('No. epoch')\nplt.legend(loc=\"upper left\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:13:41.20098Z","iopub.execute_input":"2022-01-13T16:13:41.201228Z","iopub.status.idle":"2022-01-13T16:13:41.446188Z","shell.execute_reply.started":"2022-01-13T16:13:41.201199Z","shell.execute_reply":"2022-01-13T16:13:41.445233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Calculating the F1 Score on validation set: ","metadata":{}},{"cell_type":"code","source":"#calculate F1 score\nfrom sklearn.metrics import f1_score\n\ny_pred = np.where(model.predict(x_val)>0.5,1.0,0.0).flatten().astype('int32')\ny_true = y_val\nf1_score(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:13:41.448417Z","iopub.execute_input":"2022-01-13T16:13:41.448768Z","iopub.status.idle":"2022-01-13T16:13:43.244109Z","shell.execute_reply.started":"2022-01-13T16:13:41.448725Z","shell.execute_reply":"2022-01-13T16:13:43.243149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Making predictions:\nThe results are collected in a dataframe with the twitts and the corresponding predicted labels","metadata":{}},{"cell_type":"code","source":"preds = np.where(model.predict(X_test)>0.5,1.0,0.0).flatten().astype('int32')","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:15.988996Z","iopub.execute_input":"2022-01-13T16:15:15.989916Z","iopub.status.idle":"2022-01-13T16:15:18.302777Z","shell.execute_reply.started":"2022-01-13T16:15:15.989871Z","shell.execute_reply":"2022-01-13T16:15:18.301633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = {'text':twitts_test, 'target':preds}\ndf_results = pd.DataFrame(results)\ndf_results.sample(20)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:18.304779Z","iopub.execute_input":"2022-01-13T16:15:18.30504Z","iopub.status.idle":"2022-01-13T16:15:18.323564Z","shell.execute_reply.started":"2022-01-13T16:15:18.305008Z","shell.execute_reply":"2022-01-13T16:15:18.322403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Compiling the submission file:\nWe need to to a bit of manipulations since the submission file must only contain the given twitt id with its corresponding prediction","metadata":{}},{"cell_type":"code","source":"submission = pd.merge(df_test, df_results, on='text').drop_duplicates()\nsubmission.drop(['text'],axis=1).to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:15:18.327519Z","iopub.execute_input":"2022-01-13T16:15:18.327799Z","iopub.status.idle":"2022-01-13T16:15:18.360527Z","shell.execute_reply.started":"2022-01-13T16:15:18.327765Z","shell.execute_reply":"2022-01-13T16:15:18.359194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}