{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-06-20T12:14:17.770616Z","iopub.execute_input":"2021-06-20T12:14:17.771371Z","iopub.status.idle":"2021-06-20T12:14:17.786315Z","shell.execute_reply.started":"2021-06-20T12:14:17.771255Z","shell.execute_reply":"2021-06-20T12:14:17.78524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bs4","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:17.78839Z","iopub.execute_input":"2021-06-20T12:14:17.788771Z","iopub.status.idle":"2021-06-20T12:14:29.00106Z","shell.execute_reply.started":"2021-06-20T12:14:17.788734Z","shell.execute_reply":"2021-06-20T12:14:28.99959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom collections import defaultdict, Counter\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import WordPunctTokenizer\nfrom bs4 import BeautifulSoup\n\n\nfrom wordcloud import WordCloud \n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:29.005693Z","iopub.execute_input":"2021-06-20T12:14:29.006023Z","iopub.status.idle":"2021-06-20T12:14:31.019026Z","shell.execute_reply.started":"2021-06-20T12:14:29.005991Z","shell.execute_reply":"2021-06-20T12:14:31.017831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(style=\"white\", font_scale=1.2)\nplt.rcParams[\"figure.figsize\"] = [10,8]\npd.set_option.display_max_columns = 0\npd.set_option.display_max_rows = 0\n\nnltk.download('stopwords', quiet=True)\nstopwords = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:31.021021Z","iopub.execute_input":"2021-06-20T12:14:31.021366Z","iopub.status.idle":"2021-06-20T12:14:31.171099Z","shell.execute_reply.started":"2021-06-20T12:14:31.021323Z","shell.execute_reply":"2021-06-20T12:14:31.170078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:31.172695Z","iopub.execute_input":"2021-06-20T12:14:31.173107Z","iopub.status.idle":"2021-06-20T12:14:31.250103Z","shell.execute_reply.started":"2021-06-20T12:14:31.17306Z","shell.execute_reply":"2021-06-20T12:14:31.249111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:31.251545Z","iopub.execute_input":"2021-06-20T12:14:31.252179Z","iopub.status.idle":"2021-06-20T12:14:31.288659Z","shell.execute_reply.started":"2021-06-20T12:14:31.252134Z","shell.execute_reply":"2021-06-20T12:14:31.287478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This train dataset consists of the following features:\n- **Id:** a numerical identifier for the tweet. This will be important when we upload our predictions to the leaderboard.\n- **Keyword:** a keyword from the tweet which may in some cases be missing.\n- **Location:** the location the tweet was sent from. This may also not be present.\n- **Text:** the full text of the tweet.\n- **Target:** this is the label we are trying to predict. This will be 1 if the tweet is really about a disaster and 0 if not.","metadata":{}},{"cell_type":"code","source":"#total data length\nprint('There are {} rows and {} columns in train'.format(train_df.shape[0],train_df.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test_df.shape[0],test_df.shape[1]))\n\n# unique location and keyword size of data\nprint(\"Checking train location column values\",len(train_df.location.unique()))\nprint(\"Checking train keyword column values\",len(train_df.keyword.unique()))\nprint(\"Checking test location column values\",len(test_df.location.unique()))\nprint(\"Checking test keyword column values\",len(test_df.keyword.unique()))\n\n#number of disaster tweets\nprint(\"disaster tweets\", len(train_df[train_df[\"target\"]==1]) )\nprint(\"non-disaster tweets\", len(train_df[train_df[\"target\"]==0]) )","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:31.291011Z","iopub.execute_input":"2021-06-20T12:14:31.291306Z","iopub.status.idle":"2021-06-20T12:14:31.326074Z","shell.execute_reply.started":"2021-06-20T12:14:31.291278Z","shell.execute_reply":"2021-06-20T12:14:31.324953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I plot the target value distribution","metadata":{}},{"cell_type":"code","source":"target_distribution = train_df[\"target\"].value_counts(normalize=True)\nprint(\"Not Disaster: {:.2%}, Disaster: {:.2%}\".format(target_distribution[0], target_distribution[1]))\n\nsns.barplot(x=target_distribution.index, y=target_distribution)\nplt.title(\"Histogram of Disaster vs. Non-Disaster\")\nplt.xlabel(\"0 = Non-Disaster, 1 = Disaster\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:31.328388Z","iopub.execute_input":"2021-06-20T12:14:31.329024Z","iopub.status.idle":"2021-06-20T12:14:31.530085Z","shell.execute_reply.started":"2021-06-20T12:14:31.328974Z","shell.execute_reply":"2021-06-20T12:14:31.528874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see there are more data points with the label 0 meaning tweets that are not disaster tweets and fewer data points with the label 1 which is tweets that are related to a disaster. Usually, for data that has some skewed labels, it is recommended to use an F-score instead of accuracy for model evaluation.","metadata":{}},{"cell_type":"code","source":"#take look at neutral tweets\ntrain_df[train_df.target == 0].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:31.531483Z","iopub.execute_input":"2021-06-20T12:14:31.531763Z","iopub.status.idle":"2021-06-20T12:14:31.545066Z","shell.execute_reply.started":"2021-06-20T12:14:31.531736Z","shell.execute_reply":"2021-06-20T12:14:31.543888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#take look at disaster tweets\ntrain_df[train_df.target == 1].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:31.547095Z","iopub.execute_input":"2021-06-20T12:14:31.547483Z","iopub.status.idle":"2021-06-20T12:14:31.561542Z","shell.execute_reply.started":"2021-06-20T12:14:31.547422Z","shell.execute_reply":"2021-06-20T12:14:31.56042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can take a look at the distribution of `Keywords` and `Locations`. We plot 20 most repeated values for each.","metadata":{}},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(10,5))\n#visualize top 20 train unique keywords\nplt.subplot(1,2,1)\ntrain_df.keyword.value_counts()[:20].plot(kind=\"bar\",title=\"Unique Keywords\")\n\n#visualize top 20 train unique locations\nplt.subplot(1,2,2)\ntrain_df.location.value_counts()[:20].plot(kind=\"bar\",title=\"Unique Locations\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:31.563189Z","iopub.execute_input":"2021-06-20T12:14:31.563519Z","iopub.status.idle":"2021-06-20T12:14:32.185271Z","shell.execute_reply.started":"2021-06-20T12:14:31.563489Z","shell.execute_reply":"2021-06-20T12:14:32.183937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can also plot Keywords and Location for different categories.","metadata":{}},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(10,5))\n#visualize top 20 disaster tweets and their keywords bar graph\nplt.subplot(1,2,1)\ntrain_df[train_df[\"target\"]==1].keyword.value_counts()[:20].plot(kind=\"bar\",title=\"Disaster tweets keywords\")\n\n#visualize top 20 non disaster tweets and their keywords bar graph\nplt.subplot(1,2,2)\ntrain_df[train_df[\"target\"]==0].keyword.value_counts()[:20].plot(kind=\"bar\",title=\"Non-Disaster tweets keywords\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:32.187118Z","iopub.execute_input":"2021-06-20T12:14:32.187581Z","iopub.status.idle":"2021-06-20T12:14:32.919492Z","shell.execute_reply.started":"2021-06-20T12:14:32.187534Z","shell.execute_reply":"2021-06-20T12:14:32.918526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(10,5))\n#visualize top 20 disaster tweets and their locations bar graph\nplt.subplot(1,2,1)\ntrain_df[train_df[\"target\"]==1].location.value_counts()[:20].plot(kind=\"bar\",title=\"Disaster tweets Locations\")\n\n#visualize top 20 non disaster tweets and their locations bar graph\nplt.subplot(1,2,2)\ntrain_df[train_df[\"target\"]==0].location.value_counts()[:20].plot(kind=\"bar\",title=\"Non-Disaster tweets Locations\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:32.923287Z","iopub.execute_input":"2021-06-20T12:14:32.923622Z","iopub.status.idle":"2021-06-20T12:14:33.567814Z","shell.execute_reply.started":"2021-06-20T12:14:32.923592Z","shell.execute_reply":"2021-06-20T12:14:33.566802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Taking care of null values","metadata":{}},{"cell_type":"code","source":"null_counts = pd.DataFrame({\"Number_Null\": train_df.isnull().sum()})\nnull_counts[\"Percent_Null\"] = null_counts[\"Number_Null\"] / train_df.count() * 100\nnull_counts","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:33.569194Z","iopub.execute_input":"2021-06-20T12:14:33.569599Z","iopub.status.idle":"2021-06-20T12:14:33.591237Z","shell.execute_reply.started":"2021-06-20T12:14:33.569566Z","shell.execute_reply":"2021-06-20T12:14:33.590252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Location includes `2533` null values and `61` values for keywords are null in train dataset.","metadata":{}},{"cell_type":"code","source":"null_counts = pd.DataFrame({\"Number_Null\": test_df.isnull().sum()})\nnull_counts[\"Percent_Null\"] = null_counts[\"Number_Null\"] / test_df.count() * 100\nnull_counts","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:33.59247Z","iopub.execute_input":"2021-06-20T12:14:33.592734Z","iopub.status.idle":"2021-06-20T12:14:33.611215Z","shell.execute_reply.started":"2021-06-20T12:14:33.592708Z","shell.execute_reply":"2021-06-20T12:14:33.609881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Location includes `1105` null values and`26` keywords are null in test dataset.","metadata":{}},{"cell_type":"markdown","source":"We can delete the values for these two columns as they seem not neccessary","metadata":{}},{"cell_type":"code","source":"# Let's get rid of `Location` and  `keywords` columns as they are unnecessary.\n# train_df.drop(['keyword','location'],axis=1,inplace=True)\n# test_df.drop(['keyword','location'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:33.613083Z","iopub.execute_input":"2021-06-20T12:14:33.613485Z","iopub.status.idle":"2021-06-20T12:14:33.623536Z","shell.execute_reply.started":"2021-06-20T12:14:33.613425Z","shell.execute_reply":"2021-06-20T12:14:33.622118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can examine if the neutral and disaster tweets, spread equally in our dataset","metadata":{}},{"cell_type":"code","source":"#take look at neutral tweets index distribution\ntrain_df[train_df.target == 0].index","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:33.625245Z","iopub.execute_input":"2021-06-20T12:14:33.625725Z","iopub.status.idle":"2021-06-20T12:14:33.642657Z","shell.execute_reply.started":"2021-06-20T12:14:33.625686Z","shell.execute_reply":"2021-06-20T12:14:33.64148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#take look at disaster tweets index distribution\ntrain_df[train_df.target == 1].index","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:33.644205Z","iopub.execute_input":"2021-06-20T12:14:33.644615Z","iopub.status.idle":"2021-06-20T12:14:33.657497Z","shell.execute_reply.started":"2021-06-20T12:14:33.64458Z","shell.execute_reply":"2021-06-20T12:14:33.656307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Taking care of duplicate values.","metadata":{}},{"cell_type":"code","source":"dupli_sum = train_df.text.duplicated().sum()\n\nif(dupli_sum>0):\n    print(dupli_sum, \" duplicates found\\nremoving duplicates...\")\n    train_df = train_df.loc[False==train_df.text.duplicated(), :]\n    print('There are {} rows and {} columns in train after removing duplicates'\n          .format(train_df.shape[0],train_df.shape[1]))\nelse:\n    print(\"no duplicates found\")\n    \ntrain_df","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:33.659244Z","iopub.execute_input":"2021-06-20T12:14:33.659625Z","iopub.status.idle":"2021-06-20T12:14:33.690291Z","shell.execute_reply.started":"2021-06-20T12:14:33.65959Z","shell.execute_reply":"2021-06-20T12:14:33.689034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Extracting hashtags from tweets","metadata":{}},{"cell_type":"code","source":"#extract hashtags\ntrain_df[\"hashtags\"]=train_df[\"text\"].apply(lambda x:re.findall(r\"#(\\w+)\",x.lower()))\ntest_df[\"hashtags\"]=test_df[\"text\"].apply(lambda x:re.findall(r\"#(\\w+)\",x.lower()))\n\n#convert tokens hashtags to text\ntrain_df[\"hashtags\"]=train_df[\"hashtags\"].apply(lambda x: ' '.join(x))\ntest_df[\"hashtags\"]=test_df[\"hashtags\"].apply(lambda x: ' '.join(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:33.694689Z","iopub.execute_input":"2021-06-20T12:14:33.695027Z","iopub.status.idle":"2021-06-20T12:14:33.734796Z","shell.execute_reply.started":"2021-06-20T12:14:33.694996Z","shell.execute_reply":"2021-06-20T12:14:33.733932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:33.737331Z","iopub.execute_input":"2021-06-20T12:14:33.737834Z","iopub.status.idle":"2021-06-20T12:14:33.751577Z","shell.execute_reply.started":"2021-06-20T12:14:33.737784Z","shell.execute_reply":"2021-06-20T12:14:33.750489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:33.753488Z","iopub.execute_input":"2021-06-20T12:14:33.753923Z","iopub.status.idle":"2021-06-20T12:14:33.774305Z","shell.execute_reply.started":"2021-06-20T12:14:33.753873Z","shell.execute_reply":"2021-06-20T12:14:33.772752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"Adding length of the tweets before cleaning to the dataframe.","metadata":{}},{"cell_type":"code","source":"# add the characters length of tweets\ntrain_df['text_len'] = [len(t) for t in train_df.text]\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:33.776061Z","iopub.execute_input":"2021-06-20T12:14:33.776727Z","iopub.status.idle":"2021-06-20T12:14:33.797238Z","shell.execute_reply.started":"2021-06-20T12:14:33.776681Z","shell.execute_reply":"2021-06-20T12:14:33.795786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(train_df[\"text_len\"])\nplt.title(\"Histogram of Tweet Length\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:33.799123Z","iopub.execute_input":"2021-06-20T12:14:33.799572Z","iopub.status.idle":"2021-06-20T12:14:34.155838Z","shell.execute_reply.started":"2021-06-20T12:14:33.799528Z","shell.execute_reply":"2021-06-20T12:14:34.154388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(train_df, col=\"target\", height=5)\n\ng = g.map(sns.distplot, \"text_len\")\ng.fig.subplots_adjust(top=.8)\n\nplt.suptitle(\"Distribution Tweet Length\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:34.157485Z","iopub.execute_input":"2021-06-20T12:14:34.157889Z","iopub.status.idle":"2021-06-20T12:14:34.979471Z","shell.execute_reply.started":"2021-06-20T12:14:34.157845Z","shell.execute_reply":"2021-06-20T12:14:34.978087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen in above plot that the character distribution is left skewed.\nwhat about the test dataset?","metadata":{}},{"cell_type":"code","source":"test_df['text_len'] = [len(t) for t in test_df.text]\ntest_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:34.981153Z","iopub.execute_input":"2021-06-20T12:14:34.981589Z","iopub.status.idle":"2021-06-20T12:14:34.999081Z","shell.execute_reply.started":"2021-06-20T12:14:34.981535Z","shell.execute_reply":"2021-06-20T12:14:34.99789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(test_df[\"text_len\"])\n\nplt.title(\"Histogram of Tweet Length\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:35.000984Z","iopub.execute_input":"2021-06-20T12:14:35.001433Z","iopub.status.idle":"2021-06-20T12:14:35.314171Z","shell.execute_reply.started":"2021-06-20T12:14:35.001384Z","shell.execute_reply":"2021-06-20T12:14:35.313074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test data set is also left skewed.","metadata":{}},{"cell_type":"code","source":"def count_words(x):\n    '''\n        A function to count number of words in a tweet\n        inpit : tweet\n        output: (int) number of words\n    '''\n    return len(x.split())","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:35.315747Z","iopub.execute_input":"2021-06-20T12:14:35.316067Z","iopub.status.idle":"2021-06-20T12:14:35.322247Z","shell.execute_reply.started":"2021-06-20T12:14:35.316033Z","shell.execute_reply":"2021-06-20T12:14:35.320592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"num_words_text\"] = train_df[\"text\"].apply(count_words)\n\nsns.distplot(train_df[\"num_words_text\"], bins=10)\nplt.title(\"Histogram of Number of Words per Tweet\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:35.323859Z","iopub.execute_input":"2021-06-20T12:14:35.324205Z","iopub.status.idle":"2021-06-20T12:14:35.640802Z","shell.execute_reply.started":"2021-06-20T12:14:35.324174Z","shell.execute_reply":"2021-06-20T12:14:35.639646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that number of words follow a normal distribution.As we can see the majority of tweets are between 11 to 19 words.","metadata":{}},{"cell_type":"code","source":"g = sns.FacetGrid(train_df, col=\"target\", height=5)\n\ng = g.map(sns.distplot, \"num_words_text\")\ng.fig.subplots_adjust(top=.8)\n\nplt.suptitle(\"Distribution Number of Words\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:35.642421Z","iopub.execute_input":"2021-06-20T12:14:35.642804Z","iopub.status.idle":"2021-06-20T12:14:36.389988Z","shell.execute_reply.started":"2021-06-20T12:14:35.642768Z","shell.execute_reply":"2021-06-20T12:14:36.388961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do the same for test dataset.","metadata":{}},{"cell_type":"code","source":"test_df[\"num_words_text\"] = test_df[\"text\"].apply(count_words)\n\nsns.distplot(test_df[\"num_words_text\"], bins=10)\nplt.title(\"Histogram of Number of Words per Tweet\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:36.391316Z","iopub.execute_input":"2021-06-20T12:14:36.391626Z","iopub.status.idle":"2021-06-20T12:14:36.67842Z","shell.execute_reply.started":"2021-06-20T12:14:36.391595Z","shell.execute_reply":"2021-06-20T12:14:36.677612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I do some further analysis on number of words and the length of the tweet by calculating the average word length of the tweets","metadata":{}},{"cell_type":"code","source":"def avg_word_length(x):\n    return np.sum([len(w) for w in x.split()]) / len(x.split())","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:36.679803Z","iopub.execute_input":"2021-06-20T12:14:36.680304Z","iopub.status.idle":"2021-06-20T12:14:36.684621Z","shell.execute_reply.started":"2021-06-20T12:14:36.680267Z","shell.execute_reply":"2021-06-20T12:14:36.683784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"avg_word_length\"] = train_df[\"text\"].apply(avg_word_length)\n\nsns.distplot(train_df[\"avg_word_length\"])\nplt.title(\"Histogram of Average Word Length\")\nplt.xlabel(\"Average Word Length\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:36.686078Z","iopub.execute_input":"2021-06-20T12:14:36.686623Z","iopub.status.idle":"2021-06-20T12:14:37.188093Z","shell.execute_reply.started":"2021-06-20T12:14:36.686588Z","shell.execute_reply":"2021-06-20T12:14:37.187371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g = sns.FacetGrid(train_df, col=\"target\", height=5)\ng = g.map(sns.distplot, \"avg_word_length\")\n\ng.fig.subplots_adjust(top=.8)\n\nplt.suptitle(\"Distribution Average Word Length\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:37.189399Z","iopub.execute_input":"2021-06-20T12:14:37.189913Z","iopub.status.idle":"2021-06-20T12:14:38.006874Z","shell.execute_reply.started":"2021-06-20T12:14:37.189879Z","shell.execute_reply":"2021-06-20T12:14:38.006022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"avg_word_length\"] = test_df[\"text\"].apply(avg_word_length)\n\nsns.distplot(test_df[\"avg_word_length\"])\nplt.title(\"Histogram of Average Word Length\")\nplt.xlabel(\"Average Word Length\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:38.008138Z","iopub.execute_input":"2021-06-20T12:14:38.00864Z","iopub.status.idle":"2021-06-20T12:14:38.414868Z","shell.execute_reply.started":"2021-06-20T12:14:38.008605Z","shell.execute_reply":"2021-06-20T12:14:38.413646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"tok = WordPunctTokenizer()\n\npat1 = r'@[A-Za-z0-9_]+'\npat2 = r'https?://[^ ]+'\ncombined_pat = r'|'.join((pat1, pat2))\nwww_pat = r'www.[^ ]+'\nnegations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\", \"haven't\":\"have not\",\"hasn't\":\"has not\",\n                 \"hadn't\":\"had not\",\"won't\":\"will not\",\"wouldn't\":\"would not\",\"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n                 \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\", \"mightn't\":\"might not\", \"mustn't\":\"must not\",\n                 }\n\nneg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n\ndef tweet_cleaner(text):\n    \n    soup = BeautifulSoup(text, 'lxml')\n    souped = soup.get_text()\n    \n    try:\n        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n    except:\n        bom_removed = souped\n        \n    stripped = re.sub(combined_pat, '', bom_removed)\n    stripped = re.sub(www_pat, '', stripped)\n    lower_case = stripped.lower()\n    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)   \n    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n    \n    return (\" \".join(words)).strip()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:38.416343Z","iopub.execute_input":"2021-06-20T12:14:38.416687Z","iopub.status.idle":"2021-06-20T12:14:38.429123Z","shell.execute_reply.started":"2021-06-20T12:14:38.416654Z","shell.execute_reply":"2021-06-20T12:14:38.427677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The order of the cleaning is\n1. Souping\n2. BOM removing\n3. Url address(‘http:’pattern), twitter ID removing\n4. Url address(‘www.'pattern) removing\n5. Lower-case\n6. Negation handling\n7. Removing numbers and special characters\n8. Tokenizing and joining","metadata":{}},{"cell_type":"code","source":"train_df['clean_text'] = train_df['text'].map(lambda x: tweet_cleaner(x))\ntest_df['clean_text'] = test_df['text'].map(lambda x: tweet_cleaner(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:38.431038Z","iopub.execute_input":"2021-06-20T12:14:38.43153Z","iopub.status.idle":"2021-06-20T12:14:41.408335Z","shell.execute_reply.started":"2021-06-20T12:14:38.431481Z","shell.execute_reply":"2021-06-20T12:14:41.407186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:41.409558Z","iopub.execute_input":"2021-06-20T12:14:41.409852Z","iopub.status.idle":"2021-06-20T12:14:41.424671Z","shell.execute_reply.started":"2021-06-20T12:14:41.409823Z","shell.execute_reply":"2021-06-20T12:14:41.423544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:41.425993Z","iopub.execute_input":"2021-06-20T12:14:41.426304Z","iopub.status.idle":"2021-06-20T12:14:41.448916Z","shell.execute_reply.started":"2021-06-20T12:14:41.426273Z","shell.execute_reply":"2021-06-20T12:14:41.447602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Length of tweets after cleaning\ntrain_df['clean_text_len'] = [len(t) for t in train_df.clean_text]\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:41.450468Z","iopub.execute_input":"2021-06-20T12:14:41.450769Z","iopub.status.idle":"2021-06-20T12:14:41.480091Z","shell.execute_reply.started":"2021-06-20T12:14:41.450741Z","shell.execute_reply":"2021-06-20T12:14:41.478726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Length of tweets after cleaning\ntest_df['clean_text_len'] = [len(t) for t in test_df.clean_text]\ntest_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:41.481624Z","iopub.execute_input":"2021-06-20T12:14:41.481991Z","iopub.status.idle":"2021-06-20T12:14:41.506332Z","shell.execute_reply.started":"2021-06-20T12:14:41.48196Z","shell.execute_reply":"2021-06-20T12:14:41.505062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# numbet of words in tweets after cleaning\ntrain_df[\"clean_num_words\"] = train_df[\"clean_text\"].apply(count_words)\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:41.507826Z","iopub.execute_input":"2021-06-20T12:14:41.508139Z","iopub.status.idle":"2021-06-20T12:14:41.538882Z","shell.execute_reply.started":"2021-06-20T12:14:41.50811Z","shell.execute_reply":"2021-06-20T12:14:41.537661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# numbet of words in tweets after cleaning\ntest_df[\"clean_num_words\"] = test_df[\"clean_text\"].apply(count_words)\ntest_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:41.540139Z","iopub.execute_input":"2021-06-20T12:14:41.540478Z","iopub.status.idle":"2021-06-20T12:14:41.564365Z","shell.execute_reply.started":"2021-06-20T12:14:41.540419Z","shell.execute_reply":"2021-06-20T12:14:41.563104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average words length in tweets after cleaning\ntrain_df[\"clean_avg_word_length\"] = train_df[\"clean_text\"].apply(avg_word_length)\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:41.56649Z","iopub.execute_input":"2021-06-20T12:14:41.566981Z","iopub.status.idle":"2021-06-20T12:14:41.715606Z","shell.execute_reply.started":"2021-06-20T12:14:41.566932Z","shell.execute_reply":"2021-06-20T12:14:41.714275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average words length in tweets after cleaning\ntest_df[\"clean_avg_word_length\"] = test_df[\"clean_text\"].apply(avg_word_length)\ntest_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:41.717127Z","iopub.execute_input":"2021-06-20T12:14:41.717567Z","iopub.status.idle":"2021-06-20T12:14:41.794821Z","shell.execute_reply.started":"2021-06-20T12:14:41.717519Z","shell.execute_reply":"2021-06-20T12:14:41.793784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Taking care of Stopwords","metadata":{}},{"cell_type":"code","source":"train_df['clean_text_stopword'] = train_df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:41.796353Z","iopub.execute_input":"2021-06-20T12:14:41.796693Z","iopub.status.idle":"2021-06-20T12:14:41.992895Z","shell.execute_reply.started":"2021-06-20T12:14:41.796659Z","shell.execute_reply":"2021-06-20T12:14:41.991758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['clean_text_stopword'] = test_df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntest_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:41.994395Z","iopub.execute_input":"2021-06-20T12:14:41.994755Z","iopub.status.idle":"2021-06-20T12:14:42.09363Z","shell.execute_reply.started":"2021-06-20T12:14:41.994722Z","shell.execute_reply":"2021-06-20T12:14:42.092288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping words whose length is less than 3\ntrain_df['clean_text_stopword'] = train_df['clean_text_stopword'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:42.09507Z","iopub.execute_input":"2021-06-20T12:14:42.09538Z","iopub.status.idle":"2021-06-20T12:14:42.138209Z","shell.execute_reply.started":"2021-06-20T12:14:42.095349Z","shell.execute_reply":"2021-06-20T12:14:42.136959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['clean_text_stopword'] = test_df['clean_text_stopword'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntest_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:42.141704Z","iopub.execute_input":"2021-06-20T12:14:42.142044Z","iopub.status.idle":"2021-06-20T12:14:42.171331Z","shell.execute_reply.started":"2021-06-20T12:14:42.142012Z","shell.execute_reply":"2021-06-20T12:14:42.170044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word Cloud\nA word cloud represents word usage in a document by resizing individual words proportionally to its frequency and then presenting them in a random arrangement. ","metadata":{}},{"cell_type":"markdown","source":"#### Plot Disaster tweets wordcloud","metadata":{}},{"cell_type":"code","source":"disaster_tweets = train_df[train_df.target == 1]\ndisaster_string = []\n\nfor t in disaster_tweets.clean_text_stopword:\n    disaster_string.append(t)\n    \ndisaster_string = pd.Series(disaster_string).str.cat(sep=' ')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:42.1732Z","iopub.execute_input":"2021-06-20T12:14:42.173676Z","iopub.status.idle":"2021-06-20T12:14:42.189635Z","shell.execute_reply.started":"2021-06-20T12:14:42.173627Z","shell.execute_reply":"2021-06-20T12:14:42.18841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud_disaster = WordCloud(width=1600, height=800,max_font_size=200 ,colormap='magma').generate(disaster_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud_disaster, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:42.191146Z","iopub.execute_input":"2021-06-20T12:14:42.191468Z","iopub.status.idle":"2021-06-20T12:14:45.179702Z","shell.execute_reply.started":"2021-06-20T12:14:42.191416Z","shell.execute_reply":"2021-06-20T12:14:45.178389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plot Neutral tweets wordcloud","metadata":{}},{"cell_type":"code","source":"neutral_tweets = train_df[train_df.target == 0]\nneutral_string = []\nfor t in neutral_tweets.clean_text_stopword:\n    neutral_string.append(t)\nneutral_string = pd.Series(neutral_string).str.cat(sep=' ')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:45.181303Z","iopub.execute_input":"2021-06-20T12:14:45.181967Z","iopub.status.idle":"2021-06-20T12:14:45.19522Z","shell.execute_reply.started":"2021-06-20T12:14:45.181905Z","shell.execute_reply":"2021-06-20T12:14:45.193924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud_neutral = WordCloud(width=1600, height=800,max_font_size=200).generate(neutral_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud_neutral, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:45.19717Z","iopub.execute_input":"2021-06-20T12:14:45.197636Z","iopub.status.idle":"2021-06-20T12:14:48.329947Z","shell.execute_reply.started":"2021-06-20T12:14:45.197588Z","shell.execute_reply":"2021-06-20T12:14:48.328986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize Bigram frequency distribution of tweets","metadata":{}},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(15,10))\n\nplt.subplot(1,2,1)\n#Bigram Frequency distribution for disaster tweets\n#convert disaster tweets into single string\ntxt=' '.join(train_df[train_df[\"target\"]==1][\"clean_text_stopword\"])\ndisaster_bigram=nltk.FreqDist(nltk.bigrams(nltk.word_tokenize(txt)))\ntmplst=disaster_bigram.most_common(30)\n\n#visualize Bigram frequency distribution for disaster tweets using bar graph\nwrd,cnt=zip(*tmplst)\nwrd=[ x+\",\"+y for (x,y) in wrd]\nplt.barh(wrd,cnt)\nplt.title(\"Disaster Bigram BarGraph\")\n\nplt.subplot(1,2,2)\n#Bigram Frequency distribution for non disaster tweets\n#convert non disaster tweets into single string\ntxt=' '.join(train_df[train_df[\"target\"]==0][\"clean_text_stopword\"])\nnondisaster_bigram=nltk.FreqDist(nltk.bigrams(nltk.word_tokenize(txt)))\ntmplst=nondisaster_bigram.most_common(30)\n\n#visualize Bigram frequency distribution for non disaster tweets using bar graph\nwrd,cnt=zip(*tmplst)\nwrd=[ x+\",\"+y for (x,y) in wrd]\nplt.barh(wrd,cnt)\nplt.title(\"Non Disaster Bigram BarGraph\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:48.331312Z","iopub.execute_input":"2021-06-20T12:14:48.331642Z","iopub.status.idle":"2021-06-20T12:14:50.321824Z","shell.execute_reply.started":"2021-06-20T12:14:48.331611Z","shell.execute_reply":"2021-06-20T12:14:50.320826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Visualize unigram frequency distribution for disaster hashtags","metadata":{}},{"cell_type":"code","source":"plt.subplots(1,2,figsize=(15,15))\nplt.subplot(1,2,1)\n#Uigram Frequency distribution for disaster hashtags\n#convert disaster hashtags into single string\ntxt=' '.join(train_df[train_df[\"target\"]==1][\"hashtags\"])\ndisaster_unigram_hash=nltk.FreqDist(nltk.word_tokenize(txt))\n\n#visualize unigram frequency distribution for disaster hashtags using wordcloud\ndisaster_wc = WordCloud(width=800, height=400, max_words=100).generate_from_frequencies(disaster_unigram_hash)\nplt.title(\"Disaster Unigram Frequency Distribution hashtags\")\nplt.imshow(disaster_wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\nplt.subplot(1,2,2)\n#Uigram Frequency distribution for non disaster hashtags\n#convert non disaster hashtags into single string\ntxt=' '.join(train_df[train_df[\"target\"]==0][\"hashtags\"])\nnondisaster_unigram_hash=nltk.FreqDist(nltk.word_tokenize(txt))\n\n#visualize unigram frequency distribution for non disaster hashtags using wordcloud\nnondisaster_wc = WordCloud(width=800, height=400, max_words=100).generate_from_frequencies(nondisaster_unigram_hash)\nplt.title(\"Non Disaster Unigram Frequency Distribution hashtags\")\nplt.axis(\"off\")\nplt.imshow(nondisaster_wc, interpolation=\"bilinear\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:50.323034Z","iopub.execute_input":"2021-06-20T12:14:50.323319Z","iopub.status.idle":"2021-06-20T12:14:51.712382Z","shell.execute_reply.started":"2021-06-20T12:14:50.32329Z","shell.execute_reply":"2021-06-20T12:14:51.711706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network","metadata":{}},{"cell_type":"code","source":"# Word Embedding\nfrom gensim.models import KeyedVectors\n# Keras\nfrom keras import optimizers\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Input, Embedding, Dropout\nfrom keras.layers import GlobalMaxPool1D, MaxPooling1D, GlobalMaxPooling1D\nfrom keras.layers import LSTM, Bidirectional\nfrom keras.layers.convolutional import Conv1D\nfrom keras.utils import plot_model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Visualization\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom keras.utils import plot_model\n# Measuring metrics\nfrom sklearn.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:51.713341Z","iopub.execute_input":"2021-06-20T12:14:51.713724Z","iopub.status.idle":"2021-06-20T12:14:58.075968Z","shell.execute_reply.started":"2021-06-20T12:14:51.713696Z","shell.execute_reply":"2021-06-20T12:14:58.074668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# fastText word embeddings","metadata":{}},{"cell_type":"code","source":"# Downloading fastext\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:14:58.077072Z","iopub.execute_input":"2021-06-20T12:14:58.07737Z","iopub.status.idle":"2021-06-20T12:17:49.905624Z","shell.execute_reply.started":"2021-06-20T12:14:58.077341Z","shell.execute_reply":"2021-06-20T12:17:49.90441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train_df[['id','text','clean_text_stopword','target']]\ntrain.rename(columns = {'clean_text_stopword':'clean_text'},inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:49.907501Z","iopub.execute_input":"2021-06-20T12:17:49.908075Z","iopub.status.idle":"2021-06-20T12:17:49.919745Z","shell.execute_reply.started":"2021-06-20T12:17:49.908023Z","shell.execute_reply":"2021-06-20T12:17:49.918758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:49.931384Z","iopub.execute_input":"2021-06-20T12:17:49.931817Z","iopub.status.idle":"2021-06-20T12:17:49.94961Z","shell.execute_reply.started":"2021-06-20T12:17:49.931729Z","shell.execute_reply":"2021-06-20T12:17:49.948528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test_df[['id','text','clean_text_stopword']]\ntest.rename(columns = {'clean_text_stopword':'clean_text'},inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:49.953324Z","iopub.execute_input":"2021-06-20T12:17:49.953681Z","iopub.status.idle":"2021-06-20T12:17:49.967678Z","shell.execute_reply.started":"2021-06-20T12:17:49.953648Z","shell.execute_reply":"2021-06-20T12:17:49.966692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:49.969128Z","iopub.execute_input":"2021-06-20T12:17:49.969565Z","iopub.status.idle":"2021-06-20T12:17:49.986788Z","shell.execute_reply.started":"2021-06-20T12:17:49.969532Z","shell.execute_reply":"2021-06-20T12:17:49.98573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = test['clean_text']\nprint('Number of testing sentence: ', x_test.shape)\nx_test = np.asarray(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:49.988051Z","iopub.execute_input":"2021-06-20T12:17:49.988361Z","iopub.status.idle":"2021-06-20T12:17:50.004471Z","shell.execute_reply.started":"2021-06-20T12:17:49.988325Z","shell.execute_reply":"2021-06-20T12:17:50.003477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = train['clean_text']\ny_train = train['target']","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:50.005987Z","iopub.execute_input":"2021-06-20T12:17:50.006404Z","iopub.status.idle":"2021-06-20T12:17:50.024477Z","shell.execute_reply.started":"2021-06-20T12:17:50.00637Z","shell.execute_reply":"2021-06-20T12:17:50.023462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = np.asarray(x_train)\ny_train = np.asarray(y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:50.025759Z","iopub.execute_input":"2021-06-20T12:17:50.026063Z","iopub.status.idle":"2021-06-20T12:17:50.063606Z","shell.execute_reply.started":"2021-06-20T12:17:50.026032Z","shell.execute_reply":"2021-06-20T12:17:50.062392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of training sentence: ', x_train.shape)\nprint('Number of training label: ', y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:50.06507Z","iopub.execute_input":"2021-06-20T12:17:50.065402Z","iopub.status.idle":"2021-06-20T12:17:50.100276Z","shell.execute_reply.started":"2021-06-20T12:17:50.065369Z","shell.execute_reply":"2021-06-20T12:17:50.09947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = np.asarray(x_train)\ny_train = np.asarray(y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:50.101765Z","iopub.execute_input":"2021-06-20T12:17:50.102087Z","iopub.status.idle":"2021-06-20T12:17:50.114017Z","shell.execute_reply.started":"2021-06-20T12:17:50.102058Z","shell.execute_reply":"2021-06-20T12:17:50.113176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the data number of sentence in each category \nfrom collections import Counter\ncnt = Counter(y_train)\ncnt = dict(cnt)\nprint(cnt)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:50.115322Z","iopub.execute_input":"2021-06-20T12:17:50.115874Z","iopub.status.idle":"2021-06-20T12:17:50.138262Z","shell.execute_reply.started":"2021-06-20T12:17:50.115843Z","shell.execute_reply":"2021-06-20T12:17:50.13682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = list(cnt.keys())\nsizes = list(cnt.values())\ncolors = ['#3fba36', '#66b3ff']\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, colors=colors,\n        autopct='%1.1f%%', startangle=90)\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\n# Decomment following line if you want to save the figure\n# plt.savefig('distribution.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:50.140014Z","iopub.execute_input":"2021-06-20T12:17:50.140762Z","iopub.status.idle":"2021-06-20T12:17:50.356537Z","shell.execute_reply.started":"2021-06-20T12:17:50.1407Z","shell.execute_reply":"2021-06-20T12:17:50.355639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare FastText Model","metadata":{}},{"cell_type":"code","source":"EMBEDDING_FILE = 'wiki.en.vec'\n\ndef import_with_gensim(file_address):\n    # Creating the model\n    ft_model = KeyedVectors.load_word2vec_format(file_address)\n    # Getting the tokens\n    ft_words = []\n    for ft_word in ft_model.index_to_key:\n        ft_words.append(ft_word)\n    return ft_model, ft_words\n  \nft_model, ft_words = import_with_gensim(EMBEDDING_FILE)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:17:50.3579Z","iopub.execute_input":"2021-06-20T12:17:50.358521Z","iopub.status.idle":"2021-06-20T12:39:51.241811Z","shell.execute_reply.started":"2021-06-20T12:17:50.358471Z","shell.execute_reply":"2021-06-20T12:39:51.239108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FastText embedding dimensionality\nembed_size = 300","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:39:51.245002Z","iopub.execute_input":"2021-06-20T12:39:51.245355Z","iopub.status.idle":"2021-06-20T12:39:51.260496Z","shell.execute_reply.started":"2021-06-20T12:39:51.245314Z","shell.execute_reply":"2021-06-20T12:39:51.256136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We get the mean and standard deviation of the embedding weights so that we could maintain the\n# same statistics for the rest of our own random generated weights.\n\nembedding_list = list()\n\nfor w in ft_words:\n    embedding_list.append(ft_model[w])\n\nall_embedding = np.stack(embedding_list)\nemb_mean, emb_std = all_embedding.mean(), all_embedding.std()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:39:51.265544Z","iopub.execute_input":"2021-06-20T12:39:51.267983Z","iopub.status.idle":"2021-06-20T12:40:08.175239Z","shell.execute_reply.started":"2021-06-20T12:39:51.267823Z","shell.execute_reply":"2021-06-20T12:40:08.173741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data for Deep Learning model","metadata":{}},{"cell_type":"markdown","source":"## Setting tokenizer up","metadata":{}},{"cell_type":"code","source":"num_words = 2500\n\n# Create the tokenizer\ntokenizer = Tokenizer()\n\n# fFt the tokenizer on the training documents\ntokenizer.fit_on_texts(x_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:40:08.176923Z","iopub.execute_input":"2021-06-20T12:40:08.177244Z","iopub.status.idle":"2021-06-20T12:40:08.578395Z","shell.execute_reply.started":"2021-06-20T12:40:08.177213Z","shell.execute_reply":"2021-06-20T12:40:08.576941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find maximum length of training sentences\nmax_length = max([len(s.split()) for s in x_train])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:40:08.579922Z","iopub.execute_input":"2021-06-20T12:40:08.580212Z","iopub.status.idle":"2021-06-20T12:40:08.590871Z","shell.execute_reply.started":"2021-06-20T12:40:08.580184Z","shell.execute_reply":"2021-06-20T12:40:08.589676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embed sentences","metadata":{}},{"cell_type":"code","source":"# Embed training sequences\nencoded_docs = tokenizer.texts_to_sequences(x_train)\n\n# Pad embeded training sequences\nx_train_padded = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:40:08.592279Z","iopub.execute_input":"2021-06-20T12:40:08.592627Z","iopub.status.idle":"2021-06-20T12:40:08.750902Z","shell.execute_reply.started":"2021-06-20T12:40:08.592595Z","shell.execute_reply":"2021-06-20T12:40:08.749696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define vocabulary size (largest integer value)\nvocab_size = len(tokenizer.word_index)+1","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:40:08.752246Z","iopub.execute_input":"2021-06-20T12:40:08.752576Z","iopub.status.idle":"2021-06-20T12:40:08.757219Z","shell.execute_reply.started":"2021-06-20T12:40:08.752547Z","shell.execute_reply":"2021-06-20T12:40:08.755931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We are going to set the embedding size to the pre-trained dimension as we are replicating it\nnb_words = len(tokenizer.word_index)+1\n\n# the size will be Number of Words in Vocab X Embedding Size\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n\n# With the newly created embedding matrix, we'll fill it up with the words that we have in both\n# our own dictionary and loaded pre-trained embedding.\nembeddedCount = 0\nfor word, i in tokenizer.word_index.items():\n    i -= 1\n    # then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n    if word in ft_model.index_to_key:\n        embedding_vector = ft_model[word]\n        # and store inside the embedding matrix that we will train later on.\n        embedding_matrix[i] = embedding_vector\n        embeddedCount += 1\n    else:   # Unknown words\n        embedding_vector = ft_model['subdivision_name']\n        embedding_matrix[i] = embedding_vector\n        embeddedCount += 1\n\nprint('total embedded:', embeddedCount, 'common words')\nprint('Embedding matrix shape:', embedding_matrix.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:40:08.759211Z","iopub.execute_input":"2021-06-20T12:40:08.759689Z","iopub.status.idle":"2021-06-20T12:41:08.98713Z","shell.execute_reply.started":"2021-06-20T12:40:08.759643Z","shell.execute_reply":"2021-06-20T12:41:08.98602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Embed testing sequences\nencoded_docs = tokenizer.texts_to_sequences(x_test)\n# Pad testing sequences\nx_test_padded = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:41:08.988832Z","iopub.execute_input":"2021-06-20T12:41:08.989268Z","iopub.status.idle":"2021-06-20T12:41:09.064158Z","shell.execute_reply.started":"2021-06-20T12:41:08.989224Z","shell.execute_reply":"2021-06-20T12:41:09.06324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and Validation split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(x_train_padded, y_train, test_size=0.2, random_state=2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:41:09.065555Z","iopub.execute_input":"2021-06-20T12:41:09.066235Z","iopub.status.idle":"2021-06-20T12:41:09.077957Z","shell.execute_reply.started":"2021-06-20T12:41:09.066187Z","shell.execute_reply":"2021-06-20T12:41:09.076484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train set has total {0} entries with {1:.2f}% disaster, {2:.2f}% neutral\".format(len(X_train),\n                                                                             (len(X_train[Y_train == 0]) / (len(X_train)*1.))*100,\n                                                                            (len(X_train[Y_train == 1]) / (len(X_train)*1.))*100))\nprint(\"Validation set has total {0} entries with {1:.2f}% disaster, {2:.2f}% neutral\".format(len(X_val),\n                                                                             (len(X_val[Y_val == 0]) / (len(X_val)*1.))*100,\n                                                                            (len(X_val[Y_val == 1]) / (len(X_val)*1.))*100))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:41:09.080161Z","iopub.execute_input":"2021-06-20T12:41:09.080564Z","iopub.status.idle":"2021-06-20T12:41:09.095048Z","shell.execute_reply.started":"2021-06-20T12:41:09.08053Z","shell.execute_reply":"2021-06-20T12:41:09.093495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## B-LSTM Model","metadata":{}},{"cell_type":"code","source":"model_blstm_fast = Sequential()\nmodel_blstm_fast.add(Embedding(vocab_size, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False))\nmodel_blstm_fast.add(Bidirectional(LSTM(300, return_sequences=True, name='lstm_layer')))\nmodel_blstm_fast.add(GlobalMaxPool1D())\nmodel_blstm_fast.add(Dropout(0.1))\nmodel_blstm_fast.add(Dense(300, activation=\"relu\"))\nmodel_blstm_fast.add(Dropout(0.1))\nmodel_blstm_fast.add(Dense(1, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:41:09.097113Z","iopub.execute_input":"2021-06-20T12:41:09.097506Z","iopub.status.idle":"2021-06-20T12:41:09.978133Z","shell.execute_reply.started":"2021-06-20T12:41:09.097469Z","shell.execute_reply":"2021-06-20T12:41:09.976766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_blstm_fast.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel_blstm_fast.summary()\nbatch_size_blstm = 32\nepochs_blstm = 5","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:41:09.979987Z","iopub.execute_input":"2021-06-20T12:41:09.98034Z","iopub.status.idle":"2021-06-20T12:41:10.008665Z","shell.execute_reply.started":"2021-06-20T12:41:09.980307Z","shell.execute_reply":"2021-06-20T12:41:10.007222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\nhist_blstm_fast = model_blstm_fast.fit(X_train, Y_train, batch_size=batch_size_blstm, epochs=epochs_blstm,\n                             validation_data = (X_val, Y_val))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:41:10.010741Z","iopub.execute_input":"2021-06-20T12:41:10.011238Z","iopub.status.idle":"2021-06-20T12:43:10.660207Z","shell.execute_reply.started":"2021-06-20T12:41:10.01119Z","shell.execute_reply":"2021-06-20T12:43:10.659072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate model\nloss_blstm_fast, acc_blstm_fast = model_blstm_fast.evaluate(X_val, Y_val, verbose=0)\nprint('Test Accuracy: %f' % (acc_blstm_fast*100))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:10.662552Z","iopub.execute_input":"2021-06-20T12:43:10.66302Z","iopub.status.idle":"2021-06-20T12:43:12.450463Z","shell.execute_reply.started":"2021-06-20T12:43:10.662968Z","shell.execute_reply":"2021-06-20T12:43:12.449637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get prediction label\ny_pred_val_blstm_fast = model_blstm_fast.predict_classes(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:12.451643Z","iopub.execute_input":"2021-06-20T12:43:12.45194Z","iopub.status.idle":"2021-06-20T12:43:14.978411Z","shell.execute_reply.started":"2021-06-20T12:43:12.451908Z","shell.execute_reply":"2021-06-20T12:43:14.977253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get prediction label\ny_pred_blstm_fast = model_blstm_fast.predict_classes(x_test_padded)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:14.979922Z","iopub.execute_input":"2021-06-20T12:43:14.98024Z","iopub.status.idle":"2021-06-20T12:43:18.749017Z","shell.execute_reply.started":"2021-06-20T12:43:14.980207Z","shell.execute_reply":"2021-06-20T12:43:18.747847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN Model\n","metadata":{}},{"cell_type":"code","source":"model_cnn_fast = Sequential()\nmodel_cnn_fast.add(Embedding(vocab_size, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False))\nmodel_cnn_fast.add(Conv1D(filters=64, kernel_size=4, activation='relu', padding='same'))\nmodel_cnn_fast.add(MaxPooling1D(pool_size=2))\nmodel_cnn_fast.add(Conv1D(filters=64, kernel_size=8, activation='relu', padding='same'))\nmodel_cnn_fast.add(MaxPooling1D(pool_size=2))\nmodel_cnn_fast.add(Conv1D(filters=64, kernel_size=16, activation='relu', padding='same'))\nmodel_cnn_fast.add(GlobalMaxPooling1D())\nmodel_cnn_fast.add(Dropout(0.1))\nmodel_cnn_fast.add(Dense(500, activation=\"sigmoid\"))\nmodel_cnn_fast.add(Dense(1, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:18.750759Z","iopub.execute_input":"2021-06-20T12:43:18.751079Z","iopub.status.idle":"2021-06-20T12:43:18.898835Z","shell.execute_reply.started":"2021-06-20T12:43:18.751046Z","shell.execute_reply":"2021-06-20T12:43:18.897696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_cnn_fast.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel_cnn_fast.summary()\nbatch_size_cnn = 64\nepochs_cnn = 10","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:18.900523Z","iopub.execute_input":"2021-06-20T12:43:18.900944Z","iopub.status.idle":"2021-06-20T12:43:18.926134Z","shell.execute_reply.started":"2021-06-20T12:43:18.900899Z","shell.execute_reply":"2021-06-20T12:43:18.925229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\nhist_cnn_fast = model_cnn_fast.fit(X_train, Y_train, batch_size=batch_size_cnn, epochs=epochs_cnn,\n                        validation_data = (X_val, Y_val))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:18.9277Z","iopub.execute_input":"2021-06-20T12:43:18.928355Z","iopub.status.idle":"2021-06-20T12:43:39.927209Z","shell.execute_reply.started":"2021-06-20T12:43:18.928296Z","shell.execute_reply":"2021-06-20T12:43:39.926194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate model\nloss_cnn_fast, acc_cnn_fast = model_cnn_fast.evaluate(X_val, Y_val, verbose=0)\nprint('Test Accuracy: %f' % (acc_cnn_fast*100))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:39.929534Z","iopub.execute_input":"2021-06-20T12:43:39.930036Z","iopub.status.idle":"2021-06-20T12:43:40.185857Z","shell.execute_reply.started":"2021-06-20T12:43:39.929989Z","shell.execute_reply":"2021-06-20T12:43:40.184684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_val_cnn_fast = model_cnn_fast.predict_classes(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:40.189747Z","iopub.execute_input":"2021-06-20T12:43:40.190142Z","iopub.status.idle":"2021-06-20T12:43:40.486269Z","shell.execute_reply.started":"2021-06-20T12:43:40.190107Z","shell.execute_reply":"2021-06-20T12:43:40.485083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get prediction label\ny_pred_cnn_fast = model_cnn_fast.predict_classes(x_test_padded)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:40.487624Z","iopub.execute_input":"2021-06-20T12:43:40.487968Z","iopub.status.idle":"2021-06-20T12:43:40.930426Z","shell.execute_reply.started":"2021-06-20T12:43:40.487936Z","shell.execute_reply":"2021-06-20T12:43:40.929359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation and Confusion Matrix","metadata":{}},{"cell_type":"code","source":"def model_evaluation(model):\n    t_loss = model.history['loss']\n    t_acc  = model.history['accuracy']\n    v_loss = model.history['val_loss']\n    v_acc  = model.history['val_accuracy']\n    x_axis = len(t_loss)\n\n    fig,(ax1) = plt.subplots(1,1,figsize=(12,8))\n    ax1.plot(t_acc,color = 'blue',label = 'Train')\n    ax1.plot(v_acc,color = 'orange',label = 'Val')\n    ax1.set_title('Accuracy Plot')\n    ax1.set_xlabel(\"#Epochs\")\n    ax1.set_ylabel(\"Accuracy\")\n    ax1.legend()\n    \n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:40.934177Z","iopub.execute_input":"2021-06-20T12:43:40.93452Z","iopub.status.idle":"2021-06-20T12:43:40.944391Z","shell.execute_reply.started":"2021-06-20T12:43:40.934484Z","shell.execute_reply":"2021-06-20T12:43:40.943464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation(hist_blstm_fast)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:40.945578Z","iopub.execute_input":"2021-06-20T12:43:40.946021Z","iopub.status.idle":"2021-06-20T12:43:41.228193Z","shell.execute_reply.started":"2021-06-20T12:43:40.945987Z","shell.execute_reply":"2021-06-20T12:43:41.227349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation(hist_cnn_fast)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:41.229325Z","iopub.execute_input":"2021-06-20T12:43:41.229773Z","iopub.status.idle":"2021-06-20T12:43:41.462851Z","shell.execute_reply.started":"2021-06-20T12:43:41.229739Z","shell.execute_reply":"2021-06-20T12:43:41.462004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    \n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    print(im)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\nclass_names = np.array([0, 1])\nnp.set_printoptions(precision=2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:41.464139Z","iopub.execute_input":"2021-06-20T12:43:41.464614Z","iopub.status.idle":"2021-06-20T12:43:41.477428Z","shell.execute_reply.started":"2021-06-20T12:43:41.464578Z","shell.execute_reply":"2021-06-20T12:43:41.476547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_blstm_fast, classes=class_names)\n# plt.savefig('cm-blstm.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_blstm_fast, classes=class_names, normalize=True)\n# Decomment following line if you want to save the figure\n# plt.savefig('cm-blstm-normalized.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:41.478574Z","iopub.execute_input":"2021-06-20T12:43:41.478976Z","iopub.status.idle":"2021-06-20T12:43:42.040176Z","shell.execute_reply.started":"2021-06-20T12:43:41.478946Z","shell.execute_reply":"2021-06-20T12:43:42.038792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_cnn_fast, classes=class_names)\n# plt.savefig('cm-cnn.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_cnn_fast, classes=class_names, normalize=True)\n# plt.savefig('cm-cnn-normalized.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:42.041794Z","iopub.execute_input":"2021-06-20T12:43:42.042095Z","iopub.status.idle":"2021-06-20T12:43:42.562259Z","shell.execute_reply.started":"2021-06-20T12:43:42.042066Z","shell.execute_reply":"2021-06-20T12:43:42.560858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"(Weighted) F1 score of FasttextEmb B-LSTM model:\")\nf1_score(Y_val, y_pred_val_blstm_fast, average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:42.563963Z","iopub.execute_input":"2021-06-20T12:43:42.564361Z","iopub.status.idle":"2021-06-20T12:43:42.576978Z","shell.execute_reply.started":"2021-06-20T12:43:42.564324Z","shell.execute_reply":"2021-06-20T12:43:42.57535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"(Weighted) F1 score of FasttextEmb B-LSTM model:\")\nf1_score(Y_val, y_pred_val_cnn_fast, average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:42.578978Z","iopub.execute_input":"2021-06-20T12:43:42.579497Z","iopub.status.idle":"2021-06-20T12:43:42.591383Z","shell.execute_reply.started":"2021-06-20T12:43:42.579422Z","shell.execute_reply":"2021-06-20T12:43:42.590525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Keras embedding","metadata":{}},{"cell_type":"markdown","source":"### BLSTM","metadata":{}},{"cell_type":"code","source":"model_blstm_keras = Sequential()\nmodel_blstm_keras.add(Embedding(vocab_size, 300, input_length=max_length))\nmodel_blstm_keras.add(Bidirectional(LSTM(300, return_sequences=True, name='lstm_layer')))\nmodel_blstm_keras.add(GlobalMaxPool1D())\nmodel_blstm_keras.add(Dropout(0.3))\nmodel_blstm_keras.add(Dense(300, activation=\"relu\"))\nmodel_blstm_keras.add(Dropout(0.3))\nmodel_blstm_keras.add(Dense(300, activation=\"relu\"))\nmodel_blstm_keras.add(Dropout(0.3))\nmodel_blstm_keras.add(Dense(1, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:42.592957Z","iopub.execute_input":"2021-06-20T12:43:42.593272Z","iopub.status.idle":"2021-06-20T12:43:43.24891Z","shell.execute_reply.started":"2021-06-20T12:43:42.593241Z","shell.execute_reply":"2021-06-20T12:43:43.247475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_blstm_keras.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel_blstm_keras.summary()\nbatch_size_blstm = 32\nepochs_blstm = 5","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:43.250663Z","iopub.execute_input":"2021-06-20T12:43:43.251084Z","iopub.status.idle":"2021-06-20T12:43:43.273866Z","shell.execute_reply.started":"2021-06-20T12:43:43.251041Z","shell.execute_reply":"2021-06-20T12:43:43.272457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\nhist_blstm_keras = model_blstm_keras.fit(X_train, Y_train, batch_size=batch_size_blstm, epochs=epochs_blstm,\n                             validation_data = (X_val, Y_val))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:43:43.275766Z","iopub.execute_input":"2021-06-20T12:43:43.276222Z","iopub.status.idle":"2021-06-20T12:46:44.313689Z","shell.execute_reply.started":"2021-06-20T12:43:43.276171Z","shell.execute_reply":"2021-06-20T12:46:44.312578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate model\nloss_blstm_keras, acc_blstm_keras = model_blstm_keras.evaluate(X_val, Y_val, verbose=0)\nprint('Test Accuracy: %f' % (acc_blstm_keras*100))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:46:44.315937Z","iopub.execute_input":"2021-06-20T12:46:44.316257Z","iopub.status.idle":"2021-06-20T12:46:46.157001Z","shell.execute_reply.started":"2021-06-20T12:46:44.316226Z","shell.execute_reply":"2021-06-20T12:46:46.155784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get prediction label\ny_pred_val_blstm_keras = model_blstm_keras.predict_classes(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:46:46.160583Z","iopub.execute_input":"2021-06-20T12:46:46.160916Z","iopub.status.idle":"2021-06-20T12:46:48.827738Z","shell.execute_reply.started":"2021-06-20T12:46:46.160884Z","shell.execute_reply":"2021-06-20T12:46:48.826609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get prediction label\ny_pred_blstm_keras = model_blstm_keras.predict_classes(x_test_padded)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:46:48.829401Z","iopub.execute_input":"2021-06-20T12:46:48.829898Z","iopub.status.idle":"2021-06-20T12:46:53.2689Z","shell.execute_reply.started":"2021-06-20T12:46:48.829851Z","shell.execute_reply":"2021-06-20T12:46:53.267942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation(hist_blstm_keras)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:46:53.270475Z","iopub.execute_input":"2021-06-20T12:46:53.271027Z","iopub.status.idle":"2021-06-20T12:46:53.52777Z","shell.execute_reply.started":"2021-06-20T12:46:53.270989Z","shell.execute_reply":"2021-06-20T12:46:53.526537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN Model","metadata":{}},{"cell_type":"code","source":"model_cnn_keras = Sequential()\nmodel_cnn_keras.add(Embedding(vocab_size, 300, input_length=max_length))\nmodel_cnn_keras.add(Conv1D(filters=64, kernel_size=4, activation='relu', padding='same'))\nmodel_cnn_keras.add(MaxPooling1D(pool_size=2))\nmodel_cnn_keras.add(Conv1D(filters=64, kernel_size=8, activation='relu', padding='same'))\nmodel_cnn_keras.add(MaxPooling1D(pool_size=2))\nmodel_cnn_keras.add(Conv1D(filters=64, kernel_size=16, activation='relu', padding='same'))\nmodel_cnn_keras.add(GlobalMaxPooling1D())\nmodel_cnn_keras.add(Dropout(0.1))\nmodel_cnn_keras.add(Dense(500, activation=\"sigmoid\"))\nmodel_cnn_keras.add(Dense(1, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:46:53.529482Z","iopub.execute_input":"2021-06-20T12:46:53.529832Z","iopub.status.idle":"2021-06-20T12:46:53.652078Z","shell.execute_reply.started":"2021-06-20T12:46:53.529798Z","shell.execute_reply":"2021-06-20T12:46:53.650853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel_cnn_keras.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel_cnn_keras.summary()\nbatch_size_cnn = 64\nepochs_cnn = 10","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:46:53.653456Z","iopub.execute_input":"2021-06-20T12:46:53.653731Z","iopub.status.idle":"2021-06-20T12:46:53.673526Z","shell.execute_reply.started":"2021-06-20T12:46:53.653702Z","shell.execute_reply":"2021-06-20T12:46:53.671291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model\nhist_cnn_keras = model_cnn_keras.fit(X_train, Y_train, batch_size=batch_size_cnn, epochs=epochs_cnn,\n                        validation_data = (X_val, Y_val))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:46:53.677033Z","iopub.execute_input":"2021-06-20T12:46:53.677383Z","iopub.status.idle":"2021-06-20T12:47:43.834078Z","shell.execute_reply.started":"2021-06-20T12:46:53.677352Z","shell.execute_reply":"2021-06-20T12:47:43.832634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate model\nloss_cnn_keras, acc_cnn_keras = model_cnn_keras.evaluate(X_val, Y_val, verbose=0)\nprint('Test Accuracy: %f' % (acc_cnn_keras*100))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:43.836382Z","iopub.execute_input":"2021-06-20T12:47:43.836757Z","iopub.status.idle":"2021-06-20T12:47:44.094561Z","shell.execute_reply.started":"2021-06-20T12:47:43.836722Z","shell.execute_reply":"2021-06-20T12:47:44.093716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_val_cnn_keras= model_cnn_keras.predict_classes(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:44.096052Z","iopub.execute_input":"2021-06-20T12:47:44.096336Z","iopub.status.idle":"2021-06-20T12:47:44.448013Z","shell.execute_reply.started":"2021-06-20T12:47:44.096308Z","shell.execute_reply":"2021-06-20T12:47:44.446612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get prediction label\ny_pred_cnn_keras = model_cnn_keras.predict_classes(x_test_padded)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:44.449585Z","iopub.execute_input":"2021-06-20T12:47:44.449912Z","iopub.status.idle":"2021-06-20T12:47:44.890623Z","shell.execute_reply.started":"2021-06-20T12:47:44.449879Z","shell.execute_reply":"2021-06-20T12:47:44.88962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_evaluation(hist_cnn_keras)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:44.891929Z","iopub.execute_input":"2021-06-20T12:47:44.892254Z","iopub.status.idle":"2021-06-20T12:47:45.130104Z","shell.execute_reply.started":"2021-06-20T12:47:44.892222Z","shell.execute_reply":"2021-06-20T12:47:45.128772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_blstm_keras, classes=class_names)\n# plt.savefig('cm-blstm.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_blstm_keras, classes=class_names, normalize=True)\n# Decomment following line if you want to save the figure\n# plt.savefig('cm-blstm-normalized.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:45.13432Z","iopub.execute_input":"2021-06-20T12:47:45.134742Z","iopub.status.idle":"2021-06-20T12:47:45.618343Z","shell.execute_reply.started":"2021-06-20T12:47:45.134704Z","shell.execute_reply":"2021-06-20T12:47:45.616901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_cnn_keras, classes=class_names)\n# plt.savefig('cm-cnn.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_cnn_keras, classes=class_names, normalize=True)\n# plt.savefig('cm-cnn-normalized.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:45.620261Z","iopub.execute_input":"2021-06-20T12:47:45.620721Z","iopub.status.idle":"2021-06-20T12:47:46.124233Z","shell.execute_reply.started":"2021-06-20T12:47:45.620673Z","shell.execute_reply":"2021-06-20T12:47:46.12302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"(Weighted) F1 score of KerasEmb B-LSTM model:\")\nf1_score(Y_val, y_pred_val_blstm_keras, average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:46.125836Z","iopub.execute_input":"2021-06-20T12:47:46.126152Z","iopub.status.idle":"2021-06-20T12:47:46.137064Z","shell.execute_reply.started":"2021-06-20T12:47:46.12612Z","shell.execute_reply":"2021-06-20T12:47:46.135873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"(Weighted) F1 score of KerasEmb CNN model:\")\nf1_score(Y_val, y_pred_val_cnn_keras, average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:46.138372Z","iopub.execute_input":"2021-06-20T12:47:46.138896Z","iopub.status.idle":"2021-06-20T12:47:46.152082Z","shell.execute_reply.started":"2021-06-20T12:47:46.138859Z","shell.execute_reply":"2021-06-20T12:47:46.15073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning Algorithms","metadata":{}},{"cell_type":"code","source":"# sklearn\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.pipeline import Pipeline\n\n# Measuring metrics\nfrom sklearn.metrics import f1_score\nfrom nltk.tokenize import word_tokenize","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:46.153655Z","iopub.execute_input":"2021-06-20T12:47:46.154035Z","iopub.status.idle":"2021-06-20T12:47:46.166404Z","shell.execute_reply.started":"2021-06-20T12:47:46.154003Z","shell.execute_reply":"2021-06-20T12:47:46.164958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# When building the vocabulary ignore terms that have a document frequency strictly lower than\n# the given threshold. This value is also called cut-off in the literature.\nmin_df = 1\n\n# Tokenize function used in Vectorizer\ndef tokenize(text):\n    return word_tokenize(text)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:46.168088Z","iopub.execute_input":"2021-06-20T12:47:46.168532Z","iopub.status.idle":"2021-06-20T12:47:46.174832Z","shell.execute_reply.started":"2021-06-20T12:47:46.168496Z","shell.execute_reply":"2021-06-20T12:47:46.173585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive Bayes","metadata":{}},{"cell_type":"code","source":"train.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:46.176417Z","iopub.execute_input":"2021-06-20T12:47:46.176787Z","iopub.status.idle":"2021-06-20T12:47:46.197407Z","shell.execute_reply.started":"2021-06-20T12:47:46.176744Z","shell.execute_reply":"2021-06-20T12:47:46.195998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:46.199283Z","iopub.execute_input":"2021-06-20T12:47:46.199654Z","iopub.status.idle":"2021-06-20T12:47:46.213076Z","shell.execute_reply.started":"2021-06-20T12:47:46.199619Z","shell.execute_reply":"2021-06-20T12:47:46.211854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = test['clean_text']\nprint('Number of testing sentence: ', x_test.shape)\nx_test = np.asarray(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:46.214772Z","iopub.execute_input":"2021-06-20T12:47:46.215202Z","iopub.status.idle":"2021-06-20T12:47:46.22716Z","shell.execute_reply.started":"2021-06-20T12:47:46.215165Z","shell.execute_reply":"2021-06-20T12:47:46.225679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = np.asarray(train['clean_text'])\ny_train = np.asarray(train['target'])\nprint('Number of training sentence: ', x_train.shape)\nprint('Number of training label: ', y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:46.228972Z","iopub.execute_input":"2021-06-20T12:47:46.229495Z","iopub.status.idle":"2021-06-20T12:47:46.243826Z","shell.execute_reply.started":"2021-06-20T12:47:46.229417Z","shell.execute_reply":"2021-06-20T12:47:46.242537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:46.245851Z","iopub.execute_input":"2021-06-20T12:47:46.246232Z","iopub.status.idle":"2021-06-20T12:47:46.259833Z","shell.execute_reply.started":"2021-06-20T12:47:46.246197Z","shell.execute_reply":"2021-06-20T12:47:46.258649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Naive Bayes Model\nnaive_bayes = Pipeline([('vect', CountVectorizer(tokenizer=tokenize,\n                                              analyzer='word', ngram_range=(1, 2), min_df=min_df, lowercase=False)),\n                     ('tfidf', TfidfTransformer(sublinear_tf=True)),\n                     ('clf', MultinomialNB())])\nnaive_bayes = naive_bayes.fit(X_train, Y_train)\nnaive_score = naive_bayes.score(X_val, Y_val)\nprint('Naive Bayes Model: ', naive_score)\npredict_val_nb = naive_bayes.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:46.261303Z","iopub.execute_input":"2021-06-20T12:47:46.261961Z","iopub.status.idle":"2021-06-20T12:47:48.011758Z","shell.execute_reply.started":"2021-06-20T12:47:46.261919Z","shell.execute_reply":"2021-06-20T12:47:48.010655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_nb = naive_bayes.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:48.014496Z","iopub.execute_input":"2021-06-20T12:47:48.014937Z","iopub.status.idle":"2021-06-20T12:47:48.595319Z","shell.execute_reply.started":"2021-06-20T12:47:48.014891Z","shell.execute_reply":"2021-06-20T12:47:48.594101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear Support Vector Machine Model","metadata":{}},{"cell_type":"code","source":"# Linear Support Vector Machine Model\nsvm = Pipeline([('vect', CountVectorizer(tokenizer=tokenize,\n                                                         analyzer='word', ngram_range=(1, 2),\n                                                         min_df=min_df, lowercase=False)),\n                                ('tfidf', TfidfTransformer(sublinear_tf=True)),\n                                ('clf-svm', LinearSVC(loss='hinge', penalty='l2',\n                                                      max_iter=5))])\n\nsvm = svm.fit(X_train, Y_train)\nlinear_svc_score = svm.score(X_val, Y_val)\nprint('Linear SVC Model: ', linear_svc_score)\npredict_val_svm = svm.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:48.59719Z","iopub.execute_input":"2021-06-20T12:47:48.597624Z","iopub.status.idle":"2021-06-20T12:47:50.339697Z","shell.execute_reply.started":"2021-06-20T12:47:48.597589Z","shell.execute_reply":"2021-06-20T12:47:50.338706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_svm = svm.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:50.340876Z","iopub.execute_input":"2021-06-20T12:47:50.341189Z","iopub.status.idle":"2021-06-20T12:47:50.925891Z","shell.execute_reply.started":"2021-06-20T12:47:50.341155Z","shell.execute_reply":"2021-06-20T12:47:50.925077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stochastic Gradient Descent Model","metadata":{}},{"cell_type":"code","source":"# SGD (Stochastic Gradient Descent) Model\nsgd = Pipeline([('vect', CountVectorizer(tokenizer=tokenize,\n                                                  analyzer='word', ngram_range=(1, 2), min_df=min_df, lowercase=False)),\n                         ('tfidf', TfidfTransformer(sublinear_tf=True)),\n                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n                                                   alpha=1e-3, max_iter=5))])\nsgd = sgd.fit(X_train, Y_train)\nsgd_score = sgd.score(X_val, Y_val)\nprint('SGD Model: ', sgd_score)\npredict_val_sgd = sgd.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:50.926989Z","iopub.execute_input":"2021-06-20T12:47:50.927422Z","iopub.status.idle":"2021-06-20T12:47:52.712343Z","shell.execute_reply.started":"2021-06-20T12:47:50.927389Z","shell.execute_reply":"2021-06-20T12:47:52.711306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_sgd = sgd.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:52.713678Z","iopub.execute_input":"2021-06-20T12:47:52.71409Z","iopub.status.idle":"2021-06-20T12:47:53.301473Z","shell.execute_reply.started":"2021-06-20T12:47:52.714047Z","shell.execute_reply":"2021-06-20T12:47:53.300426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix","metadata":{}},{"cell_type":"code","source":"Y_val = Y_val.astype(int)\npredict_val_nb = predict_val_nb.astype(int)\npredict_val_svm = predict_val_svm.astype(int)\npredict_val_sgd = predict_val_sgd.astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:53.302978Z","iopub.execute_input":"2021-06-20T12:47:53.303397Z","iopub.status.idle":"2021-06-20T12:47:53.308516Z","shell.execute_reply.started":"2021-06-20T12:47:53.303354Z","shell.execute_reply":"2021-06-20T12:47:53.307247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_nb, classes=class_names)\n# plt.savefig('cm-nb.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_nb, classes=class_names, normalize=True)\n# plt.savefig('cm-nb-normalized.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:53.310495Z","iopub.execute_input":"2021-06-20T12:47:53.310936Z","iopub.status.idle":"2021-06-20T12:47:53.876572Z","shell.execute_reply.started":"2021-06-20T12:47:53.31089Z","shell.execute_reply":"2021-06-20T12:47:53.87522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_svm, classes=class_names)\n# plt.savefig('cm-svm.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_svm, classes=class_names, normalize=True)\n# plt.savefig('cm-svm-normalized.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:53.878884Z","iopub.execute_input":"2021-06-20T12:47:53.879355Z","iopub.status.idle":"2021-06-20T12:47:54.473517Z","shell.execute_reply.started":"2021-06-20T12:47:53.879307Z","shell.execute_reply":"2021-06-20T12:47:54.472626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_sgd, classes=class_names)\n# plt.savefig('cm-sgd.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_sgd, classes=class_names, normalize=True)\n# plt.savefig('cm-sgd-normalized.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:54.475096Z","iopub.execute_input":"2021-06-20T12:47:54.47574Z","iopub.status.idle":"2021-06-20T12:47:54.929732Z","shell.execute_reply.started":"2021-06-20T12:47:54.475686Z","shell.execute_reply":"2021-06-20T12:47:54.928164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# F1 Score","metadata":{}},{"cell_type":"code","source":"print(\"F1 score of NB model:\")\nf1_score(Y_val, predict_val_nb, average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:54.931267Z","iopub.execute_input":"2021-06-20T12:47:54.931615Z","iopub.status.idle":"2021-06-20T12:47:54.946197Z","shell.execute_reply.started":"2021-06-20T12:47:54.931582Z","shell.execute_reply":"2021-06-20T12:47:54.944897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"F1 score of SVM model:\")\nf1_score(Y_val, predict_val_svm, average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:54.947935Z","iopub.execute_input":"2021-06-20T12:47:54.948268Z","iopub.status.idle":"2021-06-20T12:47:54.961203Z","shell.execute_reply.started":"2021-06-20T12:47:54.948236Z","shell.execute_reply":"2021-06-20T12:47:54.959844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"F1 score of SGD model:\")\nf1_score(Y_val, predict_val_sgd, average='weighted')","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:54.962943Z","iopub.execute_input":"2021-06-20T12:47:54.963415Z","iopub.status.idle":"2021-06-20T12:47:54.975187Z","shell.execute_reply.started":"2021-06-20T12:47:54.963363Z","shell.execute_reply":"2021-06-20T12:47:54.973989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Try other models","metadata":{}},{"cell_type":"code","source":"train.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:54.977047Z","iopub.execute_input":"2021-06-20T12:47:54.977522Z","iopub.status.idle":"2021-06-20T12:47:54.992699Z","shell.execute_reply.started":"2021-06-20T12:47:54.977471Z","shell.execute_reply":"2021-06-20T12:47:54.99137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:54.99435Z","iopub.execute_input":"2021-06-20T12:47:54.994942Z","iopub.status.idle":"2021-06-20T12:47:55.011427Z","shell.execute_reply.started":"2021-06-20T12:47:54.994902Z","shell.execute_reply":"2021-06-20T12:47:55.010025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport lightgbm as lgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import Perceptron\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport re\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import roc_auc_score #Compute Area Under the Curve (AUC) from prediction scores\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom warnings import filterwarnings\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.pipeline import Pipeline","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:55.013062Z","iopub.execute_input":"2021-06-20T12:47:55.013467Z","iopub.status.idle":"2021-06-20T12:47:56.676418Z","shell.execute_reply.started":"2021-06-20T12:47:55.013387Z","shell.execute_reply":"2021-06-20T12:47:56.675592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nX = train['clean_text'].to_numpy()\ny = train['target'].to_numpy()\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    #X_train = X.loc[train_index]\n    X_train, X_test = X[train_index], X[test_index]\n\n    y_train, y_test = y[train_index], y[test_index]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:56.677879Z","iopub.execute_input":"2021-06-20T12:47:56.678468Z","iopub.status.idle":"2021-06-20T12:47:56.702134Z","shell.execute_reply.started":"2021-06-20T12:47:56.678389Z","shell.execute_reply":"2021-06-20T12:47:56.700634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(2.5,5))\nplt.title(\"Distribution in Train dataset\")\np1 = sns.countplot(y_train, palette = 'plasma')\n\nfor p in p1.patches:\n        p1.annotate('{:6.2f}%'.format(p.get_height()/len(y_train)*100), (p.get_x()+0.1, p.get_height()+50))\n        \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:56.703821Z","iopub.execute_input":"2021-06-20T12:47:56.70412Z","iopub.status.idle":"2021-06-20T12:47:56.852251Z","shell.execute_reply.started":"2021-06-20T12:47:56.704091Z","shell.execute_reply":"2021-06-20T12:47:56.850846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(2.5,5))\nplt.title(\"Distribution in Test dataset\")\np1 = sns.countplot(y_test, palette = 'plasma')\n\nfor p in p1.patches:\n        p1.annotate('{:6.2f}%'.format(p.get_height()/len(y_test)*100), (p.get_x()+0.2, p.get_height()+12))\n        \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:56.853603Z","iopub.execute_input":"2021-06-20T12:47:56.853902Z","iopub.status.idle":"2021-06-20T12:47:56.987204Z","shell.execute_reply.started":"2021-06-20T12:47:56.853874Z","shell.execute_reply":"2021-06-20T12:47:56.985851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_pipeline = Pipeline([('CVec', CountVectorizer(stop_words='english')),\n                     ('Tfidf', TfidfTransformer())])\n\nX_train_tranformed = tweets_pipeline.fit_transform(X_train)\nX_test_tranformed = tweets_pipeline.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:56.988784Z","iopub.execute_input":"2021-06-20T12:47:56.989203Z","iopub.status.idle":"2021-06-20T12:47:57.143146Z","shell.execute_reply.started":"2021-06-20T12:47:56.989157Z","shell.execute_reply":"2021-06-20T12:47:57.142011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifiers = {\n    \"Logistic Regression\": LogisticRegression(class_weight='balanced'),\n    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced'),\n    \"k-Nearest Neighbors\": KNeighborsClassifier(),\n    \"Linear SVM\": SVC(class_weight='balanced'),\n    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    'RidgeClassifier': RidgeClassifier(class_weight='balanced'),\n    'AdaBoost': AdaBoostClassifier(n_estimators=100),\n    'MNB': MultinomialNB(),\n    'Perceptron': Perceptron(class_weight='balanced'),\n    'xgboost': XGBClassifier(n_estimators=300),\n    'catboost': CatBoostClassifier(verbose=0)\n        \n}","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:57.144852Z","iopub.execute_input":"2021-06-20T12:47:57.145177Z","iopub.status.idle":"2021-06-20T12:47:57.156163Z","shell.execute_reply.started":"2021-06-20T12:47:57.145144Z","shell.execute_reply":"2021-06-20T12:47:57.154304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_classifiers = len(classifiers.keys())\n\nfrom time import process_time \n\n\ndef batch_classify(X_train_tranformed, y_train, X_test_tranformed, y_test, verbose = True):\n    df_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,3)), columns = ['Classifier', 'Area Under Curve', 'Training time'])\n    count = 0\n    for key, classifier in classifiers.items():\n        t_start = process_time()  \n        classifier.fit(X_train_tranformed, y_train)\n        t_stop = process_time() \n        t_elapsed = t_stop - t_start\n        y_predicted = classifier.predict(X_test_tranformed)\n        \n        df_results.loc[count,'Classifier'] = key\n        df_results.loc[count,'Area Under Curve'] = roc_auc_score(y_test, y_predicted)\n        df_results.loc[count,'Training time'] = t_elapsed\n        if verbose:\n            print(\"trained {c} in {f:.2f} s\".format(c=key, f=t_elapsed))\n        count+=1\n\n    return df_results","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:57.158121Z","iopub.execute_input":"2021-06-20T12:47:57.158469Z","iopub.status.idle":"2021-06-20T12:47:57.170017Z","shell.execute_reply.started":"2021-06-20T12:47:57.158404Z","shell.execute_reply":"2021-06-20T12:47:57.168888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_results = batch_classify(X_train_tranformed, y_train,X_test_tranformed, y_test)\nprint(df_results.sort_values(by='Area Under Curve', ascending=False))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T12:47:57.171241Z","iopub.execute_input":"2021-06-20T12:47:57.171685Z","iopub.status.idle":"2021-06-20T12:49:20.749266Z","shell.execute_reply.started":"2021-06-20T12:47:57.171652Z","shell.execute_reply":"2021-06-20T12:49:20.748206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}