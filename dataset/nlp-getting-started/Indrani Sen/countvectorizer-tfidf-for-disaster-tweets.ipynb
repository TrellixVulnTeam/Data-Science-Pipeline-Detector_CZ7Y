{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim import parsing\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train= pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(['keyword', 'location'],axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Visualization**","metadata":{}},{"cell_type":"code","source":"plot=sns.countplot(train['target'])\nplot.set_title(\"Count of disaster and non disaster tweets\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is balanced.","metadata":{}},{"cell_type":"code","source":"mylabels=[\"Non-Disaster\", \"Disaster\"]\nmycolors=['pink', 'blue']\nplt.pie(train['target'].value_counts(), labels=mylabels, colors=mycolors,autopct='%1.1f%%')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WordCloud for Disaster Tweets**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(train[train.target == 1].text))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**WordCloud for Non-Disaster Tweets**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (15,15))\nwc = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(train[train.target == 0].text))\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"def transformText(text):\n  # All the necessary preprocessing on our text of choice\n    stops = set(stopwords.words(\"english\"))\n  # Convert text to lower\n    text = text.lower()\n  # Removing non ASCII chars    \n    text = re.sub(r'[^\\x00-\\x7f]',r' ',text) \n    text = re.sub('\\[[^]]*\\]', '', text)\n    text = re.sub('http','',text)\n    text= gensim.parsing.preprocessing.strip_non_alphanum(text)                       \n  # Strip multiple whitespaces\n    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n  # Removing all the stopwords\n    filtered_words = [word for word in text.split() if word not in stops]\n  # Removing all the tokens with lesser than 3 characters\n    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=3)\n  # Preprocessed text after stop words removal\n    text = \" \".join(filtered_words)\n  # Remove the punctuation\n    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n  # Strip all the numerics\n    text = gensim.parsing.preprocessing.strip_numeric(text)\n  # Strip multiple whitespaces\n    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n  # Stemming\n    return gensim.parsing.preprocessing.stem_text(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text']=train['text'].apply(transformText)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**N-Gram Analysis**","metadata":{}},{"cell_type":"code","source":"texts=''.join(train['text'])\nwords=texts.split(\" \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def draw_n_gram(words,i):\n    n_gram=(pd.Series(nltk.ngrams(words,i)).value_counts())[:15]\n    n_gram_df=pd.DataFrame(n_gram)\n    n_gram_df=n_gram_df.reset_index()\n    n_gram_df = n_gram_df.rename(columns={\"index\": \"word\", 0: \"count\"})\n    print(n_gram_df.head())\n    plt.figure(figsize = (16,9))\n    return sns.barplot(x='count',y='word', data=n_gram_df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Unigram Analysis**","metadata":{}},{"cell_type":"code","source":"draw_n_gram(words,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bi-gram Analysis**","metadata":{}},{"cell_type":"code","source":"draw_n_gram(words,2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tri-gram Analysis**","metadata":{}},{"cell_type":"code","source":"draw_n_gram(words,3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=train['text']\ny=train['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting data into train and test set**","metadata":{}},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction with Count Vectorizer and TfidfTransformer(Term Frequency-Inverse Document Frequency)","metadata":{}},{"cell_type":"code","source":"vectorizer=CountVectorizer()\ntransformer=TfidfTransformer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For train data**","metadata":{}},{"cell_type":"code","source":"X_train_vect=vectorizer.fit_transform(X_train)\nX_train_trans=transformer.fit_transform(X_train_vect)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_trans.toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For test data**","metadata":{}},{"cell_type":"code","source":"X_test_vect=vectorizer.transform(X_test)\nX_test_trans=transformer.transform(X_test_vect)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_trans.toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Model ","metadata":{}},{"cell_type":"code","source":"lr=LogisticRegression()\ndtc= DecisionTreeClassifier()\nrfc= RandomForestClassifier()\nsvm= SVC()\nknn= KNeighborsClassifier()\nnb= GaussianNB()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr.fit(X_train_trans,y_train)\ndtc.fit(X_train_trans,y_train)\nrfc.fit(X_train_trans ,y_train)\nsvm.fit(X_train_trans ,y_train)\nknn.fit(X_train_trans ,y_train)\n#nb.fit(X_train_trans ,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Making Predictions**","metadata":{}},{"cell_type":"code","source":"predict_lr = lr.predict(X_test_trans)\npredict_dtc = dtc.predict(X_test_trans)\npredict_rfc = rfc.predict(X_test_trans)\npredict_svm = svm.predict(X_test_trans)\npredict_knn = knn.predict(X_test_trans)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Checking model performance**","metadata":{}},{"cell_type":"code","source":"acc_1=accuracy_score(predict_lr,y_test)\nprint(\"Accuracy of LogisticRegression = \" +str(acc_1))\nacc_2=accuracy_score(predict_dtc,y_test)\nprint(\"Accuracy of DecisionTreeClassifier = \" +str(acc_2))\nacc_3 =  accuracy_score(predict_rfc,y_test)\nprint(\"Accuracy of RandomForestClassifier = \" +str(acc_3))\nacc_4 = accuracy_score(predict_svm,y_test)\nprint(\"Accuracy of SupportVectorClassifier = \" +str(acc_4))\nacc_5 = accuracy_score(predict_knn,y_test)\nprint(\"Accuracy of KNearestNeighbor = \" +str(acc_5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here, we see that SupportVectorClassifier gives the best accuracy score of 80.47%**","metadata":{}},{"cell_type":"markdown","source":"**Now, we predict for test data using SVC**","metadata":{}},{"cell_type":"code","source":"X_test=test['text']\nX_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_vec=vectorizer.transform(X_test)\nX_test_tran=transformer.transform(X_test_vec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_tran.toarray()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions= svm.predict(X_test_tran)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['id']= test['id']\nsubmission['target']= predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('solution.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}