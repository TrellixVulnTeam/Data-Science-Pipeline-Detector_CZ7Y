{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro","metadata":{}},{"cell_type":"markdown","source":"**Notebook score with bernoulli naive bayes: 0.79 <p>**\n\nIn this basic notebook, we will utilize the libraries **re** & **nltk** to clean and stem our tweets before fitting it to the model. The model in question is the naive bayes classifier from **SKLearn**, and so is the simplest there is! EDA is not a main focus here (because I may have deleted some code before... oops.)\n\nFeel free to ask any question in the comments, but I am also open to feedback and suggestions. Thanks:)\n\nCredits for inspiration:\n- [NLP Getting Started Tutorial by Phil Culliton](https://www.kaggle.com/code/philculliton/nlp-getting-started-tutorial) \n- [HuggingFace TFBertModel](https://www.kaggle.com/code/dhruv1234/huggingface-tfbertmodel/notebook?scriptVersionId=34225936)\n- [Basic EDA,Cleaning and GloVe by shahules](https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove)\n- [NLP with Disaster Tweets - EDA, Cleaning and BERT](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries & Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re # RE (Regular Expressions): For data cleaning\nimport nltk # NLTK (Natural Language Toolkit): utility\n\nfrom sklearn import feature_extraction, model_selection, metrics\nfrom sklearn import naive_bayes, tree, linear_model\nimport xgboost\n\nfrom nltk.corpus import stopwords as sw\nSTOPWORDS = sw.words('english')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# To do a full run, set to False so you can export submission results\nDUMMY_RUN = False","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:28:46.113528Z","iopub.execute_input":"2022-06-24T05:28:46.1141Z","iopub.status.idle":"2022-06-24T05:28:48.533584Z","shell.execute_reply.started":"2022-06-24T05:28:46.113967Z","shell.execute_reply":"2022-06-24T05:28:48.532306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:28:48.536398Z","iopub.execute_input":"2022-06-24T05:28:48.536884Z","iopub.status.idle":"2022-06-24T05:28:48.670462Z","shell.execute_reply.started":"2022-06-24T05:28:48.536838Z","shell.execute_reply":"2022-06-24T05:28:48.66921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"def generate_ngrams(text, n_gram=1):\n    '''credit: https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert'''\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:28:48.674455Z","iopub.execute_input":"2022-06-24T05:28:48.675401Z","iopub.status.idle":"2022-06-24T05:28:48.684454Z","shell.execute_reply.started":"2022-06-24T05:28:48.675345Z","shell.execute_reply":"2022-06-24T05:28:48.682856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keywords = train_df['keyword'].unique()\narr_df = dict()\nfor key in keywords:\n    arr_df[key] = train_df[train_df['keyword'] == key][['id','text','target']]","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:28:48.689254Z","iopub.execute_input":"2022-06-24T05:28:48.690601Z","iopub.status.idle":"2022-06-24T05:28:49.368518Z","shell.execute_reply.started":"2022-06-24T05:28:48.690539Z","shell.execute_reply":"2022-06-24T05:28:49.367558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def histogram(data, ax, color, bw):\n    sns.histplot(\n        data,\n        ax=ax,\n        color=color,\n        alpha=0.65,\n        binwidth=bw\n    )\n\ndef comparison_len_hists(keyword):\n    ''' Plots a pair of histograms of word & char lengths'''\n    non_disaster_meta_df, disaster_meta_df = pd.DataFrame(), pd.DataFrame()\n    df = arr_df[keyword]\n    \n    non_disaster_meta_df['char_length'] = df[df['target']==0]['text'].str.len()\n    non_disaster_meta_df['word_length'] = df[df['target']==0]['text'].apply(lambda x: len(x.split(' ')))\n    \n    disaster_meta_df['char_length'] = df[df['target']==1]['text'].str.len()\n    disaster_meta_df['word_length'] = df[df['target']==1]['text'].apply(lambda x: len(x.split(' ')))\n    \n    fig, ax = plt.subplots(1, 2, figsize=(15, 2.5))\n    \n    histogram(data=non_disaster_meta_df['char_length'], ax=ax[0], color='blue', bw=5)\n    histogram(data=disaster_meta_df['char_length'], ax=ax[0], color='orange', bw=5)\n    ax[0].set_title(f\"Target Distribution in char lengths of Tweets with keyword {keyword}\")\n    \n    histogram(data=non_disaster_meta_df['word_length'], ax=ax[1], color='blue', bw=1)\n    histogram(data=disaster_meta_df['word_length'], ax=ax[1], color='orange', bw=1)\n    ax[1].set_title(f\"Target Distribution in word lengths of Tweets with keyword {keyword}\")\n\n# too many to show all\nfor word in keywords[1:6]:\n    comparison_len_hists(word)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:28:49.37047Z","iopub.execute_input":"2022-06-24T05:28:49.371407Z","iopub.status.idle":"2022-06-24T05:28:52.117503Z","shell.execute_reply.started":"2022-06-24T05:28:49.371356Z","shell.execute_reply":"2022-06-24T05:28:52.116231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A very manual and vanilla approach to ngrams because I can. :)\ndef get_array_of_unigrams(df):\n    global_unigrams = dict()\n    disaster_unigrams = dict()\n    non_disaster_unigrams = dict()\n\n    arr_unigram = [non_disaster_unigrams, disaster_unigrams, global_unigrams]\n\n    for item in df.values:\n        tweet = item[-2]\n        target = item[-1]\n        non_target = 0 if (target == 1) else 1\n        for word in generate_ngrams(tweet):\n            if word in arr_unigram[2].keys():\n                arr_unigram[2][word] += 1\n                arr_unigram[target][word] += 1\n            else:\n                arr_unigram[2][word] = 1\n                arr_unigram[target][word] = 1\n                arr_unigram[non_target][word] = 0\n\n    for i, unigram in enumerate(arr_unigram):\n        new = list(unigram.items())\n        arr_unigram[i] = sorted(new, key=lambda x: x[1], reverse=True)\n    \n    return arr_unigram","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:28:52.119016Z","iopub.execute_input":"2022-06-24T05:28:52.119344Z","iopub.status.idle":"2022-06-24T05:28:52.131786Z","shell.execute_reply.started":"2022-06-24T05:28:52.119315Z","shell.execute_reply":"2022-06-24T05:28:52.130577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unigram_barplot(lst_unigram, n=40, ax=None, title=\"\"):\n    get_key = lambda x: x[0]\n    get_value = lambda x: x[1]\n    x = [get_key(item) for item in lst_unigram[:n]]\n    data = [get_value(item) for item in lst_unigram[:n]]\n    sns.barplot(y=x, x=data, ax=ax)\n    if ax==None:\n        plt.title(title)\n        plt.ylabel('Tokens')\n        plt.xlabel('Count')\n        plt.figure(figsize=(9,9))\n        plt.show()\n    else:\n        ax.set_title(title)\n        ax.set_ylabel('Tokens')\n        ax.set_xlabel('Count')\n\ndef comparison_token_unigrams(keyword=None, n=40, h=7, data=train_df):\n    ''' Plots three barplots of unigrams. '''\n    if keyword==None:\n        df=data\n        add = \"\"\n    else:\n        df = arr_df[keyword]\n        add = f\" with keyword {keyword}\"\n    \n    fig, ax = plt.subplots(1, 3, figsize=(21, h))\n    train_unigrams = get_array_of_unigrams(df)\n    unigram_barplot(train_unigrams[2], ax=ax[0], n=n, title=f\"Most frequent tokens (words) globally{add}\")\n    unigram_barplot(train_unigrams[0], ax=ax[1], n=n, title=f\"Most frequent tokens(words) in Non-Disaster Tweets{add}\")\n    unigram_barplot(train_unigrams[1], ax=ax[2], n=n, title=f\"Most frequent tokens(words) in Disaster Tweets{add}\")\n\ncomparison_token_unigrams()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:28:52.133525Z","iopub.execute_input":"2022-06-24T05:28:52.13391Z","iopub.status.idle":"2022-06-24T05:28:53.940203Z","shell.execute_reply.started":"2022-06-24T05:28:52.133875Z","shell.execute_reply":"2022-06-24T05:28:53.939093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for word in keywords[1:10]:\n    comparison_token_unigrams(word, n=10, h=5)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:28:53.941773Z","iopub.execute_input":"2022-06-24T05:28:53.942123Z","iopub.status.idle":"2022-06-24T05:28:59.243807Z","shell.execute_reply.started":"2022-06-24T05:28:53.942091Z","shell.execute_reply":"2022-06-24T05:28:59.242548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning & Preprocessing","metadata":{}},{"cell_type":"code","source":"stemmer = nltk.stem.PorterStemmer()\n\ndef clean(text):\n    ''' Cleans a piece of text. (one single tweet) '''\n    \n    # Remove URLs/links\n    new_text = remove_links(text.lower())\n    # Removes punctuation\n    new_text = re.sub('[^a-zA-Z ]', '', new_text)\n    # Lemmatization: Stems words into basic, clean/stemmed form (e.g. \"dogs\" -> \"dog\", \"cleaned\" -> \"clean\")\n    new_text = stemmer.stem(new_text)\n    # Removes 'amp' because that's a thing. (alternatively: append amp to stopwords)\n    new_text = re.sub('amp', '', new_text)\n    # Removes stopwords that don't contribute to the tweet's \"content\" (e.g. 'is', 'me', 'do')\n    new_text = remove_stopwords(new_text)\n    \n    return new_text\n\ndef remove_links(text):\n    ''' credit where due: https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove/'''\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    \n    return url.sub(r'',text)\n\ndef remove_stopwords(text):\n    ''' Removes words that are in nltk.corpus stopwords'''\n    res = []\n    for word in text.split(' '):\n        if not word in STOPWORDS:\n            res.append(word)\n    return ' '.join(res)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:28:59.245756Z","iopub.execute_input":"2022-06-24T05:28:59.246199Z","iopub.status.idle":"2022-06-24T05:28:59.257003Z","shell.execute_reply.started":"2022-06-24T05:28:59.246153Z","shell.execute_reply":"2022-06-24T05:28:59.255756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the clean() function to clean both train & test datasets\ntweets = [clean(tweet) for tweet in train_df['text']]\ntest_tweets = [clean(tweet) for tweet in test_df['text']]","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:28:59.260321Z","iopub.execute_input":"2022-06-24T05:28:59.261042Z","iopub.status.idle":"2022-06-24T05:29:00.180956Z","shell.execute_reply.started":"2022-06-24T05:28:59.260998Z","shell.execute_reply":"2022-06-24T05:29:00.179749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This converts our tweets from text form to a 'bag of words' that our model can understand\ncv = feature_extraction.text.CountVectorizer()\nX = cv.fit_transform(tweets)\ny = train_df['target']","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:29:00.182441Z","iopub.execute_input":"2022-06-24T05:29:00.182766Z","iopub.status.idle":"2022-06-24T05:29:00.351671Z","shell.execute_reply.started":"2022-06-24T05:29:00.182738Z","shell.execute_reply":"2022-06-24T05:29:00.350367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unigram with cleaned tweets","metadata":{}},{"cell_type":"markdown","source":"Let's look again at the most commonly featured words, but from the cleaned tweets","metadata":{}},{"cell_type":"code","source":"alt = pd.DataFrame({'tweet':tweets, 'target':y})\ncomparison_token_unigrams(data=alt, n=50)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:29:00.353199Z","iopub.execute_input":"2022-06-24T05:29:00.353639Z","iopub.status.idle":"2022-06-24T05:29:02.784347Z","shell.execute_reply.started":"2022-06-24T05:29:00.353593Z","shell.execute_reply":"2022-06-24T05:29:02.782831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection","metadata":{}},{"cell_type":"code","source":"# Dict of classifiers that we want to test out (in dummy run)\ndict_classifiers = {\n    'xgb'           : xgboost.XGBClassifier(),\n    'bernoulli_nb'  : naive_bayes.BernoulliNB(),\n    'bernoulli_nb_2': naive_bayes.BernoulliNB(alpha=0),\n    'bernoulli_nb_3': naive_bayes.BernoulliNB(binarize=4, alpha=0),\n    'complement_nb' : naive_bayes.ComplementNB(),\n    'decision_tree' : tree.DecisionTreeClassifier(),\n    'log_reg'       : linear_model.LogisticRegression()\n}\n\n# We cross-validate each classifier in the dict above\n# to evaluate their performance before defining 'choice'.\nif DUMMY_RUN:\n    for name in dict_classifiers:\n        clf = dict_classifiers[name]\n        scores = model_selection.cross_val_score(clf, X, y)\n        print(f\"{name} score: {np.average(scores)}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:29:02.786138Z","iopub.execute_input":"2022-06-24T05:29:02.787474Z","iopub.status.idle":"2022-06-24T05:29:02.798578Z","shell.execute_reply.started":"2022-06-24T05:29:02.787419Z","shell.execute_reply":"2022-06-24T05:29:02.797086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitting & Forecast","metadata":{}},{"cell_type":"code","source":"choice = 'bernoulli_nb'\nclf = dict_classifiers[choice]\n\nif not DUMMY_RUN:\n    # We call again our CountVectorizer\n    X_test = cv.transform(test_tweets)\n    \n    # Fit & forecast\n    clf.fit(X, y)\n    y_pred = clf.predict(X_test)\n    \n    # Export\n    out = pd.DataFrame(data={'id'   : test_df['id'],\n                            'target': y_pred})\n    out.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:29:02.800558Z","iopub.execute_input":"2022-06-24T05:29:02.801021Z","iopub.status.idle":"2022-06-24T05:29:02.879295Z","shell.execute_reply.started":"2022-06-24T05:29:02.800988Z","shell.execute_reply":"2022-06-24T05:29:02.878084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hope this has been useful and that you learnt a thing or two from this, or perhaps instead I could learn something from you if you have feedback for me :)","metadata":{}}]}