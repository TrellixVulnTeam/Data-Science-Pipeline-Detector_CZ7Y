{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import BertTokenizer, TFBertModel\n\nimport numpy as np\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameters\nmax_length = 140\nbatch_size = 32\ndev_size = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bert Tokenizer\nmodel_name = \"bert-base-multilingual-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain, dev = train_test_split(train_df, test_size=dev_size, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(data):\n    tokens = tokenizer.batch_encode_plus(data, max_length=max_length, padding='max_length', truncation=True)\n    \n    return tf.constant(tokens['input_ids'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_encoded = bert_encode(train.text)\ndev_encoded = bert_encode(dev.text)\n\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_encoded, train.target))\n    .shuffle(100)\n    .batch(batch_size)\n)\n\ndev_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((dev_encoded, dev.target))\n    .shuffle(100)\n    .batch(batch_size)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_tweets_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n    last_hidden_states = bert_encoder(input_word_ids)[0]    \n    x = tf.keras.layers.LSTM(100, dropout=0.3, recurrent_dropout=0.3)(last_hidden_states)\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    model = tf.keras.Model(inputs=input_word_ids, outputs=output)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = bert_tweets_model()\n    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    model.compile(loss='binary_crossentropy',optimizer=adam_optimizer,metrics=['accuracy'])\n\n    model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    train_dataset,\n    batch_size=batch_size,\n    epochs=3,\n    validation_data=dev_dataset,\n    verbose=2)\n    #callbacks=[tf.keras.callbacks.EarlyStopping(\n    #            patience=6,\n    #            min_delta=0.05,\n    #            baseline=0.7,\n    #            mode='min',\n    #            monitor='val_accuracy',\n    #            restore_best_weights=True,\n    #            verbose=1)\n    #          ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_encoded = bert_encode(test.text)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_encoded)\n    .batch(batch_size)\n)\n\npredicted_tweets = model.predict(test_dataset, batch_size=batch_size)\npredicted_tweets_binary = tf.cast(tf.round(predicted_tweets), tf.int32).numpy().flatten()\n\nmy_submission = pd.DataFrame({'id': test.id, 'target': predicted_tweets_binary})\nmy_submission.to_csv('/kaggle/working/my_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}