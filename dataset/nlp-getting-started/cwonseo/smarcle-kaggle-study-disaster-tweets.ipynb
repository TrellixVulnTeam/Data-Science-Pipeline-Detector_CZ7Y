{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-31T10:01:15.255771Z","iopub.execute_input":"2022-05-31T10:01:15.256137Z","iopub.status.idle":"2022-05-31T10:01:15.264719Z","shell.execute_reply.started":"2022-05-31T10:01:15.256106Z","shell.execute_reply":"2022-05-31T10:01:15.263728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. load data","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:15.271667Z","iopub.execute_input":"2022-05-31T10:01:15.272271Z","iopub.status.idle":"2022-05-31T10:01:15.30709Z","shell.execute_reply.started":"2022-05-31T10:01:15.272244Z","shell.execute_reply":"2022-05-31T10:01:15.306377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nWhat am I predicting?\n\n  - You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\nFiles\n - train.csv - the training set\n - test.csv - the test set\n - sample_submission.csv - a sample submission file in the correct format\n \nColumns\n - id - a unique identifier for each tweet\n - text - the text of the tweet\n - location - the location the tweet was sent from (may be blank)\n - keyword - a particular keyword from the tweet (may be blank)\n - target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","metadata":{}},{"cell_type":"code","source":"train.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:15.308806Z","iopub.execute_input":"2022-05-31T10:01:15.309163Z","iopub.status.idle":"2022-05-31T10:01:15.326291Z","shell.execute_reply.started":"2022-05-31T10:01:15.309126Z","shell.execute_reply":"2022-05-31T10:01:15.325036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:15.327891Z","iopub.execute_input":"2022-05-31T10:01:15.328315Z","iopub.status.idle":"2022-05-31T10:01:15.344544Z","shell.execute_reply.started":"2022-05-31T10:01:15.328277Z","shell.execute_reply":"2022-05-31T10:01:15.343618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. visualization","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:15.346381Z","iopub.execute_input":"2022-05-31T10:01:15.346815Z","iopub.status.idle":"2022-05-31T10:01:15.351764Z","shell.execute_reply.started":"2022-05-31T10:01:15.346776Z","shell.execute_reply":"2022-05-31T10:01:15.350607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"실제 재난 트윗와 재난 트윗이 아닌 것의 양","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"target\", data=train)\nplt.title(\"Disaster(1) or Not Disaster(0)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:15.353489Z","iopub.execute_input":"2022-05-31T10:01:15.354199Z","iopub.status.idle":"2022-05-31T10:01:15.520976Z","shell.execute_reply.started":"2022-05-31T10:01:15.354159Z","shell.execute_reply":"2022-05-31T10:01:15.520174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Disaster**","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:15.524643Z","iopub.execute_input":"2022-05-31T10:01:15.525632Z","iopub.status.idle":"2022-05-31T10:01:15.531293Z","shell.execute_reply.started":"2022-05-31T10:01:15.525558Z","shell.execute_reply":"2022-05-31T10:01:15.53011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text1 = dict(train[train.target==1].keyword.value_counts())\nwordcloud = WordCloud(width=800, height=400,background_color=\"white\").generate_from_frequencies(text1)\nplt.figure(figsize=[14,8])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:15.533014Z","iopub.execute_input":"2022-05-31T10:01:15.536827Z","iopub.status.idle":"2022-05-31T10:01:16.704381Z","shell.execute_reply.started":"2022-05-31T10:01:15.536769Z","shell.execute_reply":"2022-05-31T10:01:16.703358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**not disaster**","metadata":{}},{"cell_type":"code","source":"text2 = dict(train[train.target==0].keyword.value_counts())\nwordcloud = WordCloud(width=800, height=400,background_color=\"white\").generate_from_frequencies(text2)\nplt.figure(figsize=[14,8])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:16.705848Z","iopub.execute_input":"2022-05-31T10:01:16.706549Z","iopub.status.idle":"2022-05-31T10:01:17.664666Z","shell.execute_reply.started":"2022-05-31T10:01:16.706501Z","shell.execute_reply":"2022-05-31T10:01:17.657396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.DATA PROCESSING","metadata":{}},{"cell_type":"markdown","source":"# 특수문자 제거","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport re\n#tqdm? : progress bar 나타내어 진행상황 알려줌. 없어도 상관 X\ntext_list = list(train['text'])\n\nclear_text_list = [] #특수문자를 제거한 text를 입력\n\nfor i in tqdm(range(len(text_list))):\n    clear_text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]','',text_list[i])\n    #re.sub 이용하여 공백으로 치환\n    clear_text_list.append(clear_text.lower())\n    #lower 이용해 소문자로 변경\n    \ntrain['clear_text'] = clear_text_list\n#column 추가\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:17.66629Z","iopub.execute_input":"2022-05-31T10:01:17.666846Z","iopub.status.idle":"2022-05-31T10:01:17.749114Z","shell.execute_reply.started":"2022-05-31T10:01:17.666799Z","shell.execute_reply":"2022-05-31T10:01:17.748183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test데이터도 똑같이 처리","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport re\n#tqdm? : progress bar 나타내어 진행상황 알려줌. 없어도 상관 X\ntext_list = list(test['text'])\n\nclear_text_list = [] #특수문자를 제거한 text를 입력\n\nfor i in tqdm(range(len(text_list))):\n    clear_text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]','',text_list[i])\n    #re.sub 이용하여 공백으로 치환\n    clear_text_list.append(clear_text.lower())\n    #lower 이용해 소문자로 변경\n    \ntest['clear_text'] = clear_text_list\n#column 추가\ntest","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:17.750461Z","iopub.execute_input":"2022-05-31T10:01:17.751256Z","iopub.status.idle":"2022-05-31T10:01:17.800009Z","shell.execute_reply.started":"2022-05-31T10:01:17.751187Z","shell.execute_reply":"2022-05-31T10:01:17.79922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 토큰화 / 불용어 처리 / 어간 추출","metadata":{}},{"cell_type":"markdown","source":"tokenization : 모든 구문을 단어, 문자, 하위단어와 같은 더 작은 섹션으로 분할하는 작업. 여기서는 문장을 단어단위로 분할","metadata":{}},{"cell_type":"markdown","source":"stopword 제거 : I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우 있으므로 제거","metadata":{}},{"cell_type":"markdown","source":"stemming : 접사를 제거하여 어간을 추출하는 과정 예를 들어 'automate’, ‘automatic’, ‘automation’과 같은 단어를 automat으로 바꿔주는 작업","metadata":{}},{"cell_type":"code","source":"import nltk\n#자연어 처리 패키지 Natural Language Toolkit\nnltk.download(\"stopwords\")\nnltk.download(\"punkt\")\n#토큰화","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:17.803389Z","iopub.execute_input":"2022-05-31T10:01:17.804438Z","iopub.status.idle":"2022-05-31T10:01:17.812999Z","shell.execute_reply.started":"2022-05-31T10:01:17.804401Z","shell.execute_reply":"2022-05-31T10:01:17.81201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\n#from nltk.stem import LancasterStemmer\nX_train = []\n\nprint(stopwords.words('english'))\n\nstemmer = PorterStemmer()\n#l=LancasterStemmer()\nstop_words = set(stopwords.words('english'))\n\n\nclear_text_list = list(train['clear_text'])\n\nfor i in tqdm(range(len(clear_text_list))):\n    temp = word_tokenize(clear_text_list[i])\n    # 토큰화 : 문장을 단어단위로 구분\n    temp = [word for word in temp if word not in stop_words]\n    #불용어 제거\n    temp = [stemmer.stem(word) for word in temp]\n    # 어간추출(stemming)\n    temp = [word for word in temp if len(word) > 1]\n    #단어 길이\n    X_train.append(temp)\nprint(train['text'][0])\nprint(X_train[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:17.814754Z","iopub.execute_input":"2022-05-31T10:01:17.815403Z","iopub.status.idle":"2022-05-31T10:01:21.521924Z","shell.execute_reply.started":"2022-05-31T10:01:17.815366Z","shell.execute_reply":"2022-05-31T10:01:21.520993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"test데이터 똑같이 처리","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nX_test = []\nstemmer = PorterStemmer()\nstop_words = set(stopwords.words('english'))\nclear_text_list = list(test['clear_text'])\n\nfor i in tqdm(range(len(clear_text_list))):\n    temp = word_tokenize(clear_text_list[i])\n    # word_tokenize : 문장을 단어단위로 구분\n    temp = [word for word in temp if word not in stop_words]\n    #불용어 제거\n    temp = [stemmer.stem(word) for word in temp]\n    # 어간추출(stemming)\n    \n    temp = [word for word in temp if len(word) > 1]\n    X_test.append(temp)\nprint(test['text'][0])\nX_test[0]","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:21.523607Z","iopub.execute_input":"2022-05-31T10:01:21.524024Z","iopub.status.idle":"2022-05-31T10:01:23.092603Z","shell.execute_reply.started":"2022-05-31T10:01:21.523986Z","shell.execute_reply":"2022-05-31T10:01:23.091688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 단어를 정수로 인코딩","metadata":{}},{"cell_type":"markdown","source":"Why? 컴퓨터는 텍스트보다 숫자를 더 잘 처리한다.\n\n이 과정은 우리가 흔히 아는 원-핫 인코딩에도 이용된다.\n\n    1. 정수인코딩 수행\n    2. 부여된 고유정수를 인덱스로 간주하여 해당 위치에 1을 부여한다.\n\n이 중 1번 과정만 실행한다.\n","metadata":{}},{"cell_type":"code","source":"words = []\n\nfor i in tqdm(range(len(X_train))):\n    for j in range(len(X_train[i])):\n        words.append(X_train[i][j])\nlen(list(set(words)))","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:23.093863Z","iopub.execute_input":"2022-05-31T10:01:23.094295Z","iopub.status.idle":"2022-05-31T10:01:23.141378Z","shell.execute_reply.started":"2022-05-31T10:01:23.094258Z","shell.execute_reply":"2022-05-31T10:01:23.140605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nmax_words = 19526\ntokenizer = Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(X_train)\n#단어집합 생성\nX_train_vec = tokenizer.texts_to_sequences(X_train)\nX_test_vec = tokenizer.texts_to_sequences(X_test)\n#정수 시퀀스로 변환","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:23.142904Z","iopub.execute_input":"2022-05-31T10:01:23.143583Z","iopub.status.idle":"2022-05-31T10:01:23.591624Z","shell.execute_reply.started":"2022-05-31T10:01:23.143542Z","shell.execute_reply":"2022-05-31T10:01:23.590762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_vec[:1])\n\nprint(X_test_vec[:1])","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:23.592865Z","iopub.execute_input":"2022-05-31T10:01:23.593397Z","iopub.status.idle":"2022-05-31T10:01:23.599163Z","shell.execute_reply.started":"2022-05-31T10:01:23.593359Z","shell.execute_reply":"2022-05-31T10:01:23.598363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 토큰화된 트윗의 길이 맞춰주기(padding)","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nprint(\"최대 길이 : \", max(len(i) for i in X_train))\nprint(\"평균 길이 : \", sum(map(len,X_train))/len(X_train))\nplt.hist([len(s) for s in X_train], bins=50)\nplt.xlabel('length of Data')\nplt.ylabel('number of Data')\nplt.show","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:23.600763Z","iopub.execute_input":"2022-05-31T10:01:23.601518Z","iopub.status.idle":"2022-05-31T10:01:23.89776Z","shell.execute_reply.started":"2022-05-31T10:01:23.60148Z","shell.execute_reply":"2022-05-31T10:01:23.897012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nmax_len = 23\n\nX_train_vec = pad_sequences(X_train_vec, maxlen=max_len)\nX_test_vec = pad_sequences(X_test_vec, maxlen=max_len)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:23.898821Z","iopub.execute_input":"2022-05-31T10:01:23.899294Z","iopub.status.idle":"2022-05-31T10:01:23.955503Z","shell.execute_reply.started":"2022-05-31T10:01:23.899256Z","shell.execute_reply":"2022-05-31T10:01:23.95478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_vec[:1])\n\nprint(X_test_vec[:1])","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:23.956614Z","iopub.execute_input":"2022-05-31T10:01:23.956954Z","iopub.status.idle":"2022-05-31T10:01:23.962091Z","shell.execute_reply.started":"2022-05-31T10:01:23.956913Z","shell.execute_reply":"2022-05-31T10:01:23.961263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# target값 One-Hotecoding ","metadata":{}},{"cell_type":"code","source":"from keras.utils import np_utils\nimport numpy as np\n\ny_train = []\n\nfor i in range(len(train['target'])):\n    if train['target'].iloc[i] ==1:\n        y_train.append([0,1])\n    elif train['target'].iloc[i] ==0:\n        y_train.append([1,0])\n        \ny_train = np.array(y_train)\n\ny_train","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:23.963937Z","iopub.execute_input":"2022-05-31T10:01:23.964593Z","iopub.status.idle":"2022-05-31T10:01:24.11944Z","shell.execute_reply.started":"2022-05-31T10:01:23.964555Z","shell.execute_reply":"2022-05-31T10:01:24.118511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.**modeling**","metadata":{}},{"cell_type":"markdown","source":"Embedding?(https://casa-de-feel.tistory.com/28)\n\n단어를 연속적인 벡터로 매핑 하는 것\n\nEx) 강아지 = [0.2 1.8 1.1 -2.1 1.1 2.8 ... 중략 ...]\n\n자연어일 때 불가능했던 유사도 계산이 임베딩 덕분에 가능\n\n원핫인코딩의 한계\n\n    1. 카테고리 하나 추가될 때마다 벡터공간 한개 추가. 공간적 낭비\n    \n    ex) 10000개의 단어가 있으면  10000차원의 벡터가 필요\n    \n    2. 모든 단어가 0,1로 동일하게 되면 단어간 유사도 측정 불가","metadata":{}},{"cell_type":"markdown","source":"LSTM(Lont Short-Term Memory)\n\nhttps://pleasurehwang.tistory.com/19\n\n말 그대로 순서가 있는 data를 시퀀스라고 함. 예를 들어 Text 에는 문맥이라는 순서가 있으므로 시퀀스 데이터 라고 할 수 있다.\n\n그리고 RNN(순환 신경망)은 연속형 데이터를 처리하기 위해 고안된 신경망 구조이므로 RNN을 사용하고자 하였다.\n\nLSTM은 가장 기본적인 RNN인 vanilla RNN을 발전시킨 것으로 장기 메모리와 단기 메모리에 들어갈 정보를 나눠 학습한다.\n\n기존의 RNN은 먼 과거의 일로부터 학습하는 것이 산술적으로 거의 불가능했지만, LSTM은 수백만 단위 시간 전의 사건으로부터도 학습할 수 있다.\n","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, Dropout\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, 100))\n#차원이 100인 벡터를 만든다.\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2, activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nhistory = model.fit(X_train_vec, y_train, epochs=1,batch_size = 32, validation_split=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:24.120931Z","iopub.execute_input":"2022-05-31T10:01:24.121277Z","iopub.status.idle":"2022-05-31T10:01:28.174314Z","shell.execute_reply.started":"2022-05-31T10:01:24.121242Z","shell.execute_reply":"2022-05-31T10:01:28.172972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.SUBMISSION","metadata":{}},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:28.176156Z","iopub.execute_input":"2022-05-31T10:01:28.176548Z","iopub.status.idle":"2022-05-31T10:01:28.191706Z","shell.execute_reply.started":"2022-05-31T10:01:28.176507Z","shell.execute_reply":"2022-05-31T10:01:28.190645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict = model.predict(X_test_vec)\npredict_labels = np.argmax(predict, axis=1)\n\nfor i in range(len(predict_labels)):\n    predict_labels[i] = predict_labels[i]\n    \nids = list(test['id'])\n    \nsubmission_dic = {\"id\":ids, \"target\":predict_labels}\nsubmission_df = pd.DataFrame(submission_dic)\nsubmission_df.to_csv(\"kaggle_01.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T10:01:28.194715Z","iopub.execute_input":"2022-05-31T10:01:28.19694Z","iopub.status.idle":"2022-05-31T10:01:28.87709Z","shell.execute_reply.started":"2022-05-31T10:01:28.196903Z","shell.execute_reply":"2022-05-31T10:01:28.876273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}