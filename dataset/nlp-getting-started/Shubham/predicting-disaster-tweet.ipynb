{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Necessary Libraries\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nimport tensorflow_hub as hub\nimport tokenization\nfrom tensorflow.keras.layers import Dense, Input, Bidirectional, SpatialDropout1D, Embedding, add, concatenate\nfrom tensorflow.keras.layers import GRU, GlobalAveragePooling1D, LSTM, GlobalMaxPooling1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS \nfrom sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A special thanks to **Martin GÃ¶rner** to provide the code to setup TPUs in kaggle. The next cell is the code from here"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading the dataset\ndf = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if there is any null values\ndf.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seeing the target Value\ndf['target'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_words = ' '\nstopwords = set(STOPWORDS) \n  \n\nfor tweet in df.loc[df.target == 1].text: \n    tokens = tweet.split() \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n          \n    for words in tokens: \n        comment_words = comment_words + words + ' '  \n \n\nwordcloud = WordCloud(width = 1000, height = 1000, \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n                      \nplt.figure(figsize = (15, 15)) \nplt.imshow(wordcloud) \nplt.title('Most used word in Disaster Tweet (Target = 1)')\nplt.axis(\"off\") \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_words = ' '\nstopwords = set(STOPWORDS) \n  \n\nfor tweet in df.loc[df.target == 0].text: \n    tokens = tweet.split() \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n          \n    for words in tokens: \n        comment_words = comment_words + words + ' '  \n \n\nwordcloud = WordCloud(width = 1000, height = 1000, \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n                      \nplt.figure(figsize = (15, 15)) \nplt.imshow(wordcloud) \nplt.title('Most used word in Non Disaster Tweet (Target = 0)')\nplt.axis(\"off\") \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 3], df.iloc[:, 4], test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n    train = tfidf_vectorizer.fit_transform(data)\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\nsubmission_tfidf = tfidf_vectorizer.transform(test.iloc[:, 3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the XGBoost Model \nmodel = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0,\n              learning_rate=0.1, max_delta_step=0, max_depth=7,\n              min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n              nthread=None, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)\nmodel.fit(X_train_tfidf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluating the model in test data\nmodel.score(X_test_tfidf, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the predictions\npredictions = model.predict(submission_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = predictions\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('tfidf_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT"},{"metadata":{},"cell_type":"markdown","source":"I am also very new to BERT and [**this**](https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub) kernel help me a lot to Implement BERT in text classification. Special thanks for **xhlulu** for providing this nice kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encoder(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segments_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segments_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making out model architecture\ndef build_model(bert_layer, max_len=512):\n    with strategy.scope():\n        input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n        input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n        segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n        _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n        x = SpatialDropout1D(0.3)(sequence_output)\n        x = Bidirectional(GRU(LSTM_UNITS, return_sequences=True))(x)\n        x = Bidirectional(GRU(LSTM_UNITS, return_sequences=True))(x)\n        hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n        hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n        hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n        dropout = tf.keras.layers.Dropout(0.2)(hidden)\n        result = Dense(1, activation='sigmoid')(dropout)\n\n        model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=result)\n        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16\nLSTM_UNITS = 64\nEPOCHS = 10\nDENSE_HIDDEN_UNITS = 256\nmodel = build_model(bert_layer, max_len=160)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model, to_file='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making the training and submission data\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\ntrain_input = bert_encoder(df.iloc[:, 3], tokenizer, max_len=160)\ntest_input = bert_encoder(test.iloc[:, 3], tokenizer, max_len=160)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the model\nimport gc\nNUM_MODELS = 1\n    \nBATCH_SIZE = 16\nLSTM_UNITS = 64\nEPOCHS = 1\nDENSE_HIDDEN_UNITS = 256\ncheckpoint_predictions = []\ncheckpoint_val_pred = []\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(bert_layer, max_len=160)\n    for global_epoch in range(EPOCHS):\n        history = model.fit(\n            train_input, df.iloc[: ,4],\n            batch_size=BATCH_SIZE,\n            epochs=7,\n            verbose=1,\n            callbacks=[\n                LearningRateScheduler(lambda epoch: 2e-6 * (0.6** global_epoch))\n            ]\n        )\n        checkpoint_predictions.append(model.predict(test_input, batch_size=32).flatten())\n        weights.append(2 ** global_epoch)\n    del model\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.title(\"Accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.title(\"Loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = np.average(checkpoint_predictions, weights=weights, axis=0)\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}