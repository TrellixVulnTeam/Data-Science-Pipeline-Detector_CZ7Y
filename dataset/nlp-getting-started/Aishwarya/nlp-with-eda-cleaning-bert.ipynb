{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Reference:**\nImplemented BERT by taking help from below notebook\nhttps://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for installing Tokenization\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Library","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk  # for text manipulation \nimport string \nimport warnings \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt  \nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization\n\npd.set_option(\"display.max_colwidth\", 200) \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Inspection","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# looking into the number of real and unreal comments\ntrain['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of real and unreal comment\nsns.barplot(train['target'].value_counts().index,train['target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution of length of the tweets, in terms of words, in both train and test data.\nlength_train = train['text'].str.len() \nlength_test = test['text'].str.len() \nplt.hist(length_train, bins=20, label=\"train_text\") \nplt.hist(length_test, bins=20, label=\"test_text\") \nplt.legend() \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of real and unreal characters in train data\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrain_len=train[train['target']==1]['text'].str.len()\nax1.hist(train_len)\nax1.set_title('Disaster tweets')\ntrain_len=train[train['target']==0]['text'].str.len()\nax2.hist(train_len, color=\"orange\")\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#function for removing pattern\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt\n\n# remove '#' handle\ntrain['tweet'] = np.vectorize(remove_pattern)(train['text'], \"#[\\w]*\")\ntest['tweet'] = np.vectorize(remove_pattern)(test['text'], \"#[\\w]*\") \ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Delete everything except alphabet\ntrain['tweet'] = train['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ntest['tweet'] = test['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping words whose length is less than 3\ntrain['tweet'] = train['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntest['tweet'] = test['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Normalization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert all the words into lower case\ntrain['tweet'] = train['tweet'].str.lower()\ntest['tweet'] = test['tweet'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stop Words:** Stop words are generally the most common words in a language.\n\nWhy do we remove stop words?\n\nWords such as articles and some verbs are usually considered stop words because they don't help us to find the context or the true meaning of a sentence. These are words that can be removed without any negative consequences to the final model that you are training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nset(stopwords.words('english'))\n\n# set of stop words\nstops = set(stopwords.words('english')) \n\n# tokens of words  \ntrain['tokenized_sents'] = train.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\ntest['tokenized_sents'] = test.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n\n#function to remove stop words\ndef remove_stops(row):\n    my_list = row['tokenized_sents']\n    meaningful_words = [w for w in my_list if not w in stops]\n    return (meaningful_words)\n\n#removing stop words\ntrain['clean_tweet'] = train.apply(remove_stops, axis=1)\ntest['clean_tweet'] = test.apply(remove_stops, axis=1)\ntrain.drop([\"tweet\",\"tokenized_sents\"], axis = 1, inplace = True)\ntest.drop([\"tweet\",\"tokenized_sents\"], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see each word is tokenized , seperated with comma.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#re-join the words after tokenization\ndef rejoin_words(row):\n    my_list = row['clean_tweet']\n    joined_words = ( \" \".join(my_list))\n    return joined_words\n\ntrain['clean_tweet'] = train.apply(rejoin_words, axis=1)\ntest['clean_tweet'] = test.apply(rejoin_words, axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Word Cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance. \n\nWords which are bigger in size, more often they are mentioned than the smaller words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualization of all the words using word cloud\nfrom wordcloud import WordCloud \nall_word = ' '.join([text for text in train['clean_tweet']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_word) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualization of all the words which signify real disaster\nnormal_words =' '.join([text for text in train['clean_tweet'][train['target'] == 1]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualization of all the words which signify unreal disaster\nnormal_words =' '.join([text for text in train['clean_tweet'][train['target'] == 0]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the unnecessary column\ntrain.drop([\"keyword\",\"location\",\"text\"], axis = 1, inplace = True)\ntest.drop([\"keyword\",\"location\",\"text\"], axis = 1, inplace = True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementing Bert\n\n**Helper Function**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer_QA = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer_QA.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer_QA.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bert Module\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenization\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n# tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(train.clean_tweet.values, tokenizer_QA, max_len=160)\ntest_input = bert_encode(test.clean_tweet.values, tokenizer_QA, max_len=160)\ntrain_labels = train.target.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prediction on test data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Submission**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('new_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# If you like my kernal... Please upvote","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}