{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport gc\nimport random\nimport re\nimport string\nfrom tqdm.notebook import tqdm\nimport numpy as np\n\nsys.path.extend(['../input/transformer/', '../input/sacremoses/sacremoses-master/'])\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom scipy.stats import spearmanr\n    \nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, Dataset\nfrom torch import nn\nfrom torch.nn import Module\nfrom torch.nn import functional as f\n\nimport transformers\nfrom transformers import BertTokenizer, BertConfig, BertModel, XLNetConfig, XLNetModel\nfrom transformers import RobertaConfig, RobertaModel, DistilBertModel, DistilBertConfig\nfrom transformers.tokenization_bert import BasicTokenizer, whitespace_tokenize\nfrom transformers.optimization import AdamW, get_linear_schedule_with_warmup\n\nfrom nltk.corpus import stopwords\n\nstop_word = set(stopwords.words('english'))\nprint(len(stop_word))\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csv_path = '../input/nlp-getting-started/'\ntrain_path = csv_path + 'train.csv'\ntest_path = csv_path + 'test.csv'\nsubmission_path = csv_path + 'sample_submission.csv'\n\npath_model = '../input/pretrained-bert-models-for-pytorch/'\nmodel_file = path_model + 'bert-base-uncased/pytorch_model.bin'\nconfig_file = path_model + 'bert-base-uncased/bert_config.json'\nvocab_file = path_model + 'bert-base-uncased-vocab.txt'\n\npath_model = '../input/pretrained-bert-models-for-pytorch/'\nmodel_file_large = path_model + 'bert-large-uncased/pytorch_model.bin'\nconfig_file_large = path_model + 'bert-large-uncased/bert_config.json'\nvocab_file_large = path_model + 'bert-large-uncased-vocab.txt'\n\npath_roberta = '../input/roberta-transformers-pytorch/roberta-base/'\nconfig_roberta = path_roberta + 'config.json'\nvocab_roberta = path_roberta + 'vocab.json'\nmodel_roberta = path_roberta + 'pytorch_model.bin'\n\npath_distilroberta = '../input/roberta-transformers-pytorch/distilroberta-base/'\nconfig_distilroberta = path_distilroberta + 'config.json'\nvocab_distilroberta = path_distilroberta + 'vocab.json'\nmodel_distilroberta = path_distilroberta + 'pytorch_model.bin'\n\npath_xlnet = '../input/xlnet-pretrained-models-pytorch/'\nconfig_xlnet = path_xlnet + 'xlnet-base-cased-config.json'\nvocab_xlnet = path_model + 'bert-base-uncased-vocab.txt'\nmodel_xlnet = path_xlnet + 'xlnet-base-cased-pytorch_model.bin'\n\nmodel_use = 'bert'\nnum_model = 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenize = BertTokenizer.from_pretrained(vocab_file, do_lower_case=True, do_basic_tokenize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train file"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv(train_path)\ntrain_csv = train_csv[['id', 'text', 'target']]\ntrain_csv.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test file"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv = pd.read_csv(test_path)\ntest_csv = test_csv[['id', 'text']]\ntest_csv['target'] = 0\ntest_csv.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntext_length = test_csv['text'].apply(lambda x: len(tokenize.tokenize(x)))\n\nplt.figure(figsize=(10, 8))\nsns.distplot(text_length)\nprint(f'max lenth of text: {max(text_length)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(submission_path)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean data\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '\\xa0', '\\t',\n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '\\u3000', '\\u202f',\n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\"}\n\npuncts = puncts + list(string.punctuation)\n\ndef clean_text(x):\n    x = str(x).replace(\"\\n\",\"\")\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('\\d+', ' ', x)\n    return x\n\n\ndef replace_typical_misspell(text):\n    mispellings_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n\n    def replace(match):\n        return mispell_dict[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ndef remove_space(string):\n    string = BeautifulSoup(string).text.strip().lower()\n    string = re.sub(r'((http)\\S+)', 'http', string)\n    string = re.sub(r'\\s+', ' ', string)\n    return string\n\n\ndef clean_data(df, columns: list):\n    \n    for col in columns:\n        df[col] = df[col].apply(lambda x: remove_space(x).lower())        \n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n        df[col] = df[col].apply(lambda x: clean_text(x))\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntest = test_csv.loc[:, ['id', 'text']]\ntrain = train_csv.loc[:, ['text', 'target']]\n\ntest = clean_data(test, ['text'])\ntrain = clean_data(train, ['text'])\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class QueryDataset(Dataset):\n    \n    def __init__(self, data, is_train=True, max_length=512):\n        \n        super(QueryDataset, self).__init__()\n        \n        self.max_length = max_length\n        self.data = data\n        self.is_train = is_train\n        self.tokenizer = BertTokenizer.from_pretrained(vocab_file_large, do_lower_case=True, do_basic_tokenize=True)\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        token_ids = self.get_token_ids(idx)\n        \n        if self.is_train:\n            label = torch.tensor(self.data.loc[idx, 'target'], dtype=torch.float32)\n            return token_ids, label\n        else:\n            return token_ids\n        \n    \n    def get_token_ids(self, idx):\n        \n        token = self.tokenizer.tokenize(self.data.loc[idx, 'text'])\n        \n        max_seq_length = self.max_length - 2        \n        if len(token) > max_seq_length:           \n            token = token[:max_seq_length]                                                             \n                                                         \n        token = ['[CLS]'] + token + ['[SEP]']\n        token_ids_org = self.tokenizer.convert_tokens_to_ids(token)\n       \n        if len(token_ids_org) < self.max_length:\n            token_ids = token_ids_org + [0]*(self.max_length - len(token_ids_org))\n        else:\n            token_ids = token_ids_org[:self.max_length]\n            \n        token_ids = torch.tensor(token_ids)\n        del token_ids_org\n        return token_ids\n                \n    def collate_fn(self, batch):\n                \n        if self.is_train:\n            token_ids = torch.stack([x[0] for x in batch])\n            label = torch.stack([x[1] for x in batch])\n            return token_ids, label\n        else:\n            token_ids = torch.stack([x for x in batch])\n            return token_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = QueryDataset(test, is_train=False)\ntest_ld = DataLoader(test_dataset, batch_size=8,\n                     shuffle=False, num_workers=0, collate_fn=test_dataset.collate_fn)\n\nprint(len(test_ld))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertLinear(Module):\n    \n    def __init__(self, model_name, max_length=512, num_class=1):\n        super(BertLinear, self).__init__()\n        \n        if model_name == 'bert':            \n            config = BertConfig.from_json_file(config_file)\n            self.bert = BertModel.from_pretrained(model_file, config=config)\n            \n        elif model_name == 'bert-large':\n            config = BertConfig.from_json_file(config_file_large)\n            self.bert = BertModel.from_pretrained(model_file_large, config=config)\n            \n        elif model_name == 'robert':\n            config = RobertaConfig()\n            self.bert = RobertaModel(config=config)\n            \n        elif model_name == 'distilrobert':\n            config = DistilBertConfig()\n            self.bert = DistilBertModel(config=config)\n            \n        elif model_name == 'xlnet':\n            config = XLNetConfig()\n            self.bert = XLNetModel(config=config)\n                                               \n        self.bert.config.max_position_embeddings=max_length            \n        self.dropout = nn.Dropout(0.2)\n        self.fc = nn.Sequential(nn.ReLU(inplace=True),\n                                nn.Linear(3*config.hidden_size, num_class))\n    \n    def forward(self, input_ids, segment_ids=None):\n        \n        attention_mask = (input_ids > 0).float()\n        segment_ids = torch.zeros_like(input_ids)\n        layer, pooler = self.bert(input_ids=input_ids,\n                                  attention_mask=attention_mask,\n                                  token_type_ids=segment_ids)\n        \n        avg_pool = torch.mean(layer, 1)\n        max_pool, _ = torch.max(layer, 1)\n    \n        pooler = torch.cat((avg_pool, max_pool, pooler), 1)\n        \n        output = self.dropout(pooler)  \n        logits = self.fc(output)\n        \n        return logits\n    \n\ndef load_model(model_name, path_model, load_weight=True):\n    models = []    \n    model = BertLinear(model_name).to(device)\n    \n    if load_weight:\n        for weight in sorted(os.listdir(path_model)):\n            if 'pth' in weight:\n                weight_path = os.path.join(path_model, weight)\n                state = torch.load(weight_path, map_location=lambda storage, loc: storage)\n                models.append(state)\n    else:\n        for i in range(num_model):            \n            models.append(model.state_dict())\n        \n    return models\n\nbase_model = load_model(model_use, path_model='../input/tweet-bert')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train process"},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_fn(pred, expected):\n    return 0.7*f.mse_loss(torch.sigmoid(pred), expected) + 0.3*f.binary_cross_entropy_with_logits(pred, expected)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Trainer(object):\n    \n    def __init__(self, base_model, model_name='bert',\n                 weight_decay=0.1, learning_rate=2e-5):\n        \n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        \n        self.model_name = model_name\n        self.base_model = base_model\n        self.cretion = loss_fn\n    \n    def train(self, folds, epochs, train, check_number=5):\n        \n        model = BertLinear(self.model_name).to(device)\n        score_val_max = [0]*folds\n        for fold, (train_index, val_index) in enumerate(KFold(n_splits=folds, shuffle=True, random_state=37).split(train)):\n            print(f'fold: {fold}')\n            val_score_max = 0\n        \n            train_df = train.iloc[train_index]\n            train_df.reset_index(inplace=True, drop=True)\n            \n            val_df = train.iloc[val_index]\n            val_df.reset_index(inplace=True, drop=True)\n            \n            model.load_state_dict(self.base_model[fold])\n            \n            optimizer = AdamW(model.parameters(),\n                              lr=self.learning_rate,\n                              weight_decay=self.weight_decay,\n                              correct_bias=False)            \n            \n            train_dataset = QueryDataset(train_df)\n            train_ld = DataLoader(train_dataset, batch_size=8, shuffle=True,\n                                  num_workers=0, collate_fn=train_dataset.collate_fn)\n            \n            val_dataset = QueryDataset(val_df)\n            val_ld = DataLoader(val_dataset, batch_size=8, shuffle=True,\n                                num_workers=0, collate_fn=val_dataset.collate_fn)\n            \n            schedule = get_linear_schedule_with_warmup(optimizer,\n                                                       num_warmup_steps=0.5,\n                                                       num_training_steps=epochs*len(val_ld))\n            \n            del val_dataset, train_dataset, val_df, train_df\n            model.zero_grad()\n            check_score = 0\n            for epoch in range(epochs):\n                print(f'Epoch: {epoch}')\n                train_loss = 0\n                val_loss = 0\n\n                model.train()\n                for token_ids, label in tqdm(train_ld):\n\n                    optimizer.zero_grad()\n                    token_ids, label = token_ids.to(device), label.unsqueeze(1).to(device)\n                    output = model(token_ids)\n                    loss = self.cretion(output, label)\n                    loss.backward()\n\n                    train_loss += loss.item()\n                    optimizer.step()\n                    schedule.step()\n                    del token_ids, label\n                    \n                train_loss = train_loss/len(train_ld)\n                torch.cuda.empty_cache()\n                gc.collect()\n                \n                # evaluate process\n                model.eval()\n                score_val = 0\n                with torch.no_grad():\n                    for token_ids, label in tqdm(val_ld):\n                        token_ids, label = token_ids.to(device), label.unsqueeze(1).to(device)\n\n                        output = model(token_ids)\n                        loss = self.cretion(output, label)\n                        score_val += torch.sum((torch.sigmoid(output) >= 0.5).float()==label).item()/output.size(0)\n                        val_loss += loss.item()\n                    \n                    score_val = score_val/len(val_ld)\n                    val_loss = val_loss/len(val_ld)             \n                    \n                    \n                print(f'train_loss: {train_loss:.4f}, valid_loss: {val_loss:.4f}, valid_score: {score_val:.4f}')\n                schedule.step(val_loss)\n\n                if score_val >= val_score_max:\n                    score_val_max[fold] = score_val\n                    check_score+=1\n                    print(f'Validation score increased ({val_score_max:.4f} --> {score_val:.4f}). Saving model...')\n                    val_score_max = score_val\n                    check_score = 0\n                    torch.save(model.state_dict(), f'model_fold_{str(fold)}.pth')\n                else:\n                    check_score += 1\n                    print(f'{check_score} epochs of decreasing val_score')\n\n                    if check_score > check_number:\n                        print('Stopping trainning!')                    \n                        break\n                        \n            del optimizer, schedule, train_ld, val_ld\n            torch.cuda.empty_cache()\n            \n            gc.collect()\n        \n        return score_val_max\n            \n    def predict(self, test_ld, submission, threshold=0.5):\n        \n        model = BertLinear(self.model_name).to(device)  \n        list_predict = []\n        \n        for token_ids in tqdm(test_ld):\n            predicts = []\n            for index_model, model_param in enumerate(self.base_model):\n                model.load_state_dict(model_param)\n                \n                model.eval()\n                with torch.no_grad():\n                    token_ids = token_ids.to(device)                    \n                    predict_prob = torch.sigmoid(model(token_ids))\n                    predict = (predict_prob>threshold).float().cpu().numpy()\n                    \n                predicts.append(predict) \n                \n            predicts = np.sum(predicts, axis=0)\n            list_predict.extend(np.where(predicts>3, 1, 0))\n                \n        submission.target = np.array(list_predict).flatten()\n            \n        return submission        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_process = Trainer(base_model, model_use)\n\ntest_csv = train_process.predict(test_ld=test_ld, submission=test_csv, threshold=0.5)\ntrain_csv = pd.concat([train_csv, test_csv]).sort_values('id')\n\ntrain_csv.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_csv.loc[:, ['text', 'target']]\ntrain = clean_data(train, ['text'])\n\ndel train_csv, test_csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_val_max = train_process.train(folds=num_model,\n                                    epochs=3,\n                                    train=train,\n                                    check_number=3)\n\n\ndel base_model, train\n\ntorch.cuda.empty_cache()\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model1 = load_model(model_use, path_model='.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_process = Trainer(base_model1, model_use)\nsubmission = train_process.predict(test_ld=test_ld, submission=submission, threshold=0.5)\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"268663820387491785446afb028b8ec7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f21e820765442839de7878a1c429cae","placeholder":"​","style":"IPY_MODEL_828368fb75be444e9470186f5cc24cb1","value":" 408/408 [09:14&lt;00:00,  1.36s/it]"}},"356f1a99541443a0be1f94ef8df7d1b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_9969c5337798463fb0640de7fb311331","max":408,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9ff256406bd1438fbe08148000f9cb50","value":408}},"7f21e820765442839de7878a1c429cae":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"828368fb75be444e9470186f5cc24cb1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9969c5337798463fb0640de7fb311331":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ff256406bd1438fbe08148000f9cb50":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"a9c2622c6aa44a6e9f62874b05e528dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_356f1a99541443a0be1f94ef8df7d1b0","IPY_MODEL_268663820387491785446afb028b8ec7"],"layout":"IPY_MODEL_ea44fa1aabe14c0bb92d38d686104693"}},"ea44fa1aabe14c0bb92d38d686104693":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}