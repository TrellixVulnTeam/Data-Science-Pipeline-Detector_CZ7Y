{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What we are going to do?\nPre-Processing the data, so as to make it similar to BERT Vocab."},{"metadata":{},"cell_type":"markdown","source":"A lot of material for this kernel is taken from [Theo Viel](https://www.kaggle.com/theoviel) kernel on [Improve your Score with Text Preprocessing](https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2). <br>\n\n**Any feedback would be greatly appreciated. Thank you**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport operator \nimport re\nimport gc\nimport keras\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of examples: \", df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing BERT Vocab."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch-pretrained-bert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom pytorch_pretrained_bert import BertTokenizer\n\n# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"vocabulary.txt\", 'w') as f:\n    \n    # For each token...\n    for token in tokenizer.vocab.keys():\n        \n        # Write it out and escape any unicode characters.            \n        f.write(token + '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this buids the vocab. of our dataset\ndef build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(df['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(vocab.keys())[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this check how much of our vocab is similar to the BERT vocab.\ndef check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"BERT\")\noov_bert = check_coverage(vocab, tokenizer.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_bert[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.vocab[\"I\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.vocab[\"i\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above example clearly demonstrate that the word **I** is not in vocab, but the word **i** is this is because we have imported `bert-base-uncased`. Let's make the text lower."},{"metadata":{},"cell_type":"markdown","source":"## Lowering the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['lowered_text'] = df['text'].apply(lambda x: x.lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_lower = build_vocab(df['lowered_text'])\nprint(\"BERT EMBEDDINGS\")\noov_bert = check_coverage(vocab_lower, tokenizer.vocab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a significant amount of improvement. "},{"metadata":{},"cell_type":"markdown","source":"Now let us check what's missing in our vocab. "},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_bert[:25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First Faults appearing are: \n* Contractions \n* Punctuations\n* Some words like **\\x89ûò**\n> Let's correct that."},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Contractions"},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"- Known Contractions -\")\nprint(\"   BERT :\")\nprint(known_contractions(tokenizer.vocab))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh Shit! Contractions doesn't exist in BERT embeddings. <br>\nThis is really is big problem for us. Let fix it up! "},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['treated_text'] = df['lowered_text'].apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(df['treated_text'])\nprint(\"BERT : \")\noov_bert = check_coverage(vocab, tokenizer.vocab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now let us clean the special Characters."},{"metadata":{"trusted":true},"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unknown_punct(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"BERT :\")\nprint(unknown_punct(tokenizer.vocab, punct))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's great only 1 unknown punct. from our punct. set."},{"metadata":{"trusted":true},"cell_type":"code","source":"punct_mapping = { 'à': 'a'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_bert[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of the work will be done by our tokenizer that will tokenize the words. For eg. thunderstorm can be broken up as thunder + storm "},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.vocab[\"amp\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer.tokenize(\"thunderstorm\"))\nprint(tokenizer.tokenize(\"11-year-old\"))\nprint(tokenizer.tokenize(\"@youtube\"))\nprint(tokenizer.tokenize(\"\\x89û_\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let us delete some words like \\x89..."},{"metadata":{"trusted":true},"cell_type":"code","source":"bad_words = []\nfor i in range(len(oov_bert)):\n    if oov_bert[i][0][0] ==\"\\x89\":\n        bad_words.append(oov_bert[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bad_dict = {}\nfor i in range(len(bad_words)):\n    bad_dict[bad_words[i][0]] = \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = bad_dict  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['treated_text'] = df['treated_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(df['treated_text'])\nprint(\"BERT : \")\noov_bert = check_coverage(vocab, tokenizer.vocab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, that's a great improvment."},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_bert[:25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For those top 25 words let us do something."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in oov_bert[:25]:\n    print(tokenizer.tokenize(i[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above some of them are understood by tokenizer & for rest of them let us explicitly make a mapping dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"explicit_mapping = {\"\\x89û\": \"\", \"mh370\" : \"flight\", \"legionnaires\": \"pneumonia\", \n                   \"derailment\": \"railway accident\", \"inundated\": \"flood\", \"deluged\": \"flood\", \n                   \"curfew\": \"stay at home\",\"obliteration\": \"destruction\", \n                   \"quarantine\": \"prevent the spread of disease\", \"lol\": \"laugh\", \n                   \"obliterate\": \"destroy\", \"hijacking\": \"seize\", \"detonation\": \"explosion\", \n                   \"electrocuted\": \"killed\", \"destroyd\": \"destroyed\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def explicit_changes(text, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['treated_text'] = df['treated_text'].apply(lambda x: explicit_changes(x, explicit_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = build_vocab(df['treated_text'])\nprint(\"BERT : \")\noov_bert = check_coverage(vocab, tokenizer.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_bert[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have done most of our job in preprocessing our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# lower\ntrain['treated_text'] = train['text'].apply(lambda x: x.lower())\n# clean contractions\ntrain['treated_text'] = train['treated_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\n# clean special chars - this is optional as most of the punct. are in BERT embed.\ntrain['treated_text'] = train['treated_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n# cleaning some word\ntrain['treated_text'] = train['treated_text'].apply(lambda x: explicit_changes(x, explicit_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['treated_text'] = test['text'].apply(lambda x: x.lower())\ntest['treated_text'] = test['treated_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\ntest['treated_text'] = test['treated_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\ntest['treated_text'] = test['treated_text'].apply(lambda x: explicit_changes(x, explicit_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving out work\ntrain.to_csv(\"train_BERT_preprocessed.csv\")\ntest.to_csv(\"test_BERT_preprocessed.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2 style = \"color:red\" >Please Upvote, if you like this kernel.</h2>"},{"metadata":{},"cell_type":"markdown","source":"**I will be adding more stuff soon!** <br>\nSo stay in touch."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}