{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Distaster tweet classification with HF transformers\n\n## Goal\n\nPredict which Tweets are about real disasters and which ones are not"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import libraries\nimport torch\nimport os\nimport torch.nn as nn\n\n# AutoTokenizer and AutoModelForSequenceClassification will allow to try different model architerctures with minimal changes in code\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nseed = 142\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = os.path.realpath('/kaggle/input/nlp-getting-started')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataset into dataframe\ndf = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of target\n\ndf['target'].value_counts() / df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split data into train, validation sets\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_indices, val_indices = train_test_split(df.index, stratify=df['target'], test_size=0.15,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\n\nprintable_chars = set(string.printable)\n\ndef preprocess_text(str_txt: str) -> str:\n    \"\"\"Preprocessing for raw text data\"\"\"\n    # Remove urls from tweet\n    str_txt = re.sub(r'(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)', '', str_txt, flags=re.MULTILINE)\n    # Remove mentions\n    str_txt = re.sub(r'@([A-z0-9_]+)', '', str_txt, flags=re.MULTILINE)\n    # Remove # from hashtags\n    str_txt = re.sub(r'#([A-z0-9_]+)', '\\g<1>', str_txt, flags=re.MULTILINE)\n    # Convert numbers int NUMBER\n    str_txt = re.sub(r'\\d+[,.]?(?:\\d+)?', 'NUMBER', str_txt, flags=re.MULTILINE)\n    # Remove non printable characters\n    str_txt = ''.join([ch for ch in str_txt if ch in printable_chars])\n    \n    return str_txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DataFrame before preprocessing\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Preprocessing for data frame\"\"\"\n    df['text'] = df['text'].apply(preprocess_text)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = preprocess_df(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dataset and Dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"from typing import Union\n\nclass DisasterTweetsDataset(Dataset):\n    def __init__(self, tweets_df: pd.DataFrame, text_column: str, label_column: str = None) -> None:\n        super().__init__()\n        self.tweets_df = tweets_df\n        self.text_column = text_column\n        self.label_column = label_column\n\n\n    def __len__(self) -> int:\n        return self.tweets_df.shape[0]\n\n    def __getitem__(self, idx: int):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        if self.label_column:\n            df_row = self.tweets_df.loc[idx, [self.text_column, self.label_column]]\n            sample = {'text': df_row[self.text_column], 'label': df_row[self.label_column]}\n        else:\n            df_row = self.tweets_df.loc[idx, [self.text_column]]\n            sample = {'text': df_row[self.text_column]}\n        return sample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SubsetRandomSampler samples elements randomly from a given list of indices, without replacement.\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_dataset = DisasterTweetsDataset(df, 'text', 'target')\nprint(\"Dataset Size:\", len(tweet_dataset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of positive class\nprint(\"Text:\", tweet_dataset[2]['text'])\nprint(\"Label:\", tweet_dataset[2]['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of negative class\nprint(\"Text:\", tweet_dataset[18]['text'])\nprint(\"Label:\", tweet_dataset[18]['label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am going to use RoBERTa base model with pretrained weights from HuggingFace transformers. More details can be seen [here](https://huggingface.co/roberta-large) "},{"metadata":{"trusted":true},"cell_type":"code","source":"hf_weights_name = 'roberta-large'\n# Create tokenizer from pretrained weights\nhf_tokenizer = AutoTokenizer.from_pretrained(hf_weights_name)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For our case collate_fn is called with a list of data samples at each time. \n# It is expected to collate the input samples into a batch for yielding from the data loader iterator. \n\ndef collate_fn(batch):\n    if 'label' in batch[0]:\n        texts, labels = zip(*[(batch[i]['text'], batch[i]['label']) for i in range(len(batch))])\n        result = dict(labels=labels)\n    else:\n        texts = [batch[i]['text'] for i in range(len(batch))]\n        result = {}\n    hf_example_ids = hf_tokenizer.batch_encode_plus(list(texts),\n        add_special_tokens=True,\n        return_attention_mask=True,\n        padding='longest')\n    return dict(**result, **hf_example_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import multiprocessing\n\n\nnum_workers = multiprocessing.cpu_count()\nbatch_size = 8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create data loaders for train and validation sets\ntrain_loader = DataLoader(tweet_dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=collate_fn, sampler=train_sampler)\nval_loader = DataLoader(tweet_dataset, batch_size=batch_size, num_workers=num_workers, collate_fn=collate_fn, sampler=val_sampler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_loader))\nprint(len(val_loader))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loaders = {'train': train_loader, 'val': val_loader}\nprogress_bars = {}\nepoch_stats = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FineTuning procedure"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if cuda is available\ngpu_available = torch.cuda.is_available()\nprint(\"GPU is available:\", gpu_available)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check \ndevice = torch.device('cuda' if gpu_available else 'cpu')\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if gpu_available:\n    \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    print(\"Cuda Device Name:\",torch.cuda.get_device_name())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create model from pretrained weights\nmodel = AutoModelForSequenceClassification.from_pretrained(hf_weights_name, num_labels=2)\nmodel.to(device);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 4\nverbose = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\n\noptimizer = AdamW(model.parameters(), lr=1.5e-6, eps=1e-8)\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=len(train_loader)*num_epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_acc = 0.0\nbest_loss = float('inf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\n\n# Weights of best model so far\nbest_model_weights = copy.deepcopy(model.state_dict())\nepoch_bar = tqdm(desc='training routine', total=num_epochs,\n                  initial=0, position=0, disable=(verbose is not True))\nfor split, data_loader in data_loaders.items():\n    progress_bars[split] = tqdm(desc=f'split={split}',\n                                total=len(data_loader),\n                                position=1,\n                                disable=(verbose is not True),\n                                leave=True)\n    epoch_stats[split] = {'loss': [], 'accuracy': []}\n\ntraining_data = []\ntry:\n    for epoch in range(1, num_epochs + 1):\n        \n        for split, data_loader in data_loaders.items():\n            epoch_loss = torch.FloatTensor([0.0]).to(device)\n            num_correct = torch.LongTensor([0]).to(device)\n            total_samples = 0\n            is_training = (split == 'train')\n            model.train(is_training)\n            for batch in data_loader:\n                with torch.set_grad_enabled(is_training):\n                    input_ids = torch.LongTensor(batch['input_ids']).to(device)\n                    labels = torch.LongTensor(batch['labels']).to(device)\n                    masks = torch.LongTensor(batch['attention_mask']).to(device)\n                    \n                    optimizer.zero_grad()\n\n                    outputs = model(input_ids, masks, labels=labels)\n                    loss = outputs.loss\n\n                    if is_training:\n                        loss.backward()\n                    epoch_loss += loss\n                    _, predictions = torch.max(outputs.logits, 1)\n                    num_correct += torch.eq(predictions, labels).sum()\n                    total_samples += labels.size(0)\n                    \n                    if is_training:\n                        optimizer.step()\n                        scheduler.step()\n                    progress_bars[split].update()\n            epoch_loss /= len(data_loader)\n            epoch_accuracy = num_correct / total_samples\n            epoch_bar.set_postfix({f\"{split}_loss\": epoch_loss.item(), f\"{split}_acc\": round(epoch_accuracy.item(), 3)})\n            if not is_training:\n                training_data.append((epoch_loss.item(), round(epoch_accuracy.item(), 3)))\n                if epoch_accuracy.item() > best_acc:\n                    best_model_weights = copy.deepcopy(model.state_dict())\n                    best_acc = epoch_accuracy.item()\n\n        for bar in progress_bars.values():\n            bar.n = 0\n            bar.reset()\n        epoch_bar.update()\nexcept KeyboardInterrupt:\n    pass\nfinally:\n    print(training_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# [(0.46483272314071655, 0.792), (0.42263394594192505, 0.813), (0.40105751156806946, 0.83), (0.40114283561706543, 0.83)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training results visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_losses, val_acc = zip(*training_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = list(range(1, num_epochs + 1))\nfig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\naxes[0].plot(epochs, val_losses, 'o-')\naxes[0].set_ylabel('Val. loss')\naxes[1].plot(epochs, val_acc, 'o-', color=\"orange\")\naxes[1].set_ylabel('Val. accuracy')\n\nplt.xlabel(\"epochs\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load best model weights\nmodel.load_state_dict(best_model_weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check best model performance on validation set\nmodel.eval()\nnum_correct = torch.LongTensor([0]).to(device)\nwith torch.no_grad():\n    for batch in val_loader:\n        input_ids = torch.LongTensor(batch['input_ids']).to(device)\n        masks = torch.LongTensor(batch['attention_mask']).to(device)\n        labels = torch.LongTensor(batch['labels']).to(device)\n        \n        outputs = model(input_ids, masks)\n        _, predictions = torch.max(outputs.logits, 1)\n        num_correct += torch.eq(predictions, labels).sum()\nprint(\"Val. accuracy of best model:\", round(num_correct.item()/len(val_indices), 3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make predictions on test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), encoding='utf-8')\ntest_df = preprocess_df(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset for test data\ntweet_dataset_test = DisasterTweetsDataset(test_df, 'text', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loader = DataLoader(tweet_dataset_test, batch_size=batch_size, num_workers=num_workers, collate_fn=collate_fn, shuffle=False)\npredictions_all = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = torch.LongTensor(batch['input_ids']).to(device)\n        masks = torch.LongTensor(batch['attention_mask']).to(device)\n        \n        outputs = model(input_ids, masks)\n        _, predictions = torch.max(outputs.logits, 1)\n\n        predictions_all.append(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_all_tensor = torch.cat(predictions_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions for first 10 samples\npredictions_all_tensor[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nimport uuid\n\n\ndef prepare_submission(test_data: pd.DataFrame, predicted: np.ndarray):\n    f_name = f\"submissions_{uuid.uuid4()}.csv\"\n    print(f_name)\n    with open(f_name, mode=\"w\") as f:\n        csv_writer = csv.DictWriter(f, fieldnames=['id', \"target\"])\n        csv_writer.writeheader()\n        for idx, df_row in test_data.iterrows():\n            csv_writer.writerow({\"id\": df_row['id'], \"target\": predicted[idx]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prepare_submission(test_df, predictions_all_tensor.cpu().numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}