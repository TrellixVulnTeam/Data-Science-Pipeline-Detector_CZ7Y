{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center>Natural Language Processing with Disaster Tweetsn<center>","metadata":{}},{"cell_type":"markdown","source":"![](https://ak.picdn.net/shutterstock/videos/29359330/thumb/4.jpg)","metadata":{}},{"cell_type":"markdown","source":"\n\n####  <center>Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies). But, it’s not always clear whether a person’s words are actually announcing a disaster.</center>  \n\n","metadata":{}},{"cell_type":"markdown","source":"-----------------","metadata":{}},{"cell_type":"markdown","source":"Hello!\n\nThis notebook will consist of several blocks:\n1. **| EDA & Feature Engineering |**\n2. **| Data Preprocessing |**\n3. **| machine learning |**\n\nIf necessary, you can quickly move to the solution blocks of this competition that interest you using the table of contents.\n\nIn this notebook, **I will not use deep learning methods, Glove, BERT, etc.** I will probably create a separate notebook using these technologies, but for now **I will focus on the most basic NLP methods.**\n\nOkay, time to get started!\n\nFirst, let's import the start modules **(I will not import all the modules that will be used in the notebook at once. Instead, I will import them separately, depending on the tasks of the block.)**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom scipy.sparse import csr_matrix, hstack\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None # Disabling the pandas warning","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:31.555089Z","iopub.execute_input":"2022-02-22T23:28:31.555537Z","iopub.status.idle":"2022-02-22T23:28:31.564028Z","shell.execute_reply.started":"2022-02-22T23:28:31.555485Z","shell.execute_reply":"2022-02-22T23:28:31.562953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\noutput_example = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:31.565625Z","iopub.execute_input":"2022-02-22T23:28:31.566227Z","iopub.status.idle":"2022-02-22T23:28:31.622019Z","shell.execute_reply.started":"2022-02-22T23:28:31.566166Z","shell.execute_reply":"2022-02-22T23:28:31.62082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:31.623636Z","iopub.execute_input":"2022-02-22T23:28:31.623871Z","iopub.status.idle":"2022-02-22T23:28:31.637229Z","shell.execute_reply.started":"2022-02-22T23:28:31.623843Z","shell.execute_reply":"2022-02-22T23:28:31.635323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When naming the target attribute **\"target\"**, I sometimes get confused and forget what the classes of this attribute mean, so I decided to rename it to the understandable name **\"real_tweet\"**","metadata":{}},{"cell_type":"code","source":"train.rename(columns={'target':'real_tweet'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:31.638987Z","iopub.execute_input":"2022-02-22T23:28:31.639249Z","iopub.status.idle":"2022-02-22T23:28:31.654475Z","shell.execute_reply.started":"2022-02-22T23:28:31.639221Z","shell.execute_reply":"2022-02-22T23:28:31.653521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:31.657903Z","iopub.execute_input":"2022-02-22T23:28:31.658644Z","iopub.status.idle":"2022-02-22T23:28:31.697777Z","shell.execute_reply.started":"2022-02-22T23:28:31.658587Z","shell.execute_reply":"2022-02-22T23:28:31.696696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-------------","metadata":{}},{"cell_type":"markdown","source":"#  <center>| EDA & Feature Engineering |</center>","metadata":{}},{"cell_type":"markdown","source":"![](https://i.ytimg.com/vi/Xk0TTY0kZ4A/hqdefault.jpg?sqp=-oaymwEjCPYBEIoBSFryq4qpAxUIARUAAAAAGAElAADIQj0AgKJDeAE=&rs=AOn4CLB4PPIiRY9YGVA93j_JQBIUQCmF-g)  ","metadata":{}},{"cell_type":"markdown","source":"Let's start block **| EDA & Feature Engineering |**\n\nUnfortunately, most notebooks only examine and transform the **\"text\"** variable\n\nIn this notebook, I will try to **use more features**, and also try to **generate new features**","metadata":{}},{"cell_type":"markdown","source":"##  Real_tweet & Text Analysis ","metadata":{}},{"cell_type":"markdown","source":"Let's start with the analysis of the target feature, first take a look at the **ratio of classes**","metadata":{}},{"cell_type":"code","source":"train['real_tweet'].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:31.699026Z","iopub.execute_input":"2022-02-22T23:28:31.6996Z","iopub.status.idle":"2022-02-22T23:28:31.710464Z","shell.execute_reply.started":"2022-02-22T23:28:31.699553Z","shell.execute_reply":"2022-02-22T23:28:31.709427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(data=train, x='real_tweet', palette='twilight', saturation=1);","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:31.711667Z","iopub.execute_input":"2022-02-22T23:28:31.712224Z","iopub.status.idle":"2022-02-22T23:28:31.973433Z","shell.execute_reply.started":"2022-02-22T23:28:31.71219Z","shell.execute_reply":"2022-02-22T23:28:31.972221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Excellent, feature classes do **not have a strong imbalance**, an approximate ratio of **6:4**\n\nNow let's take a **look at some examples of tweets from the \"text\" feature.**\n\n**Divide them into real and fake** and display them on the screen","metadata":{}},{"cell_type":"code","source":"real_tweets = train[train['real_tweet']==1]['text']\nfake_tweets = train[train['real_tweet']==0]['text']","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:31.97489Z","iopub.execute_input":"2022-02-22T23:28:31.975227Z","iopub.status.idle":"2022-02-22T23:28:31.984819Z","shell.execute_reply.started":"2022-02-22T23:28:31.975182Z","shell.execute_reply":"2022-02-22T23:28:31.983663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_real_tweets = real_tweets.sample(5).values\nrandom_fake_tweets = fake_tweets.sample(5).values\n\n\nprint('Real tweets:\\n')\nfor i, tweet in enumerate(random_real_tweets):\n    print('[{0}] {1}\\n'.format(i+1, tweet))\n    if i == 5:\n        break\n\nprint('-'*90)\nprint('Fake tweets:\\n')\nfor i, tweet in enumerate(random_fake_tweets):\n    print('[{0}] {1}\\n'.format(i+1, tweet))\n    if i == 5:\n        break","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:31.986385Z","iopub.execute_input":"2022-02-22T23:28:31.987307Z","iopub.status.idle":"2022-02-22T23:28:32.003978Z","shell.execute_reply.started":"2022-02-22T23:28:31.987257Z","shell.execute_reply":"2022-02-22T23:28:32.002945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When executing the code above, I get always different **5 real tweet examples and 5 fake tweet examples using the sample() function\n\nSo I decided to try to manually understand the difference between a real tweet and a fake tweet.\n\n1) **It seemed to me that real tweets have a longer text length than fake tweets**\n\n2) **Real tweets seem to have more numbers in the text**\n\n3) **It can be assumed that real tweets will be written more competently and contain more punctuation in the text**\n\n4) **Tweets often use http links, but it seemed to me that real tweets contained more of them**\n\n5) **It also seemed to me that there are more hastags in real tweets**\n\nBased on the findings obtained in the process of manual analysis, we will generate new features and see how they affect the target feature\n\nLet's start by creating **\"len_text\"**","metadata":{}},{"cell_type":"code","source":"train['len_text'] = train['text'].apply(len)\ntest['len_text'] = test['text'].apply(len)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:32.005557Z","iopub.execute_input":"2022-02-22T23:28:32.006315Z","iopub.status.idle":"2022-02-22T23:28:32.023137Z","shell.execute_reply.started":"2022-02-22T23:28:32.006272Z","shell.execute_reply":"2022-02-22T23:28:32.021912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(['real_tweet'])['len_text'].describe().T","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:32.024523Z","iopub.execute_input":"2022-02-22T23:28:32.024947Z","iopub.status.idle":"2022-02-22T23:28:32.052383Z","shell.execute_reply.started":"2022-02-22T23:28:32.024908Z","shell.execute_reply":"2022-02-22T23:28:32.05135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.boxplot(data=train, x='len_text', y='real_tweet', orient='h', palette='YlGnBu');","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:32.053705Z","iopub.execute_input":"2022-02-22T23:28:32.053945Z","iopub.status.idle":"2022-02-22T23:28:32.243888Z","shell.execute_reply.started":"2022-02-22T23:28:32.053915Z","shell.execute_reply":"2022-02-22T23:28:32.243008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, real tweets on average have more characters.\n\nNow let's take a look at **points '2)' and '3)'**\n\nLet's create 2 features for each hypothesis: **check and count**\n\nI don't know which ones will give the best result as there are suggestions:\n\n**count_punctation** will be highly correlated with len_text\n\n**count_digits** will count the number 13 as 1 and 3, which may result in an incorrect estimate of the actual number of numbers in the tweet\n\n**check_digits** and check_punctation may not show a possible relationship with the target feature\n\nTherefore, we will create all 4 features and analyze them","metadata":{}},{"cell_type":"code","source":"def check_digits(text):\n    if [char for char in text if char in string.digits]:\n        return 1\n    else:\n        return 0\n    \ndef check_punctation(text):\n    if [char for char in text if char in string.punctuation]:\n        return 1\n    else:\n        return 0\n    \ndef count_punctation(text):\n    count = 0\n    for char in text:\n        if char in string.punctuation:    \n            count +=1\n    return count\n\ndef count_digits(text):\n    count = 0\n    for char in text:\n        if char in string.digits:    \n            count +=1\n    return count","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:32.24527Z","iopub.execute_input":"2022-02-22T23:28:32.245867Z","iopub.status.idle":"2022-02-22T23:28:32.25452Z","shell.execute_reply.started":"2022-02-22T23:28:32.24582Z","shell.execute_reply":"2022-02-22T23:28:32.252948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['check_digits'] = train['text'].apply(lambda x: check_digits(x))\ntrain['check_punctation'] = train['text'].apply(lambda x: check_punctation(x))\ntrain['count_punctation'] = train['text'].apply(lambda x: count_punctation(x))\ntrain['count_digits'] = train['text'].apply(lambda x: count_digits(x))\n\n\ntest['check_digits'] = test['text'].apply(lambda x: check_digits(x))\ntest['check_punctation'] = test['text'].apply(lambda x: check_punctation(x))\ntest['count_punctation'] = test['text'].apply(lambda x: count_punctation(x))\ntest['count_digits'] = test['text'].apply(lambda x: count_digits(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:32.260853Z","iopub.execute_input":"2022-02-22T23:28:32.261697Z","iopub.status.idle":"2022-02-22T23:28:32.661074Z","shell.execute_reply.started":"2022-02-22T23:28:32.261598Z","shell.execute_reply":"2022-02-22T23:28:32.659847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(['real_tweet'])['count_punctation'].describe().T","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:32.66305Z","iopub.execute_input":"2022-02-22T23:28:32.663471Z","iopub.status.idle":"2022-02-22T23:28:32.688373Z","shell.execute_reply.started":"2022-02-22T23:28:32.663434Z","shell.execute_reply":"2022-02-22T23:28:32.687502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(['real_tweet'])['count_digits'].mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:32.689713Z","iopub.execute_input":"2022-02-22T23:28:32.68998Z","iopub.status.idle":"2022-02-22T23:28:32.702041Z","shell.execute_reply.started":"2022-02-22T23:28:32.689947Z","shell.execute_reply":"2022-02-22T23:28:32.700621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax =plt.subplots(1,2, figsize=(15,10))\nsns.boxenplot(data=train, x='count_digits', y='real_tweet', orient='h', ax=ax[0], palette='cubehelix')\nsns.boxenplot(data=train, x='count_punctation', y='real_tweet', orient='h', ax=ax[1], palette='cubehelix');","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:32.704395Z","iopub.execute_input":"2022-02-22T23:28:32.704933Z","iopub.status.idle":"2022-02-22T23:28:33.048525Z","shell.execute_reply.started":"2022-02-22T23:28:32.704885Z","shell.execute_reply":"2022-02-22T23:28:33.0478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax =plt.subplots(1,2, figsize=(15,5))\nsns.countplot(data=train, x='check_digits', hue='real_tweet', ax=ax[0], palette='Set1')\nsns.countplot(data=train, x='check_punctation', hue='real_tweet', ax=ax[1], palette='Set1');","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:33.049781Z","iopub.execute_input":"2022-02-22T23:28:33.050153Z","iopub.status.idle":"2022-02-22T23:28:33.386154Z","shell.execute_reply.started":"2022-02-22T23:28:33.050121Z","shell.execute_reply":"2022-02-22T23:28:33.385492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, **check_digits** seems like a really important feature.\n\n**check_punctation** did not show any relationship with the target feature, the distribution of the target variable has approximately the same ratio\n\nThe boxplot **count_punctation** also does not show a strong relationship with the target feature, perhaps the feature does not make any sense\n\nLater we will decide what to do with them, but for now let's deal with **points 4) and 5)**","metadata":{}},{"cell_type":"code","source":"train['check_http'] = train['text'].str.contains('http')\ntrain['check_hash'] = train['text'].str.contains('#')\n\ntest['check_http'] = test['text'].str.contains('http')\ntest['check_hash'] = test['text'].str.contains('#')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:33.387352Z","iopub.execute_input":"2022-02-22T23:28:33.387727Z","iopub.status.idle":"2022-02-22T23:28:33.414612Z","shell.execute_reply.started":"2022-02-22T23:28:33.387697Z","shell.execute_reply":"2022-02-22T23:28:33.413735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax =plt.subplots(1,2, figsize=(15,5))\nsns.countplot(data=train, x='check_http', hue='real_tweet', ax=ax[0], palette='Set2' )\nsns.countplot(data=train, x='check_hash', hue='real_tweet', ax=ax[1], palette='Set2');","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:33.415913Z","iopub.execute_input":"2022-02-22T23:28:33.417091Z","iopub.status.idle":"2022-02-22T23:28:33.795829Z","shell.execute_reply.started":"2022-02-22T23:28:33.417047Z","shell.execute_reply":"2022-02-22T23:28:33.794496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we expected, **check_http** has an effect on the target attribute\n\nAt first glance, it seemed to me that real tweets contain more hashtags, but **check_hash showed that this is not true**, but the distribution of the target feature in the presence of a hashtag has a 1:1 ratio. We remember that the target feature had a ratio of 6:4, so we will assume that **this feature is important**\n\n-------------","metadata":{}},{"cell_type":"markdown","source":"Great, we've generated some new features that can help the model better distinguish between a real tweet and a fake tweet.\n\nNow let's **go back to the feature text and clean it up** (Initially I wanted to clean up the text in block 2, but I decided that I would do it now so that block 2 can be devoted to deeper predprocessing)","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:33.79708Z","iopub.execute_input":"2022-02-22T23:28:33.797473Z","iopub.status.idle":"2022-02-22T23:28:33.813863Z","shell.execute_reply.started":"2022-02-22T23:28:33.797401Z","shell.execute_reply":"2022-02-22T23:28:33.812881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create a function that will perform the process of cleaning the text and analyze it in more detail.\n\n1. To begin with, we will **remove all stop-words** (i, where, our, the, etc.) from the text, this is necessary, since regardless of the class of the target feature, these words will appear very often in tweets and will not affect in any way choice of target variable class. This makes no sense, because when studying the words of a tweet, we want to find the relationship between the words used and the distribution of target feature classes, and stop-words will only create noise in the data\n\n2. **Convert all text to lowercase**\n\n3. **Delete all http requests** (the http request link will not be deleted, but then we will remove the remaining characters using **string.punctation**)\n\n4. **Replace all punctuation from the text with char+' '**, as a similar situation can happen: \"fish,cat\". Due to the fact that there is no space after the comma, we would get fishkat\n\n5. **Remove all other characters that are not alpha, numbers and space**\n\n6. **Remove extra spaces between words**\n\n\nI **didn’t use regular expressions** in the function, because I don’t understand them very well, but I don’t want to simply copy other people’s solutions without understanding","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    \n    stop_words = stopwords.words('english')\n    stop_words.append('i\\'m') # I saw this abbreviation, but it is not in the list of stop-words, so I added it manually\n    \n    text = text.lower()\n    text = ' '.join([char for char in text.split() if char not in stop_words])\n    \n    text = ' '.join(['' if 'http' in char else char for char in text.split()])\n    \n    text = ''.join([char+' ' if char in string.punctuation else char for char in text ])\n    \n    text = ''.join([char for char in text if char in string.ascii_lowercase or char in ' ' or  char in string.digits])\n    \n    text = ' '.join(text.split())\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:33.815473Z","iopub.execute_input":"2022-02-22T23:28:33.815794Z","iopub.status.idle":"2022-02-22T23:28:33.829436Z","shell.execute_reply.started":"2022-02-22T23:28:33.815761Z","shell.execute_reply":"2022-02-22T23:28:33.828377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_text_real = train[train['real_tweet']==1]['text'].head(5)\ncheck_text_fake = train[train['real_tweet']==0]['text'].head(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:33.830371Z","iopub.execute_input":"2022-02-22T23:28:33.830617Z","iopub.status.idle":"2022-02-22T23:28:33.850609Z","shell.execute_reply.started":"2022-02-22T23:28:33.83059Z","shell.execute_reply":"2022-02-22T23:28:33.849322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With these cycles, we will demonstrate the text **before using clean_text and after using clean_text**","metadata":{}},{"cell_type":"code","source":"for i, text in enumerate(check_text_real):\n    print(f'Clean [{i+1}]:', clean_text(text), end='\\n\\n')\n    print(f'Real [{i+1}]:', text, end='\\n\\n')\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:33.852504Z","iopub.execute_input":"2022-02-22T23:28:33.852871Z","iopub.status.idle":"2022-02-22T23:28:33.878432Z","shell.execute_reply.started":"2022-02-22T23:28:33.852824Z","shell.execute_reply":"2022-02-22T23:28:33.877341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, text in enumerate(check_text_fake):\n    print(f'Clean [{i+1}]:', clean_text(text), end='\\n\\n')\n    print(f'Real [{i+1}]:', text, end='\\n\\n')\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:33.88061Z","iopub.execute_input":"2022-02-22T23:28:33.881258Z","iopub.status.idle":"2022-02-22T23:28:33.893465Z","shell.execute_reply.started":"2022-02-22T23:28:33.881208Z","shell.execute_reply":"2022-02-22T23:28:33.892471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['clean_text'] = train['text'].apply(clean_text)\ntest['clean_text'] = test['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:33.894916Z","iopub.execute_input":"2022-02-22T23:28:33.895161Z","iopub.status.idle":"2022-02-22T23:28:36.097483Z","shell.execute_reply.started":"2022-02-22T23:28:33.89513Z","shell.execute_reply":"2022-02-22T23:28:36.096479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head().T","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:36.09925Z","iopub.execute_input":"2022-02-22T23:28:36.099543Z","iopub.status.idle":"2022-02-22T23:28:36.116851Z","shell.execute_reply.started":"2022-02-22T23:28:36.099509Z","shell.execute_reply":"2022-02-22T23:28:36.115238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, we have processed the text, generated new features.\n\nNow let's analyze the starting additional features, starting with the **keyword**","metadata":{}},{"cell_type":"markdown","source":"## Keyword Analysis","metadata":{}},{"cell_type":"code","source":"train['keyword'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:36.11886Z","iopub.execute_input":"2022-02-22T23:28:36.11914Z","iopub.status.idle":"2022-02-22T23:28:36.129433Z","shell.execute_reply.started":"2022-02-22T23:28:36.119108Z","shell.execute_reply":"2022-02-22T23:28:36.128308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are **not very many missing keyword values in the data**, so I think they could be removed, but the absence of a keyword can affect the target feature, for example, when fake tweets, people do not use the keyword.\n\nLet's test this and fill in the missing values with **Empty**","metadata":{}},{"cell_type":"code","source":"train['keyword'] = train['keyword'].fillna('Empty')\ntest['keyword'] = test['keyword'].fillna('Empty')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:36.131436Z","iopub.execute_input":"2022-02-22T23:28:36.132651Z","iopub.status.idle":"2022-02-22T23:28:36.144529Z","shell.execute_reply.started":"2022-02-22T23:28:36.132606Z","shell.execute_reply":"2022-02-22T23:28:36.143766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['keyword'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:36.145864Z","iopub.execute_input":"2022-02-22T23:28:36.146119Z","iopub.status.idle":"2022-02-22T23:28:36.165397Z","shell.execute_reply.started":"2022-02-22T23:28:36.14609Z","shell.execute_reply":"2022-02-22T23:28:36.164074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['keyword'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:36.167636Z","iopub.execute_input":"2022-02-22T23:28:36.168013Z","iopub.status.idle":"2022-02-22T23:28:36.179378Z","shell.execute_reply.started":"2022-02-22T23:28:36.167967Z","shell.execute_reply":"2022-02-22T23:28:36.178512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"keyword contains many unique values that are similar in meaning to each other.\n\nThey can be combined into one value like **destroy and destroyed**\n\nHowever, the **use of more colloquial keywords may have an impact on the target feature**, so I will not process them, as we may lose the emotional color of the keyword value.\n\nP.S. I am new to NLP, so I draw some conclusions based on my own assumptions. For example, in this case, I could misunderstand the semantic meaning of keyword\n\nP.S. Still, my guesses were confirmed, the models had better results, without processing the keyword values :)","metadata":{}},{"cell_type":"code","source":"train[train['real_tweet'] == 1]['keyword'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:36.180871Z","iopub.execute_input":"2022-02-22T23:28:36.182778Z","iopub.status.idle":"2022-02-22T23:28:36.201447Z","shell.execute_reply.started":"2022-02-22T23:28:36.18272Z","shell.execute_reply":"2022-02-22T23:28:36.200691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6,75), dpi=100)\nsns.countplot(data=train, y='keyword', hue='real_tweet',\n              order=train[train['real_tweet'] == 1]['keyword'].value_counts().index,\n             palette='bone');","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-22T23:28:36.203761Z","iopub.execute_input":"2022-02-22T23:28:36.204432Z","iopub.status.idle":"2022-02-22T23:28:41.441067Z","shell.execute_reply.started":"2022-02-22T23:28:36.20436Z","shell.execute_reply":"2022-02-22T23:28:41.439683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The countplot clearly shows how most keyword values have a **serious impact on the target feature.**\n\nOk, we are done with keyword, now let's start analysis **location**","metadata":{}},{"cell_type":"markdown","source":"## Location Analysis","metadata":{}},{"cell_type":"code","source":"train['location'].isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:41.442213Z","iopub.execute_input":"2022-02-22T23:28:41.442991Z","iopub.status.idle":"2022-02-22T23:28:41.450612Z","shell.execute_reply.started":"2022-02-22T23:28:41.442952Z","shell.execute_reply":"2022-02-22T23:28:41.449642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"missing values :\", round(train['location'].isnull().sum()/train.shape[0],3),'%')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:41.452093Z","iopub.execute_input":"2022-02-22T23:28:41.452545Z","iopub.status.idle":"2022-02-22T23:28:41.465399Z","shell.execute_reply.started":"2022-02-22T23:28:41.45251Z","shell.execute_reply":"2022-02-22T23:28:41.464334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Missing location values 3:10 of the total data size!**\n\nThis is a large number, so **I'm not sure if this feature can be included in the training data.**\n\nHowever, let's take a closer look at the values of location","metadata":{}},{"cell_type":"code","source":"train['location'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:41.466844Z","iopub.execute_input":"2022-02-22T23:28:41.467587Z","iopub.status.idle":"2022-02-22T23:28:41.481556Z","shell.execute_reply.started":"2022-02-22T23:28:41.467545Z","shell.execute_reply":"2022-02-22T23:28:41.480297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_location = train['location'].value_counts(normalize=True).head(50).index","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:41.482661Z","iopub.execute_input":"2022-02-22T23:28:41.483384Z","iopub.status.idle":"2022-02-22T23:28:41.499175Z","shell.execute_reply.started":"2022-02-22T23:28:41.483336Z","shell.execute_reply":"2022-02-22T23:28:41.497757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.countplot(data=train[train['location'].isin(top_location)], x='location', hue='real_tweet', palette='Pastel2_r')\nplt.xticks(rotation=80);","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:41.501621Z","iopub.execute_input":"2022-02-22T23:28:41.50251Z","iopub.status.idle":"2022-02-22T23:28:42.794801Z","shell.execute_reply.started":"2022-02-22T23:28:41.502456Z","shell.execute_reply":"2022-02-22T23:28:42.793982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"People indicated in the location of the **city, country or city-country**\n\nThe countplot shows that most of the tweets were from the **USA and UK.**\n\nThis large number of unique values tells us that there are very few tweets per location value, not counting USA and UK.\n\nPerhaps it is worth clearing this feature and **entering only 3 unique values: UK, USA, Other**\n\nHowever, I won't do this, because I still think that the number of missing values in location is very large and by setting the value to empty we will **create a lot of artificial information.**\n\n**In the future, I will not use this feature.**\n\nP.S. If this is a really important sign and I'm wrong, write me about it. Alas, I have not seen other laptops with good results who used location\n\n\n----------------------","metadata":{}},{"cell_type":"markdown","source":"#  <center>| Data Preprocessing |</center>","metadata":{}},{"cell_type":"markdown","source":"![](https://iprofi.kg/wp-content/uploads/2021/07/60ffd0153c1d3549690580.gif)","metadata":{}},{"cell_type":"markdown","source":"Ok, let's move on to the next block of solving the competition, **data preprocessing.**\n\nBut let's take a **look at the correlation matrix**. In the past, we have created some features that can be highly correlated with each other.","metadata":{}},{"cell_type":"markdown","source":"1. **Previously, we thought that count_punctation is a better choice than check_punctation**, however, this feature is moderately correlated with other features, but carries the same information to the target feature, so we **remove count_punctation**\n\n2. **Leave check_digits** as it carries more information to the target feature\n\n3. **Remove the location**, we decided that due to the large number of missing values, it will not help in training the model\n\n4. **Remove id and text**, we won't need them anymore","metadata":{}},{"cell_type":"code","source":"clean_train = train.drop(['count_digits','count_punctation','id','location', 'text'], axis=1)\nclean_test  = test.drop(['count_digits','count_punctation','id','location', 'text'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:42.795993Z","iopub.execute_input":"2022-02-22T23:28:42.796543Z","iopub.status.idle":"2022-02-22T23:28:42.805163Z","shell.execute_reply.started":"2022-02-22T23:28:42.79651Z","shell.execute_reply":"2022-02-22T23:28:42.804499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax =plt.subplots(1,2, figsize=(15,5))\nax[0].title.set_text('Before\\n')\nsns.heatmap(train.corr(), annot=True, ax=ax[0], fmt='.1g', cmap='Greens', linewidths=2, linecolor='black')\nax[1].title.set_text('After\\n')\nsns.heatmap(clean_train.corr(), annot=True, ax=ax[1], fmt='.1g', cmap='Greens', linewidths=2, linecolor='black');","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:42.806616Z","iopub.execute_input":"2022-02-22T23:28:42.807136Z","iopub.status.idle":"2022-02-22T23:28:44.173537Z","shell.execute_reply.started":"2022-02-22T23:28:42.80709Z","shell.execute_reply":"2022-02-22T23:28:44.172496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Additional Features Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Great, now we need to process our features for further use.","metadata":{}},{"cell_type":"code","source":"clean_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:44.180845Z","iopub.execute_input":"2022-02-22T23:28:44.18112Z","iopub.status.idle":"2022-02-22T23:28:44.196122Z","shell.execute_reply.started":"2022-02-22T23:28:44.181092Z","shell.execute_reply":"2022-02-22T23:28:44.194938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert **bool features to int**\n\nP.S. In the process of generating features, I did not think that they would have to be transformed :)","metadata":{}},{"cell_type":"code","source":"clean_train['check_http'] = clean_train['check_http'].apply(lambda x: 1 if x else 0)\nclean_train['check_hash'] = clean_train['check_hash'].apply(lambda x: 1 if x else 0)\n\nclean_test['check_http'] = clean_test['check_http'].apply(lambda x: 1 if x else 0)\nclean_test['check_hash'] = clean_test['check_hash'].apply(lambda x: 1 if x else 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:44.197841Z","iopub.execute_input":"2022-02-22T23:28:44.198138Z","iopub.status.idle":"2022-02-22T23:28:44.231209Z","shell.execute_reply.started":"2022-02-22T23:28:44.198104Z","shell.execute_reply":"2022-02-22T23:28:44.230202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now import and apply **MinMaxScaler()** to standardize len_text\n\nWe use **MinMax standardization to get the values [0:1]**. In what follows, we will use **Naive Bayes**, which requires only **positive values.**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:44.232926Z","iopub.execute_input":"2022-02-22T23:28:44.233328Z","iopub.status.idle":"2022-02-22T23:28:44.250594Z","shell.execute_reply.started":"2022-02-22T23:28:44.233279Z","shell.execute_reply":"2022-02-22T23:28:44.24903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scal = MinMaxScaler()\nscal.fit(clean_train['len_text'].values.reshape(-1,1))\nclean_train['len_text'] = scal.transform(clean_train['len_text'].values.reshape(-1,1))\nclean_test['len_text'] = scal.transform(clean_test['len_text'].values.reshape(-1,1))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:44.253575Z","iopub.execute_input":"2022-02-22T23:28:44.254214Z","iopub.status.idle":"2022-02-22T23:28:44.271106Z","shell.execute_reply.started":"2022-02-22T23:28:44.254164Z","shell.execute_reply":"2022-02-22T23:28:44.270189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Preprocessing","metadata":{}},{"cell_type":"markdown","source":"For this block, I need to explain what I'm doing\n\n1. We apply **word_tokenize()** to the text in order to split() all the words of the text. Why didn't we use split()? word_tokenize was created to handle large text sizes, so **word_tokenize is faster than split()**\n\n\n2. We apply **WordNetLemmatizer()** to the text in order to bring the words to the basic forms of the word, preserving the contextual meaning of the sentence.\n\nP.S. I have a bad level of English, so it's hard for me to explain in simple words what lemmatization is :(","metadata":{}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:44.272304Z","iopub.execute_input":"2022-02-22T23:28:44.273307Z","iopub.status.idle":"2022-02-22T23:28:44.285982Z","shell.execute_reply.started":"2022-02-22T23:28:44.273265Z","shell.execute_reply":"2022-02-22T23:28:44.284917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train['clean_text'] = clean_train['clean_text'].apply(lambda x: nltk.tokenize.word_tokenize(x))\nclean_test['clean_text'] = clean_test['clean_text'].apply(lambda x: nltk.tokenize.word_tokenize(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:44.287208Z","iopub.execute_input":"2022-02-22T23:28:44.287992Z","iopub.status.idle":"2022-02-22T23:28:46.407681Z","shell.execute_reply.started":"2022-02-22T23:28:44.287953Z","shell.execute_reply":"2022-02-22T23:28:46.406721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Before: ', train['clean_text'][1])\nprint('After: ', clean_train['clean_text'][1])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:46.408901Z","iopub.execute_input":"2022-02-22T23:28:46.409254Z","iopub.status.idle":"2022-02-22T23:28:46.416372Z","shell.execute_reply.started":"2022-02-22T23:28:46.409209Z","shell.execute_reply":"2022-02-22T23:28:46.415493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lem_word(text):\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text]\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:46.417652Z","iopub.execute_input":"2022-02-22T23:28:46.417895Z","iopub.status.idle":"2022-02-22T23:28:46.437668Z","shell.execute_reply.started":"2022-02-22T23:28:46.417865Z","shell.execute_reply":"2022-02-22T23:28:46.436698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train['clean_text'] = clean_train['clean_text'].apply(lem_word)\nclean_test['clean_text'] = clean_test['clean_text'].apply(lem_word)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:46.438887Z","iopub.execute_input":"2022-02-22T23:28:46.439139Z","iopub.status.idle":"2022-02-22T23:28:49.374269Z","shell.execute_reply.started":"2022-02-22T23:28:46.439108Z","shell.execute_reply":"2022-02-22T23:28:49.373215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Before: ', train['clean_text'][16])\nprint('After: ', clean_train['clean_text'][16])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.375298Z","iopub.execute_input":"2022-02-22T23:28:49.375525Z","iopub.status.idle":"2022-02-22T23:28:49.382199Z","shell.execute_reply.started":"2022-02-22T23:28:49.375499Z","shell.execute_reply":"2022-02-22T23:28:49.381265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Constructing Training Dataset","metadata":{}},{"cell_type":"markdown","source":"For this block, I need to explain what I'm doing\n\n1. I use **sample() to shuffle the data** as it is not in random order (Look at the keyword feature. Empty values only occur in the leading and last indexes)\n\n\n2. I **divide the data into X_train, y_train, X_test**\n\n\n3. After tokenization, we **need to turn clean_text into str()** (Now it is a list)\n\n\n4. I **use TfidfVectorizer()**. This function turns the text into a **sparse matrix**, in which each word found in the dictionary is assigned its own cell. Next, for each word, **we calculate the frequency of occurrences** in clean_text and keyword. **This is necessary to give lower weights to frequently used words.**\n\n\n5. **Use hstack() to merge 2 sparse matrices**\n\n\n6. **Transform the remaining features into a sparse matrix and combine with the main sparse matrix X_[]_tfid**\n\n\nAt each step, we display the shape so as not to accidentally lose data in the process of creating training and test data","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.384291Z","iopub.execute_input":"2022-02-22T23:28:49.385078Z","iopub.status.idle":"2022-02-22T23:28:49.395998Z","shell.execute_reply.started":"2022-02-22T23:28:49.385031Z","shell.execute_reply":"2022-02-22T23:28:49.395003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train = clean_train.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.39742Z","iopub.execute_input":"2022-02-22T23:28:49.397671Z","iopub.status.idle":"2022-02-22T23:28:49.414401Z","shell.execute_reply.started":"2022-02-22T23:28:49.397641Z","shell.execute_reply":"2022-02-22T23:28:49.413448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = clean_train.drop(['real_tweet'], axis=1)\ny_train = clean_train['real_tweet']\nX_test = clean_test.copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.415882Z","iopub.execute_input":"2022-02-22T23:28:49.416373Z","iopub.status.idle":"2022-02-22T23:28:49.432483Z","shell.execute_reply.started":"2022-02-22T23:28:49.416339Z","shell.execute_reply":"2022-02-22T23:28:49.430887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train shape: ', X_train.shape[0])\nprint('\\nTest shape: ', X_test.shape[0])\nprint('-'*30)\nprint('Train y: \\n\\n', y_train.value_counts(), sep='')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.433742Z","iopub.execute_input":"2022-02-22T23:28:49.433992Z","iopub.status.idle":"2022-02-22T23:28:49.448596Z","shell.execute_reply.started":"2022-02-22T23:28:49.43396Z","shell.execute_reply":"2022-02-22T23:28:49.447333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['clean_text'] = X_train['clean_text'].apply(lambda x: ' '.join(x))\nX_test['clean_text'] = X_test['clean_text'].apply(lambda x: ' '.join(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.450558Z","iopub.execute_input":"2022-02-22T23:28:49.451065Z","iopub.status.idle":"2022-02-22T23:28:49.475935Z","shell.execute_reply.started":"2022-02-22T23:28:49.45103Z","shell.execute_reply":"2022-02-22T23:28:49.475082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfid = TfidfVectorizer()\n#count_vect = CountVectorizer()\n\nX_train_tfid_text = tfid.fit_transform(X_train['clean_text'])\nX_test_tfid_text = tfid.transform(X_test['clean_text'])\n\nX_train_tfid_keyword = tfid.fit_transform(X_train['keyword'])\nX_test_tfid_keyword = tfid.transform(X_test['keyword'])\n\n#X_train_tfid_keyword = count_vect.fit_transform(X_train['keyword'])\n#X_val_tfid_keyword = count_vect.transform(X_val['keyword'])\n#X_test_tfid_keyword = count_vect.transform(X_test['keyword'])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.477023Z","iopub.execute_input":"2022-02-22T23:28:49.477664Z","iopub.status.idle":"2022-02-22T23:28:49.776457Z","shell.execute_reply.started":"2022-02-22T23:28:49.477626Z","shell.execute_reply":"2022-02-22T23:28:49.7753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_tfid = hstack([X_train_tfid_text, X_train_tfid_keyword])\nX_test_tfid = hstack([X_test_tfid_text, X_test_tfid_keyword])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.7779Z","iopub.execute_input":"2022-02-22T23:28:49.778153Z","iopub.status.idle":"2022-02-22T23:28:49.788059Z","shell.execute_reply.started":"2022-02-22T23:28:49.778116Z","shell.execute_reply":"2022-02-22T23:28:49.786978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Text shape')\nprint('Train: ' ,X_train_tfid_text.shape)\nprint('Test: ' ,X_test_tfid_text.shape)\nprint('-'*25)\nprint('Keyword shape')\nprint('Train: ' ,X_train_tfid_keyword.shape)\nprint('Test: ' ,X_test_tfid_keyword.shape)\nprint('-'*25)\nprint('Total shape')\nprint('Train: ' ,X_train_tfid.shape)\nprint('Test: ' ,X_test_tfid.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.789298Z","iopub.execute_input":"2022-02-22T23:28:49.789692Z","iopub.status.idle":"2022-02-22T23:28:49.801142Z","shell.execute_reply.started":"2022-02-22T23:28:49.789655Z","shell.execute_reply":"2022-02-22T23:28:49.799322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def another_feature_to_csr_matrix(feature, matrix):\n    new_matrix = csr_matrix(hstack([matrix, feature.values.reshape(-1,1)]))\n    return new_matrix","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.803119Z","iopub.execute_input":"2022-02-22T23:28:49.804189Z","iopub.status.idle":"2022-02-22T23:28:49.81937Z","shell.execute_reply.started":"2022-02-22T23:28:49.804134Z","shell.execute_reply":"2022-02-22T23:28:49.818647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in X_train.loc[:,'len_text':'check_hash']:\n    X_train_tfid = another_feature_to_csr_matrix(X_train[feature], X_train_tfid)\n\nfor feature in X_test.loc[:,'len_text':'check_hash']:\n    X_test_tfid = another_feature_to_csr_matrix(X_test[feature], X_test_tfid)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.82049Z","iopub.execute_input":"2022-02-22T23:28:49.821193Z","iopub.status.idle":"2022-02-22T23:28:49.873919Z","shell.execute_reply.started":"2022-02-22T23:28:49.821151Z","shell.execute_reply":"2022-02-22T23:28:49.872785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total shape')\nprint('Train: ' ,X_train_tfid.shape)\nprint('Test: ' ,X_test_tfid.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.875236Z","iopub.execute_input":"2022-02-22T23:28:49.875539Z","iopub.status.idle":"2022-02-22T23:28:49.881811Z","shell.execute_reply.started":"2022-02-22T23:28:49.875505Z","shell.execute_reply":"2022-02-22T23:28:49.880782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, now **let's start creating and training the model**","metadata":{}},{"cell_type":"markdown","source":"------------------","metadata":{}},{"cell_type":"markdown","source":"#  <center>| Machine Learning |</center>","metadata":{}},{"cell_type":"markdown","source":"\n![](https://4.bp.blogspot.com/-R1w3hy8_V6k/WuSOGXuhn1I/AAAAAAAATio/z4_6Pdwutk4qggn2W4dwAjFQC1Fj4gFrACLcBGAs/s640/1_FYFI4jbAUMqbXxlo6V_lBA.png) ","metadata":{}},{"cell_type":"markdown","source":"Great, in this block we will do machine learning and create a model that will learn to predict the truth of a tweet\n\nFirst, import the necessary libraries\n\nIn this notebook, I will use models such as:\n\n1) **Multinomial Naive Bayes** (As recommended for working with texts)\n\n2) **Logistic Regression**\n\n3) **SVM**\n\n4) I will try to train **Logistic Regression and SVM using SGD**\n\n5) At the end we **use StackingClassifier to predict the final result based on the available models**\n","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import StackingClassifier\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:49.883039Z","iopub.execute_input":"2022-02-22T23:28:49.883647Z","iopub.status.idle":"2022-02-22T23:28:50.035792Z","shell.execute_reply.started":"2022-02-22T23:28:49.883608Z","shell.execute_reply":"2022-02-22T23:28:50.034986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will write functions to quickly call the necessary metrics\n\nWe did **not split the data into X_validation and X_train**, instead, we will use **cross-validation** on X_train and evaluate the **f1 score** of the model on **cross_val_score**","metadata":{}},{"cell_type":"code","source":"random_state = 1\n\ndef print_metrics(estimator):\n    acc = cross_val_score(estimator, X_train_tfid, y_train, cv=3, scoring='accuracy', n_jobs=-1)\n    prec = cross_val_score(estimator, X_train_tfid, y_train, cv=3, scoring='precision', n_jobs=-1)\n    rec = cross_val_score(estimator, X_train_tfid, y_train, cv=3, scoring='recall', n_jobs=-1)\n    f1 = cross_val_score(estimator, X_train_tfid, y_train, cv=3, scoring='f1', n_jobs=-1)\n    \n    print(estimator,'\\n--------------------------------')\n    print('ACCURACY:', np.mean(acc))\n    print('PRECISION:', np.mean(prec))\n    print('RECALL:', np.mean(rec))\n    print('\\nMain metric:', '\\n--------------------------------', sep='')\n    print('F1:', np.mean(f1))\n    print('--------------------------------\\n')\n\ndef save_main_metric(estimator):\n    f1 = np.mean(cross_val_score(estimator, X_train_tfid, y_train, cv=3, scoring='f1', n_jobs=-1))\n    final_scores.append(f1)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:50.037014Z","iopub.execute_input":"2022-02-22T23:28:50.037621Z","iopub.status.idle":"2022-02-22T23:28:50.048652Z","shell.execute_reply.started":"2022-02-22T23:28:50.037518Z","shell.execute_reply":"2022-02-22T23:28:50.047385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base models","metadata":{}},{"cell_type":"markdown","source":"I define the selected models by **specifying hyperparameters**\n\nI did not introduce an additional block with GridSearch, so as not to overload the notebook with unnecessary information\n\nInstead, manually **set the optimal non-overtrained parameters**","metadata":{}},{"cell_type":"code","source":"nb = MultinomialNB(fit_prior=False)\n\nlogit = LogisticRegression(C=10 ,max_iter=1000)\n\nsvc = SVC(kernel='linear')\n\nsgd_svm = SGDClassifier(alpha=5e-4, random_state=random_state)\n\nsgd_log = SGDClassifier(loss='log', alpha=5e-05, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:50.050026Z","iopub.execute_input":"2022-02-22T23:28:50.050297Z","iopub.status.idle":"2022-02-22T23:28:50.071304Z","shell.execute_reply.started":"2022-02-22T23:28:50.050264Z","shell.execute_reply":"2022-02-22T23:28:50.070267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_metrics(nb)\nprint_metrics(logit)\nprint_metrics(svc)\nprint_metrics(sgd_svm)\nprint_metrics(sgd_log)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:28:50.072367Z","iopub.execute_input":"2022-02-22T23:28:50.07307Z","iopub.status.idle":"2022-02-22T23:29:20.104233Z","shell.execute_reply.started":"2022-02-22T23:28:50.073018Z","shell.execute_reply":"2022-02-22T23:29:20.103307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great, it seems that all models show approximately the **same F1 score.**\n\nFirst you need to show the target metric formula: **F1 = 2 * (precision * recall) / (precision + recall)**\n\nF1, along with Roc auc, can be considered a metric of the overall quality of the model, since it uses precision and recall in its formula\n\n\nAs we can see, some **Precision and Recall models differ**\n\nFor example, **MultinomialNB has approximately the same Precision and Recall - ~ 0.75**\n\nAnd **SVM has Precision 0.8 and Recall 0.7**\n\nHowever, **F1 for MultinomialNB and SVM is approximately the same**","metadata":{}},{"cell_type":"markdown","source":"## Ensemble Model : Stacking","metadata":{}},{"cell_type":"markdown","source":"Training multiple models to solve the same problem are combined and get the best result.\n\nThis approach in machine learning is called the **ensemble method.**\n\nThe basic premise is that the result of multiple models will be more accurate than the result of only one model.\n\nSo let's use the StackingClassifier to get the average result by aggregating all the models.\n\nIn this notebook, I am **using a StackingClassifier with a logit meta model.**\n\nP.S. I don't understand very well how to choose a meta model, but for this task, logit showed better results than other models.","metadata":{}},{"cell_type":"code","source":"stacking_estimators = [('svc', svc), ('nb', nb), ('sgd_svm', sgd_svm), ('sgd_log', sgd_log)]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:29:20.106203Z","iopub.execute_input":"2022-02-22T23:29:20.106476Z","iopub.status.idle":"2022-02-22T23:29:20.112037Z","shell.execute_reply.started":"2022-02-22T23:29:20.106443Z","shell.execute_reply":"2022-02-22T23:29:20.110889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacking = StackingClassifier(stacking_estimators, logit, cv=5, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:29:20.113451Z","iopub.execute_input":"2022-02-22T23:29:20.113781Z","iopub.status.idle":"2022-02-22T23:29:20.129711Z","shell.execute_reply.started":"2022-02-22T23:29:20.113747Z","shell.execute_reply":"2022-02-22T23:29:20.128455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_metrics(stacking)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:29:20.130815Z","iopub.execute_input":"2022-02-22T23:29:20.131872Z","iopub.status.idle":"2022-02-22T23:30:40.612231Z","shell.execute_reply.started":"2022-02-22T23:29:20.131831Z","shell.execute_reply":"2022-02-22T23:30:40.611193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results Analysis","metadata":{}},{"cell_type":"markdown","source":"Great, let's take a **look at the results of all models**, for this I will collect their results using **save_main_metric()**\n\nP.S. I described this function earlier, along with print_metric()","metadata":{}},{"cell_type":"code","source":"final_scores = [] #check save_main_metric()\nall_model = [nb, logit, svc, sgd_svm, sgd_log, stacking]\nname_model = ['Multinomial NB','Logistic Regression','SVM','SGD SVM', 'SGD Logit', 'Stacking model']\n\nfor model in all_model:\n    save_main_metric(model)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:30:40.61463Z","iopub.execute_input":"2022-02-22T23:30:40.614887Z","iopub.status.idle":"2022-02-22T23:31:07.37631Z","shell.execute_reply.started":"2022-02-22T23:30:40.614858Z","shell.execute_reply":"2022-02-22T23:31:07.375117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df = pd.DataFrame(final_scores, index=name_model, columns=['F1 Score'])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:07.378005Z","iopub.execute_input":"2022-02-22T23:31:07.378249Z","iopub.status.idle":"2022-02-22T23:31:07.384184Z","shell.execute_reply.started":"2022-02-22T23:31:07.378219Z","shell.execute_reply":"2022-02-22T23:31:07.382753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.T","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:07.385973Z","iopub.execute_input":"2022-02-22T23:31:07.387026Z","iopub.status.idle":"2022-02-22T23:31:07.411112Z","shell.execute_reply.started":"2022-02-22T23:31:07.386968Z","shell.execute_reply":"2022-02-22T23:31:07.410468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,8), dpi=100)\nsns.pointplot(y=result_df['F1 Score'] , x=result_df.index ,markers='o',linestyles='--', color='y');","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:07.412465Z","iopub.execute_input":"2022-02-22T23:31:07.412891Z","iopub.status.idle":"2022-02-22T23:31:07.745106Z","shell.execute_reply.started":"2022-02-22T23:31:07.412859Z","shell.execute_reply":"2022-02-22T23:31:07.74406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the **Stacking model got the best result F1**\n\nGreat, in that case, use it for the final results.","metadata":{}},{"cell_type":"markdown","source":"#  <center>| Final result & Submission |</center>","metadata":{}},{"cell_type":"code","source":"stacking.fit(X_train_tfid, y_train);\npred_train_stack = stacking.predict(X_train_tfid)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:07.747178Z","iopub.execute_input":"2022-02-22T23:31:07.747545Z","iopub.status.idle":"2022-02-22T23:31:33.955828Z","shell.execute_reply.started":"2022-02-22T23:31:07.747498Z","shell.execute_reply":"2022-02-22T23:31:33.954711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conf_matrix_plot(y_true, y_pred):\n    plt.figure(figsize=(10,8))\n    sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='.4g', cmap='RdGy', linewidths=3, linecolor='black')\n    plt.title('\\nConfusion Matrix for Train\\n', fontsize=25);\n    plt.xlabel('Predicted Values', fontsize=15)\n    plt.ylabel('Actual Values', fontsize=15);","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:33.957482Z","iopub.execute_input":"2022-02-22T23:31:33.957976Z","iopub.status.idle":"2022-02-22T23:31:33.967453Z","shell.execute_reply.started":"2022-02-22T23:31:33.95793Z","shell.execute_reply":"2022-02-22T23:31:33.966389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix_plot(y_train, pred_train_stack)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:33.972419Z","iopub.execute_input":"2022-02-22T23:31:33.97322Z","iopub.status.idle":"2022-02-22T23:31:34.325317Z","shell.execute_reply.started":"2022-02-22T23:31:33.973161Z","shell.execute_reply":"2022-02-22T23:31:34.324359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be seen in the **Confusion Matrix**, the **model is better at identifying a fake tweet than a real tweet.**\n\nThis could be seen from print_metric(). **Precision is noticeably higher than Recall**\n\nHowever, **using predict_proba()**, we could get the probabilities of the classes of the target feature and by changing the treshold of probabilities to **balance these class**\n\nLet me demonstrate it","metadata":{}},{"cell_type":"code","source":"pred_train_stack_proba = stacking.predict_proba(X_train_tfid)[:,1]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:34.326713Z","iopub.execute_input":"2022-02-22T23:31:34.327077Z","iopub.status.idle":"2022-02-22T23:31:39.297841Z","shell.execute_reply.started":"2022-02-22T23:31:34.327034Z","shell.execute_reply":"2022-02-22T23:31:39.296499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"treshold = 0.36","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:39.303989Z","iopub.execute_input":"2022-02-22T23:31:39.30477Z","iopub.status.idle":"2022-02-22T23:31:39.318046Z","shell.execute_reply.started":"2022-02-22T23:31:39.304704Z","shell.execute_reply":"2022-02-22T23:31:39.316445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix_plot(y_train, np.where(pred_train_stack_proba > treshold , 1, 0))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:39.320391Z","iopub.execute_input":"2022-02-22T23:31:39.320846Z","iopub.status.idle":"2022-02-22T23:31:39.662513Z","shell.execute_reply.started":"2022-02-22T23:31:39.32078Z","shell.execute_reply":"2022-02-22T23:31:39.661228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------------------------","metadata":{}},{"cell_type":"markdown","source":"As you can see, the **mispredicted classes have become more balanced.**\n\nGreat, let's make a final prediction and put it on **Kaggle**","metadata":{}},{"cell_type":"code","source":"final_predict = stacking.predict_proba(X_test_tfid)[:,1]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:39.663971Z","iopub.execute_input":"2022-02-22T23:31:39.664195Z","iopub.status.idle":"2022-02-22T23:31:41.634954Z","shell.execute_reply.started":"2022-02-22T23:31:39.664168Z","shell.execute_reply":"2022-02-22T23:31:41.633723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predict = np.where(final_predict>0.5, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:41.641265Z","iopub.execute_input":"2022-02-22T23:31:41.641729Z","iopub.status.idle":"2022-02-22T23:31:41.64723Z","shell.execute_reply.started":"2022-02-22T23:31:41.641672Z","shell.execute_reply":"2022-02-22T23:31:41.646158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'Id':test.id, 'target': final_predict})","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:41.648893Z","iopub.execute_input":"2022-02-22T23:31:41.649337Z","iopub.status.idle":"2022-02-22T23:31:41.666628Z","shell.execute_reply.started":"2022-02-22T23:31:41.64929Z","shell.execute_reply":"2022-02-22T23:31:41.664986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.T","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:41.668686Z","iopub.execute_input":"2022-02-22T23:31:41.669914Z","iopub.status.idle":"2022-02-22T23:31:41.711436Z","shell.execute_reply.started":"2022-02-22T23:31:41.669772Z","shell.execute_reply":"2022-02-22T23:31:41.710378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission.to_csv('submission_NLP.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:31:41.717492Z","iopub.execute_input":"2022-02-22T23:31:41.721112Z","iopub.status.idle":"2022-02-22T23:31:41.729432Z","shell.execute_reply.started":"2022-02-22T23:31:41.721032Z","shell.execute_reply":"2022-02-22T23:31:41.728116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <center>Thank you for watching my project, I will be grateful if you upvoted and give feedback about my work in the comments. I want to improve my skills, and if you find any mistakes in the project, please tell me about it. Also, if you liked the structure of the project, you can see the rest of my project, in each of them I try to describe my actions in detail, as well as not just solve the problem, but use new technologies </center>","metadata":{}},{"cell_type":"markdown","source":"![](https://data.whicdn.com/images/218833361/original.gif)","metadata":{}}]}