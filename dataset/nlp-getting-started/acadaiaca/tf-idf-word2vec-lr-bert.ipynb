{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Introduction**\n* This is my first time to join kaggle competion and get sarted in the nlp field. I still have some problems in this notebook, so I would be very gtateful for anyone to help me solve the problems or suggest me improvements.\n* I tried different models here. Including tf-idf + logistic regression/decision tree, word2vec + logistic regression, embedding + cnn, word2vec + cnn, bert. The result shows that the best accuracy is bert, next is logistic regression, then is word2vec+cnn.\n* When I wrote this kernel, I learned much from kernels below. The ideas from these kernels inspired me a lot, and some of my codes came from them. Thanks a lot to them!\n* [https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert](http://)\n* [https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub](http://)\n* [https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove](http://)\n* [https://blog.csdn.net/qq_43522113/article/details/89888036?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522160680506719721942257729%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=160680506719721942257729&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_v1~rank_blog_v1-7-89888036.pc_v1_rank_blog_v1&utm_term=%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom textblob import TextBlob\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom gensim.models import Word2Vec\nfrom gensim.models.keyedvectors import KeyedVectors\nimport time\nfrom keras.layers import Dense, Input, Flatten, Dropout\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Sequential\nfrom keras import losses\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\nsubmit_data = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[train_data['text'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.groupby('target').count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploring"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"piedata = train_data['target']\nplt.figure(figsize=(6,6))\npiedata.value_counts().plot(kind = 'pie',autopct = '%.2f%%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More tweets with class 0 ( No disaster).  Accord with reality. "},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words_0 = train_data[train_data['target']==0]['text'].apply(lambda x: len(x.split()))\nnum_words_1 = train_data[train_data['target']==1]['text'].apply(lambda x: len(x.split()))\nplt.figure(figsize=(12,6))\nsns.kdeplot(num_words_0, shade=True, color = 'b').set_title('Kernel distribution of number of words')\nsns.kdeplot(num_words_1, shade=True, color = 'r')\nplt.legend(labels=['0_no disaster', '1_disaster'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_word_0 = train_data[train_data['target']==0]['text'].str.split().map(lambda x: [len(i) for i in x])\nave_len_0 = len_word_0.map(lambda x: np.mean(x))\nlen_word_1 = train_data[train_data['target']==1]['text'].str.split().map(lambda x: [len(i) for i in x])\nave_len_1 = len_word_1.map(lambda x: np.mean(x))\nplt.figure(figsize=(12,6))\nsns.kdeplot(ave_len_0, shade=True, color='b').set_title('Kernel distribution of average words lenth')\nsns.kdeplot(ave_len_1, shade=True, color='r')\nplt.legend(labels=['0_no disaster', '1_disaster'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both the distribution of words number and words average lenth of two  class are similiar. Class 1(disaster) trends to have more words and longer words length. Accord with reality."},{"metadata":{},"cell_type":"markdown","source":"# **Cleaning Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train_data, submit_data])\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'https?\\S+'), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'[\\//:,.!?@&\\-\\'\\`\\\"\\_\\n\\#]'), ' ', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'<.*?>'), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  \n                           u\"\\U0001F300-\\U0001F5FF\"  \n                           u\"\\U0001F680-\\U0001F6FF\"  \n                           u\"\\U0001F1E0-\\U0001F1FF\"  \n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'\\d'), '', x))\ndata['text'] = data['text'].apply(lambda x: re.sub(re.compile(r'[^\\w]'), ' ', x))\ndata['text'] = data['text'].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntext_series = data.loc[:,'text']\nfor i in range(len(text_series)):\n    content = text_series.iloc[i]\n    textblob = TextBlob(content)\n    text_series.iloc[i] = textblob.correct()\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Engineering 1st. TF-IDF**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train = data[0:train_data.shape[0]]\nclean_submit = data[train_data.shape[0]:-1]\n\nX_train, X_test, y_train, y_test = train_test_split(clean_train['text'], clean_train['target'],\n                                                   test_size = 0.2, random_state = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tfidf(words):\n    tfidf_vectorizer = TfidfVectorizer()\n    data_feature = tfidf_vectorizer.fit_transform(words)\n    return data_feature, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train.tolist())\nX_test_tfidf = tfidf_vectorizer.transform(X_test.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model 1st. LogisticRegression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_tfidf = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', n_jobs = -1)\nlr_tfidf.fit(X_train_tfidf, y_train)\ny_predicted_lr = lr_tfidf.predict(X_test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def score_metrics(y_test, y_predicted):\n    accuracy = accuracy_score(y_test, y_predicted)\n    precision = precision_score(y_test, y_predicted)\n    recall = recall_score(y_test, y_predicted)\n    print(\"accuracy = %0.3f, precision = %0.3f, recall = %0.3f\" % (accuracy, precision, recall))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_metrics(y_test, y_predicted_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_test, y_predicted, title='Confusion Matrix'):\n    cm = confusion_matrix(y_test, y_predicted)\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm,annot=True, fmt='.20g')\n    plt.title(title)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test, y_predicted_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fail to sort and plot the top 10 most important features in disaster and non-disaster text\n'''\nindex_to_word = [(v,k) for k,v in tfidf_vectorizer.vocabulary_.items()]\nsorted(index_to_word, key=lambda x: x[0], reverse=True)\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model 2nd. DecisionTree**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([\n    ('clf', DecisionTreeClassifier(splitter='random', class_weight='balanced'))\n])\nparameters = {\n    'clf__max_depth':(150,160,165),\n    'clf__min_samples_split':(18,20,23),\n    'clf__min_samples_leaf':(5,6,7)\n}\n\ndf_tfidf = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=-1, scoring='f1')\ndf_tfidf.fit(X_train_tfidf, y_train)\n\nprint(df_tfidf.best_estimator_.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predicted_dt = df_tfidf.predict(X_test_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_metrics(y_test, y_predicted_dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test, y_predicted_dt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Engineering 2nd. Word2vec**"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install gensim -i http://pypi.douban.com/simple --trusted-host pypi.douban.com","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests\nurl = 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\nfilename = url.split('/')[-1]\nr = requests.get(url)\nwith open(filename, \"wb\") as file:\n        file.write(r.content)\n        \n!ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Download the trained word2vec vectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = stopwords.words('english')\nfor word in ['us','no','yet']:\n    stop_words.append(word)\n\ndata_list = []\ntext_series = data['text']\nfor i in range(len(text_series)):\n    content = text_series.iloc[i]\n    cutwords = [word for word in content.split(' ') if word not in  stop_words if len(word) != 0]\n    data_list.append(cutwords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get rid of stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(data_list)):\n    content = data_list[i]\n    if len(content) <1:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_list[7626]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After deleting the stopwords, some context became null. So, when dealing with the next step vectorization, I need to change these text to zero vector. Or, at last I put feature matrix into the model will fail because of different dimension. "},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nstarttime = time.time()\nword2vec_model = Word2Vec(data_list, size=300, iter=10, min_count=10)\nusedtime = time.time() - starttime\nprint('It took %.2fseconds to train word2vec' %usedtime)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nword2vec_path='./GoogleNews-vectors-negative300.bin.gz'\nword2vec_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec_model.wv['earthquake'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_textVector(data_list, word2vec, textsVectors_list):\n    for i in range(len(data_list)):\n        words_perText = data_list[i]\n        if len(words_perText) < 1:\n            words_vector = [np.zeros(300)]\n        else:\n            words_vector = [word2vec.wv[k]  if k in word2vec_model else  np.zeros(300) for k in words_perText]\n        text_vector = np.array(words_vector).mean(axis=0)\n        textsVectors_list.append(text_vector)\n    return textsVectors_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The reasons for writing  'else  np.zeros(300)' is that some words cannot be vectorized by word2vec then becoming nan."},{"metadata":{"trusted":true},"cell_type":"code","source":"textsVectors_list = []\nget_textVector(data_list, word2vec_model, textsVectors_list)\nX = np.array(textsVectors_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a check."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.isnull(X).any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec_X = X[0:train_data.shape[0]]\ny = data['target'][0:train_data.shape[0]]\nword2vec_submit = X[train_data.shape[0]:-1]\n\nX_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(word2vec_X, y,\n                                                   test_size = 0.2, random_state = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train_word2vec.shape, y_train_word2vec.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec_lr = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', n_jobs = -1)\nword2vec_lr.fit(X_train_word2vec, y_train_word2vec)\ny_predicted_word2vec_lr = word2vec_lr.predict(X_test_word2vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_metrics(y_test_word2vec, y_predicted_word2vec_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test_word2vec, y_predicted_word2vec_lr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Less 1(disaster) predicted to be 0(no-disaster). But more 0(no-disaster) predicted to be 1."},{"metadata":{},"cell_type":"markdown","source":"Explore the wrong predicted label."},{"metadata":{"trusted":true},"cell_type":"code","source":"compare_list = []\nfor (i,j) in zip(y_test_word2vec, y_predicted_word2vec_lr):\n    k = i - j\n    compare_list.append(k)\n\nwrong_num = [i for i,j in enumerate(compare_list) if j != 0]\ntext_series[0:train_data.shape[0]][wrong_num]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model 3rd. CNN**"},{"metadata":{},"cell_type":"markdown","source":"**Embedding + CNN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lenlen = []\nfor i in range(len(data_list)):\n    content = data_list[i]\n    perlen = len(content)\n    lenlen.append(perlen)\nprint(max(lenlen))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_sequence_length = 26\nembedding_dim = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(data_list)\nsequences = tokenizer.texts_to_sequences(data_list)\nword_index = tokenizer.word_index\ncnn_data = pad_sequences(sequences, maxlen = max_sequence_length)\ncnn_label = to_categorical(np.asarray(train_data['target']))\nprint('len of word_index:', len(word_index))\nprint('shape of data tensor:', cnn_data.shape)\nprint('shape of label tensoe:', cnn_label.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainCNN_data = cnn_data[0:train_data.shape[0]]\nX_train_cnn, X_test_cnn, y_train_cnn, y_test_cnn = train_test_split(trainCNN_data, cnn_label,\n                                                   test_size = 0.2, random_state = 4)\nX_cnn, X_val_cnn, y_cnn, y_val_cnn = train_test_split(X_train_cnn, y_train_cnn,\n                                                   test_size = 0.2, random_state = 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CNNmodel = Sequential()\nCNNmodel.add(Embedding(len(word_index)+1, embedding_dim, input_length = max_sequence_length))\nCNNmodel.add(Conv1D(filters=250, kernel_size=3, strides=1, padding='valid', activation = 'relu'))\nCNNmodel.add(MaxPooling1D(pool_size=3))\nCNNmodel.add(Flatten())\nCNNmodel.add(Dense(embedding_dim, activation='relu'))\nCNNmodel.add(Dropout(0.8))\nCNNmodel.add(Dense(cnn_label.shape[1], activation='sigmoid'))\n\nCNNmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CNNmodel.compile(optimizer='adam', loss=losses.binary_crossentropy, metrics=['accuracy'])\nhistory = CNNmodel.fit(X_cnn, y_cnn, epochs=3, validation_data=(X_val_cnn, y_val_cnn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5,1])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = CNNmodel.evaluate(X_test_cnn, y_test_cnn, verbose=2)\nprint('test loss:',test_loss)\nprint('test acc:',test_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Word2Vec + CNN**"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\nfor word, i in word_index.items(): \n    if word in word2vec_model:\n        embedding_matrix[i] = np.asarray(word2vec_model.wv[word])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer = Embedding(len(word_index)+1,\n                           embedding_dim,\n                           weights = [embedding_matrix],\n                           input_length = max_sequence_length,\n                           trainable = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Conv1D(filters=150, kernel_size=3, strides=1, padding='valid', activation = 'relu'))\nmodel.add(MaxPooling1D(pool_size=3))\nmodel.add(Flatten())\nmodel.add(Dense(embedding_dim, activation='relu'))\nmodel.add(Dropout(0.8))\nmodel.add(Dense(cnn_label.shape[1], activation='sigmoid'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss=losses.binary_crossentropy, metrics=['accuracy'])\nhistory = model.fit(X_cnn, y_cnn, epochs=10, validation_data=(X_val_cnn, y_val_cnn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5,1])\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(X_test_cnn, y_test_cnn, verbose=2)\nprint('test loss:',test_loss)\nprint('test acc:',test_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model 4th. TFhub+Bert**"},{"metadata":{},"cell_type":"markdown","source":"*Ensure tensorflow's version >= 2.0 & tensorflow_hub's version >= 0.5.0*"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_hub as hub\nhub.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n# get the official tokenization created by the Google team (??)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* *I have a question: If I do not get the official tokenization script here, 'tokenizer.tokenize' in get_tokens function will returns error:'raise _exceptions.UnparsedFlagAccessError(error_message)--Trying to access flag ... before flags were parsed'.*\n* *If anyone konws the reasons, please tell me. THANKS!*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, bert_layer, max_len=128):\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    \n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        text = text[:max_len - 2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        input_ids = tokens + [0]* pad_len\n        all_tokens.append(input_ids)\n\n        masks = [1]*len(input_sequence) + [0]* pad_len\n        all_masks.append(masks)\n        \n        segments = [0]* max_len\n        all_segments.append(segments)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n    \ndef build_model(bert_layer, max_len = 128, lr = 1e-5):\n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32,name=\"segment_ids\")\n        \n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    dense_out = Dense(1,activation=\"relu\")(pooled_output)\n    drop_out = tf.keras.layers.Dropout(0.8)(dense_out)\n    out = Dense(1,activation=\"sigmoid\")(pooled_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    adam = tf.keras.optimizers.Adam(lr)\n    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n        \n    return model\n\n\ndef plot_curve(history):\n    plt.plot(history.history['accuracy'], label='accuracy')\n    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.ylim([0.5,1])\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read and encode train data\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n\ntrain_input = bert_encode(train.text.values, bert_layer, max_len=128)\ntrain_labels = np.array(train.target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Here I used the data without cleaning.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nmodel = build_model(bert_layer, max_len=128, lr = 1e-5)\nmodel.summary()\n\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)\n\nplot_curve(train_history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Obviously, the model is overfitting here. If anyone knows nice tricks to adjust overfitting, please tell me. Thanks!*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ntest_input = bert_encode(test.text.values, bert_layer, max_len=128)\nmodel.load_weights('model.h5')\ntest_pred = model.predict(test_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission['target'] = np.round(test_pred).astype('int')\nsubmission.to_csv('submission.csv', index=False)\nsubmission.groupby('target').count()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}