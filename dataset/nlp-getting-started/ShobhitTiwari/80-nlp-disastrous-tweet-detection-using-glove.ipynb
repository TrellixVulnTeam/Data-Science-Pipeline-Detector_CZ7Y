{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Imports","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nfrom numpy.random import seed\nseed(1)\nimport pandas as pd \nfrom sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\n\nimport re\nimport nltk\nfrom collections import defaultdict\nfrom collections import  Counter\nimport seaborn as sns\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read data\nLets read both train and test set. Also we will create a df for submission which we will use later.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntestset = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\ntweet.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets see data distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = tweet.replace({\"target\" : {1 : \"Disaster\", 0 : \"Not disaster\"}}).groupby(['target'])['target'].count().plot.bar(title = \"Train set count by disaster/not disaster\")\n_ = ax.set_xlabel('Disaster?')\n_ = ax.set_ylabel('Count')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create corpus of all words in trainset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus():\n    corpus=[]\n    \n    for x in tweet['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyze common words\n\nThanks to this notebook [basic-eda-cleaning-and-glove](https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus()\nlst_stopwords = nltk.corpus.stopwords.words(\"english\")\n\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in lst_stopwords) :\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyze bi-grams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(corpus)[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text coverage\n\nWith pretrained embeddings its often useful to get the text as close to embedding as possible. So we will not use techniques like stopwords removal, stemming and lammatizing. Now are are checking how many words in our vocabulary are part of embedding and what is the text coverage. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import operator\n\ndef build_vocab(X):\n    \n    tweets = X.apply(lambda s: s.split()).values      \n    vocab = {}\n    \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\ndef check_embeddings_coverage(X, embeddings):\n    \n    vocab = build_vocab(X)    \n    \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n            \n    vocab_coverage = len(covered) / len(vocab)\n    text_coverage = (n_covered / (n_covered + n_oov))\n    \n    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    return sorted_oov, vocab_coverage, text_coverage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We will use Glove 100d embeddings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check coverage on train and test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(tweet[\"text\"], embedding_dict)\ntest_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(testset['text'], embedding_dict)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thats pretty low, let do some cleaning to bring data closer to embedding. On inspecting the out of coverage item found some misspellings. So we will remove punctuation,urls, correct spellings ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data PreProcessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def utils_preprocess_text(text):\n        ## clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n     \n    ## clean urls \n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    text = url.sub(r'',text)\n    \n    url = re.compile(r'http?://\\S+|www\\.\\S+')\n    text = url.sub(r'',text)\n    \n    ## remove html \n    html=re.compile(r'<.*?>') \n    html.sub(r'',text)\n        \n        \n    text = re.sub(r'mh370','flight crash',text)     \n    text = re.sub(r'û_','',text)     \n    text = re.sub(r'ûò','',text) \n    text = re.sub(r'typhoondevastated','typhoon devastated',text)      \n    text = re.sub(r'irandeal','iran deal',text)      \n    text = re.sub(r'worldnews','world news',text)      \n    text = re.sub(r'animalrescue','animal rescue',text)      \n    text = re.sub(r'viralspell','viral spell',text)      \n    text = re.sub(r'griefûª','grief',text)      \n    text = re.sub(r'pantherattack','panther attack',text)      \n    text = re.sub(r'injuryi495','injury in 495',text) \n    text = re.sub(r'explosionproof','explosion proof',text) \n    text = re.sub(r'americaûªs','americans',text) \n    return text\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combine data in train and test set ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([tweet,testset])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets call the preprocessing routine to clean data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf[\"text_clean\"] = df[\"text\"].apply(lambda x: utils_preprocess_text(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check coverage with embeddings post cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_glove_oov, df_glove_vocab_coverage, df_glove_text_coverage = check_embeddings_coverage(df[\"text_clean\"], embedding_dict)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(df_glove_vocab_coverage, df_glove_text_coverage))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = []\ncorpus = df[\"text_clean\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert sentences to sequences using keras preprocessing\nUse Keras text preprocessing to first tokenize the words (map each unique word to index ref: Tokenizer) and then convert each sentence to sequence using these indexes(texts_to_sequences). \nWe will also pad and tuncate to make sure all sentences are 50 in length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for each word in our corpus prepare embedding matrix, which maps word to embedding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline model with glove results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the processed corpus back using train set length to train and test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now lets split train set between train and validation set ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now lets fit the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=32,epochs=10,validation_data=(X_test,y_test),verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets predict","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred_GloVe = model.predict(train)\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare submission ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_GloVe = model.predict(test)\ntest_pred_GloVe_int = test_pred_GloVe.round().astype('int')\n\nsubmission['target'] = test_pred_GloVe_int\nsubmission.head(10)\n\nsubmission.to_csv(\"submission.csv\", index=False, header=True)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}