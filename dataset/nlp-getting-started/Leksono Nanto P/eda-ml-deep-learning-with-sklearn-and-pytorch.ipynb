{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport html\nimport ftfy\n\nimport nltk\nfrom nltk import word_tokenize \nfrom nltk.util import ngrams\nfrom collections import defaultdict\nfrom nltk.corpus import stopwords\nimport string, re\nfrom scipy.stats import norm\n\nfrom tqdm.notebook import tqdm\n\nimport wandb\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.concat([train, test], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_values = train.target.value_counts().values\npercent_y_values = [(y_values[0] / sum(y_values)),  (y_values[1] / sum(y_values))]\nsns.barplot(x=['Not Disaster', 'Real Disaster'], y=percent_y_values, \n            palette=\"coolwarm\").set_title('Distribution Target in train data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def null_value_counts(df, col, null=True):\n    if null:\n        return len(df[df[col].isnull()][col].to_list())\n    else:\n        return len(df[~df[col].isnull()][col].to_list())\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_null_values = [null_value_counts(all_data, 'location', null=True), null_value_counts(all_data, 'location', null=False)]\npercent_null_values = [(count_null_values[0] / sum(count_null_values)), (count_null_values[1] / sum(count_null_values))]\nsns.barplot(x=['NaN', 'Not NaN'], y=percent_null_values, \n            palette=\"coolwarm\").set_title('Distribution Null and Not Null Values in Location')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_null_values = [null_value_counts(all_data, 'keyword', null=True), null_value_counts(all_data, 'keyword', null=False)]\npercent_null_values = [(count_null_values[0] / sum(count_null_values)), (count_null_values[1] / sum(count_null_values))]\nsns.barplot(x=['NaN', 'Not NaN'], y=percent_null_values, \n            palette=\"coolwarm\").set_title('Distribution Null and Not Null Values in Keyword')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's check Keyword Null Values more details in number"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('null_values in keyword', count_null_values[0])\nprint('percent_null_values in keyword', percent_null_values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=all_data[all_data.target == 1].location.value_counts()[:15].index, \n            x=all_data[all_data.target == 1].location.value_counts()[:15].values,\n            palette=\"coolwarm\").set_title('Top 15 Locations in Real Disaster Tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=all_data[all_data.target == 0].location.value_counts()[:15].index, \n            x=all_data[all_data.target == 0].location.value_counts()[:15].values,\n            palette=\"coolwarm\").set_title('Top 15 Locations in Not Disaster Tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=all_data[all_data.target == 1].keyword.value_counts()[:15].index, \n            x=all_data[all_data.target == 1].keyword.value_counts()[:15].values,\n           palette='coolwarm').set_title('Top 15 Keywords in Real Disaster Tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=all_data[all_data.target == 0].keyword.value_counts()[:15].index, \n            x=all_data[all_data.target == 0].keyword.value_counts()[:15].values, \n            palette='coolwarm').set_title('Top 15 Keywords in Not Disaster Tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_w_tweet_disaster = train[train.target == 1]['text'].apply(lambda x: len(x.split()))\nlen_w_tweet_not_disaster = train[train.target == 0]['text'].apply(lambda x: len(x.split()))\n\nfig,ax = plt.subplots(2, figsize=(10,25))\n\nax1,ax2 = ax.flatten()\nsns.distplot(len_w_tweet_disaster,color='red', fit=norm, ax=ax1, \n             axlabel='n_word').set_title('Tweet Word Length Distribution from Disaster Tweets', fontsize=15)\nsns.distplot(len_w_tweet_not_disaster,color='blue', fit=norm, ax=ax2, \n             axlabel='n_word').set_title('Tweet Word Length Distribution from Not Disaster Tweets', fontsize=15)\n\nplt.subplots_adjust(left=0.2, right=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist = sns.distplot(len_w_tweet_disaster,color='red', fit=norm)\nplot_not_dist = sns.distplot(len_w_tweet_not_disaster,color='blue', fit=norm, axlabel='n_word').set_title(\n    'Combine of Tweet Word Length Distribution from Disaster Tweets and Not Disaster Tweets', fontsize=15)\n\nplt.subplots_adjust(left=0.2, right=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_c_tweet_disaster = train[train.target == 1]['text'].apply(lambda x: len(x))\nlen_c_tweet_not_disaster = train[train.target == 0]['text'].apply(lambda x: len(x))\n\nfig,ax = plt.subplots(2, figsize=(10,25))\n\nax1,ax2 = ax.flatten()\nsns.distplot(len_c_tweet_disaster,color='red', fit=norm, ax=ax1, \n             axlabel='n_character').set_title('Tweet Character Length Distribution from Disaster Tweets', fontsize=15)\nsns.distplot(len_c_tweet_not_disaster,color='blue', fit=norm, ax=ax2, \n             axlabel='n_character').set_title('Tweet Character Length Distribution from Not Disaster Tweets', fontsize=15)\n\nplt.subplots_adjust(left=0.7, right=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dist = sns.distplot(len_c_tweet_disaster,color='red', fit=norm)\nplot_not_dist = sns.distplot(len_c_tweet_not_disaster,color='blue', fit=norm, axlabel='n_character').set_title(\n    'Combine of Tweet Character Length Distribution from Disaster Tweets and Not Disaster Tweets', fontsize=15)\n\nplt.subplots_adjust(left=0.7, right=3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def punctuations_checker(texts, get_top_15=True):\n    punctuations = ['\\\\' + p for p in list(string.punctuation) if p not in list(',.?\"\\'@#')]\n    dirty_words = defaultdict()\n    for text in texts:\n        for word in text.split():\n            if all([ (len(re.findall(r'|'.join(punctuations), word)) > 1), (len(word) > 1) ]):\n                try:\n                    dirty_words[word] += 1\n                except:\n                    dirty_words[word] = 1\n    if get_top_15:\n        dirty_words = {k: v for k, v in sorted(dirty_words.items(), key=lambda item: item[1], reverse=True)[:15]}\n    else:\n        dirty_words = {k: v for k, v in sorted(dirty_words.items(), key=lambda item: item[1], reverse=True)}\n    return dirty_words\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dirty_words_real_disaster = punctuations_checker(all_data.text, get_top_15=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=list(dirty_words_real_disaster.keys()), x=list(dirty_words_real_disaster.values()),\n           palette='coolwarm').set_title('Top 15 Words/Tokens Contain Punctuation Mark')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print( all_data[all_data.text.str.contains(';')].text.to_list()[:30] ) # show potentially dirty text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(texts):\n    clean_text = []\n    for text in texts:\n        text = html.unescape(text)\n        text = ftfy.fix_text(text)\n        text = re.sub(r'(?:http(?:s)?://)?\\bt\\b\\.\\bco\\b(?:/[\\w/-]+)?', ' ', text) # remove urls t.co (http://t.co/asd https://t.co/asd t.co/asd) show regex highlight : https://regex101.com/r/rGAI2w/1\n        text = re.sub(r'&gt;', '>', text)\n        text = re.sub(r'&lt;', '<', text)\n        text = re.sub(r'&amp;', '&', text)\n        text = re.sub('\\\\\\n', '\\n', text) # not raw string\n        text = re.sub(r'\\x89Û', '', text) # unicode\n        text = re.sub(r'‰|_|Ï|Ò|ª|÷|å|©|£|À|Ì|Û|Ê', ' ', text) #special character\n        text = text.replace('...', '')\n        text = text.replace('%20', ' ')\n        for p in list(string.punctuation):\n            text = text.replace(p, f' {p} ') # replace in python more faster than regex re.sub\n        text = re.sub('\\s+', ' ', text).strip().lower() # remove double or more whitespace, remove in first and/or last whitespace, and transform to lowercase\n        clean_text.append(text)\n    \n    return clean_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['text'] = preprocess(all_data.text)\n# print(preprocess(all_data.text.to_list()[:10]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def words_generator(texts, n_grams=2, get_top_50=True):\n    gram2idx = defaultdict()\n    for line in texts:\n    #     token = nltk.word_tokenize(line)\n        word_list = line.split(' ')\n        filtered_words = [word.strip() for word in word_list if word not in stopwords.words('english')]\n        grams = list(ngrams(filtered_words, n_grams)) \n#         if n_grams > 1:\n        # merge list of tuple\n        grams_merge = []\n        for g in grams:\n            g = ' '.join(map(str,g))\n            grams_merge.append(g)\n        grams = grams_merge\n        for g in grams:\n            if len(g.strip()) > 3 and any([char_g.isalpha() for char_g in set(g.strip())]):\n                try:\n                    gram2idx[g] += 1\n                except:\n                    gram2idx[g] = 1\n                \n    if get_top_50:\n#         print(len(sorted(gram2idx.items(), key=lambda item: item[1])))\n        gram2idx = {k: v for k, v in sorted(gram2idx.items(), key=lambda item: item[1], reverse=True)[:50]}\n\n    else:\n        gram2idx = {k: v for k, v in sorted(gram2idx.items(), key=lambda item: item[1], reverse=True)}\n\n    return gram2idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"realdis_trigram_count = words_generator(all_data[all_data.target == 1].text[::], n_grams=3, get_top_50=True)\nnotdis_trigram_count = words_generator(all_data[all_data.target == 0].text[::], n_grams=3, get_top_50=True)\nrealdis_bigram_count = words_generator(all_data[all_data.target == 1].text[::], n_grams=2, get_top_50=True)\nnotdis_bigram_count = words_generator(all_data[all_data.target == 0].text[::], n_grams=2, get_top_50=True)\nrealdis_words_count = words_generator(all_data[all_data.target == 1].text[::], n_grams=1, get_top_50=True)\nnotdis_words_count = words_generator(all_data[all_data.target == 0].text[::], n_grams=1, get_top_50=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(7, 25), dpi=300)\n\nsns.barplot(y=list(realdis_trigram_count.keys()), x=list(realdis_trigram_count.values()), \n            palette='coolwarm', ax=axes[0]).set_title('Top 50 Trigrams in Real Disaster Tweets')\n\nsns.barplot(y=list(notdis_trigram_count.keys()), x=list(notdis_trigram_count.values()), \n            palette='coolwarm', ax=axes[1]).set_title('Top 50 Trigrams in Not Disaster Tweets')\n\nplt.subplots_adjust(left=0.2, right=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(7, 25), dpi=300)\n\nsns.barplot(y=list(realdis_bigram_count.keys()), x=list(realdis_bigram_count.values()), \n            palette='coolwarm', ax=axes[0]).set_title('Top 50 Bigrams in Real Disaster Tweets')\n\nsns.barplot(y=list(notdis_bigram_count.keys()), x=list(notdis_bigram_count.values()), \n            palette='coolwarm', ax=axes[1]).set_title('Top 50 Bigrams in Not Disaster Tweets')\n\nplt.subplots_adjust(left=0.2, right=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(7, 25), dpi=300)\n\nsns.barplot(y=list(realdis_words_count.keys()), x=list(realdis_words_count.values()), \n            palette='coolwarm', ax=axes[0]).set_title('Top 50 Words Count in Real Disaster Tweets')\n\nsns.barplot(y=list(notdis_words_count.keys()), x=list(notdis_words_count.values()), \n            palette='coolwarm', ax=axes[1]).set_title('Top 50 Words Count in Not Disaster Tweets')\n\nplt.subplots_adjust(left=0.2, right=2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apparently from the bigrams chart and words chart, we can find out mostly in real disaster tweets are about bombs, fire, and suicide, instead about youtube videos or other things in not disaster tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and generate a word cloud image:\ndef show_word_cloud(text):\n    wordcloud = WordCloud().generate(text)\n\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_real_disaster = ' '.join(all_data[all_data.target == 1].text.to_list())\nwords_not_disaster = ' '.join(all_data[all_data.target == 0].text.to_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_word_cloud(words_real_disaster)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_word_cloud(words_not_disaster)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build and Train Model"},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n# from sklearn.metrics import f1_score\n# from sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split with stratify\ntrain['clean_text'] = preprocess(train.text)\ntrain = train.drop_duplicates(subset='clean_text', keep='first')\nprint('length train data', len(train))\nprint('split 80:20')\nX = train.clean_text\ny = train.target\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n# summarize train and test composition\ntrain_0, train_1 = len(y_train[y_train==0]), len(y_train[y_train==1])\ntest_0, test_1 = len(y_val[y_val==0]), len(y_val[y_val==1])\nprint('Train: 0=%d, 1=%d, Test: 0=%d, 1=%d' % (train_0, train_1, test_0, test_1))\ncat_0 = train_0 + test_0\ncat_1 = train_1 + test_1\nprint('Train: 0=%f, 1=%f '% (train_0/cat_0, train_1/cat_1))\nprint('Test: 0=%f, 1=%f'% (test_0/cat_0, test_1/cat_1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression, RidgeClassifier\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import FunctionTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_sgd = SGDClassifier()\nmodel_lsvm = LinearSVC() # linear support vector classifier\nmodel_rf = RandomForestClassifier()\nmodel_lr = LogisticRegression()\nmodel_r = RidgeClassifier()\n\neclf = VotingClassifier(estimators=[('lr', model_lr), ('sgd', model_sgd), ('lsvm', model_lsvm)], voting='hard')\n\nfor model, label in zip([model_sgd, model_lsvm, model_rf, model_r, model_lr, eclf], \n                        ['SGD', 'LinearSVC', 'RandomForest', 'RidgeClassifier', 'LogisticRegression', 'VotingEnsemble']):\n    clf = Pipeline([\n            ('vect', CountVectorizer(ngram_range=(1,1))),\n            ('tfidf', TfidfTransformer()),\n            ('to_dense', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True)), \n            ('model', model)\n            ])\n    clf.fit(X_train,y_train)\n    print(\"Acc: %f using\" % clf.score(X_val, y_val), f'[{label}]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_acc = clf.score(X_train, y_train)\nval_acc = clf.score(X_val, y_val)\nprint('train_acc:', train_acc)\nprint('val_acc:', val_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# download glove vector\n!wget -O \"/kaggle/working/glove.twitter.27B.zip\" \"http://nlp.stanford.edu/data/glove.twitter.27B.zip\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!unzip -j \"/kaggle/working/glove.twitter.27B.zip\" \"glove.twitter.27B.200d.txt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(360)\nnp.random.seed(360)\ntorch.manual_seed(360)\ntorch.backends.cudnn.deterministic = True\ntorch.cuda.manual_seed_all(360)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx = {}\nidx2word = {}\n\nall_text = preprocess(train.text) + preprocess(test.text)\nidx = 1 # index start from one because zero for padding\nfor text in all_text:\n    words = nltk.word_tokenize(text)\n    for word in words:\n        if word not in word2idx.keys():\n            word2idx[word] = idx\n            if idx not in idx2word.keys():\n                idx2word[idx] = word\n                idx += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(word2idx), len(idx2word))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load glove\nGLOVE_DIR = '/kaggle/working/'\nembeddings_index = {}\nf = open(os.path.join(GLOVE_DIR, 'glove.twitter.27B.200d.txt'))\nfor line in tqdm(f, desc='load glove embedding'):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n    \nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build embedding matrix\nembedding_matrix = np.zeros((len(word2idx) + 2, 200)) # +2 for padding and unknown words\nfor word, idx in tqdm(word2idx.items(), desc='build embedding matrix'):\n    try:\n        embedding_matrix[idx] = embeddings_index[word]\n    except:\n        embedding_matrix[-1] = np.random.rand(1, 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_data_loader(x, y, maxlen=128, with_target=True, bs=16):\n    all_data = []\n\n    for text, label in zip(x, y): \n        tokens = word_tokenize(text)\n        token_ids = [word2idx[t] for t in tokens]\n        # padding or truncate to maxlen\n        if len(token_ids) > maxlen:\n            token_ids = token_ids[:maxlen]\n        else:\n            token_ids = token_ids + ([0] * (maxlen - len(token_ids)))\n        \n        token_ids_tensor = torch.LongTensor(token_ids)\n        \n        target = torch.tensor(label)\n        data = {\n            'token_ids': token_ids_tensor,\n            'target' : target\n        }\n            \n        all_data.append(data)\n    \n    data_loader = DataLoader(all_data, batch_size=bs, num_workers=5)\n    \n    return data_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataloader = generate_data_loader(X_train, y_train)\nval_dataloader = generate_data_loader(X_val, y_val)\n# all_dataloader = generate_data_loader(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_n_correct(y_true, y_pred):\n    \"\"\" \n    y_true : tensor,\n    y_pred : tensor,\n    example: \n    y_true = torch.tensor([0.0, 1.0, 1.0])\n    y_pred = torch.tensor([0.3, 0.6, 0.9])\n    torch.eq is means equal of each tensor and return True or False, \n    and then the tensor of booleans turn to floats in 1.0 or 0.0\n    torch.eq(y_true, torch.round(y_pred)).float() = torch.tensor([1.0, 1.0, 1.0])\n    \"\"\"\n    y_pred = torch.sigmoid(y_pred) # apply logits y_pred to sigmoid activation function \n    \n    with torch.no_grad():\n        n_correct = torch.sum(torch.eq(y_true, torch.round(y_pred)).float()).item()\n    return n_correct\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(data_loader, criterion, model):\n    n_val_total, n_val_correct, val_loss_total = 0, 0, 0\n    model.eval()\n    with torch.no_grad():\n        for sample_batched in data_loader:\n\n            inputs = sample_batched['token_ids'].to(device)\n            outputs = model(inputs)\n\n            targets = sample_batched['target'].to(device).float().unsqueeze(1)\n\n            loss = criterion(outputs, targets) #y_pred, y\n\n            n_val_correct += get_n_correct(targets, outputs)\n            n_val_total += len(outputs)\n            val_loss_total += loss.item() * len(outputs)\n    \n    val_acc = n_val_correct / n_val_total\n    val_loss = val_loss_total / n_val_total\n    return val_acc, val_loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nif device == 'cuda':\n    print(torch.cuda.get_device_name())\n    print(device)\nelse:\n    print(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### if you'r not familiar using packed sequence, you can check this clear explanation about pack_padded_sequence and pad_packed_sequence : https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMModel(nn.Module):\n    def __init__(self, embedding_matrix, output_size=1, embedding_dim=200, hidden_dim=300, hidden_dim_2=100, \n                 n_layers=1, drop_prob=0.5, bidirectional=False, pooling=False):\n        super().__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectional)\n        self.dropout = nn.Dropout(drop_prob)\n        self.pooling = pooling\n        self.bidirectional = bidirectional\n        self.fc_b_pool = nn.Linear(hidden_dim*4, hidden_dim*2)\n        if self.bidirectional or self.pooling:\n            self.fc = nn.Linear(hidden_dim*2, hidden_dim)\n            self.fc_out = nn.Linear(hidden_dim, output_size)\n        else:\n            self.fc = nn.Linear(hidden_dim, hidden_dim_2)\n            self.fc_out = nn.Linear(hidden_dim_2, output_size)\n    \n    def forward(self, x):\n        embeds = self.embedding(x)\n        len_seq = torch.as_tensor((x != 0).sum(dim=1), dtype=torch.int64) # sum of token ids without padding ids (0s)\n\n        packed_seq = pack_padded_sequence(embeds, len_seq, batch_first=True, enforce_sorted=False)\n\n        out_packed_lstm, (h_lstm, c_lstm) = self.lstm(packed_seq)\n        \n        if self.pooling:\n            out_lstm, _ = pad_packed_sequence(out_packed_lstm, batch_first=True)\n            mean_out = torch.mean(out_lstm, dim=1) # lstm: batch_size, 1, hidden | bilstm: batch_size, 2, hidden\n            max_out, _ = torch.max(out_lstm, dim=1) # lstm: batch_size, 1, hidden | bilstm: batch_size, 2, hidden\n            h_lstm = torch.cat([mean_out, max_out], dim=1) # lstm: batch_size, 2, hidden | bilstm: batch_size, 4, hidden\n            h_lstm = h_lstm.view(h_lstm.shape[0], -1) # batch_size, seq_len, hidden > batch_size, hidden\n            if self.bidirectional:\n                h_lstm = self.fc_b_pool(h_lstm)\n        else:\n            h_lstm = h_lstm.transpose(0,1) # seq_len, batch_size, hidden > batch_size, seq_len, hidden (lstm: bs, 1, hidden | bilstm or pool: bs, 2, hidden)\n            h_lstm = h_lstm.contiguous().view(h_lstm.shape[0], -1) # batch_size, seq_len, hidden > batch_size, hidden\n        \n        out = self.dropout(self.fc(h_lstm))\n        out = self.fc_out(out)\n        return out\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\ndef train_model(model, train_dataloader, val_dataloader, learning_rate=1e-3, epochs=50, early_stopping=10, model_name='model_disaster_lstm.pt'):\n    # don't worry with higher epochs, we set early stopping\n#     wandb.init(project=\"disaster-tweet-classification\", name=model_name)\n#     wandb.config.lr = learning_rate\n\n    PATH_OUTPUT_MODEL = '/kaggle/working'\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n    criterion = nn.BCEWithLogitsLoss()\n    min_val_loss = None\n    patience = 0\n    list_lr, list_acc, list_loss, list_val_acc, list_val_loss = [], [], [], [], []\n    model = model.to(device)\n    for i in tqdm(range(epochs), desc='epochs'):\n        n_total, n_correct, loss_total = 0, 0, 0\n\n        model.train()\n\n        for sample_batched in tqdm(train_dataloader, desc='batch loader'):\n            optimizer.zero_grad()\n\n            inputs = sample_batched['token_ids'].to(device)\n            outputs = model(inputs)\n\n            targets = sample_batched['target'].to(device).float().unsqueeze(1)\n\n            loss = criterion(outputs, targets) #y_pred, y_true. y_pred already applied sigmoid by loss function\n\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                n_correct += get_n_correct(targets, outputs)\n                n_total += len(outputs)\n                loss_total += loss.item() * len(outputs)\n\n        train_acc = n_correct / n_total\n        train_loss = loss_total / n_total\n\n        learning_rate = optimizer.param_groups[0]['lr']\n        print(f'Epoch {i} :')\n        print('LR:', learning_rate)\n        print('Acc:', train_acc)\n        print('Loss:', train_loss)\n\n        val_acc, val_loss = evaluate(val_dataloader, criterion, model)\n\n        if min_val_loss == None:\n            min_val_loss = val_loss\n\n        if min_val_loss > val_loss:\n            min_val_loss = val_loss\n            print('Found Best Val Loss...')\n            torch.save(model.state_dict(), os.path.join(PATH_OUTPUT_MODEL, model_name))\n            patience = 0 # reset patience\n        else:\n            patience += 1\n            scheduler.step()\n\n\n        print('Val Acc:', val_acc)\n        print('Val Loss:', val_loss)\n\n#         wandb.log({\"lr\":learning_rate, \"loss\": train_loss, \"val_loss\": val_loss, \n#                    \"acc\":train_acc, \"val_acc\":val_acc})\n        \n        list_lr.append(learning_rate)\n        list_loss.append(train_loss)\n        list_val_loss.append(val_loss)\n        list_acc.append(train_acc)\n        list_val_acc.append(val_acc)\n        \n        if patience == early_stopping:\n            print(f'Validation Loss not decreasing {early_stopping} times in a row..')\n            print('Early Stopping..')\n            print('Best Val Loss:', min_val_loss)\n            break   \n    \n    return min_val_loss, (list_lr, list_acc, list_loss, list_val_acc, list_val_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pd.DataFrame({'train_acc':[0.5, 0.6, 0.9, 0.2, 0.4], 'loss':[], 'lr':[], 'metrics':['a','a','a','b','b'], 'epochs':[1,2,3,1,2], })\nclass MetricsBuilder(object):\n    def __init__(self):\n        self.df_metrics = pd.DataFrame({})\n        \n    def add_metrics(self, tuple_metrics, name='LSTM'):\n        list_lr, list_acc, list_loss, list_val_acc, list_val_loss = tuple_metrics\n        metrics_data = pd.DataFrame({'acc':list_acc, 'val_acc':list_val_acc, 'loss':list_loss, 'val_loss':list_val_loss, 'lr':list_lr,\n                                     'metrics': [name for i in range(len(list_lr))], 'epochs':[i for i in range(len(list_lr))]})\n        self.df_metrics = pd.concat([self.df_metrics, metrics_data])\n    \n    def __get_df_metrics__(self):\n        return self.df_metrics\n\nmetrics_builder = MetricsBuilder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"lstm_model = LSTMModel(embedding_matrix, bidirectional=False)\nmin_val_loss_lstm, metrics_lstm = train_model(lstm_model, train_dataloader, val_dataloader, model_name='lstm.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"lstm_model_pool = LSTMModel(embedding_matrix, bidirectional=False, pooling=True)\nmin_val_loss_lstm_pool, metrics_lstm_pool = train_model(lstm_model_pool, train_dataloader, val_dataloader, model_name='lstm_pool.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"bilstm_model = LSTMModel(embedding_matrix, bidirectional=True)\nmin_val_loss_bilstm, metrics_bilstm = train_model(bilstm_model, train_dataloader, val_dataloader, model_name='bilstm.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"bilstm_model_pool = LSTMModel(embedding_matrix, bidirectional=True, pooling=True)\nmin_val_loss_bilstm_pool, metrics_bilstm_pool = train_model(bilstm_model_pool, train_dataloader, val_dataloader, model_name='bilstm_pool.pt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('min_val_loss_bilstm',min_val_loss_bilstm)\nprint('min_val_loss_bilstm_pool',min_val_loss_bilstm_pool)\nprint('min_val_loss_lstm',min_val_loss_lstm)\nprint('min_val_loss_lstm_pool',min_val_loss_lstm_pool)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics_builder.add_metrics(metrics_lstm, name='LSTM')\nmetrics_builder.add_metrics(metrics_lstm_pool, name='LSTM_Pool')\nmetrics_builder.add_metrics(metrics_bilstm, name='BiLSTM')\nmetrics_builder.add_metrics(metrics_bilstm_pool, name='BiSTM_Pool')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_metrics = metrics_builder.__get_df_metrics__()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(5, figsize=(15,35))\nax1, ax2, ax3, ax4, ax5 = ax\nsns.lineplot(x='epochs', y='acc', data=df_metrics, hue=\"metrics\", style='metrics',\n                  markers=True, ax=ax1).set_title('Training Accuracy', fontsize=15)\nsns.lineplot(x='epochs', y='val_acc', data=df_metrics, hue=\"metrics\", style='metrics',\n                  markers=True, ax=ax2).set_title('Val Accuracy', fontsize=15)\nsns.lineplot(x='epochs', y='loss', data=df_metrics, hue=\"metrics\", style='metrics',\n                  markers=True, ax=ax3).set_title('Training Loss', fontsize=15)\nsns.lineplot(x='epochs', y='val_loss', data=df_metrics, hue=\"metrics\", style='metrics',\n                  markers=True, ax=ax4).set_title('Val Loss', fontsize=15)\nsns.lineplot(x='epochs', y='lr', data=df_metrics, hue=\"metrics\", style='metrics',\n                  markers=True, ax=ax5).set_title('Learning Rate', fontsize=15)\n\nplt.subplots_adjust(left=3, right=4)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load best model"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"best_bilstm_pool = LSTMModel(embedding_matrix, bidirectional=True, pooling=True).to(device)\nbest_bilstm_pool.load_state_dict(torch.load('/kaggle/working/bilstm_pool.pt', map_location=device))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"def predict_test_set(data_loader, model):\n    model.eval()\n    all_outputs = []\n    with torch.no_grad():\n        for sample_batched in data_loader:\n\n            inputs = sample_batched.to(device)\n            outputs = model(inputs)\n            outputs = (torch.sigmoid(outputs).squeeze(1) > 0.5).long().tolist()\n\n            all_outputs.extend(outputs)\n    \n    return all_outputs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_test_data_loader(x, maxlen=128, bs=16):\n    all_data = []\n\n    for text in x: \n        tokens = word_tokenize(text)\n        token_ids = [word2idx[t] for t in tokens]\n        # padding or truncate to maxlen\n        if len(token_ids) > maxlen:\n            token_ids = token_ids[:maxlen]\n        else:\n            token_ids = token_ids + ([0] * (maxlen - len(token_ids)))\n        \n        token_ids_tensor = torch.LongTensor(token_ids)           \n        all_data.append(token_ids_tensor)\n    \n    data_loader = DataLoader(all_data, batch_size=bs, num_workers=5)\n    \n    return data_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataloader = generate_test_data_loader(preprocess(test['text']))\npredicted_target = predict_test_set(test_dataloader, best_bilstm_pool)\n# predicted_target = blending_test_set(test_dataloader, lstm_model_pool, lstm_model, bilstm_model_pool)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the model to make predictions\n\nmy_submission = pd.DataFrame({'Id': test.id, 'target': predicted_target})\nmy_submission.to_csv('submission.csv', index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(predicted_target[:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}