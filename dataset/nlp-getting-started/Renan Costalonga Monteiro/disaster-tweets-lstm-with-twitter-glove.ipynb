{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro\n\n"},{"metadata":{},"cell_type":"markdown","source":"In this competition we have a challenge to predict if a tweet is about a real desaster or not. \n\nThere are several approachs for solve this challenge on Kaggle. I made this kernel to test some insights and start my contribution \nat Kaggle comunnity.\n\nIf you like, please do a upvote! "},{"metadata":{},"cell_type":"markdown","source":"# What's in this kernel?\n\n- Data Cleaning\n- A simple LSTM model using keras and [GloVe](https://nlp.stanford.edu/projects/glove/) pre trained embeddings "},{"metadata":{},"cell_type":"markdown","source":" # Acknowledgements\n \n This kernel was based in multiple great kernels that i found at this competition,\n \n Thank you all.\n \n [Cheatsheet - Text Helper Functions](https://www.kaggle.com/raenish/cheatsheet-text-helper-functions)\n \n [NLP with Disaster Tweets - EDA, Cleaning and BERT](https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n \n [NLP (Disaster Tweets) with Glove and LSTM](https://www.kaggle.com/mariapushkareva/nlp-disaster-tweets-with-glove-and-lstm)\n \n [Glove biLSTM](https://www.kaggle.com/xwalker/glove-bilstm)"},{"metadata":{},"cell_type":"markdown","source":"# Importing libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport random as rnd\nimport re\nimport string\nimport operator\nimport numpy as np\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer, word_tokenize\n\n\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.layers import (LSTM, \n                          Embedding, \n                          BatchNormalization,\n                          SpatialDropout1D,\n                          Dense, \n                          Dropout)\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.optimizers import Adam\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    precision_score, \n    recall_score, \n    f1_score, \n    classification_report,\n    accuracy_score)\n\nimport tensorflow as tf\nfrom clr_callback import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensuring reproducibility\n\n`42 is the answer to everything`"},{"metadata":{"trusted":true},"cell_type":"code","source":"rnd.seed(42) \nnp.random.seed(42)\ntf.random.set_seed(42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\ndf.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"## Editing Mislabeled Samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.loc[df['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target'] = 0\ndf.loc[df['text'] == 'Hellfire is surrounded by desires so be careful and donÛªt let your desires control you! #Afterlife', 'target'] = 0\ndf.loc[df['text'] == 'To fight bioterrorism sir.', 'target'] = 0\ndf.loc[df['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4', 'target'] = 1\ndf.loc[df['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring', 'target'] = 1\ndf.loc[df['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target'] = 0\ndf.loc[df['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target'] = 0\ndf.loc[df['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target'] = 1\ndf.loc[df['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG', 'target'] = 1\ndf.loc[df['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target'] = 0\ndf.loc[df['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target'] = 0\ndf.loc[df['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target'] = 0\ndf.loc[df['text'] == \"Hellfire! We donÛªt even want to think about it or mention it so letÛªs not do anything that leads to it #islam!\", 'target'] = 0\ndf.loc[df['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target'] = 0\ndf.loc[df['text'] == \"Caution: breathing may be hazardous to your health.\", 'target'] = 1\ndf.loc[df['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target'] = 0\ndf.loc[df['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target'] = 0\ndf.loc[df['text'] == \"that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\", 'target'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tweet Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(tweet):             \n    # Special characters\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n            \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Acronyms\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)  \n    tweet = re.sub(r\"alwx\", \"Alabama Weather\", tweet)\n    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n    \n    # Grouping same words without embeddings\n    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n    \n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # remove HTML tags\n    tweet = re.sub(r'<.*?>', '', tweet)\n    # remove words with digits (and emojis)\n    tweet = re.sub('\\w*\\d\\w*', '', tweet)\n    # remove punctuations\n    table = str.maketrans('', '', string.punctuation)\n    tweet =  tweet.translate(table)\n    \n    return tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndf['text'] = df['text'].apply(clean)\ndf_test['text'] = df_test['text'].apply(clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np.array(df['target']).tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating embedding dict from glove"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembedding_dim = 100\nembedding_dict = {}\n\nwith open(f'../input/glove-global-vectors-for-word-representation/glove.twitter.27B.{embedding_dim}d.txt','r') as f:\n    \n    for line in f:\n        \n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:],'float32')\n        embedding_dict[word] = vectors\n        \nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"There are {len(embedding_dict.keys())} tokens in this embedding representation\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building vocab"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef tokenize_corpus(df,  stopwords):\n    \n    corpus = []\n    \n    for tweet in df['text']:\n        words = [word.lower() for word in word_tokenize(tweet) if ((word.isalpha() == 1) & (word not in stopwords))]\n        corpus.append(words)\n        \n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corpus = tokenize_corpus(df, stopwords)\nlen(train_corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_corpus = tokenize_corpus(df_test, stopwords)\nlen(test_corpus)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking embedding coverage"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vocab(tweets):\n       \n    vocab = {}\n    \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\ndef check_embeddings_coverage(X, embeddings):\n    \n    vocab = build_vocab(X)    \n    \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            oov[word] = vocab[word]\n            n_oov += vocab[word]\n            \n    vocab_coverage = len(covered) / len(vocab)\n    text_coverage = (n_covered / (n_covered + n_oov))\n    \n    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n    return sorted_oov, vocab_coverage, text_coverage","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_corpus, embedding_dict)\ntest_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_corpus, embedding_dict)\n\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create embedding matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(train_corpus)\nsequences = tokenizer_obj.texts_to_sequences(train_corpus)\n\ntweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer_obj.texts_to_sequences(test_corpus)\ntest_pads = pad_sequences(test_sequences, maxlen=MAX_LEN, truncating='post', padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer_obj.word_index\nprint('Number of unique tokens:',len(word_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating embedding matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = len(word_index) + 1 # +1 refer to pad token\nembedding_matrix = np.zeros((num_words, embedding_dim))\n\nfor word,i in word_index.items():\n    emb_vec = embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking if we have embeddings for all tokens"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(embedding_matrix), num_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tweet_pad), len(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, X_val, y, y_val = train_test_split(tweet_pad, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\nprint('Length of train:', len(X))\nprint(\"Length of val:\",   len(X_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.asarray(X)\ny = np.asarray(y)\n\nX_val = np.asarray(X_val)\ny_val = np.asarray(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(y_val, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\n\nembedding=Embedding(num_words,\n                    embedding_dim,\n                    embeddings_initializer=Constant(embedding_matrix),\n                    input_length=MAX_LEN,\n                    trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimzer, metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training "},{"metadata":{"trusted":true},"cell_type":"code","source":"clr = CyclicLR(mode      = \"triangular\",\n               base_lr   = 1e-5,\n               max_lr    = 1e-4,\n               step_size = 25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping_callback = EarlyStopping(monitor='loss', mode='min', patience=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_checkpoint_callback = ModelCheckpoint(\n    filepath='best_model.h5',\n    save_weights_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X, y, \n                    batch_size=64, \n                    epochs=100, \n                    validation_data=(X_val, y_val), \n                    callbacks=[clr, early_stopping_callback, model_checkpoint_callback],\n                    shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model.load_weights('best_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def metrics(pred_tag, y_test):\n    print(\"F1-score: \", f1_score(pred_tag, y_test))\n    print(\"Precision: \", precision_score(pred_tag, y_test))\n    print(\"Recall: \", recall_score(pred_tag, y_test))\n    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n    print(\"-\"*50)\n    print(classification_report(pred_tag, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_classes(X_val)\n\nmetrics(preds, y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nperfect_sub = pd.read_csv('../input/perfect-submissiondisastertweetsnlp/perfect_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_classes(test_pads)\npreds = preds.astype(int).reshape(3263)\nlen(preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare with perfect submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics(preds, perfect_sub['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame({'id': sample_sub['id'].values.tolist(),'target': preds})\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}