{"cells":[{"metadata":{},"cell_type":"markdown","source":"## I made this Notebook just for learning Purpose and i have taken Idea from multiple sources, take what you want and if have any suggestion kindly put it down in comments. \nThis is my beginner code and I am happy to say that i have come a long way and looking forward to keep going. \nThis was something which gave me the boost to keep working.\n\nI hope it helps you the same!\nThanks :)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport spacy\nimport time\nfrom spacy.matcher import Matcher\nfrom spacy.matcher import PhraseMatcher\nfrom wordcloud import WordCloud\npd.set_option('display.max_rows',1000)\npd.set_option('display.max_colwidth',1000)\n\n\ntrain_set_orig= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_set_orig = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Training Data has {train_set_orig.shape[0]} rows and {train_set_orig.shape[1]} columns')\nprint(f'Testing Data has {test_set_orig.shape[0]} rows and {train_set_orig.shape[1]} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set_orig.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set_orig.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#number of positive and Negative tweets\nPN=train_set_orig['target'].value_counts()\nPN.plot(kind='bar',color=['red','green'])\nplt.xticks([0,1],['Negative','Real tweets'])\nprint(f'Training set has {PN[1]} Negative tweets and {PN[0]} Positive tweets')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparing length of real tweet and non real tweetss\nplt.style.use('fivethirtyeight')\ntrain_group=train_set_orig.groupby('target')\nreal_text_length= [len(text) for text in train_group.get_group(0)['text']]\nneg_text_length= [len(text) for text in train_group.get_group(1)['text']]\nbins=150\nplt.figure(figsize=(20,5))\nplt.hist(real_text_length,bins= bins,label='Positive',color='green',alpha= 0.7)\nplt.hist(neg_text_length,bins=bins,label='Negative',color='red',alpha=0.6)\nplt.xlabel('length of tweets')\nplt.ylabel('Number of tweets')\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nplt.title('Number of characters in real and Not Real tweets')\nax1.hist(real_text_length,color= 'green')\nax1.set_xlabel('Number of real tweets',color= 'green')\nax1.set_ylim([0,1000])\nax2.hist(neg_text_length,color='red')\nax2.set_xlabel('Number of not real tweets',color='red')\nax2.set_ylim([0,1000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be clearly scene that tweets size between 120 to 400 length are same in both type of tweets.\nBut are the words length same too ??","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg(x):\n    return np.sum([len(word) for word in x])\n\nneg_tweet_avg= [np.sum(list(map(avg,text)))/len(text.split()) for text in train_group.get_group(1)['text']]\npost_tweet_avg= [np.sum(list(map(avg,text)))/len(text.split()) for text in train_group.get_group(0)['text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)= plt.subplots(1,2,figsize=(15,5))\nsns.distplot(neg_tweet_avg,color='red',ax=ax1)\nax1.set_title('negative tweets')\nax1.set_xlabel('average tweet length')\nsns.distplot(post_tweet_avg,color='green',ax=ax2)\nax2.set_title('positive tweets')\nax2.set_xlabel('average tweet length')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp= spacy.load('en_core_web_lg')\nstopwords= nlp.Defaults.stop_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import time\n# tic=time.time()\n# corpus=[nlp(text) for text in train_group.get_group(1)['text']]\n# toc= time.time()\n# print(f'time taken by using list comprehension:{(toc-tic)*1000}ms')\n\n## time taken by using list comprehension:41624.28307533264ms\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tic= time.time()\npos_text= train_group.get_group(0)['text'].to_string().replace('\\n',' ')\nneg_text= train_group.get_group(1)['text'].to_string().replace('\\n',' ')\n\npos_corpus=nlp(pos_text)\nneg_corpus=nlp(neg_text)\n\ntoc=time.time()\n\nprint(f'string conversion takes:{(toc-tic)*100}ms')\n##string conversion takes:1640.8559560775757ms\n\nmatcher= PhraseMatcher(nlp.vocab)\npattern= [nlp(word) for word in stopwords]\nmatcher.add('stopwords',None,*pattern)\n\npos_matches= matcher(pos_corpus)\nneg_matches= matcher(neg_corpus)\n\npos_words_found= [str(pos_corpus[pos_matches[i][1]:pos_matches[i][2]]) for i in range( len(pos_matches))]\nneg_words_found= [str(neg_corpus[neg_matches[i][1]:neg_matches[i][2]]) for i in range( len(neg_matches))]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\npd.Series(pos_words_found).value_counts()[:40].plot(kind='bar',color='green',label='Repetitive words in postitive tweet')\npd.Series(neg_words_found).value_counts()[:40].plot(kind='bar',color='red',label='Repetitive words in negative tweet')\n\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In Both positive and Negative tweet **['The' , 'a' , 'to' , 'of' , 'you']** dominates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_special_char= train_group.get_group(1)['text'].str.findall('[^A-Za-z0-9\\s]')\nneg_special_char= train_group.get_group(0)['text'].str.findall('[^A-Za-z0-9\\s]')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig= plt.figure(figsize=(15,5))\n\nall_neg_char=[ch for char in neg_special_char for ch in char]\npd.Series(all_neg_char).value_counts()[:30].plot(kind='bar',label='characters used in negative tweets',color='red')\n\nall_pos_char=[ch for char in pos_special_char for ch in char]\npd.Series(all_pos_char).value_counts()[:30].plot(kind='bar',label='characters used in positive tweets',color='green',alpha=0.5)\n\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting more clearly using a DataFrame\nn=pd.Series(all_neg_char).value_counts()\np= pd.Series(all_pos_char).value_counts()\ndf=pd.DataFrame({'positive':p,'negative':n}).sort_values('positive',ascending=False)[:20]\ndf.plot(kind='bar',figsize=(15,5),color=['green','red'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## it will be interesting to see these figures after removing stop words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"a=pd.Series(pos_words_found).value_counts()\nb=pd.Series(neg_words_found).value_counts()\ndf= pd.DataFrame({'common_positive_words':a,'common_negative_words':b}).sort_values('common_positive_words',ascending=False)\ndf[:40].plot(kind='bar',figsize=(15,5),color=['green','red'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## most common #HashTags in negative tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nmatcher= Matcher(nlp.vocab)\npattern= [{'TEXT':{'REGEX':'#'}},{'TEXT':{'REGEX':'\\w+'}}]\nmatcher.add('p1',None,pattern)\n\ndoc= pos_corpus\nmatches= matcher(doc)\nhash_tags=[]\naa=[hash_tags.append(re.sub(r'_|\\\\n','',str(doc[matches[i][1]:matches[i][2]]))) for i in range(len(matches))]\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(' '.join(hash_tags))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## most common #HashTags in negative tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"doc= neg_corpus\nmatches= matcher(doc)\nhash_tags=[]\naa=[hash_tags.append(re.sub(r'_|\\\\n','',str(doc[matches[i][1]:matches[i][2]]))) for i in range(len(matches))]\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='red',\n                          max_font_size = 80\n                         ).generate(' '.join(hash_tags))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\n\ntrain= train_set_orig.copy()\ntest= test_set_orig.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'train':train.isna().sum()/len(train)*100,'test':test.isna().sum()/len(test)*100}).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Location column are user generated so being that much dirty and lots of empty values, it can't be used as a feature","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].value_counts()[:50]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some values in keyword section can only be written in one context","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d= defaultdict(int)\n\nunique_keywords= train['keyword'].dropna().unique()\n\nkey_group= train[['keyword','target']].groupby('keyword')\nfor word in unique_keywords:\n    d[word]= [v for v in key_group.get_group(word)['target'].value_counts().values]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for word in unique_keywords:\n    z= key_group.get_group(word)['target'].value_counts()\n    print(f'{word} keyword has {z[0]} postive tweets and {z[1]} negative tweets')\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame(d.items())\ndf2= pd.DataFrame(df[1].to_list(),columns=['positive','negative'])\ndf2.index= df[0]\n\ndf2=df2.sort_values(by='positive',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndf2=df2.sort_values(by='positive',ascending=False)\nfig,ax= plt.subplots()\nax.scatter(x='positive',y='negative',data= df2,alpha=0.4)\n\n# ax.scatter(x='positive',y='negative',data= pos,color='green')\n# ax.scatter(x='positive',y='negative',data= neg,color='red')\n\npos_keyword=pd.DataFrame([df2.iloc[i] for i in range(len(df2)) if df2['negative'][i]<5.0 and df2['positive'][i]>25])\nax.scatter(x='positive',y='negative',data= pos_keyword,color='green')\n\n\nneg_keyword= pd.DataFrame([df2.iloc[i] for i in range(len(df2)) if df2['positive'][i]<25.0 and df2['negative'][i]>15.0])\nax.scatter(x='positive',y='negative',data= neg_keyword,color='red')\n\n# # ax.annotate('ablaze',(23,13))\n\nplt.xlabel('positive')\nplt.ylabel('negative')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## List of keyword which are highly positive\n> pos_keyword.index\n1. > ['body%20bags', 'outbreak', 'typhoon', 'oil%20spill', 'harm', 'ruin',\n       'wrecked', 'explode', 'panic', 'screaming', 'traumatised', 'blazing',\n       'blizzard', 'crush', 'evacuated', 'rescuers', 'suicide%20bomb',\n       'bloody', 'body%20bag', 'suicide%20bombing', 'panicking', 'smoke',\n       'nuclear%20disaster', 'collide', 'razed', 'electrocute', 'blew%20up',\n       'blight', 'suicide%20bomber', 'stretcher', 'screamed', 'drown',\n       'wildfire', 'wild%20fires', 'obliterate', 'crushed', 'mayhem',\n       'bombing', 'obliterated', 'avalanche'],\n      \n ## List of keyword which are highly Negative\n> neg_keyword.index\n 2. >['windstorm', 'collided', 'weapons', 'damage', 'burning%20buildings',\n       'police', 'hurricane', 'ambulance', 'explosion', 'bombed', 'hijacker',\n       'tornado', 'engulfed', 'hail', 'derail', 'rainstorm',\n       'natural%20disaster', 'rescued', 'storm', 'hijack', 'lightning']","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## As the name suggest we are removing URL's","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nexample=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"\ndef clean_url(text):\n    return re.sub(r':?http[s]?:\\S+','',text)\nclean_url(example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']= train['text'].apply(lambda x: clean_url(x))\ntest['text']= test['text'].apply(lambda x:clean_url(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Clean Text if their is any Html tags out in our text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"example = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"\n\ndef clean_tag(text):\n    return re.sub(r'<.*?>','',text)\nprint(clean_tag(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']= train['text'].apply(lambda x:clean_tag(x))\ntest['text']= test['text'].apply(lambda x:clean_tag(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove Symbols, and make every text in lower format but keep @ and # because they signify the @User and #Topic","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def non_alpha(text):\n    return re.sub('[^A-Za-z0-9\\'@#]+',' ',text).lower()\nprint(non_alpha(\"ÛÏ@LeoBlakeCarter: This #dog thinks he's an am.\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']= train['text'].apply(lambda x: non_alpha(x))\ntest['text']= test['text'].apply(lambda x:non_alpha(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing @User name","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"example='@Mohit1 @ Mohit2 @m @moo mohit5t01c @mohit5t01c hello Mohit @phdsquares'\ndef remove_user(text):\n    text= re.sub(r'(@[A-Za-z0-9]?[A-Za-z0-9]*)| \\s[A-Za-z0-9]+\\s','',text).split()\n    print(text)\n    text=[word for word in text if word.isalpha() == True and len(word)>2]\n    print(text)\n    text= ' '.join(text)\n    return text\nremove_user(example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']= train['text'].apply(lambda x:remove_user(x))\ntest['text']= test['text'].apply(lambda x:remove_user(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df,target):\n    group= df.groupby('target').get_group(target)\n    corpus=[]\n    nn=[corpus.append(word) for text in group['text'] for word in text.split()]\n    return corpus\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corpus= create_corpus(train,1)\nplt.figure(figsize=(12,8))\nwc= WordCloud().generate(' '.join(train_corpus))\nplt.imshow(wc)\nplt.title(\"Common words in Disaster's tweet\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corpus= create_corpus(train,0)\nplt.figure(figsize=(12,8))\nwc= WordCloud().generate(' '.join(train_corpus))\nplt.imshow(wc)\nplt.title(\"Common words in Fake Disaster's tweet\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(['keyword','id','location'],axis=1)\ntest= test.drop(['keyword','location'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom spacy.lang.en.stop_words import STOP_WORDS #-- set\ns1= stopwords.words('english') #-- list\nprint(len(STOP_WORDS))\n\naa=[STOP_WORDS.add(word) for word in s1 if word not in STOP_WORDS]\nprint(len(STOP_WORDS))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv= CountVectorizer(stop_words=STOP_WORDS)\ntrain= pd.DataFrame(cv.fit_transform(train['text']).todense())\ntest= pd.DataFrame(cv.transform(test['text']).todense())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X= train\ny= train_set_orig['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_x,val_x,test_y,val_y = train_test_split(X,y)\nprint(train_x.shape)\nprint(val_x.shape)\nprint(test_y.shape)\nprint(val_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(val_x))\nlen(val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef initialize_parameters(X):\n    w= np.zeros(X.shape[1])\n    b=0\n    return w,b\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\n    \ndef propogate(w,b,X,y):\n    m= len(X)\n    z= X.dot(w.T)\n    A= sigmoid(z)\n    cost=- 1/m * np.sum(y * np.log(A) + (1-y)* np.log(1-A))\n    dz= A-y\n    dw= np.dot(dz.T , X) * (1/m)\n    db= sum(A-y) * (1/m)\n    \n    grads={'dw':dw,'db':db}\n    return cost,grads\n\ndef gradient_descent(w,b,X,y,grads,alpha,num_iteration):\n    \n    for i in range(num_iteration):     \n        \n        dw= grads['dw']\n        db= grads['db']\n        \n        w= w- alpha * dw\n        b= b- alpha * db\n        \n        cost,grads= propogate(w,b,X,y)\n        if i%100 ==0:\n            print(f'iterations:{i} --> cost {cost *100}')\n            \n    return cost,grads\n\nX1=train_x\nY1=test_y\n\nw,b=initialize_parameters(X1)\ncost,grads= propogate(w,b,X1,Y1)\nbest_cost,best_grads= gradient_descent(w,b,X1,Y1,grads,0.9,1000)\n\n# print('w',w.shape)\n# print('X',X1.shape)\n# print('y',Y1.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dw1= best_grads['dw']\ndef predict(X,w):\n    y_pred= sigmoid(X.dot(w.T))<0.5\n    return y_pred.map({True: 1,False: 0})\ny_pred= predict(val_x,dw1)\nmean_absolute_error(y_pred,val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1 real disaster == less than 0.5\n#0 unreal disaster \nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_pred,val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer= predict(test,dw1)\noutput= pd.DataFrame({'id':test_set_orig['id'],'target':answer})\noutput.to_csv('submission_correct.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"##  ![image.png](attachment:image.png)","attachments":{"image.png":{"image/png":"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"}},"execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}