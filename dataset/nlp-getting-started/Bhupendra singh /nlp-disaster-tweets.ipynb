{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nimport seaborn as sns \nimport re\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', 150)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/nlp-getting-started/train.csv')\ndataset_test = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## VIsualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(8,5))\nsns.countplot(x=\"target\", data=dataset , palette=\"dark\", linewidth=5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"piedata = dataset['target']\nplt.figure(figsize=(6,6))\npiedata.value_counts().plot(kind = 'pie',autopct = '%.2f%%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y = dataset.keyword,order = dataset['keyword'].value_counts().sort_values(ascending=False).iloc[0:20].index)\nplt.title(\"Count of Keywords\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disastered_tweet = dataset.groupby('keyword')['target'].mean().sort_values(ascending=False).head(15)\nnon_disasterd_tweet  = dataset.groupby('keyword')['target'].mean().sort_values().head(15)\n\nplt.figure(figsize=(7,4))\nsns.barplot(disastered_tweet, disastered_tweet.index, color='red')\nplt.title('Keywords with highest % of disaster tweets')\nsns.barplot(non_disasterd_tweet, non_disasterd_tweet.index, color='blue')\nplt.title('Keywords with lowest % of disaster tweets')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,7))\nsns.countplot(y = dataset.location, order = dataset['location'].value_counts().sort_values(ascending=False).iloc[0:15].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_loc = dataset.location.value_counts()\ntop_loc_disaster = list(raw_loc[raw_loc>=10].index)\ntop_only_disaster = dataset[dataset.location.isin(top_loc_disaster)]\n\ntop_location = top_only_disaster.groupby('location')['target'].mean().sort_values(ascending=False)\nsns.barplot(x=top_location.index, y=top_location)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.keyword.fillna('None', inplace=True) \ndataset.location.fillna('None' , inplace = True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data_Cleaning "},{"metadata":{"trusted":true},"cell_type":"code","source":"def decontraction(phrase):\n    \n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase =phrase.lower()\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub('\\[.*?\\]', ' ', phrase) \n    phrase = re.sub('https?://\\S+|www\\.\\S+', ' ', phrase)\n    phrase = re.sub('<.*?>+', ' ', phrase)\n    phrase = re.sub('\\n', ' ', phrase)\n    phrase = re.sub('\\w*\\d\\w*', ' ', phrase)\n    return phrase\n\ndataset.text = [decontraction(tweet) for tweet in dataset.text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nlemmatizer = WordNetLemmatizer()\nps = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = []\nfor i in range(len(dataset.text)):\n    review = re.sub('[^a-zA-Z]' ,' ', dataset['text'][i])\n    review =review.lower()\n    review = review.split()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [ps.stem(word) for word in review  if not word in set(all_stopwords)]\n    review = ' '.join(review)\n    corpus.append(review)\nprint(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = TfidfVectorizer()\nav = CountVectorizer()\nX = cv.fit_transform(corpus).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = dataset.iloc[:,-1].values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test , Y_train, Y_test = train_test_split(X,Y , test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape , X_test.shape , Y_train.shape , Y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NAives_Bayes "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred =classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix , accuracy_score\ncm = confusion_matrix(Y_test , Y_pred)\nsns.heatmap(cm)\naccuracy_score(Y_test , Y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_train = classifier.predict(X_train)\ncm = confusion_matrix(Y_train , Y_pred_train)\nsns.heatmap(cm)\naccuracy_score(Y_train , Y_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import one_hot\nvoc_size = 10000\nonehot_rep = [ one_hot(words, voc_size) for words in corpus]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_rep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_length = 20\nembedded_does= pad_sequences(onehot_rep , padding = 'pre', maxlen =sent_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_does","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dim = 10\nmodel = Sequential()\nmodel.add(Embedding(voc_size  ,10 , input_length = sent_length ))\nmodel.compile('adam' , 'mse')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.predict(embedded_does))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_does[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.predict(embedded_does[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(embedded_does)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape , Y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test , Y_train, Y_test = train_test_split(X,Y , test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier.fit(X_train , Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_train = classifier.predict(X_train)\nY_pred_1 = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(Y_pred_1 , Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nclassifier_xgb = XGBClassifier()\nclassifier_xgb.fit(X_train , Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_xgb = classifier_xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(Y_pred_xgb , Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## For Test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test.text = [decontraction(tweet) for tweet in dataset_test.text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decontraction(phrase):\n    \n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = phrase.lower()\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub('\\[.*?\\]', ' ', phrase) \n    phrase = re.sub('https?://\\S+|www\\.\\S+', ' ', phrase)\n    phrase = re.sub('<.*?>+', ' ', phrase)\n    phrase = re.sub('\\n', ' ', phrase)\n    phrase = re.sub('\\w*\\d\\w*', ' ', phrase)\n    return phrase\n\ndataset_test.text = [decontraction(tweet) for tweet in dataset_test.text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_test = []\nfor i in range(len(dataset_test.text)):\n    \n    review = re.sub('[^a-zA-Z]' ,' ', dataset_test['text'][i])\n    review =review.lower()\n    review = review.split()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [ps.stem(word) for word in review  if not word in set(all_stopwords)]\n    review = ' '.join(review)\n    corpus_test.append(review)\nprint(corpus_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"onehot_rep = [ one_hot(words, voc_size) for words in corpus_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_does_test= pad_sequences(onehot_rep , padding = 'pre', maxlen =sent_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedded_does_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_dataset = np.array(embedded_does_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dim = 10\nmodel = Sequential()\nmodel.add(Embedding(voc_size  ,10 , input_length = sent_length ))\nmodel.compile('adam' , 'mse')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_test_data = classifier_xgb.predict(X_test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file_test = pd.DataFrame({'Id':dataset_test['id'],'target':Y_pred_test_data})\nsubmission_file_test.to_csv('submission_file.csv',index=False)\nsubmission_file_test = pd.read_csv('submission_file.csv')\nsubmission_file_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}