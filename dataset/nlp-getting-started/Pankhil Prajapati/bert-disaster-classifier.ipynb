{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n!pip install -q tf-nightly\n!pip install -q tf-models-nightly","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\n\nfrom official.modeling import tf_utils\nfrom official import nlp\nfrom official.nlp import bert\n\n# Load the required submodules\nimport official.nlp.optimization\nimport official.nlp.bert.bert_models\nimport official.nlp.bert.configs\nimport official.nlp.bert.run_classifier\nimport official.nlp.bert.tokenization\nimport official.nlp.data.classifier_data_lib\nimport official.nlp.modeling.losses\nimport official.nlp.modeling.models\nimport official.nlp.modeling.networks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = \"../input/nlp-getting-started/train.csv\"\ntest = \"../input/nlp-getting-started/test.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (pd.read_csv(train)).iloc[:,3:]\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bert-Tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\ntf.io.gfile.listdir(gs_folder_bert)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = bert.tokenization.FullTokenizer(\n    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n     do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Vocab size:\", len(tokenizer.vocab))\ntokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)\n\nsentence = tf.ragged.constant([\n    encode_sentence(s) for s in  data['text']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Sentence shape:\", sentence.shape.as_list())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence.shape[0]\ninput_word_ids = tf.concat([cls, sentence], axis=-1)\n_ = plt.pcolormesh(input_word_ids.to_tensor())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_mask = tf.ones_like(input_word_ids).to_tensor()\n\nplt.pcolormesh(input_mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoder and classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s, tokenizer):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef bert_encode(data, tokenizer):\n    num_examples = len(data[\"text\"])\n  \n    sentence = tf.ragged.constant([\n       encode_sentence(s, tokenizer)\n       for s in np.array(data[\"text\"])])\n\n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence.shape[0]\n    input_word_ids = tf.concat([cls, sentence], axis=-1)\n\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n    type_cls = tf.zeros_like(cls)\n    type_s = tf.zeros_like(sentence)\n    input_type_ids = tf.concat(\n      [type_cls, type_s], axis=-1).to_tensor()\n\n    inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n    return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_train,d_val,d_test = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train \",d_train.shape)\nprint(\"val \",d_val.shape)\nprint(\"test \",d_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = bert_encode(d_train, tokenizer)\ndata_train_labels = d_train['target']\n\ndata_validation = bert_encode(d_val, tokenizer)\ndata_validation_labels = d_val['target']\n\ndata_test = bert_encode(d_test, tokenizer)\ndata_test_labels  = d_test['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key, value in data_train.items():\n  print(f'{key:15s} shape: {value.shape}')\n\nprint(f'glue_train_labels shape: {data_train_labels.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nconfig_dict = {'attention_probs_dropout_prob': 0.1,\n 'hidden_act': 'gelu',\n 'hidden_dropout_prob': 0.1,\n 'hidden_size': 768,\n 'initializer_range': 0.02,\n 'intermediate_size': 3072,\n 'max_position_embeddings': 512,\n 'num_attention_heads': 12,\n 'num_hidden_layers': 12,\n 'type_vocab_size': 2,\n 'vocab_size': 30522}\nbert_config = bert.configs.BertConfig.from_dict(config_dict)\n\nconfig_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n    bert_config, num_labels=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_batch = {key: val[:10] for key, val in data_train.items()}\n\nbert_classifier(\n    data_batch, training=True\n).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = tf.train.Checkpoint(model=bert_encoder)\ncheckpoint.restore(\n    os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up epochs and steps\nepochs = 3\nbatch_size = 32\neval_batch_size = 32\n\ntrain_data_size = len(data_train_labels)\nsteps_per_epoch = int(train_data_size / batch_size)\nnum_train_steps = steps_per_epoch * epochs\nwarmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n\n# creates an optimizer with learning rate schedule\noptimizer = nlp.optimization.create_optimizer(\n    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"metrics = [tf.keras.metrics.BinaryAccuracy('binary_accuracy', dtype=tf.float32)]\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True,name='binary_crossentropy')\nbert_classifier.compile(\n    optimizer=optimizer,\n    loss=loss,\n    metrics=metrics)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_classifier.fit(\n      data_train, data_train_labels,\n      validation_data=(data_validation, data_validation_labels),\n      batch_size=32,\n      epochs=epochs)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation and Accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = bert_classifier.predict(data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = bert_classifier(data_test, training=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check = [1 if i>0 else 0 for i in result]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(check, data_test_labels,))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"export_dir='./saved_model'\ntf.saved_model.save(bert_classifier, export_dir=export_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"export_dir='./saved_model'\nclf = tf.saved_model.load(export_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls saved_model/assets/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_sub = bert_encode(sub, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for key, value in data_sub.items():\n  print(f'{key:15s} shape: {value.shape}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_val = clf([data_sub['input_word_ids'],\n              data_sub['input_mask'],\n              data_sub['input_type_ids']], training=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = [1 if i>0 else 0 for i in sub_val]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'id': sub.id, 'Target': target})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}