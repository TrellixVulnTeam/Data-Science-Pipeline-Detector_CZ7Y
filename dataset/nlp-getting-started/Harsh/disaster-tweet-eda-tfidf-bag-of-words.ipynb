{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport bokeh\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n%matplotlib inline\nfrom matplotlib import style\nimport re\nimport time\nimport string\nimport warnings\n\n# for all NLP related operations on text\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom nltk.classify import NaiveBayesClassifier\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\n\n# To identify the sentiment of text\nfrom textblob import TextBlob\nfrom textblob.sentiments import NaiveBayesAnalyzer\nfrom textblob.np_extractors import ConllExtractor\n\n# ignoring all the warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# downloading stopwords corpus\nnltk.download('stopwords')\n#nltk.download('wordnet')\n#nltk.download('vader_lexicon')\n#nltk.download('averaged_perceptron_tagger')\n#nltk.download('movie_reviews')\n#nltk.download('punkt')\n#nltk.download('conll2000')\n#nltk.download('brown')\nstopwords = set(stopwords.words(\"english\"))\n\n# for showing all the plots inline\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Getting Data**","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Insight of Submission file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train File\")\ndisplay(df_train.head())\nprint(\"Test File\")\ndisplay(df_test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train File Shape\",df_train.shape)\nprint(\"Test File Shape\",df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train File\")\ndisplay(df_train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['keyword','location']\n\nsns.barplot(x=df_train[cols].isnull().sum(),y=df_train[cols].isnull().sum())\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train File\")\ndisplay(df_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train File\")\ndisplay(df_train['target'].value_counts())\ndisplay(df_train['location'].value_counts())\ndisplay(len(df_train['keyword'].value_counts()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Text_Length']=df_train['text'].apply(lambda x:len(x) - x.count(\" \"))\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['target_mean'] = df_train.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=df_train.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=df_train.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ndf_train.drop(columns=['target_mean'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df_train['target'])\nplt.tick_params(axis='x', labelsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def hashtag_extract(text_list):\n    hashtags = []\n    # Loop over the words in the tweet\n    for text in text_list:\n        ht = re.findall(r\"#(\\w+)\", text)\n        hashtags.append(ht)\n\n    return hashtags\n\ndef generate_hashtag_freqdist(hashtags):\n    a = nltk.FreqDist(hashtags)\n    d = pd.DataFrame({'Hashtag': list(a.keys()),\n                      'Count': list(a.values())})   \n    d = d.nlargest(columns=\"Count\", n = 50)\n    plt.figure(figsize=(16,7))\n    ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n    plt.xticks(rotation=80)\n    ax.set(ylabel = 'Count')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hashtags = hashtag_extract(df_train['text'])\ndf_train['hashtags'] = hashtags\nhashtags = sum(hashtags, [])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hashtags","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_hashtag_freqdist(hashtags)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['hashtags'] = df_train['hashtags'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['hashtags'] = df_train['hashtags'].str.strip('[')\ndf_train['hashtags'] = df_train['hashtags'].str.strip(']').astype(str)\ndf_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['hashtags'] = df_train['hashtags'].replace({'':np.nan})\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1 way\ndef fetch_sentiment_using_SIA(text):\n    sid = SentimentIntensityAnalyzer()\n    polarity_scores = sid.polarity_scores(text)\n    return 'neg' if polarity_scores['neg'] > polarity_scores['pos'] else 'pos'\n\n# 2 way\ndef fetch_sentiment_using_textblob(text):\n    analysis = TextBlob(text)\n    return 'pos' if analysis.sentiment.polarity >= 0 else 'neg'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiments_using_SIA = df_train.text.apply(lambda tweet: fetch_sentiment_using_SIA(tweet))\ndisplay(pd.DataFrame(sentiments_using_SIA.value_counts()))\n\nsentiments_using_textblob = df_train.text.apply(lambda tweet: fetch_sentiment_using_textblob(tweet))\ndisplay(pd.DataFrame(sentiments_using_textblob.value_counts()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['sentiment'] = sentiments_using_SIA\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x= df_train['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df_train.drop(['Text_Length','hashtags','sentiment'],axis=1)\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PreProcessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Text","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_pattern(text, pattern_regex):\n    r = re.findall(pattern_regex, text)\n    for i in r:\n        text = re.sub(i, '', text)\n    \n    return text ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are keeping cleaned tweets in a new column called 'tidy_tweets'\ntrain['tidy_tweets'] = np.vectorize(remove_pattern)(train['text'], \"@[\\w]*: | *RT*\")\ntrain.head(10)\n\ntest['tidy_tweets'] = np.vectorize(remove_pattern)(test['text'], \"@[\\w]*: | *RT*\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_tweets = []\ncleaned_tweets_test = []\n\nfor index, row in train.iterrows():\n    # Here we are filtering out all the words that contains link\n    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n    cleaned_tweets.append(' '.join(words_without_links))\n    \nfor index, row in test.iterrows():\n    # Here we are filtering out all the words that contains link\n    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n    cleaned_tweets_test.append(' '.join(words_without_links))\n\ntrain['tidy_tweets'] = cleaned_tweets\ntest['tidy_tweets'] = cleaned_tweets_test\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train['tidy_tweets']!='']\ntest = test[test['tidy_tweets']!='']\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop_duplicates(subset=['tidy_tweets'], keep=False)\ntest.drop_duplicates(subset=['tidy_tweets'], keep=False)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['absolute_tidy_tweets'] = train['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")\ntest['absolute_tidy_tweets'] = test['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords_set = set(stopwords)\ncleaned_tweets = []\ncleaned_tweets_test = []\n\nfor index, row in train.iterrows():\n    \n    # filerting out all the stopwords \n    words_without_stopwords = [word for word in row.absolute_tidy_tweets.split() if not word in stopwords_set and '#' not in word.lower()]\n    \n    # finally creating tweets list of tuples containing stopwords(list) and sentimentType \n    cleaned_tweets.append(' '.join(words_without_stopwords))\n    \n    \nfor index, row in test.iterrows():\n    \n    # filerting out all the stopwords \n    words_without_stopwords = [word for word in row.absolute_tidy_tweets.split() if not word in stopwords_set and '#' not in word.lower()]\n    \n    # finally creating tweets list of tuples containing stopwords(list) and sentimentType \n    cleaned_tweets_test.append(' '.join(words_without_stopwords))\n    \n    \ntrain['absolute_tidy_tweets'] = cleaned_tweets\ntest['absolute_tidy_tweets'] = cleaned_tweets_test\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_tweet = train['absolute_tidy_tweets'].apply(lambda x: x.split())\n\ntokenized_tweet_test = test['absolute_tidy_tweets'].apply(lambda x: x.split())\n\ntokenized_tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_lemmatizer = WordNetLemmatizer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])\n\ntokenized_tweet_test = tokenized_tweet_test.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])\ntokenized_tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, tokens in enumerate(tokenized_tweet):\n    tokenized_tweet[i] = ' '.join(tokens)\n\n\nfor i, tokens in enumerate(tokenized_tweet_test):\n    tokenized_tweet_test[i] = ' '.join(tokens)\n\n    \ntrain['absolute_tidy_tweets'] = tokenized_tweet\ntest['absolute_tidy_tweets'] = tokenized_tweet_test\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Key Phrases","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class PhraseExtractHelper(object):\n    def __init__(self):\n        self.lemmatizer = nltk.WordNetLemmatizer()\n        self.stemmer = nltk.stem.porter.PorterStemmer()\n    \n    def leaves(self, tree):\n        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n            yield subtree.leaves()\n\n    def normalise(self, word):\n        \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n        word = word.lower()\n        # word = self.stemmer.stem_word(word) # We will loose the exact meaning of the word \n        word = self.lemmatizer.lemmatize(word)\n        return word\n\n    def acceptable_word(self, word):\n        \"\"\"Checks conditions for acceptable word: length, stopword. We can increase the length if we want to consider large phrase\"\"\"\n        accepted = bool(3 <= len(word) <= 40\n            and word.lower() not in stopwords\n            and 'https' not in word.lower()\n            and 'http' not in word.lower()\n            and '#' not in word.lower()\n            )\n        return accepted\n\n    def get_terms(self, tree):\n        for leaf in self.leaves(tree):\n            term = [ self.normalise(w) for w,t in leaf if self.acceptable_word(w) ]\n            yield term","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\ngrammar = r\"\"\"\n    NBAR:\n        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n        \n    NP:\n        {<NBAR>}\n        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n\"\"\"\nchunker = nltk.RegexpParser(grammar)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key_phrases = []\nkey_phrases_test = []\nphrase_extract_helper = PhraseExtractHelper()\n\nfor index, row in train.iterrows(): \n    toks = nltk.regexp_tokenize(row.tidy_tweets, sentence_re)\n    postoks = nltk.tag.pos_tag(toks)\n    tree = chunker.parse(postoks)\n\n    terms = phrase_extract_helper.get_terms(tree)\n    tweet_phrases = []\n\n    for term in terms:\n        if len(term):\n            tweet_phrases.append(' '.join(term))\n    \n    key_phrases.append(tweet_phrases)\n    \nkey_phrases[:10]\n\nfor index, row in test.iterrows(): \n    toks = nltk.regexp_tokenize(row.tidy_tweets, sentence_re)\n    postoks = nltk.tag.pos_tag(toks)\n    tree = chunker.parse(postoks)\n\n    terms = phrase_extract_helper.get_terms(tree)\n    tweet_phrases_test = []\n\n    for term in terms:\n        if len(term):\n            tweet_phrases_test.append(' '.join(term))\n    \n    key_phrases_test.append(tweet_phrases_test)\n    \nkey_phrases_test[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"textblob_key_phrases = []\ntextblob_key_phrases_test = []\nextractor = ConllExtractor()\n\nfor index, row in train.iterrows():\n    # filerting out all the hashtags\n    words_without_hash = [word for word in row.tidy_tweets.split() if '#' not in word.lower()]\n    \n    hash_removed_sentence = ' '.join(words_without_hash)\n    \n    blob = TextBlob(hash_removed_sentence, np_extractor=extractor)\n    textblob_key_phrases.append(list(blob.noun_phrases))\n\ntextblob_key_phrases[:10]\n\nfor index, row in test.iterrows():\n    # filerting out all the hashtags\n    words_without_hash = [word for word in row.tidy_tweets.split() if '#' not in word.lower()]\n    \n    hash_removed_sentence = ' '.join(words_without_hash)\n    \n    blob = TextBlob(hash_removed_sentence, np_extractor=extractor)\n    textblob_key_phrases_test.append(list(blob.noun_phrases))\n\ntextblob_key_phrases_test[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['key_phrases'] = textblob_key_phrases\n\ntest['key_phrases'] = textblob_key_phrases_test\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_wordcloud(all_words):\n    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate(all_words)\n\n    plt.figure(figsize=(14, 10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words = ' '.join([text for text in train['absolute_tidy_tweets'][train.target == 1]])\ngenerate_wordcloud(all_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words = ' '.join([text for text in train['absolute_tidy_tweets'][train.target == 0]])\ngenerate_wordcloud(all_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df = train[train['key_phrases'].str.len()>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrase_sents = tweets_df['key_phrases'].apply(lambda x: ' '.join(x))\ntweets_df['Concatenated'] = tweets_df['absolute_tidy_tweets'] + phrase_sents","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Bag of Words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_word_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english')\nbow_word_feature = bow_word_vectorizer.fit_transform(tweets_df['absolute_tidy_tweets'])\n\nphrase_sents = tweets_df['key_phrases'].apply(lambda x: ' '.join(x))\n\nbow_phrase_vectorizer = CountVectorizer(max_df=0.90, min_df=2)\nbow_phrase_feature = bow_phrase_vectorizer.fit_transform(phrase_sents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TF-IDF features\ntfidf_word_vectorizer = TfidfVectorizer(max_features=36000,max_df=0.99, min_df=1, stop_words='english',ngram_range=(1,2))\n# TF-IDF feature matrix\ntfidf_word_feature = tfidf_word_vectorizer.fit_transform(tweets_df['Concatenated'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrase_sents = tweets_df['key_phrases'].apply(lambda x: ' '.join(x))\n# TF-IDF phrase feature\ntfidf_phrase_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, ngram_range=(1,2))\ntfidf_phrase_feature = tfidf_phrase_vectorizer.fit_transform(phrase_sents)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_variable = tweets_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(matrix):\n    plt.clf()\n    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Set2_r)\n    classNames = ['Positive', 'Negative']\n    plt.title('Confusion Matrix')\n    plt.ylabel('Predicted')\n    plt.xlabel('Actual')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames)\n    plt.yticks(tick_marks, classNames)\n    s = [['TP','FP'], ['FN', 'TN']]\n\n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(matrix[i][j]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def naive_model(X_train, X_test, y_train, y_test):\n    naive_classifier = MultinomialNB()\n    naive_classifier.fit(X_train.toarray(), y_train)\n\n    # predictions over test set\n    predictions = naive_classifier.predict(X_test.toarray())\n\n    # calculating Accuracy Score\n    print(f'Accuracy Score :: {accuracy_score(y_test, predictions)}')\n    conf_matrix = confusion_matrix(y_test, predictions, labels=[True, False])\n    plot_confusion_matrix(conf_matrix)\n    \n    return naive_classifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TF-idf Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(tfidf_word_feature, target_variable, test_size=0.3, random_state=272)\nTF_word = naive_model(X_train, X_test, y_train, y_test)\nTF_word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(tfidf_phrase_feature, target_variable, test_size=0.3, random_state=272)\nTF_phrase = naive_model(X_train, X_test, y_train, y_test)\nTF_phrase","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### BAG of WORDS model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(bow_phrase_feature, target_variable, test_size=0.3, random_state=272)\nBOW_phrase = naive_model(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(bow_word_feature, target_variable, test_size=0.3, random_state=272)\nBOW_words = naive_model(X_train, X_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"phrase_sents_test = test['key_phrases'].apply(lambda x: ' '.join(x))\ntest['Concatenated'] = test['absolute_tidy_tweets'] + phrase_sents_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TF-IDF feature matrix\ntfidf_word_feature_test = tfidf_word_vectorizer.fit_transform(test['Concatenated'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = TF_word.predict(tfidf_word_feature_test.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test[['id','target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.set_index('id',inplace=True)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}