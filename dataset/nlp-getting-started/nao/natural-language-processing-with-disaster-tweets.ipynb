{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"filename_train = '../input/nlp-getting-started/train.csv'\nfilename_test = '../input/nlp-getting-started/test.csv'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv(filename_train, index_col='id')\ndf_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(filename_test, index_col='id')\ndf_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Clean text","metadata":{}},{"cell_type":"code","source":"import re, string\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ndf_train['text'] = df_train['text'].apply(lambda x: clean_text(x))\ndf_test['text'] = df_test['text'].apply(lambda x: clean_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lemmatization","metadata":{}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n\nlem = WordNetLemmatizer()\ndf_train['text'] = df_train['text'].apply(lambda x: ' '.join([lem.lemmatize(token) for token in x.split()]))\ndf_test['text'] = df_test['text'].apply(lambda x: ' '.join([lem.lemmatize(token) for token in x.split()]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get explanatory variable","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\ndef create_corpus(texts):\n    \"\"\"Decompose text to corpus (e.g. `This is a pen` to [ `This`, `is`, `a`, `pen` ])\n    \n    Arguments:\n        texts: list(str) / Text list.\n        \n    Returns:\n        list(str) / Corpus list.\n    \"\"\"\n    \n    corpus = []\n    for tweet in texts:\n        words = [ word.lower() for word in word_tokenize(tweet) ]\n        corpus.append(words)\n        \n    return corpus","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_train = create_corpus(df_train['text'])\ncorpus_test = create_corpus(df_test['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\n# Torkenize corpus to integer list\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus_train)\nseq_train = tokenizer.texts_to_sequences(corpus_train)\nseq_test = tokenizer.texts_to_sequences(corpus_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index\nprint('Number of unique words:',len(word_index))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nnum_words = 50\nX_train = pad_sequences(seq_train, maxlen=num_words)\nX_test = pad_sequences(seq_test, maxlen=num_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get objective variable","metadata":{}},{"cell_type":"code","source":"y_train = df_train['target']\ny_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get GloVe embedding matrix","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nembedding_dict = {}\nwith open('../input/glove6b/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_words = len(word_index) + 1\nembedding_dims = 100\n\nembedding_matrix = np.zeros((max_words, embedding_dims))\nfor word, i in word_index.items():\n    if i > max_words:\n        continue\n        \n    emb_vec = embedding_dict.get(word)    \n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\ninput = tf.keras.layers.Input(shape=(num_words,))\n\nx = input\nx = tf.keras.layers.Embedding(\n    max_words, embedding_dims, \n    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n    input_length=num_words, trainable=False)(x)\nx = tf.keras.layers.LSTM(64)(x)\nx = tf.keras.layers.Dropout(0.2)(x)\n\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.models.Model(input, output)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 100\nbatch_size = 64\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n]\n\nmodel.fit(X_train, \n          y_train, \n          epochs=epochs,\n          batch_size=batch_size,\n          validation_split=0.1,\n          callbacks=callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = np.where(y_pred > 0.5, 1, 0).squeeze()\ny_pred = pd.Series(y_pred, name='target').astype(int)\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer = pd.concat([df_test.index.to_series().reset_index(drop=True), y_pred], axis=1)\nanswer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename_output = './submission.csv'\nanswer.to_csv(filename_output, index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}