{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Learn Machine Learning models with examples\n#### The motive behind working on this Kernal is to revise the basic steps or formal mathmatical calculation running behind the models and not treating any model as Black Box\nPlease comment the suggestion or upvote if you like."},{"metadata":{},"cell_type":"markdown","source":"*Note* : Only done for Logistics Regression- Few more things to be added.\n Work In Progress. \n Will do for other algos also."},{"metadata":{},"cell_type":"markdown","source":"### Importing all the required Libs"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing all the required libraries\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nfrom nltk.stem import PorterStemmer\nfrom matplotlib import pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\n\nstopwords = set(STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding few mords in stopwords - got from wordcloud below"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords.add('will')\nstopwords.add('ve')\nstopwords.add('now')\nstopwords.add('gonna')\nstopwords.add('wanna')\nstopwords.add('lol')\nstopwords.add('via')\n            ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Changing the current directory using python os module to the data directory"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/input/nlp-getting-started/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing data to memory using pandas"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Doing basic checks on the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Overview\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many null values are there?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Null values in Train data set%')\ntrain.isnull().sum()/len(train)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Null values in Test dataset %')\ntest.isnull().sum()/len(test)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we will only deal with teh text column, so we will not be focusing on the missing values in other columns"},{"metadata":{},"cell_type":"markdown","source":"### Combining all the data train and test, for cleaning purpose"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining all the text data \ntweets_data = pd.concat([train, test], axis=0, sort=False, ignore_index=True)\ntweets_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Below function contains the data cleaning steps using regex"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data cleaning steps\ndef clean_data(df, text_col, new_col='cleaned_text', stemming=False, lemmatization=True):\n    \n    '''\n    It will remove the noise from the text data(@user, characters not able to encode/decode properly)    \n    ----Arguments----\n    df : Data Frame\n    col : column name (string)\n    steming : boolean\n    lemmatization : boolean\n    '''\n    tweets_data = df.copy() # deep copying the data in order to avoid any change in the main data col  \n    \n    # Creating one more new column for new text transformation steps\n    tweets_data[new_col] = tweets_data[text_col]\n    \n    # removing @<userid>, as it is very common in the twitter data\n    tweets_data[new_col] = tweets_data[new_col].apply(lambda x : re.sub(\n        '@[A-Za-z0-9_]+', '', x)) \n    \n    # Removing &amp \n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub('&amp',' ', str(x)))\n    \n    # Removing URLs from the data\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub('https?:\\/\\/[a-zA-z0-9\\.\\/]+','',\n                                                                     str(x)))\n    tweets_data[new_col] = tweets_data[new_col].str.lower()\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\’\", \"\\'\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\s\\'\", \" \", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"n\\’t\", \" not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(\"\\'s\", \" is\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\’d\", \" would\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'[.|,|)|(|\\|/]',r' ', str(x)))\n    \n    # Trimming the sentences\n    tweets_data[new_col] = tweets_data[new_col].str.strip() \n    tweets_data[new_col] = tweets_data[new_col].apply(lambda x : re.findall(\n       \"[A-Za-z0-9]+\", x))\n    tweets_data[new_col] = tweets_data[new_col].apply(lambda x : \" \".join(x))\n    \n    # Remove stopwords\n    tweets_data[new_col] = tweets_data[new_col].apply(\n        lambda x : ['' if word in stopwords else word for word in x.split()])\n    \n    tweets_data[new_col] = tweets_data[new_col].apply(lambda x : \" \".join(x))\n        \n    # Removing extra spaces\n    tweets_data[new_col] = tweets_data[new_col].apply(lambda x : re.sub(\"\\s+\", \" \", x))\n    \n    # lemmatization\n    if lemmatization:\n        \n        lemma = WordNetLemmatizer()\n        \n        tweets_data[new_col] = tweets_data[new_col].apply(lambda sentence : \n                                         [lemma.lemmatize(word,'v') for word in sentence.split(\" \")])\n        \n        tweets_data[new_col] = tweets_data[new_col].apply(lambda x : \" \".join(x))\n     \n    # Stemming code\n    if stemming:\n        stemming = PorterStemmer()\n        \n        tweets_data[new_col] = tweets_data[new_col].apply(lambda sentence : \n                                         [stemming.stem(x) for x in sentence.split(\" \")])\n        \n        tweets_data[new_col] = tweets_data[new_col].apply(lambda x : \" \".join(x))\n\n    return tweets_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_data = clean_data(tweets_data, \"text\", \n                         'cleaned_text', \n                         lemmatization=True,\n                         stemming=False)\n\npd.set_option('display.max_colwidth', -1)\n\nprint('----- Text Before And After Cleaning -----')\n\ntweets_data[['text', 'cleaned_text']].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stemming of Text - Stemming tries to convert word into it's root form"},{"metadata":{"trusted":true},"cell_type":"code","source":"stemming = PorterStemmer()\nprint(f\"Runs converted to {stemming.stem('runs')}\")\nprint(f\"Stemming converted to {stemming.stem('stemming')}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting Word Cloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10)\nwordcloud.generate(\" \".join(tweets_data['cleaned_text']))\n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None, dpi=80) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wildfire seems to be one of the major issue."},{"metadata":{},"cell_type":"markdown","source":"### %cent of classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('% of classes')\nprint(tweets_data['target'].value_counts() / len(train) * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preparing data using TF-IDF [Term Frequency Inverse Document Matrix]"},{"metadata":{"trusted":true},"cell_type":"code","source":"vec = TfidfVectorizer(ngram_range=(1,5),\n                      #max_features=10000,\n                      min_df=3,\n                      stop_words='english')\n\ntfidf_matrix = vec.fit_transform(tweets_data['cleaned_text'])\n\ntfidf_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_matrix = pd.DataFrame(tfidf_matrix.toarray(),\n                            columns = vec.get_feature_names(),\n                            dtype='float32')\n\nprint(\"Shape of the dataframe \",tfidf_matrix.shape)\nprint('Data Frame Info')\ntfidf_matrix.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data for testing "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare the data set for model training\n\nX = tfidf_matrix.iloc[range(0, train.shape[0]), :]\n\ntest_dataset = tfidf_matrix.iloc[train.shape[0]:, :] \n                           \ny = tweets_data.loc[0:train.shape[0]-1, 'target']\n\nx_train, x_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    random_state=123, \n                                                    test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Learn And Train Different Types Models\nMy main objective is not to treat any model as a black box, let also know what is going on behind when we fit any model. \nWhen we we will discuss anything about models just keep in mind the below steps needed for a model\n> 1. Objective\n> 2. Model structure (e.g. variables, formula, equation)\n> 3. Model assumptions\n> 4. Parameter estimates and interpretation\n> 5. Model fit (e.g. goodness-of-fit tests and statistics)\n> 6. Model selection"},{"metadata":{},"cell_type":"markdown","source":"## 1. Logistic Regression \n\n- It is a statistical technique that is used to map the the values of a depedent variable(Y) to it's indepedendent or predictor variables(X).\n- It is used for classification purpose either binary like yes/no or ordinal like good, better, best.\n- Also known as logit or log odds - odds means the probability of success divided by probability of failure the function used for parameter estimation other function used is the sigmoid function.\n\n  Logistic function - Calculations\n > log(p/1-p) = b + b1x1 + b2x2 + ... + bkxk | where __p__ is the probaility and __x__ is the feature \n\n > log(p/1-p) is known as log odds\n\n > p/1-p = e^(b + b1x1 + b2x2 +...+ bkXk)\n\n > p = 1/(1 + e^(b + b1x1 + b2x2 +...+ bkXk))\n"},{"metadata":{},"cell_type":"markdown","source":"### Summary\n\n1. __Objective__ : Is to build a mode the expected value of Y as the function of X\n2. __Model Structure__: p = e^(b0+b1x1+b2x2+...bkXk)/(1+e^(b0+b1x1+b2x2+...bkXk))\n- __Model Assumption__: \n   1. Independent variables should be linearly dependent on log odds.\n   2. No Multicolinearity - Independent features(X) should not be correlated with each other.\n   3. Needed a larger sample size.\n   4. Dependent variable must be binary or ordinal.\n- __Parameter Estimation__:\n   1. b0(beta) or Intercept : It's basically a constant which means the predicted or avg value of y when the independent variables are 0.\n   2. b1, b2,b3. : It means a slope also defines the association between Y and X. In other terms - By changing 1 unit value of X1 the y will change by b1 times.\n - __Model fit(goodness-of-fit tests)__:\n    1. Accuracy(If dataset is balanced)\n    2. F1 score, precision, recall, auc-roc score, etc\n- __Model Selection__ : Removing unwanted features using Lasso or l1 penality or using Ridge(l2) penality "},{"metadata":{},"cell_type":"markdown","source":"### Fitting or Training Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(max_iter=1500,\n                        solver='lbfgs')\n\nclf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing on Test Data Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F1 Score is \", f1_score(y_test, clf.predict(x_test)))\nconfusion_matrix(y_test, clf.predict(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X, y)\n\nact_pred = clf.predict(test_dataset)\nact_pred = act_pred.astype('int')\n\nsubmission_file = pd.DataFrame({'id' : test['id'],\n                               'target' : act_pred})\n\nsubmission_file.to_csv('/kaggle/working/sub_140120_v0.4lr.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Naive Bayes Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"nv = GaussianNB()\nnv.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing on the test data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F1 Score is \", f1_score(y_test, nv.predict(x_test)))\nprint('Confusion Matrix')\nconfusion_matrix(y_test, nv.predict(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=1500,\n                            max_depth=6,\n                            oob_score=True)\nrf.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking on test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F1 Score is \", f1_score(y_test, rf.predict(x_test)))\nprint('--------Confusion Matrix---------')\nconfusion_matrix(y_test, rf.predict(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training on complete dataset for Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X, y)\n\nact_pred = rf.predict(test_dataset)\nact_pred = act_pred.astype('int')\n\nsubmission_file = pd.DataFrame({'id' : test['id'],\n                               'target' : act_pred})\n\nsubmission_file.to_csv('/kaggle/working/sub_140120_v0.1rf.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. XGBOOST"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(max_depth=6,\n                    learning_rate=0.3,\n                    n_estimators=1500,\n                    objective='binary:logistic',\n                    random_state=123,\n                    n_jobs=4)\n\nxgb.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking score on the test data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"F1 Score is \", f1_score(y_test, xgb.predict(x_test)))\nprint('--------Confusion Matrix---------')\nconfusion_matrix(y_test, xgb.predict(x_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.fit(X, y)\n\nact_pred = xgb.predict(test_dataset)\nact_pred = act_pred.astype('int')\n\nsubmission_file = pd.DataFrame({'id' : test['id'],\n                               'target' : act_pred})\n\nsubmission_file.to_csv('/kaggle/working/sub_130120_v0.1xgb.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Using Word2vec now\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim.models.word2vec as w2v\nimport nltk\nimport multiprocessing\nfrom nltk.tokenize import TweetTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"num_of_features = 300\nmin_word_count = 3\nnum_of_threads = multiprocessing.cpu_count()\ncontext_size = 7\ndownsampling = 1e-3\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vecs = w2v.Word2Vec(sg=1,\n                   seed=123,\n                   workers=num_of_threads,\n                   size=num_of_features,\n                   min_count=min_word_count,\n                   window=context_size,\n                   sample=downsampling)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens_vec = []\ntokens = TweetTokenizer()\ntokens_vec = tweets_data['cleaned_text'].apply(lambda x : tokens.tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vecs.build_vocab(tokens_vec.values.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}