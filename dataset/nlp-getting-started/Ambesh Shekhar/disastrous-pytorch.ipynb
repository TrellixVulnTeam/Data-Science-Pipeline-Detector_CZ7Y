{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport transformers\nimport time \nimport datetime\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader, \\\n                RandomSampler, SequentialSampler\nimport torch.nn.functional as F\nprint(\"Transformers:\",transformers.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntrain = train.sample(frac=1,random_state = 124).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.std([len(x) for x in train['text']]))\nprint(np.std([len(x) for x in test['text']]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased',\n                                                       do_lower_case = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.encode_plus(train['text'][1],max_length=60).keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(values):\n    ids = []\n    masks = []\n    for keyword,t in values:\n        encodes_dict = tokenizer.encode_plus(text = str(keyword),\n                                             text_pair=str(t),\n                                             truncation = True,\n                                             max_len = 64,\n                                             pad_to_max_length=True,\n                                             return_token_type_ids=False)\n        ids.append(encodes_dict['input_ids'])\n        masks.append(encodes_dict['attention_mask'])\n    return ids, masks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val,y_train,y_val = train_test_split(train.loc[:,['text','keyword']].values,\n                                               train['target'].values,\n                                               random_state = 128,\n                                               test_size = 0.2,\n                                               stratify = train['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = torch.tensor(encode(X_train))\nX_val = torch.tensor(encode(X_val))\n\ny_train = torch.tensor(y_train)\ny_val = torch.tensor(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_train,mask_train = X_train\nid_val,mask_val = X_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nX_train = TensorDataset(id_train,mask_train, y_train)\ntrain_sampler = RandomSampler(X_train)\ntrain_dataloader = DataLoader(X_train,sampler = train_sampler,\n                              batch_size = BATCH_SIZE)\n\nX_val = TensorDataset(id_val,mask_val, y_val)\nval_sampler = RandomSampler(X_val)\nval_dataloader = DataLoader(X_val,sampler = val_sampler,\n                              batch_size = BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                   output_attentions = False,\n                                                   output_hidden_states = False,\n                                                                  num_labels = 2)\nclf.trainable = False\nclf.cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Classifier(torch.nn.Module):\n    def __init__(self):\n        super(Classifier,self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\",\n                                                   output_attentions = False,\n                                                   output_hidden_states = False)\n        self.bert.trainable = False\n        self.drop = torch.nn.Dropout(0.4,inplace = True)\n        self.linear = torch.nn.Linear(768,2)\n        \n        \n    def forward(self,ids, masks,labels):\n        x = self.bert(ids,masks)\n        x = self.drop(x[1])\n        x = self.linear(F.softmax(x))\n        return x\nclf = Classifier()\nclf.cuda()        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = transformers.AdamW(clf.parameters(),\n                              lr = 2e-5,\n                              eps = 1e-8)\nepochs = 1\ntotal_steps = len(train_dataloader)*epochs\nscheduler = transformers.get_linear_schedule_with_warmup(optimizer,\n                                                         num_warmup_steps = 0,\n                                                         num_training_steps = total_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flat_accuracy(preds,labels):\n    pred_flat = np.argmax(preds,axis =1).flatten()\n    labels_flat = labels.flat()\n    return np.sum(pred_flat==labels_flat)/len(labels_flat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_time(elapsed):\n    elapsed_roounded = int(round(elapsed))\n    return str(datetime.timedelta(seconds = elapsed_rounded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random.seed(50)\nnp.random.seed(50)\ntorch.manual_seed(50)\ntorch.cuda.manual_seed_all(50)\n\nloss_arr = []\nfor i in range(epochs):\n    print(\"Epoch({:}/{:}) :\".format(i+1,epochs))\n    t0 = time.time()\n    total_loss = 0\n    clf.train()\n    for step,batch in enumerate(train_dataloader):\n        if step % 30 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0)\n            print('Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n        b_ids = batch[0].to(device)\n        b_masks = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        clf.zero_grad()\n        ouptuts = clf(b_ids,b_masks,labels = b_labels)\n        loss = outputs.item()\n        total_loss += loss.item()\n        out.backward()\n        optimtizer.step()\n        scheduler.step()\n    avg_train_loss = train_loss/len(train_dataloader)\n    \n    loss_arr.append(avg_train_loss)\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n    \n    # ========= Validation ==========\n    \n    print(\"\")\n    print(\"Running Validation...\")\n    t0 = time.time()\n    # evaluation mode\n    clf.eval()\n\n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    \n    for batch in val_dataloader:\n        b_input_ids = batch[0]\n        b_input_mask = batch[1]\n        b_labels = batch[2]\n        \n        with torch.no_grad():\n            \n            outputs = clf(b_input_ids, \n                           token_type_ids = None, \n                           attention_mask = b_input_mask)\n            \n        logits = outputs[0]\n        # move logits to cpu\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        # get accuracy\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        eval_accuracy += tmp_eval_accuracy\n        \n        nb_eval_steps += 1\n    \n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.get_device_name(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _ in range(100):\n    torch.cuda.empty_cache()\ntorch.cuda.memory_cached(0)/(1024*1024*1024)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del clf, b_ids, b_masks, b_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}