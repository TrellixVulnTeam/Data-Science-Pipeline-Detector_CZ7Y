{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport random\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB,CategoricalNB\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport re\nfrom nltk.corpus import stopwords\nimport string\nfrom sklearn import preprocessing\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn import svm\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.metrics import accuracy_score\nfrom time import time\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\nsub = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's start with EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape,test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check if ther'is null values\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove redundant samples\ntrain=train.drop_duplicates(subset=['text', 'target'], keep='first')\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have 92 redundants sapmles in our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8,6))\ntrain.groupby('target').id.count().plot.bar(ylim=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* labels are not balanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numbers of word for each sapmle in train & test data\ntrain['text_length'] = train.text.apply(lambda x: len(x.split()))\ntest['text_length'] = test.text.apply(lambda x: len(x.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text_length'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text_length'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Max number of words in all data is 31 and min is 1!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_word_count(df, data_name):\n  sns.distplot(df['text_length'].values)\n  plt.title(f'Sequence char count: {data_name}')\n  plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ig = plt.figure(figsize=(16,6))\n#plt.hist(train[\"text_length\"], bins = 30)\n#plt.show()\nplt.subplot(1, 2, 1)\nplot_word_count(train, 'Train')\n\nplt.subplot(1, 2, 2)\nplot_word_count(test, 'Test')\n\nplt.subplots_adjust(right=3.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# collecting all words in single list\nlist_= []\nfor i in train.text:\n    list_ += i\nlist_= ''.join(list_)\nallWords=list_.split()\nvocabulary= set(allWords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(vocabulary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" * We have 31480 different words in our train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df,target):\n    corpus=[]\n    \n    for x in df[df['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#most frequent 20 words when label == 0 \nimport collections\nallWords=create_corpus(train,target=0)\nvocabulary= set(allWords)\nvocabulary_list= list(vocabulary)\n\nplt.figure(figsize=(16,5))\ncounter=collections.Counter(allWords)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:20]:\n  x.append(word)\n  y.append(count)\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#most frequent 20 words when label == 1 \nimport collections\nallWords=create_corpus(train,target=1)\nvocabulary= set(allWords)\nvocabulary_list= list(vocabulary)\n\nplt.figure(figsize=(16,5))\ncounter=collections.Counter(allWords)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:20]:\n  x.append(word)\n  y.append(count)\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"## 1-Removing Punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"#List of punctuations and we will remove them from our corpus\nstring.punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for  example\ntext='hey # how are !you doing ?'\n\"\".join([char for char in text if char not in string.punctuation])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2-Removing Numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#for example \ntext='hey 4 look 333 at me0'\nre.sub('[0-9]', '', text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3-Removing Stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of stopwords\nstopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for example\ntext='hey this is me and I am here to help you  '\ntokens = word_tokenize(text)\ntokens=[word for word in tokens if word not in stopwords.words('english')]\n' '.join(tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now let's Build a function that clean our data"},{"metadata":{},"cell_type":"markdown","source":"* I just added lower function in order to lowercase all words and stemming"},{"metadata":{"trusted":true},"cell_type":"code","source":"pstem = PorterStemmer()\ndef clean_text(text):\n    text= text.lower()\n    text= re.sub('[0-9]', '', text)\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    tokens = word_tokenize(text)\n    tokens=[pstem.stem(word) for word in tokens]\n    #tokens=[word for word in tokens if word not in stopwords.words('english')]\n    text = ' '.join(tokens)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text(\"hey I am here # ! looks 4 GOOD can't see you!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"clean\"]=train[\"text\"].apply(clean_text)\ntest[\"clean\"]=test[\"text\"].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see the effect of cleaning\ntrain[[\"text\",\"clean\"]].head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# collecting all words in single list\nlist_= []\nfor i in train.clean:\n    list_ += i\nlist_= ''.join(list_)\nallWords=list_.split()\nvocabulary= set(allWords)\nlen(vocabulary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* we reduced our data from 31480 unique words to 19920"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(sublinear_tf=True,max_features=60000, min_df=1, norm='l2',  ngram_range=(1,2))\nfeatures = tfidf.fit_transform(train.clean).toarray()\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_test = tfidf.transform(test.clean).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's use some machine leaning algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data into 4 parts with same distribution of classes.\nskf = StratifiedKFold(n_splits=4, random_state=48, shuffle=True)\naccuracy=[] # list contains the accuracy for each fold\nn=1\ny=train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for trn_idx, test_idx in skf.split(features, y):\n  start_time = time()\n  X_tr,X_val=features[trn_idx],features[test_idx]\n  y_tr,y_val=y.iloc[trn_idx],y.iloc[test_idx]\n  model= LogisticRegression(max_iter=1000,C=3)\n  #model=MultinomialNB(alpha=0.5)\n  #model=svm.SVC(max_iter=1000)\n  model.fit(X_tr,y_tr)\n  s = model.predict(X_val)\n  sub[str(n)]= model.predict(features_test) \n  \n  accuracy.append(accuracy_score(y_val, s))\n  print((time() - start_time)/60,accuracy[n-1])\n  n+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(accuracy)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating Model on Validation Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_valid_y = model.predict(X_val)\nprint(classification_report(y_val, pred_valid_y ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_val, pred_valid_y ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=sub[['1','2','3','4']].mode(axis=1)# select the most frequent predicted class by our model\nsub['target']=df[0]    \nsub=sub[['id','target']]\nsub['target']=sub['target'].apply(lambda x : int(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Please If you find this kernel helpful, upvote it to help others see it 😊"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}