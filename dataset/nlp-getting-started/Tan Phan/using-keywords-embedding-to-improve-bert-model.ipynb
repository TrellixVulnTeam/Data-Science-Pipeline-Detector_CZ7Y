{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install tf-models-official==2.4.0 -q\n! pip install tensorflow-gpu==2.4.1 -q\n! pip install tensorflow-text==2.4.1 -q\n! python -m spacy download en_core_web_sm -q\n! pip install dataprep | grep -v 'already satisfied'","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:21:36.540156Z","iopub.execute_input":"2021-12-09T04:21:36.54081Z","iopub.status.idle":"2021-12-09T04:24:20.434906Z","shell.execute_reply.started":"2021-12-09T04:21:36.540716Z","shell.execute_reply":"2021-12-09T04:24:20.434084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nnp.set_printoptions(precision=4)\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom dataprep.eda import plot, plot_diff, plot_correlation, create_report\nfrom dataprep.clean import clean_text\n\n# Preprocessing and Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport spacy\nimport tensorflow_text as text\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Dropout, concatenate \nfrom tensorflow.keras import Model, regularizers \nfrom tensorflow.keras.metrics import BinaryAccuracy\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom official.nlp.optimization import create_optimizer # AdamW optimizer\n# Warning\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:20.438031Z","iopub.execute_input":"2021-12-09T04:24:20.438373Z","iopub.status.idle":"2021-12-09T04:24:26.68983Z","shell.execute_reply.started":"2021-12-09T04:24:20.438343Z","shell.execute_reply":"2021-12-09T04:24:26.689111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.__version__","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:26.691115Z","iopub.execute_input":"2021-12-09T04:24:26.691388Z","iopub.status.idle":"2021-12-09T04:24:26.698272Z","shell.execute_reply.started":"2021-12-09T04:24:26.691352Z","shell.execute_reply":"2021-12-09T04:24:26.696908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random seeds\nimport random\nimport numpy as np\nimport tensorflow as tf\nrandom.seed(319)\nnp.random.seed(319)\ntf.random.set_seed(319)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:26.701115Z","iopub.execute_input":"2021-12-09T04:24:26.701613Z","iopub.status.idle":"2021-12-09T04:24:26.710877Z","shell.execute_reply.started":"2021-12-09T04:24:26.701576Z","shell.execute_reply":"2021-12-09T04:24:26.710171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=0></a>\n## <p style=\"background-color:lightblue; font-family:newtimeroman; font-size:120%; text-align:left; border-radius: 15px 50px;\">Table of Content</p>\n* [0. Introduction and updates](#0)\n* [1. Loading Data ðŸ’Ž](#1)\n* [2. EDA ðŸ“Š](#2)\n* [3. Data Preprocessing](#3)\n* [4. Vectorization](#4)\n    * [4.1 Common Vectorizer Usage](#4.1)\n    * [4.2 If-Idf Term Weightings](#4.2)\n* [5. Transfer Learning with Hugging Face](#5)\n    * [5.1 Tokenization](#5.1)\n    * [5.2 Defining a Model Architecture](#5.2)\n    * [5.3 Training Classification Layer Weights](#5.3)\n    * [5.4 Fine-tuning DistilBert and Training All Weights](#5.4)\n* [6. Make a Submission](#6)\n* [7. References](#7)","metadata":{}},{"cell_type":"markdown","source":"<a id=0></a>\n<font size=\"+3\" color=\"#5bc0de\"><b>Introduction </b></font><br>\n[Content](#0)\n\nIn this kernel, beside the general steps working with text data as EDA, preprocessing. The workflow in Modelling can divided into 2 main stages:\n1. Defining a Model Architecture with concatenation a keyword column into BERT model\n2. Training Classification Layer Weights.\n\n<a id=1.2 ></a>\n<font size=\"+3\" color=\"#5bc0de\"><b>1.2. Update via Versions </b></font><br>\n[Content](#0)\n\n### Current Version\n* Adding 1 hidden layer in Model to incease accuracy.\n\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data ðŸ’Ž</p>\n\nJust load the dataset and global variables for colors and so on.\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"train_full = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_full = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\nprint('Training Set Shape = {}'.format(train_full.shape))\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()/2**20))\n\nprint('Test Set Shape = {}'.format(test_full.shape))\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()/2**20))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:26.712288Z","iopub.execute_input":"2021-12-09T04:24:26.712571Z","iopub.status.idle":"2021-12-09T04:24:26.776877Z","shell.execute_reply.started":"2021-12-09T04:24:26.712508Z","shell.execute_reply":"2021-12-09T04:24:26.776211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. EDA ðŸ“Š</p>\n\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"plot(train_full)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:26.778041Z","iopub.execute_input":"2021-12-09T04:24:26.778444Z","iopub.status.idle":"2021-12-09T04:24:27.990534Z","shell.execute_reply.started":"2021-12-09T04:24:26.778411Z","shell.execute_reply":"2021-12-09T04:24:27.986583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_report(train_full)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:27.991821Z","iopub.execute_input":"2021-12-09T04:24:27.992209Z","iopub.status.idle":"2021-12-09T04:24:32.695616Z","shell.execute_reply.started":"2021-12-09T04:24:27.992174Z","shell.execute_reply":"2021-12-09T04:24:32.693216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot(train_full, 'text')","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:32.697065Z","iopub.execute_input":"2021-12-09T04:24:32.697514Z","iopub.status.idle":"2021-12-09T04:24:34.0896Z","shell.execute_reply.started":"2021-12-09T04:24:32.697472Z","shell.execute_reply":"2021-12-09T04:24:34.088702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_full.text","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:34.090959Z","iopub.execute_input":"2021-12-09T04:24:34.09231Z","iopub.status.idle":"2021-12-09T04:24:34.099799Z","shell.execute_reply.started":"2021-12-09T04:24:34.092262Z","shell.execute_reply":"2021-12-09T04:24:34.099194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Range from 120 to 140 characters is the most common in tweet.","metadata":{}},{"cell_type":"markdown","source":"### Dataset is balanced","metadata":{}},{"cell_type":"code","source":"plot(train_full, \"text\", \"target\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:34.102945Z","iopub.execute_input":"2021-12-09T04:24:34.103359Z","iopub.status.idle":"2021-12-09T04:24:34.460125Z","shell.execute_reply.started":"2021-12-09T04:24:34.103313Z","shell.execute_reply":"2021-12-09T04:24:34.459157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = train_full.text[train_full.target == 0]\ndf2 = train_full.text[train_full.target == 1]\nplot_diff([df1, df2])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:34.461705Z","iopub.execute_input":"2021-12-09T04:24:34.461954Z","iopub.status.idle":"2021-12-09T04:24:34.933583Z","shell.execute_reply.started":"2021-12-09T04:24:34.461923Z","shell.execute_reply":"2021-12-09T04:24:34.932963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Pre-processing </p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.\n\n\n\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"# Main technics I used in this data\n    * [3.1] Remove 157 duplicated rows\n    * [3.2] Cleaning text\n    * [3.3] Spelling Checker\n    * [3.4] Remove Stemming\n #### Step 3.3 spends a lot time (around 4000s in 4536s in total). \n #### So, I splits Data Preprocessing into [another kernel](https://www.kaggle.com/phanttan/disastertweet-prepareddata). \n #### And the prepared data to save in to [new dataset](https://www.kaggle.com/phanttan/disastertweet-prepared2)\n #### I am so appreciate to you for using/upvoting it.\n","metadata":{}},{"cell_type":"code","source":"# Read commited-dataset\ndf_train = pd.read_csv(\"/kaggle/input/disastertweet-prepared2/train_prepared.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/disastertweet-prepared2/test_prepared.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:34.935085Z","iopub.execute_input":"2021-12-09T04:24:34.935588Z","iopub.status.idle":"2021-12-09T04:24:34.98952Z","shell.execute_reply.started":"2021-12-09T04:24:34.935547Z","shell.execute_reply":"2021-12-09T04:24:34.9886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only apply 'keyword' columns in full data, because other features cleaned in df_train/test\ntrain_full = clean_text(train_full,'keyword')\ntest_full = clean_text(test_full, 'keyword')","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:34.991057Z","iopub.execute_input":"2021-12-09T04:24:34.99156Z","iopub.status.idle":"2021-12-09T04:24:35.337788Z","shell.execute_reply.started":"2021-12-09T04:24:34.991522Z","shell.execute_reply":"2021-12-09T04:24:35.337022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding cleaned data into df_train/test\ndf_train['keyword'] = train_full['keyword']\ndf_test['keyword'] = test_full['keyword']","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:35.339008Z","iopub.execute_input":"2021-12-09T04:24:35.339371Z","iopub.status.idle":"2021-12-09T04:24:35.347192Z","shell.execute_reply.started":"2021-12-09T04:24:35.339334Z","shell.execute_reply":"2021-12-09T04:24:35.34645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Spacy Library\nnlp_spacy = spacy.load('en_core_web_sm')\n# Load the sentence encoder\nsentence_enc = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:35.348098Z","iopub.execute_input":"2021-12-09T04:24:35.348332Z","iopub.status.idle":"2021-12-09T04:24:58.747743Z","shell.execute_reply.started":"2021-12-09T04:24:35.348304Z","shell.execute_reply":"2021-12-09T04:24:58.746982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_keywords(text):\n    potential_keywords = []\n    TOP_KEYWORD = -1\n    # Create a list for keyword parts of speech\n    pos_tag = ['ADJ', 'NOUN', 'PROPN']\n    doc = nlp_spacy(text)\n    \n    for i in doc:\n        if i.pos_ in pos_tag:\n            potential_keywords.append(i.text)\n\n    document_embed = sentence_enc([text])\n    potential_embed = sentence_enc(potential_keywords)    \n    \n    vector_distances = cosine_similarity(document_embed, potential_embed)\n    keyword = [potential_keywords[i] for i in vector_distances.argsort()[0][TOP_KEYWORD:]]\n\n    return keyword\n\ndef keyword_filler(keyword, text):\n    if pd.isnull(keyword):\n        try:\n            keyword = extract_keywords(text)[0]\n        except:\n            keyword = '' \n        \n    return keyword","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:58.749178Z","iopub.execute_input":"2021-12-09T04:24:58.749434Z","iopub.status.idle":"2021-12-09T04:24:58.759953Z","shell.execute_reply.started":"2021-12-09T04:24:58.7494Z","shell.execute_reply":"2021-12-09T04:24:58.759286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.keyword = pd.DataFrame(list(map(keyword_filler, df_train.keyword, df_train.text))).astype(str)\ndf_test.keyword = pd.DataFrame(list(map(keyword_filler, df_test.keyword, df_test.text))).astype(str)\n\nprint('Null Training Keywords => ', df_train['keyword'].isnull().any())\nprint('Null Test Keywords => ', df_test['keyword'].isnull().any())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:24:58.763017Z","iopub.execute_input":"2021-12-09T04:24:58.763243Z","iopub.status.idle":"2021-12-09T04:25:04.076923Z","shell.execute_reply.started":"2021-12-09T04:24:58.763212Z","shell.execute_reply":"2021-12-09T04:25:04.076202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:04.080651Z","iopub.execute_input":"2021-12-09T04:25:04.082724Z","iopub.status.idle":"2021-12-09T04:25:04.106461Z","shell.execute_reply.started":"2021-12-09T04:25:04.082682Z","shell.execute_reply":"2021-12-09T04:25:04.105804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization the Keyword Frequency","metadata":{}},{"cell_type":"code","source":"keyword_non_disaster = df_train.keyword[df_train.target==0].value_counts().reset_index()\nsns.barplot(data=keyword_non_disaster[:10], x='keyword', y='index')\nplt.title('Non-Disaster Keyword Frequency (0)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:04.11003Z","iopub.execute_input":"2021-12-09T04:25:04.111886Z","iopub.status.idle":"2021-12-09T04:25:04.485524Z","shell.execute_reply.started":"2021-12-09T04:25:04.11185Z","shell.execute_reply":"2021-12-09T04:25:04.484809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keyword_disaster = df_train.keyword[df_train.target==1].value_counts().reset_index()\nsns.barplot(data=keyword_non_disaster[:10], x='keyword', y='index')\nplt.title('Non-Disaster Keyword Frequency (0)')\nplt.xlabel('Frequency')\nplt.ylabel('Top 10 Keywords')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:04.489687Z","iopub.execute_input":"2021-12-09T04:25:04.491707Z","iopub.status.idle":"2021-12-09T04:25:04.831842Z","shell.execute_reply.started":"2021-12-09T04:25:04.491663Z","shell.execute_reply":"2021-12-09T04:25:04.831164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spilt data\nX_train, X_val, y_train, y_val = train_test_split(df_train[['text','keyword']],\n                                                    df_train.target, \n                                                    test_size=0.2, \n                                                    random_state=42)\nX_train.shape, X_val.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:04.836116Z","iopub.execute_input":"2021-12-09T04:25:04.838378Z","iopub.status.idle":"2021-12-09T04:25:04.856641Z","shell.execute_reply.started":"2021-12-09T04:25:04.838332Z","shell.execute_reply":"2021-12-09T04:25:04.855673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create TensorFlow Datasets","metadata":{}},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\nval_ds = tf.data.Dataset.from_tensor_slices((dict(X_val), y_val))\ntest_ds = tf.data.Dataset.from_tensor_slices(dict(df_test[['text','keyword']]))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:04.860791Z","iopub.execute_input":"2021-12-09T04:25:04.863019Z","iopub.status.idle":"2021-12-09T04:25:04.88892Z","shell.execute_reply.started":"2021-12-09T04:25:04.862978Z","shell.execute_reply":"2021-12-09T04:25:04.888279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\nBUFFER_SIZE = 1000\nBATCH_SIZE = 32\nRANDOM_SEED = 319\n\ndef configure_dataset(dataset, shuffle=False, test=False):\n    if shuffle:\n        dataset = dataset.cache()\\\n                        .shuffle(BUFFER_SIZE, seed=RANDOM_SEED, reshuffle_each_iteration=True)\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\n                        .prefetch(AUTOTUNE)\n    elif test:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=False)\\\n                        .prefetch(AUTOTUNE)\n    else:\n        dataset = dataset.cache()\\\n                        .batch(BATCH_SIZE, drop_remainder=True)\\\n                        .prefetch(AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:04.890073Z","iopub.execute_input":"2021-12-09T04:25:04.890355Z","iopub.status.idle":"2021-12-09T04:25:04.903071Z","shell.execute_reply.started":"2021-12-09T04:25:04.890321Z","shell.execute_reply":"2021-12-09T04:25:04.902335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a3 = configure_dataset(train_ds, shuffle=True)\ndict3 = []\nfor elem in a3:\n    dict3.append(elem[0]['text'][0])\ndict3[:10]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:04.907351Z","iopub.execute_input":"2021-12-09T04:25:04.907958Z","iopub.status.idle":"2021-12-09T04:25:05.036825Z","shell.execute_reply.started":"2021-12-09T04:25:04.907918Z","shell.execute_reply":"2021-12-09T04:25:05.036195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configure the datasets\ntrain_ds = configure_dataset(train_ds, shuffle=True)\nval_ds = configure_dataset(val_ds)\ntest_ds = configure_dataset(test_ds, test=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:05.040584Z","iopub.execute_input":"2021-12-09T04:25:05.042464Z","iopub.status.idle":"2021-12-09T04:25:05.052676Z","shell.execute_reply.started":"2021-12-09T04:25:05.042426Z","shell.execute_reply":"2021-12-09T04:25:05.052005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Free memory\ndel X_train, X_val, y_train, y_val, df_train, df_test, train_full, test_full","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:05.056997Z","iopub.execute_input":"2021-12-09T04:25:05.059306Z","iopub.status.idle":"2021-12-09T04:25:05.067506Z","shell.execute_reply.started":"2021-12-09T04:25:05.059269Z","shell.execute_reply":"2021-12-09T04:25:05.064893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classifier Model","metadata":{}},{"cell_type":"code","source":"# Bidirectional Encoder Representations from Transformers (BERT).\nbert_encoder_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\n# Text preprocessing for BERT.\nbert_preprocessor_path = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n# Token based text embedding trained on English Google News 200B corpus.\nkeyword_embedding_path = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\"","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:05.069752Z","iopub.execute_input":"2021-12-09T04:25:05.073393Z","iopub.status.idle":"2021-12-09T04:25:05.077812Z","shell.execute_reply.started":"2021-12-09T04:25:05.073353Z","shell.execute_reply":"2021-12-09T04:25:05.077096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_encoder = hub.KerasLayer(bert_encoder_path, trainable=True, name=\"BERT_Encoder\")\nbert_preprocessor = hub.KerasLayer(bert_preprocessor_path, name=\"BERT_Preprocessor\")\nnnlm_embed = hub.KerasLayer(keyword_embedding_path, name=\"NNLM_Embedding\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:05.08211Z","iopub.execute_input":"2021-12-09T04:25:05.082343Z","iopub.status.idle":"2021-12-09T04:25:29.674055Z","shell.execute_reply.started":"2021-12-09T04:25:05.082312Z","shell.execute_reply":"2021-12-09T04:25:29.673043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kernel_initializer = tf.keras.initializers.GlorotNormal(seed=319)\n# Model function\ndef create_model():\n    # Keyword Branch\n    text_input = Input(shape=(), dtype=tf.string, name=\"text\")\n    encoder_inputs = bert_preprocessor(text_input)\n    encoder_outputs = bert_encoder(encoder_inputs)\n    # Pooled output\n    pooled_output = encoder_outputs[\"pooled_output\"]\n    bert_branch = Dropout(0.1,\n                          seed=319,\n                          name=\"BERT_Dropout\")(pooled_output)\n    # Construct keyword layers\n    keyword_input = Input(shape=(), dtype=tf.string, name='keyword')\n    keyword_embed = nnlm_embed(keyword_input)\n    keyword_flat = Flatten(name=\"Keyword_Flatten\")(keyword_embed)\n    keyword_dense1 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense1\"\n                         )(keyword_flat)\n    keyword_branch1 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout1'\n                            )(keyword_dense1)\n    keyword_dense2 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense2\"\n                         )(keyword_branch1)\n    keyword_branch2 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout2'\n                            )(keyword_dense2)\n    keyword_dense3 = Dense(128, \n                          activation='relu',\n                          kernel_initializer=kernel_initializer,\n                          kernel_regularizer=regularizers.l2(1e-4),\n                          name=\"Keyword_Dense3\"\n                         )(keyword_branch2)\n    keyword_branch3 = Dropout(0.5,\n                             seed=319,\n                             name='Keyword_dropout3'\n                            )(keyword_dense3)\n    \n    # Merge the layers and classify\n    merge = concatenate([bert_branch, keyword_branch3], name=\"Concatenate\")\n    dense = Dense(128, \n                  activation='relu',\n                  kernel_initializer=kernel_initializer,\n                  kernel_regularizer=regularizers.l2(1e-4), \n                  name=\"Merged_Dense\")(merge)\n    dropout = Dropout(0.5,\n                      seed=319,\n                      name=\"Merged_Dropout\"\n                     )(dense)\n    clf = Dense(1,\n                activation=\"sigmoid\", \n                kernel_initializer=kernel_initializer,\n                name=\"Classifier\"\n               )(dropout)\n    return Model([text_input, keyword_input], \n                 clf, \n                 name=\"BERT_Classifier\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:29:05.629025Z","iopub.execute_input":"2021-12-09T04:29:05.629383Z","iopub.status.idle":"2021-12-09T04:29:05.641939Z","shell.execute_reply.started":"2021-12-09T04:29:05.629343Z","shell.execute_reply":"2021-12-09T04:29:05.641261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_classifier = create_model()\nbert_classifier.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:29:11.173008Z","iopub.execute_input":"2021-12-09T04:29:11.173563Z","iopub.status.idle":"2021-12-09T04:29:11.361598Z","shell.execute_reply.started":"2021-12-09T04:29:11.173521Z","shell.execute_reply":"2021-12-09T04:29:11.360861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keras.utils.plot_model(bert_classifier, \n                      show_shapes=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:29:18.611904Z","iopub.execute_input":"2021-12-09T04:29:18.612475Z","iopub.status.idle":"2021-12-09T04:29:19.705092Z","shell.execute_reply.started":"2021-12-09T04:29:18.612434Z","shell.execute_reply":"2021-12-09T04:29:19.704028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AdamW Optimizer","metadata":{}},{"cell_type":"code","source":"EPOCHS = 3\nLEARNING_RATE = 5e-5\n\nSTEPS_PER_EPOCH = int(train_ds.unbatch().cardinality().numpy() / BATCH_SIZE)\nVAL_STEPS = int(val_ds.unbatch().cardinality().numpy() / BATCH_SIZE)\n# Calculate the train and warmup steps for the optimizer\nTRAIN_STEPS = STEPS_PER_EPOCH * EPOCHS\nWARMUP_STEPS = int(TRAIN_STEPS * 0.1)\n\nadamw_optimizer = create_optimizer(\n    init_lr=LEARNING_RATE,\n    num_train_steps=TRAIN_STEPS,\n    num_warmup_steps=WARMUP_STEPS,\n    optimizer_type='adamw'\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:29:26.670545Z","iopub.execute_input":"2021-12-09T04:29:26.671128Z","iopub.status.idle":"2021-12-09T04:29:26.698611Z","shell.execute_reply.started":"2021-12-09T04:29:26.671085Z","shell.execute_reply":"2021-12-09T04:29:26.697849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"STEPS_PER_EPOCH, VAL_STEPS, TRAIN_STEPS, WARMUP_STEPS","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:29:33.543042Z","iopub.execute_input":"2021-12-09T04:29:33.543639Z","iopub.status.idle":"2021-12-09T04:29:33.550584Z","shell.execute_reply.started":"2021-12-09T04:29:33.543598Z","shell.execute_reply":"2021-12-09T04:29:33.549837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model again","metadata":{}},{"cell_type":"code","source":"bert_classifier.compile(loss=BinaryCrossentropy(from_logits=True),\n                   optimizer=adamw_optimizer, \n                   metrics=[BinaryAccuracy(name=\"accuracy\")]\n                  )\nhistory = bert_classifier.fit(train_ds, \n                         epochs=EPOCHS,\n                         steps_per_epoch=STEPS_PER_EPOCH,\n                         validation_data=val_ds,\n                         validation_steps=VAL_STEPS\n                        )","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:29:37.077111Z","iopub.execute_input":"2021-12-09T04:29:37.07783Z","iopub.status.idle":"2021-12-09T04:39:02.455882Z","shell.execute_reply.started":"2021-12-09T04:29:37.077795Z","shell.execute_reply":"2021-12-09T04:39:02.455171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=6 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">6. Make a Submission</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"def submission(model, test):\n    sample_sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n    predictions =  model.predict(test)\n    y_preds = [ int(i) for i in np.rint(predictions)]\n    sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_preds})\n    sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:31.107598Z","iopub.status.idle":"2021-12-09T04:25:31.108004Z","shell.execute_reply.started":"2021-12-09T04:25:31.107779Z","shell.execute_reply":"2021-12-09T04:25:31.107801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission(bert_classifier, test_ds)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T04:25:31.109683Z","iopub.status.idle":"2021-12-09T04:25:31.110174Z","shell.execute_reply.started":"2021-12-09T04:25:31.10992Z","shell.execute_reply":"2021-12-09T04:25:31.109955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=7 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">7. References</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"[Hugging Face Transformers Fine-Tunning DistilBert for Binary Classification Tasks](https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379)\n\n[Keras functional API](https://keras.io/guides/functional_api/)\n\n[Distil Bert](https://huggingface.co/transformers/model_doc/distilbert.html)\n\n[Tensorflow Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n\n[BERT in TFHub](https://tfhub.dev/google/collections/bert)\n\n[TensorFlow NLP Modelling Toolkit](https://github.com/tensorflow/models/tree/master/official/nlp)\n\n[NLP With BERT from Tendorflow](https://www.tensorflow.org/text/tutorials/fine_tune_bert)\n\n[NLP Optimization](https://github.com/tensorflow/models/blob/master/official/nlp/optimization.py)","metadata":{}},{"cell_type":"markdown","source":"# If you like this kernel, please upvote and tell me your thought. Thank you @@","metadata":{}}]}