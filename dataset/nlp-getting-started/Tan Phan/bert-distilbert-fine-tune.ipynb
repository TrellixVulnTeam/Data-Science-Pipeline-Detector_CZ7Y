{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Visualization\n!pip install dataprep | grep -v 'already satisfied'\nfrom dataprep.eda import plot, plot_diff, plot_correlation, create_report\n\n# Preprocessing and Modelling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras tuner\n!pip install -q -U keras-tuner\nimport keras_tuner as kt\n# Warning\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:36:10.862905Z","iopub.execute_input":"2021-11-19T08:36:10.863586Z","iopub.status.idle":"2021-11-19T08:36:50.283167Z","shell.execute_reply.started":"2021-11-19T08:36:10.863444Z","shell.execute_reply":"2021-11-19T08:36:50.282356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=0></a>\n## <p style=\"background-color:lightblue; font-family:newtimeroman; font-size:120%; text-align:left; border-radius: 15px 50px;\">Table of Content</p>\n* [0. Introduction and updates](#0)\n* [1. Loading Data ðŸ’Ž](#1)\n* [2. EDA ðŸ“Š](#2)\n* [3. Data Preprocessing](#3)\n* [4. Vectorization](#4)\n    * [4.1 Common Vectorizer Usage](#4.1)\n    * [4.2 If-Idf Term Weightings](#4.2)\n* [5. Transfer Learning with Hugging Face](#5)\n    * [5.1 Tokenization](#5.1)\n    * [5.2 Defining a Model Architecture](#5.2)\n    * [5.3 Training Classification Layer Weights](#5.3)\n    * [5.4 Fine-tuning DistilBert and Training All Weights](#5.4)\n* [6. Make a Submission](#6)\n* [7. References](#7)","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">0. Introduction and update </p>\n# Introduction: \nIn this kernel, beside the general steps working with text data as EDA, preprocessing. The workflow in Modelling can divided into 3 main stages:\n1. Defining a Model Architecture.\n2. Training Classification Layer Weights.\n3. Fine-tuning DistilBert and Tranining All Weights.\n\n# Update: \nCurrent Version\n1. Use Keras-tuner to find the optimized learning rate for main model.\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data ðŸ’Ž</p>\n\nJust load the dataset and global variables for colors and so on.\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"train_full = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_full = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\nprint('Training Set Shape = {}'.format(train_full.shape))\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()/2**20))\n\nprint('Test Set Shape = {}'.format(test_full.shape))\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()/2**20))","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:36:50.285066Z","iopub.execute_input":"2021-11-19T08:36:50.285523Z","iopub.status.idle":"2021-11-19T08:36:50.400358Z","shell.execute_reply.started":"2021-11-19T08:36:50.285464Z","shell.execute_reply":"2021-11-19T08:36:50.399637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. EDA ðŸ“Š</p>\n\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"plot(train_full)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:36:50.402003Z","iopub.execute_input":"2021-11-19T08:36:50.402334Z","iopub.status.idle":"2021-11-19T08:36:51.433645Z","shell.execute_reply.started":"2021-11-19T08:36:50.402292Z","shell.execute_reply":"2021-11-19T08:36:51.432641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_report(train_full)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:36:51.435528Z","iopub.execute_input":"2021-11-19T08:36:51.435797Z","iopub.status.idle":"2021-11-19T08:36:56.207212Z","shell.execute_reply.started":"2021-11-19T08:36:51.435767Z","shell.execute_reply":"2021-11-19T08:36:56.206168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot(train_full, 'text')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:36:56.208711Z","iopub.execute_input":"2021-11-19T08:36:56.208986Z","iopub.status.idle":"2021-11-19T08:36:57.652478Z","shell.execute_reply.started":"2021-11-19T08:36:56.208953Z","shell.execute_reply":"2021-11-19T08:36:57.651683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_report(train_full.text)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:36:57.653478Z","iopub.execute_input":"2021-11-19T08:36:57.654109Z","iopub.status.idle":"2021-11-19T08:36:59.260054Z","shell.execute_reply.started":"2021-11-19T08:36:57.654075Z","shell.execute_reply":"2021-11-19T08:36:59.25878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Range from 120 to 140 characters is the most common in tweet.","metadata":{}},{"cell_type":"code","source":"create_report(train_full.target)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:36:59.261475Z","iopub.execute_input":"2021-11-19T08:36:59.262281Z","iopub.status.idle":"2021-11-19T08:37:00.290296Z","shell.execute_reply.started":"2021-11-19T08:36:59.262186Z","shell.execute_reply":"2021-11-19T08:37:00.289645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset is balanced","metadata":{}},{"cell_type":"code","source":"plot(train_full, \"text\", \"target\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:00.291374Z","iopub.execute_input":"2021-11-19T08:37:00.291642Z","iopub.status.idle":"2021-11-19T08:37:00.576738Z","shell.execute_reply.started":"2021-11-19T08:37:00.291611Z","shell.execute_reply":"2021-11-19T08:37:00.575654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = train_full.text[train_full.target == 0]\ndf2 = train_full.text[train_full.target == 1]\nplot_diff([df1, df2])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:00.578035Z","iopub.execute_input":"2021-11-19T08:37:00.578296Z","iopub.status.idle":"2021-11-19T08:37:01.133353Z","shell.execute_reply.started":"2021-11-19T08:37:00.578264Z","shell.execute_reply":"2021-11-19T08:37:01.132438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3'></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Pre-processing </p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.\n\n\n\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"# Main technics I used in this data\n    * [3.1] Remove 92 duplicated rows\n    * [3.2] Cleaning text\n    * [3.3] Spelling Checker\n    * [3.4] Remove Stemming\n #### Step 3.3 spends a lot time (around 4000s in 4536s in total). \n #### So, I splits Data Preprocessing into [another kernel](https://www.kaggle.com/phanttan/disastertweet-prepareddata). \n #### And the prepared data to save in to [new dataset](https://www.kaggle.com/phanttan/disastertweet-prepared2)\n #### I am so appreciate to you for using/upvoting it.\n","metadata":{}},{"cell_type":"code","source":"# free some space\ndel train_full, test_full\n\n# Read commited-dataset\ndf_train = pd.read_csv(\"/kaggle/input/disastertweet-prepared2/train_prepared.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/disastertweet-prepared2/test_prepared.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:01.137017Z","iopub.execute_input":"2021-11-19T08:37:01.137372Z","iopub.status.idle":"2021-11-19T08:37:01.19727Z","shell.execute_reply.started":"2021-11-19T08:37:01.137334Z","shell.execute_reply":"2021-11-19T08:37:01.196129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=4 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4. Vectorization</p>\n\nThree steps using the Bag-of-words (BOW) model:\n1. Term frequency : count occurrences of word in sentence\n2. Inverse document frequency: \n3. L2 Norm\nReference : https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"<a id=4.1 ></a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4.1 Common Vectorizer Usage</p>\nReference: https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"# Instantiate the Vectorizer\nvect = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0, max_df=0.9, max_features=100)\ndf_dtm = vect.fit_transform(df_train)\ndf_dtm.toarray()[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:01.198714Z","iopub.execute_input":"2021-11-19T08:37:01.199216Z","iopub.status.idle":"2021-11-19T08:37:01.209062Z","shell.execute_reply.started":"2021-11-19T08:37:01.19918Z","shell.execute_reply":"2021-11-19T08:37:01.20817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=4.2 ></a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">4.2 TF-IDF</p>\nReference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=0, max_df=0.98, max_features=100)\ndf_ifidf= tfidf_vect.fit_transform(df_train)\ndf_ifidf.toarray()[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:01.210309Z","iopub.execute_input":"2021-11-19T08:37:01.210581Z","iopub.status.idle":"2021-11-19T08:37:01.229816Z","shell.execute_reply.started":"2021-11-19T08:37:01.210552Z","shell.execute_reply":"2021-11-19T08:37:01.228631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=5 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">5. Transfer Learning with Hugging Face</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"BERT(*Bi-directional Encoder Representations from Transformers*)\n\n    - GLUE Score to 80.5%\n    - MultiNLI accuracy to 86.7%\n    - SQuAD v1.1 question answering Test F1 to 93.3\n    - SQuAD v2.0 Test F1 to 83.1","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE=64","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=5.1 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.1 Tokenizing Text</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:06.669022Z","iopub.execute_input":"2021-11-19T08:37:06.669526Z","iopub.status.idle":"2021-11-19T08:37:10.573653Z","shell.execute_reply.started":"2021-11-19T08:37:06.669467Z","shell.execute_reply":"2021-11-19T08:37:10.57257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 0\n# Find the longest sentence \nfor sentence in pd.concat([df_train.text, df_test.text]):\n    if len(sentence) > max_len: # number of word in a sentence tokenizer is greater max_len\n        max_len = len(sentence)\nmax_len","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:10.574984Z","iopub.execute_input":"2021-11-19T08:37:10.575238Z","iopub.status.idle":"2021-11-19T08:37:10.587609Z","shell.execute_reply.started":"2021-11-19T08:37:10.575208Z","shell.execute_reply":"2021-11-19T08:37:10.586853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using for Fine_tuning\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(df_train.text, \n                                                                    df_train.target, \n                                                                    test_size=0.2, \n                                                                    random_state=42)\n# Use padding with max_len to get train/val/test with same dimension\ntrain_encodings = tokenizer(train_texts.tolist(), truncation=True, max_length=max_len, padding=\"max_length\", return_tensors='tf')\nval_encodings = tokenizer(val_texts.tolist(), truncation=True, max_length=max_len, padding=\"max_length\", return_tensors='tf')\ntest_encodings = tokenizer(df_test.text.fillna('').tolist(), truncation=True, max_length=max_len, padding=\"max_length\", return_tensors='tf')\n\nprint(train_encodings)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:10.588992Z","iopub.execute_input":"2021-11-19T08:37:10.589227Z","iopub.status.idle":"2021-11-19T08:37:11.880941Z","shell.execute_reply.started":"2021-11-19T08:37:10.589201Z","shell.execute_reply":"2021-11-19T08:37:11.879928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using for Keras-tuner\ntrain_encodings_keras = tokenizer(df_train.text.tolist(), truncation=True, max_length=max_len, padding=\"max_length\", return_tensors=\"tf\")\ntrain_encodings_keras","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:11.882337Z","iopub.execute_input":"2021-11-19T08:37:11.8826Z","iopub.status.idle":"2021-11-19T08:37:12.513596Z","shell.execute_reply.started":"2021-11-19T08:37:11.88257Z","shell.execute_reply":"2021-11-19T08:37:12.512606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_encodings_keras['input_ids']","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:12.515054Z","iopub.execute_input":"2021-11-19T08:37:12.515543Z","iopub.status.idle":"2021-11-19T08:37:12.53118Z","shell.execute_reply.started":"2021-11-19T08:37:12.515475Z","shell.execute_reply":"2021-11-19T08:37:12.530387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode Training Data\nX_train_ids = train_encodings['input_ids'].numpy()\nX_train_attention = train_encodings['attention_mask'].numpy\n# Encode Validating Data\nX_val_ids = val_encodings['input_ids'].numpy()\nX_val_attention = val_encodings['attention_mask'].numpy()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:12.532426Z","iopub.execute_input":"2021-11-19T08:37:12.533163Z","iopub.status.idle":"2021-11-19T08:37:12.549724Z","shell.execute_reply.started":"2021-11-19T08:37:12.53311Z","shell.execute_reply":"2021-11-19T08:37:12.548613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tf_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\ntrain_tf_dataset = train_tf_dataset.shuffle(len(train_encodings)).batch(BATCH_SIZE)\n\neval_tf_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels))\neval_tf_dataset = eval_tf_dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:12.55099Z","iopub.execute_input":"2021-11-19T08:37:12.551721Z","iopub.status.idle":"2021-11-19T08:37:12.570029Z","shell.execute_reply.started":"2021-11-19T08:37:12.551674Z","shell.execute_reply":"2021-11-19T08:37:12.569178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=5.2 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.2 Define a model based in DistilBERT</p>\n\nIn this part, I try a lighter model than BERT: \n\n[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)\n\n[Content](#0)\n","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://i.ibb.co/4tzyG1P/Bert-Classification.png\" alt=\"Bert-Classification\" border=\"0\">","metadata":{}},{"cell_type":"markdown","source":"# Initialize the Base Model","metadata":{}},{"cell_type":"code","source":"from transformers import TFDistilBertModel, DistilBertConfig\n\nBERT_DROPOUT = 0.2\nBERT_ATT_DROPOUT = 0.2\n \n# Configure DistilBERT's initialization\nconfig = DistilBertConfig(dropout=BERT_DROPOUT, \n                          attention_dropout=BERT_ATT_DROPOUT, \n                          output_hidden_states=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:12.571447Z","iopub.execute_input":"2021-11-19T08:37:12.571691Z","iopub.status.idle":"2021-11-19T08:37:12.597622Z","shell.execute_reply.started":"2021-11-19T08:37:12.571665Z","shell.execute_reply":"2021-11-19T08:37:12.596614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add a Classification Head","metadata":{}},{"cell_type":"code","source":"# Model function\ndef create_model(transformer):\n    \n    # Make Transformer layers untrainable\n    for layer in transformer.layers:\n        layer.trainable = False\n    # Input layers\n    input_ids_layer = keras.Input(shape =(max_len,), \n                           dtype=tf.int32, \n                           name='input_ids') \n    input_attention_layer = keras.Input(shape=(max_len,),\n                                    dtype=tf.int32, \n                                    name='attention_mask')  \n    \n    # DistilBERT outputs a tuple where the first element at index 0\n    # represents the hidden-state at the output of the model's last layer.\n    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n    \n    # We only care about DistilBERT's output for the [CLS] token, \n    # which is located at index 0 of every encoded sequence.  \n    # Splicing out the [CLS] tokens gives us 2D data.\n    cls_token = last_hidden_state[:, 0, :]\n    # Hidden layers\n    output = keras.layers.Dense(256,\n                                kernel_initializer=keras.initializers.GlorotUniform(seed=1),  \n                                kernel_constraint=None,\n                                bias_initializer='zeros',\n                                activation='relu')(cls_token)\n    output = keras.layers.Dropout(0.2)(output)\n    output = keras.layers.Dense(64, activation = 'relu')(output)\n    # Output layer\n    output = keras.layers.Dense(1, activation='sigmoid')(output)\n    # Define the model \n    model = keras.Model([input_ids_layer, input_attention_layer],\n                       output)\n    model.summary()\n    keras.utils.plot_model(model)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:12.598898Z","iopub.execute_input":"2021-11-19T08:37:12.599195Z","iopub.status.idle":"2021-11-19T08:37:12.60932Z","shell.execute_reply.started":"2021-11-19T08:37:12.599165Z","shell.execute_reply":"2021-11-19T08:37:12.608651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distilBERT_NN_tuner(hp):\n\n    # The bare, pre-trained DistilBERT transformer model outputting raw hidden-states \n    # and without any specific head on top.\n    distilBERT = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\n    model = create_model(distilBERT)\n    # Using learning_rate is recommendated from paper BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding\n    hp_learning_rate = hp.Choice('learning_rate', values=[5e-5, 4e-5 , 3e-5, 2e-5])\n    optimizer = keras.optimizers.Adam(learning_rate=hp_learning_rate)\n    # Compile the model\n    model.compile(optimizer, \n                  loss=\"binary_crossentropy\",\n                  metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:37:12.6102Z","iopub.execute_input":"2021-11-19T08:37:12.610837Z","iopub.status.idle":"2021-11-19T08:37:12.625632Z","shell.execute_reply.started":"2021-11-19T08:37:12.610806Z","shell.execute_reply":"2021-11-19T08:37:12.624651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=5.3 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.3 Training Classification Layer Weights</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"# Using Keras-Tuner to find the best Learning-rate","metadata":{}},{"cell_type":"markdown","source":"### RandomSearch","metadata":{}},{"cell_type":"code","source":"tuner = kt.RandomSearch(distilBERT_NN_tuner,\n                objective='val_accuracy')\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n                                           patience=4, \n                                           restore_best_weights=True)\ntuner.search(train_tf_dataset,\n                epochs=25,\n                batch_size=BATCH_SIZE,\n                validation_data=eval_tf_dataset,\n                callbacks = [early_stop],\n                verbose=2)\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(f\"\"\"The hyperparameter search is complete. The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\"\"\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:43:08.279726Z","iopub.execute_input":"2021-11-19T08:43:08.280902Z","iopub.status.idle":"2021-11-19T08:47:54.39368Z","shell.execute_reply.started":"2021-11-19T08:43:08.280842Z","shell.execute_reply":"2021-11-19T08:47:54.391957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running model with the best Learning Rate","metadata":{}},{"cell_type":"code","source":"# Running with specific number\nDistilBERTmodel = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)\nmodel = create_model(DistilBERTmodel)\n# Compile the model\nmodel.compile(keras.optimizers.Adam(lr=best_hps.get('learning_rate')), \n              loss=\"binary_crossentropy\",\n              metrics=['accuracy'])\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=4, restore_best_weights=True)\n\ntrain_history1 = model.fit(train_tf_dataset,\n                           epochs=25,\n                           batch_size=BATCH_SIZE,\n                           validation_data=eval_tf_dataset,\n                           callbacks = [early_stop],\n                           verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:42:15.161317Z","iopub.status.idle":"2021-11-19T08:42:15.162416Z","shell.execute_reply.started":"2021-11-19T08:42:15.16208Z","shell.execute_reply":"2021-11-19T08:42:15.162117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=5.4 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">5.4 Fine-tune DistilBERT model and Training all Weights</p>\n\n    1. Unfrezzing layer weights in DistilBERT model\n    2. Using lower learning rate to prevent large update to pre-trained weights\n    3. Recompile model again\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"### Unfreeze all layer weights in distilBERT and make available for training","metadata":{}},{"cell_type":"code","source":"for layer in DistilBERTmodel.layers:\n    layer.Trainable = True","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:42:15.163779Z","iopub.status.idle":"2021-11-19T08:42:15.164151Z","shell.execute_reply.started":"2021-11-19T08:42:15.163965Z","shell.execute_reply":"2021-11-19T08:42:15.163984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Recompile model after unfreezing\n\nThe lower learning-rate is chosen because of preventing the major update to pre-trained weights.","metadata":{}},{"cell_type":"code","source":"model.compile(keras.optimizers.Adam(lr=1e-5), \n              loss=\"binary_crossentropy\",\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:42:15.165443Z","iopub.status.idle":"2021-11-19T08:42:15.165795Z","shell.execute_reply.started":"2021-11-19T08:42:15.165623Z","shell.execute_reply":"2021-11-19T08:42:15.165641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the model again","metadata":{}},{"cell_type":"code","source":"early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=2, restore_best_weights=True)\ntrain_history2 = model.fit(train_tf_dataset,\n                               epochs=25,\n                               batch_size=BATCH_SIZE,\n                               validation_data=eval_tf_dataset,\n                               callbacks = [early_stop],\n                               verbose=2 )","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:42:15.169891Z","iopub.status.idle":"2021-11-19T08:42:15.170488Z","shell.execute_reply.started":"2021-11-19T08:42:15.17017Z","shell.execute_reply":"2021-11-19T08:42:15.1702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=6 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">6. Make a Submission</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"code","source":"def submission(model, test):\n    sample_sub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n    predictions =  model.predict(test.data, batch_size=BATCH_SIZE, verbose =1)\n    y_preds = [ int(i) for i in np.rint(predictions)]\n    sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_preds})\n    sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:42:15.171923Z","iopub.status.idle":"2021-11-19T08:42:15.172453Z","shell.execute_reply.started":"2021-11-19T08:42:15.172163Z","shell.execute_reply":"2021-11-19T08:42:15.17219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission(model, test_encodings)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:42:15.173763Z","iopub.status.idle":"2021-11-19T08:42:15.174243Z","shell.execute_reply.started":"2021-11-19T08:42:15.174007Z","shell.execute_reply":"2021-11-19T08:42:15.174034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T08:42:15.175999Z","iopub.status.idle":"2021-11-19T08:42:15.176564Z","shell.execute_reply.started":"2021-11-19T08:42:15.176265Z","shell.execute_reply":"2021-11-19T08:42:15.176292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=7 ></a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">7. References</p>\n\n[Content](#0)","metadata":{}},{"cell_type":"markdown","source":"[Hugging Face Transformers Fine-Tunning DistilBert for Binary Classification Tasks](https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379)\n\n[Keras Tuner](https://keras.io/keras_tuner)\n\n[Distil Bert](https://huggingface.co/transformers/model_doc/distilbert.html)\n","metadata":{}},{"cell_type":"markdown","source":"# If you like this kernel, please upvote and tell me your thought. Thank you @@","metadata":{}}]}