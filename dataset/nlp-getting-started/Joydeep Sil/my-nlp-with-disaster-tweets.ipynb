{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Loading data\ntrain_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\", na_values = 'nan')\ntest_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\", na_values = 'nan')\nsubmission_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacing NaN with Missing\ntrain_data = train_data.replace(np.nan,'missing',regex=True)\ntest_data = test_data.replace(np.nan,'missing',regex=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef substituteUrls(inputString):\n    inputString = str(inputString)\n    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n    url = re.findall(regex,inputString)\n    if(len(url)>0):\n        return \"\"\n    else:\n        return inputString","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Expanding Custom Words\ndef customWordsExpaned(inputString):\n    inputString = str(inputString).lower()\n    if(inputString == \"i'll\"):\n        return \"i will\"\n    elif(inputString == \"you'll\"):\n        return \"you will\"\n    elif(inputString == \"he'll\"):\n        return \"he will\"\n    elif(inputString == \"she'll\"):\n        return \"she will\"\n    elif(inputString == \"it'll\"):\n        return \"it will\"\n    elif(inputString == \"we'll\"):\n        return \"we will\"\n    elif(inputString == \"they'll\"):\n        return \"they will\"\n    elif(inputString == \"i'd\"):\n        return \"i would\"\n    elif(inputString == \"you'd\"):\n        return \"you would\"\n    elif(inputString == \"he'd\"):\n        return \"he would\"\n    elif(inputString == \"she'd\"):\n        return \"she would\"\n    elif(inputString == \"we'd\"):\n        return \"we would\"\n    elif(inputString == \"they'd\"):\n        return \"they would\"\n    elif(inputString == \"i've\"):\n        return \"i have\"\n    elif(inputString == \"you've\"):\n        return \"you have\"\n    elif(inputString == \"we've\"):\n        return \"we have\"\n    elif(inputString == \"they've\"):\n        return \"they have\"\n    elif(inputString == \"could've\"):\n        return \"could have\"\n    elif(inputString == \"should've\"):\n        return \"should have\"\n    elif(inputString == \"would've\"):\n        return \"would have\"\n    elif(inputString == \"i'm\"):\n        return \"i am\"\n    elif(inputString == \"you're\"):\n        return \"you are\"\n    elif(inputString == \"he's\"):\n        return \"he is\"\n    elif(inputString == \"she's\"):\n        return \"she is\"\n    elif(inputString == \"it's\"):\n        return \"it is\"\n    elif(inputString == \"there's\"):\n        return \"there is\"\n    elif(inputString == \"that's\"):\n        return \"that is\"\n    elif(inputString == \"who's\"):\n        return \"who is\"\n    elif(inputString == \"what's\"):\n        return \"what is\"\n    elif(inputString == \"where's\"):\n        return \"where is\"\n    elif(inputString == \"how's\"):\n        return \"how is\"\n    elif(inputString == \"we're\"):\n        return \"we are\"\n    elif(inputString == \"they're\"):\n        return \"they are\"\n    elif(inputString == \"let's\"):\n        return \"let us\"\n    elif(inputString == \"y'all\"):\n        return \"you all\"\n    elif(inputString == \"didn't\"):\n        return \"did not\"\n    elif(inputString == \"ain't\"):\n        return \"am not\"\n    elif(inputString == \"don't\"):\n        return \"do not\"\n    elif(inputString == \"haven't\"):\n        return \"have not\"\n    elif(inputString == \"weren't\"):\n        return \"were not\"\n    elif(inputString == \"can't\"):\n        return \"cannot\"\n    elif(inputString == \"doesn't\"):\n        return \"does not\"\n    elif(inputString == \"won't\"):\n        return \"will not\"\n    elif(inputString == \"wasn't\"):\n        return \"was not\"\n    elif(inputString == \"aren't\"):\n        return \"are not\"\n    elif(inputString == \"isn't\"):\n        return \"is not\"\n    else:\n        return inputString","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing Stopwords from Text column\nfrom nltk.corpus import stopwords\n#nltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import STOPWORDS\ndef removeStopWords(text):\n    text = str(text).lower()\n    text_tokens = word_tokenize(text)\n    tokens_without_sw1 = [word for word in text_tokens if not word in stopwords.words()]\n    tokens_without_sw2 = [word for word in tokens_without_sw1 if not word in STOPWORDS]\n    return \" \".join(tokens_without_sw2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tweetCleaner(tweet):\n    #tweet = \"Severe Thunderstorm 7 Miles North of Arcadia Moving SE At 20 MPH. 60 MPH Wind Gusts and Large Hail. Hail... #okwx http://t.co/WEd69e4KRg\"\n    tweet = str(tweet)\n    tweet_list = re.split(r'[;,?.\\s]', tweet)\n    updated_list = []\n    for item in tweet_list:\n        if(len(item)>0):\n            #item = substituteUrls(item)\n            item = item.replace(\"#\",\"\")\n            if(re.search(\"^[A-Za-z]+$\", item)):\n                item = customWordsExpaned(item)\n                updated_list.append(item)\n    updated_tweet = removeStopWords(\" \".join(updated_list))\n    return updated_tweet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a second vector for classification model later.\ndisasterVec2 = train_data[\"keyword\"].to_numpy()\n\n#dropping keyword and location\ntrain_data.drop(columns =['keyword','location'], axis=1)\ntest_data.drop(columns =['keyword','location'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature Engineering - Adding column \"clean_text\"\ntrain_data[\"clean_text\"] = train_data[\"text\"].apply(tweetCleaner)\ntest_data[\"clean_text\"] = test_data[\"text\"].apply(tweetCleaner)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now that we have clean text, we no longer need the text column\ntrain_data.drop(columns =['text'], axis=1)\ntest_data.drop(columns =['text'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncount_vectorizer1 = feature_extraction.text.CountVectorizer() # will use unique words in Clean text column\ncount_vectorizer2 = feature_extraction.text.CountVectorizer() # will use unique words in keyword column (disasterVec2)\ncount_vectorizer3 = feature_extraction.text.CountVectorizer() # will use a custom list (disasterVec1)\n\ntfid_vectorizer1 = feature_extraction.text.TfidfVectorizer() # will use unique words in Clean text column\ntfid_vectorizer2 = feature_extraction.text.TfidfVectorizer() # will use unique words in keyword column (disasterVec2)\ntfid_vectorizer3 = feature_extraction.text.TfidfVectorizer() # will use a custom list (disasterVec1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Count Vectorizer 1\ntrain_vectors_cv1 = count_vectorizer1.fit_transform(train_data[\"clean_text\"])\ntest_vectors_cv1 = count_vectorizer1.transform(test_data[\"clean_text\"])\n\n#Using Tfid Vectorizer 1\ntrain_vectors_tfid1 = tfid_vectorizer1.fit_transform(train_data[\"clean_text\"])\ntest_vectors_tfid1 = tfid_vectorizer1.transform(test_data[\"clean_text\"])\n\n\n# Our vectors are really big, so we want to push our model's weights toward 0 without completely discounting different words\n#Let's use Ridge regression here \nclf_cv1 = linear_model.RidgeClassifier(random_state=0, solver='sparse_cg') # for Count Vectorizer\nclf_tfid1 = linear_model.RidgeClassifier(random_state=0, solver='sparse_cg') # for Tfid Vectorizer\n\nscores_cv1 = model_selection.cross_val_score(clf_cv1, train_vectors_cv1, train_data[\"target\"], cv=3, scoring=\"f1\")\nprint(\"F1 Scores for Count Vectorizer: \", scores_cv1)\n\nclf_cv1.fit(train_vectors_cv1, train_data[\"target\"])\nsubmission_cv1 = pd.DataFrame(columns =['id','target'])\nsubmission_cv1[\"id\"] = submission_data[\"id\"]\nsubmission_cv1[\"target\"] = clf_cv1.predict(test_vectors_cv1)\n\n\nscores_tfid1 = model_selection.cross_val_score(clf_tfid1, train_vectors_tfid1, train_data[\"target\"], cv=3, scoring=\"f1\")\nprint(\"F1 Scores for Tfid Vectorizer: \", scores_tfid1)\n\nclf_tfid1.fit(train_vectors_tfid1, train_data[\"target\"])\nsubmission_tfid1 = pd.DataFrame(columns =['id','target'])\nsubmission_tfid1[\"id\"] = submission_data[\"id\"]\nsubmission_tfid1[\"target\"] = clf_tfid1.predict(test_vectors_tfid1)\n\nprint(\"Classification Report - CV1\")\nprint(classification_report(submission_data[\"target\"],submission_cv1[\"target\"]))\nprint(\"Classification Report - TFID1\")\nprint(classification_report(submission_data[\"target\"],submission_tfid1[\"target\"]))\n\nprint(\"Confusion Matrix - CV1\")\nprint(confusion_matrix(submission_data[\"target\"],submission_cv1[\"target\"]))\nprint(\"Confusion Matrix - TFID1\")\nprint(confusion_matrix(submission_data[\"target\"],submission_tfid1[\"target\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Count Vectorizer 2\ncount_vectorizer2.fit(disasterVec2)\ntrain_vectors_cv2 = count_vectorizer2.transform(train_data[\"clean_text\"])\ntest_vectors_cv2 = count_vectorizer2.transform(test_data[\"clean_text\"])\n\n#Using Tfid Vectorizer 2\ntfid_vectorizer2.fit(disasterVec2)\ntrain_vectors_tfid2 = tfid_vectorizer2.transform(train_data[\"clean_text\"])\ntest_vectors_tfid2 = tfid_vectorizer2.transform(test_data[\"clean_text\"])\n\n# Our vectors are really big, so we want to push our model's weights toward 0 without completely discounting different words\n#Let's use Ridge regression here \nclf_cv2 = linear_model.RidgeClassifier(random_state=0, solver='sparse_cg') # for Count Vectorizer\nclf_tfid2 = linear_model.RidgeClassifier(random_state=0, solver='sparse_cg') # for Tfid Vectorizer\n\nscores_cv2 = model_selection.cross_val_score(clf_cv2, train_vectors_cv2, train_data[\"target\"], cv=3, scoring=\"f1\")\nprint(\"F1 Scores for Count Vectorizer: \", scores_cv2)\n\nclf_cv2.fit(train_vectors_cv2, train_data[\"target\"])\nsubmission_cv2 = pd.DataFrame(columns =['id','target'])\nsubmission_cv2[\"id\"] = submission_data[\"id\"]\nsubmission_cv2[\"target\"] = clf_cv2.predict(test_vectors_cv2)\n\n\nscores_tfid2 = model_selection.cross_val_score(clf_tfid2, train_vectors_tfid2, train_data[\"target\"], cv=3, scoring=\"f1\")\nprint(\"F1 Scores for Tfid Vectorizer: \", scores_tfid2)\n\nclf_tfid2.fit(train_vectors_tfid2, train_data[\"target\"])\nsubmission_tfid2 = pd.DataFrame(columns =['id','target'])\nsubmission_tfid2[\"id\"] = submission_data[\"id\"]\nsubmission_tfid2[\"target\"] = clf_tfid2.predict(test_vectors_tfid2)\n\nprint(\"Classification Report - CV2\")\nprint(classification_report(submission_data[\"target\"],submission_cv2[\"target\"]))\nprint(\"Classification Report - TFID2\")\nprint(classification_report(submission_data[\"target\"],submission_tfid2[\"target\"]))\n\nprint(\"Confusion Matrix - CV2\")\nprint(confusion_matrix(submission_data[\"target\"],submission_cv2[\"target\"]))\nprint(\"Confusion Matrix - TFID2\")\nprint(confusion_matrix(submission_data[\"target\"],submission_tfid2[\"target\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Custom Vector compiled\ndisasterDF = pd.read_csv(\"/kaggle/input/disastervec1/disasterVec1.csv\", names = ['keyword'])\ndisasterVec1 = disasterDF[\"keyword\"].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Count Vectorizer 3\ncount_vectorizer3.fit(disasterVec1)\ntrain_vectors_cv3 = count_vectorizer3.transform(train_data[\"clean_text\"])\ntest_vectors_cv3 = count_vectorizer3.transform(test_data[\"clean_text\"])\n\n#Using Tfid Vectorizer 3\ntfid_vectorizer3.fit(disasterVec1)\ntrain_vectors_tfid3 = tfid_vectorizer3.transform(train_data[\"clean_text\"])\ntest_vectors_tfid3 = tfid_vectorizer3.transform(test_data[\"clean_text\"])\n\n# Our vectors are really big, so we want to push our model's weights toward 0 without completely discounting different words\n#Let's use Ridge regression here \nclf_cv3 = linear_model.RidgeClassifier(random_state=0, solver='sparse_cg') # for Count Vectorizer\nclf_tfid3 = linear_model.RidgeClassifier(random_state=0, solver='sparse_cg') # for Tfid Vectorizer\n\nscores_cv3 = model_selection.cross_val_score(clf_cv3, train_vectors_cv3, train_data[\"target\"], cv=3, scoring=\"f1\")\nprint(\"F1 Scores for Count Vectorizer: \", scores_cv3)\n\nclf_cv3.fit(train_vectors_cv3, train_data[\"target\"])\nsubmission_cv3 = pd.DataFrame(columns =['id','target'])\nsubmission_cv3[\"id\"] = submission_data[\"id\"]\nsubmission_cv3[\"target\"] = clf_cv3.predict(test_vectors_cv3)\n\nscores_tfid3 = model_selection.cross_val_score(clf_tfid3, train_vectors_tfid3, train_data[\"target\"], cv=3, scoring=\"f1\")\nprint(\"F1 Scores for Tfid Vectorizer: \", scores_tfid3)\n\nclf_tfid3.fit(train_vectors_tfid3, train_data[\"target\"])\nsubmission_tfid3 = pd.DataFrame(columns =['id','target'])\nsubmission_tfid3[\"id\"] = submission_data[\"id\"]\nsubmission_tfid3[\"target\"] = clf_tfid3.predict(test_vectors_tfid3)\n\nprint(\"Classification Report - CV3\")\nprint(classification_report(submission_data[\"target\"],submission_cv3[\"target\"]))\nprint(\"Classification Report - TFID3\")\nprint(classification_report(submission_data[\"target\"],submission_tfid3[\"target\"]))\n\nprint(\"Confusion Matrix - CV3\")\nprint(confusion_matrix(submission_data[\"target\"],submission_cv3[\"target\"]))\nprint(\"Confusion Matrix - TFID3\")\nprint(confusion_matrix(submission_data[\"target\"],submission_tfid1[\"target\"]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}