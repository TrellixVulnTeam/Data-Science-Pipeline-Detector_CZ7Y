{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook shows a basic pipeline for the NLP task of text classification.<br>\nSpecifically it tries to classify wheter a tweet describes a real or fake disaster.\n\nThe pipeline is composed of:\n* Cleaning and pre-preocessing\n* Feature engineering\n* Pre-trained word-embedding model\n* BiLSTM RNN model"},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport seaborn as sns\n\nfrom copy import deepcopy\nimport gc\n\nimport nltk\nfrom nltk.tokenize import TweetTokenizer  # Twitter-aware tokenizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\n\nimport torch\nimport torch.nn as nn\n\nfrom gensim.models.word2vec import Word2Vec\nimport gensim.downloader as api\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning and pre-processing functions"},{"metadata":{},"cell_type":"markdown","source":"#### Text cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _normalize_tweet(text):\n    \"\"\"Returns a normalized versions of text.\"\"\"\n\n    # change hyperlinks to '<url>' tokens\n    output = re.sub(r'http[s]{0,1}://t.co/[a-zA-Z0-9]+\\b', '<url>', text)\n    \n    # separate all '#' signs from following word with one whitespace\n    output = re.sub(r'#(\\w+)', r'# \\1', output)\n\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _tokenize(tokenizer, string):\n    \"\"\"Tokenizes a sentence, but leave hastags (#) and users (@)\"\"\"\n    \n    tokenized = tokenizer.tokenize(string)\n    return tokenized","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _numbers_to_number_tokens(tokenized_string, num_token='<number>'):\n    \"\"\"Returns the tokenized string (list) with numbers replaced by a numbet token.\"\"\"\n    \n    # create a list of (word, POS-tags) tuples\n    pos_tagged = nltk.pos_tag(tokenized_string)\n    \n    # find indices of number POS tags\n    num_indices = [idx for idx in range(len(pos_tagged)) if pos_tagged[idx][1] == 'CD']\n    \n    # replace numbers by token\n    for idx in num_indices:\n        tokenized_string[idx] = num_token\n        \n    return tokenized_string    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_text(tokenizer, string):\n    \"\"\"Executes all text cleaning functions.\"\"\"\n    \n    return _numbers_to_number_tokens(_tokenize(tokenizer, _normalize_tweet(string)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Keyword cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_keyword(keyword):\n    \"\"\"Returns a clean, tokenized keyword.\"\"\"\n    \n    # return None if keywors is np.nan\n    if type(keyword) == np.float and np.isnan(keyword):\n        return\n    \n    # replace '%20' with whitespace, lower, and tokenize\n    output = re.sub(r'%20', ' ', keyword)\n    output = output.lower()\n    output = output.split()\n    return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature engineering functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_all_caps(text):\n    \"\"\"Returns an integer denoting number of ALL-CAPS words (e.g. 'CANADA', 'WELCOME').\"\"\"\n\n    return len([word for word in text.split() if word.isupper()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_capitalized(text):\n    \"\"\"Returns an integer denoting number of capitalized words (e.g. 'Beer', 'Obama').\"\"\"\n\n    return len([word for word in text.split() if word.istitle()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_words(text):\n    \"\"\"Returns an integer denoting number of words in tweet (before normalizing).\"\"\"\n\n    return len(text.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentiment_analyze_df(df, column):\n    \"\"\"Adds 4 columns of sentiment analysis scores to input DataFrame. changes occur inplace.\"\"\"\n\n    # instantiate a sentiment anlayzer\n    sid = SentimentIntensityAnalyzer()\n    \n    # instantiate a matrix and populate it with scores of each of df[column]\n    output_values = np.zeros((len(df), 4))\n    for tup in df.itertuples():\n        output_values[tup.Index, :] = list(sid.polarity_scores(' '.join(getattr(tup, column))).values())\n    \n    # adding column to input DataFrame\n    for idx, col in enumerate(['sent_neg', 'sent_neu', 'sent_pos', 'sent_compound']):\n        df[col] = output_values[:, idx]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word embedding functions"},{"metadata":{},"cell_type":"markdown","source":"#### Text embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_word_vec(embedding_model, use_norm, word):\n    \"\"\"\n    Returns a normalized embedding vector of input word.\n    \n    Takes care of special cases.\n    <url> tokens are already taken care of in normalization.\n    \"\"\"\n\n    if word[0] == '@':\n        return embedding_model.word_vec('<user>', use_norm=use_norm)\n        \n    elif word == '#':\n        return embedding_model.word_vec('<hashtag>', use_norm=use_norm)\n\n    elif word in embedding_model.vocab:\n        return embedding_model.word_vec(word, use_norm=use_norm)\n\n    else:\n        return embedding_model.word_vec('<UNK>', use_norm=use_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _text_to_vectors(embedding_model, use_norm, tokenized_text):\n    \"\"\"Returns tweet's words' embedding vector.s\"\"\"\n\n    vectors = [_get_word_vec(embedding_model, use_norm, word) for word in tokenized_text]\n    vectors = np.array(vectors)\n    \n    return vectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _trim_and_pad_vectors(text_vectors, embedding_dimension, seq_len):\n    \"\"\"Returns a padded matrix of text embedding vectors with dimensions (seq_len, embedding dimensions).\"\"\"\n\n    # instantiate 0's matrix\n    output = np.zeros((seq_len, embedding_dimension))\n\n    # trim long tweets to be seq_len long\n    trimmed_vectors = text_vectors[:seq_len]\n\n    # determine index of end of padding and beginning of tweet embedding\n    end_of_padding_index = seq_len - trimmed_vectors.shape[0]\n\n    # pad if needed, by replacing last rows with the tweet's words' embedding vectors\n    output[end_of_padding_index:] = trimmed_vectors\n\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def embedding_preprocess(embedding_model, use_norm, seq_len, tokenized_text):\n    \"\"\"Returns an embedding representation of input tokenized text, by executing text embedding functions.\"\"\"\n    \n    # get matrix of tweet's words' embedding vectors (tweet length, embedding_dimension)\n    text_vectors = _text_to_vectors(embedding_model, use_norm, tokenized_text)\n    \n    output = _trim_and_pad_vectors(text_vectors, embedding_model.vector_size, seq_len)\n    \n    return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Keyword embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"def keyword_to_avg_vector(embedding_model, use_norm, tokenized_keyword):\n    \"\"\"Returns keyword(s') average embedding vector.\"\"\"\n    \n    # return a vector of zeros if tokenized_keyword is None\n    if tokenized_keyword is None:\n        return np.zeros((1, embedding_model.vector_size))\n    \n    # otherwise, calculate average embedding vector\n    vectors = [_get_word_vec(embedding_model, use_norm, word) for word in tokenized_keyword]\n    vectors = np.array(vectors)\n    avg_vector = np.mean(vectors, axis=0)\n    avg_vector = avg_vector.reshape((1, embedding_model.vector_size))\n    return avg_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Embedding model preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load a pre-trained model, which was trained on twitter\nmodel_glove_twitter = api.load(\"glove-twitter-100\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a random vector, to represent <UNK> token (unseen word)\nrandom_vec_for_unk = np.random.uniform(-1, 1, size=model_glove_twitter.vector_size).astype('float32')\nrandom_vec_for_unk = random_vec_for_unk.reshape(1,model_glove_twitter.vector_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add the random vector to model\nmodel_glove_twitter.add(['<UNK>'], random_vec_for_unk, replace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compute noramlized vectors, and replace originals\nmodel_glove_twitter.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_SET_PATH = '../input/nlp-getting-started/train.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_SET_PATH)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a tokenizer which lowercases, reduces length of and preserves user handles ('@user')\ntokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize and tokenize texts\ntrain_df['tok_norm_text'] = [preprocess_text(tokenizer, text) for text in train_df['text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['keyword'] = train_df['keyword'].apply(preprocess_keyword)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['num_all_caps'] = train_df['text'].apply(count_all_caps)\ntrain_df['num_caps'] = train_df['text'].apply(count_capitalized)\ntrain_df['num_words'] = train_df['text'].apply(count_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a scaler to make all features be in range [-1, 1], thus suitable for a newural network model\nscaler = MinMaxScaler(feature_range=(-1, 1))\n\ncolumns_to_scale = ['num_all_caps', 'num_caps', 'num_words']\nscaler.fit(train_df[columns_to_scale])\ntrain_df[columns_to_scale] = scaler.transform(train_df[columns_to_scale])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create sentiment analysis feautres\nsentiment_analyze_df(train_df, 'tok_norm_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.distplot([len(tok) for tok in train_df['tok_norm_text']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like most of the texts are shorter than 30 words.<br>\nThus, a reasonable compromise between data loss and computational complexity will be<br>\nchoosing the maximum length of sequences to be 30, i.e. trimming text after the 30'th word."},{"metadata":{},"cell_type":"markdown","source":"#### Textual feautres to word embedding representation"},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_max_length = 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text_embedding'] = [\n    embedding_preprocess(\n        embedding_model=model_glove_twitter, use_norm=True, seq_len=sequence_max_length, tokenized_text=text)\n    for text in train_df['tok_norm_text']\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['keyword_embedding'] = [\n    keyword_to_avg_vector(embedding_model=model_glove_twitter, use_norm=True, tokenized_keyword=keyword)\n    for keyword in train_df['keyword']\n]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create one embedding representation of all chosen features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _single_values_repeat(seq_len, static_single_values):\n    \"\"\"Returns a numpy array containing seq_len-repeated values.\"\"\"\n    \n    output = static_single_values.reshape((1, len(static_single_values)))\n    output = np.repeat(output, seq_len, axis=0)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _static_embedding_repeat(seq_len, static_embedding_values):\n    \"\"\"Return a numpy array os stacked static embedding vectors.\"\"\"\n    \n    horizontally_stacked = np.hstack(static_embedding_values)\n    output = np.repeat(horizontally_stacked, seq_len, axis=0)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def concatenate_embeddings(df, embedding_model, seq_len, sequence_embedding_col, static_embedding_cols, static_singles_cols):\n    \"\"\"Returns one embedding representation of all features - main sequence, static embedded featues, and single values.\"\"\"\n    \n    emb_dim = embedding_model.vector_size\n    \n    # instantiate output matrix\n    output = np.zeros((len(df), seq_len, len(static_singles_cols) + len(static_embedding_cols) * emb_dim + emb_dim))\n    \n    for idx, row in df.iterrows():\n        \n        single_vals = _single_values_repeat(seq_len, row[static_singles_cols].values)\n        static_emb_vals = _static_embedding_repeat(seq_len, row[static_embedding_cols])\n        seq_emb_vals = row[sequence_embedding_col]\n\n        # horizontally stack embeddings and features\n        row_embedding = np.hstack((single_vals, static_emb_vals, seq_emb_vals))\n\n        output[idx, :, :] = row_embedding\n        \n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create one embedding representation of all chosen features\nembedding_matrix = concatenate_embeddings(\n    df=train_df, embedding_model=model_glove_twitter, seq_len=sequence_max_length,\n    sequence_embedding_col='text_embedding',\n    static_embedding_cols=['keyword_embedding'],\n    static_singles_cols=['num_all_caps', 'num_caps', 'num_words', 'sent_neg', 'sent_neu', 'sent_pos', 'sent_compound'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural-network creation and utility functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BiLSTM(nn.Module):\n    \"\"\"A pyTorch Bi-Directional LSTM RNN implementation\"\"\"\n\n    def __init__(self, embedding_dim, hidden_dim, num_layers, num_classes, batch_size, dropout, device):\n        super(BiLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.batch_size = batch_size\n        self.num_layers = num_layers\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.lstm = nn.LSTM(\n            input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers,\n            batch_first=True, dropout=dropout, bidirectional=True)\n        \n        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n\n        self.device = device\n        \n        # instantiate lists for evaluating and plotting\n        self.train_loss = []\n        self.train_acc = []\n        self.val_loss = []\n        self.val_acc = []\n        \n        # an attribute to hold model's best weights (used for evaluating)\n        self.best_weights = deepcopy(self.state_dict())\n\n    def _init_hidden(self, current_batch_size):\n        \"\"\"Sets initial hidden and cell states (for LSTM).\"\"\"\n\n        h0 = torch.zeros(self.num_layers * 2, current_batch_size, self.hidden_dim).to(self.device)\n        c0 = torch.zeros(self.num_layers * 2, current_batch_size, self.hidden_dim).to(self.device)\n        return h0, c0\n\n    def forward(self, x):\n        \"\"\"Forward step.\"\"\"\n\n        # Forward propagate LSTM\n        h, c = self._init_hidden(current_batch_size=x.size(0))\n        out, _ = self.lstm(x, (h, c))\n\n        # dropout\n        out = self.dropout(out)\n\n        # Decode the hidden state of the last time step\n        out = self.fc(out[:, -1, :])\n\n        return out\n    \n    def predict(self, x: torch.tensor):\n        \"\"\"Return a tensor of predictions of tensor x.\"\"\"\n\n        class_predictions = self(x).data\n        _, predicted = torch.max(class_predictions, dim=1)\n        return predicted\n\n    def _train_evaluate(self, X_train, y_train, X_val, y_val, criterion):\n        \"\"\"Evaluates model during training time, and returns train_loss, train_acc, val_loss, val_acc.\"\"\"\n\n        # set model to evaluation mode\n        self.eval()\n\n        # calculate accuracy and loss of train set and append to lists\n        epoch_train_acc = (self.predict(X_train) == y_train).sum().item() / y_train.shape[0]\n        epoch_train_loss = criterion(self(X_train), y_train).item()\n        self.train_acc.append(epoch_train_acc)\n        self.train_loss.append(epoch_train_loss)\n\n        # calculate accuracy and loss of validation set, and append to lists\n        if X_val is not None and y_val is not None:\n            epoch_val_acc = (self.predict(X_val) == y_val).sum().item() / y_val.shape[0]\n            epoch_val_loss = criterion(self(X_val), y_val).item()\n            self.val_acc.append(epoch_val_acc)\n            self.val_loss.append(epoch_val_loss)\n\n            # return all loss and accuracy values\n            return epoch_train_loss, epoch_train_acc, epoch_val_loss, epoch_val_acc\n\n        # return train set loss and accuracy values, if there is no validation set\n        return epoch_train_loss, epoch_train_acc, None, None\n    \n    @staticmethod\n    def _print_progress(epoch, train_loss, train_acc, val_loss, val_acc, improved, verbose=False):\n        \"\"\"Prints the training progress.\"\"\"\n\n        output = f'Epoch {str(epoch + 1).zfill(3)}:'\n        output += f'\\n\\t Training   Loss: {str(train_loss)[:5]} | Accuracy: {str(train_acc)[:5]}.'\n\n        if val_loss is not None and val_acc is not None:\n            output += f'\\n\\t Validation Loss: {str(val_loss)[:5]} | Accuracy: {str(val_acc)[:5]}.'\n\n        if improved:\n            output += f' Improvement!'\n\n        if verbose:\n            print(output)\n\n    def fit(self, X_train, y_train, X_val, y_val, epoch_num, criterion, optimizer, verbose=False):\n        \"\"\"Trains the model.\"\"\"\n\n        # a variable to determine whether to update best weights (and report progress)\n        best_acc = 0.0\n\n        # split dataset to batches\n        X_train_tensor_batches = torch.split(X_train, self.batch_size)\n        y_train_tensor_batches = torch.split(y_train, self.batch_size)\n\n        for epoch in range(epoch_num):\n\n            # set model to train mode\n            self.train()\n\n            for i, (X_batch, y_batch) in enumerate(zip(X_train_tensor_batches, y_train_tensor_batches)):\n\n                # Forward pass\n                outputs = self(X_batch)\n                loss = criterion(outputs, y_batch)\n\n                # Backward and optimize\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            # calculate accuracy and loss of train and validation set (if validation set is None, values are None)\n            train_loss, train_acc, val_loss, val_acc = self._train_evaluate(X_train, y_train, X_val, y_val, criterion)\n\n            # a boolean to determine the correct accuracy to consider for progress (Validation ot Training)\n            if X_val is not None and y_val is not None:\n                accuracy = val_acc\n            else:\n                accuracy = train_acc\n\n            # if accuracy outperforms previous best accuracy, print and update best accuracy and model's best weights\n            if accuracy > best_acc:\n                self._print_progress(epoch, train_loss, train_acc, val_loss, val_acc, improved=True, verbose=verbose)\n                best_acc = accuracy\n                self.best_weights = deepcopy(self.state_dict())\n\n            # else, print\n            else:\n                self._print_progress(epoch, train_loss, train_acc, val_loss, val_acc, improved=False, verbose=verbose)\n\n        gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graphs(model):\n    plt.figure(figsize=(6, 12))\n\n    plt.subplot(311)\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.plot(range(1, len(model.train_acc)+1), model.train_acc, label=\"Train\")\n    plt.plot(range(1, len(model.val_acc)+1), model.val_acc, label=\"Validation\")\n\n    plt.xticks(np.arange(0, len(model.train_acc)+1, 5))\n    plt.legend()\n\n    plt.subplot(312)\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.plot(range(1, len(model.train_loss)+1), model.train_loss, label=\"Train\")\n    plt.plot(range(1, len(model.val_loss)+1), model.val_loss, label=\"Validation\")\n\n    plt.xticks(np.arange(0, len(model.train_acc)+1, 5))\n    plt.legend()\n\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Running the network"},{"metadata":{},"cell_type":"markdown","source":"At first, I split the train data to train and test set (the test set is called held-out set):"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_val, X_held_out_set, y_train_val, y_held_out_set = train_test_split(\n    embedding_matrix, train_df['target'].values, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# determines which device to mount the model to\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Using device: {device}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert above LSTM arrays to tensors to be used in BiLSTM Neural Network\nX_train_val = torch.from_numpy(X_train_val).float().to(device)\nX_held_out_set = torch.from_numpy(X_held_out_set).float().to(device)\ny_train_val = torch.from_numpy(y_train_val).long().to(device)\ny_held_out_set = torch.from_numpy(y_held_out_set).long().to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I used the train-val set to perform 10-fold cross validation to tune the hyper-parameters of the network. <br>\nThe 10-fold cross validation is left out of the notebook for readability and order. <br>\nThe chosen hyper-parameters are as following:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# network hyprer-parameters\nembedding_dim = embedding_matrix.shape[2]\nhidden_size = 50\nnum_layers = 2\nnum_classes = 2\nbatch_size = 256\ndropout = 0.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learning hyprer-parameters\nnum_epochs = 20\nlearning_rate = 0.0005\nweight_decay = 0.0005","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To show the results, I use the train-val data to train the network, and use the held-out to test it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate Model, Loss and Optimizer\nbilstm = BiLSTM(embedding_dim, hidden_size, num_layers, num_classes, batch_size, dropout, device).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(bilstm.parameters(), lr=learning_rate, weight_decay=weight_decay)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train the model\nbilstm.fit(\n    X_train=X_train_val, y_train=y_train_val, X_val=X_held_out_set, y_val=y_held_out_set,\n    epoch_num=num_epochs, criterion=criterion, optimizer=optimizer, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training graph:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_graphs(bilstm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we now know our network is doing it's job properly, I train a network again, using all the training-data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"del(bilstm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = torch.from_numpy(embedding_matrix).float().to(device)\ny_train = torch.from_numpy(train_df['target'].values).long().to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate Model, Loss and Optimizer\nbilstm = BiLSTM(embedding_dim, hidden_size, num_layers, num_classes, batch_size, dropout, device).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(bilstm.parameters(), lr=learning_rate, weight_decay=weight_decay)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# train the model\nbilstm.fit(\n    X_train=X_train, y_train=y_train, X_val=None, y_val=None,\n    epoch_num=num_epochs, criterion=criterion, optimizer=optimizer, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training graph (note that this time there is no validation data to test on):"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_graphs(bilstm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_SET_PATH = '../input/nlp-getting-started/test.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(TEST_SET_PATH)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize and tokenize texts and keywords\ntest_df['tok_norm_text'] = [preprocess_text(tokenizer, text) for text in test_df['text']]\ntest_df['keyword'] = test_df['keyword'].apply(preprocess_keyword)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature extraction\ntest_df['num_all_caps'] = test_df['text'].apply(count_all_caps)\ntest_df['num_caps'] = test_df['text'].apply(count_capitalized)\ntest_df['num_words'] = test_df['text'].apply(count_words)\n\ntest_df[columns_to_scale] = scaler.transform(test_df[columns_to_scale])\n\nsentiment_analyze_df(test_df, 'tok_norm_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#text embedding\ntest_df['text_embedding'] = [\n    embedding_preprocess(\n        embedding_model=model_glove_twitter, use_norm=True, seq_len=sequence_max_length, tokenized_text=text)\n    for text in test_df['tok_norm_text']\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#keyword embedding\ntest_df['keyword_embedding'] = [\n    keyword_to_avg_vector(embedding_model=model_glove_twitter, use_norm=True, tokenized_keyword=keyword)\n    for keyword in test_df['keyword']\n]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create one embedding representation of all chosen features\ntest_embedding_matrix = concatenate_embeddings(\n    df=test_df, embedding_model=model_glove_twitter, seq_len=sequence_max_length,\n    sequence_embedding_col='text_embedding',\n    static_embedding_cols=['keyword_embedding'],\n    static_singles_cols=['num_all_caps', 'num_caps', 'num_words', 'sent_neg', 'sent_neu', 'sent_pos', 'sent_compound'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = torch.from_numpy(test_embedding_matrix).float().to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict\npreds = bilstm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# put predictions and id's in DataFrame\nfinal_preds = preds.cpu().numpy().reshape(-1,1)\nids = test_df['id'].values.reshape(-1,1)\ndata = np.hstack((ids, final_preds))\n\nsubmission_df = pd.DataFrame(data=data,columns = ['id', 'target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}