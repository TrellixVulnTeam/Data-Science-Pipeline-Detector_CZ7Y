{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1- Learning ðŸ¤—  - Out-of-the-box BERT [LB: 0.81029]\n\nHi, and welcome! This is the first kernel of the series `Learning ðŸ¤—`, a personal project I'm currently working on. I am an experienced data scientist diving into the hugging face transformers library and this series or kernels is a \"working diary\", as I do it. The approach I'm taking is the following: \n\n1. Explore various out-of-the-box models, without digging into their technical details. \n2. After that, I'll start going over the best ranked public kernels, understand their ideas, and reproduce them by myself. \n\nYou are invited to follow me in this journey. In this short kernel (~80 lines) we fine-tune an out-of-the-box cased BERT, with just the minimal set up required for it to run in this competition, obtaining a leaderboard score of `0.81029`. \n\nThis is an ongoing project, so expect more notebooks to be added to the series soon. Actually, we are currently working on the following ones:\n\n1. [Learning ðŸ¤—  - Out-of-the-box BERT [LB: 0.81029]](1-learning-out-of-the-box-bert-lb-0-8102) (this notebook)\n2. Learning ðŸ¤— - Out-of-the-box RoBERTa _WIP_\n3. Learning ðŸ¤— - Out-of-the-box Electra _WIP_\n4. Learning ðŸ¤— - BERT Large Uncased _WIP_\n\n### Please remember to upvote if you found the series useful for your research!\n\n\n## Using the [`transformers`](https://huggingface.co/transformers/) library\n\nWe are using a very high-level API of the library after following this quick guide article, which we recommend to read:\n[Fine-tuning a pretrained model](https://huggingface.co/transformers/training.html)\n\nWe use only 4 objects from the library: `Trainer`, `TrainingArguments`, `AutoModelForSequenceClassification`, `AutoTokenizer`. And, actually, the full list of imports is quite small as you can see below:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom transformers import Trainer, TrainingArguments, AutoTokenizer,\\\n                         AutoModelForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2021-07-01T23:19:43.070226Z","iopub.execute_input":"2021-07-01T23:19:43.070579Z","iopub.status.idle":"2021-07-01T23:19:43.075834Z","shell.execute_reply.started":"2021-07-01T23:19:43.070547Z","shell.execute_reply":"2021-07-01T23:19:43.074808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The code\n\nWe have split the code in various simple functions to separates the wheat from the chaff and focus on the parts that are new to us.\n\nDocumentation about the `Trainer` can be found [here](https://huggingface.co/transformers/main_classes/trainer.html) and about the `TrainerArguments` [here](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments).","metadata":{}},{"cell_type":"code","source":"# This is just 2 calls to pd.read_csv()\n# Loading the df_train and df_test\ndef load_dfs():\n    df_train = pd.read_csv('../input/nlp-getting-started/train.csv')[['text', 'target']]\\\n                 .rename(columns={'target': 'label'})\n    df_test = pd.read_csv('../input/nlp-getting-started/test.csv')[['id', 'text']]\n    return df_train, df_test\n\n\n# This functions is used by the Trainer to compute the metrics in the evaluation steps\n# It's just computing accuracy and f1\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {'accuracy': acc,\n            'f1': f1}\n\n\n# This function applies the tokenizer to the three dataframes\n# The result is ready for inputting it to the model\ndef tokenize(tokenizer, df_train, df_val, df_test):\n\n    def tokenize_df(tokenizer, df, has_label=True):\n        # Tokenize texts (Returns dictionary with keys: input_ids, token_type_ids, attention_mask)\n        ds = tokenizer(df['text'].tolist(), padding=\"max_length\", truncation=True)\n        # Add key 'label'\n        if has_label: ds['label'] = df['label'].tolist()\n        # Turn dictionary of lists into list of dictionaries\n        return [dict(zip(ds, t)) for t in zip(*ds.values())]\n\n    ds_train = tokenize_df(tokenizer, df_train)\n    ds_val = tokenize_df(tokenizer, df_val)\n    ds_test = tokenize_df(tokenizer, df_test, has_label=False)\n\n    return ds_train, ds_val, ds_test\n\n# Gets tokenizer and model from the modelhub, given its id\ndef get_tokenizer_and_model(model_name):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n    return tokenizer, model\n\n# Gets the predictions for ds_test from the Trainer object\ndef get_predictions(trainer, ds_test):\n    preds = trainer.predict(ds_test)\n    preds = F.softmax(torch.from_numpy(preds.predictions), dim=-1)\n    binary_preds = (preds[:, 1] > 0.5).numpy().astype(int)\n    return binary_preds\n    \n# Gets the predictions for the test set and saves them to submission.csv\ndef submit(trainer, ds_test):\n    df_res = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n    df_res['target'] = get_predictions(trainer, ds_test)\n    df_res.to_csv('submission.csv', index=False)\n    return df_res","metadata":{"execution":{"iopub.status.busy":"2021-07-01T23:19:44.849807Z","iopub.execute_input":"2021-07-01T23:19:44.850135Z","iopub.status.idle":"2021-07-01T23:19:44.865373Z","shell.execute_reply.started":"2021-07-01T23:19:44.850104Z","shell.execute_reply":"2021-07-01T23:19:44.864432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.trainer_utils import set_seed; set_seed(2021) # Set seed for reproducibility\n\n# This is the model we will use, from the modelhub:\n# https://huggingface.co/models\nMODEL_NAME = \"bert-base-cased\"\n\n# Get tokenizer and model\ntokenizer, model = get_tokenizer_and_model(MODEL_NAME)\n\n# Load dataframes\ndf_base, df_test = load_dfs()\n\n# Split train and validation sets\ndf_train, df_val = train_test_split(df_base, test_size=0.1)\n\n# Tokenize train, validation, and test sets\nds_train, ds_val, ds_test = tokenize(tokenizer, df_train, df_val, df_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T23:03:02.075447Z","iopub.execute_input":"2021-07-01T23:03:02.076042Z","iopub.status.idle":"2021-07-01T23:03:37.742252Z","shell.execute_reply.started":"2021-07-01T23:03:02.075997Z","shell.execute_reply":"2021-07-01T23:03:37.741455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fine-tune for just 1 epoch\nEPOCHS = 1\n\n# Prepare the TrainingArguments\nargs = TrainingArguments(\"/kaggle/working/model/\", \n                         num_train_epochs=EPOCHS, \n                         report_to=\"none\", # Disable \"wandb\", I don't know what it is yet\n                         evaluation_strategy=\"steps\", \n                         eval_steps=100, # Evaluate and log to screen metrics each 100 batches\n                         )\n\n# Instantiate the Trainer\ntrainer = Trainer(model=model, \n                  args=args, \n                  train_dataset=ds_train, \n                  eval_dataset=ds_val, \n                  compute_metrics=compute_metrics)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T23:20:35.690999Z","iopub.execute_input":"2021-07-01T23:20:35.691362Z","iopub.status.idle":"2021-07-01T23:29:39.087847Z","shell.execute_reply.started":"2021-07-01T23:20:35.691312Z","shell.execute_reply":"2021-07-01T23:29:39.086749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate for correlating LB with validation schema\nres = trainer.evaluate()\nprint(f\"Validation F1 : {res['eval_f1']:.2f}\")\nprint(f\"Validation Acc: {res['eval_accuracy']:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T23:35:48.657012Z","iopub.execute_input":"2021-07-01T23:35:48.657369Z","iopub.status.idle":"2021-07-01T23:36:03.337647Z","shell.execute_reply.started":"2021-07-01T23:35:48.657316Z","shell.execute_reply":"2021-07-01T23:36:03.336796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate predictions and create submission file\nsubmit(trainer, ds_test);","metadata":{"execution":{"iopub.status.busy":"2021-07-01T23:33:22.845768Z","iopub.execute_input":"2021-07-01T23:33:22.846104Z","iopub.status.idle":"2021-07-01T23:34:23.868675Z","shell.execute_reply.started":"2021-07-01T23:33:22.846074Z","shell.execute_reply":"2021-07-01T23:34:23.867791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ðŸ¤—ðŸ¤— Thanks for reading this notebook! Remember to upvote if you found it useful, and stay tuned for the next deliveries! ðŸ¤—ðŸ¤—","metadata":{}}]}