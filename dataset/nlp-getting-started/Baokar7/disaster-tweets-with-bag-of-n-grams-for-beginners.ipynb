{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### This is merely my second notebook and I have just started making them. Please do comment or give any suggestions that you can to help me learn and improve!","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import the Libraries","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's specify the data paths!","metadata":{}},{"cell_type":"code","source":"TRAIN_PATH = '../input/nlp-getting-started/train.csv'\nTEST_PATH = '../input/nlp-getting-started/test.csv'\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Preprocessing\n\n1. Tweets to lowercase","metadata":{}},{"cell_type":"code","source":"for i in range(len(train)):\n    train.text[i] = train.text[i].lower()\n\nfor i in range(len(test)):\n    test.text[i] = test.text[i].lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Remove Stopwords","metadata":{}},{"cell_type":"code","source":"tokenizer = nltk.tokenize.TreebankWordTokenizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = stopwords.words('english')\n\nfor i in range(len(train)):\n    tokens = tokenizer.tokenize(train.text[i])\n    review = [i for i in tokens if not i in stop_words]\n    train.text[i] = \" \".join(review)\n    \nfor i in range(len(test)):\n    tokens = tokenizer.tokenize(test.text[i])\n    review = [i for i in tokens if not i in stop_words]\n    test.text[i] = \" \".join(review)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. I will be doing lemmatization for token normalization with TreeBank Tokenizer and WordNet Lemmatizer. I have tried various combinations of stemmer and lemmatizer but the best result that I have got is with only doing lemmatization","metadata":{}},{"cell_type":"code","source":"lemmatizer = nltk.stem.WordNetLemmatizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(train)):\n    tokens = tokenizer.tokenize(train.text[i])\n    train.text[i] = \" \".join(lemmatizer.lemmatize(token) for token in tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(test)):\n    tokens = tokenizer.tokenize(test.text[i])\n    test.text[i] = \" \".join(lemmatizer.lemmatize(token) for token in tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We probably don't need the other 2 columns for this task, so I will drop the columns","metadata":{}},{"cell_type":"code","source":"train = train.drop(labels=['keyword', 'location'], axis=1)\ntest = test.drop(labels=['keyword', 'location'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we initialize the TFIDF vectorizer for creating the bag of n grams","metadata":{}},{"cell_type":"code","source":"tfidf = TfidfVectorizer(min_df=5, max_df=0.5, ngram_range=(1, 2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit_tfidf = tfidf.fit_transform(train.text).toarray()\ntrain_data = pd.DataFrame(fit_tfidf, columns=tfidf.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.DataFrame(tfidf.transform(test.text).toarray(), \n                         columns=tfidf.get_feature_names())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(train_data, \n                                  train.target, \n                                  test_size=0.2,\n                                  random_state=47)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the Logistic Regression Object","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fit the object on the training data","metadata":{}},{"cell_type":"code","source":"logreg.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make predictions on the validation and test data","metadata":{}},{"cell_type":"code","source":"val_pred = logreg.predict(x_val)\ny_pred = logreg.predict(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the score on validation data","metadata":{}},{"cell_type":"code","source":"logreg.score(x_val, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got an 80% probability score on the validation data, which is a decent score","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}