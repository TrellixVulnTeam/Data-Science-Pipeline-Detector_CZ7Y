{"cells":[{"metadata":{"papermill":{"duration":0.019272,"end_time":"2020-12-24T07:39:25.542448","exception":false,"start_time":"2020-12-24T07:39:25.523176","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Introduction\n\nThe *Real or Not? NLP with Disaster Tweets* competitions offers a neat opportunity to see how different approaches to natural language processing work when compared to one another. In this notebook, we'll look at how to start examining NLP data and performing some rudimentary second-order feature engineering. Here's a breakdown of what this notebook covers:\n\n1. Perform an initial exploration of some simple fields.\n2. Clean and normalize the data set.\n3. Extract first-order features and examine how useful they are.\n4. Perform rudimentary natural language processing on the text field.\n5. Evaluate our natural language model.\n6. Use the model and make predictions that we can submit to the competition."},{"metadata":{"papermill":{"duration":0.018183,"end_time":"2020-12-24T07:39:25.579819","exception":false,"start_time":"2020-12-24T07:39:25.561636","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 1. Importing the Data\n\nThe first step in the process is to import our training data so we can see what kinds of information we have to work with. For this project, we'll start by importing the entire training dataset into a single Pandas dataframe."},{"metadata":{"papermill":{"duration":1.2267,"end_time":"2020-12-24T07:39:26.824813","exception":false,"start_time":"2020-12-24T07:39:25.598113","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import time\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ndisplay(train)\n\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ndisplay(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.1 Eliminating Duplicates\n\nOne thing we should do is check to see if we have duplicated or conflicting data. Here's an easy way to check for textual duplicates against the `target` - which is the class we're trying to predict."},{"metadata":{"trusted":false},"cell_type":"code","source":"duplicates = pd.concat(x for _, x in train.groupby([\"text\"]) if len(x) > 1)\nwith pd.option_context(\"display.max_rows\", None, \"max_colwidth\", 240):\n    display(duplicates[[\"id\", \"target\", \"text\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like we have quite a few duplicates. In some instances, the duplicates resolve to the same target class, but in others such as duplicate indexes `5620` and `5641`, we have the same tweet belonging to two different classes. For those instances where the tweet belongs to the same class, we can simply delete the duplicates."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.drop(\n    [\n        6449, 7034, 3589, 3591, 3597, 3600, 3603, \n        3604, 3610, 3613, 3614, 119, 106, 115,\n        2666, 2679, 1356, 7609, 3382, 1335, 2655, \n        2674, 1343, 4291, 4303, 1345, 48, 3374,\n        7600, 164, 5292, 2352, 4308, 4306, 4310, \n        1332, 1156, 7610, 2441, 2449, 2454, 2477,\n        2452, 2456, 3390, 7611, 6656, 1360, 5771, \n        4351, 5073, 4601, 5665, 7135, 5720, 5723,\n        5734, 1623, 7533, 7537, 7026, 4834, 4631, \n        3461, 6366, 6373, 6377, 6378, 6392, 2828,\n        2841, 1725, 3795, 1251, 7607\n    ], inplace=True\n)\nduplicates = pd.concat(x for _, x in train.groupby([\"text\"]) if len(x) > 1)\nwith pd.option_context(\"display.max_rows\", None, \"max_colwidth\", 240):\n    display(duplicates[[\"id\", \"target\", \"text\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we're facing a challenge. We could keep one duplicate with one target class, but we don't have access to the method by which the dataset creators used to mark up real versus not real disaster tweets. They may have had access to more information than us, so we have to be careful if we alter the dataset - we could introduce personal bias. While it may be tempting to try to keep some of the data (e.g. `that horrible sinking feeling when you've been at home on your phone for a while and you realise its been on 3G this whole time` seems like it should be marked as `not real`), the better approach is to simply delete the offending duplicates. While this cuts our training size down, we ensure we haven't inadventently introduced bias to the dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"train.drop(\n    [\n        4290, 4299, 4312, 4221, 4239, 4244, 2830, \n        2831, 2832, 2833, 4597, 4605, 4618, 4232, \n        4235, 3240, 3243, 3248, 3251, 3261, 3266, \n        4285, 4305, 4313, 1214, 1365, 6614, 6616, \n        1197, 1331, 4379, 4381, 4284, 4286, 4292, \n        4304, 4309, 4318, 610, 624, 630, 634, 3985,\n        4013, 4019, 1221, 1349, 6091, 6094, \n        6103, 6123, 5620, 5641\n    ], inplace=True\n)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.020414,"end_time":"2020-12-24T07:39:26.867761","exception":false,"start_time":"2020-12-24T07:39:26.847347","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 1.2 Keyword and Location Normalization\n\nIt looks like we need to do a little cleanup here. Both `keyword` and `location` fields are meant to be interpreted as strings. While we're at it, we should probably convert them all to lowercase for ease of processing. We'll also fill all missing values (`<NA>` values) in `keyword` and `location` with the empty string. If we look at the `keyword` strings, we find that some entries have `%20` instead of a space. We should also stem the `keyword` field so we can collapse similar keywords into a single keyword (for example, `death` and `deaths` would become `death`). Let's go ahead and make those changes to the dataframe. "},{"metadata":{"papermill":{"duration":0.480407,"end_time":"2020-12-24T07:39:27.368745","exception":false,"start_time":"2020-12-24T07:39:26.888338","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from gensim.parsing.preprocessing import stem_text\n\ndef clean_location_keyword(df):\n    df[\"location\"] = df[\"location\"].astype(\"string\").str.lower()\n    df[\"location\"].fillna(\"<empty>\", inplace=True)\n    df[\"keyword\"] = df[\"keyword\"].astype(\"string\").str.lower()\n    df[\"keyword\"].replace(regex=r\"\\%20\", value=\" \", inplace=True)\n    df[\"keyword\"].fillna(\"<empty>\", inplace=True)\n    df[\"keyword\"] = df[\"keyword\"].apply(stem_text)\n\nclean_location_keyword(train)\nclean_location_keyword(test)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.020923,"end_time":"2020-12-24T07:39:27.411026","exception":false,"start_time":"2020-12-24T07:39:27.390103","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 2. Looking at Class Imbalance\n\nIt looks like we have 7,613 training samples. Let's see how many tweets we have that are examples of disaster versus those that are not. What we're looking at is whether or not we have a balance between samples that are both real examples of disasters, and those that are not."},{"metadata":{"papermill":{"duration":0.255637,"end_time":"2020-12-24T07:39:27.688001","exception":false,"start_time":"2020-12-24T07:39:27.432364","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"counts = pd.DataFrame(train[\"target\"].value_counts())\ncounts.rename(columns={\"target\": \"Samples\"}, index={0: \"Not Real\", 1: \"Real\"}, inplace=True)\nax = sns.barplot(x=counts.index, y=counts.Samples)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        x=p.get_x()+(p.get_width()/2),\n        y=height,\n        s=round(height),\n        ha=\"center\"\n    )","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.022105,"end_time":"2020-12-24T07:39:27.732351","exception":false,"start_time":"2020-12-24T07:39:27.710246","status":"completed"},"tags":[]},"cell_type":"markdown","source":"For this particular set of data, it looks like we have a slightly skewed distribution between the two classes. In this instance, we'll have to be careful with any machine learning algorithm we use, since we have more tweets that do not pertain to disasters than we do that contain real disasters. "},{"metadata":{"papermill":{"duration":0.021762,"end_time":"2020-12-24T07:39:27.776355","exception":false,"start_time":"2020-12-24T07:39:27.754593","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 3. Looking at Keywords\n\nLet's take a closer look at what kind of information we have in the `keyword` field, specifically what unique values we have."},{"metadata":{"trusted":false},"cell_type":"code","source":"with pd.option_context(\"display.max_rows\", None):\n    display(train[\"keyword\"].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a few things we can collapse. For example, `arson` and `arsonist` can be collapsed to `arson`. Let's go ahead and make a few of these changes."},{"metadata":{"trusted":false},"cell_type":"code","source":"def collapse_keywords(x):\n    if x == \"arsonist\":\n        return \"arson\"\n    if x == \"blaze\":\n        return \"ablaz\"\n    if x == \"bloodi\":\n        return \"blood\"\n    if x == \"build burn\" or x == \"burn build\":\n        return \"build on fire\"\n    if x == \"blew up\":\n        return \"blown up\"\n    if x == \"colli\":\n        return \"collid\"\n    if x == \"explo\":\n        return \"explod\"\n    if x == \"hailstorm\":\n        return \"hail\"\n    if x == \"injuri\":\n        return \"injur\"\n    if x == \"panick\":\n        return \"panic\"\n    if x == \"suicid bomber\":\n        return \"suicid bomb\"\n    if x == \"wildfir\":\n        return \"wild fire\"\n    return x\n\ntrain[\"keyword\"] = train[\"keyword\"].apply(lambda x: collapse_keywords(x))\ntest[\"keyword\"] = test[\"keyword\"].apply(lambda x: collapse_keywords(x))\nwith pd.option_context(\"display.max_rows\", None):\n    display(train[\"keyword\"].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's look at the keyword in relation to whether their target is real `1` or not real `0`. Let's just take a look at the first 50 rows or so."},{"metadata":{"papermill":{"duration":0.048545,"end_time":"2020-12-24T07:39:27.846875","exception":false,"start_time":"2020-12-24T07:39:27.79833","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"with pd.option_context(\"display.max_rows\", None):\n    display(pd.DataFrame(data=train[[\"id\", \"keyword\", \"target\"]].groupby([\"keyword\", \"target\"]).count()).rename(columns={\"id\": \"count\"}).head(50))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.024734,"end_time":"2020-12-24T07:39:27.897095","exception":false,"start_time":"2020-12-24T07:39:27.872361","status":"completed"},"tags":[]},"cell_type":"markdown","source":"We can see that there are certain keywords that are strongly tied to one class. For example, `airplane accident` is very strongly associated with the real disaster target - we see it appear 34 times, and 29 of those times it is a real disaster, while only 5 times it is not. This is good news, since it suggests there are likely keywords here that will provide separation between classes."},{"metadata":{"papermill":{"duration":0.034759,"end_time":"2020-12-24T07:39:28.097485","exception":false,"start_time":"2020-12-24T07:39:28.062726","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 4. Looking at Location\n\nLet's take look at the first 500 entries in the location field and see what we're working with."},{"metadata":{"papermill":{"duration":0.068787,"end_time":"2020-12-24T07:39:28.201661","exception":false,"start_time":"2020-12-24T07:39:28.132874","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"print([location for location in train[\"location\"]][:500])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.035903,"end_time":"2020-12-24T07:39:28.274472","exception":false,"start_time":"2020-12-24T07:39:28.238569","status":"completed"},"tags":[]},"cell_type":"markdown","source":"In some instances we have countries, others include states, and some include cities. Yet others include junk data such as `global` as well as `Twitter Lockout in progress`. We'll need a way to clean and normalize this data so that it's a little more useful to us. Normalizing this data may turn out to be beneficial, but based on how messy the field is, it may not be worthwhile to spend huge amounts of time trying to clean it. As it stands, let's see if we can use the Python package `pycountry` to help us sort out some of this data. What we'll do is try and sort out real locations from ones that are not real. We can compare what is in the `location` field to subdivision data from `pycountry`. If we get a match, we'll save the state and country to some new columns on the dataframe. If we don't get a match, we'll try and do a little more processing. If we have two floating point numbers, we probably have a set of geo coordinates, so we can mark that as not being location spam. Other than that, we can't do much with the data, so we'll flag it as probable spam."},{"metadata":{"papermill":{"duration":5.7847,"end_time":"2020-12-24T07:39:34.096168","exception":false,"start_time":"2020-12-24T07:39:28.311468","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import re\n\nfrom pycountry import subdivisions\n\ndef clean_state_country(df):\n    subs = [subdivision.name.lower() for subdivision in subdivisions]\n    countries = [subdivision.country_code for subdivision in subdivisions]\n    country = []\n    state = []\n    location_spam = []\n    for _, row in df.iterrows():\n        match_found = False\n        is_spam = 0\n        country_str = \"<none>\"\n        state_str = \"<none>\"\n        if row[\"location\"] != \"\":\n            for index, subdivision in enumerate(subs):\n                if subdivision in row[\"location\"]:\n                    country_str = countries[index]\n                    state_str = subdivision\n                    match_found = True\n                    break\n            if not match_found:\n                split_data = row[\"location\"].replace(\" \", \"\").split(\",\")\n                is_spam = 1\n                if len(split_data) == 2:\n                    if re.match(r\"[\\-]*[0-9]+\\.[0-9]+\", split_data[0]) and re.match(r\"[\\-]*[0-9]+\\.[0-9]+\", split_data[0]):\n                        is_spam = 0\n        location_spam.append(is_spam)\n        country.append(country_str)\n        state.append(state_str)\n    df[\"country\"] = country\n    df[\"state\"] = state\n    df[\"location_spam\"] = location_spam\n    \nclean_state_country(train)\nclean_state_country(test)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.02354,"end_time":"2020-12-24T07:39:34.144174","exception":false,"start_time":"2020-12-24T07:39:34.120634","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Now that we have country and state information, we should be able to look at those fields the same way we examined keywords. Let's take a look what happens."},{"metadata":{"papermill":{"duration":0.059275,"end_time":"2020-12-24T07:39:34.22746","exception":false,"start_time":"2020-12-24T07:39:34.168185","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"with pd.option_context(\"display.max_rows\", None):\n    display(pd.DataFrame(data=train[[\"id\", \"country\", \"target\"]].groupby([\"country\", \"target\"]).count()).rename(columns={\"id\": \"count\"}).head(50))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.026083,"end_time":"2020-12-24T07:39:34.278325","exception":false,"start_time":"2020-12-24T07:39:34.252242","status":"completed"},"tags":[]},"cell_type":"markdown","source":"There are quite a number of entries for which we have no country information. The first row shows us the number of rows without country information. The total is 5,353 entries, which is more than half of our available training data. For entries with countries, we're seeing somewhat equal splits between real and not real disasters. This is to be expected, as people geotag tweets from all countries whether or not they are actually disasters. It's unlikely that real disasters would exclusively be geotagged. This is probably the same for state information. Let's take a look."},{"metadata":{"papermill":{"duration":0.063015,"end_time":"2020-12-24T07:39:34.366993","exception":false,"start_time":"2020-12-24T07:39:34.303978","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"with pd.option_context(\"display.max_rows\", None):\n    display(pd.DataFrame(data=train[[\"id\", \"state\", \"target\"]].groupby([\"state\", \"target\"]).count()).rename(columns={\"id\": \"count\"}).head(50))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.02611,"end_time":"2020-12-24T07:39:34.419388","exception":false,"start_time":"2020-12-24T07:39:34.393278","status":"completed"},"tags":[]},"cell_type":"markdown","source":"As predicted, we're missing the same amount of state information. Looking at the low counts for each state, we're probably not going to get very useful information from this field, but we'll keep it intact for now."},{"metadata":{"papermill":{"duration":0.025839,"end_time":"2020-12-24T07:39:34.470889","exception":false,"start_time":"2020-12-24T07:39:34.44505","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 5. Simple Feature Engineering\n\nBefore we look directly at the text as a feature, let's think about some of the other first-order information we can extract from it. Here are a few features that may be informative:\n\n* Total length of the text\n* Average word length\n* Number of `@` mentions\n* Number of hashtags\n* Number of numeric values in the text (excluding timestamps)\n* Number of URLs in the text\n* Number of timestamps in the text\n* Hashtags in the text\n* `@` mentions in the text\n* Emojis in the text\n\nLet's go ahead and extract these fields."},{"metadata":{"papermill":{"duration":0.545192,"end_time":"2020-12-24T07:39:35.041956","exception":false,"start_time":"2020-12-24T07:39:34.496764","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import re\nimport emoji\n\ndef engineer_features(df):\n    df[\"total_length\"] = df[\"text\"].apply(len)\n    df[\"avg_word_length\"] = df[\"text\"].apply(lambda x: round(sum(len(word) for word in x.split()) / len(x.split())))\n    df[\"num_ats\"] = df[\"text\"].apply(lambda x: x.count(\"@\"))\n    df[\"num_hashtags\"] = df[\"text\"].apply(lambda x: x.count(\"#\"))\n    df[\"num_numeric\"] = df[\"text\"].apply(lambda x: len(re.findall(r\"\\w[0-9,]+\\w\", x)))\n    df[\"num_urls\"] = df[\"text\"].apply(lambda x: x.count(\"http\"))\n    df[\"num_timestamps\"] = df[\"text\"].apply(lambda x: len(re.findall(r\"[0-9]+:[0-9]+\", x)))\n    df[\"hashtags\"] = df[\"text\"].apply(lambda x: \" \".join([z.lower() for z in re.findall(r'#(\\w+)', x)]) or \"<none>\")\n    df[\"mentions\"] = df[\"text\"].apply(lambda x: \" \".join([z.lower() for z in re.findall(r'@(\\w+)', x)]) or \"<none>\")\n    df[\"has_emojis\"] = df[\"text\"].apply(lambda x: 1 if bool(emoji.get_emoji_regexp().search(x)) else 0)\n\nengineer_features(train)\nengineer_features(test)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.040736,"end_time":"2020-12-24T07:39:35.124089","exception":false,"start_time":"2020-12-24T07:39:35.083353","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 6. Pre-processing Text\n\nFor our textual analysis to be useful, we'll have to perform some pre-processing on the text first to make it easier to work with. First, let's check out the text fields and see what we're dealing with in more detail."},{"metadata":{"trusted":false},"cell_type":"code","source":"for _, row in train[\"text\"].head(50).iteritems():\n    print(row)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to do some simple cleanup to help out with our word analysis. First, let's convert to lowercase, and fix contractions."},{"metadata":{"trusted":false},"cell_type":"code","source":"import re\n\nfrom gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_multiple_whitespaces, strip_numeric, stem_text\n\ndef fix_contractions(x):\n    x = x.replace(\"&amp;\", \"and\")\n    x = x.replace(\"&lt;\", \"<\")\n    x = x.replace(\"&gt;\", \">\")\n    x = re.sub(\"(\\W|^)hwy\\.(\\W)\", \"\\\\1highway\\\\2\", x)\n    x = re.sub(\"(\\W|^)ave.(\\W)\", \"\\\\1avenue\\\\2\", x)\n    x = re.sub(\"(\\W|^)fyi(\\W)\", \"\\\\1for your information\\\\2\", x)\n    x = re.sub(\"(\\W|^)ain't(\\W)\", \"\\\\1am not\\\\2\", x)\n    x = re.sub(\"(\\W|^)can't(\\W)\", \"\\\\1cannot\\\\2\", x)\n    x = re.sub(\"(\\W|^)cant(\\W)\", \"\\\\1cannot\\\\2\", x)\n    x = x.replace(\"g'day\", \"good day\")\n    x = x.replace(\"giv'n\", \"given\")\n    x = x.replace(\"let's\", \"let us\")\n    x = x.replace(\"ma'am\", \"madam\")\n    x = x.replace(\"ne'er\", \"never\")\n    x = x.replace(\"o'clock\", \"of the clock\")\n    x = x.replace(\"o'er\", \"over\")\n    x = x.replace(\"ol'\", \"old\")\n    x = x.replace(\"shan't\", \"shall not\")\n    x = x.replace(\"y'all\", \"you all\")\n    x = x.replace(\"'tis\", \"it is\")\n    x = re.sub(\"\\W'twas\", \" it was\", x)\n    x = re.sub(\"\\W'cause\", \" because\", x)\n    x = re.sub(\"(\\w)'ve\", \"\\\\1 have\", x)\n    x = re.sub(\"(\\w)n't\", \"\\\\1 not\", x)\n    x = re.sub(\"(\\w)'s\", \"\\\\1 is\", x)\n    x = re.sub(\"(\\w)'d\", \"\\\\1 had\", x)\n    x = re.sub(\"(\\w)'ll\", \"\\\\1 will\", x)\n    x = re.sub(\"(\\w)'re\", \"\\\\1 are\", x)\n    x = re.sub(\"(\\w)'m\", \"\\\\1 am\", x)\n    x = x.replace(\"...\", \" \")\n    x = strip_punctuation(x)\n    x = strip_multiple_whitespaces(x)\n    return x.strip()\n\ndef lower_expand(df):\n    df[\"new_text\"] = df[\"text\"].apply(lambda x: x.lower())\n    df[\"new_text\"] = df[\"new_text\"].apply(fix_contractions)\n    \nlower_expand(train)\nlower_expand(test)\nfor _, row in train[\"new_text\"].head(50).iteritems():\n    print(row)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like there are certain tweets that have lots of personal pronouns, and some that don't. Let's see if tehre is any separation that may be useful."},{"metadata":{"trusted":false},"cell_type":"code","source":"has_personal_pronouns = []\n\ndef scan_for_pronouns(x):\n    words = x.split(\" \")\n    if \"i\" in words or \"me\" in words or \"you\" in words or \"my\" in words:\n        return 1\n    return 0\n\ndef has_personal_pronouns(df):\n    df[\"has_personal_pronouns\"] = df[\"new_text\"].apply(scan_for_pronouns)\n\nhas_personal_pronouns(train)\nhas_personal_pronouns(test)\n\ndisplay(pd.DataFrame(data=train[[\"id\", \"has_personal_pronouns\", \"target\"]].groupby([\"has_personal_pronouns\", \"target\"]).count()).rename(columns={\"id\": \"count\"}).head(50))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There may be some separation here that can work to our favor. Let's keep this categorical field of personal pronouns and move on to some more processing. Note that we may want to revisit word categories in the future. Specifically part-of-speech tagging may give us insights into different distributions of words that we may be able to make use of."},{"metadata":{},"cell_type":"markdown","source":"Here's what we're going to do:\n\n* Remove words that don't have any value, such as `the`, `of`, `and` (stopword removal)\n* We're going to strip out any links since they are `tco` encoded for Twitter\n* We'll remove all punctuation\n* We'll remove the numerics\n* We'll remove multiple whitespaces\n* Stem the text"},{"metadata":{"papermill":{"duration":2.716845,"end_time":"2020-12-24T07:39:37.881908","exception":false,"start_time":"2020-12-24T07:39:35.165063","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_multiple_whitespaces, strip_numeric, stem_text\n\ndef normalize_text(df):\n    normalized_text = []\n\n    for _, row in df.iterrows():\n        new_text = row[\"new_text\"]\n        new_text = remove_stopwords(new_text)\n        new_text = re.sub(r\"t co [\\w]+\", \"\", new_text)\n        new_text = strip_numeric(new_text)\n        new_text = strip_multiple_whitespaces(new_text)\n        new_text = stem_text(new_text)\n        normalized_text.append(new_text.strip())\n\n    df[\"normalized_text\"] = normalized_text\n\nnormalize_text(train)\nnormalize_text(test)\ntrain","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.041233,"end_time":"2020-12-24T07:39:37.965728","exception":false,"start_time":"2020-12-24T07:39:37.924495","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 7. Training and Validating the Classifier\n\nNow it's time to acutally build a classifier and see how well it does. To evaluate how well our features are working, we're going to split up our training data into a training set and a validation set. We'll do this 5 times for a 5-fold cross validation. Our difference between sets gives us an idea how robust our model actually is to variations in training data. To handle our textual data, we'll use CatBoost, as it can handle textual fields very nicely for us. It also allows us to conveniently handle categorical data as well, without having to resort to label encoding it ourselves. We'll set up CatBoost to automatically determine the best balance of each of the data fields for us, so we don't have to worry about a poorly performing field drowning out informative fields. We'll also set our evaluation metric for CatBoost to match the competition's output, so we can get a better idea of how well it's performing during our training phase."},{"metadata":{"papermill":{"duration":6.170033,"end_time":"2020-12-24T07:39:44.177505","exception":false,"start_time":"2020-12-24T07:39:38.007472","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import gc\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom catboost import CatBoostClassifier\n\nvectorizer = TfidfVectorizer()\nskf = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n\nfeatures = [\n    \"keyword\", \"state\", \"location_spam\", \"total_length\", \n    \"avg_word_length\", \"num_ats\", \"num_hashtags\", \"num_numeric\", \n    \"num_urls\", \"num_timestamps\", \"normalized_text\",\n    \"hashtags\", \"mentions\", \"has_emojis\", \"has_personal_pronouns\"\n]\n\ncat_params = {\n    \"cat_features\": [\"location_spam\", \"has_emojis\", \"has_personal_pronouns\"],\n    \"text_features\": [\"keyword\", \"state\", \"normalized_text\", \"hashtags\", \"mentions\"],\n    \"verbose\": 100,\n    \"learning_rate\": 0.05,\n    \"iterations\": 700,\n    \"eval_metric\": \"F1\",\n    \"random_state\": 2020,\n    \"depth\": 9,\n    \"auto_class_weights\": \"Balanced\",\n}\n\nimportances = pd.DataFrame()\nbest_score = 0.0\nbest_model = None\nbest_sgd = None\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train, train[\"target\"])):\n    print(\"-------> fold {} <--------\".format(fold + 1))\n    x_train, x_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[test_index])\n    y_train, y_valid = train[\"target\"].iloc[train_index], train[\"target\"].iloc[test_index]\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_valid_features = pd.DataFrame(x_valid[features])\n\n    print(\": Build CatBoost model\")\n    model = CatBoostClassifier(\n        **cat_params\n    )\n    model.fit(\n        x_train_features, \n        y_train,\n        eval_set=[(x_valid_features, y_valid)],\n        verbose=100,\n    )\n\n    train_predictions = model.predict(x_valid_features)\n    \n    print(model.get_feature_importance(prettified=True))\n    print(classification_report(y_valid, train_predictions, target_names=[\"Not Real\", \"Real\"]))\n    score = model.score(x_valid_features, y_valid)\n    if score > best_score:\n        print(\"--> This model is the best so far {:0.5}\".format(score))\n        best_model = model\n        best_score = score","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.058431,"end_time":"2020-12-24T07:39:44.295616","exception":false,"start_time":"2020-12-24T07:39:44.237185","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 8. Feature Performance\n\nWe can take a look and see how our various features are performing."},{"metadata":{"papermill":{"duration":0.644748,"end_time":"2020-12-24T07:39:44.99948","exception":false,"start_time":"2020-12-24T07:39:44.354732","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nplt.figure(figsize=(14, 35))\n_ = sns.barplot(x=\"Importances\", y=\"Feature Id\", data=best_model.get_feature_importance(prettified=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the text of the tweet, plus the keywords fields are the major driving forces for correct categorization. "},{"metadata":{"papermill":{"duration":0.051704,"end_time":"2020-12-24T07:39:45.103031","exception":false,"start_time":"2020-12-24T07:39:45.051327","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 9. Building and Submitting the Final Model\n\nLet's go ahead and build a model that uses all of the data, and takes all the features we've examined so far. Once the model is built, we can submit the result."},{"metadata":{"papermill":{"duration":1.105521,"end_time":"2020-12-24T07:39:46.2623","exception":false,"start_time":"2020-12-24T07:39:45.156779","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"train_features = pd.DataFrame(train[features])\n\nprint(\": Build CatBoost model\")\nmodel = CatBoostClassifier(\n    **cat_params\n)\nmodel.fit(\n    train_features, \n    train[\"target\"],\n)\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.045925,"end_time":"2020-12-24T07:39:46.354039","exception":false,"start_time":"2020-12-24T07:39:46.308114","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Here is the code to run the predictions on the test data, and build the submission file."},{"metadata":{"papermill":{"duration":0.442126,"end_time":"2020-12-24T07:39:46.841486","exception":false,"start_time":"2020-12-24T07:39:46.39936","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"test_features = pd.DataFrame(test[features])\npredictions = model.predict(test_features)\nsubmission = pd.DataFrame({\"id\": test[\"id\"], \"target\": predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}