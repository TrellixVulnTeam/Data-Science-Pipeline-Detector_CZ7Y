{"cells":[{"metadata":{"papermill":{"duration":0.036731,"end_time":"2020-12-28T03:04:35.862708","exception":false,"start_time":"2020-12-28T03:04:35.825977","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Introduction\n\nThe *Real or Not? NLP with Disaster Tweets* competitions offers a neat opportunity to see how different approaches to natural language processing work when compared to one another. In this notebook, we'll look at using TF/IDF vectors and a support vector machine to classify tweets. In this notebook we will:\n\n* Clean and normalize the data set.\n* Perform rudimentary natural language processing on the text field.\n* Evaluate various hyper-parameters for an `SVC` model.\n* Use the model and make predictions that we can submit to the competition.\n\n## Credits\n\nThis notebook is similar to the approach of [Disaster Tweets 80.263% accuracy using SVC](https://www.kaggle.com/sauravjoshi23/disaster-tweets-80-263-accuracy-using-svc) by Saurav Joshi. This notebook differs in that it removes duplicate tweets and uses `gensim` functions to perform most of the cleaning. This notebook also performs simple kernel performance tuning by looking at some optional values to `SVC`. Otherwise, the approach is the same. Please examine his notebook and upvote it!"},{"metadata":{"papermill":{"duration":0.034396,"end_time":"2020-12-28T03:04:35.931679","exception":false,"start_time":"2020-12-28T03:04:35.897283","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 1. Importing the Data\n\nThe first step in the process is to import our training data so we can see what kinds of information we have to work with. For this project, we'll start by importing the entire training dataset into a single Pandas dataframe."},{"metadata":{"papermill":{"duration":1.272412,"end_time":"2020-12-28T03:04:37.239142","exception":false,"start_time":"2020-12-28T03:04:35.96673","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ndisplay(train)\n\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ndisplay(test)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.03667,"end_time":"2020-12-28T03:04:37.313599","exception":false,"start_time":"2020-12-28T03:04:37.276929","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 1.1 Eliminating Duplicates\n\nOne thing we should do is check to see if we have duplicated or conflicting data. Here's an easy way to check for textual duplicates against the `target` - which is the class we're trying to predict."},{"metadata":{"papermill":{"duration":0.367193,"end_time":"2020-12-28T03:04:37.718059","exception":false,"start_time":"2020-12-28T03:04:37.350866","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"duplicates = pd.concat(x for _, x in train.groupby([\"text\"]) if len(x) > 1)\nwith pd.option_context(\"display.max_rows\", None, \"max_colwidth\", 240):\n    display(duplicates[[\"id\", \"target\", \"text\"]])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.040467,"end_time":"2020-12-28T03:04:37.799587","exception":false,"start_time":"2020-12-28T03:04:37.75912","status":"completed"},"tags":[]},"cell_type":"markdown","source":"It looks like we have quite a few duplicates. In some instances, the duplicates resolve to the same target class, but in others such as duplicate indexes `5620` and `5641`, we have the same tweet belonging to two different classes. For those instances where the tweet belongs to the same class, we can simply delete the duplicates."},{"metadata":{"papermill":{"duration":0.326166,"end_time":"2020-12-28T03:04:38.166699","exception":false,"start_time":"2020-12-28T03:04:37.840533","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"train.drop(\n    [\n        6449, 7034, 3589, 3591, 3597, 3600, 3603, \n        3604, 3610, 3613, 3614, 119, 106, 115,\n        2666, 2679, 1356, 7609, 3382, 1335, 2655, \n        2674, 1343, 4291, 4303, 1345, 48, 3374,\n        7600, 164, 5292, 2352, 4308, 4306, 4310, \n        1332, 1156, 7610, 2441, 2449, 2454, 2477,\n        2452, 2456, 3390, 7611, 6656, 1360, 5771, \n        4351, 5073, 4601, 5665, 7135, 5720, 5723,\n        5734, 1623, 7533, 7537, 7026, 4834, 4631, \n        3461, 6366, 6373, 6377, 6378, 6392, 2828,\n        2841, 1725, 3795, 1251, 7607\n    ], inplace=True\n)\nduplicates = pd.concat(x for _, x in train.groupby([\"text\"]) if len(x) > 1)\nwith pd.option_context(\"display.max_rows\", None, \"max_colwidth\", 240):\n    display(duplicates[[\"id\", \"target\", \"text\"]])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.042574,"end_time":"2020-12-28T03:04:38.251736","exception":false,"start_time":"2020-12-28T03:04:38.209162","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Now we're facing a challenge. We could keep one duplicate with one target class, but we don't have access to the method by which the dataset creators used to mark up real versus not real disaster tweets. They may have had access to more information than us, so we have to be careful if we alter the dataset - we could introduce personal bias. While it may be tempting to try to keep some of the data (e.g. `that horrible sinking feeling when you've been at home on your phone for a while and you realise its been on 3G this whole time` seems like it should be marked as `not real`), the better approach is to simply delete the offending duplicates. While this cuts our training size down, we ensure we haven't inadventently introduced bias to the dataset."},{"metadata":{"papermill":{"duration":0.056837,"end_time":"2020-12-28T03:04:38.351072","exception":false,"start_time":"2020-12-28T03:04:38.294235","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"train.drop(\n    [\n        4290, 4299, 4312, 4221, 4239, 4244, 2830, \n        2831, 2832, 2833, 4597, 4605, 4618, 4232, \n        4235, 3240, 3243, 3248, 3251, 3261, 3266, \n        4285, 4305, 4313, 1214, 1365, 6614, 6616, \n        1197, 1331, 4379, 4381, 4284, 4286, 4292, \n        4304, 4309, 4318, 610, 624, 630, 634, 3985,\n        4013, 4019, 1221, 1349, 6091, 6094, \n        6103, 6123, 5620, 5641\n    ], inplace=True\n)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.050776,"end_time":"2020-12-28T03:10:05.450325","exception":false,"start_time":"2020-12-28T03:10:05.399549","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 2. Pre-processing Text\n\nFor our textual analysis to be useful, we'll have to perform some pre-processing on the text first to make it easier to work with. We'll convert to lowercase, and fix contractions. We'll also remove any stopwords, strip the punctuation, remove multiple whitespaces, and stem the text."},{"metadata":{"papermill":{"duration":31.433266,"end_time":"2020-12-28T03:10:37.156495","exception":false,"start_time":"2020-12-28T03:10:05.723229","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import re\n\nfrom gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_multiple_whitespaces, strip_numeric, stem_text\nfrom textblob import TextBlob\n\ndef fix_text_issues(x):\n    x = x.lower()\n    x = x.replace(\"&amp;\", \"and\")\n    x = x.replace(\"&lt;\", \"<\")\n    x = x.replace(\"&gt;\", \">\")\n    x = re.sub(\"(\\W|^)hwy\\.(\\W)\", \"\\\\1highway\\\\2\", x)\n    x = re.sub(\"(\\W|^)ave.(\\W)\", \"\\\\1avenue\\\\2\", x)\n    x = re.sub(\"(\\W|^)fyi(\\W)\", \"\\\\1for your information\\\\2\", x)\n    x = re.sub(\"(\\W|^)ain't(\\W)\", \"\\\\1am not\\\\2\", x)\n    x = re.sub(\"(\\W|^)can't(\\W)\", \"\\\\1cannot\\\\2\", x)\n    x = re.sub(\"(\\W|^)cant(\\W)\", \"\\\\1cannot\\\\2\", x)\n    x = re.sub(\"(\\W|^)rt(\\W)\", \"\\\\1retweet\\\\2\", x)\n    x = x.replace(\"g'day\", \"good day\")\n    x = x.replace(\"giv'n\", \"given\")\n    x = x.replace(\"let's\", \"let us\")\n    x = x.replace(\"ma'am\", \"madam\")\n    x = x.replace(\"ne'er\", \"never\")\n    x = x.replace(\"o'clock\", \"of the clock\")\n    x = x.replace(\"o'er\", \"over\")\n    x = x.replace(\"ol'\", \"old\")\n    x = x.replace(\"shan't\", \"shall not\")\n    x = x.replace(\"y'all\", \"you all\")\n    x = x.replace(\"'tis\", \"it is\")\n    x = x.replace(\"'hood\", \"neighborhood\")\n    x = x.replace(\"can ªt\", \"cannot\")\n    x = x.replace(\"åÊ\", \" \")\n    x = x.replace(\"ÛÏ\", \" \")\n    x = x.replace(\"?Û\", \" \")\n    x = x.replace(\"Û_\", \" \")\n    x = x.replace(\"Û÷\", \" \")\n    x = x.replace(\"ûò\", \" \")\n    x = x.replace(\"û\", \" \")\n    x = re.sub(\"\\W'twas\", \" it was\", x)\n    x = re.sub(\"\\W'cause\", \" because\", x)\n    x = re.sub(\"(\\w)'ve\", \"\\\\1 have\", x)\n    x = re.sub(\"(\\w)n't\", \"\\\\1 not\", x)\n    x = re.sub(\"(\\w)'s\", \"\\\\1 is\", x)\n    x = re.sub(\"(\\w)'d\", \"\\\\1 had\", x)\n    x = re.sub(\"(\\w)'ll\", \"\\\\1 will\", x)\n    x = re.sub(\"(\\w)'re\", \"\\\\1 are\", x)\n    x = re.sub(\"(\\w)'m\", \"\\\\1 am\", x)\n    x = re.sub(\"http[s]*://t.co/\\S+\", \"\", x)\n    x = x.replace(\"...\", \" \")\n    x = strip_multiple_whitespaces(x)\n    x = strip_punctuation(x)\n    x = stem_text(x)\n    return x.strip()\n\ndef clean_text(df):\n    df[\"text\"] = df[\"text\"].apply(fix_text_issues)\n\nclean_text(train)\nclean_text(test)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.056186,"end_time":"2020-12-28T03:10:41.509935","exception":false,"start_time":"2020-12-28T03:10:41.453749","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 3. Training and Validating the Classifier\n\nLet's build a classifier to try and classify various forms of tweets. We'll use the `SVC` classifier for the task. We'll use the `GridSearchCV` classifier to perform an exhaustive search over some common parameters to the `TfidfVectorizer` and the `SVC` class."},{"metadata":{"papermill":{"duration":3378.293409,"end_time":"2020-12-28T04:06:59.859927","exception":false,"start_time":"2020-12-28T03:10:41.566518","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([('tfidf', TfidfVectorizer(decode_error=\"ignore\")), ('clf', SVC(random_state=2020))])\nparameters = {\n    'tfidf__ngram_range': ((1,1), (1,2), (2,2)),\n    'tfidf__use_idf': (True, False), \n    'tfidf__smooth_idf': (True, False),\n    'tfidf__sublinear_tf': (True, False),\n    'clf__C': (1.5, 1.7, 1.9),\n}\n\ngrid = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=3, cv=3)\ngrid_result = grid.fit(train[\"text\"], train[\"target\"])\n\nprint(\"Best score: {:0.5f}\".format(grid_result.best_score_))\nprint(\"Best parameters: {}\".format(grid_result.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use those best tuning parameters, and run it on a train test split to see more detailed performance."},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(\n    train[\"text\"], train[\"target\"], test_size=0.2, random_state=2020\n)\n\nvectorizer = TfidfVectorizer(\n    decode_error=\"ignore\", \n    ngram_range=(1,2), \n    smooth_idf=False, \n    sublinear_tf=True, \n    use_idf=True\n)\nx_train_tfidf = vectorizer.fit_transform(x_train)\nx_valid_tfidf = vectorizer.transform(x_valid)\n\nmodel = SVC(random_state=2020, C=1.7)\nmodel.fit(x_train_tfidf, y_train)\n\ntrain_predictions = model.predict(x_valid_tfidf)\n    \nprint(classification_report(y_valid, train_predictions, target_names=[\"Not Real\", \"Real\"]))\nscore = model.score(x_valid_tfidf, y_valid)\nprint(\"--> Mean accuracy {:0.5}\".format(score))\nprint(\"\")","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.078153,"end_time":"2020-12-28T04:07:03.196683","exception":false,"start_time":"2020-12-28T04:07:03.11853","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# 4. Building and Submitting the Final Model\n\nLet's go ahead and build a model that uses all of the data. Once the model is built, we can submit the result."},{"metadata":{"papermill":{"duration":756.897864,"end_time":"2020-12-28T04:19:40.172325","exception":false,"start_time":"2020-12-28T04:07:03.274461","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n    decode_error=\"ignore\", \n    ngram_range=(1,2), \n    smooth_idf=False, \n    sublinear_tf=True, \n    use_idf=True\n)\ntrain_tfidf = vectorizer.fit_transform(train[\"text\"])\nmodel = SVC(random_state=2020, C=1.7)\nmodel.fit(train_tfidf, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.081831,"end_time":"2020-12-28T04:19:40.337452","exception":false,"start_time":"2020-12-28T04:19:40.255621","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Here is the code to run the predictions on the test data, and build the submission file."},{"metadata":{"papermill":{"duration":0.428486,"end_time":"2020-12-28T04:19:40.904522","exception":false,"start_time":"2020-12-28T04:19:40.476036","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"clean_text(test)\ntest_tfidf = vectorizer.transform(test[\"text\"])\npredictions = model.predict(test_tfidf)\nsubmission = pd.DataFrame({\"id\": test[\"id\"], \"target\": predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}