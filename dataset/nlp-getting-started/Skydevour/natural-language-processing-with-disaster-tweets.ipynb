{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n#-*- coding:utf-8 -*- \n# @Time : 2021/1/16 16:46 \n# @Author : 万**\n# @File : NLP.py \n# @Software: PyCharm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文\nplt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号\n\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n# print(train_df.head())\n# print(test_df.head())\n\n# print(train_df.info())\n# print(test_df.info())\n\n# print(train_df.isnull().sum())\n# print(test_df.isnull().sum())\n\ntrain_df = train_df.drop(['location','keyword'],axis=1)\ntest_df = test_df.drop(['location','keyword'],axis=1)\n\n# 填补\n# train_df.keyword.fillna('accident',inplace=True)\n\n# print(train_df.isnull().sum())\n# print(test_df.isnull().sum())\n\n# 找0和1的占比\nreal_tweets = len(train_df[train_df[\"target\"] == 1])\nreal_tweets_percentage = real_tweets/train_df.shape[0]*100\nfake_tweets_percentage = 100 - real_tweets_percentage\n\n# print(\"Real tweets percentage: \",real_tweets_percentage)\n# print(\"Fake tweets percentage: \",fake_tweets_percentage)\n\nsns.countplot(x='target',data=train_df)\n# plt.show()\n\n# disaster tweets\ndisaster_tweets = train_df[train_df['target'] == 1]['text']\n# non-disaster tweets\nnon_disaster_tweets = train_df[train_df['target'] != 1]['text']\n\n# print(disaster_tweets)\n\n# word cloud of disaster and non-disaster tweets\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 5])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40)\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40)\n# plt.show()\n\n# 数据清洗\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ntrain_df['text'] = train_df['text'].apply(lambda x: clean_text(x))\ntest_df['text'] = test_df['text'].apply(lambda x: clean_text(x))\n\n# print(train_df['text'].head())\n\ntokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain_df['text'] = train_df['text'].apply(lambda x:tokenizer.tokenize(x))\ntest_df['text'] = test_df['text'].apply(lambda x:tokenizer.tokenize(x))\n# print(train_df['text'].head())\n\n# 删除停用词\ndef remove_stopwords(text):\n    words = [w for w in text if w not in ['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\"]]\n    return words\ntrain_df['text'] = train_df['text'].apply(lambda x : remove_stopwords(x))\ntest_df['text'] = test_df['text'].apply(lambda x : remove_stopwords(x))\n# print(test_df.head())\n\n# 词性还原\nlem = WordNetLemmatizer()\ndef lem_word(x):\n    return [lem.lemmatize(w) for w in x]\n\ntrain_df['text'] = train_df['text'].apply(lem_word)\ntest_df['text'] = test_df['text'].apply(lem_word)\n\n# print(train_df['text'][:10])\n\n\ndef combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain_df['text'] = train_df['text'].apply(lambda x: combine_text(x))\ntest_df['text'] = test_df['text'].apply(lambda x: combine_text(x))\n# print(train_df.head())\n\n# 统计词汇量\ncount_vectorizer = CountVectorizer()\ntrain_vector = count_vectorizer.fit_transform(train_df['text'])\ntest_vector = count_vectorizer.transform(test_df['text'])\n# print(train_vector[0].todense())\n\n# 判断该词的重要性\ntfidf = TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2))\ntrain_tfidf = tfidf.fit_transform(train_df['text'])\ntest_tfidf = tfidf.transform(test_df['text'])\n\n# XGB Classifier\nxgb_param = xgb.XGBClassifier(max_depth=5,n_estimators=500,colsample_bytree=0.8,nthread=10,learning_rate=0.05)\n\nscores_vector = model_selection.cross_val_score(xgb_param,train_vector,train_df['target'],cv=5,scoring='f1')\n# print(scores_vector)\n\nscores_tfidf = model_selection.cross_val_score(xgb_param,train_tfidf,train_df['target'],cv=5,scoring='f1')\n# print(scores_tfidf)\n# print(xgb_param.get_params())\n\n# MultiNomial Naive Bayes\nmnb = MultinomialNB(alpha = 2.0)\nscores_vector = model_selection.cross_val_score(mnb,train_vector,train_df['target'],cv=5,scoring='f1')\n# print(\"score:\",scores_vector)\nscores_tfidf = model_selection.cross_val_score(mnb,train_tfidf,train_df['target'],cv=5,scoring='f1')\n# print(\"score of tfidf:\",scores_tfidf)\n# print(mnb.get_params())\n\n# Logistic Regression\nlg = LogisticRegression(C = 1.0)\nscores_vector = model_selection.cross_val_score(lg, train_vector, train_df[\"target\"], cv=5, scoring=\"f1\")\nprint(\"score:\",scores_vector)\nscores_tfidf = model_selection.cross_val_score(lg, train_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\nprint(\"score of tfidf:\",scores_tfidf)\nprint(lg.get_params())\n\nmnb.fit(train_tfidf, train_df[\"target\"])\ny_pred = mnb.predict(test_tfidf)\nprint(y_pred)\n\n# Submission\nsubmission_df2 = pd.DataFrame({'Id':test_df['id'],'target':y_pred})\nsubmission_df2.to_csv('submission_df2.csv',index=False)\nsubmission_df2 = pd.read_csv('submission_df2.csv')\nprint(submission_df2.head())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}