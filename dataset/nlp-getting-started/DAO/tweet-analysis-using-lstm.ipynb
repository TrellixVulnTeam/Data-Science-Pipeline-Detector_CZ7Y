{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Tweet analysis using LSTM**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras.layers import Dense, LSTM, Embedding, Activation\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import RMSprop\nfrom transformers import BertTokenizer\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = train_df['text'].values\nfor i in range(5):\n    print('{} : {}'.format(i, tweets[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Vectorize word**"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 50\nx_train = []\nfor tweet in tweets:\n    vec = encode_sentence(tweet)\n    x_train.append(vec[:max_len] + [0] * (max_len - len(vec)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = np.array(x_train)\nn = np.amax(x_train)\nprint(x_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_df['target'].values\ny_train = np.array(y_train)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 15\nBATCH_SIZE = 32\nmodel=Sequential()\nmodel.add(Embedding(n + 1, BATCH_SIZE, mask_zero=True))\nmodel.add(LSTM(BATCH_SIZE))\nmodel.add(Dense(2, activation = 'sigmoid'))\noptimizer = RMSprop(lr = 0.01)\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer = optimizer)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train, y_train, epochs = EPOCHS, batch_size = BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/nlp-getting-started/test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = test_df['text'].values\nx_test = []\nfor tweet in tweets:\n    vec = encode_sentence(tweet)\n    x_test.append(vec[:max_len] + [0] * (max_len - len(vec)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predict answer**"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"y_test = [np.argmax(model.predict(np.array([x_test_]))) for x_test_ in x_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame({'id':test_df['id'].values, 'target':y_test})\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('./submission.csv', index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}