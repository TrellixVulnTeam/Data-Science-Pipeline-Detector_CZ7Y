{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **IMPORT DATA**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train_data,test_data],axis=0)\nprint(data.describe())\nimport matplotlib.pyplot as plt\nplt.hist(data.target)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **TEXT PREPROCESSING**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load('en')\ndef preprocessing(text):\n  text = text.replace('#','')\n  text = decontracted(text)\n  text = re.sub('\\S*@\\S*\\s?','',text)\n  text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n  token=[]\n  result=''\n  text = re.sub('[^A-z]', ' ',text.lower())\n  \n  text = nlp(text)\n  for t in text:\n    if not t.is_stop and len(t)>2:  \n      token.append(t.lemma_)\n  result = ' '.join([i for i in token])\n\n  return result.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.text = data.text.apply(lambda x : preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **USING PRE-TRAINED GLOVE WEIGHTS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nembedding_dict={}\nwith open('../input/glove6b200d/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nMAX_LEN=40\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnum_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,200))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **IMPLEMENTING MODEL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D,Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\nmodel=Sequential()\n\nembedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64,dropout=0.2, recurrent_dropout=0.2))\n\n\nmodel.add(Dense(1, activation='sigmoid'))\n\n\n\noptimzer=Adam(learning_rate=0.001)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **USING CALLBACKS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint,EarlyStopping\nfilepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\nes = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=30)\ncallbacks_list = [checkpoint,es]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntweet =data.iloc[:7613,:]\ntest = data.iloc[7613:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.2)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=100,epochs=20,callbacks=callbacks_list,validation_data=(X_test,y_test),verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **LOADING BEST MODEL AND PREDICTING RESULTS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.models import load_model\n# model = load_model('../input/finalmodel/weights-improvement-21-0.84.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pre = model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##DON'T FORGET TO UPVOTE##\n##1ST NOTEBOOK KERNEL##"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}