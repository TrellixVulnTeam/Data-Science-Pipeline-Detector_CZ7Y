{"cells":[{"metadata":{"id":"l8v_jfycBi-9"},"cell_type":"markdown","source":"**HuggingFace Transformers**\n\nðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch."},{"metadata":{"id":"D3I__ITb5bJN","outputId":"69f1b0af-c668-4cdb-dc0d-c9338ec55a46","trusted":true},"cell_type":"code","source":"!pip install transformers\nimport transformers","execution_count":null,"outputs":[]},{"metadata":{"id":"yUWeJm3M1aN2","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"nT7RFUmB2TtH","outputId":"5297e49b-1f55-4911-dc7d-f1fc5c229fdc","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.title('Train Data')\nplt.xlabel('Target Distribution')\nplt.ylabel('Samples')\nplt.hist(train.target)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"P7jihemPCcmc"},"cell_type":"markdown","source":"**Pre-Processing** (Optional)\n\n1--> Removing Contraction (Decontraction)\n\n2--> Dealing with HashTags\n\n3--> Removing URLs and Email\n\n4--> Removing Stopwords and Lemmatization\n\n"},{"metadata":{"id":"xJyXCSw_2vVX","trusted":true},"cell_type":"code","source":"\n# def decontracted(phrase):\n#     # specific\n#     phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n#     phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n#     # general\n#     phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n#     phrase = re.sub(r\"\\'re\", \" are\", phrase)\n#     phrase = re.sub(r\"\\'s\", \" is\", phrase)\n#     phrase = re.sub(r\"\\'d\", \" would\", phrase)\n#     phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n#     phrase = re.sub(r\"\\'t\", \" not\", phrase)\n#     phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n#     phrase = re.sub(r\"\\'m\", \" am\", phrase)\n#     return phrase","execution_count":null,"outputs":[]},{"metadata":{"id":"7r0mOv_S20aT","trusted":true},"cell_type":"code","source":"# import spacy\n# import re\n# nlp = spacy.load('en')\n# def preprocessing(text):\n#   text = text.replace('#','')\n#   text = decontracted(text)\n#   text = re.sub('\\S*@\\S*\\s?','',text)\n#   text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n#   token=[]\n#   result=''\n#   text = re.sub('[^A-z]', ' ',text.lower())\n  \n#   text = nlp(text)\n#   for t in text:\n#     if not t.is_stop and len(t)>2:  \n#       token.append(t.lemma_)\n#   result = ' '.join([i for i in token])\n\n#   return result.strip()","execution_count":null,"outputs":[]},{"metadata":{"id":"Vc_vvnTh25C8","trusted":true},"cell_type":"code","source":"# train.text = train.text.apply(lambda x : preprocessing(x))\n# test.text = test.text.apply(lambda x : preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"_mSAYpvLEuJT"},"cell_type":"markdown","source":"**Loading BertTokenizer**\n\nIt is based on WordPiece Approach"},{"metadata":{"id":"S7DVIetj5Wzm","trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"cw76Bm3g5qxL","trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf ","execution_count":null,"outputs":[]},{"metadata":{"id":"xTqQtN0LFKVw"},"cell_type":"markdown","source":"**BERT Encoding**\n\nData is encoded according to BERT requirement.There is a very helpful function called encode_plus provided in the Tokenizer class. It can seamlessly perform the following operations:\n\n\n\n*   Tokenize the text\n*   Add special tokens - [CLS] and [SEP]\n\n*   Add special tokens - [CLS] and [SEP]\n*   create token IDs\n\n*   Pad the sentences to a common length\n*   Create attention masks for the above PAD tokens\n"},{"metadata":{"id":"Cd9SmmP73QDh","trusted":true},"cell_type":"code","source":"def bert_encode(data,maximum_length) :\n  input_ids = []\n  attention_masks = []\n  \n\n  for i in range(len(data.text)):\n      encoded = tokenizer.encode_plus(\n        \n        data.text[i],\n        add_special_tokens=True,\n        max_length=maximum_length,\n        pad_to_max_length=True,\n        \n        return_attention_mask=True,\n        \n      )\n      \n      input_ids.append(encoded['input_ids'])\n      attention_masks.append(encoded['attention_mask'])\n  return np.array(input_ids),np.array(attention_masks)","execution_count":null,"outputs":[]},{"metadata":{"id":"CwRLIO7LLuMf"},"cell_type":"markdown","source":"**Input are 2 Numpy array. Let me briefly go over them:**\n\n1) input_ids : list of token ids to be fed to a model\n\n2) attention_masks: list of indices specifying which tokens should be attended to by the model.The input sequences are denoted by 1 and the padded ones by 0. These masks help to differentiate between the two.\n\n**Note** : Token Ids are not necessary as it is used Two Sentence Problem (To differentiate two sentence)\n"},{"metadata":{"id":"qEy-naSN30-K","trusted":true},"cell_type":"code","source":"train_input_ids,train_attention_masks = bert_encode(train,60)\ntest_input_ids,test_attention_masks = bert_encode(test,60)","execution_count":null,"outputs":[]},{"metadata":{"id":"f6El9miVNGf_"},"cell_type":"markdown","source":"**Creating Custom Model**\n\nBase TFBert Model with Dense layer and sigmoid activation as head. "},{"metadata":{"id":"L4jA6BAc46II","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\ndef create_model(bert_model):\n  input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n  attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n  \n  output = bert_model([input_ids,attention_masks])\n  output = output[1]\n  output = tf.keras.layers.Dense(32,activation='relu')(output)\n  output = tf.keras.layers.Dropout(0.2)(output)\n\n  output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n  model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n  model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])\n  return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"IjU6OOBYNoeb"},"cell_type":"markdown","source":"**TFBertModel**\n\nThe bare Bert Model transformer outputing raw hidden-states without any specific head on top. \nhttps://huggingface.co/transformers/model_doc/bert.html#tfbertmodel"},{"metadata":{"id":"-8ea4pXR6N7J","trusted":true},"cell_type":"code","source":"from transformers import TFBertModel\nbert_model = TFBertModel.from_pretrained('bert-large-uncased')","execution_count":null,"outputs":[]},{"metadata":{"id":"QmbsABFiOBKo"},"cell_type":"markdown","source":"**Implementing Custom Model**"},{"metadata":{"id":"8GCGbmmX6iHx","outputId":"cd2510fe-7396-4cd2-ad60-e16534d9c8aa","trusted":true},"cell_type":"code","source":"model = create_model(bert_model)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"AWjS_3BJOFXs"},"cell_type":"markdown","source":"**Callbacks** (Optional)\n\nTo get best model according to maximum validation accuracy.\n\n"},{"metadata":{"id":"DbdKL8euphGn","trusted":true},"cell_type":"code","source":"# filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n# checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n# callbacks_list = [checkpoint]\n# add callbacks = callbacks_list to model.fit","execution_count":null,"outputs":[]},{"metadata":{"id":"1xNZokWcObGQ"},"cell_type":"markdown","source":"**Training**"},{"metadata":{"id":"nPsSo1eT6j_F","outputId":"e0f2f210-9c20-4513-e3b3-737c99bd2dfc","trusted":true},"cell_type":"code","source":"history = model.fit([train_input_ids,train_attention_masks],train.target,validation_split=0.2, epochs=2,batch_size=10)","execution_count":null,"outputs":[]},{"metadata":{"id":"ubZYSXJTOg_8"},"cell_type":"markdown","source":"**Testing**"},{"metadata":{"id":"GGln1FTiHwgs","trusted":true},"cell_type":"code","source":"result = model.predict([test_input_ids,test_attention_masks])\nresult = np.round(result).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"id":"msmk-9LWOluB"},"cell_type":"markdown","source":"**Preparing Submission File**"},{"metadata":{"id":"cDUCScg-H5Wc","trusted":true},"cell_type":"code","source":"result = pd.DataFrame(result)\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\noutput = pd.DataFrame({'id':submission.id,'target':result[0]})\noutput.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"BJu_Feg3qhO5","outputId":"3d21311c-428c-4a15-fe32-dd49c344320d","trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"_X2h4_9tqkS6","outputId":"991f6184-ff01-4dcf-efb6-02a3aa15891b","trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}