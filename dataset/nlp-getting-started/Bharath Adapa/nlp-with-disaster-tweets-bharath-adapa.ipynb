{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Importing the Text Processing Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport sklearn\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nimport re\nfrom nltk.tokenize import TweetTokenizer\nimport gensim\nfrom gensim.models import Word2Vec\nimport re\nimport random","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:21:08.559531Z","iopub.execute_input":"2021-12-10T09:21:08.559951Z","iopub.status.idle":"2021-12-10T09:21:10.168226Z","shell.execute_reply.started":"2021-12-10T09:21:08.559909Z","shell.execute_reply":"2021-12-10T09:21:10.167284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading the Tweets Data","metadata":{}},{"cell_type":"code","source":"path='../input/nlp-getting-started'\ntrain=pd.read_csv(path+'/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:21:10.171055Z","iopub.execute_input":"2021-12-10T09:21:10.171626Z","iopub.status.idle":"2021-12-10T09:21:10.226589Z","shell.execute_reply.started":"2021-12-10T09:21:10.171584Z","shell.execute_reply":"2021-12-10T09:21:10.225607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the type of training data provided","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:21:10.228737Z","iopub.execute_input":"2021-12-10T09:21:10.229118Z","iopub.status.idle":"2021-12-10T09:21:10.258666Z","shell.execute_reply.started":"2021-12-10T09:21:10.229089Z","shell.execute_reply":"2021-12-10T09:21:10.257748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for null values in the train data","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()/len(train)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:21:10.260091Z","iopub.execute_input":"2021-12-10T09:21:10.26061Z","iopub.status.idle":"2021-12-10T09:21:10.27831Z","shell.execute_reply.started":"2021-12-10T09:21:10.26057Z","shell.execute_reply":"2021-12-10T09:21:10.277476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since, here Twitter Data is being used, the glove embeddings for twitter will be used. Loading the path for the embeddings","metadata":{}},{"cell_type":"code","source":"glove_loc='../input/d/fullmetal26/glovetwitter27b100dtxt/glove.twitter.27B.200d.txt'","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:21:10.280727Z","iopub.execute_input":"2021-12-10T09:21:10.28124Z","iopub.status.idle":"2021-12-10T09:21:10.285607Z","shell.execute_reply.started":"2021-12-10T09:21:10.281199Z","shell.execute_reply":"2021-12-10T09:21:10.28445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Designing Custom Tokenizer which has special tokens like 'user','number','hashtag','allcaps' and 'repeat'which replaces many of words which are specific like username, url link, hashtag before a word.","metadata":{}},{"cell_type":"code","source":"def custom_tokenize(seq):\n    url_regex=r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n    #replaces all the links with url\n    seq=re.sub(url_regex,'<URL><sp>',seq) \n    \n    # removes words which cannot be encoded\n    seq=re.sub('[^\\x00-\\x7F]+.','',seq)\n    \n    # replaces all the numbers with a general token NUMBER\n    seq=re.sub('[0-9]+','<NUMBER><sp>',seq)\n    \n    # Replaces all the usernames with generalized token USER\n    seq=re.sub('@\\w+', \"<USER><sp>\",seq)\n    \n    #Replaces the hashtags before words with token HASHTAG\n    hash_words=re.findall('#[A-z]{1,}',seq)\n    for i in hash_words:\n        seq=re.sub(i,'<HASHTAG><sp>'+i.split('#')[1],seq)\n    reg_split=r'\\s|<sp>'\n    seq=re.split(reg_split,seq)\n    \n    \n    for i,j in enumerate(seq):\n\n        non_cap=re.findall('[A-Z]{1}[a-z]+',j)\n        #replaces all the capital letter words with this ALLCAPS token followed with \n        #the original words\n        if len(non_cap)>0:\n            cap_in_noncap=re.split(non_cap[0],j)\n            if len(non_cap[0])//len(j)==0:\n                if len(cap_in_noncap)>0:\n                    #replaces all the capital letter words with this ALLCAPS token followed with \n                    #the original words\n                    seq[i]=' '.join([cap_in_noncap[0].lower(),'<ALLCAPS>',non_cap[0].lower()])\n        \n        all_cap=re.findall('^([A-Z]{2,}[^a-z]+)$',j)\n        if len(all_cap)>0:\n            seq[i]=re.findall('[^(.,`;@_!#$%^&*()<>?/|}{~:\\'\\-)]+',j)[0].lower()+' <ALLCAPS>'\n\n\n        repeat=re.findall('[.,!?:\\'\\-]',j)\n        \n        # Repeating special characters will be replaced with this token REPEAT\n        \n        if len(repeat)>1:\n            seq[i]=' '.join([seq[i],repeat[0]+' <REPEAT>'])\n        else:\n            if repeat:\n                seq[i]=' '.join([re.split('[.,!?:\\'\\-]',j)[0],re.findall('[.,!?:\\'\\-]',j)[0]])\n        \n        spe_words=re.split('[.,`;_!$%^&*()?/|}{~:\\'\\-]',j)\n        expre=re.split('[^(.,`;_!$%^&*()?/|}{~:)\\'\\-]',j) \n        \n        if len(spe_words)>2:\n            temp1=[z for z in spe_words if z]\n            temp2=[z for z in expre if z][-1]\n            temp3=re.findall('[.,`;_!$%^&*()?/|}{~:\\'\\-]',temp2)\n            if len(temp3)>1:\n                seq[i]=' '.join(temp1+[temp3[0],'<REPEAT>'])\n        \n        exp=re.findall('[.,`;_!$%^&*()?/|}{~:\\'\\-]',j)\n        if len(exp)==1:\n            seq[i]=re.sub('[.,`;_!$%^&*()?/|}{~:\\'\\-]',' '+exp[0]+' ',j)\n    seq=' '.join(seq)\n    seq=seq.split(' ')\n    return [i.lower() for i in seq if i]","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:21:10.287727Z","iopub.execute_input":"2021-12-10T09:21:10.288101Z","iopub.status.idle":"2021-12-10T09:21:10.309083Z","shell.execute_reply.started":"2021-12-10T09:21:10.288062Z","shell.execute_reply":"2021-12-10T09:21:10.308113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function for converting the twitter data into numerical embeddings based on glovetwitter","metadata":{}},{"cell_type":"code","source":"def text_processing(seq,glove_loc,len_vec=200):\n    stpwds_set=set(stopwords.words('english'))\n    \n    embeddings_dict = {}\n    with open(glove_loc, 'r') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], \"float32\")\n            embeddings_dict[word] = vector\n    \n    seq=seq.apply(lambda x:custom_tokenize(x))\n    seq=seq.apply(lambda x:[i for i in x if i not in stpwds_set])\n    for i,j in enumerate(seq):\n        temp=[]\n        for l,k in enumerate(j):\n            \n            try:\n                temp.append(embeddings_dict[k])\n            except(Exception):\n                continue\n        seq[i]=temp\n    return seq","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:21:10.31087Z","iopub.execute_input":"2021-12-10T09:21:10.311308Z","iopub.status.idle":"2021-12-10T09:21:10.322051Z","shell.execute_reply.started":"2021-12-10T09:21:10.31126Z","shell.execute_reply":"2021-12-10T09:21:10.320927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Converting the twitter text data into numerical embeddings","metadata":{}},{"cell_type":"code","source":"train_data=text_processing(train.text,glove_loc)\ntrain_data=pd.DataFrame(train_data).join(train.target)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:21:10.323565Z","iopub.execute_input":"2021-12-10T09:21:10.324727Z","iopub.status.idle":"2021-12-10T09:22:57.614542Z","shell.execute_reply.started":"2021-12-10T09:21:10.324656Z","shell.execute_reply":"2021-12-10T09:22:57.613358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the data for any class imbalance","metadata":{}},{"cell_type":"code","source":"train_data.target.value_counts()/len(train_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:22:57.620002Z","iopub.execute_input":"2021-12-10T09:22:57.62024Z","iopub.status.idle":"2021-12-10T09:22:57.642409Z","shell.execute_reply.started":"2021-12-10T09:22:57.620211Z","shell.execute_reply":"2021-12-10T09:22:57.640416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading the Test data","metadata":{}},{"cell_type":"code","source":"test=pd.read_csv(path+'/test.csv')\ntest_data=pd.DataFrame(text_processing(test.text,glove_loc))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:22:57.644414Z","iopub.execute_input":"2021-12-10T09:22:57.644908Z","iopub.status.idle":"2021-12-10T09:24:36.521828Z","shell.execute_reply.started":"2021-12-10T09:22:57.644855Z","shell.execute_reply":"2021-12-10T09:24:36.520821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculating maximum sequence length among all the tweets","metadata":{}},{"cell_type":"code","source":"max_seq=train_data.text.apply(lambda x:len(x)).max()\nmax_seq","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-10T09:24:36.53205Z","iopub.execute_input":"2021-12-10T09:24:36.532322Z","iopub.status.idle":"2021-12-10T09:24:36.554247Z","shell.execute_reply.started":"2021-12-10T09:24:36.53229Z","shell.execute_reply":"2021-12-10T09:24:36.552776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_seq=train_data.text.apply(lambda x:len(x)).min()\nmin_seq","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:24:36.555509Z","iopub.execute_input":"2021-12-10T09:24:36.556284Z","iopub.status.idle":"2021-12-10T09:24:36.5706Z","shell.execute_reply.started":"2021-12-10T09:24:36.556237Z","shell.execute_reply":"2021-12-10T09:24:36.569457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importing the keras modules","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,BatchNormalization,Dropout,Conv1D, MaxPooling1D, Activation, Flatten,Bidirectional\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:21:02.807876Z","iopub.execute_input":"2021-12-10T09:21:02.808955Z","iopub.status.idle":"2021-12-10T09:21:08.557315Z","shell.execute_reply.started":"2021-12-10T09:21:02.80878Z","shell.execute_reply":"2021-12-10T09:21:08.556293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tweet_gen(data,batch_size,max_seq,min_seq,aug):\n    \n    while True:\n        data=np.random.permutation(data)\n        num_batches = data.shape[0]//batch_size\n        rem_d=data.shape[0]%batch_size\n        \n        for batch in range(num_batches): # we iterate over the number of batches\n            if aug==True:\n                maxlen=random.choice(range(min_seq,max_seq))\n                padding=random.choice(['pre','post'])\n                truncating=random.choice(['pre','post'])\n                temp=tf.keras.preprocessing.sequence.pad_sequences(data[batch*batch_size:(batch+1)*batch_size][:,0], maxlen=maxlen, dtype='float32', padding=padding,truncating=truncating, value=0.0)\n                yield tf.keras.preprocessing.sequence.pad_sequences(temp, maxlen=max_seq, dtype='float32', padding='pre',truncating='pre', value=0.0),data[batch*batch_size:(batch+1)*batch_size][:,1].astype('float32')\n            else:\n                yield tf.keras.preprocessing.sequence.pad_sequences(data[batch*batch_size:(batch+1)*batch_size][:,0], maxlen=max_seq, dtype='float32', padding='pre',truncating='pre', value=0.0),data[batch*batch_size:(batch+1)*batch_size][:,1].astype('float32')\n    \n        if rem_d!=0:\n            fm=num_batches*batch_size\n            to=fm+rem_d\n            if aug==True:\n                maxlen=random.choice(range(min_seq,max_seq))\n                padding=random.choice(['pre','post'])\n                truncating=random.choice(['pre','post'])\n                \n                temp=tf.keras.preprocessing.sequence.pad_sequences(data[fm:to][:,0], maxlen=maxlen, dtype='float32', padding=padding,truncating=truncating, value=0.0)\n                yield tf.keras.preprocessing.sequence.pad_sequences(temp, maxlen=max_seq, dtype='float32', padding='pre',truncating='pre', value=0.0),data[fm:to][:,1].astype('float32')\n            else:\n                yield tf.keras.preprocessing.sequence.pad_sequences(data[fm:to][:,0], maxlen=max_seq, dtype='float32', padding='pre',truncating='pre', value=0.0),data[fm:to][:,1].astype('float32')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:24:36.572592Z","iopub.execute_input":"2021-12-10T09:24:36.574348Z","iopub.status.idle":"2021-12-10T09:24:36.592317Z","shell.execute_reply.started":"2021-12-10T09:24:36.574303Z","shell.execute_reply":"2021-12-10T09:24:36.591334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=8\n\ntrain_gen=tweet_gen(data=train_data.iloc[:int(len(train_data)*0.8),:],batch_size=batch_size,max_seq=max_seq,min_seq=min_seq,aug=True)\nval_gen=tweet_gen(data=train_data.iloc[int(len(train_data)*0.8):,:],batch_size=batch_size,max_seq=max_seq,min_seq=min_seq,aug=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:24:36.593767Z","iopub.execute_input":"2021-12-10T09:24:36.594884Z","iopub.status.idle":"2021-12-10T09:24:36.608097Z","shell.execute_reply.started":"2021-12-10T09:24:36.594822Z","shell.execute_reply":"2021-12-10T09:24:36.607104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building  the ANN model based on 1 D Convolutional Neural Network and LSTM","metadata":{}},{"cell_type":"code","source":"def plot(history):\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n    axes[0].plot(history.history['loss'])   \n    axes[0].plot(history.history['val_loss'])\n    axes[0].legend(['loss','val_loss'])\n\n    axes[1].plot(history.history['accuracy'])   \n    axes[1].plot(history.history['val_accuracy'])\n    axes[1].legend(['accuracy','val_accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:24:36.610173Z","iopub.execute_input":"2021-12-10T09:24:36.610708Z","iopub.status.idle":"2021-12-10T09:24:36.621468Z","shell.execute_reply.started":"2021-12-10T09:24:36.610637Z","shell.execute_reply":"2021-12-10T09:24:36.620399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(keras.layers.InputLayer(input_shape=(max_seq,200)))\n\n\nmodel.add(Conv1D(64, 5, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=4))\n\nmodel.add(Conv1D(128, 5, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=4))\n\nmodel.add(tf.keras.layers.Bidirectional(LSTM(256)))\nmodel.add(Dropout(0.3))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:24:36.622868Z","iopub.execute_input":"2021-12-10T09:24:36.623556Z","iopub.status.idle":"2021-12-10T09:24:40.985992Z","shell.execute_reply.started":"2021-12-10T09:24:36.623371Z","shell.execute_reply":"2021-12-10T09:24:40.985006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Function for plotting the loss and accuracies","metadata":{}},{"cell_type":"code","source":"es = EarlyStopping(monitor='val_loss', mode='min',verbose=1, patience=20)\n#mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\nnum_epochs=100\nnum_train_sequences=int(len(train_data)*0.8)\nnum_val_sequences=len(train_data)-int(len(train_data)*0.8)\n\nif (num_train_sequences%batch_size) == 0:\n    steps_per_epoch = int(num_train_sequences/batch_size)\nelse:\n    steps_per_epoch = (num_train_sequences//batch_size) + 1\n\nif (num_val_sequences%batch_size) == 0:\n    validation_steps = int(num_val_sequences/batch_size)\nelse:\n    validation_steps = (num_val_sequences//batch_size) + 1\n\nhistory1=model.fit(train_gen, steps_per_epoch=steps_per_epoch,callbacks=es, epochs=num_epochs, verbose=1, validation_data=val_gen,validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\nplot(history1)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T09:24:40.987769Z","iopub.execute_input":"2021-12-10T09:24:40.98809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data=tf.keras.preprocessing.sequence.pad_sequences(test_data.text, maxlen=max_seq, dtype='float32', padding='pre',truncating='pre', value=0.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_var=model.predict(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.target=pre_var","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['target']=submission.target.apply(lambda x:1 if x>0.5 else 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}