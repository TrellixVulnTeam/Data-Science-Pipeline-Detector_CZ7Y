{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Modules"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option(\"max_colwidth\", -1)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\nimport nltk\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize\n\nimport tensorflow as tf\nimport keras\nfrom keras import preprocessing\n\nfrom sklearn.model_selection import KFold\n\nimport string\nimport time\nimport os ,re\nimport functools\nfrom collections import Counter\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load Dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_tweet = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_tweet = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n# I prefer indexing train and test with dictionary\n# rather than define train and tweet one by one\ndf_map = {\"train\": train_tweet, \"test\": test_tweet}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"### Get Dataset Information"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df_info(df_map):\n    for key, df in df_map.items():\n        print(\"{} dataframe info: \".format(key))\n        print(\"-\" * 30)\n        print(df.info(), end=\"\\n\\n\")\n        \n        \ndef check_null_values(df_map):\n    for key, df in df_map.items():\n        print(\"{} dataframe missing values: \".format(key))\n        print(\"-\" * 30)\n        print(df.isnull().sum(axis=0), end=\"\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_df_info(df_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"check_null_values(df_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There were no missing values on text columns, we're good to go"},{"metadata":{},"cell_type":"markdown","source":"### Class Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_class_count(train):\n    \n    fig, ax = plt.subplots(figsize=(5, 5))\n    plt.suptitle(\"Class count\")\n    sns.countplot(x=\"target\", data=train, ax=ax)\n        \n    \nplot_class_count(train_tweet)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Common Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_list(sentences):\n    result = [word.lower() for sentence in sentences \n                           for word in word_tokenize(sentence)]\n    return result\n\ndef create_corpus(train_tweet):\n    df = train_tweet.copy()\n        \n    # convert pandas series => list of words\n    class_0_corpus = word_list(df.query(\"target == 0\")[\"text\"]) \n    class_1_corpus = word_list(df.query(\"target == 1\")[\"text\"])\n\n    return class_0_corpus, class_1_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define corpus, punc_list and stopwords\nclass_0_corpus, class_1_corpus = create_corpus(train_tweet)\npunc_list = [punct for punct in string.punctuation]\nstop = list(stop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to plot the common words, I will use **decorator**. **Decorator** is used to expand our defined function without explicitly modify it. Just bear with me, it may looks complicated but the concept is simple. \n\nNote: (Skip to the plotting section if you already understand Decorator)"},{"metadata":{},"cell_type":"markdown","source":"### Decorator Mini Guide\nLet's say you want to measure how long your function running time. You will do the following..."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sum_all():\n    start = time.time()\n    \n    result = 0\n    for num in range(100):\n        result += num\n    \n    end = time.time()\n    print(\"Elapsed time: {:4f} sec\".format(end - start))\n    print(\"The sum is {}\".format(result))\n\n\nsum_all()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quite simple right? What if, you want to measure the execution time on another function?"},{"metadata":{"trusted":true},"cell_type":"code","source":"def mul_all():\n    start = time.time()\n    \n    result = 0\n    for num in range(100):\n        result *= num\n    \n    end = time.time()\n    print(\"Elapsed time: {:4f} sec\".format(end - start))\n    print(\"The product is {}\".format(result))\n    \nmul_all()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You must copy-paste those start and end variable into the new function. Imagine if you do that on another 10 functions. Time waster isn't it? Now this is the time for **Decorator** to shine. This is how you create a simple decorator."},{"metadata":{},"cell_type":"markdown","source":"Step 1. Build Wrapper Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the wrapper function first, \n# :parameter: func here is our base function like: sum_all and mul_all\ndef simple_decorator(func):\n    \n    # 2. Our function will goes inside here\n    def decorated():\n        start = time.time()  # 3. Timer start\n        result = func()      # 4. Function Executed\n        end = time.time()    # 5. Timer end\n        \n        print(\"{} function\".format(func.__name__))\n        print(\"Elapsed time: {:4f} sec\".format(end - start))\n        print(\"The result is {}\".format(result))\n        print()\n        \n        return result # 6. (Optional) Return the function result here\n    \n    # 1. This function will be called first, then\n    return decorated\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 2. Add @wrapper_function on top of Base Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"@simple_decorator\ndef sum_all():\n    result = 0\n    for x in range(100):\n        result += x\n    return result\n\n@simple_decorator\ndef mul_all():\n    result = 0\n    for x in range(100):\n        result *= x\n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 3. Execute"},{"metadata":{"trusted":true},"cell_type":"code","source":"mul_all()\nsum_all()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### End of Decorator Guide"},{"metadata":{},"cell_type":"markdown","source":"Let's do it for plotting common words"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_common_words(func):\n    \n    def decorated(corpus, name=\"dataset\"):\n        # Get word_list from decorated function\n        word_list = func(corpus)\n        \n        # Count words inside the list\n        word_counts = Counter(word_list)\n    \n        # Get top 10 most frequent word\n        top_10_words = word_counts.most_common(10)\n        \n        # Plot the result\n        plt.figure(figsize=(8, 5))\n        plt.suptitle(\"{} in {}\".format(func.__name__, name))\n        \n        x, y = zip(*top_10_words)\n        \n        labels = list(x)\n        bplot = sns.barplot(x=labels, y=y)\n        bplot.set_xticklabels(labels=labels, rotation=30)\n        \n        # Return nothing here, because we just want the plot\n        return\n    \n    return decorated\n\n\n@plot_common_words\ndef common_words(corpus):\n    word_list = [word for word in corpus \n                      if word not in stop and word.isalpha()]\n    return word_list\n\n@plot_common_words\ndef common_puncts(corpus):\n    punc_regex = r\"[{}]+\".format(string.punctuation)\n    word_list = [word for word in corpus if re.fullmatch(punc_regex, word)]\n    return word_list\n\n@plot_common_words\ndef common_nonalpha(corpus):\n    punc_regex = r\"[{}]+\".format(string.punctuation)\n    word_list = [word for word in corpus \n                      if not re.fullmatch(punc_regex, word) and \n                         not word.isalpha() and\n                         not word.isdigit()]\n    return word_list\n\n@plot_common_words\ndef common_stops(corpus):\n    word_list = [word for word in corpus if word in stop]\n    return word_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_words(class_0_corpus, name=\"Class 0 Train\")\ncommon_words(class_1_corpus, name=\"Class 1 Train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_puncts(class_0_corpus, name=\"Class 0 Train\")\ncommon_puncts(class_1_corpus, name=\"Class 1 Train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_nonalpha(class_0_corpus, name=\"Class 0 Train\")\ncommon_nonalpha(class_1_corpus, name=\"Class 1 Train\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_stops(class_0_corpus, name=\"Class 0 Train\")\ncommon_stops(class_1_corpus, name=\"Class 1 Train\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Noise Removal"},{"metadata":{},"cell_type":"markdown","source":"### Common Noises\nRemove common noises found on the above EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"noise_patterns = { \n        \"url\"            : r\"https?(://\\S+|\\S+)|www\\.\\S+\",\n        \"html_tag\"       : r\"<.*?>\",          \n        \"non_ascii\"      : r\"[^\\x00-\\x7f]+\",\n        \"RT word\"        : r\"\\b[Rr][Tt]\\b\",  # RT commonly appear in retweeted tweet\n        \"amp word\"       : r\"\\bamp\\b\",       # what is \"amp\" ? why this is so common in tweets\n}\n\n\ndef noise_check(df_map, patterns):\n    \n    for key, df in df_map.items():\n        print(\"{} DataFrame noises: \".format(key))\n        print(\"-\" * 30)\n        for indicator, pattern in patterns.items():\n            count = df[\"text\"].str.contains(pattern).sum()\n            print(\"There were {:5} rows with {}\".format(count, indicator))\n        print()\n    \n\nnoise_check(df_map, noise_patterns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Insert pattern into parentheses\ndef add_paren(text):\n    return \"({})\".format(text)\n\n# Every noise pattern will be inserted into parentheses, then\n# join all noise patterns with '|' \nnoises_all = \"|\".join(add_paren(pattern) for pattern in noise_patterns.values())\n\n# Result is in format: (pattern1)|(pattern2)|(pattern3)|(pattern4)\nprint(noises_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clear all noises simultaneously\ndef clean_noise(text):\n    return re.sub(noises_all, r\" \", text)\n\ntrain_tweet[\"text\"] = train_tweet[\"text\"].apply(clean_noise)\ntest_tweet[\"text\"] = test_tweet[\"text\"].apply(clean_noise)\n\nnoise_check(df_map, noise_patterns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Unique Pattern Investigation\nTo handle these unique patterns, we need to identify what the patterns are. This still in test, and we can skip it."},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_sampling(df_map, n=5):\n    for key, df in df_map.items():\n        print(\"{} sample from {} dataset\".format(n, key))\n        print(\"-\" * 30)\n        print(df[\"text\"].sample(n))\n        print()\n        \nrandom_sampling(df_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Normalization"},{"metadata":{},"cell_type":"markdown","source":"### Lowercase all word\nBy using lower(), we have normalized the text to lowercase so that the distinction between The and the is ignored.\n\nreference: https://www.nltk.org/book/ch03.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"word1 = \"The\"\nword2 = \"the\"\n\nprint(word1.lower() == word2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Unravel Apostrophe Words\n\nreference: https://stackoverflow.com/questions/43018030/replace-apostrophe-short-words-in-python"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n# testing\nprint( decontracted(\"I'm Lord of Darkness\") ) \nprint( decontracted(\"Who've made this burger?\") )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove Punctuations\nI didn't place this section in noise removal because I need to unravel the apostrophe words before begin removing all punctuations. In this function I replace **\",@#\"** with empty space and replace the rest with whitespace"},{"metadata":{"trusted":true},"cell_type":"code","source":"def punct2espace(text): # comma to empty space\n    return re.sub(\",\", \"\", text)\n\ndef punct2wspace(text): # punc to white space\n    return re.sub(r\"[{}]+\".format(string.punctuation), \" \", text)\n\ndef residual_punc(text): # remove remaining bacward slash\n    table = str.maketrans(\"\", \"\", string.punctuation)\n    return text.translate(table)\n\ndef normalize_wspace(text): # normalize multiple whitespace\n    return re.sub(r\"\\s+\", \" \", text)\n\ndef replace_punctuations(text):\n    text = punct2espace(text)\n    text = punct2wspace(text)\n    text = residual_punc(text)\n    text = normalize_wspace(text)\n    \n    return text.strip()\n\n\nreplace_punctuations(r\"@@rakka@@ alhazimi@@hai typhoon--devastation \\\\\\\\\\conclusively\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correct Mispelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspellchecker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spellchecker import SpellChecker\n\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"speling correctin\"\ncorrect_spellings(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatization\nText lemmatization is the process of eliminating redundant prefix or suffix of a word and extract the base word (lemma).\n\nreference:\n\nhttps://medium.com/text-classification-algorithms/text-classification-algorithms-a-survey-a215b7ab7e2d\nhttps://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing#Removal-of-Frequent-words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\nlemmatize_words(\"rakka alhazimi are cool\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Wrap all function into one."},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_text(text):\n    text = text.lower()                                         # lowercase\n    text = decontracted(text)                                   # unravel apostrophe words\n    text = replace_punctuations(text)                           # remove punctuations\n    text = lemmatize_words(text)                                # lemmatize word \n    \n    return text\n\nnormalize_text(\"Hello andrew, I'm from kuvukiland nuce to meet you\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pandarallel # Use pandarallel for fast apply","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandarallel import pandarallel\n\npandarallel.initialize()\n\nstart = time.time()\ntrain_tweet[\"text\"] = train_tweet[\"text\"].parallel_apply(normalize_text)\ntest_tweet[\"text\"] = test_tweet[\"text\"].parallel_apply(normalize_text)\n\nelapsed = time.time() - start\nprint(\"Elapsed time: {:.4f} min\".format(elapsed / 60))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Save to CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweet.to_csv(\"train_tweet.csv\")\ntest_tweet.to_csv(\"test_tweet.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tweet[[\"text\", \"target\"]].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# How many Unknown Words?"},{"metadata":{"trusted":true},"cell_type":"code","source":"english_vocab = nltk.corpus.words.words(\"en\")\nenglish_vocab = set(english_vocab)\n\ncorpus_0, corpus_1 = create_corpus(train_tweet)\ncorpus = corpus_0 + corpus_1\n\ntweet_words = set(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unknown_words = list(tweet_words.difference(english_vocab))\nprint(\"There were {} unknown words. \".format(len(unknown_words)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Representation"},{"metadata":{},"cell_type":"markdown","source":"### Convert text into array of integer"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 50  # max sentence length\n\ndef tokenize(df_map):\n    sentence_list = []\n    for key, df in df_map.items():\n        \n        # From sentence -> filter(word) -> sentence\n        # What we filter is : digits\n        sentence = [\" \".join(word for word in sen.split() if not word.isdigit()) # <- here's the digits\n                                  for sen in df[\"text\"]]\n        \n        length = df.shape[0] # Store test tweet length for splitting                    \n        \n        sentence_list += sentence\n        \n    # Index all train and test tweets words\n    tokenizer = preprocessing.text.Tokenizer()\n    tokenizer.fit_on_texts(sentence_list)\n    \n    # Convert text into sequences of integer\n    tensor = tokenizer.texts_to_sequences(sentence_list)\n    tensor = preprocessing.sequence.pad_sequences(tensor, padding=\"post\", maxlen=MAX_LEN)\n    \n    # Split train and test tweets\n    input_tensor_train, input_tensor_test = tensor[:-length], tensor[-length:]\n    \n    return input_tensor_train, input_tensor_test, tokenizer\n\n\n# Input tensor train/test and tokenizer\nX_train, X_test, tokenizer = tokenize(df_map)\n\n# Target tensor train\ny_train = train_tweet[\"target\"].values\ny_train = y_train.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Word Embedding"},{"metadata":{},"cell_type":"markdown","source":"### Fasttext"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fasttext\nwith open(\"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\", \"r\") as vector:\n    fasttext = vector.readlines()\n    \nlen(fasttext)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GloVe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GloVe\nwith open(\"../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\", \"r\") as vector:\n    glove = vector.readlines()\n    \nlen(glove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transform Embedding into Dict"},{"metadata":{"trusted":true},"cell_type":"code","source":"def embed2dict(tokenizer, embedding):\n    word_index = tokenizer.word_index\n    embedding_dict = {line.split()[0]: line.split()[1:] for line in embedding[1:]\n                                                        if word_index.get(line.split()[0])}\n    return embedding_dict\n    \nfasttext_dict = embed2dict(tokenizer, fasttext)\nglove_dict = embed2dict(tokenizer, glove)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Similar Words between Two Words Embedding\nWords vector in both fasttext and glove is sure not similar. We need to find the intersect using python set() data structure."},{"metadata":{"trusted":true},"cell_type":"code","source":"fasttext_word = set(fasttext_dict.keys())\nglove_word = set(glove_dict.keys())\n\nintersection_word = fasttext_word.intersection(glove_word)\n\nprint(\"Similar embedding words:\", len(intersection_word))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's update the dictionaies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# update glove_dict\ndef update_embed_dict(embed_dict):\n    result = {word: vector for word, vector in embed_dict.items()\n                           if word in intersection_word}\n    return result\n\nfasttext_dict = update_embed_dict(fasttext_dict)\nglove_dict = update_embed_dict(glove_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Change Embedding Dict into Vector"},{"metadata":{"trusted":true},"cell_type":"code","source":"fasttext_dim = 300\nglove_dim = 200\n\nnum_words = len(tokenizer.word_index) + 1 # word_index starts at 1\n\ndef dict2vector(tokenizer, embed_dict, embed_dim):\n    \n    vector = np.zeros(shape=(num_words, embed_dim))\n    word_index = tokenizer.word_index\n    \n    for word, index in word_index.items():\n        if index > num_words:\n            continue\n        \n        if embed_dict.get(word):\n            vector[index] = embed_dict.get(word)\n    \n    return vector\n    \nfasttext_vector = dict2vector(tokenizer, fasttext_dict, fasttext_dim)\nglove_vector = dict2vector(tokenizer, glove_dict, glove_dim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concatenate Two Embedding Vectors into One"},{"metadata":{"trusted":true},"cell_type":"code","source":"def concat_vector(vectors):\n    # result shape (num_words, fasttext_dim + glove_dim)\n    result = np.hstack(vectors)\n    return result\n\nembedding_vector = concat_vector([fasttext_vector, glove_vector])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_vector.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nEPOCHS = 8\n\nembedding_dim = embedding_vector.shape[1]                                               ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build NN Model"},{"metadata":{},"cell_type":"markdown","source":"The model architecture is Bidirectional LSTM with MaxPooling2D.\n1. Embeddings\n2. BLSTM\n3. Reshape into Image dim like\n4. Conv2D\n5. GlobalMaxPooling2D\n6. Dropout\n7. Dense\n\nreference: https://arxiv.org/abs/1611.06639"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(emb_init=None, emb_train=True):\n    \n    embed_params = {\"input_dim\": num_words, \n                    \"output_dim\": embedding_dim,\n                    \"embeddings_initializer\": keras.initializers.Constant(\n                        embedding_vector if emb_init is None else emb_init),\n                    \"trainable\": emb_train,\n                    \"mask_zero\": True, # ignore \"zero\" paddings\n                   }\n    \n    model = keras.Sequential([\n                keras.layers.Embedding(**embed_params),\n\n                keras.layers.Bidirectional(keras.layers.LSTM(128, \n                                                             dropout=0.3,\n                                                             recurrent_dropout=0.3,\n                                                             return_sequences=True\n                                                            )),\n                # Reshape size (x, y, hidden) where x * y = MAX_LEN\n                keras.layers.Reshape((5, 10, 256)),\n                keras.layers.Conv2D(32, 2),\n                keras.layers.GlobalMaxPooling2D(),\n                keras.layers.Dropout(0.3),\n                keras.layers.Dense(1, activation=\"sigmoid\")])\n\n    optimizer = keras.optimizers.Adam(1e-4, clipvalue=0.5)\n\n    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"acc\"])\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train OOV Words Embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model = build_model(emb_train=True)\ntest_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_model.fit(X_train, y_train,\n              epochs=10,\n              batch_size=BATCH_SIZE,\n              validation_split=0.2,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_embedding = test_model.get_weights()[0]\noov_embedding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Insert OOV vector into Original Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_embeddings(oov_embed, embed_vector):\n    new_vector = embed_vector.copy()\n    \n    for index, row in enumerate(embed_vector):\n        if row.sum() == 0:\n            new_vector[index] = oov_embed[index]\n    \n    return new_vector\n\nnew_embedding = merge_embeddings(oov_embedding, embedding_vector)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Kfold 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_splits = 5\nsplits = list(KFold(n_splits=n_splits, shuffle=True, random_state=0).split(X_train,y_train))\nmodel_record = {}\npredictions = np.zeros((X_test.shape[0], 1))\n\nfor n, fold in enumerate(splits):\n    \n    # Current Fold Status\n    print()\n    print(\"Fold {}\".format(n + 1))\n    \n    # Define Model Name\n    model_fold = \"lstm_fold0{}.h5\".format(n + 1)\n    \n    # Callback List\n    model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=model_fold,\n                                                       monitor=\"val_acc\",\n                                                       save_best_only=True,)\n    callbacks_list = [model_checkpoint]\n    \n    # Split data into train and val using fold index\n    X_train_fold, y_train_fold = X_train[fold[0]], y_train[fold[0]]\n    X_val_fold, y_val_fold = X_train[fold[1]], y_train[fold[1]]\n    \n    # Build model\n    model = build_model(emb_init=new_embedding, emb_train=False) # Embedding is not trainable\n    \n    history = model.fit(X_train_fold, y_train_fold,\n                          batch_size=BATCH_SIZE,\n                          epochs=EPOCHS,\n                          validation_data=(X_val_fold, y_val_fold),\n                          callbacks=callbacks_list,)\n\n    # Record model with best val_accuracy    \n    model_record[model_fold] = max(history.history[\"val_acc\"])\n    \n    # Use the best model\n    model = keras.models.load_model(model_fold)\n    \n    predictions += model.predict(X_test)\n\npredictions /= n_splits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_record","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_acc = max(model_record.keys(), key=model_record.get)\ntop_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class_prediction = np.where(predictions > 0.5, 1, 0)\nbest_model = keras.models.load_model(top_acc)\nclass_prediction = best_model.predict_classes(X_test)\nsubmission = pd.DataFrame({\"id\": test_tweet[\"id\"], \n                           \"target\": class_prediction.flatten()})\n\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}