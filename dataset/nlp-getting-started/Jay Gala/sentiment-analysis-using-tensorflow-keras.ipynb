{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP with Disaster Tweets using TensorFlow, Keras\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Quick Look at Data\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_path = '/kaggle/input/nlp-getting-started/'\n\ntrain = pd.read_csv(os.path.join(input_path, 'train.csv'))\ntest = pd.read_csv(os.path.join(input_path, 'test.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train: ', train.shape)\nprint('Test: ', test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the missing values for keyword and location\nlen(train['keyword'].isnull()), len(train['location'].isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# non disaster tweet\ntrain[train['target'] == 0]['text'].values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# disaster tweet\ntrain[train['target'] == 1]['text'].values[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build a Model\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport unicodedata\nimport spacy\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reference:~ https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/contractions.py\n\nCONTRACTION_MAP = {\n    \"ain't\": \"is not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"I'd\": \"I would\",\n    \"I'd've\": \"I would have\",\n    \"I'll\": \"I will\",\n    \"I'll've\": \"I will have\",\n    \"I'm\": \"I am\",\n    \"I've\": \"I have\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so as\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Load the spacy en_core library and add a sentencizer to pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading the spacy's en_core_web_sm\nnlp = spacy.load('en_core_web_sm')\nnlp.pipe_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create and add sentencizer to the pipeline\nsent = nlp.create_pipe('sentencizer')\nnlp.add_pipe(sent, before='parser')\nnlp.pipe_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_cleaning(text):\n    \"\"\"\n    Returns cleaned text (Accented Characters, Expand Contractions, Special Characters)\n    Parameters\n    ----------\n    text -> String\n    \"\"\"\n    # remove accented characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n    # remove emails\n    text = ' '.join([i for i in text.split() if '@' not in i])\n    \n    # remove urls\n    text = re.sub('http[s]?://\\S+', '', text)\n    \n    # expand contractions\n    for word in text.split():\n        if word.lower() in CONTRACTION_MAP:\n            text = text.replace(word[1:], CONTRACTION_MAP[word.lower()][1:])\n    \n    # remove special characters\n    pattern = r'[^a-zA-Z0-9\\s]'\n    text = re.sub(pattern, '', text)\n    \n    # remove extra white spaces\n    text = re.sub('\\s+', ' ', text)\n    \n    doc = nlp(text)\n    tokens = []\n    \n    for token in doc:\n        if token.lemma_ != '-PRON-':\n            tokens.append(token.lemma_.lower().strip())\n        else:\n            tokens.append(token.lower_)\n\n    return ' '.join(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_cleaning(\"I don't like this movie. The plot is   terrible :(\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data into inputs and outputs\nX_train = train['text'].apply(text_cleaning)\ny_train = train['target']\nX_test = test['text'].apply(text_cleaning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the train set into train and valid set\nsplit = 0.8\ntrain_size = int(len(X_train) * 0.8)\nidx = np.random.permutation(X_train.index)\ntrain_idx = idx[:train_size]\nvalid_idx = idx[train_size:]\ntrain_data = X_train.iloc[train_idx]\ntrain_labels = y_train.iloc[train_idx]\nvalid_data = X_train.iloc[valid_idx]\nvalid_labels = y_train.iloc[valid_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oov_token = '<unk>'\npadding_type = 'post'\ntrunc_type = 'post'\nembedding_dim = 100\nmax_len = max([len(x) for x in train_data])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data)\nword_index = tokenizer.word_index\nvocab_size = len(word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Vocab size : ', vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seq = tokenizer.texts_to_sequences(train_data)\ntrain_pad = pad_sequences(train_seq, padding=padding_type, truncating=trunc_type, maxlen=max_len)\n\nvalid_seq = tokenizer.texts_to_sequences(valid_data)\nvalid_pad = pad_sequences(valid_seq, padding=padding_type, truncating=trunc_type, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_seq = tokenizer.texts_to_sequences(X_test)\ntest_pad = pad_sequences(test_seq, padding=padding_type, truncating=trunc_type, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_idx = {}\nglove_path = '/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt'\nwith open(glove_path, 'r') as f:\n    for line in f:\n        data = line.split()\n        word = data[0]\n        values = np.asarray(data[1:], dtype=np.float32)\n        embeddings_idx[word] = values\n\nprint('Building Embedding Matrix...')\nembeddings_matrix = np.zeros((vocab_size + 1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vec = embeddings_idx.get(word)\n    if embedding_vec is not None:\n        embeddings_matrix[i] = embedding_vec\nprint('Embedding Matrix Generating...')\nprint('Embedding Matrix Shape -> ', embeddings_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build a model\nmodel =  keras.models.Sequential([\n    Embedding(vocab_size + 1, embedding_dim, input_length=max_len, weights=[embeddings_matrix], trainable=False),\n    Dropout(0.2),\n    Bidirectional(LSTM(64, return_sequences=True)),\n    Bidirectional(LSTM(32)),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = Adam(lr=3e-4)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_pad, train_labels, epochs=20, validation_data=(valid_pad, valid_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model analysis\nimport matplotlib.pyplot as plt\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_' + string])\n    plt.xlabel('# epochs')\n    plt.ylabel(string)\n    plt.legend([string, 'val_' + string])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_graphs(history, 'accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_graphs(history, 'loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the sample submission csv file\nsample_submission = pd.read_csv(os.path.join(input_path, 'sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict on test data\nsample_submission['target'] = model.predict_classes(test_pad)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the sample submission csv file\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}