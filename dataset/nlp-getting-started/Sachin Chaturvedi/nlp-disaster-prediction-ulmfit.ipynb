{"cells":[{"metadata":{},"cell_type":"markdown","source":"Importing the libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom fastai.text import *\nfrom fastai.callbacks import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path('../input/nlp-getting-started')\npath.ls()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(path/'train.csv')\ntest = pd.read_csv(path/'test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use both our training and test text for our language model. Pay attention to the fact that we are not taking any labels here. We are not building a classifier now rather we are buliding a language model where we are using a pretrained architecture trained on the Wikitext-103 and then training it on our tweet data.\n\nThe paragraph given below is taken from [Understanding building blocks of ULMFIT](https://medium.com/mlreview/understanding-building-blocks-of-ulmfit-818d3775325b#:~:text=High%20level%20idea%20of%20ULMFIT,learning%20rates%20in%20multiple%20stages)\n> High level idea of ULMFIT is to train a language model using a very large corpus like Wikitext-103 (103M tokens), then to take this pretrained model’s encoder and combine it with a custom head model, e.g. for classification, and to do the good old fine tuning using discriminative learning rates in multiple stages carefully.\nArchitecture that ULMFIT uses for it’s language modeling task is an [AWD-LSTM](https://arxiv.org/pdf/1708.02182.pdf). The name is an abbreviation of ASGD Weight-Dropped LSTM.\n\nRefer to this paper of you want to read more about ULMFiT : https://arxiv.org/abs/1801.06146\n\nULMFiT brought the concept of transfer learning in Computer Vision to NLP","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lm = (TextList.from_df(pd.concat([train[['text']], test[['text']]], ignore_index=True, axis=0))\n           .split_by_rand_pct(0.15)\n           .label_for_lm()\n           .databunch(bs=128))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lm.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create lm learner with pre-trained model\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callback = SaveModelCallback(learn,monitor=\"accuracy\", mode=\"max\", name=\"best_lang_model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7), callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('fine_tuned')\nlearn.save_encoder('fine_tuned_enc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save the language model and the encoder too","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train[['text', 'target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = test[['text']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a Text Databunch for our classifier. We are taking 10% as validation set and keeping the vocabulary same as the language model databunch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clas = (TextList.from_df(df, vocab=data_lm.vocab)\n             #.split_none()\n             .split_by_rand_pct(0.1)\n             .label_from_df('target')\n             .add_test(TextList.from_df(df_test, vocab=data_lm.vocab))\n             .databunch(bs=128))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check test set looks ok\ndata_clas.show_batch(ds_type=DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building the Classifier with the same encoder from the language model. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, FBeta(beta=1)])\nlearn.load_encoder('fine_tuned_enc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot(suggestion=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(1, 1e-3, moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## unfreeze the last 2 layers and train for 1 cycle\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-2), moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## unfreeze the last 3 layers and train for 1 cycle\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using callbacks to select the best classification model out of the given epochs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = SaveModelCallback(learn,monitor=\"accuracy\", mode=\"max\", name=\"best_classification_model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## unfreeze all and train for 2 cycles\nlearn.unfreeze()\nlearn.fit_one_cycle(15, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7), callbacks=[callbacks])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, _ = learn.get_preds(ds_type=DatasetType.Test,  ordered=True)\npreds = preds.argmax(dim=-1)\n\nid = test['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'id': id, 'target': preds})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}