{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsubmission=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)\nprint(submission.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(train.head)\ndisplay(test.head)\ndisplay(submission.head)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning Part","metadata":{}},{"cell_type":"code","source":"## remove #tags,@words and links from the text field\nimport re,string\n\ndef strip_links(text):\n    link_text = re.sub('http://\\S+|https://\\S+', '', text)\n    return link_text\n    \n\ndef strip_all_entities(text):\n    entity_prefixes = ['@','#']\n    for separator in  string.punctuation:\n        if separator not in entity_prefixes :\n            text = text.replace(separator,' ')\n    words = []\n    for word in text.split():\n        word = word.strip()\n        if word:\n            if word[0] not in entity_prefixes:\n                words.append(word)\n    return ' '.join(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove hyperlinks","metadata":{}},{"cell_type":"code","source":"train['text_1']=train['text'].apply(lambda x:strip_links(x))\ntest['text_1']=test['text'].apply(lambda x:strip_links(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove hashtags and mentions if any","metadata":{}},{"cell_type":"code","source":"train['text_2']=train['text_1'].apply(lambda x:strip_all_entities(x))\ntest['text_2']=test['text_1'].apply(lambda x:strip_all_entities(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test['text'][3260])\nprint(test['text_1'][3260])\nprint(test['text_2'][3260])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove special characters","metadata":{}},{"cell_type":"code","source":"train['text_2'] = train['text_2'].apply(lambda x: x.lower())\ntrain['text_3'] = train['text_2'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['text_2'] = test['text_2'].apply(lambda x: x.lower())\ntest['text_3'] = test['text_2'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test['text'][3260])\nprint(test['text_3'][3260])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check the number of 0's and 1's in tweets","metadata":{}},{"cell_type":"code","source":"print(train[ train['target'] == 1].size)\nprint(train[ train['target'] == 0].size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check the length of tokens","metadata":{}},{"cell_type":"code","source":"## For train\nl=[]\nfor i in range(len(train)):\n    l.append(len([w for w in train.loc[i,'text_3'].split(' ')]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## For test\nl1=[]\nfor i in range(len(test)):\n    l1.append(len([w for w in test.loc[i,'text_3'].split(' ')]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Add a column of length of tokens\ntrain['token_cnt']=l\ntest['token_cnt']=l1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['token_cnt'].max()## Maximumm tokens in train set =34\ntest['token_cnt'].max()## Maximumm tokens in train set =32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop unnecessary columns","metadata":{}},{"cell_type":"code","source":"train.drop(['keyword','location','text','text_1','text_2','token_cnt'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop(['keyword','location','text','text_1','text_2','token_cnt'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split the data of train into trainn and validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(\n    train['text_3'],\n    train['target'],\n    test_size=0.2, \n    random_state=123\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the Tokenizer","metadata":{}},{"cell_type":"code","source":"from tensorflow.python.keras.preprocessing.text import Tokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Declare the vocabulary size for word embedding\ntop_words = 1000\nt = Tokenizer(num_words=top_words) # num_words -> Vocablury size\nt.fit_on_texts(X_train.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generate the word index for train and test","metadata":{}},{"cell_type":"code","source":"X_train = t.texts_to_sequences(X_train.tolist())\nX_val = t.texts_to_sequences(X_val.tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pad the sequence of tokens","metadata":{}},{"cell_type":"code","source":"from tensorflow.python.keras.preprocessing import sequence\nmax_review_length = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = sequence.pad_sequences(X_train,maxlen=max_review_length,padding='post')\nX_val = sequence.pad_sequences(X_val, maxlen=max_review_length, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load pretrained embedding model from gensim","metadata":{}},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check if embeddings have been downloaded\n!ls -l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#unzip the file, we get multiple embedding files. We can use either one of them\n!unzip glove.6B.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.scripts.glove2word2vec import glove2word2vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Glove file - we are using model with 50 embedding size\nglove_input_file = 'glove.6B.50d.txt'\n\n#Name for word2vec file\nword2vec_output_file = 'glove.6B.50d.txt.word2vec'\n\n#Convert Glove embeddings to Word2Vec embeddings\nglove2word2vec(glove_input_file, word2vec_output_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get embedding from loaded pretrained model ","metadata":{}},{"cell_type":"code","source":"### We will extract word embedding for which we are interested in; the pre trained has 400k words each with 50 embedding vector size.\nfrom gensim.models import Word2Vec, KeyedVectors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pretrained Glove model (in word2vec form)\nglove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Embedding length based on selected model - we are using 50d here.\nembedding_vector_length = 50","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initialize embedding matrix\nembedding_matrix = np.zeros((top_words + 1, embedding_vector_length))\nprint(embedding_matrix.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for word, i in sorted(t.word_index.items(),key=lambda x:x[1]):\n    if i > (top_words+1):\n        break\n    try:\n        embedding_vector = glove_model[word] #Reading word's embedding from Glove model for a given word\n        embedding_matrix[i] = embedding_vector\n    except:\n        pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix[3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Intitate the model","metadata":{}},{"cell_type":"code","source":"#Initialize model\nimport tensorflow as tf\ntf.keras.backend.clear_session()\nmodel = tf.keras.Sequential()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.add(tf.keras.layers.Embedding( top_words+ 1, #Vocablury size\n                                    embedding_vector_length, #Embedding size\n                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n                                    input_length=max_review_length) #Number of words in each review\n         )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add LSTM layers with 256 cell and hidden state size","metadata":{}},{"cell_type":"code","source":"model.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.LSTM(256)) #RNN State - size of cell state and hidden state\nmodel.add(tf.keras.layers.Dropout(0.2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n#Compile the model\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the Model","metadata":{}},{"cell_type":"code","source":"model.fit(X_train,y_train,\n          epochs=10,\n          batch_size=32,          \n          validation_data=(X_val, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Store the output of lstm","metadata":{}},{"cell_type":"code","source":"x = model.get_layer('lstm').output\nmodel2 = tf.keras.Model(model.input, x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the model on the whole train set","metadata":{}},{"cell_type":"code","source":"### Declare the vocabulary size for word embedding\ntop_words = 1000\nt = Tokenizer(num_words=top_words) # num_words -> Vocablury size\nt.fit_on_texts(train['text_3'].tolist())\ntrain_seq  = t.texts_to_sequences(train['text_3'].tolist())\ntest_seq  = t.texts_to_sequences(test['text_3'].tolist())\n\ntrain_seq = sequence.pad_sequences(train_seq,maxlen=max_review_length,padding='post')\ntest_seq = sequence.pad_sequences(test_seq, maxlen=max_review_length, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_seq","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(train_seq,train['target'],\n          epochs=10,\n          batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## train for 10 more rounds\nmodel.fit(train_seq,train['target'],\n          epochs=20,\n          initial_epoch=10,\n          batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred=model.predict(test_seq)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred=test_pred.reshape((3263,))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Append to test data","metadata":{}},{"cell_type":"code","source":"test['prediction']=test_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['target']=np.where(test['prediction']>0.5,1,0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['target'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop(['text_3','prediction'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.to_csv('sample_submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}