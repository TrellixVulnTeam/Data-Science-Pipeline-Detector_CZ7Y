{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#install\n\n!python -m spacy download en_core_web_md\n!pip install wordcloud\n!pip install pyspellchecker\n!pip install contractions\n!pip install -U textblob\n!python -m textblob.download_corpora\n!pip install empath\n############################################\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport pandas_profiling as pdp\nimport gc\ngc.enable()\nimport spacy\nimport contractions\nimport en_core_web_md\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom spellchecker import SpellChecker\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nimport unicodedata\nimport re\nimport scattertext as st\nfrom textblob import TextBlob\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import IFrame\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:98% !important; }</style>\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop([\"id\",\"keyword\",\"location\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"reporte = pdp.ProfileReport(data, title=\"Pandas Profiling Reporte\",minimal=True)\nreporte","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['contara'] = data[\"text\"].apply(lambda x: len(str(x).split()))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nsns.barplot(data = data, x = \"target\", y = \"contara\")\nplt.title('Number of words in each target', fontsize= 25)\nplt.xlabel('predator')\nplt.ylabel('contara')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data=data,x='target',orient=\"h\")\nplt.title(\"Disaster and fake\")\nplt.xlabel(\"target\")\nplt.ylabel(\"count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\ndata.target.value_counts().plot(kind=\"pie\", autopct='%1.0f%%')\nplt.ylabel(\"target\")\n\nplt.title(\"fake or not\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## NLP and stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = en_core_web_md.load()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords_spacy = list(STOP_WORDS)\nprint(stopwords_spacy)\nlen(stopwords_spacy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"en_stop_words=STOP_WORDS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spellcheck Activation."},{"metadata":{"trusted":true},"cell_type":"code","source":"spell = SpellChecker(language='en',distance=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spell_check(x):\n    correct_word = []\n    mispelled_word = x\n    for word in mispelled_word:\n        correct_word.append(spell.correction(word))\n    return ' '.join(correct_word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['text']=data['text'].apply(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npattern = r\"[$&+,:;=_?@#|\\[\\]{}'<>.^*()%!-]\"\ntw_lemm=[]\nlm = WordNetLemmatizer()\nfor i in range(data.shape[0]):\n    tweet = data.iloc[i].text\n    tweet = ' '.join(re.sub(pattern, '', tweet).strip().split())\n    tweet = ' '.join([word for word in tweet.split() if word.isalpha()])\n    tweet = tweet.lower()\n    tweet = contractions.fix(tweet)\n    tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    tweet = ' '.join(word for word in tweet.split() if word not in en_stop_words)\n    tweet = spell.correction(tweet)\n    tweet = ' '.join([lm.lemmatize(word) for word in tweet.split()])\n    tw_lemm.append(tweet)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"tweet_lemms\"]=pd.Series(tw_lemm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\npattern = r\"[$&+,:;=_?@#|\\[\\]{}'<>.^*()%!-]\"\ntw_test_lemm=[]\nlm = WordNetLemmatizer()\nfor i in range(test.shape[0]):\n    tweet = test.iloc[i].text\n    tweet = ' '.join(re.sub(pattern, '', tweet).strip().split())\n    tweet = ' '.join([word for word in tweet.split() if word.isalpha()])\n    tweet = tweet.lower()\n    tweet = contractions.fix(tweet)\n    tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    tweet = ' '.join(word for word in tweet.split() if word not in en_stop_words)\n    tweet = spell.correction(tweet)\n    tweet = ' '.join([lm.lemmatize(word) for word in tweet.split()])\n    tw_test_lemm.append(tweet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"tweet_lemms\"]=pd.Series(tw_test_lemm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## wordcloud."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_real = data[data.target == 1]\ndata_fake = data[data.target == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(data_real.shape[0]):\n    x = data_real.iloc[i].tweet_lemms\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1]\n    real.append(x) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=[line for line in real for line in set(line)]\npred = Counter(pred)\npred = pred.most_common(20)\npred=pd.DataFrame(pred,columns = ['Words', 'Frequency'])\npred.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fake = []\nregexp=RegexpTokenizer(r\"\\w+\")\n\nfor i in range(data_fake.shape[0]):\n    x = data_fake.iloc[i].tweet_lemms\n    x = regexp.tokenize(x) \n    x = [t for t in x if len(t)>1] \n    fake.append(x) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nopred=[line for line in fake for line in set(line)]\nnopred = Counter(nopred)\nnopred = nopred.most_common(20)\nnopred=pd.DataFrame(nopred,columns = ['Words', 'Frequency'])\nnopred.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\n\nplt.subplot(321)\nreal=(\" \").join(pred[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800 ).generate(real)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('Real disaster', fontsize=25)\n\nplt.subplot(322)\nfake=(\" \").join(nopred[\"Words\"])\nwc = WordCloud(max_words = 20, width = 1800 , height = 800).generate(fake)\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.title('fake disaster', fontsize=25)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scattertext."},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2['cat']=data['target'].astype(\"category\").cat.rename_categories({0:'fake',1:'real'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nscatter_corpus_nlp = st.CorpusFromPandas(data2,\n                             category_col='cat',\n                             text_col='tweet_lemms',nlp=nlp).build()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization of words in real / fake."},{"metadata":{"trusted":true},"cell_type":"code","source":"html = st.produce_scattertext_explorer(scatter_corpus_nlp,\n         category='real',category_name='real',         \n        not_category_name='fake',width_in_pixels=1000,\n          metadata=data2['cat'])\nopen(\"tweet-Visualization.html\", 'wb').write(html.encode('utf-8'))\nIFrame(src='tweet-Visualization.html', width = 1300, height=700)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization of specific terms but that have low occurrence compared to general terms."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfeat_builder = st.FeatsFromOnlyEmpath()\nempath_corpus = st.CorpusFromParsedDocuments(data2,\n                                              category_col='cat',\n                                              feats_from_spacy_doc=feat_builder,\n                                              parsed_col='tweet_lemms').build()\nhtml = st.produce_scattertext_explorer(empath_corpus,\n                                        category='real',\n                                        category_name='real',\n                                        not_category_name='fake',\n                                        width_in_pixels=1000,\n                                        metadata=data2['cat'],\n                                        use_non_text_features=True,\n                                        use_full_doc=True,\n                                        topic_model_term_lists=feat_builder.get_top_model_term_lists())\nopen(\"tweet-Visualization-Empath.html\", 'wb').write(html.encode('utf-8'))\nIFrame(src='tweet-Visualization-Empath.html', width = 1300, height=700)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Textblob."},{"metadata":{"trusted":true},"cell_type":"code","source":"blob = TextBlob(str(data['tweet_lemms']))\npos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])\npos_df = pos_df.pos.value_counts()[:20]\npos_df.iplot(\n    kind='bar',\n    xTitle='POS',\n    yTitle='count', \n    title='Top 12 Part-of-speech tagging para tweet_lemms')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"def counter_word (text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = counter_word(data[\"tweet_lemms\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(counter)\nembedding_dim = 32\n\nmax_length = 20\ntrunc_type='post'\npadding_type='post'\n\noov_tok = \"<XXX>\"\nseq_len = 12","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = data[\"tweet_lemms\"].values\nX_test = test[\"tweet_lemms\"].values\ny_train = data.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train, maxlen=max_length, padding=padding_type, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = tokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, maxlen=max_length, padding=padding_type, truncating=trunc_type)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Deep Learning."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers.embeddings import Embedding\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import GaussianNoise\nimport keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Earlystopping."},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_early_stopping = EarlyStopping(\n    monitor='val_accuracy', \n    patience=50,\n    restore_best_weights=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.GaussianNoise(0.5),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.GaussianNoise(0.6),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.GaussianNoise(0.6),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.GaussianNoise(0.5),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 100\nbatch_size = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.3,callbacks=[custom_early_stopping])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"subm = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\npreds = model.predict(X_test)\nsubm['target'] = (preds > 0.5).astype(int)\nsubm.to_csv(\"LSTM.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}