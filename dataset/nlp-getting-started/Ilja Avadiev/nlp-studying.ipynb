{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Studying NLP: #FirstSteps #NLP"},{"metadata":{},"cell_type":"markdown","source":"> This notebook is designed to help me study NLP. The approach will be similar to my titanic notebook. Do as much as you can on your own. After the first draft look at other kaggler's notebooks for inspiration. I hope the notebook will help me (and maybe even other beginners)to get more proficient with NLP in order to be able to participate in NLP competitions."},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    WORK IN PROGRESS !!!\n    <hr/>\n    <ul> \n        <li>Sep 13 2020: RNN, LSTM, 1DConv, Embeddings</li> \n        <li>Sep 12 2020: Start of Neural Network section</li>        \n        <li>Sep 11 2020: Text Vectorization and start of ML</li>\n        <li>Sep 10 2020: Text Normalization</li>\n        <li>Sep 09 2020: Initial Public Version, Collecting TODOs and starting with RegEx and EDA</li>\n    </ul>\n</div>"},{"metadata":{},"cell_type":"markdown","source":"## Sources\n\n* [Book: Natural Language Processing in Action](https://www.amazon.com/Natural-Language-Processing-Action-Understanding/dp/1617294632/ref=sr_1_1?crid=2ZHG2WNKHLSCB&dchild=1&keywords=natural+language+processing+in+action&qid=1599757009&sprefix=natural+lan%2Caps%2C340&sr=8-1)\n* [Kaggle: Getting Started With NLP](https://www.kaggle.com/parulpandey/getting-started-with-nlp-a-general-intro)\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"**What is NLP?**\n\nNLP (Natural Language Processing) deals with the interaction between computers and \"natural\" (human) languages. \n\nA human language consists of letters, which in turn make up words, which (when put into appropriate order) create context and meaning. To teach a computer to understand (a part of) a language many steps are necessary. Each word (or a letter) has to be transformed into a representation, that a computer can understand (e.g. numbers and vectors). The words might need to be cleaned or otherwise preprocessed before they can be input into a model. Based on the task, the availability of data and computational power a different model might be appropriate. NLP has to deal with all (and more) of the above steps.\n\n\n**What is NLP used for?**\n\n* Machine Translation\n* Sentiment Analysis\n* Text To Speach\n* Text Generation\n* ... and much more\n\n**What are some very common definintions in NLP?**\n\n* [Corpus](https://en.wiktionary.org/wiki/corpus)\n--\nA corpus is the collection of documents or texts used for a particular NLP Problem.\n* [Lexicon](https://en.wiktionary.org/wiki/lexicon)\n--\nA lexicon is the collection of possible tokens (e.g words, letters). \n* [Token](https://en.wiktionary.org/wiki/token)\n--\nA single example of the smallest entity used for NLP (e.g. a single word).\n\n\n**What is the goal of this task?**\n\nGiven the text of a tweet can you learn to predict whether the tweet describes a real disaster?"},{"metadata":{},"cell_type":"markdown","source":"## Necessary Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \npd.set_option('display.max_colwidth', None)\n#pd.set_option('display.max_rows', 0)\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nimport re\n\n#model selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\n#nlp libraries\nimport spacy\nfrom spacy import displacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#ml\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n#metrics\nfrom sklearn.metrics import f1_score\n\n\n#deep learning\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'CPU'\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\nif tf.config.experimental.list_physical_devices('GPU'):\n    device = 'GPU'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print the current library versions\npackages = [np, pd, mpl, sns, spacy]\n\nprint('-'*35)\nfor package in packages:\n    print(package.__name__, 'version:', package.__version__)\n    print('-'*35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Necessary Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"paths = []\nfor root, dirs, files in os.walk('/kaggle/input'):\n    for file in sorted(files):\n        paths.append(os.path.join(root, file))\nprint(paths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(paths[2], index_col='id')\ntest_df = pd.read_csv(paths[1], index_col='id')\nconcat = [train_df, test_df]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NLP Process"},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# how large is the corpus\nprint('CORPUS')\nprint('Training set corpus size:', train_df.shape[0])\nprint('Test set corpus size:', test_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keyword and location are apparently missing in many cases\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will have to deal with keyword and location null values\nprint('-'*50)\nprint('\\nTRAINING DATA:\\n')\nprint(train_df.info())\nprint('-'*50)\nprint('\\nTESTING DATA:\\n')\nprint(test_df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#how do keywords look like?\n#there are 221 unique values\ntrain_df['keyword'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how do locations look like?\n# there are a lot unique values, which probably don't have a lot of predictive power\ntrain_df['location'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regular Expressions (Introduction and applictaion)"},{"metadata":{},"cell_type":"markdown","source":"Regular expressions allow you to manipulate text using pattern matching. Often it is uesefull to clean the text corpus."},{"metadata":{},"cell_type":"markdown","source":"> ***Cheat Sheet for Regular Expressions***\n\n**Character Classes**\n* .  (The dot matches any Character but not the new line)\n* \\d (Digits between 0 and 9)\n* \\D (Anything but digits)\n* \\w (Word characters a-z, A-Z, 0-9, _)\n* \\D (Not Words)\n* \\s (Whitespace \\t\\r\\n\\v\\f)\n* \\S (Not a whitespace)\n\n**Anchors**\n* \\b (Word boundary)\n* \\B (Not word boundary)\n* ^ (Beginning of a string)\n* $ (End of string)\n\n**Groups/Ranges**\n* [abc] (Character set: a or b or c)\n* [^abc] (Character set: not a or b or c)\n* () (Group)\n* () | () (Group 1 or group 2)\n\n**Qantifiers**\n\n* \\* (zero or more)\n* \\+ (one or more)\n* ? (optional 0 or more)\n* {5} (exactly 5)\n* {5, 8} (range of numbers)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets look at some regex examples\nexample = 'The quick brown fox jumps over the lazy dog.'\n# find the word jumps\nprint(re.findall(r'j\\w+', example))\n# find words that are exactly 4 letters long\nprint(re.findall(r'\\b\\w{4}\\b', example))\n# find pairs of words\nprint(re.findall(r'\\w+\\s\\w+', example))\n# find the last word in the sentence\nprint(re.findall(r'\\w+.?$', example))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Regular expression can be usefull to look for entries with hashtags."},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in concat:  \n    # find a hashtag and create a new column\n    df['hashtags'] = df.text.str.findall(r'#\\w+')\n    # find a user mentioned and create a new column\n    df['user'] = df.text.str.findall(r'@\\w*')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many hashtags are in a tweet?\nfor df in concat:\n    df['hash_count'] = df['hashtags'].str.len()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# most tweets don't actually have a hashtag\nplt.figure(figsize=(20,10))\nsns.countplot(x='hash_count', data=train_df).set_title('Count for number of #hashtags')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# it does not look like people necessarily use more hashtags in case of emmergency\nplt.figure(figsize=(20,10))\nsns.barplot(x='hash_count', y='target', data=train_df).set_title('Number of hashtags vs % of real disaster tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#how many of the tweets are actual disater tweets\nsns.countplot(x='target', data=train_df).set_title('Not Disaster vs Disater Tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at the tweets themselves\n#some of the text seems to repeat\ntrain_df.describe(include=object)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in concat:\n    df['length'] = pd.Series(df.loc[:, 'text'].str.len())\n    df['length'] = pd.Series(df.loc[:, 'text'].str.len())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25, 10))\naxes = sns.countplot(x='length', data=train_df)\naxes.set_title('Length of tweet and disaster')\naxes.set_xticklabels(axes.get_xticklabels(), rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Null Values and Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#deal with missing hashtags\nfor df in concat:\n    df['hashtags'] = df['hashtags'].apply(lambda x: ['NoHashTag'] if not x else x)\n    df['hashtags'] = df['hashtags'].apply(lambda x: ' '.join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#deal with missing location and keyword\nfor df in concat:\n    df['keyword'].fillna(value='Missing', inplace=True)\n    df['location'].fillna(value='Missing', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clean text\n#lets write a function, as we probably have to do several cleaning steps\n#there are probably better solutions out there\n#TODO look at other notebooks\ndef clean_tweet(doc):\n    \n    #remove urls\n    doc =  re.sub(r'https?://\\w?\\.?\\w*/\\w*', '', doc)\n    #remove @user\n    doc = re.sub(r'@\\w*', '', doc)\n    #remove date in the format nn/nn/nn\n    doc = re.sub(r'\\d+/\\d+/\\d+', '', doc)\n    #remove time in the format hh:mm\n    doc = re.sub(r'\\d+:\\d+', '', doc)\n    #remove special signs\n    doc = re.sub(r'[#@.?:-=/\\\\<>\\]\\[]', '', doc)\n    #remove words containing numbers\n    doc = re.sub(r'(\\w+\\d+|\\d+\\w+)', '', doc)\n    return doc\n\nfor df in concat:\n    df['text'] = df['text'].apply(clean_tweet)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenizer"},{"metadata":{},"cell_type":"markdown","source":"**Tokens**\n\nAs the name suggests a tokenizer splits your corpus into tokens.\n\nFor example the sentence 'The quick brown fox jumps over the lazy dog.' might be split into the following list, where each word is stripped out, the unnecessary punctuation is removed and case folding is performed.\n\n**['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']**\n<hr/>\n\n**n - grams**\n\nn-grams are combination of words\n\nn stands for the number of words that have to be combined. \n\n2-grams look for example as follows (without any cleaning)\n\n**[('The', 'quick'),\n ('quick', 'brown'),\n ('brown', 'fox'),\n ('fox', 'jumps'),\n ('jumps', 'over'),\n ('over', 'the'),\n ('the', 'lazy'),\n ('lazy', 'dog.')]**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1-gram tokenizer\nexample = 'The quick brown fox jumps over the lazy dog.'\n\n# remove the dots and make all words lower case\nclean_example = re.sub(r'\\.', '', example).lower()\nprint(clean_example.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2-gram tokenizer\n\nexample = 'The quick brown fox jumps over the lazy dog.'\n\nwithout_first = example.split()[1:]\nwithout_last = example.split()[:-1]\n\nlist(zip(without_last, without_first))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What is a hashing trick? Why is it useful?"},{"metadata":{},"cell_type":"markdown","source":"### Stemmer"},{"metadata":{},"cell_type":"markdown","source":"According to Wikipedia:\n> In linguistics, a stem is a part of a word used with slightly different meanings... \n\nIf we take the following words for example\n* jump\n* jumps\n* jumped\n* jumping\n\nthen the stem of all the above words would be \"jump\".\n\nA stemmer usually works by simply cutting parts of the words.\n\nFor example.\n\n<ul> \n    <li>jump</span></li>\n    <li>jump<span style=\"color: red\">s</span></li>\n    <li>jump<span style=\"color: red\">ed</span></li>\n    <li>jump<span style=\"color: red\">ing</span></li>\n</ul>\n\n**What is the purpose of the stemmer**?\n\nYou assume that these words have a similar influence on your output variable. For example the words fire, fires, fire-alarm, firemen, firefighter might all appear in a sentence when a fire disater occurs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# a simple stemmer that only takes care of the trailing s could for example look like this\ndef stem(sentence):\n    stemmed_sentence = []\n    for word in sentence.split():\n        stemmed_word = re.findall(r'^(.*?)(s)?$', word)[0][0]\n        stemmed_sentence.append(stemmed_word)\n    return ' '.join(stemmed_sentence)\n\n# the stemmer works pretty good on this example\nexample = 'The quick brown fox jumps over the lazy dog.'\nprint(stem(example))\n\n# a lot of meaning is lost with this stemmer\nexample_2 = 'He was on the bus with his abs'\nprint(stem(example_2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"porter stemmer\n\nsnowball stemmer"},{"metadata":{},"cell_type":"markdown","source":"### Lemmatization"},{"metadata":{},"cell_type":"markdown","source":"Lemmatization is similar to stemming but instead of looking for a stem of a word you look for its lemma.\n\nFor example the words:\n* go\n* goes\n* went\n\nall have the lemma go. A tokenizer would not transform \"went\" to go and might transform \"goes\" to \"goe\" depending on the quality of the stemmer."},{"metadata":{"trusted":true},"cell_type":"code","source":"# I can imagine, that for very simple tools there is a simple lookup table\nlemma_lookup = {'go': 'go',\n               'went': 'go',\n               'goes': 'go',\n               'gone': 'go',\n               'going': 'go',\n               'jumps': 'jump',\n               'jumped': 'jump',\n               'jumping': 'jump'}\n\ndef lemma(sentence):\n    for word in sentence.lower().split():\n        print(lemma_lookup.get(word, word))\n\nexample = 'The quick brown fox jumps over the lazy dog.'\nlemma(example)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stop Words"},{"metadata":{},"cell_type":"markdown","source":"Stop words are words that appear very often in sentences, but not necessarily make up the meaning of the sentence.\n\nExamples of stop words are\n* the\n* and\n* or\n* to\n\nThere are libraries that have a collection of stop words. The collections can be used to remove the stop words from your corpus. But that approach is sometimes frowned upon, as you might lose to much information by removing them. There are other approaches to deal with stop words. For example TF-IDF deals with stop words implicitly by giving these words less meaning."},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = ['the', 'over', 'by', 'to', 'from']\nexample = 'The quick brown fox jumps over the lazy dog.'\n\n#removing stop words is really simple with a list comprehension\n[word for word in example.lower().split() if word not in stop_words]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vectorization"},{"metadata":{},"cell_type":"markdown","source":"In order to make the text computer readable it is necessary to transform the text into a numeric format. Usually this is done by creating a vector for each word."},{"metadata":{},"cell_type":"markdown","source":"#### One Hot Encoding"},{"metadata":{},"cell_type":"markdown","source":"One hot encoding creates sparse (contain mostly zeros) vectors. \"One hot\" means that only one cell per word is hot (meaning 1), the rest of the cells are cold (meaning 0).\n\nThe rows of the following matrix show each word of the sentence: 'the quick and brown fox jumps over the lazy dog'.\n\nThe columns of the matrix are as long as there are distinct words in the corpus. Because the word 'the' appears twice in the sentence the number of columns is larger than the number of rows.\n\n\n| the | quick | brown | fox | jumps | over | the | lazy | dog |\n| :-: | :----:| :---: | :-: | :---: | :--: | :-: | :--: | :-: |\n| 1   | 0     | 0     | 0   | 0     | 0    | 1   | 0    | 0   |\n| 0   | 1     | 0     | 0   | 0     | 0    | 0   | 0    | 0   |\n| 0   | 0     | 1     | 0   | 0     | 0    | 0   | 0    | 0   |\n| 0   | 0     | 0     | 1   | 0     | 0    | 0   | 0    | 0   |\n| 0   | 0     | 0     | 0   | 1     | 0    | 0   | 0    | 0   |\n| 0   | 0     | 0     | 0   | 0     | 1    | 0   | 0    | 0   |\n| 0   | 0     | 0     | 0   | 0     | 0    | 0   | 1    | 0   |\n| 0   | 0     | 0     | 0   | 0     | 0    | 0   | 0    | 1   |\n\n<br/>\nNot all models can deal \"one hot\" vecorized sentences. In the above depiction the row size is static and is as long as there are tokens in the corpus, but the size of the columns depends on the length of the sentence. Some models like RNN (Recurrent Neural Networks) can deal with varying array size, but for the simple models we are going to utilize first this representaion is not fit.\n<hr/> \n\n* orthonormal vecors\n* too many dimensions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#example of one hot encoding\nprint('-' * 100)\n\nsentence = 'the quick brown fox jumps over the lazy dog'\nrow_lookup = {}\n\nprint('Origninal sentence: {}'.format(sentence))\nprint('-' * 100)\n\n# unique words in the corpus represent the number of rows in the matrix\nrow_names = set(sentence.split())\n\n\nfor i, row in enumerate(row_names):\n    row_lookup[row] = i\n    \n\nrows = len(row_names)\nprint('Row Encodings: ', row_lookup)\nprint('Column Encodings: ', sentence)\n# the length of the sentence is the number of columns\ncolumns = len(sentence.split())\n\nprint('-' * 100)\none_hot = np.zeros((rows, columns))\n\nfor i, column in enumerate(sentence.split()):\n    one_hot[row_lookup[column], i] = 1\n\nprint(one_hot)\n\nprint('-' * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bag of words\n\nIn the bag of words representation of a sentence you count up the number of occurences of a word.\n\nThe sentence 'the quick and brown fox jumps over the lazy dog' would like as follows in the bag of words representation.\n\n| the | quick | brown | fox | jumps | over | lazy | dog | other | word |\n| :-: | :----:| :---: | :-: | :---: | :--: | :--: | :-: | :---: | :--: |\n| 2   | 1     | 1     | 1   | 1     | 1    | 1    | 1   |0      |0     |\n\n<br/>\nEach cell of the vector contains the number of times a word appears in the sentence. The word 'the' appears twice. The words 'other' and 'word' are counted as 0. These words are not used in the sentence, but are available somewhere in the corpus.\nThis is a more efficient representation as less numbers have to be stored in memory. The problem is that the order of the words gets jumbled especially when the corpus contains out of thousands of words which might lead to a loss of information. This representaion should only be used when the you can accept this loss of information. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating bag of words\nfrom collections import Counter\nwords = []\n\nsentence_1 = 'the quick brown fox jumps over the lazy dog'\nsentence_2 = 'other word'\n\nwords_1 = [word for word in sentence_1.split()] \nwords_2 = [word for word in sentence_2.split()]\nunique_words = set(words_1 + words_2)\n\ncounter_1 = Counter(words_1)\ncounter_2 = Counter(words_2)\n\nbag_1 = {}\nbag_2 = {}\nfor word in unique_words:\n    bag_1[word] = counter_1[word]\n    bag_2[word] = counter_2[word]\n\nprint('-'*120)\nprint('First bag of words')\nprint(bag_1)\nprint('-'*120)\nprint('Second bag of words')\nprint(bag_2)\nprint('-'*120)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### TF-IDF"},{"metadata":{},"cell_type":"markdown","source":"**TF-IDF stands for \"term frequency times inverse document frequency\"**\n\nIt approximates how important a word is(related to the document and all other documents).\n\nLets look at 3 sentences to understand what TF represents and how it changes depending on the tokens.\n\n* 'the quick brown fox jumps over the lazy dog.'\n* 'the lazy and dirty dog enjoys his meal'\n* 'the fox hunts and eats a chicken'\n\n**Term frequency is calculated as follows:**\n\n$$TF = \\frac{TokenXInDocument}{TokensInDocument}$$\n\nHow does the TF look for the word 'the' in the first sentence?\n\n$$TF_{'the_{FirstSentence}'} = \\frac{2}{9}$$\n\n'the' occurs twice in the sentence. There are 9 tokens (words) in the sentens. So the TF is 2/9. The more often a word occurs in the sentence the more important it becomes.\n\n**Inverse document frequency is calculated as follows:**\n\n$$IDF = log\\frac{NumberOfDocuments}{DocumentsWithTokenX}$$\n\n$$IDF_{'the'_{FirstSentence}} = log\\frac{3}{3}$$\n\n'the' occurs in each sentence and gets therefore an IDF of 0 (log of 1 is 0). The more documents there are which contain a certain word the less important it becomes.\n\n$$TF{-}IDF = TF * IDF$$\n\n$$TF{-}IDF_{'the'_{FirstSentence}} = \\frac{2}{9} * 0 = 0$$\n\n'the' is a very common stop word, but we did not have to remove the word explicitly, yet it still becomes unimportant through it's vector representation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate tf idf with pandas\n# lets look at the following sentences to understand TF-IDF\nsentence_1 = 'the quick brown fox jumps over the lazy dog'\nsentence_2 = 'the lazy and dirty dog enjoys his meal'\nsentence_3 = 'the fox hunts and eats the chicken'\nsentence_4 = 'the angry hunter wants to avenge the chicken'\nsentence_5 = 'the hunter can`t find the fox and punishes the dog'\nsentence_6 = 'the dog is hungry'\nsentence_7 = 'the dog eats the fox'\n\n#will be used for pandas df index\nindex=['sentence_1', 'sentence_2', 'sentence_3', 'sentence_4', 'sentence_5', 'sentence_6', 'sentence_7']\n\n#for easier looping\ncorpus=[sentence_1, sentence_2, sentence_3, sentence_4, sentence_5, sentence_6, sentence_7]\n\n#'document' : Counter('token: count_in_document')\ncounters = {}\nfor idx, document in enumerate(corpus):\n    counters[index[idx]] = Counter(document.split())\n      \ndf = pd.DataFrame(counters).transpose()\n\nrow_sum = df.sum(axis=1)\ncol_count = df.count(axis=0)\n\ndf.fillna(value=0, inplace=True)\n\n#term frequency\ntf = df.div(row_sum, axis='index')\n#inverse document frequency\nidf = np.log(len(corpus) / col_count)\n\n#finally tf-idf\ntf_idf = tf * idf\ntf_idf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cosine Similarity"},{"metadata":{},"cell_type":"markdown","source":"In some applications (for example search engines) it is essential to find out how similar two words or sentences are. You can calculate how similar two vectors are by calculating the cosine of the angle between two vectors. If the sentences have no similarities, there will be at a 90 degree angle and thus have a similarity of 0. If the sentences are very similar they will have a cosine close to 1 (TF-IDF can not have negative values and thus no negative cosine values). All one-hot vectors are by design dissimilar (orthogonal vectors). Therefore they are usually transformed (embedded) into a smaller dimensional vector space where similarity can be calculated (more about that will be discussed in the embedding section). \n\n$$similarity= \\cos(θ) = \\frac{A\\cdot B}{\\lVert A \\rVert \\cdot \\lVert B \\rVert}$$"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can use numpy to calculate similarity between the TF-IDF representaion\ndef similarity(doc_1, doc_2):\n    #numerator\n    num = np.dot(doc_1, doc_2)\n    #denominator\n    norm_a = np.linalg.norm(doc_1)\n    norm_b = np.linalg.norm(doc_2)\n    den = norm_a * norm_b\n    return num * den\n\n# sentence 1 and 4 are very dissimilar (no common words)\nprint(similarity(tf_idf.loc['sentence_1', :], tf_idf.loc['sentence_4', :]))\n\n# sentence 1 and 7 are more similar, because the both have dog and fox mentioned\nprint(similarity(tf_idf.loc['sentence_1', :], tf_idf.loc['sentence_7', :]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NLP Libraries: Normalization and Vectorization"},{"metadata":{},"cell_type":"markdown","source":"#### Normalization with spaCy"},{"metadata":{},"cell_type":"markdown","source":"There are a lot of NLP and ML libraries out there with can take over the whole process text preprocessing. Lets look at spaCy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We start by working with dummy corpus to learn the libraries\n# We introduce some punctuation e.t.c. to test how good the libraries are\ncorpus = [\n    'The quick brown fox jumps over the lazy dog.',\n    'The lazy and dirty dog enjoys his meal.',\n    'The fox hunts and eats the chicken.',\n    'The angry hunter wants to avenge the chicken.',\n    'The hunter can`t find the fox and punishes the dog.',\n    'The dog is hungry!!!',\n    \"The dog doesn't let the fox hunt a chicken again.\",\n    'The hunter went to the doctor!!!'\n]\n\ncorpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#spaCy\n# Load English tokenizer, tagger, parser, NER and word vectors\nnlp = spacy.load(\"en_core_web_sm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(corpus[0])\n\n#at first clance this is just a sentence\nprint(doc)\n\n#But we receive a Doc class\n#According to spaCy documentation \"A Doc is a sequence of Token objects\"\nprint(type(doc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SpaCy tokenizes the text and uses it's internal model to create linguistic features. SpyCy creates lemmas, categorizes words into stop words and so on. The functionality is impressive."},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets loop over the doc object\n\n#https://spacy.io/usage/linguistic-features\n    #Text: The original word text.\n    #Lemma: The base form of the word.\n    #POS: The simple UPOS part-of-speech tag.\n    #Tag: The detailed part-of-speech tag.\n    #Dep: Syntactic dependency, i.e. the relation between tokens.\n    #Shape: The word shape – capitalization, punctuation, digits.\n    #is alpha: Is the token an alpha character?\n    #is stop: Is the token part of a stop list, i.e. the most common words of the language?\n\nfor token in doc:\n    print('TOKEN: ', token.text, '\\t', '| LEMMA: ', token.lemma_, '   \\t', '| POS: ', token.pos_, '\\t' \\\n          '| TAG: ', token.tag_, '\\t',  '| DEPENDENCY: ', token.dep_, '     \\t', \\\n          '| SHAPE: ',  token.shape_, '   \\t', 'ISALPHA: ', token.is_alpha, '\\t', 'ISSTOPWORD: ', token.is_stop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# spacy has an explain method to help with the language terminology\nspacy.explain('ADJ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets take it one step at a time and look at a simple sentence\nprint(corpus[-1:])\n\ndoc = nlp(corpus[-1])\n\nprint('\\nLEMMAS:')\nprint('-'*30)\n# How do the lemmas look like?\n# The tokens look natural. And it correctly transforms 'went' into 'go'\nfor token in doc:\n    print(token.lemma_, end=\" \")\n    \n# What are the stop words\nprint('\\n\\nSTOP WORDS:')\nprint('-'*30)\nfor token in doc:\n    if token.is_stop: print(token.text, end=\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# an additional advantage of spacy is that it allows you to draw the relationships in a sentence\ndisplacy.render(doc, style=\"dep\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets reduce the pipeline\n# we don't need all component of spacy, removing parts in the pipeline improves the performance\n# https://spacy.io/usage/processing-pipelines#pipelines\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n\n#nlp = spacy.load(\"en_core_web_sm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspiration for the function\n# https://towardsdatascience.com/turbo-charge-your-spacy-nlp-pipeline-551435b664ad\ndef lemmatize(text):\n    doc = nlp(text)\n    #remove stop words and punctuation and return the lemmas ow words\n    return ' '.join([token.lemma_.lower() for token in doc if not token.is_stop and not token.pos_ == 'PUNCT'])\n\nfor df in concat:\n    df['lemma'] = df['text'].apply(lemmatize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# looks reasonable enough\ntrain_df.loc[:, ['text', 'lemma']].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Vectorization with sklearn"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"spaCy can do a lot more, but for now we are going to turn to sklearn to vectorize the \"lemma\" version of the sentences."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we use the TF-IDF Class to generate the vectorized form of the corpus\nvectorizer = TfidfVectorizer()\ncorpus_train = train_df.loc[:, 'lemma']\ncorpus_test = test_df.loc[:, 'lemma']\n\n# I don't quite understand how the fit function would work for the test data if we fit the data on the training set\n# The TF has to be calculated for each document, but the IDF has to be calculated based on each word and the documents that the word appears\n# do we use the test data, the test + train data or only the train data to calculate the idf (I assume train data)\n# if we fit the model on the training data I would assume, that only the training data is used, but how is dealt with new words\n\n#building a pipeline in sklearn might be a good idea overall\nX_train = vectorizer.fit_transform(corpus_train)\nX_test = vectorizer.transform(corpus_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there are 7613 sentences and 12394 words\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we can get all the available words\n#print(vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training using classic (non NN) supervised learning"},{"metadata":{},"cell_type":"markdown","source":"We are ready to run some models and see how we perform. Cross Validation will be used throughout the evaluation and F1 will be used as the decision criteria."},{"metadata":{},"cell_type":"markdown","source":"#### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression could be used as a baseline\nlog_reg = LogisticRegression()\nscores = cross_val_score(log_reg, X_train, y_train, cv=10, scoring='f1', n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores)\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model on all data points for submission\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_train, log_reg.predict(X_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### K-nearest neighbors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression could be used as a baseline\nknn = KNeighborsClassifier(n_neighbors=100)\nscores = cross_val_score(knn, X_train, y_train, cv=10, scoring='f1', n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores)\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=100)\n#this part takes a lot of time if you use cv\n# scores = cross_val_score(rf, X_train, y_train, cv=10, scoring='f1', n_jobs=-1)\nrf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_train, rf.predict(X_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SOME PARAMETERS OF XGBOOST\n# eta = learning_rate (default=0.3)\n# gamma = min_split_loss (default=0)\n# objective = loss_function (default=reg:squarederror), we will use binary:logistic\n\n#here we create a cross val cross validation set in order to be able to use early stopping\nX_train_small, X_cv, y_train_small, y_cv = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)\n\n\n#we determined at the top if we are using the gpu\n#if device == 'GPU':\n    # better turn on the gpu\n    #xgb_clf = xgb.XGBClassifier(n_estimators=1000, gamma=0.1, objective='binary:logistic', tree_method='gpu_hist')\n\n    #xgb_clf.fit(X_train_small, y_train_small, eval_set=[(X_cv, y_cv)], eval_metric=\"logloss\", verbose=True, early_stopping_rounds=10)\n\n    # in case we want to use stratified cross validation\n    #scores = cross_val_score(xgb_clf, X_train, y_train, cv=10, scoring='f1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(scores)\n#print(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if device == 'GPU':\n#    #xgb_clf.fit(X_train, y_train)\n#    f1_score(y_train, xgb_clf.predict(X_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of all the classifiers Logistic Regression actually performed the best in the training set. But it is entirely possible that by tweaking RandomForests and XGBoos we could achieve better results. \n\nNext we will dive in deep learning."},{"metadata":{},"cell_type":"markdown","source":"##### Dimensinality Reduction techinques\nTODO"},{"metadata":{},"cell_type":"markdown","source":"### Deep Learning"},{"metadata":{},"cell_type":"markdown","source":"Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"We won't be using the tokenization and the tf-idf representation calculated before. Instead we will use the Keras specific tokenization to learn the Keras pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_df.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# over 15000 unique tokens\n# that is probably too much for such a small dataset size\nprint(len(tokenizer.word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = tokenizer.texts_to_matrix(train_df.text, mode='binary')\nX_test = tokenizer.texts_to_matrix(test_df.text, mode='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Multilayer Perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"# keras model\n#mlp_model = keras.models.Sequential()\n#mlp_model.add(layers.Input(shape=(X_train.shape[1],)))\n#mlp_model.add(layers.Dense(100, activation='relu'))\n#mlp_model.add(layers.Dense(20, activation='relu'))\n#mlp_model.add(layers.Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#history = mlp_model.fit(X_train, y_train, epochs=200, batch_size=128, validation_split=0.25, callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keras model\n'''\nembed_model = keras.models.Sequential()\n#embed_model.add(layers.Input(shape=(X_train.shape[1],)))\nembed_model.add(layers.Embedding(X_train.shape[1], 100, input_length=X_train.shape[1]))\nembed_model.add(layers.Flatten())\nembed_model.add(layers.Dense(100, activation='relu'))\nembed_model.add(layers.Dense(10, activation='relu'))\nembed_model.add(layers.Dense(1, activation='sigmoid'))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#embed_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#history = embed_model.fit(X_train, y_train, epochs=200, batch_size=128, validation_split=0.25, callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### RNN (Recurrent Neural Network)"},{"metadata":{},"cell_type":"markdown","source":"##### Simple RNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nrnn_model = keras.models.Sequential()\nrnn_model.add(layers.Input(shape=(None, X_train.shape[1])))\n#rnn_model.add(layers.Embedding(X_train.shape[1], 10))\nrnn_model.add(layers.SimpleRNN(10))\nrnn_model.add(layers.Dense(10, activation='relu'))\nrnn_model.add(layers.Dense(1, activation='sigmoid'))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#history = rnn_model.fit(X_train.reshape((7613, 1, 15365)), y_train, epochs=200, batch_size=128, validation_split=0.25, callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = 5000\nembedding_dim = 100\nmaxlen = 100\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(train_df.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = tokenizer.texts_to_sequences(train_df.text)\nX_test = tokenizer.texts_to_sequences(test_df.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\nX_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nlstm_model = keras.models.Sequential()\nlstm_model.add(layers.Embedding(max_words, embedding_dim, input_length=maxlen))\nlstm_model.add(layers.LSTM(32,return_sequences=True))\nlstm_model.add(layers.LSTM(32,return_sequences=True))\nlstm_model.add(layers.LSTM(32,return_sequences=True))\nlstm_model.add(layers.LSTM(32))\nlstm_model.add(layers.Dense(1, activation='sigmoid'))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lstm_model.compile(optimizer=keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#history = lstm_model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.25, callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1d CNN (Convolutional Neural Network)"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ncnn_model = keras.models.Sequential()\ncnn_model.add(layers.Embedding(max_words, embedding_dim, input_length=maxlen))\ncnn_model.add(layers.Conv1D(16, 7, activation='relu'))\ncnn_model.add(layers.Dropout(0.5))\ncnn_model.add(layers.MaxPooling1D(5))\ncnn_model.add(layers.Conv1D(32, 7, activation='relu'))\ncnn_model.add(layers.Dropout(0.5))\ncnn_model.add(layers.GlobalMaxPooling1D())\ncnn_model.add(layers.Dense(1, activation='sigmoid'))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#history = cnn_model.fit(X_train, y_train, epochs=200, batch_size=128, validation_split=0.25, callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Reuse Embeddings from TFHub"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_hub as hub\nimport tensorflow as tfhub\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nhub_model = keras.Sequential([\n    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", dtype=tf.string, input_shape=[], output_shape=[50]),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation='sigmoid')\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hub_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hub_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_df['text'].to_numpy()\ny_train = train_df['target'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_df['text'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tr = X_train[:6000]\nX_train_cv = X_train[6000:] \n\ny_train_tr = y_train[:6000]\ny_train_cv = y_train[6000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = tf.data.Dataset.from_tensor_slices((X_train_tr, y_train_tr))\nvalidation_data = tf.data.Dataset.from_tensor_slices((X_train_cv, y_train_cv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_examples_batch, train_labels_batch = next(iter(train_dataset.batch(10)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_examples_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = hub_model.fit(train_data.shuffle(10000).batch(512),\n                    epochs=20,\n                    verbose=1,\n                    validation_data=validation_data.batch(512))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = hub_model\n\n# for sklearn\n#y_pred = model.predict(X_test)\n\n# for keras\ny_pred = model.predict_classes(X_test)\ny_pred = y_pred.reshape(y_pred.shape[0],)\n\nsubmit_df = pd.DataFrame({'id': test_df.index, 'target': y_pred})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_df.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}