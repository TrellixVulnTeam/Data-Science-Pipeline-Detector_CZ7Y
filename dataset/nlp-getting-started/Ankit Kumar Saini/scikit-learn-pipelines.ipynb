{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import Section"},{"metadata":{"_uuid":"641f0fa2-1b4d-4629-b1ea-a00e7bccedf7","_cell_guid":"f41d23ed-aed1-4d3c-bec5-a75920185630","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport numpy as np \nimport pandas as pd \n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, log_loss\nfrom sklearn.feature_extraction.text import (CountVectorizer, \n                                             TfidfVectorizer, \n                                             TfidfTransformer)\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load training and test data sets"},{"metadata":{"_uuid":"c753c965-34ff-43de-aada-203e3b32f7c1","_cell_guid":"bbe34c9a-6680-47f8-88b8-ebd0ddcd559a","trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the shape of the training and test data sets\nprint(\"Shape of training data:\", train_df.shape)\nprint(\"Shape of test data:\", test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the number of null values in the training data\ntrain_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the number of null values in the test data\ntest_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop location column as it contains a lot of null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop('location', axis = 1, inplace = True)\ntest_df.drop('location', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display few tweets from training data set\nprint(train_df['text'][:10].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clean text by removing\n- Hash tag\n- Punctuation marks\n- Lower case all the text "},{"metadata":{"trusted":true},"cell_type":"code","source":"# substitute url with a placeholder in the text messages\nurl_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n\ndef url_holder(text):\n    \"\"\"\n    This function will receive text message as input and will\n    substitute url with a placeholder\n    \n    Args: \n        text: text message \n    \"\"\"\n    # find all the urls in the text message\n    detected_url = re.findall(url_regex, text)\n    \n    # iterate over the urls and substitute with the placeholder\n    for url in detected_url:\n        text = text.replace(url, 'url')\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add some new features to the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_features(df):\n    \"\"\"\n    This function will create additional features to improve the performace\n    of the model. Features such as length of the message, number of words, \n    number of non stopwords and average word length in each message will be\n    created by this method.\n    \n    Args: \n        df: original dataframe\n        \n    Returns:\n        df: dataframe with new added features\n    \"\"\"\n    # create a set of stopwords\n    StopWords = set(stopwords.words('english'))\n    \n    # substitute url with the placeholder in the text message\n    train_df['text'] = train_df['text'].apply(url_holder)\n    \n    # lowering and removing punctuation\n    df['processed_text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n    \n    # apply lemmatization\n    df['processed_text'] = df['processed_text'].apply(\n        lambda x: ' '.join([WordNetLemmatizer().lemmatize(token) for token in x.split()]))\n    \n    # get length of the message\n    df['length'] = df['processed_text'].apply(lambda x: len(x))\n    \n    # get number of words in each message\n    df['num_words'] = df['processed_text'].apply(lambda x: len(x.split()))\n    \n    # get the number of non stopwords in each message\n    df['non_stopwords'] = df['processed_text'].apply(\n        lambda x: len([t for t in x.split() if t not in StopWords]))\n    \n    # get the average word length\n    df['avg_word_len'] = df['processed_text'].apply(\n        lambda x: np.mean([len(t) for t in x.split() if t not in StopWords]) \\\n        if len([len(t) for t in x.split() if t not in StopWords]) > 0 else 0)\n    \n    # update stop words (didn't want to remove negation)\n    StopWords = StopWords.difference(\n        [\"aren't\", 'nor', 'not', 'no', \"isn't\", \"couldn't\", \"hasn't\", \"hadn't\", \"haven't\",\n         \"didn't\", \"doesn't\", \"wouldn't\", \"can't\"])\n    \n    # remove stop words from processed text message\n    df['processed_text'] = df['processed_text'].apply(\n        lambda x: ' '.join([token for token in x.split() if token not in StopWords]))\n        \n    # filter the words with length > 2\n    df['processed_text'] = df['processed_text'].apply(\n        lambda x: ' '.join([token for token in x.split() if len(token) > 2]))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = add_features(train_df)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display few processed text messages\nprint(train_df['processed_text'][:10].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill null values with the most frequent in keyword column\ntrain_df['keyword'] = train_df['keyword'].fillna('0')\ntest_df['keyword'] = test_df['keyword'].fillna('0')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split data into train and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(train_df.drop('target', axis = 1).iloc[:, 1:],\n                                                  train_df['target'].values,\n                                                  test_size = 0.2, \n                                                  stratify = train_df['target'].values, \n                                                  random_state = 42)\n\n# print the shape of the training and validation sets\nprint(f'x_train shape: {x_train.shape}\\ny_train shape: {y_train.shape}')\nprint(f'x_val shape: {x_val.shape}\\ny_val shape: {y_val.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations.\n    This class will select columns containing text data.\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        return X[self.key]\n    \n    \n\nclass NumColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to select a single column from the data frame to perform additional transformations.\n    This class will select the columns containing numeric data.\n    \"\"\"\n    def __init__(self, key):\n        self.key = key\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        return X[[self.key]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scikit Learn Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create separate pipelines to process individual features\n\n# pipeline to process num_words column\nnum_words = Pipeline([\n    ('selector', NumColumnSelector(key = 'num_words')),\n    ('scaler', StandardScaler())\n])\n\n# pipeline to process non_stopwords column\nnum_non_stopwords = Pipeline([\n    ('selector', NumColumnSelector(key = 'non_stopwords')),\n    ('scaler', StandardScaler())\n])\n\n# pipeline to process avg_word_len column\navg_word_length = Pipeline([\n    ('selector', NumColumnSelector(key = 'avg_word_len')),\n    ('scaler', StandardScaler())\n])\n\n# pipeline to process processed_text column\nmessage_processing = Pipeline([\n    ('selecor', TextColumnSelector(key = 'processed_text')),\n    ('tfidf', TfidfVectorizer())\n])\n\n\n# pipeline to process length column\nlength = Pipeline([\n    ('selector', NumColumnSelector(key = 'length')),\n    ('scaler', StandardScaler())\n])\n\n\n# pipeline to process keyword column\ncounter = Pipeline([\n    ('selector', TextColumnSelector(key = 'keyword')),\n    ('counter', CountVectorizer())\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Union to combine data processing"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# process all the pipelines in parallel using feature union\nfeature_union = FeatureUnion([\n    ('num_words', num_words),\n    ('num_non_stopwords', num_non_stopwords),\n    ('avg_word_length', avg_word_length),\n    ('message_processing', message_processing),\n    ('length', length),\n    ('counter', counter)\n])\n\n\n# create final pipeline to train the classifier\nfinal_pipeline = Pipeline([\n    ('feature_union', feature_union),\n    ('clf', RandomForestClassifier())\n])\n\n# fit the pipeline on trainig data\nfinal_pipeline.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate performance on validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate accuracy on validation data\ny_pred = final_pipeline.predict(x_val)\nprint(f'Accuracy on validation data: {accuracy_score(y_val, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter tuning"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# get the parameters of final pipeline\nfinal_pipeline.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# prepare dictionary of parameters\nparameters = {'feature_union__message_processing__tfidf__max_df': [0.5, 0.75, 1.0],\n              'feature_union__message_processing__tfidf__ngram_range': [(1, 1), (1, 2)],\n              'feature_union__message_processing__tfidf__use_idf': [True, False],\n              'clf__n_estimators': [200, 400],\n              'clf__max_features': ['auto', 'sqrt', 'log2'],\n             }\n\n\n# create GridSearchCV object\ngrid_cv = GridSearchCV(final_pipeline, parameters, cv = 5, n_jobs = -1)\n\n# Fit and tune the model\ngrid_cv.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display the best parameters\ngrid_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# refitting on entire training data using best settings\ngrid_cv.refit\n\n# calculate accuracy on validation data\ny_pred = grid_cv.predict(x_val)\nprint(f'Accuracy on validation data: {accuracy_score(y_val, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare test data for making predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = add_features(test_df)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract test features\ntest_features = test_df.iloc[:, 1:]\ntest_preds = grid_cv.predict(test_features)\n\n# create submission file\nsubmission = {'id': test_df.id, 'target': test_preds}\nsubmission = pd.DataFrame(submission)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save as csv file\nsubmission.to_csv('pipeline.csv', index = None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}