{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nCreating an accurate model for disaster analysis is challenging, it is even harder when it comes to Twitter text. Here you can find a model which is easy to understand and open to improve by changing model structure and hyperparameters. I got inspired from many research papers which are some of listed below. <br />\nI implemented the idea \"bringing the different task models together for getting better results\", and confirmed the power of CNN-BiLSTM pipeline. \n\nProposed **BERT-CNN-BiLSTM** learning pipeline, which consists of **three sequential modules**.<br />\nBERT produces competitive results, and can be considered as one of the new electricity of natural\nlanguage processing tasks such as sentiment analysis, named entity recognition (NER), and topic\nmodeling. The combination of CNN and BiLSTM models requires a particular design, since each\nmodel has a specific architecture and its own strengths:<br />\n• BERT is utilized to transform word tokens from the raw Tweet messages to contextual word\nembeddings.<br />\n• CNN is known for its ability to extract as many features as possible from the text.<br />\n• BiLSTM keeps the chronological order between words in a document, thus it has the ability\nto ignore unnecessary words using the delete gate.<br />\n\n\n\n**References:**<br />\n1) [A Sentiment-Aware Contextual Model for Real-Time Disaster Prediction Using Twitter Data](https://www.mdpi.com/1999-5903/13/7/163/htm) -> The idea comes from and really worth to check on, however, I am not using the same model.<br />\n2) [Automatic identification of eyewitness messages on twitter during disasters](https://reader.elsevier.com/reader/sd/pii/S0306457319303590?token=985D740724AEDB812611486EBAD3B68FA4393520D4DCD96FDADE4A642A9805D728945987C1BBBE0FDAA8EC3684E372C7&originRegion=eu-west-1&originCreation=20210920022341)<br />\n3) [Convolutional Neural Networks for Sentence Classification](http://arxiv.org/abs/1408.5882)<br />\n4) [BERT: Pre-training of Deep Bidirectional Transformers for Language\n               Understanding](http://arxiv.org/abs/1810.04805)<br />\n5) [– Understanding LSTM –\na tutorial into Long Short-Term Memory\nRecurrent Neural Networks](https://arxiv.org/pdf/1909.09586.pdf)<br />","metadata":{}},{"cell_type":"markdown","source":"# I recommend to read before starting\n\n___\n## Why Development Set is Exluded ?\nIt is simply because we have pretty hard dataset which is specified by ambiguous keywords to distinguish. The dataset is not big enough for model to understand some differences so we won't divide our little dataset. Our goal is maximizing our submission results. However, using dev set to improve analysis of results is a must almost every time. \n\nFeel free to check `version 3` for more explanatory notebook. It includes dev set application, error analysis and more surprises.\n____\n## Dataset & cleaning\nBased on this [paper](https://aclanthology.org/2020.pam-1.15.pdf) punctuations are important.  It significantly affects counting in BERT's context extraction phase. Therefore, we will not clean the punctuations which are in the texts. Even though Twitter data is a mess, sometimes these kinds of little tricky changes increase accurracy in remarkable amount.\n\nFinding perfect hyperparameters are an actual issue after preprocessing done properly. We should not do every preprocessing transaction. I did some of them to show how to see in `version 3`, however, generally traditional preprocessing affects texts in a really bad way to be learned by BERT or any contextual structures. We need to check and think how embedding models which we are going to use trained and why we need to clean that any specific property.\n\nThe best results is obtained in raw data with contextual models.\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras.layers import (Embedding, LSTM, Dense, Bidirectional, \n                                     SpatialDropout1D, Input, Conv1D, Dropout)\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom transformers import BertTokenizer, TFBertModel\n\nimport numpy as np\nimport os\nimport pandas as pd\nimport time","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-06T17:04:39.135332Z","iopub.execute_input":"2022-03-06T17:04:39.135644Z","iopub.status.idle":"2022-03-06T17:04:39.141222Z","shell.execute_reply.started":"2022-03-06T17:04:39.135611Z","shell.execute_reply":"2022-03-06T17:04:39.140445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing environment for training","metadata":{}},{"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:04:39.984455Z","iopub.execute_input":"2022-03-06T17:04:39.985196Z","iopub.status.idle":"2022-03-06T17:04:39.989941Z","shell.execute_reply.started":"2022-03-06T17:04:39.985146Z","shell.execute_reply":"2022-03-06T17:04:39.988563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-06T17:04:40.138455Z","iopub.execute_input":"2022-03-06T17:04:40.138917Z","iopub.status.idle":"2022-03-06T17:04:47.973848Z","shell.execute_reply.started":"2022-03-06T17:04:40.138877Z","shell.execute_reply":"2022-03-06T17:04:47.972735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyperparameters\nmax_length = 128\nbatch_size = 32\ndev_size = 0.1","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:04:47.975496Z","iopub.execute_input":"2022-03-06T17:04:47.975751Z","iopub.status.idle":"2022-03-06T17:04:47.979885Z","shell.execute_reply.started":"2022-03-06T17:04:47.975711Z","shell.execute_reply":"2022-03-06T17:04:47.979072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bert Tokenizer\nmodel_name = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-06T17:04:47.981272Z","iopub.execute_input":"2022-03-06T17:04:47.981558Z","iopub.status.idle":"2022-03-06T17:04:48.909238Z","shell.execute_reply.started":"2022-03-06T17:04:47.981461Z","shell.execute_reply":"2022-03-06T17:04:48.90834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the data\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:04:48.910812Z","iopub.execute_input":"2022-03-06T17:04:48.911045Z","iopub.status.idle":"2022-03-06T17:04:48.939505Z","shell.execute_reply.started":"2022-03-06T17:04:48.911019Z","shell.execute_reply":"2022-03-06T17:04:48.938605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(data):\n    tokens = tokenizer.batch_encode_plus(data, max_length=max_length, padding='max_length', truncation=True)\n    \n    return tf.constant(tokens['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:04:48.940679Z","iopub.execute_input":"2022-03-06T17:04:48.940921Z","iopub.status.idle":"2022-03-06T17:04:48.946687Z","shell.execute_reply.started":"2022-03-06T17:04:48.940888Z","shell.execute_reply":"2022-03-06T17:04:48.946035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding text data","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_encoded = bert_encode(train.text)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:04:48.947839Z","iopub.execute_input":"2022-03-06T17:04:48.948065Z","iopub.status.idle":"2022-03-06T17:04:55.117451Z","shell.execute_reply.started":"2022-03-06T17:04:48.948039Z","shell.execute_reply":"2022-03-06T17:04:55.116597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_encoded, train.target))\n    .shuffle(64)\n    .batch(batch_size)\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:04:55.118615Z","iopub.execute_input":"2022-03-06T17:04:55.118871Z","iopub.status.idle":"2022-03-06T17:04:55.129909Z","shell.execute_reply.started":"2022-03-06T17:04:55.118841Z","shell.execute_reply":"2022-03-06T17:04:55.129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Proposed Model","metadata":{}},{"cell_type":"code","source":"def bert_tweets_model():\n\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n    last_hidden_states = bert_encoder(input_word_ids)[0]   \n    x = SpatialDropout1D(0.2)(last_hidden_states)\n    x = Conv1D(64, 3, activation='relu')(x)\n    x = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.4)(x)\n    x = Dense(128, activation='relu')(x)\n    outputs = Dense(1, activation='sigmoid')(x)\n    model = Model(input_word_ids, outputs)\n   \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:04:55.131274Z","iopub.execute_input":"2022-03-06T17:04:55.131619Z","iopub.status.idle":"2022-03-06T17:04:55.140321Z","shell.execute_reply.started":"2022-03-06T17:04:55.131571Z","shell.execute_reply":"2022-03-06T17:04:55.139351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = bert_tweets_model()\n    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:04:55.141541Z","iopub.execute_input":"2022-03-06T17:04:55.141835Z","iopub.status.idle":"2022-03-06T17:05:13.777391Z","shell.execute_reply.started":"2022-03-06T17:04:55.141804Z","shell.execute_reply":"2022-03-06T17:05:13.776196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:05:13.779786Z","iopub.execute_input":"2022-03-06T17:05:13.780035Z","iopub.status.idle":"2022-03-06T17:05:15.018583Z","shell.execute_reply.started":"2022-03-06T17:05:13.780008Z","shell.execute_reply":"2022-03-06T17:05:15.017559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"%%time\nhistory = model.fit(\n    train_dataset,\n    batch_size=batch_size,\n    epochs=3,\n    verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:05:15.020377Z","iopub.execute_input":"2022-03-06T17:05:15.020746Z","iopub.status.idle":"2022-03-06T17:07:22.079032Z","shell.execute_reply.started":"2022-03-06T17:05:15.020707Z","shell.execute_reply":"2022-03-06T17:07:22.077801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SUBMISSION","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_encoded = bert_encode(test.text)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_encoded)\n    .batch(batch_size)\n)\n\npredicted_tweets = model.predict(test_dataset, batch_size=batch_size)\npredicted_tweets_binary = tf.cast(tf.round(predicted_tweets), tf.int32).numpy().flatten()","metadata":{"execution":{"iopub.status.busy":"2022-03-06T17:07:22.081824Z","iopub.execute_input":"2022-03-06T17:07:22.082166Z","iopub.status.idle":"2022-03-06T17:07:34.729744Z","shell.execute_reply.started":"2022-03-06T17:07:22.08213Z","shell.execute_reply":"2022-03-06T17:07:34.728804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_submission = pd.DataFrame({'id': test.id, 'target': predicted_tweets_binary})\nmy_submission.to_csv('./submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}