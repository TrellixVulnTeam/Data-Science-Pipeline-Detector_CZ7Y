{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Classification With Bert**\n\nOn my last notebook(https://www.kaggle.com/satoshiss/nlp-with-disaster-tweets?scriptVersionId=81728347), I tried several feature engineering with XGB classifier, but I could not see any improvements. My best score is 0.78394. Meanwhile, I checked the competition discussion and saw many people mentioned Bert. I will use apply Bert on this notebook while referring to the website\n(https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/ , https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb, https://www.analyticsvidhya.com/blog/2021/06/why-and-how-to-use-bert-for-nlp-text-classification/)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\n\nimport transformers as ppb # pytorch transformers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\n\nimport torch.nn as nn\nfrom sklearn.metrics import classification_report\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-09T01:03:15.631459Z","iopub.execute_input":"2021-12-09T01:03:15.631872Z","iopub.status.idle":"2021-12-09T01:03:15.648219Z","shell.execute_reply.started":"2021-12-09T01:03:15.631828Z","shell.execute_reply":"2021-12-09T01:03:15.646704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ndf_test =pd.read_csv(\"../input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:03:17.007085Z","iopub.execute_input":"2021-12-09T01:03:17.007434Z","iopub.status.idle":"2021-12-09T01:03:17.057805Z","shell.execute_reply.started":"2021-12-09T01:03:17.0074Z","shell.execute_reply":"2021-12-09T01:03:17.056656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name= \"bert-base-uncased\"\nmax_langth=15\n\ntexts = df_train.text\nlabels = df_train.target\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:03:18.688086Z","iopub.execute_input":"2021-12-09T01:03:18.688732Z","iopub.status.idle":"2021-12-09T01:03:18.693964Z","shell.execute_reply.started":"2021-12-09T01:03:18.688697Z","shell.execute_reply":"2021-12-09T01:03:18.692865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\nbert =AutoModel.from_pretrained(model_name,return_dict=False)\ntokenizer = BertTokenizerFast.from_pretrained(model_name)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:03:20.295924Z","iopub.execute_input":"2021-12-09T01:03:20.296682Z","iopub.status.idle":"2021-12-09T01:03:42.438015Z","shell.execute_reply.started":"2021-12-09T01:03:20.296646Z","shell.execute_reply":"2021-12-09T01:03:42.437015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = ['this is a bert model tutorial','we will fine-tune a bert model']\n\n\n#encode text\nsend_id=tokenizer.batch_encode_plus(text,padding=True)\n\n# output\nprint(send_id)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:03:42.440491Z","iopub.execute_input":"2021-12-09T01:03:42.440849Z","iopub.status.idle":"2021-12-09T01:03:42.451042Z","shell.execute_reply.started":"2021-12-09T01:03:42.440803Z","shell.execute_reply":"2021-12-09T01:03:42.449832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_len = [len(i.split()) for i in df_train.text]\n\npd.Series(seq_len).hist(bins=30)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:03:42.452579Z","iopub.execute_input":"2021-12-09T01:03:42.453182Z","iopub.status.idle":"2021-12-09T01:03:42.820078Z","shell.execute_reply.started":"2021-12-09T01:03:42.453119Z","shell.execute_reply":"2021-12-09T01:03:42.819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train['input_ids'] = df_train.text.apply(lambda x:tokenizer(x)['input_ids'])\n#df_train['token_type_id'] = df_train.text.apply(lambda x:tokenizer(x)['token_type_ids'])\n#df_train['attention_mask'] = df_train.text.apply(lambda x:tokenizer(x)['attention_mask'])\n\n#df_test['input_ids'] = df_test.text.apply(lambda x:tokenizer(x)['input_ids'])\n#df_test['token_type_id'] = df_test.text.apply(lambda x:tokenizer(x)['token_type_ids'])\n#df_test['attention_mask'] = df_test.text.apply(lambda x:tokenizer(x)['attention_mask'])","metadata":{"execution":{"iopub.status.busy":"2021-12-07T14:05:34.809365Z","iopub.execute_input":"2021-12-07T14:05:34.809662Z","iopub.status.idle":"2021-12-07T14:05:40.385456Z","shell.execute_reply.started":"2021-12-07T14:05:34.809627Z","shell.execute_reply":"2021-12-07T14:05:40.384677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['word_count']=df_train['text'].apply(lambda x: len(str(x).split()))\ndf_train.word_count.describe()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:03:42.82261Z","iopub.execute_input":"2021-12-09T01:03:42.823088Z","iopub.status.idle":"2021-12-09T01:03:42.859772Z","shell.execute_reply.started":"2021-12-09T01:03:42.823039Z","shell.execute_reply":"2021-12-09T01:03:42.858305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_valid,y_train,y_valid= train_test_split(df_train,df_train.target,test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:03:42.861132Z","iopub.execute_input":"2021-12-09T01:03:42.861723Z","iopub.status.idle":"2021-12-09T01:03:42.875117Z","shell.execute_reply.started":"2021-12-09T01:03:42.861685Z","shell.execute_reply":"2021-12-09T01:03:42.874066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens_train = tokenizer.batch_encode_plus(\n      X_train.text.tolist(),\n      max_length=25,\n      pad_to_max_length=True,\n      truncation=True)\n\ntokens_val = tokenizer.batch_encode_plus(\n    X_valid.text.tolist(),\n    max_length = 25,\n    pad_to_max_length=True,\n    truncation=True\n)\n\ntokens_test = tokenizer.batch_encode_plus( \n    df_test.text.tolist(),\n    max_length = 25,\n    pad_to_max_length=True,\n    truncation=True\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:57:06.760507Z","iopub.execute_input":"2021-12-09T01:57:06.760809Z","iopub.status.idle":"2021-12-09T01:57:08.121727Z","shell.execute_reply.started":"2021-12-09T01:57:06.760779Z","shell.execute_reply":"2021-12-09T01:57:08.120696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#covert lists to tensors\n\ntrain_seq=torch.tensor(tokens_train['input_ids'])\ntrain_mask =torch.tensor(tokens_train['attention_mask'])\ntrain_y= torch.tensor(y_train.tolist())\n\nval_seq=torch.tensor(tokens_val['input_ids'])\nval_mask =torch.tensor(tokens_val['attention_mask'])\nval_y= torch.tensor(y_valid.tolist())\n\ntest_seq=torch.tensor(tokens_test['input_ids'])\ntest_mask =torch.tensor(tokens_test['attention_mask'])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:57:37.417636Z","iopub.execute_input":"2021-12-09T01:57:37.417964Z","iopub.status.idle":"2021-12-09T01:57:37.49899Z","shell.execute_reply.started":"2021-12-09T01:57:37.417934Z","shell.execute_reply":"2021-12-09T01:57:37.497972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n#define a batch size\nbatch_size = 32\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_y)\n\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n\n# dataLoader for train set\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_y)\n\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n\n# dataLoader for validation set\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:57:46.056904Z","iopub.execute_input":"2021-12-09T01:57:46.057573Z","iopub.status.idle":"2021-12-09T01:57:46.06458Z","shell.execute_reply.started":"2021-12-09T01:57:46.057536Z","shell.execute_reply":"2021-12-09T01:57:46.063563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#freeze all the parameters\nfor param in bert.parameters():\n    param.requires_gred=False","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:57:47.899871Z","iopub.execute_input":"2021-12-09T01:57:47.900368Z","iopub.status.idle":"2021-12-09T01:57:47.905785Z","shell.execute_reply.started":"2021-12-09T01:57:47.900333Z","shell.execute_reply":"2021-12-09T01:57:47.904844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n      \n      super(BERT_Arch, self).__init__()\n\n      self.bert = bert \n      \n      # dropout layer\n      self.dropout = nn.Dropout(0.1)\n      \n      # relu activation function\n      self.relu =  nn.ReLU()\n\n      # dense layer 1\n      self.fc1 = nn.Linear(768,512)\n      \n      # dense layer 2 (Output layer)\n      self.fc2 = nn.Linear(512,2)\n\n      #softmax activation function\n      self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n\n      #pass the inputs to the model  \n      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n      \n      x = self.fc1(cls_hs)\n\n      x = self.relu(x)\n\n      x = self.dropout(x)\n\n      # output layer\n      x = self.fc2(x)\n      \n      # apply softmax activation\n      x = self.softmax(x)\n\n      return x","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:57:49.376547Z","iopub.execute_input":"2021-12-09T01:57:49.376851Z","iopub.status.idle":"2021-12-09T01:57:49.38976Z","shell.execute_reply.started":"2021-12-09T01:57:49.37682Z","shell.execute_reply":"2021-12-09T01:57:49.388507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERT_Arch(bert)\n\nmodel=model.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:17:02.997592Z","iopub.execute_input":"2021-12-09T01:17:02.998657Z","iopub.status.idle":"2021-12-09T01:17:08.971663Z","shell.execute_reply.started":"2021-12-09T01:17:02.998608Z","shell.execute_reply":"2021-12-09T01:17:08.970669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\n\noptimizer =AdamW(model.parameters(), lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:57:51.224627Z","iopub.execute_input":"2021-12-09T01:57:51.225083Z","iopub.status.idle":"2021-12-09T01:57:51.239592Z","shell.execute_reply.started":"2021-12-09T01:57:51.225051Z","shell.execute_reply":"2021-12-09T01:57:51.238397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n\nclass_weights=compute_class_weight('balanced',np.unique(y_train),y_train)\n\nprint(\"Class Weights:\",class_weights)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:57:52.830414Z","iopub.execute_input":"2021-12-09T01:57:52.830704Z","iopub.status.idle":"2021-12-09T01:57:52.842502Z","shell.execute_reply.started":"2021-12-09T01:57:52.830673Z","shell.execute_reply":"2021-12-09T01:57:52.841465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights=torch.tensor(class_weights,dtype=torch.float)\n\nweights = weights.to(device)\n\ncross_entropy = nn.NLLLoss(weight=weights)\n\nepochs=10","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:57:54.263478Z","iopub.execute_input":"2021-12-09T01:57:54.264334Z","iopub.status.idle":"2021-12-09T01:57:54.272178Z","shell.execute_reply.started":"2021-12-09T01:57:54.264282Z","shell.execute_reply":"2021-12-09T01:57:54.26989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to train the model\ndef train():\n  \n  model.train()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save model predictions\n  total_preds=[]\n  \n  # iterate over batches\n  for step,batch in enumerate(train_dataloader):\n    \n    # progress update after every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n    # push the batch to gpu\n    batch = [r.to(device) for r in batch]\n \n    sent_id, mask, labels = batch\n\n    # clear previously calculated gradients \n    model.zero_grad()        \n\n    # get model predictions for the current batch\n    preds = model(sent_id, mask)\n\n    # compute the loss between actual and predicted values\n    loss = cross_entropy(preds, labels)\n\n    # add on to the total loss\n    total_loss = total_loss + loss.item()\n\n    # backward pass to calculate the gradients\n    loss.backward()\n\n    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n    # update parameters\n    optimizer.step()\n\n    # model predictions are stored on GPU. So, push it to CPU\n    preds=preds.detach().cpu().numpy()\n\n    # append the model predictions\n    total_preds.append(preds)\n\n  # compute the training loss of the epoch\n  avg_loss = total_loss / len(train_dataloader)\n  \n  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  #returns the loss and predictions\n  return avg_loss, total_preds","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:57:55.878452Z","iopub.execute_input":"2021-12-09T01:57:55.878731Z","iopub.status.idle":"2021-12-09T01:57:55.890153Z","shell.execute_reply.started":"2021-12-09T01:57:55.8787Z","shell.execute_reply":"2021-12-09T01:57:55.88805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for evaluating the model\ndef evaluate():\n  \n  print(\"\\nEvaluating...\")\n  \n  # deactivate dropout layers\n  model.eval()\n\n  total_loss, total_accuracy = 0, 0\n  \n  # empty list to save the model predictions\n  total_preds = []\n\n  # iterate over batches\n  for step,batch in enumerate(val_dataloader):\n    \n    # Progress update every 50 batches.\n    if step % 50 == 0 and not step == 0:\n      \n      # Calculate elapsed time in minutes.\n      #elapsed = format_time(time.time() - t0)\n            \n      # Report progress.\n      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n    # push the batch to gpu\n    batch = [t.to(device) for t in batch]\n\n    sent_id, mask, labels = batch\n\n    # deactivate autograd\n    with torch.no_grad():\n      \n      # model predictions\n      preds = model(sent_id, mask)\n\n      # compute the validation loss between actual and predicted values\n      loss = cross_entropy(preds,labels)\n\n      total_loss = total_loss + loss.item()\n\n      preds = preds.detach().cpu().numpy()\n\n      total_preds.append(preds)\n\n  # compute the validation loss of the epoch\n  avg_loss = total_loss / len(val_dataloader) \n\n  # reshape the predictions in form of (number of samples, no. of classes)\n  total_preds  = np.concatenate(total_preds, axis=0)\n\n  return avg_loss, total_preds","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:57:57.471582Z","iopub.execute_input":"2021-12-09T01:57:57.471918Z","iopub.status.idle":"2021-12-09T01:57:57.485504Z","shell.execute_reply.started":"2021-12-09T01:57:57.471886Z","shell.execute_reply":"2021-12-09T01:57:57.484193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def format_time(t):\n#    \"\"\"Return time object (t) as a formatted string\"\"\"\n#    return '%.2d:%.2d:%.2d' % (t.hour, t.minute, t.second)\n\n#import time","metadata":{"execution":{"iopub.status.busy":"2021-12-07T15:14:01.63369Z","iopub.execute_input":"2021-12-07T15:14:01.634557Z","iopub.status.idle":"2021-12-07T15:14:01.638949Z","shell.execute_reply.started":"2021-12-07T15:14:01.634508Z","shell.execute_reply":"2021-12-07T15:14:01.638097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n#for each epoch\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:58:00.056156Z","iopub.execute_input":"2021-12-09T01:58:00.056489Z","iopub.status.idle":"2021-12-09T02:02:02.548081Z","shell.execute_reply.started":"2021-12-09T01:58:00.05641Z","shell.execute_reply":"2021-12-09T02:02:02.547186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load weights of best model\npath = 'saved_weights.pt'\nmodel.load_state_dict(torch.load(path))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:02:02.550212Z","iopub.execute_input":"2021-12-09T02:02:02.550741Z","iopub.status.idle":"2021-12-09T02:02:02.801876Z","shell.execute_reply.started":"2021-12-09T02:02:02.550692Z","shell.execute_reply":"2021-12-09T02:02:02.800753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get predictions for test data\nwith torch.no_grad():\n  preds = model(test_seq.to(device), test_mask.to(device))\n  preds = preds.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:02:02.803898Z","iopub.execute_input":"2021-12-09T02:02:02.804258Z","iopub.status.idle":"2021-12-09T02:02:04.996458Z","shell.execute_reply.started":"2021-12-09T02:02:02.804215Z","shell.execute_reply":"2021-12-09T02:02:04.995422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:03:04.362671Z","iopub.execute_input":"2021-12-09T02:03:04.363311Z","iopub.status.idle":"2021-12-09T02:03:04.379945Z","shell.execute_reply.started":"2021-12-09T02:03:04.363274Z","shell.execute_reply":"2021-12-09T02:03:04.378723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds1= [a.argmax() for a in preds] \nsample_submission =pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission['target'] = [a.argmax() for a in preds]\nsample_submission.to_csv('submission.csv',index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:10:04.225437Z","iopub.execute_input":"2021-12-09T02:10:04.225722Z","iopub.status.idle":"2021-12-09T02:10:04.260919Z","shell.execute_reply.started":"2021-12-09T02:10:04.225691Z","shell.execute_reply":"2021-12-09T02:10:04.259866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}