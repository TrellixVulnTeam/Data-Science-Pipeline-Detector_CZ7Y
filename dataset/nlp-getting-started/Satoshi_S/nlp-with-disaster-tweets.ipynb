{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **My First Notebook of NLP**\n\nReferred to NLP Getting Started Tutorial(https://www.kaggle.com/philculliton/nlp-getting-started-tutorial) and Natural Language Processing courses(https://www.kaggle.com/learn/natural-language-processing).\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"#import library\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom functools import partial\nimport optuna\nfrom xgboost import XGBRegressor\n\n\nfrom sklearn import feature_extraction,linear_model,model_selection,preprocessing\n\nfrom sklearn.metrics import mean_squared_error,roc_auc_score,precision_score,accuracy_score,log_loss\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ndf_test =pd.read_csv(\"../input/nlp-getting-started/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Explore the Data**","metadata":{}},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ratio of disaster or not.\n\nax =sns.countplot(x='target',data=df_train)\nax.set_xticklabels(['0: Not Disaster','1: Disaster'],ha=\"center\")\nplt.title(\"Disaster or Not\")\nplt.style.use(\"seaborn-whitegrid\")\ntotal= len(df_train.target)\nfor p in ax.patches:\n    percentage = f'{100 * p.get_height() / total:.1f}%\\n'\n    x = p.get_x() + p.get_width() / 2\n    y = p.get_height()\n    ax.annotate(percentage, (x, y), ha='center', va='center')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#not disaster example\ndf_train[df_train['target'] ==0].tail(100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#disaster example\ndf_train[df_train['target']==1].head(100)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-09T00:01:13.730316Z","iopub.execute_input":"2021-12-09T00:01:13.730604Z","iopub.status.idle":"2021-12-09T00:01:13.749049Z","shell.execute_reply.started":"2021-12-09T00:01:13.730575Z","shell.execute_reply":"2021-12-09T00:01:13.748195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **New Featuer Creations**\nI did not have any good ideas how to make new features from the data, but I found a really good notebook to refer for that. NLP with Disaster Tweets - EDA, Cleaning and BERT(https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert). \nI am going to use some ideas from  this notebook.","metadata":{}},{"cell_type":"code","source":"#word_count\n\ndf_train['word_count'] =df_train['text'].apply(lambda x: len(str(x).split()))\ndf_test['word_count'] =df_test['text'].apply(lambda x: len(str(x).split()))\n\n#unique_word_count\n\ndf_train['unique_word_count'] =df_train['text'].apply(lambda x:len(set(str(x).split())))\ndf_test['unique_word_count'] =df_test['text'].apply(lambda x:len(set(str(x).split())))\n\n#stop_word_count\n\nfrom nltk.corpus import stopwords\nstopwords_en=set(stopwords.words('english'))\n\ndf_train['stop_word_count'] =df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords_en]))\ndf_test['stop_word_count'] =df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords_en]))\n\n\n#mean_word_length\n\ndf_train['mean_word_length'] = df_train['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\ndf_test['mean_word_length'] = df_test['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\n\n#char_count\ndf_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\ndf_test['char_count'] =df_test['text'].apply(lambda x: len(str(x)))\n\n#punctuation_count\nfrom string import punctuation\n\npunctuations = set(punctuation)\n\ndf_train['punctuation_count'] = df_train['text'].apply(lambda x:len([c for c in str(x) if c in punctuations]))\ndf_test['punctuation_count'] = df_test['text'].apply(lambda x:len([c for c in str(x) if c in punctuations]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_features=['word_count','unique_word_count','stop_word_count','mean_word_length','char_count','punctuation_count']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nscaler= preprocessing.MinMaxScaler()\ndf_train[new_features] = scaler.fit_transform(df_train[new_features])\ndf_test[new_features] = scaler.transform(df_test[new_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#keywords\nprint(\"keywords:\" + str(len(df_train.keyword.unique())))\nprint(\"Total tweets:\" + str(len(df_train)) +\"\\n\")\n      \nprint(str(df_train.keyword.unique()))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#location\nprint(\"keywords:\" + str(len(df_train.location.unique())))\nprint(\"Total tweets:\" + str(len(df_train)) +\"\\n\")\n\nprint(str(df_train.location.unique()))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using keywords to make new feature**\n\nToo many unique values in the location column to use them as features compared with keywords.\nLet's see which keywords are useful to predict if a predict is a disaster or not.","metadata":{}},{"cell_type":"code","source":"df_train_withkeywords = df_train.dropna(subset=['keyword'],axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_withkeywords.head(5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_withkeywords['target_mean'] = df_train_withkeywords.groupby('keyword')['target'].transform('mean')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_withkeywords","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This code is from Referred to Basic NLP with NLTK(https://www.kaggle.com/alvations/basic-nlp-with-nltk/notebook)\n# as mentioned below.\n\n\nfig =plt.figure(figsize=(12,72),dpi=100)\n\nsns.countplot(y=df_train_withkeywords.sort_values(by='target_mean',ascending=False)['keyword'],\n             hue=df_train_withkeywords.sort_values(by='target_mean',ascending=False)['target'])\n\nplt.tick_params(axis='x',labelsize=15)\nplt.tick_params(axis='y',labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g= df_train_withkeywords.groupby(by='keyword')['target_mean'].apply(lambda x:list(np.unique(x)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_keywords=[]\nnon_disaster_keywords=[]\n\nfor word in df_train_withkeywords.keyword.unique():\n    if g[word][0] >0.7:\n        disaster_keywords.append(word)\n    if g[word][0] <0.3:\n        non_disaster_keywords.append(word)\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Informative keywords list\n\n\ndef disaster(keyword):\n    if keyword in disaster_keywords:\n        return 1\n    return 0\ndef nondisaster(keyword):\n    if keyword in non_disaster_keywords:\n        return 1\n    return 0\n    \ndf_train['is_disaster_keyword'] =df_train['keyword'].apply(disaster)\ndf_train['is_non_disaster_keyword'] =df_train['keyword'].apply(nondisaster)\n\ndf_test['is_disaster_keyword'] =df_test['keyword'].apply(disaster)\ndf_test['is_non_disaster_keyword'] =df_test['keyword'].apply(nondisaster)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Text Cleaning**\n\nReferred to Basic NLP with NLTK(https://www.kaggle.com/alvations/basic-nlp-with-nltk/notebook).","metadata":{}},{"cell_type":"code","source":"#example\n\n#from nltk import sent_tokenize, word_tokenize\n#text = df_train.text[0]\n#for sent in sent_tokenize(text):\n#    print([word.lower() for word in word_tokenize(sent)])","metadata":{"execution":{"iopub.status.busy":"2021-12-05T03:18:37.673822Z","iopub.execute_input":"2021-12-05T03:18:37.674088Z","iopub.status.idle":"2021-12-05T03:18:37.680044Z","shell.execute_reply.started":"2021-12-05T03:18:37.674057Z","shell.execute_reply":"2021-12-05T03:18:37.679224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#text0_list = list(map(str.lower, word_tokenize(text)))\n\n#remove stopwords(non-content words)\n\n#from nltk.corpus import stopwords\n#stopwords_en=set(stopwords.words('english'))\n#remove punctuations\n\n#from string import punctuation\n#stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n\n\n#text0_list1=[word for word in text0_list if word not in stopwords_en_withpunct]\n#print(text0_list1)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T03:13:17.648825Z","iopub.execute_input":"2021-12-05T03:13:17.649077Z","iopub.status.idle":"2021-12-05T03:13:17.656113Z","shell.execute_reply.started":"2021-12-05T03:13:17.64905Z","shell.execute_reply":"2021-12-05T03:13:17.655439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from nltk.stem import PorterStemmer\n#porter=PorterStemmer()\n#for word in text0_list1:\n#    print(porter.stem(word))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T03:13:06.183728Z","iopub.execute_input":"2021-12-05T03:13:06.183979Z","iopub.status.idle":"2021-12-05T03:13:06.190674Z","shell.execute_reply.started":"2021-12-05T03:13:06.183951Z","shell.execute_reply":"2021-12-05T03:13:06.189351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def preprocessing(text):\n#    li1 = [word.lower() for word in text.split()]\n#    li2 =  [word for word in li1 if word not in stopwords_en_withpunct]\n#    li3= [porter.stem(word) for word in li2]\n#    return ' '.join(li3)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T03:45:44.609258Z","iopub.execute_input":"2021-12-05T03:45:44.609847Z","iopub.status.idle":"2021-12-05T03:45:44.615312Z","shell.execute_reply.started":"2021-12-05T03:45:44.609809Z","shell.execute_reply":"2021-12-05T03:45:44.614545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train.text[2]","metadata":{"execution":{"iopub.status.busy":"2021-12-05T03:39:01.063845Z","iopub.execute_input":"2021-12-05T03:39:01.0641Z","iopub.status.idle":"2021-12-05T03:39:01.071979Z","shell.execute_reply.started":"2021-12-05T03:39:01.064072Z","shell.execute_reply":"2021-12-05T03:39:01.071288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preprocessing(df_train.text[2])","metadata":{"execution":{"iopub.status.busy":"2021-12-05T03:45:52.021791Z","iopub.execute_input":"2021-12-05T03:45:52.022044Z","iopub.status.idle":"2021-12-05T03:45:52.027771Z","shell.execute_reply.started":"2021-12-05T03:45:52.022017Z","shell.execute_reply":"2021-12-05T03:45:52.027052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train['cleaned_text'] = df_train['text'].apply(preprocessing)\n#df_test['cleaned_text'] =df_test['text'].apply(preprocessing)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T03:45:56.002378Z","iopub.execute_input":"2021-12-05T03:45:56.002891Z","iopub.status.idle":"2021-12-05T03:45:59.100878Z","shell.execute_reply.started":"2021-12-05T03:45:56.002857Z","shell.execute_reply":"2021-12-05T03:45:59.100155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train.head(100)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T03:46:01.180623Z","iopub.execute_input":"2021-12-05T03:46:01.181416Z","iopub.status.idle":"2021-12-05T03:46:01.199739Z","shell.execute_reply.started":"2021-12-05T03:46:01.181372Z","shell.execute_reply":"2021-12-05T03:46:01.199019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Building Vectors**\n\nUse sckit-learn's countvectorizer to count the words in each tweet and turn them into data that machine learning model can process.","metadata":{}},{"cell_type":"code","source":"count_vectorizer = feature_extraction.text.CountVectorizer()\n\nexample_train_vectors = count_vectorizer.fit_transform(df_train['text'][0:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(example_train_vectors[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T00:01:53.39431Z","iopub.execute_input":"2021-12-09T00:01:53.394946Z","iopub.status.idle":"2021-12-09T00:01:53.401564Z","shell.execute_reply.started":"2021-12-09T00:01:53.394905Z","shell.execute_reply":"2021-12-09T00:01:53.400833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_vectors = count_vectorizer.fit_transform(df_train[\"text\"])\ntest_vectors = count_vectorizer.transform(df_test['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#making dataframe from the vector\n\nvect_train_df= pd.DataFrame(train_vectors.todense(),columns=count_vectorizer.get_feature_names())\nvect_test_df =pd.DataFrame(test_vectors.todense(),columns=count_vectorizer.get_feature_names())\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vect_train_df['is_disaster_keyword'] = df_train['is_disaster_keyword']\nvect_train_df['is_nondisaster_keyword'] =df_train['is_non_disaster_keyword']\nvect_test_df['is_disaster_keyword'] = df_test['is_disaster_keyword']\nvect_test_df['is_nondisaster_keyword'] =df_test['is_non_disaster_keyword']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vect_train_df=vect_train_df.join(df_train[new_features])\nvect_test_df=vect_test_df.join(df_train[new_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train,y_test = train_test_split(vect_train_df,df_train.target,test_size=0.3,random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Create Model**","metadata":{}},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=10, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\nmodel = keras.Sequential([\n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.2),\n   # keras.layers.Dense(300, activation='relu'),\n   # keras.layers.Dropout(0.5),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dropout(0.6),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.4),\n    keras.layers.Dense(2, activation=\"softmax\")\n])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam',loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.asarray(X_train)\ny_train = np.asarray(y_train)\nX_test = np.asarray(X_test)\ny_test = np.asarray(y_test)\n\nvect_test=np.asarray(vect_test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\nX_train,y_train,\nvalidation_data =(X_test,y_test),\nbatch_size=512,\nepochs =1000,\ncallbacks=[early_stopping],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(model.predict(X_test))\nlen(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred = [a.argmax() for a in y_pred]\n\n\nprint(mean_squared_error(y_test,y_pred,squared=False))\nprint(roc_auc_score(y_test,y_pred))\n\n\n#0.4333917065178553\n#0.7976782008772676\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vect_test= [a.argmax() for a in model.predict(vect_test)]\nsample_submission[\"target\"] =vect_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}