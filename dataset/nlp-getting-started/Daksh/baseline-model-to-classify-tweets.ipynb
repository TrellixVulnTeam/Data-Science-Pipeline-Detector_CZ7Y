{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro to nlp\nNatural language processing is field of machine learning in which we focus on language interaction of computer with that of human.it is subfield of linguistic,computer science,information engineering concerned with interation between computer and human language..Their are so many application of nlp.For example when you search on google and by mistake you type wrong spelling it automatically correct it and get you the most relevant results.Many chatting apps came with option of smart reply.In short,we encountered nlp algorithm in our day to day life.","metadata":{}},{"cell_type":"markdown","source":"# What this  competition is all about?\nThis competition is ideal for anyone who is getting started with nlp.This competiotion is all about making and evaluating a machine learning model that can predict whether a tweet is about disaster or not.\nIn this competion there were three datasets ,train,test and submission files.We will use train datset to train our machine learning algorithm.\nTrain contains following  columns:-\n* text-The text of tweet\n* keyword-tweet containg keyword\n* Location-The location from where tweet was sent\n* target-This is the columns which say whether a tweet if about disaster or not\n* id-It just contains a uniques number for each row\nDropping the target columns test  data contains same above columns.Submission file is give just to take a look at the format in which submmission is required.\n\n**Evaluation Metrics**\n\nEvery machine learning model is evaluated based on some some metrics.The evaluation metrics that is used in this competition is \"F1 scoring\".F1 is the reciprocal of precision and recall .A good f1 score means you have low false positive and low false negative.Keeping it simple  a good model has high f1 score close to 1 while a total failure of model to predict the true positive  brings the score to zero.\n","metadata":{}},{"cell_type":"markdown","source":"# Contents\n* [Importing libraries](#imports)\n* [Reading Dataset ](#reading)\n* [Basic EDA](#eda)\n* [Text processing](#processing)\n* [Vectorization](#vectorization)\n* [Classification Model](#models)","metadata":{}},{"cell_type":"markdown","source":"# Importing libraries  <a name=\"imports\" a > ","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer \nimport pandas as pd\nimport numpy as np\nimport spacy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import cross_val_score\nimport re\nfrom sklearn .metrics import classification_report\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport string\nfrom nltk.tokenize import WhitespaceTokenizer,word_tokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"reading\"></a> Reading Dataset","metadata":{}},{"cell_type":"code","source":"#reading train and test data\ntrain=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"training_data_shape:{train.shape}\")\nprint(f\"testing_data_shape:{test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"eda\"></a>Basic EDA","metadata":{}},{"cell_type":"markdown","source":"Take a look at the top ten rows","metadata":{}},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing Values\n","metadata":{}},{"cell_type":"code","source":"#function to get percentage of Nan in a Column\ndef get_nan(data):\n    s=data.isna().sum()\n    per=s/data.shape[0]\n    return per*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_nan=get_nan(train)\ntest_nan=get_nan(test)\nnan_data=pd.DataFrame({\"train\":train_nan,\"test\":test_nan})\nnan_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So here we can see that  80 percent of keyword columns  in training data are Nan and 79 percent in test data. The test data doesn't contain target column so it shows Nan above.We will feed our model with only text columns so there's no need to  deal with Nan in other columns so i will leave them as it is.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Know your target!! ","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='target',data=train,palette='Blues')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Target columns contains two classes**\n* 1 - if the tweet is about real disaster\n* 0 - if the tweet is not about disaster ","metadata":{}},{"cell_type":"markdown","source":"## Keywords!!","metadata":{}},{"cell_type":"code","source":"sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above figure shows the top 20 keywords based on frequency of occurence.We can see that most of keywords are around 40. ","metadata":{}},{"cell_type":"markdown","source":"## Location Column","metadata":{}},{"cell_type":"code","source":"train['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\n\nsns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],\n            orient='h')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Most of the tweets are from USA followed by Uk and so on.","metadata":{}},{"cell_type":"markdown","source":"# <a name=\"processing\"></a>Text Processing\nGetting data in required forma is  so that it can be feed into ml model is the most time taking and an important part of any data scince project.In nlp,their are several general steps to be followed for data cleaning.\n* In this project first we will make all the words lowercase so that no two same words treated differently just beacause one in lowercase and then other in lowercase. \n\n* After that,i will remove the punctuation,unwanted strings like \"yibf6,uds8mdh\" which isn't a standard word .\n\n* Just cleaning the data is not enough for any nlp pipeline,now we will move on to our next step to tokenize the data.\n\n**Tokenization**:Tokens are the building block for ml model.Tokenization is the process of breaking the text into  smaller pieces  of strings called tokens.It can be of three types word,subword and subword.We will use word tokenization in our project.For example when we want to tokenize the sentence \"He is running.\" considering space as delimiter we can break the sentence and form a token of words [He,is,running].\nFor more info <a href=\"https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/\" >visit</a>\n\n* Now after cleaning the text and tokenizing it's time to remove stopwords from the data.Since they are frequently used in a sentence and didn't convey much information we should remove them.","metadata":{}},{"cell_type":"code","source":"#function to clean text\ndef clean_text(text):\n    text=text.lower()\n    text = re.sub('\\[.*?\\]', \"\", text)\n    text = re.sub('https?://\\S+|www\\.\\S+', \"\", text)\n    text=re.sub('<.*?>+','',text)\n    text=re.sub(f\"[{string.punctuation}]\",'',text)\n    text=re.sub('\\n','',text)\n    text=re.sub('\\w\\d\\w','',text)\n    return text\n   \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#applying the cleaning process on both train and test\ntrain['text']=train['text'].apply(lambda x:clean_text(x))\ntest['text']=train['text'].apply(lambda x:clean_text(x))\ndisaster_tweet=train[train['target']==1]['text']\nnon_disaster_tweet=train[train['target']==0]['text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=[28,7])\nwordcloud1=WordCloud(background_color='white',height=100,width=200).generate(\" \".join(disaster_tweet))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=32)\nwordcloud2=WordCloud(background_color='white',height=100,width=200).generate(\" \".join(non_disaster_tweet))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non-Disaster Tweets',fontsize=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to remove stopwords\ndef stop_words_removal(text):\n    text=[w for w in  text.split(\" \")  if w not in stopwords.words('english')]\n    return \" \".join(text)\n#applying the function on train and test\ntrain['text']=train['text'].apply(lambda x: stop_words_removal(x))\ntest['text']=test['text'].apply(lambda x:stop_words_removal(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#looking at top three rows after cleaning\ntrain['text'][0:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"vectorization\"></a>Vectorization\nComputers didn't make sense of word as we humans do.They are good at crunching numbers.So,in a nlp pipeline we must convert  the words into numbers.But how do we can do it? One very general approach  is we will map words from a vocablury to vectors of real numbers.Here,vocablury means set of unique words included in text corpus.For example let's consider two sentence \"kaggle is best.\" and \"kaggle is good.\" .Now the vocablury consist of [kaggle,is,best,good].In vectors form when we write first sentence we can write it as [1,1,0,1] and similarly 2nd one as [1,1,1,0].Thsi is just tip of ice-berg we can do much more with word-vectors .To learn more here is  a nice <a href=\"https://medium.com/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf\">blog</a>  ","metadata":{}},{"cell_type":"markdown","source":"## Tf-idf\nTf-idf is method to penalize most occuring word in word vectors.This method rescale the frequency of words of how often it occur in a document.\n* Term frequency\n Term frequency :It is scoring of frequency of words\n In short it refers to \n(Number of times word w occur in document)/(Total number of terms in document)\n* Inverse Document Frequency\nInverse Document frequency :It measures the rarity of a word.\n(1+log(N/n)) where N is total document and n is number of document in which word w occur\n\nTf-idf=term_frequency*inverse_document_frequency\n","metadata":{}},{"cell_type":"code","source":"tf_idf=TfidfVectorizer(max_features=1000)\ntrain_X=tf_idf.fit_transform(train['text'])\ntest_X=tf_idf.transform(test['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"models\"></a>Model Building\nWith everything set,it's time to make a working model for us to predict the class of tweets.","metadata":{}},{"cell_type":"markdown","source":"\n## Logistic Regression ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\ntrain_Y=train['target']\nlog_reg=LogisticRegression(C=2.02)\nscores=cross_val_score(log_reg,train_X,train_Y,cv=5,scoring='f1')\nprint(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We get a decent f1 score with logistic regression","metadata":{}},{"cell_type":"code","source":"log_reg.fit(train_X,train_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## let's try Naive Bayes ","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nBayes=MultinomialNB(alpha=1.5)\nscores_bayes=cross_val_score(Bayes,train_X,train_Y,cv=5,scoring='f1')\nprint(scores_bayes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Bayes.fit(train_X,train_Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XgBoost ","metadata":{}},{"cell_type":"code","source":"from   xgboost import XGBClassifier as xgb\nxg=xgb(n_estimators=250,max_depth=12,colsample_bytree=0.1,learning_rate=0.2,subsample=0.4)\nxg_scores=cross_val_score(xg,train_X,train_Y,cv=5,scoring='f1')\nprint(xg_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In above three models naive bayes performs best we make our submission with it.","metadata":{}},{"cell_type":"markdown","source":"## Time to submit!! ","metadata":{}},{"cell_type":"code","source":"def submit_model(path,model,vectors):\n    sample_submission=pd.read_csv(path)\n    sample_submission['target']=model.predict(vectors)\n    sample_submission.to_csv('submission.csv',index=False)\nsubmission_file_path = \"../input/nlp-getting-started/sample_submission.csv\"\ntest_vectors=test_X\nsubmit_model(submission_file_path,Bayes,test_vectors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Thoughts \nSo We now have a model that can predict whether a tweet is disasterous or not.If you find this kernel please upvote it.I take most of the help from this <a href=\"https://www.kaggle.com/parulpandey/getting-started-with-nlp-feature-vectors\">kernel</a>.","metadata":{}}]}