{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\n\nimport logging\nimport nltk\nimport string\nimport collections\nfrom collections import Counter\nimport wordcloud\nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth',200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s :: %(name)s :: %(levelname)s :: %(message)s',datefmt='%d-%b-%y %H:%M:%S')\nlogger = logging.getLogger(__name__)\nlogger.info('Logger initialised...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_df['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger.info(\"% of samples where keyword column is 0\")\nlen(train_df[train_df['keyword'].isna()])*100/len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger.info(\"% of samples where location column is 0\")\nlen(train_df[train_df['location'].isna()])*100/len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train_df['keyword'].value_counts()[:20].values,train_df['keyword'].value_counts()[:20].index,orient=\"H\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Keyword chart when target is 1\nsns.barplot(train_df[train_df['target']==1]['keyword'].value_counts()[:20].values,train_df[train_df['target']==1]['keyword'].value_counts()[:20].index,orient=\"H\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train_df['location'].value_counts()[:20].values,\\\n            train_df['location'].value_counts()[:20].index,orient=\"H\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Location chart when target is 1\nsns.barplot(train_df[train_df['target']==1]['location'].value_counts()[:20].values,\\\n            train_df[train_df['target']==1]['location'].value_counts()[:20].index,orient=\"H\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### Cleaning text\n* Converting Text Lowercase\n* Tokenization\n* Removing Punctuatons\n* Stop Words removal\n* Stemmning\n* Lemmatization\n* POS Tagging"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Make text lowercase"},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_lowercases(x):\n    return x.lower()\n\ntrain_df['text']=train_df['text'].apply(to_lowercases)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Clean text"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(x):\n    text = re.sub('(\\d+)','',x)    \n    return text\n\ntrain_df['text'] = train_df['text'].apply(clean_text)\n\ntrain_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove URL"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_url(x):\n    text = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})\\/([a-zA-Z0-9_]+]*)',' ',x)\n    return text\n\ntrain_df['text'] = train_df['text'].apply(remove_url)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove Punctuations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(x):\n    text_without_puct = [t for t in x if t not in string.punctuation]\n    text_without_puct = ''.join(text_without_puct)\n    return text_without_puct\n\ntrain_df['text'] = train_df['text'].apply(remove_punct)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tokens(x):\n    tokens = nltk.word_tokenize(x)\n    return tokens\n\ntrain_df['text'] = train_df['text'].apply(get_tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove Stop Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = nltk.corpus.stopwords.words('english')\n\ndef remove_stop_words(x):\n    text_without_stopwords = [t for t in x if t not in stop_words]\n    \n    return text_without_stopwords\n\ntrain_df['text'] = train_df['text'].apply(remove_stop_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemma = WordNetLemmatizer()\n\ndef lemmatization(x):\n    try:\n        lemmatized = np.vectorize(lemma.lemmatize)(x)\n        return lemmatized\n    except ValueError:\n        return []\n    \ntrain_df['text_lemm'] = train_df['text'].apply(lemmatization)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import FreqDist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fdist = FreqDist()\ndef freq_dist(x):\n    for word in x:\n        fdist[word]+=1\n    \n    return fdist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text_lemm'].apply(freq_dist)[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fdist = FreqDist()\ndef freq_dist(x):\n    for word in x:\n        fdist[word]+=1\n    \n    return fdist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common = Counter(train_df['text_lemm'].apply(freq_dist)[1]).most_common(50)\nl=[]\nfor k,v in most_common:\n    l.append(k.replace(\"\\'\",''))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(background_color='white',\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1).generate(str(l))\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bigrams and Trigrams"},{"metadata":{"trusted":true},"cell_type":"code","source":"fdist = nltk.FreqDist()\ndef bigrams(x):\n    y = list(nltk.bigrams(x))\n    for word in y:\n        fdist[word]+= 1\n    \n    return fdist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigrams = train_df['text_lemm'].apply(bigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(fdist).most_common(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fdist = nltk.FreqDist()\ndef trigrams(x):\n    y = list(nltk.trigrams(x))\n    for word in y:\n        fdist[word]+= 1\n    \n    return fdist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trigrams = train_df['text_lemm'].apply(trigrams)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(fdist).most_common(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bag of Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"l = []\nfor i in range(len(train_df)):\n    l.append(' '.join(train_df.loc[i,'text_lemm']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncountvect = CountVectorizer()\n\ncountvect_text = countvect.fit_transform(l)\n\ncountvect_text.get_shape()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### TFIDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntf = TfidfVectorizer()\ntf_text = tf.fit_transform(l)\ntf_text.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Glove Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"f = open('../input/glove-100d-word-embeddings/glove.6B.100d.txt')\nembeddings = {}\nfor line in f:\n    word = line.split(' ')\n    embeddings[word[0]] = np.asarray(word[1:])\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train_df.target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical,plot_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=10000)\n\ntokenizer.fit_on_texts(l)\n\nsequences = tokenizer.texts_to_sequences(l)\n\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique Tokens %s\"%len(tokenizer.word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pad_sequences(list(sequences),maxlen=20,truncating='post',padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\n\ndata = data[indices]\nlabels_y = labels[indices]\n\nnb_validation_sample = int(.15*data.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = data[:-nb_validation_sample]\ny_train = labels_y[:-nb_validation_sample].reshape(-1,1)\nx_test = data[-nb_validation_sample:]\ny_test = labels_y[-nb_validation_sample:].reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for word,i in word_index.items():\n    try:\n        vector = embeddings[word]\n        if vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = vector\n    except KeyError:\n        continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import backend\n\nfrom tensorflow.keras.layers import Input,Dense,Activation,Embedding,Flatten,LSTM,Dropout,SpatialDropout1D\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(x_train.shape[1],))\nemb = Embedding(len(embedding_matrix),\n                100,\n                weights=[embedding_matrix],\n                trainable=False,\n               input_length=20)(inp)\n\n#out = Flatten()(emb)\nout = SpatialDropout1D(rate=0.2)(emb)\nout = LSTM(100)(out)\nout = Dropout(rate=0.2)(out)\n#out = Dense(20,activation='relu')(out)\nout = Dense(1,activation='sigmoid')(out)\nadam = Adam(learning_rate=.001)\n\nmodel = Model(inputs=[inp],outputs=[out])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train,y_train,verbose=1,\n          batch_size=4,\n          epochs=10,\n          validation_data=[x_test,y_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}