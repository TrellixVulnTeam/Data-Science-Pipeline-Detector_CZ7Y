{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Updating pip and installing keras-tuner package\n!/opt/conda/bin/python3.7 -m pip install --upgrade pip\n!pip install -U keras-tuner","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom kerastuner import HyperModel, Objective\nimport tensorflow as tf\nfrom kerastuner.tuners import RandomSearch\nimport keras.backend as K\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Initializing the embedding dimention\nbatch_size = 64\nembedding_dim = 512\n\n# Read the input data.\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n# Fill all the na data with empty character.\ntrain_df = train_df.fillna('empty')\ntest_df = test_df.fillna('empty')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join with the text, keyword as well as the location\ntrain_df['text'] = train_df['text'] + ' ' + train_df['keyword'].astype(str) + ' ' + train_df['location'].astype(str)\ntest_df['text'] = test_df['text'] + ' ' + test_df['keyword'].astype(str) + ' ' + test_df['location'].astype(str)\n\n# Strip off the whitespace from the front and back of the sentence\ntrain_df['text'] = train_df['text'].str.strip()\ntest_df['text'] = test_df['text'].str.strip()\n\n# Replace all the links, with just link as word.\n# It matters more to know if there is a link or not in place of which link it is actually.\n# But this remains to be seen later.\ntrain_df['text'] = train_df['text'].str.replace(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', 'link')\ntest_df['text'] = test_df['text'].str.replace(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', 'link')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To see the output of the pre-processing steps.\ntrain_df.to_csv('train.csv', index=False)\n\n# Drop the kexword and location column as it is not with the text column\ntrain_df.drop(columns=['keyword', 'location'])\n\n# Use the tokenizer and remove all the special characters. Add oov_character as irrelevant\ntokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"'<irrelevant>'\",\n                                                  filters='!\"$%&()*+.,-/:;=?@[\\]^_`{|}~# ')\n\n# Fit the tokenizer on the trainig data.\ntokenizer.fit_on_texts(train_df.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the texts to tokens\ntrain_df['text_tokenized'] = tokenizer.texts_to_sequences(train_df.text)\ntest_df['text_tokenized'] = tokenizer.texts_to_sequences(test_df.text)\n\n# Pad the sequence as we will like to have sentences of equal length.\n# We can also use bucket with sequence length.\nnp_matrix_train = tf.keras.preprocessing.sequence.pad_sequences(train_df['text_tokenized'])\nnp_matrix_train = np.append(np_matrix_train, np.expand_dims(train_df['target'], axis=-1), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the data to x and y, Later batch the dataset.\ntrain_dataset = tf.data.Dataset.from_tensor_slices(np_matrix_train)\ntrain_dataset_all = train_dataset.map(lambda x: (x[:-1], x[-1])).batch(batch_size)\n\n\n\n# Do the similar to the test dataset.\nnp_matrix_test = tf.keras.preprocessing.sequence.pad_sequences(test_df['text_tokenized'])\ntest_dataset = tf.data.Dataset.from_tensor_slices(np_matrix_test).batch(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_test(x, y):\n    return x % 5 == 0\n\ndef is_train(x, y):\n    return not is_test(x, y)\n\nrecover = lambda x,y: y\n\nvalidation_dataset = train_dataset_all.enumerate() \\\n                    .filter(is_test) \\\n                    .map(recover)\n\ntrain_dataset = train_dataset_all.enumerate() \\\n                    .filter(is_train) \\\n                    .map(recover)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf  ./real_or_not*","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the keras tuner model.\nclass MyHyperModel(HyperModel):\n    \n    def build(self, hp):\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, embedding_dim))\n        for i in range(hp.Int('num_layers', 1, 3)):\n            model.add(tf.keras.layers.Conv1D(filters=hp.Choice('num_filters', values=[32, 64], default=64),activation='relu',\n                                             kernel_size=3,\n                                             bias_initializer='glorot_uniform'))\n            model.add(tf.keras.layers.MaxPool1D())\n        \n        model.add(tf.keras.layers.GlobalMaxPool1D())\n        \n        for i in range(hp.Int('num_layers_rnn', 1, 3)):\n            model.add(tf.keras.layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'))\n            model.add(tf.keras.layers.Dropout(0.2))\n        \n        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n        \n        model.compile(\n            optimizer=hp.Choice('optimizer', values= ['Adam', 'Adadelta', 'Adamax']),\n            loss='binary_crossentropy',\n            metrics=[f1])\n        return model\n\n\nhypermodel = MyHyperModel()\n\ntuner = RandomSearch(\n    hypermodel,\n    objective=Objective('val_f1', direction=\"max\"),\n    max_trials=15,\n    directory='./',\n    project_name='real_or_not')\n\ntuner.search(train_dataset,\n             epochs=10, validation_data=validation_dataset)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner.results_summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = tuner.get_best_models(num_models=1)\n\nmodels[0].summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[0].predict(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_dataframe = pd.DataFrame(columns=['id', 'target'])\nresult_dataframe['id'] = test_df['id']\nresult_dataframe['target'] = np.where(np.array(models[0].predict(test_dataset)) >= 0.5, 1, 0 )\nresult_dataframe.to_csv('result.csv', index= False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}