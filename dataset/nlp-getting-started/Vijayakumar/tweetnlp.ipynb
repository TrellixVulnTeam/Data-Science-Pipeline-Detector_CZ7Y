{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Real Tweet or Not\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# File system manangement\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Dataset**\nThe dataset are taken from kaggle competition"},{"metadata":{},"cell_type":"markdown","source":"# **Train Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"root_path = \"../input/nlp-getting-started\"\ntrain_path = os.path.join(root_path,\"train.csv\")\ntrain = pd.read_csv(train_path)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Test Data**"},{"metadata":{"id":"5AUe7BNSGPcY","colab_type":"code","outputId":"17323c3a-8499-42a8-e365-ff6daf198298","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"test_path = os.path.join(root_path,\"test.csv\")\ntest = pd.read_csv(test_path)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"D_33SxlXJIW5","colab_type":"code","outputId":"1b26423e-eb7a-4537-f2ce-ebf33fe128a3","colab":{"base_uri":"https://localhost:8080/","height":68},"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{"id":"ZpMSBmVFJ8YF","colab_type":"code","outputId":"f3d0906c-fc94-450c-9e31-ab426c9e107c","colab":{"base_uri":"https://localhost:8080/","height":296},"trusted":true},"cell_type":"code","source":"sns.barplot(x=(train['target']==1).value_counts(),y=train['target'].value_counts(),palette=\"magma\",data=train)","execution_count":null,"outputs":[]},{"metadata":{"id":"L1p0H6CNMfGv","colab_type":"code","outputId":"900402fa-230d-4243-93e8-ce1af4d7efb5","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"train['text'][:10]","execution_count":null,"outputs":[]},{"metadata":{"id":"FqWroCoiM8dL","colab_type":"code","outputId":"359604c3-d695-4dbb-a6dd-fe7f77ada88f","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"# A disaster tweet\ndisaster_tweets = train[train['target']==1]['text']\ndisaster_tweets.values[1]","execution_count":null,"outputs":[]},{"metadata":{"id":"wJCZDZy8NA7s","colab_type":"code","outputId":"1e0d0deb-6c88-42d8-92ad-1ee105fc4f73","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"#not a disaster tweet\nnon_disaster_tweets = train[train['target']==0]['text']\nnon_disaster_tweets.values[1]","execution_count":null,"outputs":[]},{"metadata":{"id":"SEuAU3aJMs1F","colab_type":"code","outputId":"a11ed12b-c03b-4c77-91b9-24a54f857e0f","colab":{"base_uri":"https://localhost:8080/","height":409},"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 8])\nwordcloud1 = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\nBefore getting our hands in the model we need to clean the data\nSome of the preprocessing methods need to be followed in case of text are given below\n1. Removal of punctuation\n2. Tokenization\n3. Removal of Stopwors\n4. Stemming and Lemmatization\n"},{"metadata":{},"cell_type":"markdown","source":"# Removal of Punctuatuation"},{"metadata":{"id":"ewxcb8oZMhNN","colab_type":"code","outputId":"7db302d8-f427-4e6f-eb70-3d9835d1567b","colab":{"base_uri":"https://localhost:8080/","height":119},"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenization"},{"metadata":{"id":"iMpqwvTPNjE1","colab_type":"code","outputId":"9efb052e-fd25-4bdc-a7cf-b9e0d01e7c04","colab":{"base_uri":"https://localhost:8080/","height":119},"trusted":true},"cell_type":"code","source":"text = \"Are you coming , aren't you\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \",text)\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","execution_count":null,"outputs":[]},{"metadata":{"id":"0wx5ZVaWNj6E","colab_type":"code","outputId":"346960cb-0584-4213-c4d9-dd34b8370f37","colab":{"base_uri":"https://localhost:8080/","height":119},"trusted":true},"cell_type":"code","source":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removal of Stopwords\nFor that we download the stopword from nltk"},{"metadata":{"id":"Ht-jeFT3OL5T","colab_type":"code","outputId":"3eedc614-cb4e-43f4-85aa-1793d918edfc","colab":{"base_uri":"https://localhost:8080/","height":68},"trusted":true},"cell_type":"code","source":"#Needed While you run in COLAB\nnltk.download('stopwords')","execution_count":null,"outputs":[]},{"metadata":{"id":"PqeSlDkBOBhV","colab_type":"code","outputId":"91fcc689-2aa2-474c-c492-4933babcef8f","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stemming and Lemmatization\nFor this process we download the wordnet data from nltk"},{"metadata":{"id":"HqMYTaOVOylT","colab_type":"code","outputId":"20162b0e-a7b4-43b6-bfca-2612cb553a01","colab":{"base_uri":"https://localhost:8080/","height":68},"trusted":true},"cell_type":"code","source":"#Needed While you run in COLAB\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"id":"tZU7UwA8Om-N","colab_type":"code","outputId":"6535a5d3-c402-46bd-bc12-e0fb9151c79e","colab":{"base_uri":"https://localhost:8080/","height":51},"trusted":true},"cell_type":"code","source":"# Stemming and Lemmatization examples\ntext = \"feet cats wolves talked\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer\nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer=nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","execution_count":null,"outputs":[]},{"metadata":{"id":"ILW1A_dyOnvk","colab_type":"code","outputId":"fcf7c3d5-63ca-4676-94f6-d4bd6f73fe52","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"def combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generalization\nThe above steps of data cleaning are combined and made as single function"},{"metadata":{"id":"h0JUoc57OvO8","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bag of Words - Countvectorizer Features"},{"metadata":{"id":"ys9XRVRXO8uz","colab_type":"code","outputId":"815aaa5f-ee2a-424b-8d53-62f1e1aa5228","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[0].todense())","execution_count":null,"outputs":[]},{"metadata":{"id":"bT92mQ0_O_xb","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple Naive Bayes on TFIDF"},{"metadata":{"id":"jMp3KXRdRBBL","colab_type":"code","outputId":"7c1eca3e-e9c6-4820-fa7e-21c3d2423f67","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"clf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"id":"yoDHlZt2RVT6","colab_type":"code","outputId":"cff41cf0-f1a6-4e8d-b9e9-4adc67a4cea3","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Submission CSV"},{"metadata":{"id":"vLv7FALUQCo7","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"vL_jjRwtQFor","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"submission_file_path = os.path.join(root_path,\"sample_submission.csv\")\ntest_vectors=test_tfidf\nsubmission(submission_file_path,clf_NB_TFIDF,test_vectors)","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Real Tweet or Not.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":4}