{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import defaultdict\nimport string\nimport tensorflow as tf\nimport re\nfrom tensorflow import keras\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport transformers\nfrom transformers import AutoTokenizer, TFAutoModel","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:15:39.657172Z","iopub.execute_input":"2021-06-08T08:15:39.657521Z","iopub.status.idle":"2021-06-08T08:15:47.355478Z","shell.execute_reply.started":"2021-06-08T08:15:39.657444Z","shell.execute_reply":"2021-06-08T08:15:47.354542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the Dataset ","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv('../input/nlp-getting-started/train.csv',index_col='id')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv',index_col='id')\ny=train['target']\ntrain.drop(['location','keyword'],inplace=True,axis=1)\ntest.drop(['location','keyword'],inplace=True,axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:15:52.421653Z","iopub.execute_input":"2021-06-08T08:15:52.422004Z","iopub.status.idle":"2021-06-08T08:15:52.508674Z","shell.execute_reply.started":"2021-06-08T08:15:52.421974Z","shell.execute_reply":"2021-06-08T08:15:52.507922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Use regex to clean the data\ndef remove_url(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndef decontraction(text):\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)\n    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    return text \n\ndef seperate_alphanumeric(text):\n    words = text\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\n\ndef cont_rep_char(text):\n    tchr = text.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] \n\ndef unique_char(rep, text):\n    substitute = re.sub(r'(\\w)\\1+', rep, text)\n    return substitute\n\ntrain['text']=train['text'].apply(lambda x : remove_url(x))\ntrain['text']=train['text'].apply(lambda x : remove_punct(x))\ntrain['text']=train['text'].apply(lambda x : remove_emoji(x))\ntrain['text']=train['text'].apply(lambda x : decontraction(x))\ntrain['text']=train['text'].apply(lambda x : seperate_alphanumeric(x))\ntrain['text']=train['text'].apply(lambda x : unique_char(cont_rep_char,x))\n\ntest['text']=test['text'].apply(lambda x : remove_url(x))\ntest['text']=test['text'].apply(lambda x : remove_punct(x))\ntest['text']=test['text'].apply(lambda x : remove_emoji(x))\ntest['text']=test['text'].apply(lambda x : decontraction(x))\ntest['text']=test['text'].apply(lambda x : seperate_alphanumeric(x))\ntest['text']=test['text'].apply(lambda x : unique_char(cont_rep_char,x))","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:15:53.337155Z","iopub.execute_input":"2021-06-08T08:15:53.337567Z","iopub.status.idle":"2021-06-08T08:15:54.065725Z","shell.execute_reply.started":"2021-06-08T08:15:53.337525Z","shell.execute_reply":"2021-06-08T08:15:54.064976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"onehot_encoder = OneHotEncoder(sparse=False)\ny = (np.asarray(y)).reshape(-1,1)\nY = onehot_encoder.fit_transform(y)\n\nX_train, X_val, y_train, y_val = train_test_split(train.text,Y, random_state=10, test_size=0.2, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:15:54.787252Z","iopub.execute_input":"2021-06-08T08:15:54.787559Z","iopub.status.idle":"2021-06-08T08:15:54.797386Z","shell.execute_reply.started":"2021-06-08T08:15:54.78753Z","shell.execute_reply":"2021-06-08T08:15:54.796296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape,X_val.shape,y_train.shape,y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:15:55.606496Z","iopub.execute_input":"2021-06-08T08:15:55.60682Z","iopub.status.idle":"2021-06-08T08:15:55.611575Z","shell.execute_reply.started":"2021-06-08T08:15:55.606784Z","shell.execute_reply":"2021-06-08T08:15:55.610734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare the Tokens","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"distilbert-base-uncased\"\nbatch_size = 16","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:15:57.223935Z","iopub.execute_input":"2021-06-08T08:15:57.224247Z","iopub.status.idle":"2021-06-08T08:15:57.228152Z","shell.execute_reply.started":"2021-06-08T08:15:57.224218Z","shell.execute_reply":"2021-06-08T08:15:57.227286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:16:04.965197Z","iopub.execute_input":"2021-06-08T08:16:04.965524Z","iopub.status.idle":"2021-06-08T08:16:14.983248Z","shell.execute_reply.started":"2021-06-08T08:16:04.965496Z","shell.execute_reply":"2021-06-08T08:16:14.982299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:16:16.054329Z","iopub.execute_input":"2021-06-08T08:16:16.054636Z","iopub.status.idle":"2021-06-08T08:16:16.063293Z","shell.execute_reply.started":"2021-06-08T08:16:16.054607Z","shell.execute_reply":"2021-06-08T08:16:16.06238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:16:16.923932Z","iopub.execute_input":"2021-06-08T08:16:16.924251Z","iopub.status.idle":"2021-06-08T08:16:16.928639Z","shell.execute_reply.started":"2021-06-08T08:16:16.924222Z","shell.execute_reply":"2021-06-08T08:16:16.927825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_t = regular_encode(list(X_train), tokenizer, maxlen=512)\nX_val_t = regular_encode(list(X_val), tokenizer, maxlen=512)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:16:17.75963Z","iopub.execute_input":"2021-06-08T08:16:17.759969Z","iopub.status.idle":"2021-06-08T08:16:19.412164Z","shell.execute_reply.started":"2021-06-08T08:16:17.759938Z","shell.execute_reply":"2021-06-08T08:16:19.411303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train_t, y_train))\n    .repeat()\n    .shuffle(1995)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val_t, y_val))\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:16:20.042963Z","iopub.execute_input":"2021-06-08T08:16:20.043545Z","iopub.status.idle":"2021-06-08T08:16:25.989228Z","shell.execute_reply.started":"2021-06-08T08:16:20.043502Z","shell.execute_reply":"2021-06-08T08:16:25.98847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the Model","metadata":{}},{"cell_type":"code","source":"def build_model(transformer, max_len=160):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(2, activation='softmax')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:16:25.990772Z","iopub.execute_input":"2021-06-08T08:16:25.991111Z","iopub.status.idle":"2021-06-08T08:16:25.998844Z","shell.execute_reply.started":"2021-06-08T08:16:25.991074Z","shell.execute_reply":"2021-06-08T08:16:25.997664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_layer = TFAutoModel.from_pretrained(model_checkpoint)\nmodel_base = build_model(transformer_layer, max_len=512)\nmodel_base.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:16:29.794566Z","iopub.execute_input":"2021-06-08T08:16:29.794904Z","iopub.status.idle":"2021-06-08T08:17:11.724935Z","shell.execute_reply.started":"2021-06-08T08:16:29.794874Z","shell.execute_reply":"2021-06-08T08:17:11.724038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"n_steps = X_train.shape[0] // batch_size\nmodel_base.fit(train_dataset,steps_per_epoch=n_steps,validation_data=valid_dataset,epochs=3)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:17:11.726476Z","iopub.execute_input":"2021-06-08T08:17:11.72683Z","iopub.status.idle":"2021-06-08T08:35:18.785416Z","shell.execute_reply.started":"2021-06-08T08:17:11.726786Z","shell.execute_reply":"2021-06-08T08:35:18.784586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting on new tweets","metadata":{}},{"cell_type":"code","source":"X_test = regular_encode(list(test.text), tokenizer, maxlen=512)\ntest1 = (tf.data.Dataset.from_tensor_slices(X_test).batch(batch_size))\npred = model_base.predict(test1,verbose = 0)\npred = np.argmax(pred,axis=-1)\npred = pred.astype('int16')\npred[:5]","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:36:35.114027Z","iopub.execute_input":"2021-06-08T08:36:35.114383Z","iopub.status.idle":"2021-06-08T08:37:09.677037Z","shell.execute_reply.started":"2021-06-08T08:36:35.114349Z","shell.execute_reply":"2021-06-08T08:37:09.676351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res=pd.read_csv('../input/nlp-getting-started/sample_submission.csv',index_col=None)  \nres['target'] = pred\nres.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T08:37:22.425383Z","iopub.execute_input":"2021-06-08T08:37:22.425686Z","iopub.status.idle":"2021-06-08T08:37:22.611004Z","shell.execute_reply.started":"2021-06-08T08:37:22.425657Z","shell.execute_reply":"2021-06-08T08:37:22.610172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}