{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchtext\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimport spacy\n\nnlp = spacy.load(\"en\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def is_token_allowed(token):\n    '''\n    Only allow valid tokens which are not stop words\n    and punctuation symbols.\n    '''\n    if (not token or not token.string.strip() or\n        token.is_stop or token.is_punct):\n        return False\n    return True\n\ndef preprocess_token(token):\n    # Reduce token to its lowercase lemma form\n    return token.lemma_.strip().lower()\n\ndef tokenizer(text):\n    return [ preprocess_token(token) for token in nlp.tokenizer(text) if is_token_allowed(token)]\n\nfrom torchtext import data\n\ntext = data.Field(\n        tokenize=tokenizer,\n        pad_first=True\n)\n\n\ntrain_dataset = data.TabularDataset(\n            path=\"/kaggle/input/nlp-getting-started/train.csv\", format='csv',\n            fields=[('id', data.Field()),\n                    ('keyword', text),\n                    ('location', data.Field()),\n                    ('text', text),\n                    ('target', data.Field())], \n            skip_header=True)\n\ntest_dataset = data.TabularDataset(\n            path=\"/kaggle/input/nlp-getting-started/test.csv\", format='csv',\n            fields=[('id', data.Field()),\n                    ('keyword', text),\n                    ('location', data.Field()),\n                    ('text', text)], \n            skip_header=True)\n\nMIN_FREQ = 2\n\ntext.build_vocab(train_dataset, test_dataset, min_freq=MIN_FREQ)\n\nVOCAB_SIZE = len(text.vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NGRAMS = 2\nBATCH_SIZE = 8\nEMBED_DIM = 768\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# This model was obtained from this tutorial: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n\nclass TextSentiment(nn.Module):\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super().__init__()\n        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n        self.fc = nn.Linear(embed_dim, num_class)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n        \n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return F.softmax(self.fc(embedded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext.data.utils import ngrams_iterator\n\n\ndef generate_batch(batch):\n    label = torch.tensor([int(entry.target[0]) for entry in batch])\n    _text = []\n    for entry in batch:\n        _entry = []\n        for t in entry.text:\n            _entry.append(text.vocab.stoi[t])\n        _text.append(torch.tensor(_entry,dtype=torch.long))\n    offsets = [0] + [len(entry) for entry in _text]\n    # torch.Tensor.cumsum returns the cumulative sum\n    # of elements in the dimension dim.\n    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n    _text = torch.cat(_text)\n    return _text, offsets, label\n\n\nfrom torch.utils.data import DataLoader\n\n\ndef train_func(sub_train_):\n    # Train the model\n    train_loss = 0\n    train_acc = 0\n    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n                      collate_fn=generate_batch)\n    for i, (text, offsets, cls) in enumerate(data):\n        optimizer.zero_grad()\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        output = model(text, offsets)\n        loss = criterion(output, cls)\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        train_acc += (output.argmax(1) == cls).sum().item()\n\n    # Adjust the learning rate\n    scheduler.step()\n    \n    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n\ndef test(data_):\n    loss = 0\n    acc = 0\n    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n    for text, offsets, cls in data:\n        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n        with torch.no_grad():\n            output = model(text, offsets)\n            loss = criterion(output, cls)\n            loss += loss.item()\n            acc += (output.argmax(1) == cls).sum().item()\n\n    return loss / len(data_), acc / len(data_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nfrom torch.utils.data.dataset import random_split\n\nmodel = TextSentiment(VOCAB_SIZE, EMBED_DIM, 2).to(device)\n\nN_EPOCHS = 5\nmin_valid_loss = float('inf')\n\ncriterion = torch.nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=4)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n\ntrain_len = int(len(train_dataset) * 0.95)\nsub_train_, sub_valid_ = \\\n    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    train_loss, train_acc = train_func(sub_train_)\n    valid_loss, valid_acc = test(sub_valid_)\n\n    secs = int(time.time() - start_time)\n    mins = secs / 60\n    secs = secs % 60\n\n    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(_text, model, vocab, ngrams):\n    if len(_text) == 0:\n        return 0\n    \n    with torch.no_grad():\n        _text = [vocab.stoi[token] for token in ngrams_iterator(_text, ngrams)]\n        output = model(torch.tensor(_text), torch.tensor([0]))\n        return output.argmax(1).item()\n\nmodel = model.to(\"cpu\")\npredictions = [predict(entry.text, model, text.vocab, NGRAMS) for entry in test_dataset]\ntweet_id = [entry.id[0] for entry in test_dataset]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'id': tweet_id, 'target': predictions})\noutput.to_csv('my_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}