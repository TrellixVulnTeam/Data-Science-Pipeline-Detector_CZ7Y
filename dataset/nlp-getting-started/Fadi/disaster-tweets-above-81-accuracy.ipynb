{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><b><u>Disaster Tweets Classification </u></b></h1>\n\n<h4 style='text-align: justify;'>\nThis notebook is mainly to preprocess the dataset. At the beginning, I tested the accuracy that can be reached by using trivial methods to predict the output. Afterwards, the data is procesed to be use by BERT. The machine learning models are notshown in this notebook because I focused on the models on another notebook. Neural networks with BERT was the main interest, yet other algorithm were used too.\n<br/>\nFor the machine learning models, check the link below:\n    <a href='https://www.kaggle.com/fmakarem/disaster-tweets-bert'>Machine Learning Notebook</a>\n</h4>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-05T11:20:48.218382Z","iopub.execute_input":"2021-12-05T11:20:48.218868Z","iopub.status.idle":"2021-12-05T11:20:48.235323Z","shell.execute_reply.started":"2021-12-05T11:20:48.218788Z","shell.execute_reply":"2021-12-05T11:20:48.23444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom collections import OrderedDict,Counter\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.manifold import TSNE\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nimport string\nimport re","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:53:48.378884Z","iopub.execute_input":"2021-12-05T11:53:48.379229Z","iopub.status.idle":"2021-12-05T11:53:48.385928Z","shell.execute_reply.started":"2021-12-05T11:53:48.379198Z","shell.execute_reply":"2021-12-05T11:53:48.384994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_splits=7\nrandom_state=27","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:31:50.329436Z","iopub.execute_input":"2021-12-05T11:31:50.329748Z","iopub.status.idle":"2021-12-05T11:31:50.334499Z","shell.execute_reply.started":"2021-12-05T11:31:50.329722Z","shell.execute_reply":"2021-12-05T11:31:50.333622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop=stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:55:21.988219Z","iopub.execute_input":"2021-12-05T11:55:21.988527Z","iopub.status.idle":"2021-12-05T11:55:21.992623Z","shell.execute_reply.started":"2021-12-05T11:55:21.9885Z","shell.execute_reply":"2021-12-05T11:55:21.991978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>Functions</b></h2>\n\n<h4>\nHere some useful functions are created\n</h4>","metadata":{}},{"cell_type":"code","source":"def get_keyword_mean(train_df,val_df,column='keyword',target='target'):\n    aggregate_column=column+'_mean'\n    train_df[aggregate_column]=train_df.groupby(column)[target].transform('mean')\n    \n    val_df = val_df.merge(\n                train_df[[column, aggregate_column]].drop_duplicates(),\n                on=column,\n                how=\"left\",\n            )\n    return train_df,val_df","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:05:48.435173Z","iopub.execute_input":"2021-11-19T22:05:48.435585Z","iopub.status.idle":"2021-11-19T22:05:48.448073Z","shell.execute_reply.started":"2021-11-19T22:05:48.43555Z","shell.execute_reply":"2021-11-19T22:05:48.44697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cv_datasets(df,model,regex='^BERT',target_column='target',use_keyword=False,model_function=None,compile_dic={},**kwargs):\n    '''\n    df: Dataframe to divide the data.\n    model: Model used to fit and evaluate the data.\n    regex: Regex used to pick a column. By default, it searches for the columns starting with BERT.\n    target_column: Name of the column containing the output.\n    compile_dic: It is only used for keras models\n    '''\n    kfold=KFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n    results={'Total':0}\n    i=0\n    \n    #iterate through the folds\n    for train_index,val_index in kfold.split(df):\n        \n        if 'keras' in str(type(model)):\n            #keras\n            model_copy=model_function()\n            model_copy.compile(**compile_dic)\n    \n            model_copy.summary()\n        else:\n            #sklearn\n            model_copy= sklearn.base.clone(model)\n            \n        train_df=df.iloc[train_index]\n        val_df=df.iloc[val_index]\n        \n        if use_keyword:\n            train_df,val_df=get_keyword_mean(train_df,val_df)\n            regex+='|^keyword_'\n        \n        X_train=train_df.filter(regex=regex,axis=1)\n        y_train=train_df[target_column]\n        \n        X_val=val_df.filter(regex=regex,axis=1)\n        y_val=val_df[target_column]\n        \n        #print(X_train)\n        X_train=X_train.copy().values\n        y_train=y_train.copy().values\n        \n        X_val=X_val.copy().values\n        y_val=y_val.copy().values\n        \n        'The problem with keras mmodels is the copy itself'\n        model_copy.fit(X_train,y_train,**kwargs)\n        \n        predicted=model_copy.predict(X_val)\n        \n        metric=confusion_matrix(y_val,predicted)\n        \n        tn, fp, fn, tp = metric.ravel()\n        \n        results_matrix=metric\n        results['Batch '+str(i)]=results_matrix \n        results['Total']+=np.array(results_matrix)\n        i+=1\n        \n        print(f'{i}th run:\\naccuracy: {(tp+tn)/(tn+fp+fn+tp)}\\nprecision:{tp/(tp+fp)}\\nrecall:{tp/(tp+fn)}\\n')\n    \n    \n    tn, fp, fn, tp = results['Total'].ravel()\n    \n    print(f'Total:\\naccuracy: {(tp+tn)/(tn+fp+fn+tp)}\\nprecision:{tp/(tp+fp)}\\nrecall:{tp/(tp+fn)}\\n')\n    \n    return results\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:05:48.44981Z","iopub.execute_input":"2021-11-19T22:05:48.450239Z","iopub.status.idle":"2021-11-19T22:05:48.465155Z","shell.execute_reply.started":"2021-11-19T22:05:48.450195Z","shell.execute_reply":"2021-11-19T22:05:48.464423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_and_predict(model,train_df,test_df,use_keyword=False,regex='^BERT',target_column='target',**kwargs):\n    if use_keyword:\n        train_df,test_df=get_keyword_mean(train_df,test_df)\n        regex+='|^keyword_'\n\n    X_train=train_df.filter(regex=regex,axis=1)\n    y_train=train_df[target_column]\n\n    X_test=test_df.filter(regex=regex,axis=1)\n    \n    model.fit(X_train,y_train,**kwargs)\n    \n    test_df['predictions']=(model.predict(X_test)>0.5).astype(int)\n    \n    return test_df","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:05:48.466163Z","iopub.execute_input":"2021-11-19T22:05:48.466561Z","iopub.status.idle":"2021-11-19T22:05:48.480247Z","shell.execute_reply.started":"2021-11-19T22:05:48.466532Z","shell.execute_reply":"2021-11-19T22:05:48.479417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_submission(test_df,predictions_column='predictions',name='submission'):\n#     new_submission=pd.DataFrame()\n#     new_submission['id']=test_df['id']\n#     new_submission['target']=predictions\n#     new_submission=new_submission.set_index('id')\n    \n    new_submission=test_df[['id',predictions_column]].set_index('id').rename(columns={predictions_column:'target'})\n    print(f'saving the results in {name}.csv')\n    new_submission.to_csv(name+'.csv')\n    print('finished saving')\n    \n    return new_submission","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:05:48.481332Z","iopub.execute_input":"2021-11-19T22:05:48.481631Z","iopub.status.idle":"2021-11-19T22:05:48.49234Z","shell.execute_reply.started":"2021-11-19T22:05:48.481603Z","shell.execute_reply":"2021-11-19T22:05:48.491608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_results(df,model,name='submission',regex='^BERT',target_column='target',use_keyword=False,compile_dic={},model_function=None,**kwargs):\n    df['keyword']=df['keyword'].fillna('unk')\n    df['location']=df['location'].fillna('unk_location')\n    \n    train_df=df[df['trainable']==1]\n    test_df=df[~(df['trainable']==1)]\n    \n    results=cv_datasets(train_df,model,regex=regex,target_column=target_column,model_function=model_function,use_keyword=use_keyword,**kwargs)\n    \n    test_df=fit_and_predict(model,train_df,test_df,use_keyword=use_keyword,regex=regex,target_column=target_column,**kwargs)\n    \n    test_df['predictions']=test_df['predictions'].values>0.5\n    test_df['predictions']=test_df['predictions'].astype(int)\n    \n    submission=generate_submission(test_df,predictions_column='predictions',name=name)\n    \n    return results,test_df,submission","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:05:48.494425Z","iopub.execute_input":"2021-11-19T22:05:48.494959Z","iopub.status.idle":"2021-11-19T22:05:48.503708Z","shell.execute_reply.started":"2021-11-19T22:05:48.494926Z","shell.execute_reply":"2021-11-19T22:05:48.502864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/bert-features/BERT_processed.csv')\nresults,test_df,submission=generate_results(df,model=LogisticRegression(solver='sag'),use_keyword=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-19T22:05:48.505362Z","iopub.execute_input":"2021-11-19T22:05:48.505671Z","iopub.status.idle":"2021-11-19T22:05:56.189209Z","shell.execute_reply.started":"2021-11-19T22:05:48.505642Z","shell.execute_reply":"2021-11-19T22:05:56.188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/bert-features/BERT_processed.csv')\n\nrf_parameters={\n    'n_estimators':300,\n    'max_depth':20,\n    'min_samples_split':80,\n    'min_samples_leaf':30,\n    'min_weight_fraction_leaf':0.0,\n    'max_features':'auto',\n    'max_leaf_nodes':40,\n    'min_impurity_decrease':0.0,\n    'min_impurity_split':None,\n}\nresults,test_df,submission=generate_results(df,model=RandomForestClassifier(**rf_parameters),use_keyword=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-19T22:05:56.19109Z","iopub.execute_input":"2021-11-19T22:05:56.191576Z","iopub.status.idle":"2021-11-19T22:07:20.901481Z","shell.execute_reply.started":"2021-11-19T22:05:56.191512Z","shell.execute_reply":"2021-11-19T22:07:20.90047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Best till now\ndf=pd.read_csv('/kaggle/input/bert-features/BERT_processed.csv')\nresults,test_df,submission=generate_results(df,model=SVC(C=5),use_keyword=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-19T22:07:20.903658Z","iopub.execute_input":"2021-11-19T22:07:20.904096Z","iopub.status.idle":"2021-11-19T22:08:16.431283Z","shell.execute_reply.started":"2021-11-19T22:07:20.904049Z","shell.execute_reply":"2021-11-19T22:08:16.430353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/bert-features/BERT_processed.csv')\nresults,test_df,submission=generate_results(df,model=KNeighborsClassifier(200,weights='distance',p=2),use_keyword=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-19T22:08:16.432474Z","iopub.execute_input":"2021-11-19T22:08:16.432798Z","iopub.status.idle":"2021-11-19T22:08:36.491904Z","shell.execute_reply.started":"2021-11-19T22:08:16.432769Z","shell.execute_reply":"2021-11-19T22:08:36.491034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results,test_df,submission\nresults['Total']","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-19T22:08:36.493176Z","iopub.execute_input":"2021-11-19T22:08:36.493469Z","iopub.status.idle":"2021-11-19T22:08:36.501743Z","shell.execute_reply.started":"2021-11-19T22:08:36.49344Z","shell.execute_reply":"2021-11-19T22:08:36.500823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tn, fp, fn, tp = results['Total'].ravel()\n(tn+tp)/(tn+fp+fn+tp)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-19T22:08:36.502955Z","iopub.execute_input":"2021-11-19T22:08:36.503323Z","iopub.status.idle":"2021-11-19T22:08:36.51526Z","shell.execute_reply.started":"2021-11-19T22:08:36.503293Z","shell.execute_reply":"2021-11-19T22:08:36.514592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>Get The Data</b></h2>","metadata":{}},{"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsample_submission=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:32:05.304899Z","iopub.execute_input":"2021-12-05T11:32:05.305508Z","iopub.status.idle":"2021-12-05T11:32:05.396562Z","shell.execute_reply.started":"2021-12-05T11:32:05.30547Z","shell.execute_reply":"2021-12-05T11:32:05.395505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:36.595883Z","iopub.execute_input":"2021-11-19T22:08:36.596499Z","iopub.status.idle":"2021-11-19T22:08:36.616476Z","shell.execute_reply.started":"2021-11-19T22:08:36.596455Z","shell.execute_reply":"2021-11-19T22:08:36.615326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[train_df['keyword'].notnull()]","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:32:12.681386Z","iopub.execute_input":"2021-12-05T11:32:12.681946Z","iopub.status.idle":"2021-12-05T11:32:12.715565Z","shell.execute_reply.started":"2021-12-05T11:32:12.681911Z","shell.execute_reply":"2021-12-05T11:32:12.714921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['keyword']=test_df['keyword'].fillna('unk')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:32:13.062762Z","iopub.execute_input":"2021-12-05T11:32:13.06323Z","iopub.status.idle":"2021-12-05T11:32:13.078439Z","shell.execute_reply.started":"2021-12-05T11:32:13.063197Z","shell.execute_reply":"2021-12-05T11:32:13.077749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:32:13.382016Z","iopub.execute_input":"2021-12-05T11:32:13.38239Z","iopub.status.idle":"2021-12-05T11:32:13.407517Z","shell.execute_reply.started":"2021-12-05T11:32:13.382358Z","shell.execute_reply":"2021-12-05T11:32:13.406811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.isna(train_df).sum()/len(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:32:17.393407Z","iopub.execute_input":"2021-12-05T11:32:17.394033Z","iopub.status.idle":"2021-12-05T11:32:17.418349Z","shell.execute_reply.started":"2021-12-05T11:32:17.393988Z","shell.execute_reply":"2021-12-05T11:32:17.417523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_class=train_df['target'].sum()/len(train_df)\nprint(f'the percentage of ones in the dataset is {np.round(positive_class,2)}')","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:32:27.478832Z","iopub.execute_input":"2021-12-05T11:32:27.479435Z","iopub.status.idle":"2021-12-05T11:32:27.485527Z","shell.execute_reply.started":"2021-12-05T11:32:27.479387Z","shell.execute_reply":"2021-12-05T11:32:27.484751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>Simple Methods</b></h2>\n\n<h4 style='text-align: justify;'>\nFirst I did submit the sample results. It achieved a 57% accuracy score. Afterwards, I used generated output files from each of the keyword and location columns. Hence, we can sense what score can be reached using easy methods and whatmore advanced ones can do.\n</h4>","metadata":{}},{"cell_type":"code","source":"keyword_df=pd.DataFrame()\ni=0\nfor keyword,group_df in train_df[['keyword','target']].fillna('unk').groupby('keyword'):\n    positive_Kkeyword=group_df['target'].sum()/len(group_df)\n    keyword_df.loc[i,['keyword']]=keyword\n    keyword_df.loc[i,['count']]=len(group_df)\n    keyword_df.loc[i,['% positive']]=positive_Kkeyword\n    i+=1\n#keyword_df.to_csv('keyword_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:36.710631Z","iopub.execute_input":"2021-11-19T22:08:36.710995Z","iopub.status.idle":"2021-11-19T22:08:37.497094Z","shell.execute_reply.started":"2021-11-19T22:08:36.710966Z","shell.execute_reply":"2021-11-19T22:08:37.495999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_df=pd.DataFrame()\ni=0\nfor keyword,group_df in train_df[['location','target']].fillna('unk').groupby('location'):\n    positive_Kkeyword=group_df['target'].sum()/len(group_df)\n    location_df.loc[i,['location']]=keyword\n    location_df.loc[i,['count']]=len(group_df)\n    location_df.loc[i,['% positive']]=positive_Kkeyword\n    i+=1\n#location_df.to_csv('location_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:37.500919Z","iopub.execute_input":"2021-11-19T22:08:37.501258Z","iopub.status.idle":"2021-11-19T22:08:49.385837Z","shell.execute_reply.started":"2021-11-19T22:08:37.501224Z","shell.execute_reply":"2021-11-19T22:08:49.384742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>The idea is to group each row with the same column value (for keyword_df the keyword column while for the location_df the location column). Afterwards, some useful information is added, the most important one (the one that is used in the model) is the % positive which is basically the mean of the target for each group.</h4>","metadata":{}},{"cell_type":"code","source":"keyword_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:49.388104Z","iopub.execute_input":"2021-11-19T22:08:49.388497Z","iopub.status.idle":"2021-11-19T22:08:49.401282Z","shell.execute_reply.started":"2021-11-19T22:08:49.388462Z","shell.execute_reply":"2021-11-19T22:08:49.400574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(keyword_df[keyword_df['keyword']==test_df.iloc[0]['keyword']]['% positive'].values[0])\nnew_submission=pd.DataFrame()\nnew_submission['id']=test_df['id']\nnew_submission['target']=test_df.apply(lambda row: int(keyword_df[keyword_df.keyword==row['keyword']]['% positive'].values[0]>=0.5),axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:49.402264Z","iopub.execute_input":"2021-11-19T22:08:49.402722Z","iopub.status.idle":"2021-11-19T22:08:51.312633Z","shell.execute_reply.started":"2021-11-19T22:08:49.402682Z","shell.execute_reply":"2021-11-19T22:08:51.311729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#new_submission.rename({0:'target'},axis=1)\nnew_submission.to_csv('to_submit.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:51.313886Z","iopub.execute_input":"2021-11-19T22:08:51.314164Z","iopub.status.idle":"2021-11-19T22:08:51.327978Z","shell.execute_reply.started":"2021-11-19T22:08:51.314137Z","shell.execute_reply":"2021-11-19T22:08:51.32702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_submission","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:51.329342Z","iopub.execute_input":"2021-11-19T22:08:51.329701Z","iopub.status.idle":"2021-11-19T22:08:51.34381Z","shell.execute_reply.started":"2021-11-19T22:08:51.329669Z","shell.execute_reply":"2021-11-19T22:08:51.342814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>\nThe keyword generated output file got 72% score while the location file got a lower score. The location output file generated a lower score than the keyword. This is expected because the location column contains a lot of null values. <br/>\nNotice the difference between the sample submission (57%) and the result only using the keyword (72%). This shows the value of the keyword column for this task. It is expected because the keyword is a summary of the tweet using one word.\n</h4>","metadata":{}},{"cell_type":"markdown","source":"<h2><b>Visualizations</b></h2>","metadata":{}},{"cell_type":"code","source":"train_ls=list(train_df['text'].str.split(' ').to_numpy())\ntrain_outputs=train_df['target'].values\n\nassert len(train_ls)==len(train_outputs), 'ERROR'","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:10.859363Z","iopub.execute_input":"2021-12-05T11:35:10.85967Z","iopub.status.idle":"2021-12-05T11:35:10.884386Z","shell.execute_reply.started":"2021-12-05T11:35:10.859643Z","shell.execute_reply":"2021-12-05T11:35:10.883502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tokenizer=keras.preprocessing.text.Tokenizer(oov_token=0)\nTokenizer.fit_on_texts(train_ls)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:51.374183Z","iopub.execute_input":"2021-11-19T22:08:51.374607Z","iopub.status.idle":"2021-11-19T22:08:51.515496Z","shell.execute_reply.started":"2021-11-19T22:08:51.374562Z","shell.execute_reply":"2021-11-19T22:08:51.514715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0\nn_words=len(Tokenizer.word_counts)\nfor word,count in Counter(Tokenizer.word_counts).most_common():\n    print(f'{word}: {count}')\n    i+=1\n    if count<100:\n        break\n\nprint(f'{i} word are the most common from {n_words} which is {i/n_words*100}%')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:51.516473Z","iopub.execute_input":"2021-11-19T22:08:51.516957Z","iopub.status.idle":"2021-11-19T22:08:51.545738Z","shell.execute_reply.started":"2021-11-19T22:08:51.516923Z","shell.execute_reply":"2021-11-19T22:08:51.544752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='target',data=train_df)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:51.546809Z","iopub.execute_input":"2021-11-19T22:08:51.547097Z","iopub.status.idle":"2021-11-19T22:08:51.694507Z","shell.execute_reply.started":"2021-11-19T22:08:51.54707Z","shell.execute_reply":"2021-11-19T22:08:51.693614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_characters=np.max(train_df['text'].apply(lambda x: len(x)))\nprint(f'the max number of characters in a tweet is {max_characters}\\nThus, the number of word < {max_characters}')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:51.696086Z","iopub.execute_input":"2021-11-19T22:08:51.696474Z","iopub.status.idle":"2021-11-19T22:08:51.708558Z","shell.execute_reply.started":"2021-11-19T22:08:51.696432Z","shell.execute_reply":"2021-11-19T22:08:51.707633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(Counter(Tokenizer.word_counts).most_common())","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:51.709903Z","iopub.execute_input":"2021-11-19T22:08:51.710218Z","iopub.status.idle":"2021-11-19T22:08:51.738308Z","shell.execute_reply.started":"2021-11-19T22:08:51.710189Z","shell.execute_reply":"2021-11-19T22:08:51.73731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_100=Counter(Tokenizer.word_counts).most_common()[:100]\n\nx_top_100=[x for x,y in top_100]\ny_top_100=[y for x,y in top_100]\nax=sns.barplot(x=y_top_100,y=x_top_100)\nax.figure.set_size_inches(15,20)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:51.739545Z","iopub.execute_input":"2021-11-19T22:08:51.739952Z","iopub.status.idle":"2021-11-19T22:08:53.480189Z","shell.execute_reply.started":"2021-11-19T22:08:51.739903Z","shell.execute_reply":"2021-11-19T22:08:53.479239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4 style='text-align: justify;'>\nLooking at the top 100 words makes us realized the most used words like 'a' do not give a lot of information if a disaster did occur or not. Actually, only emergency and disaster are word that indicate the possibility of a disaster occuring. Hence, most of the words are stop words that do not give much information about the tweet itself. These words will be removed.\n</h4>","metadata":{}},{"cell_type":"code","source":"bottom_100=Counter(Tokenizer.word_counts).most_common()[-100:]\n\nx_bottom_100=[x for x,y in bottom_100]\ny_bottom_100=[y for x,y in bottom_100]\n\n#Horizontal barplot\nax=sns.barplot(x=y_bottom_100,y=x_bottom_100)\nax.figure.set_size_inches(15,20)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:53.481656Z","iopub.execute_input":"2021-11-19T22:08:53.481948Z","iopub.status.idle":"2021-11-19T22:08:55.767907Z","shell.execute_reply.started":"2021-11-19T22:08:53.48192Z","shell.execute_reply":"2021-11-19T22:08:55.767072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>\nThe bottom 100 words are all links. I do not think that the presence of a link means a disater happened. I do not expect that a disaster happened if someone tweeted 'check this link http:...'. If there was a link or a radio source that only give information about disasters. Hence, the links will be removed too.\n</h4>","metadata":{}},{"cell_type":"markdown","source":"<h4>\n    <b>Relation beteen words and disaster:</b><br/><br/>\n    Now let us see the probability of positive knowing the word. I ill create a dictionary with keys as words and values as a tuple (total_occurances, positive_occurances,disaster_probability).<br/><br/>\n    total_occurances: The number of times the word appears.<br/>\n    positive_occurances: The number of times the word occurs in a positive sentence.\n    <br/>\n    <br/>\n    Afterwards, we can approximate the probability of disaster knowing word gy dividing positive_occurances by total_occurances. This may look useless, but the model may learn to classify sentences as disasters by just checking the occurance of certain words. That is why it is good to know this metric.\n</h4>","metadata":{}},{"cell_type":"code","source":"word_occurances={}\nword_occurances_no_stopwords={}\n\nfor i in range(len(train_ls)):\n    sentence=train_ls[i]\n    output=train_outputs[i]\n    \n    sentence=list(filter(lambda a: a != '', sentence))\n    \n    #Get the unique set of words. If a word appeare more than one in a sentence, I do not want to count it\n    unique_words=set(sentence)\n    \n    for word in unique_words:\n        word_lower=word.lower()#convert to lower\n        #get the values from dictionary of present, else return them as (0,0)\n        total_number,positive_number=word_occurances.get(word_lower,(0,0))\n        \n        #remove non alphanumeric words\n        word_lower=re.sub(r'[^a-zA-Z0-9 ]', '', word_lower)\n        \n        total_number+=1#increment total_number\n        # If output is 1, then increment positive_number \n        if output==1:\n            positive_number+=1\n        \n        #update the dictionaries\n        word_occurances[word_lower]=(total_number,positive_number)\n        if (word_lower not in stop) and ('http' not in word_lower):\n            word_occurances_no_stopwords[word_lower]=(total_number,positive_number)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:32:17.619659Z","iopub.execute_input":"2021-12-05T12:32:17.620335Z","iopub.status.idle":"2021-12-05T12:32:18.154017Z","shell.execute_reply.started":"2021-12-05T12:32:17.620291Z","shell.execute_reply":"2021-12-05T12:32:18.153112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generate new dictonaries with key as word and value as the probability\n#Let us take the values that occured between 20 and 100.\n#If a word appeared just once and it is a disaster, it is difficult to say that the word indictes a disaster.\n#Feel free to change the boundaries and test them.\ndisaster_prob={key:(value[1]/value[0]) for key,value in word_occurances.items() if value[0]>20 and value[0]<100}\ndisaster_prob_no_stopwords={key:(value[1]/value[0]) for key,value in word_occurances_no_stopwords.items() if value[0]>20 and value[0]<100}","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:35:49.356711Z","iopub.execute_input":"2021-12-05T12:35:49.357333Z","iopub.status.idle":"2021-12-05T12:35:49.368564Z","shell.execute_reply.started":"2021-12-05T12:35:49.357295Z","shell.execute_reply":"2021-12-05T12:35:49.36752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('length of the dictionary:',len(disaster_prob))\nprint('words with probability<1:',len([key for key,value in disaster_prob.items() if value !=1]))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:35:50.450086Z","iopub.execute_input":"2021-12-05T12:35:50.450438Z","iopub.status.idle":"2021-12-05T12:35:50.456442Z","shell.execute_reply.started":"2021-12-05T12:35:50.450408Z","shell.execute_reply":"2021-12-05T12:35:50.455424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_100=Counter(disaster_prob).most_common()[:100]\n\nx_top_100,y_top_100=zip(*top_100)\nax=sns.barplot(x=list(y_top_100),y=list(x_top_100))\nax.figure.set_size_inches(15,20)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:35:55.40268Z","iopub.execute_input":"2021-12-05T12:35:55.402989Z","iopub.status.idle":"2021-12-05T12:35:57.105997Z","shell.execute_reply.started":"2021-12-05T12:35:55.402962Z","shell.execute_reply":"2021-12-05T12:35:57.10531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bottom_100=Counter(disaster_prob).most_common()[-100:]\n\nx_bottom_100,y_bottom_100=zip(*bottom_100)\nax=sns.barplot(x=list(y_bottom_100),y=list(x_bottom_100))\nax.figure.set_size_inches(15,20)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:40:29.760198Z","iopub.execute_input":"2021-12-05T12:40:29.760519Z","iopub.status.idle":"2021-12-05T12:40:31.314325Z","shell.execute_reply.started":"2021-12-05T12:40:29.760492Z","shell.execute_reply":"2021-12-05T12:40:31.313373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('length of the dictionary',len(disaster_prob_no_stopwords))\nprint('words with probability<1',len([key for key,value in disaster_prob_no_stopwords.items() if value !=1]))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:37:01.596233Z","iopub.execute_input":"2021-12-05T12:37:01.596585Z","iopub.status.idle":"2021-12-05T12:37:01.602541Z","shell.execute_reply.started":"2021-12-05T12:37:01.596552Z","shell.execute_reply":"2021-12-05T12:37:01.6013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_100=Counter(disaster_prob_no_stopwords).most_common()[:100]\n\nx_top_100,y_top_100=zip(*top_100)\nax=sns.barplot(x=list(y_top_100),y=list(x_top_100))\nax.figure.set_size_inches(15,20)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:41:35.683636Z","iopub.execute_input":"2021-12-05T12:41:35.683981Z","iopub.status.idle":"2021-12-05T12:41:37.321814Z","shell.execute_reply.started":"2021-12-05T12:41:35.683952Z","shell.execute_reply":"2021-12-05T12:41:37.320949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bottom_100=Counter(disaster_prob_no_stopwords).most_common()[-100:]\n\nx_bottom_100,y_bottom_100=zip(*bottom_100)\nax=sns.barplot(x=list(y_bottom_100),y=list(x_bottom_100))\nax.figure.set_size_inches(15,20)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T12:41:59.395818Z","iopub.execute_input":"2021-12-05T12:41:59.396141Z","iopub.status.idle":"2021-12-05T12:42:01.327039Z","shell.execute_reply.started":"2021-12-05T12:41:59.396111Z","shell.execute_reply":"2021-12-05T12:42:01.326173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><b>Process Tweets</b></h2>\n\n<h4>\nThe tweets are processed using the nltk library.\n</h4>","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nimport string\nimport re","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:55.768968Z","iopub.execute_input":"2021-11-19T22:08:55.769247Z","iopub.status.idle":"2021-11-19T22:08:55.774188Z","shell.execute_reply.started":"2021-11-19T22:08:55.76922Z","shell.execute_reply":"2021-11-19T22:08:55.773264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop=stopwords.words('english')\n\n# for i in stop:\n#     print(i)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:55.775484Z","iopub.execute_input":"2021-11-19T22:08:55.775822Z","iopub.status.idle":"2021-11-19T22:08:55.791953Z","shell.execute_reply.started":"2021-11-19T22:08:55.775791Z","shell.execute_reply":"2021-11-19T22:08:55.790902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_df=pd.DataFrame()\n\ntrain_df['trainable']=1\ntest_df['trainable']=0\n\ntotal_df=pd.concat((train_df,test_df))\ntotal_df","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:55.793352Z","iopub.execute_input":"2021-11-19T22:08:55.793813Z","iopub.status.idle":"2021-11-19T22:08:55.819033Z","shell.execute_reply.started":"2021-11-19T22:08:55.793764Z","shell.execute_reply":"2021-11-19T22:08:55.818052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ls_tmp=[]\nstemmer= PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:55.820406Z","iopub.execute_input":"2021-11-19T22:08:55.821004Z","iopub.status.idle":"2021-11-19T22:08:55.825619Z","shell.execute_reply.started":"2021-11-19T22:08:55.820955Z","shell.execute_reply":"2021-11-19T22:08:55.824504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_stem(word:str,stemmer):\n    '''\n    Get the stem of the word\n    '''\n    return stemmer.stem(word)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:55.827021Z","iopub.execute_input":"2021-11-19T22:08:55.827432Z","iopub.status.idle":"2021-11-19T22:08:55.83793Z","shell.execute_reply.started":"2021-11-19T22:08:55.827392Z","shell.execute_reply":"2021-11-19T22:08:55.837063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_sentence(text,stemmer):\n    new_sentence=[]\n    \n    text = text.replace(r\"won't \", \"will not \")\n    text = text.replace(r\"can't \", \"can not \")\n    text = text.replace(r\"ain't \", \"am not \")\n    \n    text = text.replace(r\"n't \", \" not \")\n    text = text.replace(r\"'re \", \" are \")\n    text = text.replace(r\"'s \", \" is \")\n    text = text.replace(r\"'d \", \" would \")\n    text = text.replace(r\"'ll \", \" will \")\n    text = text.replace(r\"'t \", \" not \")\n    text = text.replace(r\"'ve \", \" have \")\n    text = text.replace(r\"'m \", \" am \")\n    \n    text = text.translate(str.maketrans('','',string.punctuation))\n    \n    text=re.sub(r'[^a-zA-Z0-9 ]', '', text)\n    #print(text)\n    sentence=text.split(' ')\n    \n    #remove words that are not in english\n    sentence=list(filter(lambda a: a != '', sentence))\n    \n    for word in sentence:\n        #word=stemmer.stem(word)\n        word_lower=word.lower()\n        if (word_lower not in stop) and ('http' not in word_lower):# and ():\n            new_sentence.append(word_lower)\n    \n    return ' '.join(new_sentence)\n\npreprocess_sentence(\"Hey I'm Yann, ° Ñ  how're you and how's it going ? That's interesting: I'd love to hear more about it. http:idk.com\",stemmer)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:55.839079Z","iopub.execute_input":"2021-11-19T22:08:55.839581Z","iopub.status.idle":"2021-11-19T22:08:55.853452Z","shell.execute_reply.started":"2021-11-19T22:08:55.839514Z","shell.execute_reply":"2021-11-19T22:08:55.852386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_df['preprocessed_text']=total_df['text'].apply(lambda text: preprocess_sentence(text,stemmer))\ntotal_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:55.855022Z","iopub.execute_input":"2021-11-19T22:08:55.855754Z","iopub.status.idle":"2021-11-19T22:08:56.453176Z","shell.execute_reply.started":"2021-11-19T22:08:55.855702Z","shell.execute_reply":"2021-11-19T22:08:56.45207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_df.to_csv('preprocessed_text.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-19T22:08:56.454612Z","iopub.execute_input":"2021-11-19T22:08:56.455058Z","iopub.status.idle":"2021-11-19T22:08:56.587434Z","shell.execute_reply.started":"2021-11-19T22:08:56.455007Z","shell.execute_reply":"2021-11-19T22:08:56.58659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Conclusion</h1>\n\n<h4 style='text-align: justify;'>\nThis notebook started by generating outputs using simple methods from the non-tweet features. The keyword output got 72% accuracy. Afterwards, some plots for visualization were shown. The plot showed the effect of stop words and links. Finally, the text was preprocessed to be used with BERT.\n</h4>\n\n<h4>\n    <b>\n        Thank You for reading. I hope it was helpful.\n    </b>\n</h4>\n\n<h4>\n    If you wish to see the prediction part, Checkout my BERT notebook <a href='https://www.kaggle.com/fmakarem/disaster-tweets-bert'>Here</a>\n</h4>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}