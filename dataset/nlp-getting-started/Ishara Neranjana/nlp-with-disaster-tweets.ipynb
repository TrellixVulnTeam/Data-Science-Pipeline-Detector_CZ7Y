{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"background-color:#blue;color:black;font-size:22px;text-align:center;border-radius:10px 10px;font-weight:bold;border:2px solid black;\">Natural Language Processing with Disaster Tweets<span style='font-size:28px; background-color:blue ;'></span></p>\n\n\n<center><img src=\"https://github.com/Isharaneranjana/kaggle_gif/blob/main/NLP%20WITH%20DISASTER%20TWEETS.gif?raw=true\"></center>","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#FC7D77;color:black;font-size:20px;text-align:center;border-radius:10px 10px;\"> Introduction ðŸŽ¯</p>\n<font size=\"4\">Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency theyâ€™re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter. In this notebook, I am going to build a machine learning model that predicts which Tweets are about real disasters and which oneâ€™s arenâ€™t. The dataset consists of 10,000 tweets that were hand classified. </font>\n","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-18T16:52:12.249471Z","iopub.execute_input":"2022-06-18T16:52:12.249746Z","iopub.status.idle":"2022-06-18T16:52:12.256882Z","shell.execute_reply.started":"2022-06-18T16:52:12.249717Z","shell.execute_reply":"2022-06-18T16:52:12.256019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tokenization","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:12.338074Z","iopub.execute_input":"2022-06-18T16:52:12.339015Z","iopub.status.idle":"2022-06-18T16:52:21.115122Z","shell.execute_reply.started":"2022-06-18T16:52:12.338981Z","shell.execute_reply":"2022-06-18T16:52:21.113943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom wordcloud import WordCloud\nimport gc\nimport re\nimport string\nimport operator\nfrom collections import defaultdict\nimport tokenization\nfrom wordcloud import STOPWORDS","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:21.116684Z","iopub.execute_input":"2022-06-18T16:52:21.116874Z","iopub.status.idle":"2022-06-18T16:52:21.122804Z","shell.execute_reply.started":"2022-06-18T16:52:21.116851Z","shell.execute_reply":"2022-06-18T16:52:21.121947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest=  pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:21.124346Z","iopub.execute_input":"2022-06-18T16:52:21.124611Z","iopub.status.idle":"2022-06-18T16:52:21.164184Z","shell.execute_reply.started":"2022-06-18T16:52:21.124581Z","shell.execute_reply":"2022-06-18T16:52:21.163671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:21.165702Z","iopub.execute_input":"2022-06-18T16:52:21.165978Z","iopub.status.idle":"2022-06-18T16:52:21.175096Z","shell.execute_reply.started":"2022-06-18T16:52:21.165955Z","shell.execute_reply":"2022-06-18T16:52:21.174285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 6))\nsns.heatmap(train.isna(), yticklabels = False, cbar = False, cmap = 'Reds')\nplt.title(\"Missing values\", fontsize = 14)\nplt.xticks(rotation = 35, fontsize = 12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:21.176053Z","iopub.execute_input":"2022-06-18T16:52:21.176213Z","iopub.status.idle":"2022-06-18T16:52:21.313335Z","shell.execute_reply.started":"2022-06-18T16:52:21.176193Z","shell.execute_reply":"2022-06-18T16:52:21.312683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(12, 4))\nplt.tight_layout()\n\ntrain.groupby('target').count()['id'].plot(kind='pie', ax=axes[0], labels=['Not Disaster (57%)', 'Disaster (43%)'],colors=['lightcoral','lightskyblue'])\nsns.countplot(x=train['target'], hue=train['target'], ax=axes[1], palette=\"RdBu\")\n\naxes[0].set_ylabel('')\naxes[1].set_ylabel('')\naxes[1].set_xticklabels(['Not Disaster (4342)', 'Disaster (3271)'])\naxes[0].tick_params(axis='x', labelsize=12)\naxes[0].tick_params(axis='y', labelsize=12)\naxes[1].tick_params(axis='x', labelsize=12)\naxes[1].tick_params(axis='y', labelsize=12)\n\naxes[0].set_title('Target Distribution in Training Set', fontsize=13)\naxes[1].set_title('Target Count in Training Set', fontsize=13)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:21.316488Z","iopub.execute_input":"2022-06-18T16:52:21.318246Z","iopub.status.idle":"2022-06-18T16:52:21.545655Z","shell.execute_reply.started":"2022-06-18T16:52:21.318212Z","shell.execute_reply":"2022-06-18T16:52:21.545239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (9, 6))\nax = plt.axes()\nax.set_facecolor('white')\nax = ((train.location.value_counts())[:10]).plot(kind = 'bar', color = 'lightcoral', linewidth = 2, edgecolor = 'white')\nplt.title('Location Count', fontsize = 14)\nplt.xlabel('Location', fontsize = 12)\nplt.ylabel('Count', fontsize = 12)\nax.xaxis.set_tick_params(labelsize = 12, rotation = 30)\nax.yaxis.set_tick_params(labelsize = 12)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:21.546561Z","iopub.execute_input":"2022-06-18T16:52:21.546735Z","iopub.status.idle":"2022-06-18T16:52:21.735303Z","shell.execute_reply.started":"2022-06-18T16:52:21.546715Z","shell.execute_reply":"2022-06-18T16:52:21.734387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\nN = 50\nDISASTER_TWEETS = train['target'] == 1\n# Unigrams\ndisaster_unigrams = defaultdict(int)\nnondisaster_unigrams = defaultdict(int)\n\nfor tweet in train[DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet):\n        disaster_unigrams[word] += 1\n        \nfor tweet in train[~DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet):\n        nondisaster_unigrams[word] += 1\n\ndf_disaster_unigrams = pd.DataFrame(sorted(disaster_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_unigrams = pd.DataFrame(sorted(nondisaster_unigrams.items(), key=lambda x: x[1])[::-1])\n\n# Bigrams\ndisaster_bigrams = defaultdict(int)\nnondisaster_bigrams = defaultdict(int)\n\nfor tweet in train[DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        disaster_bigrams[word] += 1\n        \nfor tweet in train[~DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        nondisaster_bigrams[word] += 1\n        \ndf_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])\n\n# Trigrams\ndisaster_trigrams = defaultdict(int)\nnondisaster_trigrams = defaultdict(int)\n\nfor tweet in train[DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        disaster_trigrams[word] += 1\n        \nfor tweet in train[~DISASTER_TWEETS]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        nondisaster_trigrams[word] += 1\n        \ndf_disaster_trigrams = pd.DataFrame(sorted(disaster_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_trigrams = pd.DataFrame(sorted(nondisaster_trigrams.items(), key=lambda x: x[1])[::-1])","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:21.736563Z","iopub.execute_input":"2022-06-18T16:52:21.736835Z","iopub.status.idle":"2022-06-18T16:52:22.054713Z","shell.execute_reply.started":"2022-06-18T16:52:21.736796Z","shell.execute_reply":"2022-06-18T16:52:22.053946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(18, 30), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_disaster_unigrams[0].values[:N], x=df_disaster_unigrams[1].values[:N], ax=axes[0], color='lightcoral')\nsns.barplot(y=df_nondisaster_unigrams[0].values[:N], x=df_nondisaster_unigrams[1].values[:N], ax=axes[1], color='lightskyblue')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common unigrams in Disaster Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common unigrams in Non-disaster Tweets', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:22.055603Z","iopub.execute_input":"2022-06-18T16:52:22.055793Z","iopub.status.idle":"2022-06-18T16:52:23.669166Z","shell.execute_reply.started":"2022-06-18T16:52:22.05577Z","shell.execute_reply":"2022-06-18T16:52:23.668025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(18,30), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_disaster_bigrams[0].values[:N], x=df_disaster_bigrams[1].values[:N], ax=axes[0], color='lightcoral')\nsns.barplot(y=df_nondisaster_bigrams[0].values[:N], x=df_nondisaster_bigrams[1].values[:N], ax=axes[1], color='lightskyblue')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common bigrams in Disaster Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common bigrams in Non-disaster Tweets', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:23.672622Z","iopub.execute_input":"2022-06-18T16:52:23.672904Z","iopub.status.idle":"2022-06-18T16:52:25.546758Z","shell.execute_reply.started":"2022-06-18T16:52:23.672874Z","shell.execute_reply":"2022-06-18T16:52:25.545991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(20, 30), dpi=100)\n\nsns.barplot(y=df_disaster_trigrams[0].values[:N], x=df_disaster_trigrams[1].values[:N], ax=axes[0], color='lightcoral')\nsns.barplot(y=df_nondisaster_trigrams[0].values[:N], x=df_nondisaster_trigrams[1].values[:N], ax=axes[1], color='lightskyblue')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=11)\n\naxes[0].set_title(f'Top {N} most common trigrams in Disaster Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common trigrams in Non-disaster Tweets', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:25.548238Z","iopub.execute_input":"2022-06-18T16:52:25.548461Z","iopub.status.idle":"2022-06-18T16:52:27.480168Z","shell.execute_reply.started":"2022-06-18T16:52:25.548431Z","shell.execute_reply":"2022-06-18T16:52:27.47951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport folium \nfrom folium import plugins \n\nnew_df = pd.DataFrame()\nnew_df['location'] = ((train['location'].value_counts())[:10]).index\nnew_df['count'] = ((train['location'].value_counts())[:10]).values\ngeolocator = Nominatim(user_agent = 'Rahil')\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds = 0.5)\nlat = {}\nlong = {}\nfor i in new_df['location']:\n    location = geocode(i)\n    lat[i] = location.latitude\n    long[i] = location.longitude\nnew_df['latitude'] = new_df['location'].map(lat)\nnew_df['longitude'] = new_df['location'].map(long)\nmap = folium.Map(location = [10.0, 10.0], tiles = 'CartoDB dark_matter', zoom_start = 1.5)\nmarkers = []\ntitle = '''<h1 align = \"center\" style = \"font-size: 15px\"><b>Top 10 Tweet Locations</b></h1>'''\nfor i, r in new_df.iterrows():\n    loss = r['count']\n    if r['count'] > 0:\n        counts = r['count'] * 0.4\n        folium.CircleMarker([float(r['latitude']), float(r['longitude'])], radius = float(counts), color = 'lightcoral', fill = True).add_to(map)\nmap.get_root().html.add_child(folium.Element(title))\nmap","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:27.481344Z","iopub.execute_input":"2022-06-18T16:52:27.481705Z","iopub.status.idle":"2022-06-18T16:52:32.62191Z","shell.execute_reply.started":"2022-06-18T16:52:27.481671Z","shell.execute_reply":"2022-06-18T16:52:32.62089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_mislabeled = train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\ndf_mislabeled.index.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:32.624948Z","iopub.execute_input":"2022-06-18T16:52:32.625162Z","iopub.status.idle":"2022-06-18T16:52:32.663536Z","shell.execute_reply.started":"2022-06-18T16:52:32.625135Z","shell.execute_reply":"2022-06-18T16:52:32.662753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### data cleaning","metadata":{}},{"cell_type":"code","source":"#drop the id column since it does not contain any valuable information\ntrain=train.drop(['id','keyword','location'],1)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:32.6649Z","iopub.execute_input":"2022-06-18T16:52:32.665126Z","iopub.status.idle":"2022-06-18T16:52:32.672329Z","shell.execute_reply.started":"2022-06-18T16:52:32.665099Z","shell.execute_reply":"2022-06-18T16:52:32.671433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:32.673383Z","iopub.execute_input":"2022-06-18T16:52:32.673589Z","iopub.status.idle":"2022-06-18T16:52:32.688782Z","shell.execute_reply.started":"2022-06-18T16:52:32.673562Z","shell.execute_reply":"2022-06-18T16:52:32.687697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:32.68976Z","iopub.execute_input":"2022-06-18T16:52:32.689959Z","iopub.status.idle":"2022-06-18T16:52:32.696675Z","shell.execute_reply.started":"2022-06-18T16:52:32.689918Z","shell.execute_reply":"2022-06-18T16:52:32.696003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk import FreqDist\nimport nltk\n\ntrain.reset_index(drop=True)\n#tokenizes the sentences and convert it to the lowercase and add those values to the list corpus\ncorpus=[]\nfor i in range(0,(len(train)-1)):\n    sentence= train['text'][i]\n    tokens = nltk.wordpunct_tokenize(sentence)\n    words = [w.lower() for w in tokens]\n    corpus.append(words)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:32.698146Z","iopub.execute_input":"2022-06-18T16:52:32.698396Z","iopub.status.idle":"2022-06-18T16:52:32.834722Z","shell.execute_reply.started":"2022-06-18T16:52:32.698357Z","shell.execute_reply":"2022-06-18T16:52:32.833994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a list from above list of lists\nfrom pandas.core.common import flatten\nwc=list(flatten(corpus))\nwc[:10]","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:32.836866Z","iopub.execute_input":"2022-06-18T16:52:32.837195Z","iopub.status.idle":"2022-06-18T16:52:32.9698Z","shell.execute_reply.started":"2022-06-18T16:52:32.837157Z","shell.execute_reply":"2022-06-18T16:52:32.969122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dist = FreqDist(wc)\nwordtotal=0\nfor word in dist.keys():\n    wordtotal=wordtotal+ dist[word]\n\nprint(\"total words with punctuations:\",wordtotal)\nprint(\"total unique words           :\",len(dist))","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:32.970613Z","iopub.execute_input":"2022-06-18T16:52:32.970772Z","iopub.status.idle":"2022-06-18T16:52:33.086807Z","shell.execute_reply.started":"2022-06-18T16:52:32.970751Z","shell.execute_reply":"2022-06-18T16:52:33.086204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing puntuation marks. taking only the words.\nimport re\ndef alphaFreqDist (words):\n    adist =FreqDist()\n    pattern = re.compile('.*[^a-z].*')\n    for word in words:\n        if not pattern.match(word):\n            adist[word] += 1\n    return adist\n\nadist= alphaFreqDist(wc)\nwordtot=0\nfor word in adist.keys():\n    wordtot=wordtot+ adist[word]\n\nprint(\"total words without punctuations:\",wordtot)\nprint(\"total unique words              :\",len(adist))","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:33.088572Z","iopub.execute_input":"2022-06-18T16:52:33.089203Z","iopub.status.idle":"2022-06-18T16:52:33.369337Z","shell.execute_reply.started":"2022-06-18T16:52:33.089172Z","shell.execute_reply":"2022-06-18T16:52:33.368229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#removing stopwords from the corpus\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nwithout_stopwords_wc = [t for t in wc if not t in stopwords.words(\"english\")]","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:52:33.372156Z","iopub.execute_input":"2022-06-18T16:52:33.372359Z","iopub.status.idle":"2022-06-18T16:53:00.256069Z","shell.execute_reply.started":"2022-06-18T16:52:33.372332Z","shell.execute_reply":"2022-06-18T16:53:00.255167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def withoutStopwordsDist (words):\n    wdist =FreqDist()\n    pattern = re.compile('.*[^a-z].*')\n    for word in words:\n        if not pattern.match(word):\n            wdist[word] += 1\n    return wdist\n\nwdist= withoutStopwordsDist(without_stopwords_wc)\nwithout_sw=list(wdist.keys())\nwordtotws=0\nfor word in wdist.keys():\n    wordtotws=wordtotws+ wdist[word]\n\nprint(\"total words without stopwords   :\",wordtotws)\nprint(\"total unique words              :\",len(wdist))","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:00.257014Z","iopub.execute_input":"2022-06-18T16:53:00.257204Z","iopub.status.idle":"2022-06-18T16:53:00.45019Z","shell.execute_reply.started":"2022-06-18T16:53:00.257177Z","shell.execute_reply":"2022-06-18T16:53:00.449382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lemmatization\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n\nlemmatizer = WordNetLemmatizer()\nlemmawords=[]\nfor w in range(1,len(without_stopwords_wc)):\n    lemmaword= lemmatizer.lemmatize(without_stopwords_wc[w-1])\n    lemmawords.append(lemmaword)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:00.451125Z","iopub.execute_input":"2022-06-18T16:53:00.451312Z","iopub.status.idle":"2022-06-18T16:53:01.319168Z","shell.execute_reply.started":"2022-06-18T16:53:00.451286Z","shell.execute_reply":"2022-06-18T16:53:01.318285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmaDist (words):\n    ldist =FreqDist()\n    pattern = re.compile('.*[^a-z].*')\n    for word in words:\n        if not pattern.match(word):\n            ldist[word] += 1\n    return ldist\nldist= lemmaDist(lemmawords)\nwordtotle=0\nfor word in ldist.keys():\n    wordtotle=wordtotle+ ldist[word]\n\nprint(\"total words lemmatized:\",wordtotle)\nprint(\"total unique words    :\",len(ldist))","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:01.320513Z","iopub.execute_input":"2022-06-18T16:53:01.320724Z","iopub.status.idle":"2022-06-18T16:53:01.505052Z","shell.execute_reply.started":"2022-06-18T16:53:01.320697Z","shell.execute_reply":"2022-06-18T16:53:01.504324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adding words count to the dataset. this can be used as feture to increase model accuracy later\ntrain['wordscount'] = train['text'].apply(lambda x:len(str(x).split())) \ntrain['text']= [w.lower() for w in train['text']]\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:01.506011Z","iopub.execute_input":"2022-06-18T16:53:01.506185Z","iopub.status.idle":"2022-06-18T16:53:01.53581Z","shell.execute_reply.started":"2022-06-18T16:53:01.506161Z","shell.execute_reply":"2022-06-18T16:53:01.534989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('maximum number of words in a sentence :',max(train['wordscount']))\nprint('minimum number of words in a sentence :',min(train['wordscount']))","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:01.536893Z","iopub.execute_input":"2022-06-18T16:53:01.537119Z","iopub.status.idle":"2022-06-18T16:53:01.544256Z","shell.execute_reply.started":"2022-06-18T16:53:01.537087Z","shell.execute_reply":"2022-06-18T16:53:01.543536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this function removes stopwords from the reviews\nimport string\ndef stopwordremover(review):\n    stop_words = stopwords.words('english')\n    review = review.split()\n    review = \" \".join([word for word in review if not word in stop_words])\n    review = review.translate(str.maketrans('', '', string.punctuation))\n    return review","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:01.545344Z","iopub.execute_input":"2022-06-18T16:53:01.545603Z","iopub.status.idle":"2022-06-18T16:53:01.557434Z","shell.execute_reply.started":"2022-06-18T16:53:01.545573Z","shell.execute_reply":"2022-06-18T16:53:01.557018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use apply function to remove stopwords from the dataframe column\ntrain['text']= train['text'].apply(stopwordremover)\n#after removing stop words then count the number of words in the review.\ntrain['text']= [w.lower() for w in train['text']]\ntrain['wordscount1'] = train['text'].apply(lambda x:len(str(x).split())) \ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:01.587753Z","iopub.execute_input":"2022-06-18T16:53:01.588021Z","iopub.status.idle":"2022-06-18T16:53:03.386299Z","shell.execute_reply.started":"2022-06-18T16:53:01.587992Z","shell.execute_reply":"2022-06-18T16:53:03.385502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('maximum number of words in a sentence :',max(train['wordscount1']))\nprint('minimum number of words in a sentence :',min(train['wordscount1']))","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:03.38721Z","iopub.execute_input":"2022-06-18T16:53:03.387389Z","iopub.status.idle":"2022-06-18T16:53:03.393216Z","shell.execute_reply.started":"2022-06-18T16:53:03.387365Z","shell.execute_reply":"2022-06-18T16:53:03.392584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#first try to model those data only using words in the reviews. so to do that we can use vectorizor and tfidf vectorizor functions.\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:03.394098Z","iopub.execute_input":"2022-06-18T16:53:03.394289Z","iopub.status.idle":"2022-06-18T16:53:03.40495Z","shell.execute_reply.started":"2022-06-18T16:53:03.394263Z","shell.execute_reply":"2022-06-18T16:53:03.404304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(train['text'])","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:03.405951Z","iopub.execute_input":"2022-06-18T16:53:03.406143Z","iopub.status.idle":"2022-06-18T16:53:03.55537Z","shell.execute_reply.started":"2022-06-18T16:53:03.406118Z","shell.execute_reply":"2022-06-18T16:53:03.554752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=train['target']","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:03.556541Z","iopub.execute_input":"2022-06-18T16:53:03.556799Z","iopub.status.idle":"2022-06-18T16:53:03.561019Z","shell.execute_reply.started":"2022-06-18T16:53:03.556732Z","shell.execute_reply":"2022-06-18T16:53:03.560464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import LinearSVC\n\n#libraries for model evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import auc","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:03.561906Z","iopub.execute_input":"2022-06-18T16:53:03.562329Z","iopub.status.idle":"2022-06-18T16:53:03.574786Z","shell.execute_reply.started":"2022-06-18T16:53:03.562305Z","shell.execute_reply":"2022-06-18T16:53:03.573983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#divide the dataset into train set and test set \nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:03.575995Z","iopub.execute_input":"2022-06-18T16:53:03.576753Z","iopub.status.idle":"2022-06-18T16:53:03.589429Z","shell.execute_reply.started":"2022-06-18T16:53:03.576714Z","shell.execute_reply":"2022-06-18T16:53:03.588864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)\nprint(x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:03.590379Z","iopub.execute_input":"2022-06-18T16:53:03.590571Z","iopub.status.idle":"2022-06-18T16:53:03.600564Z","shell.execute_reply.started":"2022-06-18T16:53:03.590548Z","shell.execute_reply":"2022-06-18T16:53:03.599644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rc =RidgeClassifier()\nmodel0=rc.fit(x_train, y_train)\nprint(\"train accuracy:\",model0.score(x_train, y_train),\"\\n\",\"test accuracy:\",model0.score(x_test,y_test))\n\nrcpred = rc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for ridge classifier\")\nprint(classification_report(y_test,rcpred))\nprint(\"\\n\")\nprint(\"confusion matrix for ridge classifier\")\nConfusionMatrixDisplay.from_estimator(rc, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:03.60198Z","iopub.execute_input":"2022-06-18T16:53:03.602741Z","iopub.status.idle":"2022-06-18T16:53:03.896404Z","shell.execute_reply.started":"2022-06-18T16:53:03.602706Z","shell.execute_reply":"2022-06-18T16:53:03.895562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#logistic regression\nlr = LogisticRegression(max_iter=2000,penalty='l2')\nmodel1=lr.fit(x_train, y_train)\nprint(\"train accuracy:\",model1.score(x_train, y_train),\"\\n\",\"test accuracy:\",model1.score(x_test,y_test))\nlrpred = lr.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for logistic regression\")\nprint(classification_report(y_test,lrpred))\nprint(\"\\n\")\nprint(\"confusion matrix for logistic regression\")\nConfusionMatrixDisplay.from_estimator(lr, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:03.897446Z","iopub.execute_input":"2022-06-18T16:53:03.898053Z","iopub.status.idle":"2022-06-18T16:53:04.551492Z","shell.execute_reply.started":"2022-06-18T16:53:03.898023Z","shell.execute_reply":"2022-06-18T16:53:04.550856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#support vector machines\nsvm =LinearSVC()\nmodel2=svm.fit(x_train, y_train)\nprint(\"train accuracy:\",model2.score(x_train, y_train),\"\\n\",\"test accuracy:\",model2.score(x_test,y_test))\nsvmpred = svm.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for support vector machines \")\nprint(classification_report(y_test,svmpred))\nprint(\"\\n\")\nprint(\"confusion matrix for support vector machines\")\nConfusionMatrixDisplay.from_estimator(svm, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:04.552603Z","iopub.execute_input":"2022-06-18T16:53:04.552848Z","iopub.status.idle":"2022-06-18T16:53:04.840717Z","shell.execute_reply.started":"2022-06-18T16:53:04.552818Z","shell.execute_reply":"2022-06-18T16:53:04.840125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#decision tree classifier\ndt=DecisionTreeClassifier()\nmodel3=dt.fit(x_train, y_train)\nprint(\"train accuracy:\",model3.score(x_train, y_train),\"\\n\",\"test accuracy:\",model3.score(x_test,y_test))\n\ndtpred = dt.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for decision tree classifier\")\nprint(classification_report(y_test,dtpred))\nprint(\"\\n\")\nprint(\"confusion matrix for decision tree classifier\")\nConfusionMatrixDisplay.from_estimator(dt, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:04.841943Z","iopub.execute_input":"2022-06-18T16:53:04.842324Z","iopub.status.idle":"2022-06-18T16:53:07.986578Z","shell.execute_reply.started":"2022-06-18T16:53:04.842291Z","shell.execute_reply":"2022-06-18T16:53:07.986086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#random forest classifier\nrf=RandomForestClassifier(random_state=1234)\nmodel4=rf.fit(x_train, y_train)\nprint(\"train accuracy:\",model4.score(x_train, y_train),\"\\n\",\"test accuracy:\",model4.score(x_test,y_test))\n\nrfpred = rf.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for random forest classifier\")\nprint(classification_report(y_test,rfpred))\nprint(\"\\n\")\nprint(\"confusion matrix for random forest classifier\")\nConfusionMatrixDisplay.from_estimator(rf, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:07.987651Z","iopub.execute_input":"2022-06-18T16:53:07.988359Z","iopub.status.idle":"2022-06-18T16:53:32.393532Z","shell.execute_reply.started":"2022-06-18T16:53:07.988323Z","shell.execute_reply":"2022-06-18T16:53:32.393052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gradient boost classifier \ngbm=GradientBoostingClassifier()\nmodel5=gbm.fit(x_train, y_train)\nprint(\"train accuracy:\",model5.score(x_train, y_train),\"\\n\",\"test accuracy:\",model5.score(x_test,y_test))\n\ngbmpred = gbm.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for gradient boosting classifier\")\nprint(classification_report(y_test,gbmpred))\nprint(\"\\n\")\nprint(\"confusion matrix for gradient boosting classifier\")\nConfusionMatrixDisplay.from_estimator(gbm, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:32.394769Z","iopub.execute_input":"2022-06-18T16:53:32.395075Z","iopub.status.idle":"2022-06-18T16:53:41.775805Z","shell.execute_reply.started":"2022-06-18T16:53:32.395048Z","shell.execute_reply":"2022-06-18T16:53:41.775282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adaboost classifier \nada=AdaBoostClassifier()\nmodel6=ada.fit(x_train, y_train)\nprint(\"train accuracy:\",model6.score(x_train, y_train),\"\\n\",\"test accuracy:\",model6.score(x_test,y_test))\n\nadapred = ada.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(y_test,adapred))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\nConfusionMatrixDisplay.from_estimator(ada, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:41.776966Z","iopub.execute_input":"2022-06-18T16:53:41.777317Z","iopub.status.idle":"2022-06-18T16:53:46.456399Z","shell.execute_reply.started":"2022-06-18T16:53:41.777286Z","shell.execute_reply":"2022-06-18T16:53:46.455495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extreme gradient boost classifier\nxgb = XGBClassifier(random_state=1234)\nmodel7=xgb.fit(x_train, y_train)\nprint(\"train accuracy:\",model7.score(x_train, y_train),\"\\n\",\"test accuracy:\",model7.score(x_test,y_test))\nxgbpred = xgb.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for extreme gradient boosting classifier\")\nprint(classification_report(y_test,xgbpred))\nprint(\"\\n\")\nprint(\"confusion matrix for extreme gradient boosting classifier\")\nConfusionMatrixDisplay.from_estimator(xgb, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:46.457658Z","iopub.execute_input":"2022-06-18T16:53:46.457874Z","iopub.status.idle":"2022-06-18T16:53:47.644084Z","shell.execute_reply.started":"2022-06-18T16:53:46.457844Z","shell.execute_reply":"2022-06-18T16:53:47.643085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extra tree classifier\nextree = ExtraTreesClassifier()\nmodel8=extree.fit(x_train, y_train)\nprint(\"train accuracy:\",model8.score(x_train, y_train),\"\\n\",\"test accuracy:\",model8.score(x_test,y_test))\n\nextpred = extree.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for extra tree classifier\")\nprint(classification_report(y_test,extpred))\nprint(\"\\n\")\nprint(\"confusion matrix for extra tree classifier\")\nConfusionMatrixDisplay.from_estimator(extree, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:53:47.645285Z","iopub.execute_input":"2022-06-18T16:53:47.645495Z","iopub.status.idle":"2022-06-18T16:54:36.799056Z","shell.execute_reply.started":"2022-06-18T16:53:47.645465Z","shell.execute_reply":"2022-06-18T16:54:36.798153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# voting classifer\nfrom sklearn.ensemble import VotingClassifier\nclf1 = RandomForestClassifier()\nclf2 = LogisticRegression(max_iter=2000,penalty='l2')\n\nvc = VotingClassifier(estimators=[('ada', clf1),('lr', clf2)], voting='soft')\nmodel9=vc.fit(x_train, y_train)\nprint(\"train accuracy:\",model9.score(x_train, y_train),\"\\n\",\"test accuracy:\",model9.score(x_test,y_test))\n\nvcpred = vc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for voting classifier\")\nprint(classification_report(y_test,vcpred))\nprint(\"\\n\")\nprint(\"confusion matrix for voting classifier\")\nConfusionMatrixDisplay.from_estimator(vc, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:54:36.800031Z","iopub.execute_input":"2022-06-18T16:54:36.800188Z","iopub.status.idle":"2022-06-18T16:54:58.780604Z","shell.execute_reply.started":"2022-06-18T16:54:36.800166Z","shell.execute_reply":"2022-06-18T16:54:58.779928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stacking classifier \nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nestimators = [('rf', RandomForestClassifier()),('ext', make_pipeline(LogisticRegression(max_iter=2000,penalty='l2')))]\nsc= StackingClassifier( estimators=estimators)\n\nmodel10=sc.fit(x_train, y_train)\nprint(\"train accuracy:\",model10.score(x_train, y_train),\"\\n\",\"test accuracy:\",model10.score(x_test,y_test))\n\nscpred = sc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for stacking classifier\")\nprint(classification_report(y_test,scpred))\nprint(\"\\n\")\nprint(\"confusion matrix for stacking classifier\")\nConfusionMatrixDisplay.from_estimator(sc, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:54:58.781691Z","iopub.execute_input":"2022-06-18T16:54:58.781903Z","iopub.status.idle":"2022-06-18T16:56:59.070675Z","shell.execute_reply.started":"2022-06-18T16:54:58.78187Z","shell.execute_reply":"2022-06-18T16:56:59.069956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install catboost","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:56:59.071542Z","iopub.execute_input":"2022-06-18T16:56:59.07174Z","iopub.status.idle":"2022-06-18T16:57:06.65233Z","shell.execute_reply.started":"2022-06-18T16:56:59.071713Z","shell.execute_reply":"2022-06-18T16:57:06.651225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\ncc = CatBoostClassifier(silent=True )\nmodel11=cc.fit(x_train, y_train)\nprint(\"train accuracy:\",model11.score(x_train, y_train),\"\\n\",\"test accuracy:\",model11.score(x_test,y_test))\n\nccpred = cc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for cat boost classifier\")\nprint(classification_report(y_test,ccpred))\nprint(\"\\n\")\nprint(\"confusion matrix for cat boost classifier\")\nConfusionMatrixDisplay.from_estimator(cc, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:57:06.654165Z","iopub.execute_input":"2022-06-18T16:57:06.654486Z","iopub.status.idle":"2022-06-18T16:57:40.809363Z","shell.execute_reply.started":"2022-06-18T16:57:06.654439Z","shell.execute_reply":"2022-06-18T16:57:40.808672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:57:40.810362Z","iopub.execute_input":"2022-06-18T16:57:40.810548Z","iopub.status.idle":"2022-06-18T16:57:40.813962Z","shell.execute_reply.started":"2022-06-18T16:57:40.810522Z","shell.execute_reply":"2022-06-18T16:57:40.813502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting tfidf vlaues for feedback\nvectorizertf = TfidfVectorizer()\nXt = vectorizertf.fit_transform(train['text'])","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:57:40.814848Z","iopub.execute_input":"2022-06-18T16:57:40.815343Z","iopub.status.idle":"2022-06-18T16:57:40.945427Z","shell.execute_reply.started":"2022-06-18T16:57:40.815313Z","shell.execute_reply":"2022-06-18T16:57:40.944991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dividing the dataset to train and test \nxtrain, xtest, ytrain, ytest = train_test_split(Xt, y, test_size=0.2, random_state=1234)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:57:40.946483Z","iopub.execute_input":"2022-06-18T16:57:40.946701Z","iopub.status.idle":"2022-06-18T16:57:40.954684Z","shell.execute_reply.started":"2022-06-18T16:57:40.946673Z","shell.execute_reply":"2022-06-18T16:57:40.953785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rc =RidgeClassifier()\nmodel0=rc.fit(x_train, y_train)\nprint(\"train accuracy:\",model0.score(x_train, y_train),\"\\n\",\"test accuracy:\",model0.score(x_test,y_test))\n\nrcpred = rc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for ridge classifier\")\nprint(classification_report(y_test,rcpred))\nprint(\"\\n\")\nprint(\"confusion matrix for ridge classifier\")\nConfusionMatrixDisplay.from_estimator(rc, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:57:40.955812Z","iopub.execute_input":"2022-06-18T16:57:40.955967Z","iopub.status.idle":"2022-06-18T16:57:41.224858Z","shell.execute_reply.started":"2022-06-18T16:57:40.955946Z","shell.execute_reply":"2022-06-18T16:57:41.223946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#logistic regression\nlr = LogisticRegression(max_iter=2000,penalty='l2')\nmodel1=lr.fit(x_train, y_train)\nprint(\"train accuracy:\",model1.score(x_train, y_train),\"\\n\",\"test accuracy:\",model1.score(x_test,y_test))\nlrpred = lr.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for logistic regression\")\nprint(classification_report(y_test,lrpred))\nprint(\"\\n\")\nprint(\"confusion matrix for logistic regression\")\nConfusionMatrixDisplay.from_estimator(lr, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:57:41.226713Z","iopub.execute_input":"2022-06-18T16:57:41.226977Z","iopub.status.idle":"2022-06-18T16:57:41.866511Z","shell.execute_reply.started":"2022-06-18T16:57:41.226946Z","shell.execute_reply":"2022-06-18T16:57:41.865594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#support vector machines\nsvm =LinearSVC()\nmodel2=svm.fit(x_train, y_train)\nprint(\"train accuracy:\",model2.score(x_train, y_train),\"\\n\",\"test accuracy:\",model2.score(x_test,y_test))\nsvmpred = svm.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for support vector machines \")\nprint(classification_report(y_test,svmpred))\nprint(\"\\n\")\nprint(\"confusion matrix for support vector machines\")\nConfusionMatrixDisplay.from_estimator(svm, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:57:41.867799Z","iopub.execute_input":"2022-06-18T16:57:41.868036Z","iopub.status.idle":"2022-06-18T16:57:42.182902Z","shell.execute_reply.started":"2022-06-18T16:57:41.868002Z","shell.execute_reply":"2022-06-18T16:57:42.182167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#decision tree classifier\ndt=DecisionTreeClassifier()\nmodel3=dt.fit(x_train, y_train)\nprint(\"train accuracy:\",model3.score(x_train, y_train),\"\\n\",\"test accuracy:\",model3.score(x_test,y_test))\n\ndtpred = dt.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for decision tree classifier\")\nprint(classification_report(y_test,dtpred))\nprint(\"\\n\")\nprint(\"confusion matrix for decision tree classifier\")\nConfusionMatrixDisplay.from_estimator(dt, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:57:42.184196Z","iopub.execute_input":"2022-06-18T16:57:42.184375Z","iopub.status.idle":"2022-06-18T16:57:44.346743Z","shell.execute_reply.started":"2022-06-18T16:57:42.184352Z","shell.execute_reply":"2022-06-18T16:57:44.345982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#random forest classifier\nrf=RandomForestClassifier(random_state=1234)\nmodel4=rf.fit(x_train, y_train)\nprint(\"train accuracy:\",model4.score(x_train, y_train),\"\\n\",\"test accuracy:\",model4.score(x_test,y_test))\n\nrfpred = rf.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for random forest classifier\")\nprint(classification_report(y_test,rfpred))\nprint(\"\\n\")\nprint(\"confusion matrix for random forest classifier\")\nConfusionMatrixDisplay.from_estimator(rf, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:57:44.347739Z","iopub.execute_input":"2022-06-18T16:57:44.347935Z","iopub.status.idle":"2022-06-18T16:58:15.059011Z","shell.execute_reply.started":"2022-06-18T16:57:44.347902Z","shell.execute_reply":"2022-06-18T16:58:15.058207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gradient boost classifier \ngbm=GradientBoostingClassifier()\nmodel5=gbm.fit(x_train, y_train)\nprint(\"train accuracy:\",model5.score(x_train, y_train),\"\\n\",\"test accuracy:\",model5.score(x_test,y_test))\n\ngbmpred = gbm.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for gradient boosting classifier\")\nprint(classification_report(y_test,gbmpred))\nprint(\"\\n\")\nprint(\"confusion matrix for gradient boosting classifier\")\nConfusionMatrixDisplay.from_estimator(gbm, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:58:15.059865Z","iopub.execute_input":"2022-06-18T16:58:15.060048Z","iopub.status.idle":"2022-06-18T16:58:24.083186Z","shell.execute_reply.started":"2022-06-18T16:58:15.060023Z","shell.execute_reply":"2022-06-18T16:58:24.081855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adaboost classifier \nada=AdaBoostClassifier()\nmodel6=ada.fit(x_train, y_train)\nprint(\"train accuracy:\",model6.score(x_train, y_train),\"\\n\",\"test accuracy:\",model6.score(x_test,y_test))\n\nadapred = ada.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(y_test,adapred))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\nConfusionMatrixDisplay.from_estimator(ada, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:58:24.084318Z","iopub.execute_input":"2022-06-18T16:58:24.084505Z","iopub.status.idle":"2022-06-18T16:58:28.818432Z","shell.execute_reply.started":"2022-06-18T16:58:24.084479Z","shell.execute_reply":"2022-06-18T16:58:28.817479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extreme gradient boost classifier\nxgb = XGBClassifier(random_state=1234)\nmodel7=xgb.fit(x_train, y_train)\nprint(\"train accuracy:\",model7.score(x_train, y_train),\"\\n\",\"test accuracy:\",model7.score(x_test,y_test))\nxgbpred = xgb.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for extreme gradient boosting classifier\")\nprint(classification_report(y_test,xgbpred))\nprint(\"\\n\")\nprint(\"confusion matrix for extreme gradient boosting classifier\")\nConfusionMatrixDisplay.from_estimator(xgb, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:58:28.819646Z","iopub.execute_input":"2022-06-18T16:58:28.819844Z","iopub.status.idle":"2022-06-18T16:58:29.974639Z","shell.execute_reply.started":"2022-06-18T16:58:28.819817Z","shell.execute_reply":"2022-06-18T16:58:29.973944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extra tree classifier\nextree = ExtraTreesClassifier()\nmodel8=extree.fit(x_train, y_train)\nprint(\"train accuracy:\",model8.score(x_train, y_train),\"\\n\",\"test accuracy:\",model8.score(x_test,y_test))\n\nextpred = extree.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for extra tree classifier\")\nprint(classification_report(y_test,extpred))\nprint(\"\\n\")\nprint(\"confusion matrix for extra tree classifier\")\nConfusionMatrixDisplay.from_estimator(extree, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:58:29.975575Z","iopub.execute_input":"2022-06-18T16:58:29.975779Z","iopub.status.idle":"2022-06-18T16:59:24.233416Z","shell.execute_reply.started":"2022-06-18T16:58:29.975755Z","shell.execute_reply":"2022-06-18T16:59:24.232528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# voting classifer\nfrom sklearn.ensemble import VotingClassifier\nclf1 = AdaBoostClassifier()\nclf2 = LogisticRegression(max_iter=2000,penalty='l2')\n\nvc = VotingClassifier(estimators=[('ada', clf1),('lr', clf2)], voting='soft')\nmodel9=vc.fit(x_train, y_train)\nprint(\"train accuracy:\",model9.score(x_train, y_train),\"\\n\",\"test accuracy:\",model9.score(x_test,y_test))\n\nvcpred = vc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for voting classifier\")\nprint(classification_report(y_test,vcpred))\nprint(\"\\n\")\nprint(\"confusion matrix for voting classifier\")\nConfusionMatrixDisplay.from_estimator(vc, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:59:24.235621Z","iopub.execute_input":"2022-06-18T16:59:24.235885Z","iopub.status.idle":"2022-06-18T16:59:29.230805Z","shell.execute_reply.started":"2022-06-18T16:59:24.235849Z","shell.execute_reply":"2022-06-18T16:59:29.230228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stacking classifier \nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nestimators = [('ada', AdaBoostClassifier()),('ext', make_pipeline(LogisticRegression(max_iter=2000,penalty='l2')))]\nsc= StackingClassifier( estimators=estimators)\n\nmodel10=sc.fit(x_train, y_train)\nprint(\"train accuracy:\",model10.score(x_train, y_train),\"\\n\",\"test accuracy:\",model10.score(x_test,y_test))\n\nscpred = sc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for stacking classifier\")\nprint(classification_report(y_test,scpred))\nprint(\"\\n\")\nprint(\"confusion matrix for stacking classifier\")\nConfusionMatrixDisplay.from_estimator(sc, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:59:29.232251Z","iopub.execute_input":"2022-06-18T16:59:29.232861Z","iopub.status.idle":"2022-06-18T16:59:50.676053Z","shell.execute_reply.started":"2022-06-18T16:59:29.232827Z","shell.execute_reply":"2022-06-18T16:59:50.675371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\ncc = CatBoostClassifier(silent=True )\nmodel11=cc.fit(x_train, y_train)\nprint(\"train accuracy:\",model11.score(x_train, y_train),\"\\n\",\"test accuracy:\",model11.score(x_test,y_test))\n\nccpred = cc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for cat boost classifier\")\nprint(classification_report(y_test,ccpred))\nprint(\"\\n\")\nprint(\"confusion matrix for cat boost classifier\")\nConfusionMatrixDisplay.from_estimator(cc, x_test, y_test,cmap=\"Blues\")","metadata":{"execution":{"iopub.status.busy":"2022-06-18T16:59:50.677281Z","iopub.execute_input":"2022-06-18T16:59:50.677667Z","iopub.status.idle":"2022-06-18T17:00:24.834144Z","shell.execute_reply.started":"2022-06-18T16:59:50.677618Z","shell.execute_reply":"2022-06-18T17:00:24.833388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stacking classifier with random forest classifier and logistic regression model has the best results for the given problem. The precision, recall and classification accuracy values are higher than all the other models. ","metadata":{}}]}