{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport re\nfrom bs4 import BeautifulSoup\nimport random","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install bert-for-tf2\n!pip install sentencepiece","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/nlp-getting-started/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_tweet(tweet):\n    tweet=BeautifulSoup(tweet,'lxml').get_text()\n    tweet= re.sub(r'@[A-Za-z0-9]+',' ',tweet)\n    tweet=re.sub(r'https?://[A-Za-z0-9./]+',' ',tweet)\n    tweet=re.sub(r'[^a-zA-Z.!?]',' ',tweet)\n    tweet=re.sub(r' +',' ',tweet)\n    return tweet\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clean=data['text'].apply(lambda text:clean_tweet(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_labels=data.target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fulltokenizer=bert.bert_tokenization.FullTokenizer\nbert_layer=hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer=fulltokenizer(vocab_file,do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(sent):\n    return ['[CLS]']+tokenizer.tokenize(sent)+['[SEP]']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_inputs=[encode_sentence(sent) for sent in data_clean]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ids(token):\n    return tokenizer.convert_tokens_to_ids(token)\n\ndef get_masks(token):\n    return np.char.not_equal(token,['PAD']).astype('int')\n\ndef get_segments(token):\n    seg_ids=[]\n    current_id=0\n    for tok in token:\n        seg_ids.append(current_id)\n        if tok=='[SEP]':\n            current_id=1-current_id\n    return seg_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_with_len = [[sent, data_labels[i], len(sent)]\n                 for i, sent in enumerate(data_inputs)]\nrandom.shuffle(data_with_len)\ndata_with_len.sort(key=lambda x: x[2])\nsorted_all = [([get_ids(sent_lab[0]),\n                get_masks(sent_lab[0]),\n                get_segments(sent_lab[0])],\n               sent_lab[1])\n              for sent_lab in data_with_len if sent_lab[2] > 7]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A list is a type of iterator so it can be used as generator for a dataset\nall_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n                                             output_types=(tf.int32, tf.int32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nall_batched = all_dataset.padded_batch(BATCH_SIZE,\n                                       padded_shapes=((3, None), ()),\n                                       padding_values=(0, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\nNB_BATCHES_TEST = NB_BATCHES // 10\nall_batched.shuffle(NB_BATCHES)\ntest_dataset = all_batched.take(NB_BATCHES_TEST)\ntrain_dataset = all_batched.skip(NB_BATCHES_TEST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DCNNBERTEmbedding(tf.keras.Model):\n    \n    def __init__(self,\n                 nb_filters=50,\n                 FFN_units=512,\n                 nb_classes=2,\n                 dropout_rate=0.1,\n                 name=\"dcnn\"):\n        super(DCNNBERTEmbedding, self).__init__(name=name)\n        \n        self.bert_layer = hub.KerasLayer(\n            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n            trainable=False)\n\n        self.bigram = layers.Conv1D(filters=nb_filters,\n                                    kernel_size=2,\n                                    padding=\"valid\",\n                                    activation=\"relu\")\n        self.trigram = layers.Conv1D(filters=nb_filters,\n                                     kernel_size=3,\n                                     padding=\"valid\",\n                                     activation=\"relu\")\n        self.fourgram = layers.Conv1D(filters=nb_filters,\n                                      kernel_size=4,\n                                      padding=\"valid\",\n                                      activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D()\n        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        if nb_classes == 2:\n            self.last_dense = layers.Dense(units=1,\n                                           activation=\"sigmoid\")\n        else:\n            self.last_dense = layers.Dense(units=nb_classes,\n                                           activation=\"softmax\")\n    \n    def embed_with_bert(self, all_tokens):\n        _, embs = self.bert_layer([all_tokens[:, 0, :],\n                                   all_tokens[:, 1, :],\n                                   all_tokens[:, 2, :]])\n        return embs\n\n    def call(self, inputs, training):\n        x = self.embed_with_bert(inputs)\n\n        print(x.shape)\n\n        x_1 = self.bigram(x)\n        x_1 = self.pool(x_1)\n        x_2 = self.trigram(x)\n        x_2 = self.pool(x_2)\n        x_3 = self.fourgram(x)\n        x_3 = self.pool(x_3)\n        \n        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n        merged = self.dense_1(merged)\n        merged = self.dropout(merged, training)\n        output = self.last_dense(merged)\n        \n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_FILTERS = 100\nFFN_UNITS = 256\nNB_CLASSES = 2\n\nDROPOUT_RATE = 0.2\n\nBATCH_SIZE = 32\nNB_EPOCHS = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n                         FFN_units=FFN_UNITS,\n                         nb_classes=NB_CLASSES,\n                         dropout_rate=DROPOUT_RATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if NB_CLASSES == 2:\n    Dcnn.compile(loss=\"binary_crossentropy\",\n                 optimizer=\"adam\",\n                 metrics=[\"accuracy\"])\nelse:\n    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n                 optimizer=\"adam\",\n                 metrics=[\"sparse_categorical_accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_path = \"/kaggle/working/\"\n\nckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print(\"Latest checkpoint restored!!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyCustomCallback(tf.keras.callbacks.Callback):\n\n    def on_epoch_end(self, epoch, logs=None):\n        ckpt_manager.save()\n        print(\"Checkpoint saved at {}.\".format(checkpoint_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dcnn.fit(train_dataset,\n         epochs=NB_EPOCHS,\n         validation_data=(test_dataset),\n         callbacks=[MyCustomCallback()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_prediction(sentence):\n    sentence=clean_tweet(sentence)\n    tokens = encode_sentence(sentence)\n\n    input_ids = get_ids(tokens)\n    input_mask = get_masks(tokens)\n    segment_ids = get_segments(tokens)\n\n    inputs = tf.stack(\n        [tf.cast(input_ids, dtype=tf.int32),\n         tf.cast(input_mask, dtype=tf.int32),\n         tf.cast(segment_ids, dtype=tf.int32)],\n         axis=0)\n    inputs = tf.expand_dims(inputs, 0) # simulates a batch\n\n    output = Dcnn(inputs, training=False)\n\n    sentiment = math.floor(output*2)\n\n    if sentiment == 0:\n        print(\"Output of the model: {}\\nPredicted sentiment: negative\".format(\n            output))\n    elif sentiment == 1:\n        print(\"Output of the model: {}\\nPredicted sentiment: positive\".format(\n            output))\n    return sentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds=[]\nfor i in test['text']:\n    preds.append(get_prediction(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target']=preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}