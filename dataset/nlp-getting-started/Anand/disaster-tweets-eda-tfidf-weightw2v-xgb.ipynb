{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"text-align:center;font-size:30px;\" > Disaster Tweets </h1>"},{"metadata":{"colab_type":"text","id":"LRzmxjKxs5Vw"},"cell_type":"markdown","source":"<h1> Description </h1>"},{"metadata":{"colab_type":"text","id":"1nlaIYe9s5Vx"},"cell_type":"markdown","source":"<p>twitter is a place where users post and interact with messages known as \"tweets\". tweets are limited to 280 characters. this tweets could be image/words.\n</p>"},{"metadata":{"colab_type":"text","id":"wdWP5SdFs5Vy"},"cell_type":"markdown","source":"__ Problem Statement __\n- Twitter has become an important communication channel in times of emergency.it’s not always clear whether a person’s words are actually announcing a disaster.\n- Now we are tasked with predict whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0."},{"metadata":{"colab_type":"text","id":"rty1PZv3s5V_"},"cell_type":"markdown","source":"<h1> Data Overview </h1>\n<p> \n- Data will be in a file Train.csv & test .csv <br>\n- Train.csv contains 5 columns : id, text, location, keyword, target <br>\n- Number of rows in Train.csv = 7613\n<p>"},{"metadata":{"colab_type":"text","id":"-gu8pAt3s5WB"},"cell_type":"markdown","source":"Files\n\n    train.csv - the training set\n    test.csv - the test set\n    sample_submission.csv - a sample submission file\n\nColumns\n\n    id - a unique identifier for each tweet\n    text - the text of the tweet\n    location - the location the tweet was sent from (may be blank)\n    keyword - a particular keyword from the tweet (may be blank)\n    target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"},{"metadata":{"colab_type":"text","id":"JfBn0LYPs5WI"},"cell_type":"markdown","source":"<h2> Type of Machine Leaning Problem </h2>"},{"metadata":{"colab_type":"text","id":"QEqiUD_Ps5WJ"},"cell_type":"markdown","source":"<p> It is a binary classification problem, for a given datapoints or tweets we need to predict if the tweet is about real disaster or not. </p>"},{"metadata":{},"cell_type":"markdown","source":"# 1. Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport gc\n#text preprocess\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nimport re\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.manifold import TSNE\nfrom scipy import sparse\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, log_loss, f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of data points in train data\", train.shape)\nprint('-'*50)\nprint(\"The attributes of data :\", train.columns.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 Exploratory Data Analysis\n<h3>  2.1 Distribution of data points among output classes</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_value_counts = train['target'].value_counts()\nprint(\"Number of tweets that are real disaster \", y_value_counts[1], \", (\", (y_value_counts[1]/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\nprint(\"Number of tweets that are not disaster \", y_value_counts[0], \", (\", (y_value_counts[0]/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(aspect=\"equal\"))\nrecipe = [\"Disaster\", \"Not disaster\"]\n\ndata = [y_value_counts[1], y_value_counts[0]]\n\nwedges, texts = ax.pie(data, wedgeprops=dict(width=0.5), startangle=-40)\n\nbbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\nkw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n          bbox=bbox_props, zorder=0, va=\"center\")\n\nfor i, p in enumerate(wedges):\n    ang = (p.theta2 - p.theta1)/2. + p.theta1\n    y = np.sin(np.deg2rad(ang))\n    x = np.cos(np.deg2rad(ang))\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n    ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n                 horizontalalignment=horizontalalignment, **kw)\n\nax.set_title(\"Number of tweets that are disaster and not disaster\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:**\n- From above information we found that there are null values in keywords and location"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,6))\nsns.set_style('darkgrid')\nplt.subplot(1, 2, 1)\nplt.title('train keyword column') \nx=train['keyword'].isnull().value_counts()\nsns.barplot(['non null','null'],x)\n\nsns.set_style('darkgrid')\nplt.subplot(1, 2, 2)\nplt.title('train location column') \nx=train['location'].isnull().value_counts()\nsns.barplot(['non null','null'],x)\nplt.show()\n\nprint(\"train column Keyword percentage of null value is %.2f\" %(train.keyword.isnull().sum()/train.keyword.notnull().sum()*100))\nprint(\"train column location percentage of null value is %.2f\" %(train.location.isnull().sum()/train.location.notnull().sum()*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:**\n- for location column we are having almost 50% of null values and there are 3341 unique words. \n- its better to discard location feature than to impute it. As imputing almost 40% of your data would be introducing significant amount of error in it.\n- we will be keeping the keyword column because it will not have any impact over data imbalance. since the percentage of null value in keyword column is very low 0.8% which is 61 out of 7613 data points."},{"metadata":{},"cell_type":"markdown","source":"Keyword replace NaN with string"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].fillna(\"No keyword\",inplace=True)\ntest['keyword'].fillna(\"No keyword\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Drop location feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['location'],axis=1,inplace=True)\ntest.drop(['location'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Univariate Analysis:"},{"metadata":{},"cell_type":"markdown","source":"### Text feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count = train.text.str.split().apply(len).value_counts()\nprint('total number of words present in each text feature')\nprint(word_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_dict = dict(word_count)\nword_dict = dict(sorted(word_dict.items(), key=lambda kv: kv[1]))\n\n\nind = np.arange(len(word_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind, list(word_dict.values()))\n\nplt.ylabel('Number of tweets')\nplt.xlabel('Number of words in each text')\nplt.title('Words for each text of the tweets')\nplt.xticks(ind, list(word_dict.keys()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:**\n- from above chart there are almost 500 tweets have words count of 11 to 20\n- Very few tweets have words >30"},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_word_count = train[train['target']==1]['text'].str.split().apply(len)\ndisaster_word_count = disaster_word_count.values\n\nnot_disater_word_count = train[train['target']==0]['text'].str.split().apply(len)\nnot_disater_word_count = not_disater_word_count.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://glowingpython.blogspot.com/2012/09/boxplot-with-matplotlib.html\nplt.boxplot([disaster_word_count, not_disater_word_count])\nplt.title('Words for each text of the tweets')\nplt.xticks([1,2],('Disaster','Not a Disaster'))\nplt.ylabel('Words in tweets')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,3))\nsns.distplot(disaster_word_count, hist=False, label=\"Disaster\")\nsns.distplot(not_disater_word_count, hist=False, label=\"Not a Disaster\")\nplt.title('Words for each text of the tweets')\nplt.xlabel('Number of words in each text')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:**\n- we clearly identified that number words frequency in tweets says its is disaster or not are almost equal. \n- word counts are equally balanced in both class we can find it from box plot and distribution plot."},{"metadata":{},"cell_type":"markdown","source":"### keyword feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_count_k = train.keyword.value_counts()\nprint('total number of words present in each keyword')\nprint(word_count_k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(word_count_k.values)\nplt.title('distribution of unique words in keyword')\nplt.xlabel('Number of unique words in keyword feature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:**\n* total of 221 unique words present in the keyword feature.\n* there are few words which occurs more often between 30 to 40 times\n* all the words present in each keyword feature have only single word with %20 which is space.\n* “%20,” it represents a space in an encoded URL\n* we will be replacing the %20 with space in all keyword feature and do the data analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.keyword = train.keyword.str.replace('%20', ' ', regex=True)\ntest.keyword = test.keyword.str.replace('%20', ' ', regex=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keyword_count = train.keyword.str.split().apply(len).value_counts()\nprint('total number of words present in each text feature')\nprint(keyword_count)\nkeyword_dict = dict(keyword_count)\nkeyword_dict = dict(sorted(keyword_dict.items(), key=lambda kv: kv[1]))\n\n\nind_key = np.arange(len(keyword_dict))\nplt.figure(figsize=(20,5))\np1 = plt.bar(ind_key, list(keyword_dict.values()))\n\nplt.ylabel('Number of tweets')\nplt.xlabel('Number of words in each text')\nplt.title('Words for each text of the tweets')\nplt.xticks(ind_key, list(keyword_dict.keys()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:**\n- maximum word count is 3. \n- there are 6387 single word, 1132 two words and 33 three words in keyword feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.keyword[101:120]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text[101:120]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:**\n- Keyword feature is nothing but the keyword used in the text feature. so ther is correlation between this two features. \n- based on this we will be extracting couple of features which will be used in our model."},{"metadata":{},"cell_type":"markdown","source":"# 3 Basic Feature Extraction (before cleaning)"},{"metadata":{},"cell_type":"markdown","source":"Let us now construct a few features like:\n - ____n_special_word____ = number of special words count starts with @, # and digits from text feature\n - ____freq_keyword____ = Frequency of keyword \n - ____textlen____ = Length of text\n - ____keywordlen____ = Length of keyword\n - ____text_n_words____ = Number of words in text\n - ____keyword_n_words____ = Number of words in keyword\n - ____word_Common____ = (Number of common unique words in keyword and text)\n - ____word_Total____ =(Total num of words in text + Total num of words in keyword)\n - ____word_share____ = (word_common)/(word_Total)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['n_special_word'] = train['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#') or x.startswith('@') or x.isdigit()]))\ntest['n_special_word'] = test['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#') or x.startswith('@') or x.isdigit()]))\n\ntrain['freq_keyword'] = train.groupby('keyword')['keyword'].transform('count')\ntest['freq_keyword'] = test.groupby('keyword')['keyword'].transform('count')\n\ntrain['textlen'] = train['text'].str.len() \ntrain['keywordlen'] = train['keyword'].str.len()\ntrain['text_n_words'] = train['text'].apply(lambda row: len(row.split(\" \")))\ntrain['keyword_n_words'] = train['keyword'].apply(lambda row: len(row.split(\" \")))\n\ntest['textlen'] = test['text'].str.len() \ntest['keywordlen'] = test['keyword'].str.len()\ntest['text_n_words'] = test['text'].apply(lambda row: len(row.split(\" \")))\ntest['keyword_n_words'] = test['keyword'].apply(lambda row: len(row.split(\" \")))\n\ndef normalized_word_Common(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['keyword'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['text'].split(\" \")))    \n    return 1.0 * len(w1 & w2)\ntrain['word_Common'] = train.apply(normalized_word_Common, axis=1)\ntest['word_Common'] = test.apply(normalized_word_Common, axis=1)\n\ndef normalized_word_Total(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['keyword'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['text'].split(\" \")))    \n    return 1.0 * (len(w1) + len(w2))\ntrain['word_Total'] = train.apply(normalized_word_Total, axis=1)\ntest['word_Total'] = test.apply(normalized_word_Total, axis=1)\n\ndef normalized_word_share(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['keyword'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['text'].split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\ntrain['word_share'] = train.apply(normalized_word_share, axis=1)\ntest['word_share'] = test.apply(normalized_word_share, axis=1)\n\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 Preprocessing of Text"},{"metadata":{},"cell_type":"markdown","source":"- Preprocessing:\n    - Removing html tags \n    - Removing Punctuations\n    - Performing stemming\n    - Removing Stopwords\n    - Expanding contractions etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"# To get the results in 4 decemal points\nSAFE_DIV = 0.0001 \n\nSTOP_WORDS = stopwords.words(\"english\")\n\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    x = re.sub(r\"\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s\\/]*))*\", \"\", x)\n    x = x.replace(\"  \", \" \")\n    x = x.replace(\"_\", \" \")\n    \n    \n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    \n    if type(x) == type(''):\n        x = porter.stem(x)\n        example1 = BeautifulSoup(x, \"lxml\")\n        x = example1.get_text()\n               \n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"text\"] = train[\"text\"].apply(preprocess)\ntest[\"text\"] = test[\"text\"].apply(preprocess)\ntrain[\"keyword\"] = train[\"keyword\"].apply(preprocess)\ntest[\"keyword\"] = test[\"keyword\"].apply(preprocess)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking for null or empty values again"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Visualization"},{"metadata":{},"cell_type":"markdown","source":"<h3> 5.1 Plotting Word clouds </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_disaster = train[train['target'] == 1]\ntrain_not_disaster = train[train['target'] == 0]\ndis = ' '.join(train_disaster.text)\nnot_dis = ' '.join(train_not_disaster.text)\nstopwordswc = set(STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwc1 = WordCloud(background_color=\"white\", max_words=len(dis), stopwords=stopwordswc)\nwc1.generate(dis)\nwc2 = WordCloud(background_color=\"white\", max_words=len(not_dis), stopwords=stopwordswc)\nwc2.generate(not_dis)\n\nfig = plt.figure(figsize=(20,15))\n\nax = fig.add_subplot(1,2,1)\nplt.title('Word Cloud for Disaster Text')\nax.imshow(wc1, interpolation='bilinear')\nax.axis('off')\n\nax = fig.add_subplot(1,2,2)\nplt.title(\"Word Cloud for not Disaster Text\")\nax.imshow(wc2, interpolation='bilinear')\nax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 2D Visualization using TSNE"},{"metadata":{"colab":{},"colab_type":"code","id":"0QhZBdx1s6Ig","trusted":true},"cell_type":"code","source":"# Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 3 dimention\ndfp_subsampled = train[0:5000]\nX = MinMaxScaler().fit_transform(dfp_subsampled[['freq_keyword', 'textlen', 'keywordlen', 'text_n_words', 'keyword_n_words', 'word_Common', 'word_Total', 'word_share', 'n_special_word']])\ny = dfp_subsampled['target'].values","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"7Agv3hmXs6Ij","outputId":"85e1bea1-0623-4f68-e8a2-1bd83bdcc62b","trusted":true},"cell_type":"code","source":"tsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"qhTHOhWns6Io","outputId":"1e664fdf-7041-4f98-cce2-5ac69618a890","trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# draw the plot in appropriate place in the grid\nsns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, height=8,palette=\"Set1\",markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Make Data Model Ready:"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train[\"target\"].values\ntrain = train.drop(['target'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1 Encoding text feature "},{"metadata":{},"cell_type":"markdown","source":"#### TFIDF-W2V"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('../input/glove-vectors/glove_vectors', 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TFIDF Word2Vec\n# compute TFIDF word2vec for each review.\ndef train_tfidfw2v(x):\n    tfidf_model = TfidfVectorizer()\n    tfidf_model.fit(x)\n    # we are converting a dictionary with word as a key, and the idf as a value\n    dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n    tfidf_words = set(tfidf_model.get_feature_names())\n    tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n    for sentence in tqdm(x): # for each review/sentence\n        vector = np.zeros(300) # as word vectors are of zero length\n        tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n        for word in sentence.split(): # for each word in a review/sentence\n            if (word in glove_words) and (word in tfidf_words):\n                vec = model[word] # getting the vector for each word\n                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n                tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n                vector += (vec * tf_idf) # calculating tfidf weighted w2v\n                tf_idf_weight += tf_idf\n        if tf_idf_weight != 0:\n            vector /= tf_idf_weight\n        tfidf_w2v_vectors.append(vector)\n    return tfidf_w2v_vectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_w2v_vectors_keyword = train_tfidfw2v(train['keyword'])\ntfidf_w2v_vectors_text = train_tfidfw2v(train['text'])\n\ntfidf_w2v_vectors_keyword_test = train_tfidfw2v(test['keyword'])\ntfidf_w2v_vectors_text_test = train_tfidfw2v(test['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2 encoding numerical features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_fre_len_n_common_total_share_special = StandardScaler().fit_transform(train[['freq_keyword', 'textlen', 'keywordlen', 'text_n_words', 'keyword_n_words', 'word_Common', 'word_Total', 'word_share', 'n_special_word']])\nx_test_fre_len_n_common_total_share_special = StandardScaler().fit_transform(test[['freq_keyword', 'textlen', 'keywordlen', 'text_n_words', 'keyword_n_words', 'word_Common', 'word_Total', 'word_share', 'n_special_word']])\nprint(\"after Standardizing numerical features\")\nprint(x_train_fre_len_n_common_total_share_special.shape, y_train.shape)\nprint(x_test_fre_len_n_common_total_share_special.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Concatinating all the features: (standardscalar + tfidfW2v)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tfidf_w2v = sparse.csr_matrix(np.hstack((tfidf_w2v_vectors_keyword, \n                                                   tfidf_w2v_vectors_text,\n                                                   x_train_fre_len_n_common_total_share_special)))\n\nX_test_tfidf_w2v = sparse.csr_matrix(np.hstack((tfidf_w2v_vectors_keyword_test, \n                                                  tfidf_w2v_vectors_text_test,\n                                                  x_test_fre_len_n_common_total_share_special)))\n\n\nprint(\"Final Data matrix for tfidf set 2\")\nprint(X_train_tfidf_w2v.shape, y_train.shape)\nprint(X_test_tfidf_w2v.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Machine Learning Model"},{"metadata":{"colab_type":"text","id":"YgY29g_qtASq"},"cell_type":"markdown","source":"<h2> 7.1 XGB with hyperparameter tuning </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr_tr, tpr_tr):\n    '''\n    plot the ROC curve for the FPR and TPR value\n    '''\n    plt.plot(fpr_tr, tpr_tr, 'k.-', color='green', label='ROC_train AUC = {:0.2f} '.format(auc(fpr_tr, tpr_tr)))\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_best_threshold(threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    return t\n\ndef predict_with_best_t(proba, threshould):\n    predictions = []\n    for i in proba:\n        if i>=threshould:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train_tfidf_w2v, label=y_train)\n\nwatchlist = [(d_train, 'train')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\npredict_y = bst.predict(d_train)\nprint(\"The test log loss is:\",log_loss(y_train, predict_y, eps=1e-15))\n\nfpr_tfidf, tpr_tfidf, t_tfidf = roc_curve(y_train, predict_y)\nprint('F1 score',f1_score(y_train,predict_with_best_t(predict_y, find_best_threshold(t_tfidf,fpr_tfidf,tpr_tfidf))))\nplot_roc_curve(fpr_tfidf,tpr_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nd_test = xgb.DMatrix(X_test_tfidf_w2v)\ny_ = bst.predict(d_test)\ny_pred = predict_with_best_t(y_, find_best_threshold(t_tfidf,fpr_tfidf,tpr_tfidf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = y_pred\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This is my first prediction Competition @ Kaggle :) Hope it is helpful. Please upvote if you like this kernel."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}