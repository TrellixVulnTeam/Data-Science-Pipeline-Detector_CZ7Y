{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nâ‰ˆ\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport string \nstring.punctuation\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords.words('english')\nimport re\nimport unicodedata\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test=pd.read_csv('../input/nlp-getting-started/test.csv')\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\ndf_sample.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Date preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['target'].value_counts().plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['keyword'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby(['keyword']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clean the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"EMOJIS = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\nURLPATTERN        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\nUSERPATTERN       = '@[^\\s]+'\nSEQPATTERN   = r\"(.)\\1\\1+\"\nSEQREPLACE = r\"\\1\\1\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tweets_cleaning(text):\n    lowercase = text.lower()\n    punc_removal = [char for char in lowercase if char not in string.punctuation]\n    punc_removal_joined = ''.join(punc_removal)\n    url_removal = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', punc_removal_joined, flags=re.MULTILINE)\n    for emoji in EMOJIS.keys():\n        url_removal = url_removal.replace(emoji, \"EMOJI\" + EMOJIS[emoji])  \n   # emoji_removal = url_removal.encode('ascii', 'ignore').decode('ascii')\n    emoji_removal=url_removal\n    stopwords_removal = [word for word in emoji_removal.split() if word not in stopwords.words('english')]\n    return stopwords_removal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['cleaned_text']=df_train['text'].apply(tweets_cleaning).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['cleaned_text']=df_test['text'].apply(tweets_cleaning).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_clean=df_train[['cleaned_text','target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(df_train_clean['cleaned_text'].values)\nX = tokenizer.texts_to_sequences(df_train_clean['cleaned_text'].values)\nX = pad_sequences(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM Model training"},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.get_dummies(df_train_clean['target']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 16\nmodel.fit(X_train, Y_train, epochs = 19, batch_size=batch_size, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(df_test['cleaned_text'].values)\nX = tokenizer.texts_to_sequences(df_test['cleaned_text'].values)\nX = pad_sequences(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=model.predict(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels=[0,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_predict=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['id'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx=0\nfor p in pred:\n    predict=[]\n    t_id=df_test['id'][idx]\n    idx+=1\n    target=labels[np.argmax(p)]\n    predict.append(t_id)\n    predict.append(target)\n    tweet_predict.append(predict)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub=pd.DataFrame(tweet_predict,columns=[\"id\",\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub.to_csv('submission_lstm.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LSTM Ends"},{"metadata":{},"cell_type":"markdown","source":"## Save prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_prediction(pred,model):\n    df_test=pd.read_csv('../input/nlp-getting-started/test.csv')\n    submission_list=[]\n    idx=0\n    tweet_predict=[]\n    for p in pred:\n        predict=[]\n        t_id=df_test['id'][idx]\n        idx+=1\n        target=p\n        predict.append(t_id)\n        predict.append(target)\n        tweet_predict.append(predict)\n    df_sub=pd.DataFrame(tweet_predict,columns=['id','target'])\n    df_sub.to_csv('submission_'+model+\".csv\",index=False)\n    print(model+\"- model prediction\")\n    print(df_sub['target'].value_counts())\n    \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic regression +CounteVcetorizer"},{"metadata":{},"cell_type":"markdown","source":"##### We create a new column called kfold and fill it with -1"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"kfold\"]=-1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### randomize the row of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=df_train.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fetch labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=df_train.target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### initiate the kfold class from model_selection module"},{"metadata":{"trusted":true},"cell_type":"code","source":"kf=model_selection.StratifiedKFold(n_splits=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f,(t_,v_) in enumerate(kf.split(X=df_train,y=y)):\n    df_train.loc[v_,'kfold']=f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['kfold'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_ in range(5):\n    train_df=df_train[df_train.kfold!=fold_].reset_index(drop=True)\n    if fold_==4:\n        train_df=df_train.reset_index(drop=True)\n        test_df=df_test.reset_index(drop=True)\n    else:\n        test_df=df_train[df_train.kfold==fold_].reset_index(drop=True)\n    ##### initilize CountVectorizer with NLTK's word_tokenize\n    ##### function as tokenizer\n    count_vec=CountVectorizer(\n        tokenizer=word_tokenize,\n        token_pattern=None)\n    #### fit count_vec on training data reviews\n    count_vec.fit(train_df.text)\n    #transform training and validation data reviews\n    x_train=count_vec.transform(train_df.text)\n    x_test=count_vec.transform(test_df.text)\n    # initialize logistic regression model\n    model_logistic=linear_model.LogisticRegression(max_iter=1000)\n    # fit the model on training data reviews and sentiment\n    model_logistic.fit(x_train,train_df.target)\n    preds_log=model_logistic.predict(x_test)\n    ##Calculate accuracy\n    if fold_!=4:\n        accuracy=metrics.accuracy_score(test_df.target,preds_log)\n        print(f\"Fold:{fold_}\")\n        print(f\"Accuracy={accuracy}\")\n        print(\"\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"set(preds_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(preds_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_prediction(preds_log,model='logistic')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import naive_bayes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_ in range(5):\n    train_df=df_train[df_train.kfold!=fold_].reset_index(drop=True)\n    if fold_==4:\n        train_df=df_train.reset_index(drop=True)\n        test_df=df_test.reset_index(drop=True)\n    else:\n        test_df=df_train[df_train.kfold==fold_].reset_index(drop=True)\n    ##### initilize CountVectorizer with NLTK's word_tokenize\n    ##### function as tokenizer\n    count_vec=CountVectorizer(\n        tokenizer=word_tokenize,\n        token_pattern=None)\n    #### fit count_vec on training data reviews\n    count_vec.fit(train_df.text)\n    #transform training and validation data reviews\n    x_train=count_vec.transform(train_df.text)\n    x_test=count_vec.transform(test_df.text)\n    # initialize naive bayes model\n    model_naive=naive_bayes.MultinomialNB()\n    # fit the model on training data reviews and sentiment\n    model_naive.fit(x_train,train_df.target)\n    preds_naive=model_naive.predict(x_test)\n    ##Calculate accuracy\n    if fold_!=4:\n        accuracy=metrics.accuracy_score(test_df.target,preds_naive)\n        print(f\"Fold:{fold_}\")\n        print(f\"Accuracy={accuracy}\")\n        print(\"\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"save_prediction(preds_naive,model='naive')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic +TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_ in range(5):\n    train_df=df_train[df_train.kfold!=fold_].reset_index(drop=True)\n    if fold_==4:\n        train_df=df_train.reset_index(drop=True)\n        test_df=df_test.reset_index(drop=True)\n    else:\n        test_df=df_train[df_train.kfold==fold_].reset_index(drop=True)\n\n    ##### initilize TfidfVectorizer with NLTK's word_tokenize\n    ##### function as tokenizer\n    tfidf_vec=TfidfVectorizer(\n        tokenizer=word_tokenize,\n        token_pattern=None)\n    #### fit count_vec on training data reviews\n    tfidf_vec.fit(train_df.text)\n    #transform training and validation data reviews\n    x_train=tfidf_vec.transform(train_df.text)\n    x_test=tfidf_vec.transform(test_df.text)\n    # initialize logistic regression model\n    model_logistic=linear_model.LogisticRegression(max_iter=1000)\n    # fit the model on training data reviews and sentiment\n    model_logistic.fit(x_train,train_df.target)\n    preds_logtfidf=model_logistic.predict(x_test)\n    ##Calculate accuracy\n    if fold_!=4:\n        accuracy=metrics.accuracy_score(test_df.target,preds_logtfidf)\n        print(f\"Fold:{fold_}\")\n        print(f\"Accuracy={accuracy}\")\n        print(\"\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}