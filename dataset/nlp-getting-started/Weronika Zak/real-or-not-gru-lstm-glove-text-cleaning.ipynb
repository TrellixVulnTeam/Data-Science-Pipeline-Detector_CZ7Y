{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import layers as ly\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.optimizers import Adam\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom spellchecker import SpellChecker","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_train = df_train.drop([\"id\"], axis=1)\ndf_train = df_train.drop_duplicates()\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ndf_test = df_test.drop([\"id\"], axis=1)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nrandom.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = np.array(df_train[\"text\"])\ntest_sentences = np.array(df_test[\"text\"])\n\nlabels = np.array(df_train[\"target\"])\n\nrandom.shuffle(labels)\nrandom.shuffle(sentences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning the tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english')) \nlemmatizer = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize_sentence(text):\n    words = nltk.word_tokenize(text)\n    lemmatized = ' '.join([lemmatizer.lemmatize(w) for w in words])\n    return lemmatized\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n        u\"\\U0001F600-\\U0001F64F\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U0001F1F2\"\n        u\"\\U0001F1F4\"\n        u\"\\U0001F620\"\n        u\"\\u200d\"\n        u\"\\u2640-\\u2642\"\n        \"]+\", flags=re.UNICODE)\n\n    text = emoji_pattern.sub(r'', text)\n    return text\n\n\ndef clean_sentence(text):\n    text = re.sub(r\"http\\S+\", \"\", text) # remove urls\n    text = re.sub(r'@[^\\s]+','',text) # remove usernames\n    text = re.sub(r'[0-9]+', '', text)\n    text = text.replace(\"#\", \"\")\n    text = text.replace(\"can't\", \"can not\").replace(\"won't\", \"will not\").replace(\"n't\", \" not\")\n    text = text.replace(\"'m\", \" am\").replace(\"'re\", \" are\").replace(\"'s\", \"  is\").replace(\"'d\", \" would\")\n    text = text.replace(\"'ll\", \" will\").replace(\"'t\", \" not\").replace(\"'ve\", \"  have\")\n    text = remove_emoji(text)\n    for word in stop_words:\n        text = text.replace(\" \"+word+\" \", \" \")\n    text = ''.join([i for i in text if not i.isdigit()])\n    text = ' '.join([i for i in text.split(' ') if len(i) > 2])\n    text = lemmatize_sentence(text.lower())\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = [clean_sentence(sentence) for sentence in sentences]\n\ntest_sentences = [clean_sentence(text) for text in test_sentences]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenizing & making sequences","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE = len(tokenizer.word_index)\nMAX_LENGTH = 20\nEMBEDDING_DIM = 16\nSPLIT = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(sentences)\npadded = pad_sequences(sequences, padding=\"post\", maxlen=MAX_LENGTH, truncating=\"post\")\n\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(test_sequences, padding=\"post\",maxlen=MAX_LENGTH, truncating=\"post\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"padded[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\ncallback = ModelCheckpoint(\"model_NLP.h5\", monitor=\"val_accuracy\", save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([   \n    ly.Embedding(VOCAB_SIZE+1, EMBEDDING_DIM, input_length=MAX_LENGTH, trainable=False),\n    ly.Dropout(0.2),\n    ly.Conv1D(64, 5, activation='relu'),\n    ly.MaxPooling1D(4),\n    ly.LSTM(64),\n    ly.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.001), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = padded[SPLIT:]\ntrain_y = labels[SPLIT:]\n\nval_x = padded[:SPLIT]\nval_y = labels[:SPLIT]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_x, train_y,\n                    epochs=50,\n                    validation_data=(val_x, val_y),\n                    callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_results(his):\n    loss = his.history[\"loss\"]\n    val_loss = his.history[\"val_loss\"]\n    acc = his.history[\"accuracy\"]\n    val_acc = his.history[\"val_accuracy\"]\n\n    plt.plot(loss)\n    plt.plot(val_loss)\n    plt.legend([\"loss\", \"val_loss\"])\n    plt.show()\n\n    plt.plot(acc)\n    plt.plot(val_acc)\n    plt.legend([\"acc\", \"val_acc\"])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_predictions(best_model=True):\n    if best_model:\n        m = tf.keras.models.load_model(\"model_NLP.h5\")\n    else:\n        m = model\n    \n    prediction = m.predict(test_padded)\n\n    for i in range(15):\n        result = \"REAL\" if prediction[i] > 0.5 else \"FAKE\"\n        print(df_test.iloc[i][\"text\"], \" - \", result)\n    return prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = make_predictions()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = make_predictions(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Not the best results tbh. Let's try something else","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict = dict()\nwith open(\"../input/glove6b200d/glove.6B.200d.txt\", \"r\") as f:\n    for line in f:\n        vals = line.split()\n        word = vals[0]\n        vects = np.array(vals[1:], dtype=\"float32\")\n        embedding_dict[word] = vects","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index\n\nembedding_matrix = np.zeros((VOCAB_SIZE+1, 200))\nfor word, i in word_index.items():\n    if i > VOCAB_SIZE+1:\n        continue\n    emb_vec = embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    ly.Embedding(VOCAB_SIZE+1, 200, embeddings_initializer=Constant(embedding_matrix), input_length=MAX_LENGTH, trainable=False),\n    ly.Dropout(0.2),\n    ly.Conv1D(64, 5, activation='relu'),\n    ly.MaxPooling1D(4),\n    ly.LSTM(64),\n    ly.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.001), metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_x, train_y,\n                    epochs=50,\n                    validation_data=(val_x, val_y),\n                    callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_results(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = make_predictions()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = make_predictions(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Not the best either, folks  ¯\\\\_(ツ)_/¯\n## Welp, kinda shame, let's save the results to the submission file anyways","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def save_results(prediction):\n    TEST_RESULTS = []\n\n    for i in range(len(prediction)):\n        r = 1 if prediction[i] > 0.5 else 0\n        TEST_RESULTS.append(r)\n    \n    sub_df = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n    sub_df[\"target\"] = TEST_RESULTS\n    sub_df.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"save_results(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}