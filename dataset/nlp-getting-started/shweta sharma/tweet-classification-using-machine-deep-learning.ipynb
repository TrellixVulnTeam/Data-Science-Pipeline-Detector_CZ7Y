{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SIMPLE TEXT CLASSIFICATION STEPS\n*****************\n\n* **SUMMARY OF DATA**\n\n    - Total Samples\n    - Total Features\n    - Check Null Values\n    - Check the balance of the target classes\n**********\n* **CLEANING** \n\n    - Drop Duplicates\n    - Drop Null Values\n    - Resampling for imbalanced classes\n***************\n* **TEXT PREPROCESSING**\n\n    * Removing irrelevant words such as @mentions or http links etc.\n    * Remove Punctuations\n    * Lowercase \n*****************\n* **VECTORIZATION**\n\n    * Convert text into numerical features using Tf-idf\n*************\n* **MODEL TRAINING & EVALUATION**\n\n    * Creating pipeline of simple models\n****************\n* **CONCLUSION**\n\n    * Choosing model with best F1 score\n*******************\n* **PREDICTION**\n\n    * Predicting on test dataset\n******************\n****************","metadata":{}},{"cell_type":"markdown","source":"# LOADING DATA","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport spacy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-01T11:52:33.125386Z","iopub.execute_input":"2021-07-01T11:52:33.125748Z","iopub.status.idle":"2021-07-01T11:52:33.131548Z","shell.execute_reply.started":"2021-07-01T11:52:33.125715Z","shell.execute_reply":"2021-07-01T11:52:33.130347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntrain_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-01T11:52:33.510292Z","iopub.execute_input":"2021-07-01T11:52:33.510727Z","iopub.status.idle":"2021-07-01T11:52:33.556963Z","shell.execute_reply.started":"2021-07-01T11:52:33.510687Z","shell.execute_reply":"2021-07-01T11:52:33.555908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SUMMARY OF DATA","metadata":{}},{"cell_type":"code","source":"print(\"Total Number of Samples \", len(train_df))\nprint(\"\\nTotal Number of Features \", len(train_df.columns))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:34.45092Z","iopub.execute_input":"2021-07-01T11:52:34.451298Z","iopub.status.idle":"2021-07-01T11:52:34.457865Z","shell.execute_reply.started":"2021-07-01T11:52:34.451264Z","shell.execute_reply":"2021-07-01T11:52:34.456764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check for null data in the \"text\" column","metadata":{}},{"cell_type":"code","source":"train_df['text'].isna().any()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:35.477336Z","iopub.execute_input":"2021-07-01T11:52:35.477716Z","iopub.status.idle":"2021-07-01T11:52:35.487648Z","shell.execute_reply.started":"2021-07-01T11:52:35.477683Z","shell.execute_reply":"2021-07-01T11:52:35.486412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train_df[['id','text','target']]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:36.052285Z","iopub.execute_input":"2021-07-01T11:52:36.052813Z","iopub.status.idle":"2021-07-01T11:52:36.058437Z","shell.execute_reply.started":"2021-07-01T11:52:36.052777Z","shell.execute_reply":"2021-07-01T11:52:36.057573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:36.700882Z","iopub.execute_input":"2021-07-01T11:52:36.70148Z","iopub.status.idle":"2021-07-01T11:52:36.712581Z","shell.execute_reply.started":"2021-07-01T11:52:36.70144Z","shell.execute_reply":"2021-07-01T11:52:36.711354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check whether the target class is balanced or imbalanced","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\n\nsns.barplot(x=['Normal','Disaster'], y= df.target.value_counts().values)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:37.987982Z","iopub.execute_input":"2021-07-01T11:52:37.988326Z","iopub.status.idle":"2021-07-01T11:52:38.230904Z","shell.execute_reply.started":"2021-07-01T11:52:37.988295Z","shell.execute_reply":"2021-07-01T11:52:38.229808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a slight difference between the two classes, so we can say it is balanced. Hence would not require any kind of resampling techniques.","metadata":{}},{"cell_type":"markdown","source":"# CLEANING\n******************\n\n* Drop Duplicates\n* Drop Null Values","metadata":{}},{"cell_type":"markdown","source":"### Dropping Duplicates","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(action='ignore')\n\nl = len(df)\ndf.drop_duplicates(subset='text', inplace=True)\nprint(\"Total Duplicates \", l - len(df))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:40.714331Z","iopub.execute_input":"2021-07-01T11:52:40.714737Z","iopub.status.idle":"2021-07-01T11:52:40.732215Z","shell.execute_reply.started":"2021-07-01T11:52:40.714703Z","shell.execute_reply":"2021-07-01T11:52:40.730709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:41.310989Z","iopub.execute_input":"2021-07-01T11:52:41.311378Z","iopub.status.idle":"2021-07-01T11:52:41.318461Z","shell.execute_reply.started":"2021-07-01T11:52:41.311344Z","shell.execute_reply":"2021-07-01T11:52:41.317113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dropping NaN Values\n\n***************\n\nThere are no NaN values in the df","metadata":{}},{"cell_type":"code","source":"null_rows = df['text'][df['text'].isna()]\nnull_rows","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:42.358961Z","iopub.execute_input":"2021-07-01T11:52:42.359301Z","iopub.status.idle":"2021-07-01T11:52:42.368261Z","shell.execute_reply.started":"2021-07-01T11:52:42.359272Z","shell.execute_reply":"2021-07-01T11:52:42.367195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TEXT PREPROCESSING\n*************\n\n* Removing irrelevant words such as @mentions or http links etc.\n* Remove Punctuations\n* Lowercase ","metadata":{}},{"cell_type":"code","source":"import re","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:43.351768Z","iopub.execute_input":"2021-07-01T11:52:43.352201Z","iopub.status.idle":"2021-07-01T11:52:43.355631Z","shell.execute_reply.started":"2021-07-01T11:52:43.352172Z","shell.execute_reply":"2021-07-01T11:52:43.354923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function\ndef clean_text(text):\n    te = str(text).encode('ascii','ignore').decode('UTF-8')\n    te = re.sub(r'@[\\w]+', '', te)\n    te = re.sub(r'https?://t.co/[\\w]+', '', te)\n    te = re.sub(r'#', '', te)\n    te = re.sub(r\"RT @[\\w]+:\",'',te)\n    te = re.sub(r\"RT @[\\w]+:\",'',te)\n    te = re.sub(r\" RT \",'',te)\n    te = re.sub(r\"https://[\\w]+.[\\w]+/[\\w]+\",'',te)\n    te = re.sub(r\"[][]\",'',te)\n    te = re.sub(r\"&amp\",\"and\", te)\n    # remove the characters [\\], ['] and [\"]\n    text = re.sub(r\"\\\\\", \"\", te)    \n    text = re.sub(r\"\\'\", \"\", text)    \n    text = re.sub(r\"\\\"\", \"\", text)    \n    \n    # convert text to lowercase\n    text = text.strip().lower()\n    \n    # replace punctuation characters with spaces\n    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n    translate_dict = dict((c, \" \") for c in filters)\n    translate_map = str.maketrans(translate_dict)\n    text = text.translate(translate_map)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:43.836352Z","iopub.execute_input":"2021-07-01T11:52:43.836953Z","iopub.status.idle":"2021-07-01T11:52:43.845432Z","shell.execute_reply.started":"2021-07-01T11:52:43.836919Z","shell.execute_reply":"2021-07-01T11:52:43.844722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VECTORIZATION\n*****************\n* Convert the text into numerical features using Tf-idf","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:44.874027Z","iopub.execute_input":"2021-07-01T11:52:44.874716Z","iopub.status.idle":"2021-07-01T11:52:44.879867Z","shell.execute_reply.started":"2021-07-01T11:52:44.874676Z","shell.execute_reply":"2021-07-01T11:52:44.878558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transform each text into a vector of word counts\nvectorizer = TfidfVectorizer(stop_words=\"english\",\n                             preprocessor=clean_text,\n                             ngram_range=(1, 2))\n\ntraining_features = vectorizer.fit_transform(df.text)    ","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:52:45.491747Z","iopub.execute_input":"2021-07-01T11:52:45.49212Z","iopub.status.idle":"2021-07-01T11:52:46.060406Z","shell.execute_reply.started":"2021-07-01T11:52:45.492088Z","shell.execute_reply":"2021-07-01T11:52:46.059334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL TRAINING & EVALUATION","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import precision_score, recall_score, make_scorer, f1_score, accuracy_score\nfrom sklearn.model_selection import KFold, cross_val_score","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:53:44.839721Z","iopub.execute_input":"2021-07-01T11:53:44.840089Z","iopub.status.idle":"2021-07-01T11:53:44.84512Z","shell.execute_reply.started":"2021-07-01T11:53:44.840059Z","shell.execute_reply":"2021-07-01T11:53:44.844058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"est = []\nest.append(('LogisticRegression', Pipeline([('LR', LogisticRegression())])))\nest.append(('BernoulliNB', Pipeline([('BNB', BernoulliNB())])))\nest.append(('MultinomialNB', Pipeline([('MNB', MultinomialNB())])))\nest.append(('LinearSVC', Pipeline([('LNB', LinearSVC())])))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:53:31.103Z","iopub.execute_input":"2021-07-01T11:53:31.103528Z","iopub.status.idle":"2021-07-01T11:53:31.110177Z","shell.execute_reply.started":"2021-07-01T11:53:31.103495Z","shell.execute_reply":"2021-07-01T11:53:31.109517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Training\nmodel_scores = {}\n\np_scorer = make_scorer(precision_score)\nr_scorer = make_scorer(recall_score)\nf1_scorer = make_scorer(f1_score)\na_scorer = make_scorer(accuracy_score)\n\nfor i in est:\n    kfold = KFold(n_splits=7, shuffle=True, random_state=4)\n    p_scores = cross_val_score(i[1], training_features, df.target, cv=kfold, scoring=p_scorer)\n    r_scores = cross_val_score(i[1], training_features, df.target, cv=kfold, scoring=r_scorer)\n    f1_scores = cross_val_score(i[1], training_features, df.target, cv=kfold, scoring=f1_scorer)\n    a_scores = cross_val_score(i[1], training_features, df.target, cv=kfold, scoring=a_scorer)\n    \n    model_scores.update({ i[0]:{'accuracy': a_scores.mean(), 'f1_score':f1_scores.mean(), 'precision': p_scores.mean(), 'recall':r_scores.mean()} })","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:53:47.236845Z","iopub.execute_input":"2021-07-01T11:53:47.237383Z","iopub.status.idle":"2021-07-01T11:56:57.6769Z","shell.execute_reply.started":"2021-07-01T11:53:47.237345Z","shell.execute_reply":"2021-07-01T11:56:57.675182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in model_scores:\n    print('\\n', i)\n    print('\\n', model_scores[i])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:56:57.678671Z","iopub.execute_input":"2021-07-01T11:56:57.678991Z","iopub.status.idle":"2021-07-01T11:56:57.687989Z","shell.execute_reply.started":"2021-07-01T11:56:57.678956Z","shell.execute_reply":"2021-07-01T11:56:57.686646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONCLUSION\n********************\n\n* We will choose the model that gives the best F1 score which is a combination of precision and recall.","metadata":{}},{"cell_type":"code","source":"# model with top f1 score\n\ntop_models_score = sorted(model_scores.items(), key=lambda k:k[1]['f1_score'], reverse=True)\ntop_models_score[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:56:57.690628Z","iopub.execute_input":"2021-07-01T11:56:57.69094Z","iopub.status.idle":"2021-07-01T11:56:57.704684Z","shell.execute_reply.started":"2021-07-01T11:56:57.690911Z","shell.execute_reply":"2021-07-01T11:56:57.703358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_model = dict(est)[top_models_score[0][0]]\ntop_model.fit(training_features, df.target)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:56:57.706506Z","iopub.execute_input":"2021-07-01T11:56:57.706827Z","iopub.status.idle":"2021-07-01T11:56:57.772461Z","shell.execute_reply.started":"2021-07-01T11:56:57.706799Z","shell.execute_reply":"2021-07-01T11:56:57.77121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREDICTION","metadata":{}},{"cell_type":"code","source":"# load test csv\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:56:57.776026Z","iopub.execute_input":"2021-07-01T11:56:57.776303Z","iopub.status.idle":"2021-07-01T11:56:57.81439Z","shell.execute_reply.started":"2021-07-01T11:56:57.776276Z","shell.execute_reply":"2021-07-01T11:56:57.813412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_features = vectorizer.transform(test_df.text)\npredictions = top_model.predict(test_features)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:56:57.815693Z","iopub.execute_input":"2021-07-01T11:56:57.815973Z","iopub.status.idle":"2021-07-01T11:56:57.972108Z","shell.execute_reply.started":"2021-07-01T11:56:57.815944Z","shell.execute_reply":"2021-07-01T11:56:57.97114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(columns=['id', 'target'])\nsubmission['id'] = test_df['id']\nsubmission['target'] = predictions\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:56:57.973433Z","iopub.execute_input":"2021-07-01T11:56:57.973776Z","iopub.status.idle":"2021-07-01T11:56:57.990864Z","shell.execute_reply.started":"2021-07-01T11:56:57.973744Z","shell.execute_reply":"2021-07-01T11:56:57.989876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}