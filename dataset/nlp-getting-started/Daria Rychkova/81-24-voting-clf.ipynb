{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install autocorrect ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport re\nimport numpy as np\nimport pandas as pd\nfrom unidecode import unidecode\nfrom autocorrect import Speller\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus.reader import CategorizedPlaintextCorpusReader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_train.head(15)\n# in our dataset we have a lot of noise that would not be good for analysis\n# hashtags, mentions, urls, emojis and so on ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.isnull().sum())\n\n# import missingno as msno \n# msno.bar(df_train) \n\n# as we can see there are a lot of missing data of location so I would better better drop this col\n# 'keyword' cal might be used later on","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First step would be basic precleaning process of removing stuff we dont need in text\n1. Convert all text to lowercase\n2. Convert text to unicode because we have unrecognized chars\n3. Remove from text:\nall urls - they are meaningless for this dataset\nall mentions - not needed for our analysis\nhashtag symbol - hashtags themselve might have meaning so we leave them\nnumbers and words that contains numbers\nall punctuation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_from_text(col):\n    \n    col = col.str.replace('åÊ', ' ', case=False) # we can replace not unicode chars using .replace\n    col = col.str.replace('&lt', ' ', case=False)\n    col = col.str.replace('&gt', ' ', case=False)\n    col = col.str.replace('Û÷', ' ', case=False)\n    col = col.str.replace('Ûª', '\\'', case=False)\n    col = col.str.replace('Û', '\\'', case=False)\n    col = col.str.replace('&amp', ' ', case=False)\n    col = col.str.replace('ï', ' ', case=False)\n    col = col.str.replace('ó', ' ', case=False)\n    col = col.str.replace('ò', ' ', case=False)\n    col = col.str.replace('\\x89', '', case=False)\n    \n   \n    col = col.str.lower()                  #everything to lower case\n    col = [unidecode(x) for x in col]      #everything to unicode\n    \n    \n    col = [re.sub(r\"http\\S+|www.\\S+\", ' ', x) for x in col]  #remove urls\n    col = [re.sub(r\"@(\\w+)\", ' ', x) for x in col]           #remove mentions\n    col = [re.sub(r\"#\", ' ', x) for x in col]                #remove hashtags\n    col = [re.sub(r\"\\w*\\d\\w*\", ' ', x) for x in col]         #remove nums\n    col = [re.sub(r\"[^\\w\\d'\\s]+\", ' ', x) for x in col]      #remove punctuation\n    col = [re.sub(r\"\\'s\", ' is', x) for x in col]            #all 's to is\n    col = [re.sub(r\"\\'\", '', x) for x in col]                #remove ap. that are left\n    col = [re.sub(r\"_\", ' ', x) for x in col]                #remove underscore\n    col = [re.sub(r\"-\", ' ', x) for x in col]                #remove dash\n        \n    return col","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Next step is to do nltk transformation of the text:\n1. tokenize\n2. remove trailing letters\n3. spell check of each words\n4. remove all stop words\n5. lemmatize\n6. stemm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# define needed functions\ndef reduce_lengthening(text):\n    pattern = re.compile(r\"(.)\\1{1,}\")\n    word = pattern.sub(r\"\\1\\1\", text)\n    return word\n\nspell = Speller(lang='en')\ndef spell_check(text):\n    spell_correct_text = spell(text) \n    return spell_correct_text\n\n\nstop_words = stopwords.words('english')\nstop_words.append('sorry') #to regular list of stop words add\nstop_words.append('like')  #these two the most common words\nstop_words.append('im')    #it is 'I am' and 'You' \nstop_words.append('u')     #wich shold be removed as stopword\n\ndef stop_word(text):\n    without_stopw = [word for word in text if not word in stop_words]\n    return without_stopw\n\ndef word_stemmer(text):\n    stemmer = PorterStemmer()\n    stem_text = [stemmer.stem(i) for i in text]\n    return stem_text\n\ndef word_lemmatizer(text):\n    lemmatizer = WordNetLemmatizer()\n    lem_text = [lemmatizer.lemmatize(i) for i in text]\n    return lem_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In this function we combine everything together in the right order ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def nlp_transform_df(df):\n    \n#      remove all defined noise (@, #, punctuation and so on)\n    \n    df['text'] = remove_from_text(df['text'])\n     \n    \n#      SPELL CHECK CAN BE VERY SLOW PROCESS IN ORDER TO DO IT FASTER (about 5 min) WE NEED TO DO FOLLOWING STEPS\n    \n#      1. Lets fix all trailing letters so word would have a chance to be corrected\n#      for example: goooooooal, finalllllly, looooooool\n#      reduce_lengthening(arg) function is defined above\n\n    df['text'] = [[reduce_lengthening(x) for x in y.split()] for y in df['text']]\n    \n#     2. Now we creat a list of all distinct(about 14K words)\n#     ittirate throu each word in text and append all words that are not already in a list\n\n    words_from_text = []\n    for line in df['text']:\n        for w in line:\n            if w not in words_from_text:\n                words_from_text.append(w)\n\n#     3. This step is to retrieve all words that are not existing in english dictionary from list above\n#     import english words from nltk corpus \n#     ittirate throu words_from_text list and check weather we have it in english dictionary\n#     if not we store it in a list not_word_from_dict\n\n    from nltk.corpus import words\n    d = words.words()\n    not_word_from_dict = []\n    for w in words_from_text:\n            if w not in d:\n                not_word_from_dict.append(w) \n                \n#     4. Now we apply function spell_check(arg) to every item from not_word_from_dict  \n#     and store as corected_words\n#     only words that are possible to correct will be corrected\n\n    corected_words = [spell_check(x) for x in not_word_from_dict]\n\n\n#     5. Here we create dictionary: key   - initial word from text, \n#                                   value - corrected word\n\n    dict_correct_speling = dict(zip(not_word_from_dict, corected_words))\n\n    for key in dict_correct_speling.copy():\n         if key == dict_correct_speling[key]:\n            dict_correct_speling.pop(key)\n                  \n#     6. Last step is to apply dicionary to out text\n#     while performing reduce_lengthening(arg) we split our string into sep. words \n#     so lets join them back as str\n#     and apply function replace_with_corrected_word(df) defined above\n\n    df['text'] = [' '.join(x) for x in df['text']]\n    \n    def replace_with_corrected_word(df):\n        for i in range(len(df)):   \n            for w in df.loc[i, 'text'].split():\n                if w in dict_correct_speling:\n                    pattern = r'\\b'+w+r'\\b'\n                    df.loc[i, 'text'] = re.sub(pattern, dict_correct_speling[w], df.loc[i, 'text'])\n        return df\n    \n    df = replace_with_corrected_word(df)\n    \n#      After spell check is done we can perform basic NLP steps to transform our data\n\n    tokenizer = RegexpTokenizer(r'\\w+')\n    df['text'] = df['text'].apply(lambda x: tokenizer.tokenize(x))         #tokenize\n    df['text'] = df['text'].apply(lambda x: stop_word(x))                  #remove stop words\n    df['text'] = df['text'].apply(lambda x: word_lemmatizer(x))            #lemmatize\n    df['text'] = df['text'].apply(lambda x: word_stemmer(x))               #stemm\n    df['text'] = [' '.join(x) for x in df['text']]                         #join tokens as str\n    \n    \n    df['text'] = df['text'].fillna('')  #in case we have Null vals we replace it with empty str\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import dataset\ndf_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply transform function\n%time df_train = nlp_transform_df(df_train)\ndf_train.drop(['keyword', 'location'], axis = 1, inplace = True)\ndf_train.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ndf_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply transform function\n%time df_test = nlp_transform_df(df_test)\ndf_test.drop(['keyword', 'location'], axis = 1, inplace = True)\ndf_test.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train our Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def vectorizer\nvectorizer = CountVectorizer(ngram_range = (1,1), min_df = 2, analyzer='word') #checking ngrams which is better\ntrain_vectors = vectorizer.fit_transform(df_train['text'])\ntest_vectors = vectorizer.transform(df_test['text'])\n\nprint(train_vectors.shape) #train\nprint(test_vectors.shape)  #test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# voting classifier\nclassifiers = [\n    ('RandomForest', RandomForestClassifier(n_estimators=100)),\n    ('BernoulliNB', BernoulliNB()),\n    ('DecisionTree', DecisionTreeClassifier()),\n    ('NuSVC', NuSVC()),\n    ('LogisticRegression', LogisticRegression()),\n    ('MultinomialNB', MultinomialNB()),\n    ('SVC', SVC()),\n]\nclf = VotingClassifier(classifiers, n_jobs=-2)\n\nclf = VotingClassifier(classifiers, n_jobs=-2)\n\n# fit \nclf.fit(train_vectors, df_train['target'])\n\n# make a prediction on our test\ntarget = clf.predict(test_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Found answers online so we can check how well our model perform\n#or can find out through submitt on kaggle\nanswers_txt = open('/kaggle/input/answers-from-web/answers.txt', 'r')\nanswers = answers_txt.read().split()\ndf = pd.DataFrame(answers)\ndf_answer = pd.DataFrame(df.iloc[1::2]).reset_index()[0]\n\ndf_target = pd.DataFrame()\ndf_target['answer'] = df_answer.astype(int)\ndf_target['target'] = target\n\nprint('Wrong Predictions: ', len(df_target[df_target['target'] != df_target['answer']]), ' out of ', len(df_target))\nprint('Accuracy of clf: ', metrics.accuracy_score(df_target['answer'], target))\nprint('Report: ', classification_report(df_target['answer'], target))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\ndf_submission['target'] = target\ndf_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I would love to hear back if you know how to improve my code\n## PS. This is my first nltk work","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}