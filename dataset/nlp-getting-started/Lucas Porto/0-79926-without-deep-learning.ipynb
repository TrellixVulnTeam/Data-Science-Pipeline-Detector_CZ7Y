{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center>Natural Language Processing with Disaster Tweets<center>","metadata":{}},{"cell_type":"markdown","source":"The objective of this work is to present my solution for the Natural Language Processing with Disaster Tweets competition.\n\n<p style='text-align: justify;'>The vast majority of the winning solutions of this competition use neural networks, but in this work I seek to present a solution that achieves good performance using only the Multinomial Naive Bayes model. Furthermore, as a strategy for text processing, I will use the Bag of Words approach (CountVectorizer) which seems to lead us to better results than the TfidfVectorizer approach.","metadata":{}},{"cell_type":"markdown","source":"More information about this competition at https://www.kaggle.com/competitions/nlp-getting-started/overview","metadata":{}},{"cell_type":"code","source":"# Some basic imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport spacy\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split , KFold\nfrom sklearn.metrics import f1_score\nfrom IPython.display import Image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading Training and Test Files","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Dataset Dimensions\n\ntrain.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Dataset Dimensions\n\ntest.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Viewing the first 5 lines of the training Dataset\n\ntrain.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Types\n\ntrain.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# % of Real Disasters in the Training Dataset\n\n(train['target'].sum()/train['target'].count())*100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our modeling we observed that including the variables \"keyword\" and \"location\" in the models brings us worse results, so we will only pre-process the variable \"text\". So we won't worry about dealing with the missing values ​​of the 'keyword' and 'location' variables.","metadata":{}},{"cell_type":"markdown","source":"## Text preprocessing","metadata":{}},{"cell_type":"markdown","source":"Creating a function that receives a string and returns a string composed only of words composed only of alphabetic characters (letters from \"a\" to \"z\"). In addition this function applies Lemmatization.","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_preprocessing(string):\n    doc = nlp(string)\n    \n    doc2 = \" \".join([str(token) for token in doc if str(token).isalpha()])\n    \n    doc3 = nlp(doc2)\n    \n    return \" \".join([token.lemma_ for token in doc3])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating the \"pre-processed text\" column in the training and test datasets. This column is the result of preprocessing the texts present in the \"text\" column through the text_preprocessing function.","metadata":{}},{"cell_type":"code","source":"train['pre-processed text'] = train['text'].apply(text_preprocessing)\n\ntest['pre-processed text'] = test['text'].apply(text_preprocessing)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Taking a look at the preprocessing result\n\ntrain[['text' , 'pre-processed text']].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-Validation","metadata":{}},{"cell_type":"code","source":"# Multinomial Naive-Bayes\n\nmodel = MultinomialNB() ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = CountVectorizer(lowercase = True , stop_words = 'english')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(train['pre-processed text'])\ny = np.array(train['target'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting the training dataset for cross validation\n\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , stratify = y , random_state = 12)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits = 10 , shuffle = True , random_state = 451)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the average of the 10 cross-validation scores according to the f1 metric\n\nscores = []\nfor train_index , test_index in kf.split(X):\n        X_train = X[train_index]\n        X_test = X[test_index]\n        y_train = y[train_index]\n        y_test = y[test_index]\n        bow_train = vectorizer.fit_transform(X_train)\n        bow_test = vectorizer.transform(X_test)\n        model.fit(bow_train , y_train)\n        pred = model.predict(bow_test)\n        scores.append(f1_score(y_test , pred))\nnp.mean(scores)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='text-align: justify;'>A score of 74.63% is an excellent cross-validation score. By training our Bag of Words on the entire training set, the vocabulary obtained will be greater and, therefore, the chance of getting a better result when submitting to Kaggle is great. Let's see what score we get in Kaggle when we submit the predictions of a Multinomial Naive-Bayes model with the default Scikit-Learn configuration:","metadata":{}},{"cell_type":"markdown","source":"## First Submission to Kaggle","metadata":{}},{"cell_type":"code","source":"bow_train = vectorizer.fit_transform(X)\n\nbow_test = vectorizer.transform(test['pre-processed text'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training MultinomialNB()\n\nmodel.fit(bow_train , y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictions\n\ntest['target'] = model.predict(bow_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[['id' , 'target']].to_csv('submission1.csv' , index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image('../input/imagem1/kaggle1.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got a score of 0.79895 on Kaggle. To get a score of 0.79926 we need to optimize the hyperparameters. Let's see how to do this:","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Tuning ","metadata":{}},{"cell_type":"code","source":"# Splitting the training dataset for cross validation\n\nX_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.3 , stratify = y , random_state = 12)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits = 10 , shuffle = True , random_state = 451)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Observing the average of the cross-validation scores for 100 different values ​​of the alpha parameter\n\nalpha_scores = []\nalphas = np.arange(0.1 , 10.1 , 0.1)\nfor alpha in alphas :\n    scores = []\n    for train_index , test_index in kf.split(X):\n        X_train = X[train_index]\n        X_test = X[test_index]\n        y_train = y[train_index]\n        y_test = y[test_index]\n        bow_train = vectorizer.fit_transform(X_train)\n        bow_test = vectorizer.transform(X_test)\n        model = MultinomialNB(alpha = alpha)\n        model.fit(bow_train , y_train)\n        pred = model.predict(bow_test)\n        scores.append(f1_score(y_test , pred))\n    alpha_scores.append(np.mean(scores))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking at alpha values sorted by their respective average cross-validation scores\n\ndic = {'Alphas' : alphas , 'Average Score Cross-Validation' : alpha_scores}\n\ndf = pd.DataFrame(dic)\n\ndf.sort_values(by = ['Average Score Cross-Validation'] , ascending = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best scores obtained occurred for alpha = 2.2 and for alpha = 2.1 . When submitting to Kaggle the best result occurs for alpha = 2.1","metadata":{}},{"cell_type":"markdown","source":"## Second Submission to Kaggle","metadata":{}},{"cell_type":"code","source":"bow_train = vectorizer.fit_transform(X)\n\nbow_test = vectorizer.transform(test['pre-processed text'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MultinomialNB(alpha = 2.1)\n\nmodel.fit(bow_train , y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictions\n\ntest['target'] = model.predict(bow_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[['id' , 'target']].to_csv('submission2.csv' , index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image('../input/imagem12/kaggle12.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Comments\n","metadata":{}},{"cell_type":"markdown","source":"<p style='text-align: justify;'>I hope I have been able to show a simple yet effective approach to dealing with word processing problems. Of course, there are other approaches that easily allow for a better score, but most of them use neural networks. A suggestion for those looking for a better ranking is to use Google BERT (Bidirectional Encoder Representations from Transformers). This Deep Learning algorithm easily leads to a better score.","metadata":{}}]}