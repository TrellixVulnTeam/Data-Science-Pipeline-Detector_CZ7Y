{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing Required Libraries"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# General Libraries\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n# specific for data preproressing and visualization\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud,STOPWORDS \nfrom statistics import mode\n# classifiers\nfrom nltk.classify.scikitlearn import SklearnClassifier\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB,BernoulliNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC,LinearSVC,NuSVC\nfrom nltk.classify import ClassifierI","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the traub file\nTweetData =  pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\",index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning tweets without lammetization\ndef clean_tweets(x):\n    clean1 = re.sub('https?://[A-Za-z0-9./]+','',x)\n    clean2 = re.sub(r'[^\\w\\s]','',clean1).lower()\n    words = word_tokenize(clean2)\n    words = [w for w in words if not w in stop_words]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Cleaning tweets with lammetization\n# from nltk.stem import WordNetLemmatizer\n# lemmetizer  = WordNetLemmatizer()\n# def clean_tweets(x):\n#     clean1 = re.sub('https?://[A-Za-z0-9./]+','',x)\n#     clean2 = re.sub(r'[^\\w\\s]','',clean1).lower()\n#     words = word_tokenize(clean2)\n#     words = [w for w in words if not w in stop_words]\n#     words =[lemmetizer.lemmatize(w) for w in words]\n#     return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData['words'] = TweetData['text'].apply(lambda x: clean_tweets(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will use POS tagging ( which is used for tagging a word for its part of speech as per engilsh grammer) for filtering words which can be used as features for classification of tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# POS tagging code\n'''CC coordinating conjunction\nCD cardinal digit\nDT determiner\nEX existential there (like: \"there is\" ... think of it like \"there exists\")\nFW foreign word\nIN preposition/subordinating conjunction\nJJ adjective 'big'\nJJR adjective, comparative 'bigger'\nJJS adjective, superlative 'biggest'\nLS list marker 1)\nMD modal could, will\nNN noun, singular 'desk'\nNNS noun plural 'desks'\nNNP proper noun, singular 'Harrison'\nNNPS proper noun, plural 'Americans'\nPDT predeterminer 'all the kids'\nPOS possessive ending parent's\nPRP personal pronoun I, he, she\nPRP$ possessive pronoun my, his, hers\nRB adverb very, silently,\nRBR adverb, comparative better\nRBS adverb, superlative best\nRP particle give up\nTO to go 'to' the store.\nUH interjection errrrrrrrm\nVB verb, base form take\nVBD verb, past tense took\nVBG verb, gerund/present participle taking\nVBN verb, past participle taken\nVBP verb, sing. present, non-3d take\nVBZ verb, 3rd person sing. present takes\nWDT wh-determiner which\nWP wh-pronoun who, what\nWP$ possessive wh-pronoun whose\nWRB wh-abverb where, when''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All word Freqency curve with out any pos tagging filter\nAll_words = []\nfor words in TweetData['words']:\n    for word in words:\n            All_words.append(word)\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nsns.barplot('Words','freq',data = Freq_word_DF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets see top 15 words based on its frequency after filtering on POS tagging**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# All word adjective Freqency curve \nallowed_word_type = [\"JJ\",\"JJR\",\"JJS\"]\nAll_words = []\nfor words in TweetData['words']:\n    pos = nltk.pos_tag(words)\n    for w in pos:\n        if w[1] in allowed_word_type:\n                    All_words.append(w[0])\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nsns.barplot('Words','freq',data = Freq_word_DF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All word verb Freqency curve \nallowed_word_type = [\"VB\",\"VBD\",\"VBN\",\"VBP\",\"VBZ\"]\nAll_words = []\nfor words in TweetData['words']:\n    pos = nltk.pos_tag(words)\n    for w in pos:\n        if w[1] in allowed_word_type:\n                    All_words.append(w[0])\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nsns.barplot('Words','freq',data = Freq_word_DF)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All word Noun Freqency curve \nallowed_word_type = [\"NN\",\"NNS\"]\nAll_words = []\nfor words in TweetData['words']:\n    pos = nltk.pos_tag(words)\n    for w in pos:\n        if w[1] in allowed_word_type:\n                    All_words.append(w[0])\nAll_words_freq = nltk.FreqDist(All_words)\nFreq_word_DF = pd.DataFrame({\"Data\":All_words_freq.most_common(15)})\nFreq_word_DF['Words'] = Freq_word_DF['Data'].apply(lambda x : x[0])\nFreq_word_DF['freq'] = Freq_word_DF['Data'].apply(lambda x : x[1])\nsns.set()\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.2})\nsns.set_context(\"paper\", font_scale=2)\nfig=plt.figure(figsize =(20,8),dpi=50)\nsns.barplot('Words','freq',data = Freq_word_DF)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets make a word cloud from the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# wordcloud from the words\nfrom wordcloud import WordCloud,STOPWORDS \nstopwords = set(STOPWORDS) \ncomment_words = ' '\nfor text in TweetData['words']:\n    for words in text: \n        comment_words = comment_words + words + ' '\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words)\nsns.set()\nplt.figure(figsize = (8, 8), facecolor = None,dpi=100) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing the data for model building**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Frequency data of words \n# allowed_word_type = [\"NN\",\"NNS\"]\n# all_words = []\n# for words in TweetData['words']:\n#     pos = nltk.pos_tag(words)\n#     for w in pos:\n#         if w[1] in allowed_word_type:\n#                     all_words.append(w[0])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allowed_word_type = [\"NN\",\"NNS\",\"JJ\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency Data by target\n# for target 1\nall_words = []\nfor words in TweetData[TweetData['target']==1]['words']:\n    pos = nltk.pos_tag(words)\n    for w in pos:\n        if w[1] in allowed_word_type:\n            if w[0] not in ['im','amp']:\n                all_words.append(w[0])\nall_words_freq_1 = nltk.FreqDist(all_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for target 0\nall_words = []\nfor words in TweetData[TweetData['target']==0]['words']:\n    pos = nltk.pos_tag(words)\n    for w in pos:\n        if w[1] in allowed_word_type:\n            if w[0] not in ['im','amp']:\n                all_words.append(w[0])\nall_words_freq_0 = nltk.FreqDist(all_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature1 =[w[0] for w in all_words_freq_1.most_common(1000)]\nfeature0 = [w[0] for w in all_words_freq_0.most_common(500)]\nword_feature = list(set(feature1 + feature0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# all_words_freq = nltk.FreqDist(all_words)\n# word_feature = list(all_words_freq.keys())[:2000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetData['combined']  = TweetData[['words', 'target']].apply(tuple, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_features(doc):\n    words =  set(doc)\n    features = {}\n    for w in word_feature:\n        features[w] = (w in words)\n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"featuresets = [(find_features(tweet),category) for (tweet,category) in TweetData['combined']]\nlen(featuresets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_set = featuresets[:5000]\ntesting_set = featuresets[5000:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying Naivebase Classifier of NLTK"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier =  nltk.NaiveBayesClassifier.train(training_set)\nprint(\"Naive Base algorith accuracy : \", nltk.classify.accuracy(classifier,testing_set)*100)\nclassifier.show_most_informative_features(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Applying SK Learn Classifiers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"MNB_classifier = SklearnClassifier(MultinomialNB())\nMNB_classifier.train(training_set)\n# Guss_classifier = SklearnClassifier(GaussianNB())\n# Guss_classifier.train(training_set)\nBerr_classifier = SklearnClassifier(BernoulliNB())\nBerr_classifier.train(training_set)\nlogistic_classifier = SklearnClassifier(LogisticRegression())\nlogistic_classifier.train(training_set)\nSDG_classifier = SklearnClassifier(SGDClassifier())\nSDG_classifier.train(training_set)\nrandom_classifier = SklearnClassifier(RandomForestClassifier())\nrandom_classifier.train(training_set)\nSVC_classifier = SklearnClassifier(SVC())\nSVC_classifier.train(training_set)\nLinerSVC_classifier =  SklearnClassifier(LinearSVC())\nLinerSVC_classifier.train(training_set)\nNUSVC_classifier = SklearnClassifier(NuSVC())\nNUSVC_classifier.train(training_set)\nprint(\"MNB algorith accuracy : \", nltk.classify.accuracy(MNB_classifier,testing_set)*100)\nprint(\"BErr algorith accuracy : \", nltk.classify.accuracy(Berr_classifier,testing_set)*100)\nprint(\"Logistic algorith accuracy : \", nltk.classify.accuracy(logistic_classifier,testing_set)*100)\nprint(\"SDG algorith accuracy : \", nltk.classify.accuracy(SDG_classifier,testing_set)*100)\nprint(\"Random Forest algorith accuracy : \", nltk.classify.accuracy(random_classifier,testing_set)*100)\nprint(\"SVC algorith accuracy : \", nltk.classify.accuracy(SVC_classifier,testing_set)*100)\nprint(\"LinerSVC algorith accuracy : \", nltk.classify.accuracy(LinerSVC_classifier,testing_set)*100)\nprint(\"NUSVC algorith accuracy : \", nltk.classify.accuracy(NUSVC_classifier,testing_set)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Developing a vote based combination of all classifiers**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Voteclssifier(ClassifierI):\n    def __init__(self,*classifiers):\n        self._classifiers = classifiers\n        \n    def classify(self,features):\n        vote = []\n        for c in self._classifiers:\n            v = c.classify(features)\n            vote.append(v)\n#         if vote.count(1)== vote.count(0):\n#             s=1\n#         else:\n#             s=mode(vote)\n        return mode(vote)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"votedclassifier = Voteclssifier(classifier,MNB_classifier,Berr_classifier,logistic_classifier,NUSVC_classifier)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"VoteVlassifier algorith accuracy : \", nltk.classify.accuracy(votedclassifier,testing_set)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\",index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['words'] = test['text'].apply(lambda x: clean_tweets(x))\ntest['feature'] = test['words'].apply(lambda x : find_features(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"votedclassifier.classify(test['feature'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = test['feature'].apply(lambda x : votedclassifier.classify(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Submission =  pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\",index_col=0)\nSubmission['target'] = test['target']\nSubmission.to_csv(\"Submission.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}