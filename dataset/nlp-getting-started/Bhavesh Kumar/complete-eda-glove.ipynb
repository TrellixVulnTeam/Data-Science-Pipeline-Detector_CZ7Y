{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Disaster or Not Disaster Tweets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true}},{"cell_type":"markdown","source":"## NLP:\n* EDA (with WordCloud)\n* Bag of Words\n* TF IDF\n* GloVe\n* PCA visualization for the main models\n* Showing Confusion Matrices for GloVe","metadata":{}},{"cell_type":"markdown","source":"Reference - https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert","metadata":{}},{"cell_type":"markdown","source":"## About Data\nEach sample in the train and test set has the following information:\n\n* The text of a tweet\n* A keyword from that tweet\n* The location the tweet was sent from","metadata":{}},{"cell_type":"markdown","source":"## Import necessary Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop_words=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:33.269065Z","iopub.execute_input":"2021-07-30T11:05:33.269565Z","iopub.status.idle":"2021-07-30T11:05:41.701567Z","shell.execute_reply.started":"2021-07-30T11:05:33.26946Z","shell.execute_reply":"2021-07-30T11:05:41.700608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:41.703898Z","iopub.execute_input":"2021-07-30T11:05:41.704243Z","iopub.status.idle":"2021-07-30T11:05:41.796483Z","shell.execute_reply.started":"2021-07-30T11:05:41.704211Z","shell.execute_reply":"2021-07-30T11:05:41.795263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:41.798178Z","iopub.execute_input":"2021-07-30T11:05:41.798491Z","iopub.status.idle":"2021-07-30T11:05:41.834887Z","shell.execute_reply.started":"2021-07-30T11:05:41.79846Z","shell.execute_reply":"2021-07-30T11:05:41.833722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:41.836491Z","iopub.execute_input":"2021-07-30T11:05:41.836788Z","iopub.status.idle":"2021-07-30T11:05:41.849765Z","shell.execute_reply.started":"2021-07-30T11:05:41.836759Z","shell.execute_reply":"2021-07-30T11:05:41.848691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Class Distribution\nlet's check the class distribution","metadata":{}},{"cell_type":"code","source":"sns.countplot(x = 'target', data=train)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:41.85337Z","iopub.execute_input":"2021-07-30T11:05:41.854042Z","iopub.status.idle":"2021-07-30T11:05:42.004805Z","shell.execute_reply.started":"2021-07-30T11:05:41.853954Z","shell.execute_reply":"2021-07-30T11:05:42.003864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the class is little biased towards the Disaster tweets","metadata":{}},{"cell_type":"markdown","source":"## Calculate number of words in texts\n","metadata":{}},{"cell_type":"code","source":"train['num_words'] = train['text'].apply(lambda x: len(str(x).split()))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:42.007501Z","iopub.execute_input":"2021-07-30T11:05:42.008061Z","iopub.status.idle":"2021-07-30T11:05:42.029554Z","shell.execute_reply.started":"2021-07-30T11:05:42.008022Z","shell.execute_reply":"2021-07-30T11:05:42.028357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:42.031159Z","iopub.execute_input":"2021-07-30T11:05:42.03149Z","iopub.status.idle":"2021-07-30T11:05:42.045179Z","shell.execute_reply.started":"2021-07-30T11:05:42.031458Z","shell.execute_reply":"2021-07-30T11:05:42.044202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot number of words to check","metadata":{}},{"cell_type":"code","source":"sns.distplot(x=train[train['target'] == 0]['num_words'], label='Not Disaster')\nsns.distplot(x=train[train['target'] == 1]['num_words'], label='Disaster')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:42.046687Z","iopub.execute_input":"2021-07-30T11:05:42.047016Z","iopub.status.idle":"2021-07-30T11:05:42.698789Z","shell.execute_reply.started":"2021-07-30T11:05:42.046984Z","shell.execute_reply":"2021-07-30T11:05:42.697975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"so here we can see both categories are in the normalized form","metadata":{}},{"cell_type":"markdown","source":"## let's calculate average length of words","metadata":{}},{"cell_type":"code","source":"train['avg_len_words'] = train['text'].apply(lambda x: str(x).split())\ntrain['avg_len_words'] = train['avg_len_words'].apply(lambda x: [len(word) for word in x])","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:42.700359Z","iopub.execute_input":"2021-07-30T11:05:42.700655Z","iopub.status.idle":"2021-07-30T11:05:42.754882Z","shell.execute_reply.started":"2021-07-30T11:05:42.700626Z","shell.execute_reply":"2021-07-30T11:05:42.753416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['avg_len_words'] = train['avg_len_words'].apply(lambda x: np.mean(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:42.756603Z","iopub.execute_input":"2021-07-30T11:05:42.756944Z","iopub.status.idle":"2021-07-30T11:05:42.909235Z","shell.execute_reply.started":"2021-07-30T11:05:42.756887Z","shell.execute_reply":"2021-07-30T11:05:42.908116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(x=train[train['target'] == 0]['avg_len_words'], label='Not Disaster')\nsns.distplot(x=train[train['target'] == 1]['avg_len_words'], label='Disaster')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:42.910777Z","iopub.execute_input":"2021-07-30T11:05:42.911137Z","iopub.status.idle":"2021-07-30T11:05:43.408972Z","shell.execute_reply.started":"2021-07-30T11:05:42.911101Z","shell.execute_reply":"2021-07-30T11:05:43.407931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of character's in the tweet","metadata":{}},{"cell_type":"code","source":"train['length'] = train['text'].apply(lambda x: len(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:43.410813Z","iopub.execute_input":"2021-07-30T11:05:43.411208Z","iopub.status.idle":"2021-07-30T11:05:43.423659Z","shell.execute_reply.started":"2021-07-30T11:05:43.411173Z","shell.execute_reply":"2021-07-30T11:05:43.422305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nsns.distplot(x=train[train['target'] == 1]['length'], label='Disaster')\nsns.distplot(x=train[train['target'] == 0]['length'], label='Not Disaster')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:43.425359Z","iopub.execute_input":"2021-07-30T11:05:43.425693Z","iopub.status.idle":"2021-07-30T11:05:43.818351Z","shell.execute_reply.started":"2021-07-30T11:05:43.42566Z","shell.execute_reply":"2021-07-30T11:05:43.816942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of both seems to be almost same.120 to 140 characters in a tweet are the most common among both.","metadata":{}},{"cell_type":"markdown","source":"## Create function for Diaster and Non-Disaster tweet words","metadata":{}},{"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n    for words in train[train['target'] == target]['text'].str.split():\n        for word in words:\n            corpus.append(word)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:43.819833Z","iopub.execute_input":"2021-07-30T11:05:43.820181Z","iopub.status.idle":"2021-07-30T11:05:43.827119Z","shell.execute_reply.started":"2021-07-30T11:05:43.820143Z","shell.execute_reply":"2021-07-30T11:05:43.825856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First we will analyze tweets with class 0","metadata":{}},{"cell_type":"code","source":"corpus = create_corpus(0)\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop_words:\n        dic[word] += 1\n\ntop = sorted(dic.items(), key = lambda x: x[1], reverse=True)[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:43.828612Z","iopub.execute_input":"2021-07-30T11:05:43.828938Z","iopub.status.idle":"2021-07-30T11:05:43.871973Z","shell.execute_reply.started":"2021-07-30T11:05:43.828877Z","shell.execute_reply":"2021-07-30T11:05:43.870652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Common stopwords","metadata":{}},{"cell_type":"code","source":"np.array(stop_words)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:43.873444Z","iopub.execute_input":"2021-07-30T11:05:43.873749Z","iopub.status.idle":"2021-07-30T11:05:43.882037Z","shell.execute_reply.started":"2021-07-30T11:05:43.873719Z","shell.execute_reply":"2021-07-30T11:05:43.880791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nx, y = zip(*top)\nplt.bar(x, y, color='green')\nplt.title('Top 10 stop words in Non-Disaster Tweets', fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:43.883612Z","iopub.execute_input":"2021-07-30T11:05:43.884022Z","iopub.status.idle":"2021-07-30T11:05:44.088509Z","shell.execute_reply.started":"2021-07-30T11:05:43.883919Z","shell.execute_reply":"2021-07-30T11:05:44.087116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now create corpus for class 1","metadata":{}},{"cell_type":"code","source":"corpus1 = create_corpus(1)\n\ndic1 = defaultdict(int)\nfor word in corpus1:\n    if word in stop_words:\n        dic1[word] += 1\n\ntop1 = sorted(dic1.items(), key = lambda x: x[1], reverse=True)[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:44.089945Z","iopub.execute_input":"2021-07-30T11:05:44.090293Z","iopub.status.idle":"2021-07-30T11:05:44.125106Z","shell.execute_reply.started":"2021-07-30T11:05:44.09026Z","shell.execute_reply":"2021-07-30T11:05:44.123844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nx, y = zip(*top1)\nplt.bar(x, y, color='red')\nplt.title('Top 10 stop words in Disaster Tweets', fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:44.128973Z","iopub.execute_input":"2021-07-30T11:05:44.129291Z","iopub.status.idle":"2021-07-30T11:05:44.310619Z","shell.execute_reply.started":"2021-07-30T11:05:44.129261Z","shell.execute_reply":"2021-07-30T11:05:44.309364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.","metadata":{}},{"cell_type":"markdown","source":"## Analyzing Punctuations","metadata":{}},{"cell_type":"markdown","source":"### for class 0","metadata":{}},{"cell_type":"code","source":"dic = defaultdict(int)\nfor word in corpus:\n    if word in string.punctuation:\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key = lambda x: x[1], reverse=True)\n\nplt.figure(figsize=(10, 5))\nx, y = zip(*top)\nplt.bar(x, y, color='green')\nplt.title('Punctuation in Non Disaster Tweets', fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:44.312427Z","iopub.execute_input":"2021-07-30T11:05:44.31287Z","iopub.status.idle":"2021-07-30T11:05:44.587189Z","shell.execute_reply.started":"2021-07-30T11:05:44.312827Z","shell.execute_reply":"2021-07-30T11:05:44.586202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### for class 1","metadata":{}},{"cell_type":"code","source":"dic = defaultdict(int)\nfor word in corpus1:\n    if word in string.punctuation:\n        dic[word] += 1\n        \ntop = sorted(dic.items(), key = lambda x: x[1], reverse=True)\n\nplt.figure(figsize=(10, 5))\nx, y = zip(*top)\nplt.bar(x, y, color='red')\nplt.title('Punctuation in Disaster Tweets', fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:44.588276Z","iopub.execute_input":"2021-07-30T11:05:44.588564Z","iopub.status.idle":"2021-07-30T11:05:44.864Z","shell.execute_reply.started":"2021-07-30T11:05:44.588536Z","shell.execute_reply":"2021-07-30T11:05:44.862989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Common words in tweets","metadata":{}},{"cell_type":"code","source":"counter = Counter(corpus)\n\ncommon_words = counter.most_common()\nx = []\ny = []\nfor word, count in common_words[:40]:\n    if word not in stop_words:\n        if word not in string.punctuation:\n            x.append(word)\n            y.append(count)\n\nplt.figure(figsize=(8, 8))\nsns.barplot(x=y, y=x)\nplt.title('Common words in Non Disaster tweets', fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:44.865503Z","iopub.execute_input":"2021-07-30T11:05:44.865806Z","iopub.status.idle":"2021-07-30T11:05:45.063105Z","shell.execute_reply.started":"2021-07-30T11:05:44.865772Z","shell.execute_reply":"2021-07-30T11:05:45.062246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lot of cleaning needed !","metadata":{}},{"cell_type":"markdown","source":" ### Let's First clean tweets","metadata":{}},{"cell_type":"code","source":"df = pd.concat([train, test])","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.064205Z","iopub.execute_input":"2021-07-30T11:05:45.064603Z","iopub.status.idle":"2021-07-30T11:05:45.072012Z","shell.execute_reply.started":"2021-07-30T11:05:45.064572Z","shell.execute_reply":"2021-07-30T11:05:45.071085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.073353Z","iopub.execute_input":"2021-07-30T11:05:45.073808Z","iopub.status.idle":"2021-07-30T11:05:45.083157Z","shell.execute_reply.started":"2021-07-30T11:05:45.073776Z","shell.execute_reply":"2021-07-30T11:05:45.082001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_urls(text):\n    '''Removes url from the tweets'''\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\nremove_urls(example)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.084723Z","iopub.execute_input":"2021-07-30T11:05:45.085123Z","iopub.status.idle":"2021-07-30T11:05:45.097573Z","shell.execute_reply.started":"2021-07-30T11:05:45.085091Z","shell.execute_reply":"2021-07-30T11:05:45.09653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: remove_urls(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.099169Z","iopub.execute_input":"2021-07-30T11:05:45.099665Z","iopub.status.idle":"2021-07-30T11:05:45.162737Z","shell.execute_reply.started":"2021-07-30T11:05:45.099625Z","shell.execute_reply":"2021-07-30T11:05:45.161179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.164613Z","iopub.execute_input":"2021-07-30T11:05:45.164989Z","iopub.status.idle":"2021-07-30T11:05:45.169815Z","shell.execute_reply.started":"2021-07-30T11:05:45.16494Z","shell.execute_reply":"2021-07-30T11:05:45.168563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_html(text):\n    '''Removes html from the tweets'''\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\n\nprint(remove_html(example))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.171658Z","iopub.execute_input":"2021-07-30T11:05:45.172024Z","iopub.status.idle":"2021-07-30T11:05:45.185436Z","shell.execute_reply.started":"2021-07-30T11:05:45.171977Z","shell.execute_reply":"2021-07-30T11:05:45.184133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: remove_html(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.18702Z","iopub.execute_input":"2021-07-30T11:05:45.187425Z","iopub.status.idle":"2021-07-30T11:05:45.218069Z","shell.execute_reply.started":"2021-07-30T11:05:45.187387Z","shell.execute_reply":"2021-07-30T11:05:45.216871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_emojis(text):\n    '''Removes emojis from the tweets'''\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emojis('Omg another Earthquake 😔😔')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.220179Z","iopub.execute_input":"2021-07-30T11:05:45.220626Z","iopub.status.idle":"2021-07-30T11:05:45.234693Z","shell.execute_reply.started":"2021-07-30T11:05:45.220579Z","shell.execute_reply":"2021-07-30T11:05:45.233801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: remove_emojis(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.235698Z","iopub.execute_input":"2021-07-30T11:05:45.236001Z","iopub.status.idle":"2021-07-30T11:05:45.334287Z","shell.execute_reply.started":"2021-07-30T11:05:45.235971Z","shell.execute_reply":"2021-07-30T11:05:45.333334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punct(text):\n    '''Removes punctuations from the tweets'''\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\nexample=\"I am ,.a #king\"\nprint(remove_punct(example))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.341037Z","iopub.execute_input":"2021-07-30T11:05:45.341549Z","iopub.status.idle":"2021-07-30T11:05:45.34772Z","shell.execute_reply.started":"2021-07-30T11:05:45.341515Z","shell.execute_reply":"2021-07-30T11:05:45.346592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: remove_punct(x))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.349427Z","iopub.execute_input":"2021-07-30T11:05:45.349742Z","iopub.status.idle":"2021-07-30T11:05:45.430918Z","shell.execute_reply.started":"2021-07-30T11:05:45.349713Z","shell.execute_reply":"2021-07-30T11:05:45.429883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## N-grams analysis","metadata":{}},{"cell_type":"code","source":"def get_top_ngrams(corpus,n_grams, n=None):\n    vec = CountVectorizer(ngram_range=(n_grams,n_grams)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    \n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.432319Z","iopub.execute_input":"2021-07-30T11:05:45.432618Z","iopub.status.idle":"2021-07-30T11:05:45.439655Z","shell.execute_reply.started":"2021-07-30T11:05:45.432588Z","shell.execute_reply":"2021-07-30T11:05:45.43854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bigram analysis","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\n\ntop_tweet_ngrams = get_top_ngrams(df['text'], 2, 10)\nx, y = map(list, zip(*top_tweet_ngrams))\nsns.barplot(x=y, y=x)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:45.440868Z","iopub.execute_input":"2021-07-30T11:05:45.441205Z","iopub.status.idle":"2021-07-30T11:05:46.861587Z","shell.execute_reply.started":"2021-07-30T11:05:45.441174Z","shell.execute_reply":"2021-07-30T11:05:46.860389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's see tri-gram analysis for diaster and non-disaster tweets","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\n\ntop_tweet_ngrams = get_top_ngrams(df[df['target'] == 0]['text'], 3, 10)\nx, y = map(list, zip(*top_tweet_ngrams))\nsns.barplot(x=y, y=x)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:46.863059Z","iopub.execute_input":"2021-07-30T11:05:46.863477Z","iopub.status.idle":"2021-07-30T11:05:47.583603Z","shell.execute_reply.started":"2021-07-30T11:05:46.863445Z","shell.execute_reply":"2021-07-30T11:05:47.582253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\n\ntop_tweet_ngrams = get_top_ngrams(df[df['target'] == 1]['text'], 3, 10)\nx, y = map(list, zip(*top_tweet_ngrams))\nsns.barplot(x=y, y=x)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:47.587317Z","iopub.execute_input":"2021-07-30T11:05:47.587637Z","iopub.status.idle":"2021-07-30T11:05:48.182546Z","shell.execute_reply.started":"2021-07-30T11:05:47.587606Z","shell.execute_reply":"2021-07-30T11:05:48.181684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can see the clear difference in words of both disaster and non diaster tweets ","metadata":{}},{"cell_type":"markdown","source":"## WordCloud","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:48.184131Z","iopub.execute_input":"2021-07-30T11:05:48.184441Z","iopub.status.idle":"2021-07-30T11:05:48.189452Z","shell.execute_reply.started":"2021-07-30T11:05:48.184413Z","shell.execute_reply":"2021-07-30T11:05:48.188064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_corpus_df(df, target):\n    corpus = []\n    for words in df[df['target'] == target]['text'].str.split():\n        for word in words:\n            corpus.append(word)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:48.191329Z","iopub.execute_input":"2021-07-30T11:05:48.191696Z","iopub.status.idle":"2021-07-30T11:05:48.202563Z","shell.execute_reply.started":"2021-07-30T11:05:48.191666Z","shell.execute_reply":"2021-07-30T11:05:48.201145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Non-Disaster Tweets","metadata":{}},{"cell_type":"code","source":"corpus_new0 = create_corpus_df(df, 0)\nlen(corpus_new0)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:48.204533Z","iopub.execute_input":"2021-07-30T11:05:48.204891Z","iopub.status.idle":"2021-07-30T11:05:48.235367Z","shell.execute_reply.started":"2021-07-30T11:05:48.20486Z","shell.execute_reply":"2021-07-30T11:05:48.233704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_new0[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:48.236992Z","iopub.execute_input":"2021-07-30T11:05:48.23732Z","iopub.status.idle":"2021-07-30T11:05:48.243988Z","shell.execute_reply.started":"2021-07-30T11:05:48.237288Z","shell.execute_reply":"2021-07-30T11:05:48.242531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12, 8))\nwordcloud = WordCloud(background_color='black', max_font_size=80).generate(\" \".join(corpus_new0[:50]))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:48.245808Z","iopub.execute_input":"2021-07-30T11:05:48.24651Z","iopub.status.idle":"2021-07-30T11:05:48.470343Z","shell.execute_reply.started":"2021-07-30T11:05:48.246461Z","shell.execute_reply":"2021-07-30T11:05:48.469108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Disaster tweets","metadata":{}},{"cell_type":"code","source":"corpus_new1 = create_corpus_df(df, 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:48.471915Z","iopub.execute_input":"2021-07-30T11:05:48.472539Z","iopub.status.idle":"2021-07-30T11:05:48.496428Z","shell.execute_reply.started":"2021-07-30T11:05:48.472492Z","shell.execute_reply":"2021-07-30T11:05:48.495281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_new1[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:48.4984Z","iopub.execute_input":"2021-07-30T11:05:48.49883Z","iopub.status.idle":"2021-07-30T11:05:48.505848Z","shell.execute_reply.started":"2021-07-30T11:05:48.498786Z","shell.execute_reply":"2021-07-30T11:05:48.50503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nwordcloud = WordCloud(background_color='black', max_font_size=80).generate(\" \".join(corpus_new1[:50]))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:48.507315Z","iopub.execute_input":"2021-07-30T11:05:48.507598Z","iopub.status.idle":"2021-07-30T11:05:48.725779Z","shell.execute_reply.started":"2021-07-30T11:05:48.50757Z","shell.execute_reply":"2021-07-30T11:05:48.725007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:48.727009Z","iopub.execute_input":"2021-07-30T11:05:48.72751Z","iopub.status.idle":"2021-07-30T11:05:48.746622Z","shell.execute_reply.started":"2021-07-30T11:05:48.727479Z","shell.execute_reply":"2021-07-30T11:05:48.745266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bag of Words counts","metadata":{"trusted":true}},{"cell_type":"code","source":"def cv(data):\n    count_vectorizer = CountVectorizer()\n    emb = count_vectorizer.fit_transform(data)\n    return emb, count_vectorizer\n\nlist_corpus = df['text'].tolist()\nlist_labels = df['target'].tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2,\n                                                    random_state=1)\n\nX_train_counts, count_vectorizer = cv(X_train)\nX_test_counts = count_vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:48.748111Z","iopub.execute_input":"2021-07-30T11:05:48.748439Z","iopub.status.idle":"2021-07-30T11:05:49.045577Z","shell.execute_reply.started":"2021-07-30T11:05:48.748411Z","shell.execute_reply":"2021-07-30T11:05:49.044383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing the Embeddings","metadata":{}},{"cell_type":"code","source":"def plot_LSA(test_data, test_labels, savepath='PCA_demp.csv', plot=True):\n    lsa = TruncatedSVD(n_components=2)\n    lsa.fit(test_data)\n    lsa_scores = lsa.transform(test_data)\n    color_mapper = {label:idx for idx, label in enumerate(set(test_labels))}\n    color_column = [color_mapper[label] for label in test_labels]\n    colors = ['orange', 'blue']\n    if plot:\n        plt.scatter(lsa_scores[:, 0], lsa_scores[:, 1], s=8, alpha=0.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n        orange_patch = mpatches.Patch(color='orange', label='Non Disaster')\n        blue_patch = mpatches.Patch(color='blue', label='Disaster')\n        plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n        \nfig = plt.figure(figsize=(16, 16))\nplot_LSA(X_train_counts, y_train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:49.04695Z","iopub.execute_input":"2021-07-30T11:05:49.047316Z","iopub.status.idle":"2021-07-30T11:05:49.886303Z","shell.execute_reply.started":"2021-07-30T11:05:49.047285Z","shell.execute_reply":"2021-07-30T11:05:49.884887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TF IDF vectorizer","metadata":{}},{"cell_type":"code","source":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n    \n    train = tfidf_vectorizer.fit_transform(data)\n    \n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:49.887704Z","iopub.execute_input":"2021-07-30T11:05:49.888074Z","iopub.status.idle":"2021-07-30T11:05:50.205873Z","shell.execute_reply.started":"2021-07-30T11:05:49.888039Z","shell.execute_reply":"2021-07-30T11:05:50.204558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(16,16))\nplot_LSA(X_train_tfidf, y_train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:50.20728Z","iopub.execute_input":"2021-07-30T11:05:50.207582Z","iopub.status.idle":"2021-07-30T11:05:51.008209Z","shell.execute_reply.started":"2021-07-30T11:05:50.207553Z","shell.execute_reply":"2021-07-30T11:05:51.006747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GloVe Model\nGloVe method is built on an important idea,\n  \n  **\"You can derive semantic relationships between words   from the co-occurrence matrix.\"**\n  \nGiven a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follow.\n\n![Glove-matrix](https://miro.medium.com/max/347/1*QWcK8CIDs8kMkOwsOxvywA.png)","metadata":{}},{"cell_type":"markdown","source":"Read more about GloVe - https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010","metadata":{}},{"cell_type":"markdown","source":"Here we will use GloVe pretrained corpus model to represent our words. It is available in 3 varieties : 50D, 100D and 200 Dimentional. We will try 100D here.","metadata":{}},{"cell_type":"code","source":"def create_corpus_new(df):\n    corpus = []\n    for text in tqdm(df['text']):\n        words = [word.lower() for word in word_tokenize(text)]\n        corpus.append(words)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:51.009949Z","iopub.execute_input":"2021-07-30T11:05:51.010401Z","iopub.status.idle":"2021-07-30T11:05:51.017238Z","shell.execute_reply.started":"2021-07-30T11:05:51.010358Z","shell.execute_reply":"2021-07-30T11:05:51.015941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = create_corpus_new(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:51.018885Z","iopub.execute_input":"2021-07-30T11:05:51.019255Z","iopub.status.idle":"2021-07-30T11:05:53.682749Z","shell.execute_reply.started":"2021-07-30T11:05:51.019225Z","shell.execute_reply":"2021-07-30T11:05:53.68166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create embedding dictionary to store vecotors","metadata":{}},{"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:05:53.684012Z","iopub.execute_input":"2021-07-30T11:05:53.684309Z","iopub.status.idle":"2021-07-30T11:06:12.644974Z","shell.execute_reply.started":"2021-07-30T11:05:53.68428Z","shell.execute_reply":"2021-07-30T11:06:12.643611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize the text using Tokenizer()","metadata":{}},{"cell_type":"code","source":"Max_len = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences, maxlen=Max_len, truncating='post', padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:06:12.646794Z","iopub.execute_input":"2021-07-30T11:06:12.647332Z","iopub.status.idle":"2021-07-30T11:06:13.025356Z","shell.execute_reply.started":"2021-07-30T11:06:12.647284Z","shell.execute_reply":"2021-07-30T11:06:13.024403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer_obj.word_index\nprint('Number of unique words : ', len(word_index))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:06:13.026703Z","iopub.execute_input":"2021-07-30T11:06:13.027042Z","iopub.status.idle":"2021-07-30T11:06:13.031718Z","shell.execute_reply.started":"2021-07-30T11:06:13.027008Z","shell.execute_reply":"2021-07-30T11:06:13.030932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create embedding matrix of words","metadata":{}},{"cell_type":"code","source":"num_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words, 100))\n\nfor word, i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:06:13.03302Z","iopub.execute_input":"2021-07-30T11:06:13.033312Z","iopub.status.idle":"2021-07-30T11:06:13.120268Z","shell.execute_reply.started":"2021-07-30T11:06:13.033285Z","shell.execute_reply":"2021-07-30T11:06:13.119069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_pad[0][0:]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:06:13.121986Z","iopub.execute_input":"2021-07-30T11:06:13.122323Z","iopub.status.idle":"2021-07-30T11:06:13.129381Z","shell.execute_reply.started":"2021-07-30T11:06:13.122295Z","shell.execute_reply":"2021-07-30T11:06:13.128046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline Model with GloVe results","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nembedding = Embedding(num_words, 100, embeddings_initializer=Constant(embedding_matrix), input_length=Max_len, trainable=False)\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\noptimizer = Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:06:13.130895Z","iopub.execute_input":"2021-07-30T11:06:13.131201Z","iopub.status.idle":"2021-07-30T11:06:13.450264Z","shell.execute_reply.started":"2021-07-30T11:06:13.131171Z","shell.execute_reply":"2021-07-30T11:06:13.449017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:06:13.451824Z","iopub.execute_input":"2021-07-30T11:06:13.452258Z","iopub.status.idle":"2021-07-30T11:06:13.460462Z","shell.execute_reply.started":"2021-07-30T11:06:13.452224Z","shell.execute_reply":"2021-07-30T11:06:13.459049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the data into train and test","metadata":{}},{"cell_type":"code","source":"train1 = tweet_pad[:train.shape[0]]\ntest1 = tweet_pad[train.shape[0]:]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:06:13.462197Z","iopub.execute_input":"2021-07-30T11:06:13.462687Z","iopub.status.idle":"2021-07-30T11:06:13.471767Z","shell.execute_reply.started":"2021-07-30T11:06:13.462646Z","shell.execute_reply":"2021-07-30T11:06:13.470618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train1, train['target'].values, test_size=0.2)\nprint('Shape of train', X_train.shape)\nprint('Shape of validation', X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:06:13.473629Z","iopub.execute_input":"2021-07-30T11:06:13.474131Z","iopub.status.idle":"2021-07-30T11:06:13.488374Z","shell.execute_reply.started":"2021-07-30T11:06:13.474082Z","shell.execute_reply":"2021-07-30T11:06:13.487428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:06:13.489593Z","iopub.execute_input":"2021-07-30T11:06:13.489926Z","iopub.status.idle":"2021-07-30T11:06:13.510631Z","shell.execute_reply.started":"2021-07-30T11:06:13.489892Z","shell.execute_reply":"2021-07-30T11:06:13.509293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(16, 16))\nplot_LSA(train1, train['target'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:06:13.512153Z","iopub.execute_input":"2021-07-30T11:06:13.512505Z","iopub.status.idle":"2021-07-30T11:06:14.163159Z","shell.execute_reply.started":"2021-07-30T11:06:13.512471Z","shell.execute_reply":"2021-07-30T11:06:14.159114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit the model","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, batch_size=4, epochs=10,\n                    validation_data=(X_test, y_test), verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:06:14.165608Z","iopub.execute_input":"2021-07-30T11:06:14.166066Z","iopub.status.idle":"2021-07-30T11:22:21.646532Z","shell.execute_reply.started":"2021-07-30T11:06:14.166024Z","shell.execute_reply":"2021-07-30T11:22:21.645213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pred_Glove = model.predict(train1)\ntrain_pred_Glove_int = train_pred_Glove.round().astype('int')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:22:21.649359Z","iopub.execute_input":"2021-07-30T11:22:21.649731Z","iopub.status.idle":"2021-07-30T11:22:25.090749Z","shell.execute_reply.started":"2021-07-30T11:22:21.649694Z","shell.execute_reply":"2021-07-30T11:22:25.089777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot Confusion matrix","metadata":{}},{"cell_type":"code","source":"def plot_cm(y_true, y_pred, title, figsize=(5, 5)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n                \n            elif c == 0:\n                annot[i, j] = ''\n                \n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n                \n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap='YlGnBu', annot=annot, fmt='', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:22:25.093002Z","iopub.execute_input":"2021-07-30T11:22:25.09339Z","iopub.status.idle":"2021-07-30T11:22:25.105159Z","shell.execute_reply.started":"2021-07-30T11:22:25.093358Z","shell.execute_reply":"2021-07-30T11:22:25.103948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_cm(train_pred_Glove_int, train['target'].values, 'Confusion matrix for Glove model', figsize=(7, 7))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:22:25.107196Z","iopub.execute_input":"2021-07-30T11:22:25.107645Z","iopub.status.idle":"2021-07-30T11:22:25.381124Z","shell.execute_reply.started":"2021-07-30T11:22:25.107602Z","shell.execute_reply":"2021-07-30T11:22:25.380247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's my first notebook on kaggle \nI hope you find this notebook useful and enjoyble\n\nYour comment and feedback are most welcome.","metadata":{}}]}