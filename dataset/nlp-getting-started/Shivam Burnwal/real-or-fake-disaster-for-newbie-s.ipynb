{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom xgboost import XGBClassifier\nfrom wordcloud import WordCloud\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier, LinearRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout, Embedding, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# loading data.\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling nan values in the columns. \ntrain_df.keyword.fillna('', inplace=True)\ntrain_df.location.fillna('', inplace=True)\n\ntest_df.keyword.fillna('', inplace=True)\ntest_df.location.fillna('', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text'] = train_df['text'] + ' ' + train_df['keyword'] + ' ' + train_df['location']\ntest_df['text'] = test_df['text'] + ' ' + test_df['keyword'] + ' ' + test_df['location']\n\ndel train_df['keyword']\ndel train_df['location']\ndel train_df['id']\ndel test_df['keyword']\ndel test_df['location']\ndel test_df['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train_df.target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see the target column is balanced.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Text Cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we already know there are lots of stopwords like 'a', 'our' which are no use to us while feature selection for our data.\n# So we should remove them from our text\n# creating list of stopwords.\nstop = set(stopwords.words('english'))\npunctuations = list(string.punctuation)\nstop.update(punctuations)\nprint(stop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions to clean up the text like removing numbers and urls.\ndef remove_numbers(text):\n    text = ''.join([i for i in text if not i.isdigit()])         \n    return text\n\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.text = train_df.text.apply(remove_numbers)\ntrain_df.text = train_df.text.apply(remove_URL)\ntrain_df.text = train_df.text.apply(remove_html)\ntrain_df.text = train_df.text.apply(remove_emoji)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.text = test_df.text.apply(remove_numbers)\ntest_df.text = test_df.text.apply(remove_URL)\ntest_df.text = test_df.text.apply(remove_html)\ntest_df.text = test_df.text.apply(remove_emoji)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function return the part of speech of a word.\ndef get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.**\n\n**Text preprocessing includes both Stemming as well as Lemmatization.Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.**\n\n**You guyz can read about lemmatization https://www.geeksforgeeks.org/python-lemmatization-with-nltk/ here.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ndef clean_text(text):\n    clean_text = []\n    for w in word_tokenize(text):\n        if w.lower() not in stop:\n            pos = pos_tag([w])\n            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n            clean_text.append(new_w)\n    return \" \".join(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.text = train_df.text.apply(clean_text)\ntest_df.text = test_df.text.apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualisation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"real = train_df.text[train_df.target[train_df.target==1].index]\nfake = train_df.text[train_df.target[train_df.target==0].index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,24)) # Text Reviews with real disaster\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(real))\nplt.imshow(wordcloud,interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (18,24)) # Text Reviews with fake disaster\nwordcloud = WordCloud(min_font_size = 3,  max_words = 2500 , width = 1200 , height = 800).generate(\" \".join(fake))\nplt.imshow(wordcloud,interpolation = 'bilinear')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see in wordcloud some words like 'amp' is very frequent in our both data so it makes sense to ignore this word using attribute max_df (explained below)**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Splitting Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting our training data into train and validation just to check our model.\nx_train_text, x_val_text, y_train, y_val = train_test_split(train_df.text, train_df.target, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Min_df : It ignores terms that have a document frequency (presence in % of documents) strictly lower than the given threshold.\n#For example, Min_df=0.66 requires that a term appear in 66% of the docuemnts for it to be considered part of the vocabulary.\n\n#Max_df : When building the vocabulary, it ignores terms that have a document frequency strictly higher than the given threshold.\n#This could be used to exclude terms that are too frequent and are unlikely to help predict the label.\ntv=TfidfVectorizer(min_df=0,max_df=0.8,use_idf=True,ngram_range=(1,3))\n\n#transformed train reviews\ntv_train_reviews=tv.fit_transform(x_train_text)\n\n#transformed validation reviews\ntv_val_reviews=tv.transform(x_val_text)\n\n#transformed test reviews\ntv_test_reviews=tv.transform(test_df.text)\n\nprint('tfidf_train:',tv_train_reviews.shape)\nprint('tfidf_validation:',tv_val_reviews.shape)\nprint('tfidf_test:',tv_test_reviews.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**1. Multinomial NaiveBayes Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining classifier\nnb = MultinomialNB()\n\n# fitting for tfidf vectorizer.\ntfidf = nb.fit(tv_train_reviews, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting for validation data\ntfidf_val_predict = tfidf.predict(tv_val_reviews)\nprint('Tfidf Vectorizer score :',accuracy_score(y_val, tfidf_val_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_val, tfidf_val_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_val, tfidf_val_predict)\ncm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. SVC Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC()\n\n# fitting for tfidf vectorizer.\ntfidf = svc.fit(tv_train_reviews, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting for validation data\ntfidf_val_predict = tfidf.predict(tv_val_reviews)\nprint('Tfidf Vectorizer score :',accuracy_score(y_val, tfidf_val_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_val, tfidf_val_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_val, tfidf_val_predict)\ncm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. XgBoostClassifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier()\n\n# fitting for tfidf vectorizer.\ntfidf = xgb.fit(tv_train_reviews, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting for validation data\ntfidf_val_predict = tfidf.predict(tv_val_reviews)\nprint('Tfidf Vectorizer score :',accuracy_score(y_val, tfidf_val_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_val, tfidf_val_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_val, tfidf_val_predict)\ncm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Random Forest Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier()\n\n# fitting for tfidf vectorizer.\ntfidf = rfc.fit(tv_train_reviews, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_val_predict = tfidf.predict(tv_val_reviews)\nprint('Tfidf Vectorizer score :',accuracy_score(y_val, tfidf_val_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_val, tfidf_val_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_val, tfidf_val_predict)\ncm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5. Creating Our Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(units = 512 , activation = 'relu' , input_dim = tv_train_reviews.shape[1]))\nmodel.add(Dense(units = 256 , activation = 'relu'))\nmodel.add(Dense(units = 100 , activation = 'relu'))\nmodel.add(Dense(units = 10 , activation = 'relu'))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(tv_train_reviews, y_train, validation_data=(tv_val_reviews, y_val), batch_size=128, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting accuracy and loss curves for train and validation data.\nplt.figure(figsize=(10,12))\nplt.subplot(221)\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\n\nplt.subplot(222)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_val_predict = model.predict_classes(tv_val_reviews)\ncm = confusion_matrix(y_val, model_val_predict)\ncm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting For Test Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_classes(tv_test_reviews)\nsubmission.target = y_pred\nsubmission.to_csv(\"submission.csv\" , index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rfc.predict(tv_test_reviews)\nsubmission.target = y_pred\nsubmission.to_csv(\"submission.csv\" , index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = xgb.predict(tv_test_reviews)\nsubmission.target = y_pred\nsubmission.to_csv(\"submission.csv\" , index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = svc.predict(tv_test_reviews)\nsubmission.target = y_pred\nsubmission.to_csv(\"submission.csv\" , index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = nb.predict(tv_test_reviews)\nsubmission.target = y_pred\nsubmission.to_csv(\"submission.csv\" , index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# If you face any kind of difficulty in code do comment down. Any kind of suggestions is much appreciated.\n# Don't forget to upvote. It's free :-)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}