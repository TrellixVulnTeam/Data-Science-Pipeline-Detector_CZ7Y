{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom transformers import TFAutoModel, AutoTokenizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from nltk.tokenize import TweetTokenizer\nfrom emoji import demojize\nimport re\n\ntweet_tokenizer = TweetTokenizer()\n\ndef normalizeToken(token):\n    lowercased_token = token.lower()\n    if token.startswith(\"@\"):\n        return \"@USER\"\n    elif lowercased_token.startswith(\"http\") or lowercased_token.startswith(\"www\"):\n        return \"HTTPURL\"\n    elif len(token) == 1:\n        return demojize(token)\n    else:\n        if token == \"’\":\n            return \"'\"\n        elif token == \"…\":\n            return \"...\"\n        else:\n            return token\n\ndef normalizeTweet(tweet):\n    tokens = tweet_tokenizer.tokenize(tweet.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n    normTweet = \" \".join([normalizeToken(token) for token in tokens])\n\n    normTweet = normTweet.replace(\"cannot \", \"can not \").replace(\"n't \", \" n't \").replace(\"n 't \", \" n't \").replace(\"ca n't\", \"can't\").replace(\"ai n't\", \"ain't\")\n    normTweet = normTweet.replace(\"'m \", \" 'm \").replace(\"'re \", \" 're \").replace(\"'s \", \" 's \").replace(\"'ll \", \" 'll \").replace(\"'d \", \" 'd \").replace(\"'ve \", \" 've \")\n    normTweet = normTweet.replace(\" p . m .\", \"  p.m.\") .replace(\" p . m \", \" p.m \").replace(\" a . m .\", \" a.m.\").replace(\" a . m \", \" a.m \")\n\n    normTweet = re.sub(r\",([0-9]{2,4}) , ([0-9]{2,4})\", r\",\\1,\\2\", normTweet)\n    normTweet = re.sub(r\"([0-9]{1,3}) / ([0-9]{2,4})\", r\"\\1/\\2\", normTweet)\n    normTweet = re.sub(r\"([0-9]{1,3})- ([0-9]{2,4})\", r\"\\1-\\2\", normTweet)\n    \n    return \" \".join(normTweet.split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data normalizion is done only in the purpose of EDA\ndef load_train_set():\n    df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")[[\"text\", \"target\"]]\n    df[\"text\"] = df[\"text\"].apply(normalizeTweet)\n    return df\n\ndef load_test_set():\n    df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")[[\"id\", \"text\"]]\n    df[\"text\"] = df[\"text\"].apply(normalizeTweet)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = load_train_set()\ntest = load_test_set()\n\ntrain.info()\nprint()\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['target'].value_counts())\nprint()\nprint(train['target'].value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences_length=[len(tweet.split()) for tweet in train[\"text\"]]\nplt.figure(figsize=(18,5))\nplt.title('Tweet length histogram')\nplt.hist(sequences_length,bins=range(np.unique(sequences_length)[-1]+1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_tweets = train[train['target']==1]['text']\nnon_disaster_tweets = train[train['target']==0]['text']\n\nfreq_dist_disaster_tweets= nltk.FreqDist([word for tweet in disaster_tweets for word in tweet.lower().split() if word not in stopwords.words(\"english\") and len(word) > 2])\nfreq_dist_non_disaster_tweets= nltk.FreqDist([word for tweet in non_disaster_tweets for word in tweet.lower().split() if word not in stopwords.words(\"english\") and len(word) > 2])\n\n    \nresults = {\n    'word' : [],\n    'disaster_count' : [],\n    'non_disaster_count' : []\n}\nfor word, value in freq_dist_disaster_tweets.most_common(200):\n    results['word'].append(word)\n    results['disaster_count'].append(freq_dist_disaster_tweets[word])\n    results['non_disaster_count'].append(freq_dist_non_disaster_tweets[word])\ndf = pd.DataFrame(results)\ndf.head(30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_dist_disaster_tweets.most_common(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nplt.title(f'Frequency Distribution (Disaster, Top 50 words)')\nfreq_dist_disaster_tweets.plot(50, marker='|', markersize=20)\n\nplt.figure(figsize=(25,5))\nplt.title(f'Frequency Distribution (Non Disaster, Top 50 words)')\nfreq_dist_non_disaster_tweets.plot(50, marker='|', markersize=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\ntweets_length = [len(tokenizer.encode(tweet, max_length=512, truncation=True)) for tweet in train[\"text\"]]\nprint(\"Average length: {:.1f}\".format(np.mean(tweets_length)))\nprint(\"Max length: {}\".format(max(tweets_length)))\n\nplt.figure(figsize=(10,5))\nax = sns.distplot(tweets_length, bins=150, kde=False, hist_kws=dict(alpha=0.8))\nax.set(xlabel='Number of tokens')\n\n# Finalize the plot\nsns.despine(bottom=True)\nplt.tight_layout(h_pad=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LENGTH = 50 # in terms of generated tokens (not words)\nshort_tweets = sum(np.array(tweets_length) <= MAX_LENGTH)\nlong_tweets = sum(np.array(tweets_length) > MAX_LENGTH)\n\nprint(\"{} reviews with LEN > {} ({:.2f} % of total data)\".format(\n    long_tweets,\n    MAX_LENGTH,\n    100 * long_tweets / len(train)\n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_tweets(tokenizer, tweets, max_len):\n    nb_tweets = len(tweets)\n    tokens = np.ones((nb_tweets,max_len),dtype='int32')\n    masks = np.zeros((nb_tweets,max_len),dtype='int32')\n    segs = np.zeros((nb_tweets,max_len),dtype='int32')\n\n    for k in range(nb_tweets):        \n        # INPUT_IDS\n        tweet = tweets[k]\n        enc = tokenizer.encode(tweet)                   \n        if len(enc)<max_len-2:\n            tokens[k,:len(enc)+2] = [0] + enc + [2]\n            masks[k,:len(enc)+2] = 1\n        else:\n            tokens[k,:max_len] = [0] + enc[:max_len-2] + [2]\n            masks[k,:max_len] = 1 \n    return tokens,masks,segs\n\n\ndef build_model(max_len):\n    ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n    \n    bertweet = TFAutoModel.from_pretrained(\"vinai/bertweet-base\")\n    x,_ = bertweet(ids,attention_mask=att,token_type_ids=tok)\n\n    out=tf.keras.layers.Dense(1,activation='sigmoid')(x[:,0,:])\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=out)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    return model\n\n    \nmodel = build_model(MAX_LENGTH)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens, train_masks, train_segs = encode_tweets(tokenizer,train[\"text\"].to_list(), MAX_LENGTH)\ntrain_labels = train[\"target\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=3, restore_best_weights=True, verbose=1)\ntrain_labels = train['target']\ntrain_history = model.fit(\n    [train_tokens,train_masks,train_segs], train_labels,\n    validation_split=0.2,\n    epochs=5,\n    batch_size=16,\n    verbose = 1,\n    callbacks = [es]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tokens, test_masks, test_segs = encode_tweets(tokenizer,test[\"text\"].to_list(), MAX_LENGTH)\ntest[\"target\"] = model.predict([test_tokens, test_masks, test_segs]).round().astype(int)\nsubmission = test[[\"id\", \"target\"]]\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}