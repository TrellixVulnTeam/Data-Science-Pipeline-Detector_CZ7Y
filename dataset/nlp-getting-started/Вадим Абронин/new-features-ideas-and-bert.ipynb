{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-13T10:14:16.899168Z","iopub.execute_input":"2022-02-13T10:14:16.899891Z","iopub.status.idle":"2022-02-13T10:14:16.930884Z","shell.execute_reply.started":"2022-02-13T10:14:16.899794Z","shell.execute_reply":"2022-02-13T10:14:16.929966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n%config Completer.use_jedi = False \n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom wordcloud import STOPWORDS\nfrom collections import defaultdict\n#%%time\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\nfrom transformers import AutoTokenizer,TFBertModel\n\nmax_len = 36\n\nimport tensorflow as tf\ntf.config.experimental.list_physical_devices('GPU')\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy,BinaryAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Input, Dense\n\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:16.932601Z","iopub.execute_input":"2022-02-13T10:14:16.933117Z","iopub.status.idle":"2022-02-13T10:14:24.638085Z","shell.execute_reply.started":"2022-02-13T10:14:16.933079Z","shell.execute_reply":"2022-02-13T10:14:24.637249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:24.639367Z","iopub.execute_input":"2022-02-13T10:14:24.640886Z","iopub.status.idle":"2022-02-13T10:14:24.70618Z","shell.execute_reply.started":"2022-02-13T10:14:24.640842Z","shell.execute_reply":"2022-02-13T10:14:24.705439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:24.708322Z","iopub.execute_input":"2022-02-13T10:14:24.708604Z","iopub.status.idle":"2022-02-13T10:14:24.728635Z","shell.execute_reply.started":"2022-02-13T10:14:24.708567Z","shell.execute_reply":"2022-02-13T10:14:24.728017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:24.729864Z","iopub.execute_input":"2022-02-13T10:14:24.730484Z","iopub.status.idle":"2022-02-13T10:14:24.74089Z","shell.execute_reply.started":"2022-02-13T10:14:24.730447Z","shell.execute_reply":"2022-02-13T10:14:24.74023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text preprocessing","metadata":{}},{"cell_type":"code","source":"train_df.text=[a.lower() for a in train_df.text]\ntest_df.text=[a.lower() for a in test_df.text]\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:24.742347Z","iopub.execute_input":"2022-02-13T10:14:24.742844Z","iopub.status.idle":"2022-02-13T10:14:24.764045Z","shell.execute_reply.started":"2022-02-13T10:14:24.742804Z","shell.execute_reply":"2022-02-13T10:14:24.76339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['word_count'] = train_df['text'].apply(lambda x: len(str(x).split()))\ntest_df['word_count'] = test_df['text'].apply(lambda x: len(str(x).split()))\ntrain_df['unique_word_count'] = train_df['text'].apply(lambda x: len(set(str(x).split())))\ntest_df['unique_word_count'] = test_df['text'].apply(lambda x: len(set(str(x).split())))\ntrain_df['url_count'] = train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ntest_df['url_count'] = test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ntrain_df['mean_word_length'] = train_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df['mean_word_length'] = test_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntrain_df['char_count'] = train_df['text'].apply(lambda x: len(str(x)))\ntest_df['char_count'] = test_df['text'].apply(lambda x: len(str(x)))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:24.76518Z","iopub.execute_input":"2022-02-13T10:14:24.76544Z","iopub.status.idle":"2022-02-13T10:14:25.172629Z","shell.execute_reply.started":"2022-02-13T10:14:24.765404Z","shell.execute_reply":"2022-02-13T10:14:25.171825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\ntrain_df['text'] = train_df['text'].apply(lambda x : remove_URL(x))\ntest_df['text'] = test_df['text'].apply(lambda x : remove_URL(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:25.17688Z","iopub.execute_input":"2022-02-13T10:14:25.178901Z","iopub.status.idle":"2022-02-13T10:14:25.250131Z","shell.execute_reply.started":"2022-02-13T10:14:25.178862Z","shell.execute_reply":"2022-02-13T10:14:25.249403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ntrain_df['text'] = train_df['text'].apply(lambda x : remove_html(x))\ntest_df['text'] = test_df['text'].apply(lambda x : remove_html(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:25.254206Z","iopub.execute_input":"2022-02-13T10:14:25.256202Z","iopub.status.idle":"2022-02-13T10:14:25.289906Z","shell.execute_reply.started":"2022-02-13T10:14:25.256162Z","shell.execute_reply":"2022-02-13T10:14:25.289118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ntrain_df['text'] = train_df['text'].apply(lambda x: remove_emoji(x))\ntest_df['text'] = test_df['text'].apply(lambda x: remove_emoji(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:25.29638Z","iopub.execute_input":"2022-02-13T10:14:25.302411Z","iopub.status.idle":"2022-02-13T10:14:25.423549Z","shell.execute_reply.started":"2022-02-13T10:14:25.302369Z","shell.execute_reply":"2022-02-13T10:14:25.422788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cleaner(tweet):\n    tweet = re.sub(r\"Typhoon-Devastated\", \"typhoon devastated\", tweet)\n    tweet = re.sub(r\"TyphoonDevastated\", \"typhoon devastated\", tweet)\n    tweet = re.sub(r\"typhoondevastated\", \"typhoon devastated\", tweet)\n    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight\", tweet)\n    tweet = re.sub(r\"MH\", \"Malaysia Airlines Flight\", tweet)\n    tweet = re.sub(r\"mh370\", \"Malaysia Airlines Flight\", tweet)\n    tweet = re.sub(r\"year-old\", \"years old\", tweet)\n    tweet = re.sub(r\"yearold\", \"years old\", tweet)\n    tweet = re.sub(r\"yr old\", \"years old\", tweet)\n    tweet = re.sub(r\"PKK\", \"Kurdistan Workers Party\", tweet)\n    tweet = re.sub(r\"MP\", \"madhya pradesh\", tweet)\n    tweet = re.sub(r\"rly\", \"railway\", tweet)\n    tweet = re.sub(r\"CDT\", \"Central Daylight Time\", tweet)\n    tweet = re.sub(r\"sensorsenso\", \"sensor senso\", tweet)\n    tweet = re.sub(r\"pm\", \"\", tweet)\n    tweet = re.sub(r\"PM\", \"\", tweet)\n    tweet = re.sub(r\"nan\", \" \", tweet)\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n    tweet = re.sub(r\"prebreak\", \"pre break\", tweet)\n    tweet = re.sub(r\"nowplaying\", \"now playing\", tweet)\n    tweet = re.sub(r\"RT\", \"retweet\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n    tweet = re.sub(r\"%20\", \" \", tweet)\n    tweet = re.sub(r\"%\", \" \", tweet)\n    tweet = re.sub(r\"@\", \" \", tweet)\n    tweet = re.sub(r\"'\", \" \", tweet)\n    tweet = re.sub(r\"\\x89û_\", \" \", tweet)\n    tweet = re.sub(r\"\\x89ûò\", \" \", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"re\\x89û_\", \" \", tweet)\n    tweet = re.sub(r\"\\x89û\", \" \", tweet)\n    tweet = re.sub(r\"\\x89Û\", \" \", tweet)\n    tweet = re.sub(r\"re\\x89Û\", \"re \", tweet)\n    tweet = re.sub(r\"re\\x89û\", \"re \", tweet)\n    tweet = re.sub(r\"\\x89ûª\", \"'\", tweet)\n    tweet = re.sub(r\"\\x89û\", \" \", tweet)\n    tweet = re.sub(r\"\\x89ûò\", \" \", tweet)\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"Im\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"can not\", tweet)\n    tweet = re.sub(r\"cant\", \"can not\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"dont\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)\n    return tweet\ntrain_df['text'] = train_df['text'].apply(lambda s : cleaner(s))\ntest_df['text'] = test_df['text'].apply(lambda s : cleaner(s))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:25.428333Z","iopub.execute_input":"2022-02-13T10:14:25.428599Z","iopub.status.idle":"2022-02-13T10:14:27.710027Z","shell.execute_reply.started":"2022-02-13T10:14:25.428565Z","shell.execute_reply":"2022-02-13T10:14:27.709311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['text'] = train_df['text'].str.replace('   ', ' ')\ntrain_df['text'] = train_df['text'].str.replace('     ', ' ')\ntrain_df['text'] = train_df['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\ntrain_df['text'] = train_df['text'].str.replace('  ', ' ')\ntrain_df['text'] = train_df['text'].str.replace('—', ' ')\ntrain_df['text'] = train_df['text'].str.replace('–', ' ')\ntest_df['text'] = test_df['text'].str.replace('   ', ' ')\ntest_df['text'] = test_df['text'].str.replace('     ', ' ')\ntest_df['text'] = test_df['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\ntest_df['text'] = test_df['text'].str.replace('  ', ' ')\ntest_df['text'] = test_df['text'].str.replace('—', ' ')\ntest_df['text'] = test_df['text'].str.replace('–', ' ')","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:27.711301Z","iopub.execute_input":"2022-02-13T10:14:27.712676Z","iopub.status.idle":"2022-02-13T10:14:27.781089Z","shell.execute_reply.started":"2022-02-13T10:14:27.712632Z","shell.execute_reply":"2022-02-13T10:14:27.780471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hashtags can say a lot about situation\n# I calculate all hashtags and mean target by every hashtag","metadata":{}},{"cell_type":"code","source":"train_df['hashtags']=train_df['text']\nfor i in range(train_df.shape[0]):\n    j=0\n    a=[]\n    s=train_df.text[i]\n    while(j<len(s)):\n        if(s[j]=='#'):\n            j+=1\n            j1=j\n            while(j<len(s) and s[j]!=' '):\n                j+=1\n            a.append(s[j1:j])\n        else:\n            j+=1\n    train_df.hashtags[i]=a","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:27.783402Z","iopub.execute_input":"2022-02-13T10:14:27.783644Z","iopub.status.idle":"2022-02-13T10:14:30.737946Z","shell.execute_reply.started":"2022-02-13T10:14:27.783614Z","shell.execute_reply":"2022-02-13T10:14:30.737189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:30.739237Z","iopub.execute_input":"2022-02-13T10:14:30.739472Z","iopub.status.idle":"2022-02-13T10:14:30.755672Z","shell.execute_reply.started":"2022-02-13T10:14:30.739441Z","shell.execute_reply":"2022-02-13T10:14:30.754727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['hashtags']=test_df['text']\nfor i in range(test_df.shape[0]):\n    j=0\n    a=[]\n    s=test_df.text[i]\n    while(j<len(s)):\n        if(s[j]=='#'):\n            j+=1\n            j1=j\n            while(j<len(s) and s[j]!=' ' and s[j]!='\\n' and s[j]!='#'):\n                j+=1\n            a.append(s[j1:j])\n        else:\n            j+=1\n    test_df.hashtags[i]=a\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:30.75725Z","iopub.execute_input":"2022-02-13T10:14:30.757743Z","iopub.status.idle":"2022-02-13T10:14:32.172444Z","shell.execute_reply.started":"2022-02-13T10:14:30.757704Z","shell.execute_reply":"2022-02-13T10:14:32.171459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st1=set()\nfor i in train_df.hashtags:\n    for j in i:\n        st1.add(j)\nst2=set()\nfor i in test_df.hashtags:\n    for j in i:\n        st2.add(j)\nprint(st2-st1)                ","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:32.17419Z","iopub.execute_input":"2022-02-13T10:14:32.17448Z","iopub.status.idle":"2022-02-13T10:14:32.188346Z","shell.execute_reply.started":"2022-02-13T10:14:32.174441Z","shell.execute_reply":"2022-02-13T10:14:32.187629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:32.189809Z","iopub.execute_input":"2022-02-13T10:14:32.190115Z","iopub.status.idle":"2022-02-13T10:14:32.206636Z","shell.execute_reply.started":"2022-02-13T10:14:32.190081Z","shell.execute_reply":"2022-02-13T10:14:32.205916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m={}\nfor i in range(train_df.shape[0]):\n    for j in train_df.hashtags[i]:\n        if(j not in m):\n            m[j]=train_df['target'][i]\n        else:\n            m[j]+=train_df['target'][i]","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:32.207787Z","iopub.execute_input":"2022-02-13T10:14:32.208256Z","iopub.status.idle":"2022-02-13T10:14:32.319312Z","shell.execute_reply.started":"2022-02-13T10:14:32.208221Z","shell.execute_reply":"2022-02-13T10:14:32.318687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=[]\nfor i in m.keys():\n    x.append((m[i],i))\nx=sorted(x)\nx=list(reversed(x))\nprint(x[:40])","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:32.320481Z","iopub.execute_input":"2022-02-13T10:14:32.320717Z","iopub.status.idle":"2022-02-13T10:14:32.330169Z","shell.execute_reply.started":"2022-02-13T10:14:32.320685Z","shell.execute_reply":"2022-02-13T10:14:32.329434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['mean_hashtags_target']=train_df['target']\nfor i in range(train_df.shape[0]):\n    s=0\n    for j in train_df.hashtags[i]:\n        if(j not in m):\n            s+=0\n        else:\n            s+=m[j]\n    if(len(train_df.hashtags[i])==0):\n        s=0\n    else:\n        s/=len(train_df.hashtags[i])\n    train_df['mean_hashtags_target'][i]=s","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:32.331409Z","iopub.execute_input":"2022-02-13T10:14:32.331765Z","iopub.status.idle":"2022-02-13T10:14:34.641299Z","shell.execute_reply.started":"2022-02-13T10:14:32.33173Z","shell.execute_reply":"2022-02-13T10:14:34.640564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['mean_hashtags_target']=test_df['text']\nfor i in range(test_df.shape[0]):\n    s=0\n    for j in test_df.hashtags[i]:\n        if(j not in m):\n            s+=0\n        else:\n            s+=m[j]\n    if(len(test_df.hashtags[i])==0):\n        s=0\n    else:\n        s/=len(test_df.hashtags[i])\n    test_df['mean_hashtags_target'][i]=s","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:34.642574Z","iopub.execute_input":"2022-02-13T10:14:34.642809Z","iopub.status.idle":"2022-02-13T10:14:35.716275Z","shell.execute_reply.started":"2022-02-13T10:14:34.642777Z","shell.execute_reply":"2022-02-13T10:14:35.715558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Null preprocessing","metadata":{}},{"cell_type":"code","source":"train_df['location']=train_df['location'].fillna('NAN')\ntest_df['location']=test_df['location'].fillna('NAN')\ntrain_df['keyword']=train_df['keyword'].fillna('NAN')\ntest_df['keyword']=test_df['keyword'].fillna('NAN')","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:35.717608Z","iopub.execute_input":"2022-02-13T10:14:35.717846Z","iopub.status.idle":"2022-02-13T10:14:35.727906Z","shell.execute_reply.started":"2022-02-13T10:14:35.717815Z","shell.execute_reply":"2022-02-13T10:14:35.727001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['mean_target_keyword']=train_df['target']\ntrain_df['mean_target_keyword']=train_df['keyword'].map(train_df.groupby('keyword')['target'].mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:35.729116Z","iopub.execute_input":"2022-02-13T10:14:35.729357Z","iopub.status.idle":"2022-02-13T10:14:35.743317Z","shell.execute_reply.started":"2022-02-13T10:14:35.729323Z","shell.execute_reply":"2022-02-13T10:14:35.742623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mean target by keyword","metadata":{}},{"cell_type":"code","source":"test_df['mean_target_keyword']=test_df['keyword'].map(train_df.groupby('keyword')['target'].mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:35.745432Z","iopub.execute_input":"2022-02-13T10:14:35.745855Z","iopub.status.idle":"2022-02-13T10:14:35.753323Z","shell.execute_reply.started":"2022-02-13T10:14:35.745819Z","shell.execute_reply":"2022-02-13T10:14:35.752637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Count !,?,hashtags","metadata":{}},{"cell_type":"code","source":"train_df['hashtags_count']=[a.count('#') for a in train_df.text]\ntest_df['hashtags_count']=[a.count('#') for a in test_df.text]\ntrain_df['!_count']=[a.count('!') for a in train_df.text]\ntest_df['!_count']=[a.count('!') for a in test_df.text]\ntrain_df['?_count']=[a.count('?') for a in train_df.text]\ntest_df['?_count']=[a.count('?') for a in test_df.text]\ntrain_df['?_count'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:35.754809Z","iopub.execute_input":"2022-02-13T10:14:35.755196Z","iopub.status.idle":"2022-02-13T10:14:35.793559Z","shell.execute_reply.started":"2022-02-13T10:14:35.75516Z","shell.execute_reply":"2022-02-13T10:14:35.792555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Install bert model","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\nbert = TFBertModel.from_pretrained('bert-large-uncased')  ","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:14:35.795355Z","iopub.execute_input":"2022-02-13T10:14:35.795661Z","iopub.status.idle":"2022-02-13T10:15:28.633873Z","shell.execute_reply.started":"2022-02-13T10:14:35.795621Z","shell.execute_reply":"2022-02-13T10:15:28.63315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train=tokenizer(\n    text=train_df.text.tolist(),\n    add_special_tokens=True,\n    max_length=36,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:15:28.651653Z","iopub.execute_input":"2022-02-13T10:15:28.651839Z","iopub.status.idle":"2022-02-13T10:15:29.338478Z","shell.execute_reply.started":"2022-02-13T10:15:28.651817Z","shell.execute_reply":"2022-02-13T10:15:29.337708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train['attention_mask'])","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:15:29.343238Z","iopub.execute_input":"2022-02-13T10:15:29.345401Z","iopub.status.idle":"2022-02-13T10:15:29.357035Z","shell.execute_reply.started":"2022-02-13T10:15:29.345359Z","shell.execute_reply":"2022-02-13T10:15:29.356078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\nembeddings = bert(input_ids,attention_mask = input_mask)[1]\nout = tf.keras.layers.Dropout(0.1)(embeddings)\nout = Dense(128, activation='relu')(out)\nout = tf.keras.layers.Dropout(0.1)(out)\nout = Dense(32,activation = 'relu')(out)\ny = Dense(1,activation = 'sigmoid')(out)\nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[2].trainable = True","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:15:29.361585Z","iopub.execute_input":"2022-02-13T10:15:29.363646Z","iopub.status.idle":"2022-02-13T10:15:38.408844Z","shell.execute_reply.started":"2022-02-13T10:15:29.363605Z","shell.execute_reply":"2022-02-13T10:15:38.407127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:15:38.410076Z","iopub.execute_input":"2022-02-13T10:15:38.41048Z","iopub.status.idle":"2022-02-13T10:15:38.477372Z","shell.execute_reply.started":"2022-02-13T10:15:38.410441Z","shell.execute_reply":"2022-02-13T10:15:38.476563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(\n    learning_rate=6e-06, # this learning rate is for bert model.\n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\nloss = BinaryCrossentropy(from_logits = True)\nmetric = BinaryAccuracy('accuracy'),\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:15:38.480445Z","iopub.execute_input":"2022-02-13T10:15:38.480817Z","iopub.status.idle":"2022-02-13T10:15:39.759579Z","shell.execute_reply.started":"2022-02-13T10:15:38.480776Z","shell.execute_reply":"2022-02-13T10:15:39.758753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, show_shapes = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:15:39.762807Z","iopub.execute_input":"2022-02-13T10:15:39.763011Z","iopub.status.idle":"2022-02-13T10:15:40.752042Z","shell.execute_reply.started":"2022-02-13T10:15:39.762987Z","shell.execute_reply":"2022-02-13T10:15:40.750884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train=train_df['target'].values\nfinal = model.fit(\n    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n    y = y_train,\n    epochs=9,\n    batch_size=8  \n)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:15:40.753644Z","iopub.execute_input":"2022-02-13T10:15:40.754692Z","iopub.status.idle":"2022-02-13T10:33:41.959334Z","shell.execute_reply.started":"2022-02-13T10:15:40.754651Z","shell.execute_reply":"2022-02-13T10:33:41.958556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = tokenizer(\n    text=test_df.text.tolist(),\n    add_special_tokens=True,\n    max_length=36,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\ndata=pd.DataFrame()\npredicted = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})\ny_predicted = np.where(predicted>0.5,1,0)\ny_predicted = y_predicted.reshape((1,3263))[0]\ndata['id'] = test_df.id\ndata['target'] = y_predicted","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:36:21.573011Z","iopub.execute_input":"2022-02-13T10:36:21.573334Z","iopub.status.idle":"2022-02-13T10:36:39.789697Z","shell.execute_reply.started":"2022-02-13T10:36:21.573299Z","shell.execute_reply":"2022-02-13T10:36:39.788877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.to_csv('submission.csv',index = False)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T10:36:43.638608Z","iopub.execute_input":"2022-02-13T10:36:43.638949Z","iopub.status.idle":"2022-02-13T10:36:43.666003Z","shell.execute_reply.started":"2022-02-13T10:36:43.638907Z","shell.execute_reply":"2022-02-13T10:36:43.66533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I get info about BERT model from https://www.kaggle.com/aishwarya2210/prediction-of-tweets-using-bert-model Thank!\n# You also can copy my notebook and fit gyperparams to improve score","metadata":{}},{"cell_type":"markdown","source":"# Unfortunately I cant use my feature and I only feat my model on text\n# Maybe you try make DataFrame with BERT prediction+ text features and fit xgboost or lighbgm on this DataFrame","metadata":{}}]}