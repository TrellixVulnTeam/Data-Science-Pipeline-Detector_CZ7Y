{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom xgboost import XGBClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\n##### Columns\n- id : a unique identifier for each tweet\n- text : the text of the tweet\n- location : the location the tweet was sent from (may be blank)\n- keyword : a particular keyword from the tweet (may be blank)\n- target : in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training data shape (rows, cols): ', df_train.shape)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keyword and location columns have some nulls\ndf_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test data shape (rows, cols): ', df_test.shape)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keyword and location columns have some nulls\ndf_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Null check\ndf_train['keyword'].isnull().value_counts() / df_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['location'].isnull().value_counts() / df_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text'].isnull().value_counts() / df_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['keyword'].isnull().value_counts() / df_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['location'].isnull().value_counts() / df_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['text'].isnull().value_counts() / df_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Target Distribution (0 or 1)\ndist_class = df_train['target'].value_counts()\nlabels = ['Non-disaster tweet', 'Disaster tweet']\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n\nsns.barplot(x=dist_class.index, y=dist_class, ax=ax1).set_title(\"Target Count\")\n\nax2.pie(dist_class,\n        labels=labels,\n        counterclock=False,\n        startangle=90,\n        autopct='%1.1f%%',\n        pctdistance=0.7)\nplt.title(\"Target Frequency Proportion\")\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_tweet_length = df_train[df_train['target']==1]['text'].str.len()\nnondisaster_tweet_length = df_train[df_train['target']==0]['text'].str.len()\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n\nax1.hist(disaster_tweet_length, color='red')\nax1.set_title(\"Disaster Tweets\")\n\nax2.hist(nondisaster_tweet_length, color='green')\nax2.set_title(\"Non-Disaster Tweets\")\n\nfig.suptitle(\"Characters in tweets\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_tweet_words = df_train[df_train['target']==1]['text'].str.split().map(lambda x: len(x))\nnondisaster_tweet_words = df_train[df_train['target']==0]['text'].str.split().map(lambda x: len(x))\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n\nax1.hist(disaster_tweet_words, color='red')\nax1.set_title(\"Disaster Tweets\")\n\nax2.hist(nondisaster_tweet_words, color='green')\nax2.set_title(\"Non-Disaster Tweets\")\n\nfig.suptitle(\"Words in tweets\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_keyword = pd.DataFrame({\n    'keyword': df_train['keyword'].value_counts().index,\n    'count': df_train['keyword'].value_counts().values\n})\n\ndf_train_location = pd.DataFrame({\n    'location': df_train['location'].value_counts().index,\n    'count': df_train['location'].value_counts().values\n})\n\nprint('Number fo unique keywords in training data: ', df_train_keyword.shape[0])\n\npx.bar(\n    df_train_keyword,\n    x='keyword',\n    y='count',\n    title=\"Each unique keyword count in training data\"\n).show()\n\npx.bar(\n    df_train_location,\n    x=df_train_location['location'][:20],\n    y=df_train_location['count'][:20],\n    title=\"Top 20 location countin training data\"\n).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['target'] == 1]['keyword'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['target'] == 0]['keyword'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['target'] == 1]['location'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['target'] == 0]['location'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_keyword = pd.DataFrame({\n    'keyword': df_test['keyword'].value_counts().index,\n    'count': df_test['keyword'].value_counts().values\n})\n\ndf_test_location = pd.DataFrame({\n    'location': df_test['location'].value_counts().index,\n    'count': df_test['location'].value_counts().values\n})\n\nprint('Number fo unique keywords in test data: ', df_test_keyword.shape[0])\n\npx.bar(\n    df_test_keyword,\n    x='keyword',\n    y='count',\n    title=\"Each unique keyword count in test data\"\n).show()\n\npx.bar(\n    df_test_location,\n    x=df_test_location['location'][:20],\n    y=df_test_location['count'][:20],\n    title=\"Top 20 location count in test data\"\n).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_tweet = dict(df_train[df_train['target']==1]['keyword'].value_counts())\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords=stopwords, width=800, height=400, background_color=\"white\").generate_from_frequencies(disaster_tweet)\n\nplt.figure(figsize=[10,6])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nondisaster_tweet = dict(df_train[df_train['target']==0]['keyword'].value_counts())\n\nwordcloud = WordCloud(stopwords=stopwords, width=800, height=400, background_color=\"white\").generate_from_frequencies(nondisaster_tweet)\n\nplt.figure(figsize=[10,6])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tweet = dict(df_test['keyword'].value_counts())\n\nwordcloud = WordCloud(stopwords=stopwords, width=800, height=400, background_color=\"white\").generate_from_frequencies(test_tweet)\n\nplt.figure(figsize=[10,6])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering (Data Cleaning? Data Pre-Processing?)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove\ndef remove_url(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nimport string\ndef remove_punc(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\nslang_abbrev_dict = {\n    'AFAIK': 'As Far As I Know',\n    'AFK': 'Away From Keyboard',\n    'ASAP': 'As Soon As Possible',\n    'ATK': 'At The Keyboard',\n    'ATM': 'At The Moment',\n    'A3': 'Anytime, Anywhere, Anyplace',\n    'BAK': 'Back At Keyboard',\n    'BBL': 'Be Back Later',\n    'BBS': 'Be Back Soon',\n    'BFN': 'Bye For Now',\n    'B4N': 'Bye For Now',\n    'BRB': 'Be Right Back',\n    'BRT': 'Be Right There',\n    'BTW': 'By The Way',\n    'B4': 'Before',\n    'B4N': 'Bye For Now',\n    'CU': 'See You',\n    'CUL8R': 'See You Later',\n    'CYA': 'See You',\n    'FAQ': 'Frequently Asked Questions',\n    'FC': 'Fingers Crossed',\n    'FWIW': 'For What It\\'s Worth',\n    'FYI': 'For Your Information',\n    'GAL': 'Get A Life',\n    'GG': 'Good Game',\n    'GN': 'Good Night',\n    'GMTA': 'Great Minds Think Alike',\n    'GR8': 'Great!',\n    'G9': 'Genius',\n    'IC': 'I See',\n    'ICQ': 'I Seek you',\n    'ILU': 'I Love You',\n    'IMHO': 'In My Humble Opinion',\n    'IMO': 'In My Opinion',\n    'IOW': 'In Other Words',\n    'IRL': 'In Real Life',\n    'KISS': 'Keep It Simple, Stupid',\n    'LDR': 'Long Distance Relationship',\n    'LMAO': 'Laugh My Ass Off',\n    'LOL': 'Laughing Out Loud',\n    'LTNS': 'Long Time No See',\n    'L8R': 'Later',\n    'MTE': 'My Thoughts Exactly',\n    'M8': 'Mate',\n    'NRN': 'No Reply Necessary',\n    'OIC': 'Oh I See',\n    'OMG': 'Oh My God',\n    'PITA': 'Pain In The Ass',\n    'PRT': 'Party',\n    'PRW': 'Parents Are Watching',\n    'QPSA?': 'Que Pasa?',\n    'ROFL': 'Rolling On The Floor Laughing',\n    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n    'ROTFLMAO': 'Rolling On The Floor Laughing My Ass Off',\n    'SK8': 'Skate',\n    'STATS': 'Your sex and age',\n    'ASL': 'Age, Sex, Location',\n    'THX': 'Thank You',\n    'TTFN': 'Ta-Ta For Now!',\n    'TTYL': 'Talk To You Later',\n    'U': 'You',\n    'U2': 'You Too',\n    'U4E': 'Yours For Ever',\n    'WB': 'Welcome Back',\n    'WTF': 'What The Fuck',\n    'WTG': 'Way To Go!',\n    'WUF': 'Where Are You From?',\n    'W8': 'Wait',\n    '7K': 'Sick:-D Laugher'\n}\n\ndef unslang(text):\n    if text.upper() in slang_abbrev_dict.keys():\n        return slang_abbrev_dict[text.upper()]\n    else:\n        return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\nstopword = nltk.corpus.stopwords.words('english')\ndef remove_stopwords(text):\n    text = [word for word in text if word not in stopword]\n    return text\n\ndef stemming(text):\n    text = [stemmer.stem(word) for word in text]\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for datas in [df_train, df_test]:\n    datas['cleaned_text'] = datas['text'].apply(lambda x : remove_url(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : remove_html(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : remove_emoji(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : unslang(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : remove_punc(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : tokenization(x.lower()))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : remove_stopwords(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : stemming(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : ' '.join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text'][100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['cleaned_text'][100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['text'][100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['cleaned_text'][100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling and Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer='word', binary=True)\nvectorizer.fit(df_train['cleaned_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train_feature = df_train[['keyword', 'location', 'cleaned_text']]\n# X = vectorizer.transform(df_train_feature).todense()\nX = vectorizer.transform(df_train['cleaned_text']).todense()\ny = df_train['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# params = {\n#     'max_depth': list(range(5, 11)),\n#     'learning_rate': list(np.arange(0.05, 0.30, 0.05)),\n#     'gamma': list(np.arange(0.01, 0.06, 0.01)),\n#     'min_child_weight': list(range(1, 6)),\n    \n#     # fixed params\n#     'n_estimators' : [1500],\n#     'n_jobs': [4],\n#     'objective': ['binary:logistic'],\n#     'eval_metric' : ['logloss'],\n#     'random_state': [42]\n# }\n\n# model = XGBClassifier(tree_method='gpu_hist')\n# cv = GridSearchCV(model, params, cv=5, n_jobs=4, scoring='roc_auc')\n\n# cv.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About how to tune the hyperparameters of XGBClassifier, I used ['Complete Guide to Parameter Tuning in XGBoost with codes in Python'](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) as reference.  \nActually I'd like to use GridSearchCV, however I gave up using it due to kernel memory issues."},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_classifir = XGBClassifier(tree_method='gpu_hist',\n                              learning_rate=0.1,\n                              num_round=1000,\n                              max_depth=10,\n                              min_child_weight=2,\n                              colsample_bytree=0.8,\n                              subsample=0.9,\n                              gamma=0.4,\n                              reg_alpha=1e-5,\n                              reg_lambda=1,\n                              n_estimators=2000,\n                              objective='binary:logistic',\n                              eval_metric=[\"auc\", \"logloss\", \"error\"],\n                              early_stopping_rounds=50)\n\n# https://www.coursera.org/learn/competitive-data-science/lecture/wzi5a/hyperparameter-tuning-ii","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nxgb_classifir.fit(X_train, y_train, eval_set=[(X_train, y_train),(X_valid, y_valid)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_xgb = xgb_classifir.predict(X_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_valid, y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_valid, y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_valid, y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, _ = roc_curve(y_valid, y_pred_xgb)\nauc_score = auc(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, marker='.', label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([-0.1,1.1])\nplt.ylim([-0.1,1.1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [round(value) for value in y_pred_xgb]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retrieve performance metrics\nresults = xgb_classifir.evals_result()\nepochs = len(results['validation_0']['auc'])\nx_axis = range(0, epochs)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18,6))\n\n# plot auc\nax1.plot(x_axis, results['validation_0']['auc'], label='Train')\nax1.plot(x_axis, results['validation_1']['auc'], label='Validation')\nax1.set_title(\"XGBoost AUC\")\nax1.legend()\n\n# plot log loss\nax2.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax2.plot(x_axis, results['validation_1']['logloss'], label='Validation')\nax2.set_title(\"XGBoost Log Loss\")\nax2.legend()\n\n# plot classification error\nax3.plot(x_axis, results['validation_0']['error'], label='Train')\nax3.plot(x_axis, results['validation_1']['error'], label='Validation')\nax3.set_title(\"XGBoost Classification Error\")\nax3.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Trying to use ensamble method (the simplest bagging?)\nReferrence: https://www.coursera.org/learn/competitive-data-science/lecture/MJKCi/introduction-into-ensemble-methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Suppport Vecter Machine\nfrom sklearn.svm import SVC\n\nsvm_classifier = SVC(kernel='rbf')\nsvm_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svm = svm_classifier.predict(X_valid)\nprint(y_pred_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K-Nearest neighbour\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_classifier = KNeighborsClassifier(n_neighbors = 7,weights = 'distance',algorithm = 'brute')\nknn_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_knn = knn_classifier.predict(X_valid)\nprint(y_pred_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_testset = vectorizer.transform(df_test['cleaned_text']).todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_testset.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred_ave = (xgb_classifir.predict(X_testset) + svm_classifier.predict(X_testset) + knn_classifier.predict(X_testset)) / 3\nprint(y_test_pred_ave)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = np.where(y_test_pred_ave >= 0.5, 1, 0)\nprint(y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file = pd.DataFrame({'id': df_test['id'], 'target': y_test_pred})\nsubmission_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file.to_csv('submission_xgb_20200130.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Future work: Use of DNN, also reserach for speeding up lerning on the kernel.\n\n### <font color=\"Red\">**If you like this kernel, please upvote:)**</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}