{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Loading in comments...')\n\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_length'] = train['text'].apply(lambda x : len(x))\ntrain['comment_length'].hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom keras.models import Model, Input\nfrom keras.layers import Dense, LSTM, Dropout, Embedding, SpatialDropout1D, Bidirectional, concatenate\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import accuracy_score\nfrom eli5.lime import TextExplainer\nimport regex as re\nimport numpy as np\nimport eli5\n\n\nclass KerasTextClassifier(BaseEstimator, TransformerMixin):\n    '''Wrapper class for keras text classification models that takes raw text as input.'''\n    \n    def __init__(self, max_words=30000, input_length=100, emb_dim=20, n_classes=2, \n                 epochs=5, batch_size=32):\n        self.max_words = max_words\n        self.input_length = input_length\n        self.emb_dim = emb_dim\n        self.n_classes = n_classes\n        self.epochs = epochs\n        self.bs = batch_size\n        self.model = self._get_model()\n        self.tokenizer = Tokenizer(num_words=self.max_words+1,\n                                   lower=True, split=' ', oov_token=\"UNK\")\n    \n    def _get_model(self):\n        input_text = Input((self.input_length,))\n        text_embedding = Embedding(input_dim=self.max_words + 2, output_dim=self.emb_dim,\n                                   input_length=self.input_length, mask_zero=False)(input_text)\n        text_embedding = SpatialDropout1D(0.5)(text_embedding)\n        bilstm = Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.5))(text_embedding)\n        x = concatenate([GlobalAveragePooling1D()(bilstm), GlobalMaxPooling1D()(bilstm)])\n        x = Dropout(0.7)(x)\n        x = Dense(512, activation=\"relu\")(x)\n        x = Dropout(0.6)(x)\n        x = Dense(512, activation=\"relu\")(x)\n        x = Dropout(0.5)(x)\n        out = Dense(units=self.n_classes, activation=\"softmax\")(x)\n        model = Model(input_text, out)\n        model.compile(optimizer=\"adam\",\n                      loss=\"sparse_categorical_crossentropy\",\n                      metrics=[\"accuracy\"])\n        return model\n    \n    def _get_sequences(self, texts):\n        seqs = self.tokenizer.texts_to_sequences(texts)\n        return pad_sequences(seqs, maxlen=self.input_length, value=0)\n    \n    def _preprocess(self, texts):\n        return [re.sub(r\"\\d\", \"DIGIT\", x) for x in texts]\n    \n    def fit(self, X, y):\n        '''\n        Fit the vocabulary and the model.\n        \n        :params:\n        X: list of texts.\n        y: labels.\n        '''\n        \n        self.tokenizer.fit_on_texts(self._preprocess(X))\n        self.tokenizer.word_index = {e: i for e,i in self.tokenizer.word_index.items() if i <= self.max_words}\n        self.tokenizer.word_index[self.tokenizer.oov_token] = self.max_words + 1\n        seqs = self._get_sequences(self._preprocess(X))\n        self.model.fit(seqs, y, batch_size=self.bs, epochs=self.epochs, validation_split=0.1)\n    \n    def predict_proba(self, X, y=None):\n        seqs = self._get_sequences(self._preprocess(X))\n        return self.model.predict(seqs)\n    \n    def predict(self, X, y=None):\n        return np.argmax(self.predict_proba(X), axis=1)\n    \n    def score(self, X, y):\n        y_pred = self.predict(X)\n        return accuracy_score(y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_model = KerasTextClassifier(epochs=5, max_words=20000, input_length=160)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_model.fit(train.text, train.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_model.score(train.text , train.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Loading in comments...')\n\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\npred = text_model.predict(test.text)\ntest['pred'] = pred \n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Loading in Submission File...')\n\nsubmit_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmit_df['target'] = test['pred']\n\nsubmit_df.to_csv('5E_submit.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from eli5.lime import TextExplainer\nimport eli5\ndoc = train.iloc[25]['text']\ntarget = train.iloc[25]['target']\nte = TextExplainer(random_state=42)\nte.fit(doc, text_model.predict_proba)\nprint(doc,target )\nte.show_prediction(target_names=['real','fake'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}