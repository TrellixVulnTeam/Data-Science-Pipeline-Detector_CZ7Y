{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-21T11:17:00.852265Z","iopub.execute_input":"2022-04-21T11:17:00.852529Z","iopub.status.idle":"2022-04-21T11:17:00.860754Z","shell.execute_reply.started":"2022-04-21T11:17:00.852499Z","shell.execute_reply":"2022-04-21T11:17:00.859972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:42:22.39448Z","iopub.execute_input":"2022-04-21T12:42:22.395297Z","iopub.status.idle":"2022-04-21T12:42:22.401268Z","shell.execute_reply.started":"2022-04-21T12:42:22.395245Z","shell.execute_reply":"2022-04-21T12:42:22.400488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n#train_data.head()\ntest_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntrain_data.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:42:25.363985Z","iopub.execute_input":"2022-04-21T12:42:25.364302Z","iopub.status.idle":"2022-04-21T12:42:25.467964Z","shell.execute_reply.started":"2022-04-21T12:42:25.364251Z","shell.execute_reply":"2022-04-21T12:42:25.467348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Explore Data:","metadata":{}},{"cell_type":"code","source":"train_data[train_data['target']==1].values[1]\nx = train_data.target.value_counts()\nprint(x)\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('Y-Axiz')\nplt.gca().set_xlabel('X-Axiz')","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:42:28.610596Z","iopub.execute_input":"2022-04-21T12:42:28.611377Z","iopub.status.idle":"2022-04-21T12:42:28.784209Z","shell.execute_reply.started":"2022-04-21T12:42:28.611334Z","shell.execute_reply":"2022-04-21T12:42:28.783274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_data[train_data['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=train_data[train_data['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:42:32.049968Z","iopub.execute_input":"2022-04-21T12:42:32.050758Z","iopub.status.idle":"2022-04-21T12:42:32.377736Z","shell.execute_reply.started":"2022-04-21T12:42:32.050715Z","shell.execute_reply":"2022-04-21T12:42:32.375952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Cleaning:","metadata":{}},{"cell_type":"code","source":"#Text Preporcessing\n#Convert LowerCase:\ntrain_data[\"text_clean\"] = train_data[\"text\"].apply(lambda x: x.lower())\ntest_data[\"text_clean\"] = test_data[\"text\"].apply(lambda x: x.lower())\n\n#Special Characters Removal:\ntrain_data['text_clean']= train_data['text_clean'].str.replace('rt ',\"\").str.replace('@','').str.replace('#','').str.replace('[^\\w\\s]','').str.replace('[1-9]','')\ntest_data['text_clean']= test_data['text_clean'].str.replace('rt ',\"\").str.replace('@','').str.replace('#','').str.replace('[^\\w\\s]','').str.replace('[1-9]','')\n# Remove URL\ndef remove_URL(text):\n    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n\ntrain_data[\"text_clean\"] = train_data[\"text_clean\"].apply(lambda x: remove_URL(x))\ntest_data[\"text_clean\"] = test_data[\"text_clean\"].apply(lambda x: remove_URL(x))\n\ndef remove_html(text):\n    html = re.compile(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\")\n    return re.sub(html, \"\", text)\n\ntrain_data[\"text_clean\"] = train_data[\"text_clean\"].apply(lambda x: remove_html(x))\ntest_data[\"text_clean\"] = test_data[\"text_clean\"].apply(lambda x: remove_html(x))\n\ndef remove_emojis(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntrain_data[\"text_clean\"] = train_data[\"text_clean\"].apply(lambda x: remove_emojis(x))\ntest_data[\"text_clean\"] = test_data[\"text_clean\"].apply(lambda x: remove_emojis(x))\n\ndef remove_punct(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\n\ntrain_data[\"text_clean\"] = train_data[\"text_clean\"].apply(lambda x: remove_punct(x))\ntest_data[\"text_clean\"] = test_data[\"text_clean\"].apply(lambda x: remove_punct(x))\n\ndef remove_non_ascii(text):\n    return re.sub(r'[^\\x00-\\x7f]',r'', text)\n\ntrain_data[\"text_clean\"] = train_data[\"text_clean\"].apply(lambda x: remove_non_ascii(x))\ntest_data[\"text_clean\"] = test_data[\"text_clean\"].apply(lambda x: remove_non_ascii(x))\nprint(train_data[\"text_clean\"])\nprint(test_data[\"text_clean\"])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:42:35.508761Z","iopub.execute_input":"2022-04-21T12:42:35.509049Z","iopub.status.idle":"2022-04-21T12:42:36.80632Z","shell.execute_reply.started":"2022-04-21T12:42:35.509014Z","shell.execute_reply":"2022-04-21T12:42:36.805475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\ntrain_data['text'] = train_data['text'].apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\ntest_data['text'] = train_data['text'].apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:42:45.25482Z","iopub.execute_input":"2022-04-21T12:42:45.255365Z","iopub.status.idle":"2022-04-21T12:42:45.536522Z","shell.execute_reply.started":"2022-04-21T12:42:45.255328Z","shell.execute_reply":"2022-04-21T12:42:45.535856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#Filling missing values\ntrain_data.isnull().sum()\ntest_data.isnull().sum()\ntrain_data['location'] = train_data['location'].fillna('None')\ntrain_data['keyword'] = train_data['keyword'].fillna('None')\ntest_data['location'] = test_data['location'].fillna('None')\ntest_data['keyword'] = test_data['keyword'].fillna('None')","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:42:48.664468Z","iopub.execute_input":"2022-04-21T12:42:48.665252Z","iopub.status.idle":"2022-04-21T12:42:48.684261Z","shell.execute_reply.started":"2022-04-21T12:42:48.665213Z","shell.execute_reply":"2022-04-21T12:42:48.683645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()\n\n\n# Dropping Un-necessary features:\n#train_data = train_data.drop('id', axis=1)\ntrain_data.drop(columns=['id'])\ntest_data.drop(columns=['id'])\n#test_data = test_data.drop('id', axis=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:42:51.896965Z","iopub.execute_input":"2022-04-21T12:42:51.897443Z","iopub.status.idle":"2022-04-21T12:42:51.916799Z","shell.execute_reply.started":"2022-04-21T12:42:51.897401Z","shell.execute_reply":"2022-04-21T12:42:51.916233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenization\nimport nltk\nfrom nltk import TweetTokenizer\n\ntokenizer = TweetTokenizer()\n\ntrain_data['tokens'] = [tokenizer.tokenize(item) for item in train_data.text]\ntest_data['tokens'] = [ tokenizer.tokenize(item) for item in test_data.text]\nprint(train_data['tokens'])\nprint(test_data['tokens'])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:42:55.699021Z","iopub.execute_input":"2022-04-21T12:42:55.699504Z","iopub.status.idle":"2022-04-21T12:42:56.608406Z","shell.execute_reply.started":"2022-04-21T12:42:55.699437Z","shell.execute_reply":"2022-04-21T12:42:56.607587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lemmatization\n\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_item(item):\n    new_item = []\n    for x in item:\n        x = lemmatizer.lemmatize(x)\n        new_item.append(x)\n    return \" \".join(new_item)\n\ntrain_data['tokens'] = [lemmatize_item(item) for item in train_data.tokens]\ntest_data['tokens'] = [lemmatize_item(item) for item in test_data.tokens]\nprint(train_data['tokens'])","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:42:59.847794Z","iopub.execute_input":"2022-04-21T12:42:59.848065Z","iopub.status.idle":"2022-04-21T12:43:00.627152Z","shell.execute_reply.started":"2022-04-21T12:42:59.848033Z","shell.execute_reply":"2022-04-21T12:43:00.626304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vectorization\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\n\n\nX = vectorizer.fit_transform(train_data.text).toarray()\ny = train_data['target']","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:43:04.013033Z","iopub.execute_input":"2022-04-21T12:43:04.013353Z","iopub.status.idle":"2022-04-21T12:43:04.710545Z","shell.execute_reply.started":"2022-04-21T12:43:04.013318Z","shell.execute_reply":"2022-04-21T12:43:04.709879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data Splitting\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:43:10.445059Z","iopub.execute_input":"2022-04-21T12:43:10.44583Z","iopub.status.idle":"2022-04-21T12:43:11.498448Z","shell.execute_reply.started":"2022-04-21T12:43:10.445785Z","shell.execute_reply":"2022-04-21T12:43:11.497652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying Model\n# LOGISTIC REGRESSION\n\"\"\"\nfrom sklearn.linear_model import LogisticRegression\n\n# Logistic Regression Model \nlog = LogisticRegression(random_state = 0)\n\n# Fitting data\nlog.fit(X_train, y_train)\n\n# Predicting the data\nlog_pred = log.predict(X_test)\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:31:48.106151Z","iopub.execute_input":"2022-04-21T11:31:48.106447Z","iopub.status.idle":"2022-04-21T11:32:04.580847Z","shell.execute_reply.started":"2022-04-21T11:31:48.106418Z","shell.execute_reply":"2022-04-21T11:32:04.579912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nlog = RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nlog.fit(X_train,y_train)\n\nlog_pred=log.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:43:17.463105Z","iopub.execute_input":"2022-04-21T12:43:17.463534Z","iopub.status.idle":"2022-04-21T12:44:49.805936Z","shell.execute_reply.started":"2022-04-21T12:43:17.463502Z","shell.execute_reply":"2022-04-21T12:44:49.805065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom sklearn.ensemble import AdaBoostClassifier\n\nlog = AdaBoostClassifier()\nlog.fit(X_train,y_train)\n\nlog_pred=log.predict(X_test)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:27:04.421092Z","iopub.execute_input":"2022-04-21T12:27:04.42149Z","iopub.status.idle":"2022-04-21T12:29:28.785151Z","shell.execute_reply.started":"2022-04-21T12:27:04.421441Z","shell.execute_reply":"2022-04-21T12:29:28.784358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL EVALUATION FOR LOGISTIC REGRESSION\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score,KFold\n\n# Accuracy\nacc = accuracy_score(y_test,log_pred)\nprint(\"\\nACCURACY : \",acc)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:45:57.290893Z","iopub.execute_input":"2022-04-21T12:45:57.291225Z","iopub.status.idle":"2022-04-21T12:45:57.299818Z","shell.execute_reply.started":"2022-04-21T12:45:57.291161Z","shell.execute_reply":"2022-04-21T12:45:57.29898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KFold Cross Validation\nkfold = KFold(n_splits=15)\nresults = cross_val_score(log, X, y,cv=kfold)\nprint(\"\\nK-FOLD CROSS VALIDATION : \",results.mean())\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:46:02.820382Z","iopub.execute_input":"2022-04-21T12:46:02.821025Z","iopub.status.idle":"2022-04-21T13:16:10.389832Z","shell.execute_reply.started":"2022-04-21T12:46:02.820986Z","shell.execute_reply":"2022-04-21T13:16:10.38887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification Report\nclf_report = classification_report(y_test,log_pred)\nprint(\"\\nCLASSIFICATION REPORT:\\n\", clf_report)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T13:17:56.519246Z","iopub.execute_input":"2022-04-21T13:17:56.519585Z","iopub.status.idle":"2022-04-21T13:17:56.533932Z","shell.execute_reply.started":"2022-04-21T13:17:56.519545Z","shell.execute_reply":"2022-04-21T13:17:56.532932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix\nprint(\"\\nCONFUSION MATRIX:\")\nsns.heatmap(confusion_matrix(y_test,log_pred),annot=True,fmt='g')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T13:18:02.896292Z","iopub.execute_input":"2022-04-21T13:18:02.896906Z","iopub.status.idle":"2022-04-21T13:18:03.155265Z","shell.execute_reply.started":"2022-04-21T13:18:02.896866Z","shell.execute_reply":"2022-04-21T13:18:03.154483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = vectorizer.transform(test_data.text).toarray()\npred = log.predict(test)\nsubmission_df = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsubmission_df['target'] = pred\nsubmission_df.to_csv('submission.csv', index=False)\nresult = pd.read_csv('submission.csv')\nprint (result)\n#result","metadata":{"execution":{"iopub.status.busy":"2022-04-21T13:21:45.801644Z","iopub.execute_input":"2022-04-21T13:21:45.802686Z","iopub.status.idle":"2022-04-21T13:21:47.177722Z","shell.execute_reply.started":"2022-04-21T13:21:45.802627Z","shell.execute_reply":"2022-04-21T13:21:47.176863Z"},"trusted":true},"execution_count":null,"outputs":[]}]}