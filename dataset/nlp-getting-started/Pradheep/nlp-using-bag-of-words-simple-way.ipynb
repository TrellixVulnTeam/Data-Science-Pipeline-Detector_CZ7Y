{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of content\n* Introduction\n* Importing Necessary Library\n* Loading The Dataset\n* Cleaning The Text Data\n* Convert Text To Machine Readable Form\n* Model Creation\n* Checking Accuracy\n* Output Prediction\n* Submission File","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\n\nImage(\"../input/twitterimg/images.png\", width = \"400px\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n### Twitter has become an important communication channel in times of emergency.\n### The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n### Our task in this project is to predict the twitter tweets is Disaster Tweets or Not, I will explain everything in a simple way.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Importing Necessary Library","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading The Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n\nprint(df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot('target',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"keyword\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There are more missing values in the keyword and location so we can drop it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data=df.drop(['location','keyword'],axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning The Text Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### when ever we have a text data we have to clean the data for remove some unnecessary symboles,and uncesessary stopwords.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaning the reviews\n\ncorpus = []\nfor i in range(0,7613):\n\n  # Cleaning special character from the tweets\n  review = re.sub(pattern='[^a-zA-Z]',repl=' ', string=data['text'][i])#remove everything apart from capital A to Z and small a to z\n  \n\n  # Converting the entire tweets into lower case\n  tweets = review.lower()\n\n  # Tokenizing the tweetsby words\n  tweets_words = tweets.split()\n \n  # Removing the stop words\n  tweets_words = [word for word in tweets_words if not word in set(stopwords.words('english'))]\n  \n  # lemmitizing  the words\n  lemmatizer = WordNetLemmatizer()\n  tweets= [lemmatizer.lemmatize(word) for word in tweets_words]\n\n  # Joining the lemmitized words\n  tweets = ' '.join(tweets)\n  \n  # Creating a corpus\n  corpus.append(tweets)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatization:\n### Let’s start with Why we need lemmatization ?\n### As textual data is non linear and there might be some noise present, so in order to remove the noise(unwanted stuff) we have to perform some tasks on the textual data. This process of removing noise is what we call normalization.\n### Lemmatization is the one of the text normalization techniques. In lemmatization, the words are replaced by the root words or the words with similar context.\n### E.g.- Walking will be replaced by Walk(walk is the root word of walking)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Our cleaned text data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert Text To Machine Readable Form","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Every text will be converted into machnie read able form","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer()\nX = cv.fit_transform(corpus).toarray()\ny = data['target']\nprint(X.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Every text will be converted like this:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/05/Screenshot-from-2020-05-21-12-46-42.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Creation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Splitting data into training part and testing part","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)\n\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy=confusion_matrix(y_test,y_pred )\nprint(\"confusion_matrix:\",accuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy=accuracy_score(y_test,y_pred )\nprint(\"accuracy_score:\",accuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(classification_report(y_test,y_pred ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We have to do samething for the test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test=pd.read_csv(\"../input/nlp-getting-started/test.csv\")\nprint(data_test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_tes=data_test.drop(['keyword','location'],axis=1)\ndata_tes.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning text in testing dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncorpus1 = []\nfor i in range(0,3263):\n\n\n  # Cleaning special character from the tweets\n  review = re.sub(pattern='[^a-zA-Z]',repl=' ', string=data_tes['text'][i])\n  \n\n  # Converting the entire tweets into lower case\n  tweets = review.lower()\n\n  # Tokenizing the tweets by words\n  tweets_words = review.split()\n \n  # Removing the stop words\n  tweets_words = [word for word in tweets_words if not word in set(stopwords.words('english'))]\n  \n  # lemmitizing the words\n  lemmatizer = WordNetLemmatizer()\n  tweets = [lemmatizer.lemmatize(word) for word in tweets_words]\n\n  # Joining the lemmitized words\n  tweets = ' '.join(tweets)\n\n  y_pred=cv.transform([review]).toarray()\n  pre=classifier.predict(y_pred)\n  corpus1.append(pre)\n\nprint(len(corpus1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission File","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\n\nsubmission['id'] = data_tes['id']\nsubmission['target'] = corpus1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's convert our submission dataframe 'Survived' column to ints\nsubmission['target'] = submission['target'].astype(int)\nprint('Converted Survived column to integers.')\n\nprint(submission.head())\n\n\n\n\n# Are our test and submission dataframes the same length?\nif len(submission) == len(data_tes):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")\n\n# Convert submisison dataframe to csv for submission to csv \n# for Kaggle submisison\nsubmission.to_csv('../submission_nlp1.csv', index=False)\nprint('Submission CSV is ready!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# If  this Notebook is useful for you please upvote it !!!!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}