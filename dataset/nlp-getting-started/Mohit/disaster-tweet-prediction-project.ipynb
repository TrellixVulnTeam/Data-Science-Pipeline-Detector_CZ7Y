{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Disaster tweet prediction","metadata":{}},{"cell_type":"markdown","source":"Hey everyone! This project is a beginner project for **natural language processing**!\nIn this notebook I'll go step by step on how to solve this classification problem and try to submit the result in the competition and check for accuracy! and try to improve it too! \nHappy learning!","metadata":{}},{"cell_type":"markdown","source":"Let's start by importing the necessary libraries!","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords # these are basically the words which don't convey much meaning like a the an etc.\nfrom nltk.stem.porter import PorterStemmer # this is used to stem the word like for eg if we have loved --> love!\nfrom sklearn.feature_extraction.text import CountVectorizer #to vectorize the words into a vector of frequent words count!\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.392444Z","iopub.execute_input":"2021-08-24T03:44:17.392936Z","iopub.status.idle":"2021-08-24T03:44:17.400722Z","shell.execute_reply.started":"2021-08-24T03:44:17.392884Z","shell.execute_reply":"2021-08-24T03:44:17.399625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at our stop words!","metadata":{}},{"cell_type":"code","source":"print(stopwords.words('english')) # These words don't really give us much info","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.407019Z","iopub.execute_input":"2021-08-24T03:44:17.407399Z","iopub.status.idle":"2021-08-24T03:44:17.418219Z","shell.execute_reply.started":"2021-08-24T03:44:17.407296Z","shell.execute_reply":"2021-08-24T03:44:17.417301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's load the dataset!","metadata":{}},{"cell_type":"code","source":"tweet_dataset = pd.read_csv('../input/nlp-getting-started/train.csv')\ntweet_dataset_test = pd.read_csv('../input/nlp-getting-started/test.csv')\ntweet_dataset.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.420009Z","iopub.execute_input":"2021-08-24T03:44:17.420769Z","iopub.status.idle":"2021-08-24T03:44:17.466477Z","shell.execute_reply.started":"2021-08-24T03:44:17.420624Z","shell.execute_reply":"2021-08-24T03:44:17.465313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the features in the training set!","metadata":{}},{"cell_type":"code","source":"tweet_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.468692Z","iopub.execute_input":"2021-08-24T03:44:17.469109Z","iopub.status.idle":"2021-08-24T03:44:17.480359Z","shell.execute_reply.started":"2021-08-24T03:44:17.469065Z","shell.execute_reply":"2021-08-24T03:44:17.479709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_dataset.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.481534Z","iopub.execute_input":"2021-08-24T03:44:17.481941Z","iopub.status.idle":"2021-08-24T03:44:17.502555Z","shell.execute_reply.started":"2021-08-24T03:44:17.48191Z","shell.execute_reply":"2021-08-24T03:44:17.501893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize the number of \n* 0 --> Not an actual disaster!\n* 1 --> Actual disaster!","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.countplot(x=tweet_dataset['target'])\nplt.title('Target vs count',fontsize=20)\nplt.xlabel('Target',fontsize=15)\nplt.ylabel('Count',fontsize=15);","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.503556Z","iopub.execute_input":"2021-08-24T03:44:17.50398Z","iopub.status.idle":"2021-08-24T03:44:17.621283Z","shell.execute_reply.started":"2021-08-24T03:44:17.503939Z","shell.execute_reply":"2021-08-24T03:44:17.620303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check for any missing data!","metadata":{}},{"cell_type":"code","source":"tweet_dataset.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.622403Z","iopub.execute_input":"2021-08-24T03:44:17.622653Z","iopub.status.idle":"2021-08-24T03:44:17.631073Z","shell.execute_reply.started":"2021-08-24T03:44:17.622628Z","shell.execute_reply":"2021-08-24T03:44:17.630396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alright! so we're gonna use the keyword, location and the text for analysis and check to see if we get a better accuracy!\nTo fill the **null values** let's fill it with empty string **''**\n","metadata":{}},{"cell_type":"code","source":"tweet_dataset = tweet_dataset.fillna('')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.632047Z","iopub.execute_input":"2021-08-24T03:44:17.63244Z","iopub.status.idle":"2021-08-24T03:44:17.645375Z","shell.execute_reply.started":"2021-08-24T03:44:17.632402Z","shell.execute_reply":"2021-08-24T03:44:17.644668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's add a new column named context with the required text combined!","metadata":{}},{"cell_type":"code","source":"tweet_dataset['context'] =  tweet_dataset['text'] + \" \" + tweet_dataset['location'] + ' ' + tweet_dataset['keyword']","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.646385Z","iopub.execute_input":"2021-08-24T03:44:17.646804Z","iopub.status.idle":"2021-08-24T03:44:17.680181Z","shell.execute_reply.started":"2021-08-24T03:44:17.646748Z","shell.execute_reply":"2021-08-24T03:44:17.679288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.684224Z","iopub.execute_input":"2021-08-24T03:44:17.684497Z","iopub.status.idle":"2021-08-24T03:44:17.696197Z","shell.execute_reply.started":"2021-08-24T03:44:17.684468Z","shell.execute_reply":"2021-08-24T03:44:17.695243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's create a function that would skim the column context we created, skim, clean, and remove the stopwords!","metadata":{}},{"cell_type":"code","source":"stem = PorterStemmer() # basically creating an object for stemming! Stemming is basically getting the root word, for eg: loved --> love! ","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.698692Z","iopub.execute_input":"2021-08-24T03:44:17.699119Z","iopub.status.idle":"2021-08-24T03:44:17.707829Z","shell.execute_reply.started":"2021-08-24T03:44:17.699078Z","shell.execute_reply":"2021-08-24T03:44:17.706896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now let's create a function to preprocess a cell and then apply it to the entire feature!\ndef stemming(content):\n    stemmed_content = re.sub('[^a-zA-Z]', ' ',content) # this basically replaces everything other than lower a-z & upper A-Z with a ' ', for eg apple,bananna --> apple bananna\n    stemmed_content = stemmed_content.lower() # to make all text lower case\n    stemmed_content = stemmed_content.split() # this basically splits the line into words with delimiter as ' '\n    stemmed_content = [stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')] # basically remove all the stopwords and apply stemming to the final data\n    stemmed_content = ' '.join(stemmed_content) # this basically joins back and returns the cleaned sentence\n    return stemmed_content","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.709417Z","iopub.execute_input":"2021-08-24T03:44:17.709844Z","iopub.status.idle":"2021-08-24T03:44:17.722403Z","shell.execute_reply.started":"2021-08-24T03:44:17.709752Z","shell.execute_reply":"2021-08-24T03:44:17.721404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's apply the stemming function on our column context","metadata":{}},{"cell_type":"code","source":"# let's apply the function on our feature content\ntweet_dataset['context'] = tweet_dataset['context'].apply(stemming)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:17.723643Z","iopub.execute_input":"2021-08-24T03:44:17.723984Z","iopub.status.idle":"2021-08-24T03:44:37.191595Z","shell.execute_reply.started":"2021-08-24T03:44:17.723947Z","shell.execute_reply":"2021-08-24T03:44:37.19087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's split our training data into labels and text  so we can train our classifier model!","metadata":{}},{"cell_type":"code","source":"X = tweet_dataset['context'].values\ny = tweet_dataset['target'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:37.192751Z","iopub.execute_input":"2021-08-24T03:44:37.193103Z","iopub.status.idle":"2021-08-24T03:44:37.197479Z","shell.execute_reply.started":"2021-08-24T03:44:37.193073Z","shell.execute_reply":"2021-08-24T03:44:37.196459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have our text! we have to vectorize the text so we can feed it to our classifer model! we have to convert the cleaned text into numbers rather in a vector that the model will understand and learn from. Basically it creates a vector with the count of the words!","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:37.198604Z","iopub.execute_input":"2021-08-24T03:44:37.198855Z","iopub.status.idle":"2021-08-24T03:44:37.34701Z","shell.execute_reply.started":"2021-08-24T03:44:37.19883Z","shell.execute_reply":"2021-08-24T03:44:37.346029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have X and y let's train our model on a classifier model!","metadata":{}},{"cell_type":"code","source":"#classifier = LogisticRegression()\n#classifier = svm.SVC(kernel = 'linear')\n#classifier = KNeighborsClassifier(n_neighbors = 10,weights = 'uniform', metric = 'minkowski' , p=2)\nclassifier = svm.SVC(kernel = 'rbf', C=1, gamma =0.1)\n#classifier = GaussianNB()\n#classifier = DecisionTreeClassifier(criterion='entropy')\n#classifier = RandomForestClassifier(criterion = 'entropy', max_depth = 8, max_features  = 'sqrt', n_estimators= 10)\n#from xgboost import XGBClassifier\n#classifier = XGBClassifier(use_label_encoder=False, eval_metric = 'error')\nclassifier.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:37.348227Z","iopub.execute_input":"2021-08-24T03:44:37.348621Z","iopub.status.idle":"2021-08-24T03:44:46.507278Z","shell.execute_reply.started":"2021-08-24T03:44:37.348579Z","shell.execute_reply":"2021-08-24T03:44:46.506226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's analyse our model and evaluate accuracy! We have got a excellent accuracy using logistic classifier but to not leave it to chance that we got lucky on the train set let's check the accuracy using K-Fold cross validation and tune the hyper parameters if possible!","metadata":{}},{"cell_type":"code","source":"# accuracy score on training data\ny_pred_train = classifier.predict(X)\naccuracy_train = accuracy_score(y,y_pred_train)\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier,X = X,y= y , cv = 10)\n\n\nprint(\"-------------------------------\")\nprint(\"Accuracy score on training data: \", accuracy_train)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f}\".format(accuracies.std()*100))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:44:46.510124Z","iopub.execute_input":"2021-08-24T03:44:46.51039Z","iopub.status.idle":"2021-08-24T03:46:13.893506Z","shell.execute_reply.started":"2021-08-24T03:44:46.510362Z","shell.execute_reply":"2021-08-24T03:46:13.8929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncf_matrix = confusion_matrix(y, y_pred_train)\nprint(cf_matrix)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:46:13.894392Z","iopub.execute_input":"2021-08-24T03:46:13.894744Z","iopub.status.idle":"2021-08-24T03:46:13.90954Z","shell.execute_reply.started":"2021-08-24T03:46:13.894716Z","shell.execute_reply":"2021-08-24T03:46:13.908294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#.  visualizing the confusion matrix!\nsns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n            fmt='.2%', cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:46:13.910964Z","iopub.execute_input":"2021-08-24T03:46:13.911364Z","iopub.status.idle":"2021-08-24T03:46:14.140107Z","shell.execute_reply.started":"2021-08-24T03:46:13.911327Z","shell.execute_reply":"2021-08-24T03:46:14.13872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualzing. with labels!\nlabels = ['True Neg','False Pos','False Neg','True Pos']\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:47:36.828784Z","iopub.execute_input":"2021-08-24T03:47:36.829399Z","iopub.status.idle":"2021-08-24T03:47:37.077725Z","shell.execute_reply.started":"2021-08-24T03:47:36.829352Z","shell.execute_reply":"2021-08-24T03:47:37.076697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So as we can see the accuracy,\n* logistic regression - 56%\n* svm linear kernel - 54%\n* knn - 56%\n* svm kernel -  57%\n* naive bayes - 52%\n* decision tree - 52%\n* random forest - 58.56%\n* XGB  - 55%","metadata":{}},{"cell_type":"markdown","source":"Let's try to improve the accuracy!","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n# parameters = [{'C':[0.25,0.5,0.75,1], 'kernel' : ['linear']},\n#               {'C':[0.25,0.5,0.75,1], 'kernel' : ['rbf'], 'gamma' : [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}]\n# grid_search = GridSearchCV(estimator=classifier,\n#                           param_grid=parameters,\n#                           scoring='accuracy',\n#                           cv=10)\n# grid_search.fit(X,y)\n# print(\"Best Accuracy: {:.2f} %\".format(grid_search.best_score_*100))\n# print(\"Best Parameters: \", grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:47:48.365951Z","iopub.execute_input":"2021-08-24T03:47:48.366296Z","iopub.status.idle":"2021-08-24T03:47:48.370246Z","shell.execute_reply.started":"2021-08-24T03:47:48.366267Z","shell.execute_reply":"2021-08-24T03:47:48.369196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try using random forest for our test set! and check accuracy!","metadata":{}},{"cell_type":"code","source":"tweet_dataset_test.isnull().sum()\ntweet_dataset_test =  tweet_dataset_test.fillna('')\ntweet_dataset_test['context'] =  tweet_dataset_test['text'] + \" \" + tweet_dataset_test['location'] + ' ' + tweet_dataset_test['keyword']\ntweet_dataset_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:47:48.97848Z","iopub.execute_input":"2021-08-24T03:47:48.978833Z","iopub.status.idle":"2021-08-24T03:47:48.999482Z","shell.execute_reply.started":"2021-08-24T03:47:48.97878Z","shell.execute_reply":"2021-08-24T03:47:48.998735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_dataset_test['context'] = tweet_dataset_test['context'].apply(stemming)\ntweet_dataset_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:47:49.201607Z","iopub.execute_input":"2021-08-24T03:47:49.201967Z","iopub.status.idle":"2021-08-24T03:47:57.645214Z","shell.execute_reply.started":"2021-08-24T03:47:49.201935Z","shell.execute_reply":"2021-08-24T03:47:57.644239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = tweet_dataset_test['context']\nX_test = vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:47:57.646528Z","iopub.execute_input":"2021-08-24T03:47:57.646825Z","iopub.status.idle":"2021-08-24T03:47:57.698153Z","shell.execute_reply.started":"2021-08-24T03:47:57.646779Z","shell.execute_reply":"2021-08-24T03:47:57.697372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test = classifier.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:47:57.699781Z","iopub.execute_input":"2021-08-24T03:47:57.700349Z","iopub.status.idle":"2021-08-24T03:48:00.515858Z","shell.execute_reply.started":"2021-08-24T03:47:57.700305Z","shell.execute_reply":"2021-08-24T03:48:00.514998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create our submission file!","metadata":{}},{"cell_type":"code","source":"results = pd.DataFrame(tweet_dataset_test['id'],columns=['id'])\nresults['target'] = y_pred_test\nresults.shape\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:48:00.517261Z","iopub.execute_input":"2021-08-24T03:48:00.517834Z","iopub.status.idle":"2021-08-24T03:48:00.527834Z","shell.execute_reply.started":"2021-08-24T03:48:00.517768Z","shell.execute_reply":"2021-08-24T03:48:00.526679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_dataset_test.shape\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:48:00.530744Z","iopub.execute_input":"2021-08-24T03:48:00.531054Z","iopub.status.idle":"2021-08-24T03:48:00.541264Z","shell.execute_reply.started":"2021-08-24T03:48:00.531027Z","shell.execute_reply":"2021-08-24T03:48:00.540266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.to_csv('Results.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T03:48:00.542642Z","iopub.execute_input":"2021-08-24T03:48:00.542985Z","iopub.status.idle":"2021-08-24T03:48:00.561009Z","shell.execute_reply.started":"2021-08-24T03:48:00.542954Z","shell.execute_reply":"2021-08-24T03:48:00.559779Z"},"trusted":true},"execution_count":null,"outputs":[]}]}