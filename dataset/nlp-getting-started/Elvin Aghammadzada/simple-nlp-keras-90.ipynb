{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.text\ny = train.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.astype('str')\ntest['text'] = test['text'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport re\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\n# def clean_text(df, wrong_words_dict, autocorrect=True):\n#     df.fillna(\"__NA__\", inplace=True)\n#     tokinizer = RegexpTokenizer(r'\\w+')\n#     regexps = [re.compile(\"([a-zA-Z]+)([0-9]+)\"), re.compile(\"([0-9]+)([a-zA-Z]+)\")]\n#     texts = df.tolist()\n#     result = []\n#     for text in tqdm(texts):\n#         tokens = tokinizer.tokenize(text.lower())\n#         tokens = [split_text_and_digits(token, regexps) for token in tokens]\n#         tokens = [substitute_repeats(token, 3) for token in tokens]\n#         text = ' '.join(tokens)\n#         if autocorrect:\n#             for wrong, right in wrong_words_dict.items():\n#                 text = text.replace(wrong, right)\n#         result.append(text)\n#     return result ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.apply(lambda x: remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x: remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wnl = nltk.stem.WordNetLemmatizer()\ndef lemmatize(text):\n    words = [wnl.lemmatize(w) for w in text]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.apply(lambda x: lemmatize(x))\ntest['text'] = test['text'].apply(lambda x: lemmatize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bog\nMAX_NB_WORDS = 100000\nmax_seq_len = 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_NB_WORDS+1, char_level=False)\ntokenizer.fit_on_texts(pd.concat([X, test['text']]))\nword_seq_train = tokenizer.texts_to_sequences(X)\nword_seq_test = tokenizer.texts_to_sequences(test['text'])\nword_index = tokenizer.word_index\nprint(\"dictionary size: \", len(tokenizer.word_index))\n\n#pad sequences\n# [[1], [2, 3],[4, 5, 6]] =>\n# [[0, 0, 1],\n#  [0, 2, 3],\n#  [4, 5, 6]]\nword_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\nword_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import codecs\nfrom tqdm import tqdm\n\nft = codecs.open('../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec', encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\n\nfor line in tqdm(ft):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\n    \nft.close()\nprint('found %s word vectors' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 54 \nnum_epochs = 15\n\n#model parameters\nnum_filters = 40\nembed_dim = 300 \nweight_decay = 1e-4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words_not_found = []\nnb_words = min(MAX_NB_WORDS, len(word_index))\nembedding_matrix = np.zeros((nb_words+1, embed_dim))\nfor word, i in word_index.items():\n    if i >= nb_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \nfrom keras import regularizers\nfrom keras import optimizers\nfrom keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In 1D CNN, the kernel moves in 1 direction. \n# The input and output data of 1D CNN is 2 dimensional. \n# Mostly used on Time-Series Data, Natural Language Processing tasks.\n\n# In 2D CNN, the kernel moves in 2 directions. \n# The input and output data of 2D CNN is 3 dimensional. \n# Mostly used on Image data. \n\n# In 3D CNN, the kernel moves in 3 directions. \n# The input and output data of 3D CNN is 4 dimensional. \n# Mostly used on 3D Image data (MRI, CT Scans). \nmodel = Sequential()\nmodel.add(Embedding(nb_words+1, embed_dim,\n          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(MaxPooling1D(2))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\ncallbacks_list = [early_stopping]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(word_seq_train, y, batch_size=batch_size,\n                 epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = pd.DataFrame({\"loss\": history.history['loss'],\n                        \"val_loss\": history.history['val_loss']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\ntarget = model.predict(word_seq_test)\nsubmission['target'] = target.round().astype(int)\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}