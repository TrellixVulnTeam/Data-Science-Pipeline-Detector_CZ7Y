{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-01T09:49:48.697475Z","iopub.execute_input":"2021-10-01T09:49:48.697825Z","iopub.status.idle":"2021-10-01T09:49:48.704049Z","shell.execute_reply.started":"2021-10-01T09:49:48.697791Z","shell.execute_reply":"2021-10-01T09:49:48.703397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get to know the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:36:55.003445Z","iopub.execute_input":"2021-10-01T09:36:55.003756Z","iopub.status.idle":"2021-10-01T09:36:55.074229Z","shell.execute_reply.started":"2021-10-01T09:36:55.003726Z","shell.execute_reply":"2021-10-01T09:36:55.073391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's look at what are the unique values are there for the keyword and location columns\n\ndf['keyword'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:39:24.005763Z","iopub.execute_input":"2021-10-01T09:39:24.006454Z","iopub.status.idle":"2021-10-01T09:39:24.016048Z","shell.execute_reply.started":"2021-10-01T09:39:24.006413Z","shell.execute_reply":"2021-10-01T09:39:24.015112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['location'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:40:13.572397Z","iopub.execute_input":"2021-10-01T09:40:13.572692Z","iopub.status.idle":"2021-10-01T09:40:13.581017Z","shell.execute_reply.started":"2021-10-01T09:40:13.572663Z","shell.execute_reply":"2021-10-01T09:40:13.579846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since, the keywords can be found in the tweet itself and location isn't really important for our text classification model, we will be removing these two columns along with the id.","metadata":{}},{"cell_type":"code","source":"df = df.drop(['id', 'keyword', 'location'], axis = 1)\n\ndf","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:45:44.623246Z","iopub.execute_input":"2021-10-01T09:45:44.623556Z","iopub.status.idle":"2021-10-01T09:45:44.640056Z","shell.execute_reply.started":"2021-10-01T09:45:44.623525Z","shell.execute_reply":"2021-10-01T09:45:44.639447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will seperating the text feature and the target, \n# and then splitting both into train and validation sets\n\nX = df['text']\ny = df['target']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25)\n\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:50:41.054495Z","iopub.execute_input":"2021-10-01T09:50:41.055189Z","iopub.status.idle":"2021-10-01T09:50:41.067599Z","shell.execute_reply.started":"2021-10-01T09:50:41.055151Z","shell.execute_reply":"2021-10-01T09:50:41.066771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The NLP Pipeline","metadata":{}},{"cell_type":"code","source":"# Initialize all the preprocessing objects\n\ntokenizer = RegexpTokenizer(r\"\\w+\") # only select alphanumeric characters\nen_stop = set(stopwords.words('english')) # get all the English language stopwords\nps = PorterStemmer() # to extract stem out of any given word","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:51:34.667229Z","iopub.execute_input":"2021-10-01T09:51:34.667559Z","iopub.status.idle":"2021-10-01T09:51:34.679672Z","shell.execute_reply.started":"2021-10-01T09:51:34.667527Z","shell.execute_reply":"2021-10-01T09:51:34.678531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getStemmedTweet(tweet):\n    \"\"\"\n        This function takes the tweet string and then performs the preprocessing steps on it\n        to return the cleaned tweet which will be more effective in predictions later made by the \n        classifier.\n    \"\"\"\n    tweet = tweet.lower()\n    \n    tokens = tokenizer.tokenize(tweet)\n    new_tokens = [token for token in tokens if token not in en_stop]\n    stemmed_tokens = [ps.stem(token) for token in new_tokens]\n    \n    cleaned_review = ' '.join(stemmed_tokens)\n    \n    return cleaned_review","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:54:50.262429Z","iopub.execute_input":"2021-10-01T09:54:50.262762Z","iopub.status.idle":"2021-10-01T09:54:50.269807Z","shell.execute_reply.started":"2021-10-01T09:54:50.262732Z","shell.execute_reply":"2021-10-01T09:54:50.268872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check out the results of the function \nrand_num = 34\nprint(\"Review ===> \", X_train[rand_num])\nprint(\"Preprocessed Review ===>\", getStemmedTweet(X_train[rand_num]))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:55:16.658484Z","iopub.execute_input":"2021-10-01T09:55:16.659233Z","iopub.status.idle":"2021-10-01T09:55:16.668124Z","shell.execute_reply.started":"2021-10-01T09:55:16.659193Z","shell.execute_reply":"2021-10-01T09:55:16.667392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the preprocessed review is much more shorter, and conveys the same meaning as the original tweet.","metadata":{}},{"cell_type":"code","source":"# Apply the preprocessing pipeline function on the whole dataset\nX_cleaned = X_train.apply(getStemmedTweet)\nXval_cleaned = X_val.apply(getStemmedTweet)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:55:59.836997Z","iopub.execute_input":"2021-10-01T09:55:59.837261Z","iopub.status.idle":"2021-10-01T09:56:02.2671Z","shell.execute_reply.started":"2021-10-01T09:55:59.837237Z","shell.execute_reply":"2021-10-01T09:56:02.26653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the classifier","metadata":{}},{"cell_type":"code","source":"## First of all though, we'll need to convert our data into a count vector to be able \n## to work with the Multinomial Naive Bayes model\n\ncv = CountVectorizer()\n\nX_vec = cv.fit_transform(X_cleaned).toarray()\nXval_vec = cv.transform(Xval_cleaned).toarray()\n\nprint(X_vec.shape)\nprint(Xval_vec.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:58:31.639122Z","iopub.execute_input":"2021-10-01T09:58:31.640166Z","iopub.status.idle":"2021-10-01T09:58:32.267051Z","shell.execute_reply.started":"2021-10-01T09:58:31.640108Z","shell.execute_reply":"2021-10-01T09:58:32.265941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the classifier\n\nmnb = MultinomialNB()\nmnb.fit(X_vec, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T09:59:16.742617Z","iopub.execute_input":"2021-10-01T09:59:16.742927Z","iopub.status.idle":"2021-10-01T09:59:19.248196Z","shell.execute_reply.started":"2021-10-01T09:59:16.742897Z","shell.execute_reply":"2021-10-01T09:59:19.24763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Test the accuracy of our classifier on the validation set\n\nmnb.score(Xval_vec, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:00:27.148768Z","iopub.execute_input":"2021-10-01T10:00:27.149093Z","iopub.status.idle":"2021-10-01T10:00:27.350241Z","shell.execute_reply.started":"2021-10-01T10:00:27.149062Z","shell.execute_reply":"2021-10-01T10:00:27.34926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PREDICTION TIME!","metadata":{}},{"cell_type":"code","source":"# get the test dataset\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\n\n# get the main 'text' column\ntest_ids = test['id']\ntest = test['text']\n\n# apply the preprocessing pipeline\ntest = test.apply(getStemmedTweet)\n\n# creating the count vectors from the dataset\ntest_vec = cv.transform(test).toarray()\n\npredictions = mnb.predict(test_vec)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:05:24.573774Z","iopub.execute_input":"2021-10-01T10:05:24.574077Z","iopub.status.idle":"2021-10-01T10:05:26.077299Z","shell.execute_reply.started":"2021-10-01T10:05:24.574048Z","shell.execute_reply":"2021-10-01T10:05:26.076159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pd.Series(predictions)\nids = pd.Series(test_ids)\n\npred_df = pd.concat([ids, predictions], keys = ['id', 'target'], axis = 1)\n\npred_df.to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T10:09:44.372966Z","iopub.execute_input":"2021-10-01T10:09:44.37333Z","iopub.status.idle":"2021-10-01T10:09:44.392484Z","shell.execute_reply.started":"2021-10-01T10:09:44.373296Z","shell.execute_reply":"2021-10-01T10:09:44.391736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}