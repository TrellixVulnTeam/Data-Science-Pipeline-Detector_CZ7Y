{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Import relevant libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom textblob import TextBlob, Word\nimport collections\nimport re\nimport string\nimport emoji\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import StackingClassifier\nimport xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\n\nimport tensorflow as tf\nfrom collections import Counter\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Class distribution","metadata":{}},{"cell_type":"code","source":"train_df[\"target\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"target\"].value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"countplot = sns.countplot(x=\"target\", data=train_df, palette=\"Set1\")\ncountplot.set_title(\"Real disaster tweets count\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_labels=[\"Non-Disaster\", \"Disaster\"]\nplt.pie(train_df['target'].value_counts(), labels=my_labels, colors = [\"Blue\",\"Red\"])\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.drop(columns=['id','keyword','location'], axis=1, inplace=True)\ntest_df.drop(columns=['keyword','location'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Wordclouds","metadata":{}},{"cell_type":"markdown","source":"### 2.2.1 Wordcloud for real disaster tweets","metadata":{}},{"cell_type":"code","source":"ax = plt.figure(figsize=(20,20))\nwordcloud = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(train_df[train_df.target == 1].text))\nplt.imshow(wordcloud , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.2 Wordcloud for non-disaster tweets","metadata":{}},{"cell_type":"code","source":"ax = plt.figure(figsize=(20,20))\nwordcloud = WordCloud(max_words = 500 , width = 1000 , height = 500 , stopwords = STOPWORDS).generate(\" \".join(train_df[train_df.target == 0].text))\nplt.imshow(wordcloud , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, there are many noisy words that should not be indicative of disaster. Let's clean this up","metadata":{}},{"cell_type":"markdown","source":"# 3. Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Defining variables","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\ncontraction_map = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n}\n\nall_punctuation = set(string.punctuation)\nall_punctuation.add(\"...\")\nall_punctuation.add('’')\nall_punctuation.add('-')\nall_punctuation.add('“')\nall_punctuation.add('[')\nall_punctuation.add(']')\nall_punctuation.add(' ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create some functions so it'll be easier to implement them down the line","metadata":{}},{"cell_type":"markdown","source":"## 3.2 Helper functions","metadata":{}},{"cell_type":"markdown","source":"### 3.2.1 Lower-casing text","metadata":{}},{"cell_type":"code","source":"def uncapitalize(text):\n    return text.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2 Remove emojis","metadata":{}},{"cell_type":"code","source":"def removeEmojis(text):\n    allchars = [c for c in text]\n    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI[\"en\"]]\n    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n    return clean_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.3 Expand abbreviations","metadata":{}},{"cell_type":"code","source":"def expand_abbr(article):\n    new_article = article\n    for item in contraction_map:\n        if item in article:\n            new_article = article.replace(item,contraction_map[item])\n    return new_article","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.4 Remove website urls","metadata":{}},{"cell_type":"code","source":"def strip_links(text):\n    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n    links         = re.findall(link_regex, text)\n    for link in links:\n        text = text.replace(link[0], ', ')    \n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.5 Stripping all entities","metadata":{}},{"cell_type":"code","source":"def strip_all_entities(text):\n    entity_prefixes = ['@','#']\n    for separator in  string.punctuation:\n        if separator not in entity_prefixes :\n            text = text.replace(separator,' ')\n    words = []\n    for word in text.split():\n        word = word.strip()\n        if word:\n            if word[0] not in entity_prefixes:\n                words.append(word)\n    return ' '.join(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.6 Lemmantization","metadata":{}},{"cell_type":"code","source":"def lemmatize_with_postag(text):\n    sent = TextBlob(text)\n    tag_dict = {\"J\": 'a', \n                \"N\": 'n', \n                \"V\": 'v', \n                \"R\": 'r'}\n    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n    return \" \".join(lemmatized_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.7 Remove stopwords","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(text):\n    stop_words = set(stopwords.words('english'))\n    word_tokens = word_tokenize(text)\n    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n    return filtered_sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.8 Remove punctuations","metadata":{}},{"cell_type":"code","source":"def remove_punctuation(token_list):\n    new_list = []\n    for tok in token_list:\n        if tok not in all_punctuation:\n            new_list.append(tok)\n    final_list = [x for x in new_list if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n    final_sentence = \" \".join(final_list)\n    return final_sentence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Cleaning text","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = uncapitalize(text)\n    text = removeEmojis(text)\n    text = expand_abbr(text)\n    text = strip_links(text)\n    text = strip_all_entities(text)\n    text = lemmatize_with_postag(text)\n    cleaned_tokens = remove_stopwords(text)\n    final_text = remove_punctuation(cleaned_tokens)\n    return final_text\nprocessed_train_df = train_df.copy(deep=True)\nprocessed_train_df[\"text\"] = processed_train_df.text.apply(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks good! Now let's take a look at more visualizations to easier understand our data","metadata":{}},{"cell_type":"markdown","source":"# 4. Further visualizations","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Wordclouds ","metadata":{}},{"cell_type":"markdown","source":"### 4.1.1 Wordcloud for real disaster tweets","metadata":{}},{"cell_type":"code","source":"ax = plt.figure(figsize=(20,20))\nwordcloud = WordCloud(max_words = 500, width = 1000, height = 500).generate(\" \".join(processed_train_df[processed_train_df.target == 1].text))\nplt.imshow(wordcloud , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.2 Wordcloud for non-disaster tweets","metadata":{}},{"cell_type":"code","source":"ax = plt.figure(figsize=(20,20))\nwordcloud = WordCloud(max_words = 500, width = 1000, height = 500).generate(\" \".join(processed_train_df[processed_train_df.target == 0].text))\nplt.imshow(wordcloud , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 n-grams of real disaster tweets","metadata":{}},{"cell_type":"markdown","source":"### 4.2.1 Preprocessing","metadata":{}},{"cell_type":"code","source":"def extract_ngrams(text, num):\n    n_grams = ngrams(nltk.word_tokenize(text), num)\n    return [' '.join(grams) for grams in n_grams]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_text = \" \".join(processed_train_df[processed_train_df.target == 1].text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.2 Uni-grams (most common words)","metadata":{}},{"cell_type":"code","source":"real_one_gram = extract_ngrams(disaster_text, 1)\nreal_one_gram_freq = collections.Counter(real_one_gram)\nreal_one_gram_freq.most_common(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_list = real_one_gram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(10,10))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=90)\nax.set_ylabel(\"count\")\nax.set_title(\"unigram of real disaster tweets\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.3 Bi-grams","metadata":{}},{"cell_type":"code","source":"real_bigram = extract_ngrams(disaster_text, 2)\nreal_bigram_freq = collections.Counter(real_bigram)\nreal_bigram_freq.most_common(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_list = real_bigram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(10,10))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=90)\nax.set_ylabel(\"count\")\nax.set_title(\"bigram of real disaster tweets\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2.4 Tri-grams","metadata":{}},{"cell_type":"code","source":"real_trigram = extract_ngrams(disaster_text, 3)\nreal_trigram_freq = collections.Counter(real_trigram)\nreal_trigram_freq.most_common(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_list = real_trigram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(10,10))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=90)\nax.set_ylabel(\"count\")\nax.set_title(\"trigram of real disaster tweets\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3 n-grams of non-disaster tweets","metadata":{}},{"cell_type":"markdown","source":"### 4.3.1 Preprocessing","metadata":{}},{"cell_type":"code","source":"non_disaster_text = \" \".join(processed_train_df[processed_train_df.target == 0].text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.2 Uni-grams (most common words)","metadata":{}},{"cell_type":"code","source":"non_disaster_one_gram = extract_ngrams(non_disaster_text, 1)\nnon_disaster_one_gram_freq = collections.Counter(non_disaster_one_gram)\nnon_disaster_one_gram_freq.most_common(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_list = non_disaster_one_gram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(10,10))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=45)\nax.set_ylabel(\"count\")\nax.set_title(\"unigram of non-disaster tweets\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.3 Bi-grams","metadata":{}},{"cell_type":"code","source":"non_disaster_bigram = extract_ngrams(non_disaster_text, 2)\nnon_disaster_bigram_freq = collections.Counter(non_disaster_bigram)\nnon_disaster_bigram_freq.most_common(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_list = non_disaster_bigram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(18,1))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=90)\nax.set_ylabel(\"count\")\nax.set_title(\"bigram of non-disaster tweets\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.4 Tri-grams","metadata":{}},{"cell_type":"code","source":"non_disaster_trigram = extract_ngrams(non_disaster_text, 3)\nnon_disaster_trigram_freq = collections.Counter(non_disaster_trigram)\nnon_disaster_trigram_freq.most_common(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq_list = non_disaster_trigram_freq.most_common(15)\nfig,ax = plt.subplots()\n\nfig = plt.figure(figsize=(10,10))\nx = []\ny = []\nfor item in freq_list:\n    x.append(item[0])\n    y.append(item[1])\n    \nax.vlines(x,ymin=8, ymax=y, color=\"green\")\nax.plot(x,y, \"o\", color=\"maroon\")\nax.set_xticklabels(x, rotation=90)\nax.set_ylabel(\"count\")\nax.set_title(\"trigram of non-disaster tweets\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Importing word embeddings (GloVe)","metadata":{}},{"cell_type":"code","source":"embedding_df = processed_train_df.copy(deep=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_embeddings = {}\nf = open('/kaggle/input/glove6b/glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Converting sentences into vectors","metadata":{}},{"cell_type":"code","source":"def get_sentence_vectors(text):\n    sentence_vector = np.zeros((100,))\n    if len(text) == 0:\n        return sentence_vector\n    else:\n        tokens = text.split()\n        for token in tokens:\n            try:\n                sentence_vector += word_embeddings[token]\n            except:\n                pass\n        sentence_vector = sentence_vector/len(tokens)\n        return sentence_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_df[\"text\"] = embedding_df.text.apply(get_sentence_vectors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Machine learning models","metadata":{}},{"cell_type":"markdown","source":"## 7.1 Train test split","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(embedding_df[\"text\"],embedding_df[\"target\"],test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.2 Naive Bayes","metadata":{}},{"cell_type":"code","source":"print(f\"**********Naive Bayes**********\")\nmodel =  GaussianNB()\nstart = time.time()\nmodel.fit(x_train.to_list(),y_train)\ny_pred = model.predict(x_test.to_list())\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.3 Logistic Regression","metadata":{}},{"cell_type":"code","source":"print(f\"**********Logistic Regression**********\")\nmodel = LogisticRegression()\nstart = time.time()\nmodel.fit(x_train.to_list(),y_train)\ny_pred = model.predict(x_test.to_list())\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.4 Random Forest","metadata":{}},{"cell_type":"code","source":"print(f\"**********Random Forest**********\")\nmodel = RandomForestClassifier()\nstart = time.time()\nmodel.fit(x_train.to_list(),y_train)\ny_pred = model.predict(x_test.to_list())\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.5 AdaBoost","metadata":{}},{"cell_type":"code","source":"print(f\"**********AdaBoost**********\")\nmodel = AdaBoostClassifier()\nstart = time.time()\nmodel.fit(x_train.to_list(),y_train)\ny_pred = model.predict(x_test.to_list())\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.6 XGBoost","metadata":{}},{"cell_type":"code","source":"print(f\"**********XGBoost**********\")\nmodel = XGBClassifier()\nstart = time.time()\nmodel.fit(np.asarray(x_train.to_list()),y_train)\ny_pred = model.predict(np.asarray(x_test.to_list()))\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.7 Support Vector Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nprint(f\"**********SVC**********\")\nmodel = SVC()\nstart = time.time()\nmodel.fit(x_train.to_list(),y_train)\ny_pred = model.predict(x_test.to_list())\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7.8 Stacking","metadata":{}},{"cell_type":"code","source":"print(\"**********Stacking**********\")\nstart = time.time()\nestimators = [(\"xgb\", XGBClassifier()), (\"SVC\",SVC()), (\"rfe\",RandomForestClassifier())]\nfinal_estimator = LinearSVC()\nstacking_clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\nstacking_clf.fit(np.asarray(x_train.to_list()),y_train)\ny_pred = stacking_clf.predict(np.asarray(x_test.to_list()))\nf1score = f1_score(y_test,y_pred)\naccuracyscore = accuracy_score(y_test,y_pred)\nprecisionscore = precision_score(y_test,y_pred)\nrecallscore = recall_score(y_test,y_pred)\nprint(f\"f1_score: {f1score}\")\nprint(f\"Accuracy: {accuracyscore}\")\nprint(f\"Precision: {precisionscore}\")\nprint(f\"Recall: {recallscore}\")\nprint('Time Taken :' + str(round(start - time.time(),2) * -1))\nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Artificial Neural Networks (ANNs)","metadata":{}},{"cell_type":"markdown","source":"## 8.1 Stop function","metadata":{}},{"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch,logs={}):\n    if(logs.get('accuracy')>0.90):\n      print(\"\\nReached 90% accuracy so cancelling training\")\n      self.model.stop_training=True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.2 Creating model","metadata":{}},{"cell_type":"code","source":"callbacks = myCallback()\nann_model = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\nann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"accuracy\"])\nann_model.fit(np.asarray(x_train.to_list()), y_train, epochs=1000, validation_data=(np.asarray(x_test.to_list()), y_test), callbacks=[callbacks])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.3 Evaluate ANN","metadata":{}},{"cell_type":"code","source":"ann_model.evaluate(np.asarray(x_test.to_list()),y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Preperation for LSTM","metadata":{}},{"cell_type":"markdown","source":"## 9.1 Create word frequency","metadata":{}},{"cell_type":"code","source":"processed_train_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def counter_word(text):\n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counter = counter_word(processed_train_df.text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words = len(counter)\n#Max number of words in a sequence\nmax_length = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.2 Create train test split","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(processed_train_df[\"text\"],processed_train_df[\"target\"],test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.3 Word index","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words = num_words)\ntokenizer.fit_on_texts(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer.word_index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.4 Create word sequence","metadata":{}},{"cell_type":"code","source":"train_sequences = tokenizer.texts_to_sequences(x_train)\ntrain_sequences[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 9.5 Text padding","metadata":{}},{"cell_type":"code","source":"train_padded = pad_sequences(\n    train_sequences, maxlen= max_length, padding='post', truncating = 'post'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(x_test)\ntest_sequences[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_padded = pad_sequences(\n    test_sequences, maxlen= max_length, padding='post', truncating = 'post'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train.head(1))\nprint(train_sequences[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Building LSTM model","metadata":{}},{"cell_type":"markdown","source":"## 10.1 Stop function","metadata":{}},{"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch,logs={}):\n    if(logs.get('accuracy')>=0.85):\n      print(\"\\nReached 85% accuracy so cancelling training\")\n      self.model.stop_training=True\ncallbacks = myCallback()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10.2 Creating model","metadata":{}},{"cell_type":"code","source":"lstm_model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(num_words, 32, input_length=max_length),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.LSTM(200),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(64, activation=\"relu\"),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(32, activation=\"relu\"),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\n\nlstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = lstm_model.fit(\n    train_padded,\n    y_train,\n    epochs=30,\n    verbose=1,\n    validation_data=(test_padded, y_test),\n    callbacks=[callbacks]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 10.3 Evaluate LSTM","metadata":{}},{"cell_type":"code","source":"lstm_model.evaluate(test_padded,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Output","metadata":{}},{"cell_type":"markdown","source":"## 11.1 Preprocess test dataset","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = uncapitalize(text)\n    text = removeEmojis(text)\n    text = expand_abbr(text)\n    text = strip_links(text)\n    text = strip_all_entities(text)\n    text = lemmatize_with_postag(text)\n    cleaned_tokens = remove_stopwords(text)\n    final_text = remove_punctuation(cleaned_tokens)\n    return final_text\n\nprocessed_test_df = test_df.copy(deep=True)\nprocessed_test_df[\"text\"] = processed_test_df.text.apply(clean_text)\n\nfinal_train_df = train_df.copy(deep=True)\nfinal_train_df[\"text\"] = final_train_df.text.apply(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11.2 Convert text to vectors","metadata":{}},{"cell_type":"code","source":"test_embedding_df = processed_test_df.copy(deep=True)\ntest_embedding_df[\"text\"] = test_embedding_df.text.apply(get_sentence_vectors)\n\nfinal_train_embedding = final_train_df.copy(deep=True)\nfinal_train_embedding[\"text\"] = final_train_embedding.text.apply(get_sentence_vectors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_train_embedding.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 11.3 Output predictions","metadata":{}},{"cell_type":"code","source":"estimators = [(\"xgb\", XGBClassifier()), (\"SVC\",SVC()), (\"rfe\",RandomForestClassifier())]\nfinal_estimator = LinearSVC()\nstacking_clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\nstacking_clf.fit(np.asarray(final_train_embedding[\"text\"].to_list()),final_train_embedding[\"target\"])\npredictions = stacking_clf.predict(np.asarray(test_embedding_df[\"text\"].to_list()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df = pd.Series(np.array(predictions).flatten()).to_frame()\nresult_df = pd.concat([test_df,predictions_df], axis = 1)\nresult_df.drop(columns=['text'], axis=1, inplace=True)\nresult_df = result_df.rename(columns={0: \"target\"})\nresult_df['target'] = result_df['target'].map(lambda a: int(a))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv('result.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}