{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nif torch.cuda.is_available():  \n    device = torch.device(\"cuda\")\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_data = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['keyword'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['keyword'].nunique()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['location'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"text\"][4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib as plt\nax = sns.countplot(train_data.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport re\nimport nltk\nnltk.download('stopwords')\nstopwords = nltk.corpus.stopwords.words('english')\n\n# data preprocessing\ndef clean_text(text):\n    text = text.lower()\n    # remove hyperlinks\n    text = re.sub(r\"http\\S+\", \"\", text)\n    # remove spcl characters\n    text = \"\".join([word for word in text if word not in string.punctuation])\n    text = re.sub(\"\\W\", \" \", str(text))\n    # remove stopwords\n    text = [word for word in text.split() if word not in stopwords]\n    # remove any numeric characters\n    text = [word for word in text if re.search(\"\\d\", word)== None]\n    # convert split to text again\n    text = ' '.join(word for word in text)\n    return text\n\n# train_data['text_clean'] = train_data['text'].apply(lambda x: clean_text(x))\ntrain_data['text_clean'] = train_data['text'].apply(clean_text)\n\ntest_data['text_clean'] = test_data['text'].apply(clean_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop(['id','keyword','location', 'text'], axis = 1) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.drop(['id','keyword','location', 'text'], axis = 1) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"target\"].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_data['text_clean'].values\nlabels = train_data['target'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ELECTRA "},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import ElectraTokenizer, ElectraForSequenceClassification,AdamW\nimport torch\ntokenizer = ElectraTokenizer.from_pretrained('google/electra-base-discriminator')\nmodel = ElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator',num_labels=2)\nmodel.cuda()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#important to know the max len of each sentence\n\nimport matplotlib.pyplot as plt\ndef plot_sentence_embeddings_length(text_list, tokenizer):\n    tokenized_texts = list(map(lambda t: tokenizer.tokenize(t), text_list))\n    tokenized_texts_len = list(map(lambda t: len(t), tokenized_texts))\n    fig, ax = plt.subplots(figsize=(8, 5));\n    ax.hist(tokenized_texts_len, bins=60);\n    ax.set_xlabel(\"Length of Comment Embeddings\");\n    ax.set_ylabel(\"Number of Comments\");\n    return max(tokenized_texts_len)\n\n\nplot_sentence_embeddings_length(data, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_lens = []\nfor txt in data:\n    \n    tokens = tokenizer.encode(txt, max_length=70)\n    token_lens.append(len(tokens))\n\nsns.distplot(token_lens)\nplt.xlim([0, 40]);\nplt.xlabel('Token count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(token_lens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph we can conclude that the max number of tweets have less than 30 tokens. so let us take the max_len as 36."},{"metadata":{"trusted":true},"cell_type":"code","source":"indices=tokenizer.batch_encode_plus(data,max_length=38,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids=indices[\"input_ids\"]\nattention_masks=indices[\"attention_mask\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Use 99% for training and 1% for validation.\ntrain_ids, val_ids, train_labels, val_labels = train_test_split(input_ids, labels, \n                                                            random_state=42, test_size=0.2)\n# Do the same for the masks.\ntrain_masks, val_masks, _, _ = train_test_split(attention_masks, labels,\n                                             random_state=42, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_ids)\n# len(train_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"convert data to tensors"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ids = torch.tensor(train_ids)\nval_ids = torch.tensor(val_ids)\ntrain_labels = torch.tensor(train_labels)\nval_labels = torch.tensor(val_labels)\ntrain_masks = torch.tensor(train_masks)\nval_masks = torch.tensor(val_masks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Create DATALOADER for training and testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n#TRAINING DATA\n\ntrain_data = TensorDataset(train_ids, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data,sampler = train_sampler,batch_size = 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_dataloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_iter3 = iter(train_dataloader)\nprint(type(train_iter3))\n\nprint(len(train_iter3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Validation Data\n\nval_data = TensorDataset(val_ids, val_masks, val_labels)\nval_sampler = RandomSampler(val_data)\nval_dataloader = DataLoader(val_data,sampler = val_sampler,batch_size = 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = AdamW(model.parameters(),lr = 6e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5\neps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                 )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 5\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def acc_score(y_pred, y_true):\n    # correct labels = 0\n    cor = 0\n    # loop over all the entries in test data\n    for i in range(len(y_pred)):\n        # if predicted = actual label, add 1 to correct labels\n        if(y_pred[i] == y_true[i]):\n            cor +=1\n    # return accuracy score\n    return cor/len(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\n# This training code is based on the `run_glue.py` script here:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Store the average loss after each epoch so we can plot them.\nloss_values = []\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch_i in range(epochs):\n    print(\"epoch is\" + str(epoch_i))\n    print(\"training...\")\n    t0 = time.time()\n    total_loss = 0\n    model.train()\n    for step,batch in enumerate(train_dataloader): # total steps are 191... runs from step 0 to steps 190\n        print(\"step\",step)\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        model.zero_grad()\n        outputs = model(b_input_ids,token_type_ids = None, attention_mask = b_input_mask,labels = b_labels)\n        loss = outputs[0]\n        total_loss += loss.item()\n        loss. backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        avg_train_loss = total_loss / len(train_dataloader)\n        print(\"avg_train_loss\",avg_train_loss)\n        loss_values.append(avg_train_loss)\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"Training complete!\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\n\npred = []\ntrue = []\neval_acc = 0\nnb_eval_steps = 0\n\nfor batch in val_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    \n    b_input_ids, b_input_mask, b_labels = batch\n    with torch.no_grad():\n        outputs = model(b_input_ids,token_type_ids = None, attention_mask = b_input_mask)\n        logits = outputs[0]\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        pred.append(logits)\n        true.append(label_ids)\n        temp_eval_acc = flat_accuracy(logits,label_ids)\n        eval_acc += temp_eval_acc\n        nb_eval_steps += 1\n        \nprint(\"  Accuracy: {0:.2f}\".format(eval_acc/nb_eval_steps))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine the predictions for each batch into a single list of 0s and 1s.\nflat_predictions = [item for sublist in pred for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n# Combine the correct labels for each batch into a single list.\nflat_true_labels = [item for sublist in true for item in sublist]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(flat_predictions,flat_true_labels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\n\npred = []\ntrue = []\neval_acc = 0\nnb_eval_steps = 0\n\nfor batch in val_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    \n    b_input_ids, b_input_mask, b_labels = batch\n    with torch.no_grad():\n        outputs = model(b_input_ids,token_type_ids = None, attention_mask = b_input_mask)\n        logits = outputs[0]\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        pred.append(logits)\n        true.append(label_ids)\n        temp_eval_acc = flat_accuracy(logits,label_ids)\n        eval_acc += temp_eval_acc\n        nb_eval_steps += 1\n        \nprint(\"  Accuracy: {0:.2f}\".format(eval_acc/nb_eval_steps))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.text.values\n\nindices=tokenizer.batch_encode_plus(test_data,max_length=38,add_special_tokens=True, return_attention_mask=True,pad_to_max_length=True,truncation=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids = indices[\"input_ids\"]\natt_mask = indices[\"attention_mask\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ids = torch.tensor(input_ids)\ntest_mask = torch.tensor(att_mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\n\n\nprediction_data = TensorDataset(test_ids, test_mask)\nprediction_sampler = SequentialSampler(prediction_data)\nprediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\n\npredictions = []\n\nfor batch in prediction_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask = batch\n    \n    with torch.no_grad():\n        outputs = model(b_input_ids,token_type_ids = None, attention_mask = b_input_mask)\n        logits = outputs[0]\n        \n        logits = logits.detach().cpu().numpy()\n        \n        predictions.append(logits)\n        \n        \nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsubmit=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':flat_predictions})\nsubmit.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}