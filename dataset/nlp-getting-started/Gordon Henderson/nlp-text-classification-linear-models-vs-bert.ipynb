{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nThis competition is a nice introduction to Natural Language Processing, classyfing tweets into those relating to diasters and those that don't. The first and most simple stratgy with NLP is usually to represent our texts as a bag of words or TF-IDF (text frequency - inverse document frequency) vectors and use a clinear classification model. A more sophisticated approach is to use BERT (Bidirectional Encoder Representations from Transformers). This is a nice introdcution to BERT - https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/.\n\nIn this notebook, I'll try both methods.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the official tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization\n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n \nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and Prepare Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training data\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\nprint('Training data shape: ', train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Testing data \ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\nprint('Testing data shape: ', test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing values in training set\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing values in test set\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We've got a relatively balanced dataset. Lots of location values are missing but relatively small number of few keywords are missing.\n\nIt's possible to use Keyword and Location as meta-features for our linear model. I won't explore that in this notebook (I might come back and add it later). Location is very sparsely populated so I left it alone. I did try just appending the keyword to the to see if that improved the results but it didn't","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Model 1: Traditional NLP - Bag of Words + Linear Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# take copies of the data to leave the originals for BERT\ntrain1 = train.copy()\ntest1 = test.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data cleaning:** In summary, we want to tokenize our text then send it through a round of cleaning where we turn all characters to lower case, remove brackets, URLs, html tags, punctuation, numbers, etc. We'll also remove emojis from the text and remove common stopwords. This is a vital step in the Bag-of-words + linear model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying a first round of text cleaning techniques\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower() # make text lower case\n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text) # remove URLs\n    text = re.sub('<.*?>+', '', text) # remove html tags\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    text = re.sub('\\n', '', text) # remove words conatinaing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[‘’“”…]', '', text)\n\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# emoji removal\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n# Applying the de=emojifying function to both test and training datasets\ntrain1['text'] = train1['text'].apply(lambda x: remove_emoji(x))\ntest1['text'] = test1['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# text preprocessing function\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer_reg = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer_reg.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text\n\n# Applying the cleaning function to both test and training datasets\ntrain1['text'] = train1['text'].apply(lambda x: text_preprocessing(x))\ntest1['text'] = test1['text'].apply(lambda x: text_preprocessing(x))\n\n# Let's take a look at the updated text\ntrain1['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bag of Words Vectorizer\n\nHere we're only going to use uni-grams and add any word that appears to the vocabulary. Adding 2- and 3- grams didn't improve the score, surprisingly.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#count_vectorizer = CountVectorizer()\ncount_vectorizer = CountVectorizer(ngram_range = (1,1), min_df = 1)\ntrain_vectors = count_vectorizer.fit_transform(train1['text'])\ntest_vectors = count_vectorizer.transform(test1[\"text\"])\n\n## Keeping only non-zero elements to preserve space \ntrain_vectors.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TF-IDF Vectorizer\n\nHere we use 1- and 2-grams where each terms has to appear at least twice and we ignore terms appearing in over 50% of text examples.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df = 2, max_df = 0.5)\ntrain_tfidf = tfidf.fit_transform(train1['text'])\ntest_tfidf = tfidf.transform(test1[\"text\"])\n\ntrain_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models\n\nFit Logistic Regression and Multinomial Naive Bayes models with BoW and TF-IDF, giving four models in total. This is not an extensive list of vectorization options and models and I won't tune any of the models. It's more of an example framework for linear models in NLP and (spoiler) BERT is going to beat whatever linear model we can come up with.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Logistic Regression on BoW\nlogreg_bow = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(logreg_bow, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Logistic Regression on TFIDF\nlogreg_tfidf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(logreg_tfidf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Naive Bayes on BoW\nNB_bow = MultinomialNB()\nscores = model_selection.cross_val_score(NB_bow, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nNB_tfidf = MultinomialNB()\nscores = model_selection.cross_val_score(NB_tfidf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best score is when we use MNB on the bag of words vectors. It gives a training score of 0.6585 and a leaderboard score of 0.7945.\n\nBag of Words is significantly better than TF-IDF in both models and it's a little bit surprising that 1-grams with no minumum limit seems to give the best results. I think this might be partly due to the nature of the data. Tweets are usually pretty short and probably don't have much of a 'standard' layout or structure. This might be why a fairly simple BoW model does really well compared to TF-IDF or higher gram models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_bow.fit(train_vectors, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsample_submission[\"target\"] = NB_bow.predict(test_vectors)\n\nimport os\nos.chdir('/kaggle/working')\n    \nsample_submission.to_csv(\"submission1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Helper Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Encoding function takes the text column from train or test dataframe, the tokenizer,\n# and the maximum length of text string as input.\n\n# Outputs:\n# Tokens\n# Pad masks - BERT learns by masking certain tokens in each sequence.\n# Segment id\n\ndef bert_encode(texts, tokenizer, max_len = 512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build and compile the model\n\ndef build_model(bert_layer, max_len = 512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and Prepare Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Cleaning** - I've found that a relatively quick and generic data cleaning like I did with the BoW + linear model does not improve the result. The best results seem to be achieved with a painstakingly detailed clean up of train and test text which isn't particularly realistic irl. Some simple data cleaning code in hidden cells below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# def decontracted(phrase):\n#     # specific\n#     phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n#     phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n#     # general\n#     phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n#     phrase = re.sub(r\"\\'re\", \" are\", phrase)\n#     phrase = re.sub(r\"\\'s\", \" is\", phrase)\n#     phrase = re.sub(r\"\\'d\", \" would\", phrase)\n#     phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n#     phrase = re.sub(r\"\\'t\", \" not\", phrase)\n#     phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n#     phrase = re.sub(r\"\\'m\", \" am\", phrase)\n#     return phrase","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import spacy\n# import re\n# nlp = spacy.load('en')\n# def preprocessing(text):\n#   text = text.replace('#','')\n#   text = decontracted(text)\n#   text = re.sub('\\S*@\\S*\\s?','',text)\n#   text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n#   token=[]\n#   result=''\n#   text = re.sub('[^A-z]', ' ',text.lower())\n  \n#   text = nlp(text)\n#   for t in text:\n#     if not t.is_stop and len(t)>2:  \n#       token.append(t.lemma_)\n#   result = ' '.join([i for i in token])\n\n#   return result.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.text = train.text.apply(lambda x : preprocessing(x))\n# test.text = test.text.apply(lambda x : preprocessing(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download BERT architecture\n# BERT-Large uncased: 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters\n\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The BERT model gives a validation score of 0.8345 and a leaderboard score of 0.83542.\n\nSo it improves on the BoW + MNB model but not by an insane amount given how much more complex the BERT model is. This is a bit of a general principle in NLP. Relatively models can give really good results. Deep Learning models do tend to perform better but not always but as much as you might expect.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Predictions and Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = test_pred.round().astype(int)\nimport os\nos.chdir('/kaggle/working')\n    \nsubmission.to_csv(\"submission2.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}