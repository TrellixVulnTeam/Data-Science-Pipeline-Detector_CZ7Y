{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Real or Not? NLP with Disaster Tweets","execution_count":null},{"metadata":{"id":"-reiJc0U2J94"},"cell_type":"markdown","source":"This notebook is for the kaggle [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) competition.\n\nThe work has been done by the Vegeta team. The team members are :\n\n* Sarah Abidli\n* Moez Abid\n* Fedi Bayoudh\n* Wissal Bayoudh \n* Montassar Thabti\n* Mohamed Iheb Bousnina","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"VLlDNRN-2C6s"},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"cBAE1GBz2C6y"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"AR2bPyHL2C63"},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsample=pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"5L8OZ_LA2C67"},"cell_type":"code","source":"train.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__1 is a disaster, while 0 is not a disaster.__","execution_count":null},{"metadata":{"trusted":true,"id":"agDh5XFo2C6_"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"fNtEvbMX2C7C"},"cell_type":"code","source":"# Dividing the dataset according to the target\ntweetsDisaster = train[train['target']==1]['text']\nprint('<Disaster> : ',tweetsDisaster.values[1] )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"bTjTVvlT2C7G"},"cell_type":"code","source":"tweetsNotDisaster = train[train['target']==0]['text']\nprint('<Not a disaster> : ',tweetsNotDisaster.values[1])","execution_count":null,"outputs":[]},{"metadata":{"id":"ZVpUtRYf2C7K"},"cell_type":"markdown","source":"# Data mining and analysis","execution_count":null},{"metadata":{"id":"7sUb4llx2C7M"},"cell_type":"markdown","source":"*   Missing data for train","execution_count":null},{"metadata":{"trusted":true,"id":"AsXaWKot2C7M"},"cell_type":"code","source":"data_info=pd.DataFrame(train.dtypes).T.rename(index={0:'column type'})\ndata_info=data_info.append(pd.DataFrame(train.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ndata_info=data_info.append(pd.DataFrame(train.isnull().sum()/train.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(data_info)","execution_count":null,"outputs":[]},{"metadata":{"id":"Z14fNIfz2C7Q"},"cell_type":"markdown","source":"* Missing data for test","execution_count":null},{"metadata":{"trusted":true,"id":"BsmmXrua2C7R"},"cell_type":"code","source":"data_info=pd.DataFrame(test.dtypes).T.rename(index={0:'column type'})\ndata_info=data_info.append(pd.DataFrame(test.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ndata_info=data_info.append(pd.DataFrame(test.isnull().sum()/test.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(data_info)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"Ks2CHrql2C7U"},"cell_type":"markdown","source":"**Conclusion :**","execution_count":null},{"metadata":{"id":"tBghr3bZ2C7U"},"cell_type":"markdown","source":"* We have Around 80% missing information in keyword feature\n* We have 34% missing information in location feature.","execution_count":null},{"metadata":{"id":"q4rQfqHE2C7V"},"cell_type":"markdown","source":"### Target variable visualization.","execution_count":null},{"metadata":{"trusted":true,"id":"TSfk-tsw2C7W"},"cell_type":"code","source":"import plotly.express as px\nfig = px.bar(train, x=['Not Real','Real'] , y=train['target'].value_counts(),height=400,template=\"plotly_dark\")\n\nfig.update_layout(\n    autosize=False,\n    width=500,\n    height=500,\n    )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"n4BLEfVJ2C7Z"},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"FT3iGLzN2C7d"},"cell_type":"code","source":"sns.barplot(train['target'].value_counts().index,train['target'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS \nstopwords = set(STOPWORDS) \nstop_word=list(stopwords)+['http','co','https','wa','amp','û','Û','HTTP','HTTPS']\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',stopwords = stop_word,\n                        width=600,\n                        height=400).generate(\" \".join(train[train['target']==1]['text']))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',stopwords = stop_word,\n                        width=600,\n                        height=400).generate(\" \".join(train[train['target']==0]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20],orient='h')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#les pays qui contiennet le plus des menaces ordonnés\nsns.barplot(y=train['location'].value_counts()[:10].index,x=train['location'].value_counts()[:10],\n            orient='h')","execution_count":null,"outputs":[]},{"metadata":{"id":"MB6TW8hW2C7g"},"cell_type":"markdown","source":"# 1- Exploratory Data Analysis of tweets","execution_count":null},{"metadata":{"id":"Pc2awAlw2C7h"},"cell_type":"markdown","source":"* We will begin by doing a basic analysis, for the characters, words and the sentences.","execution_count":null},{"metadata":{"id":"tDo8uvDP2C7h"},"cell_type":"markdown","source":"**1. Character analysis**","execution_count":null},{"metadata":{"trusted":true,"id":"hv9GVkfZ2C7i"},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"U4hVmyMV2C7p"},"cell_type":"markdown","source":"* The distribution looks similar for both classes, 120 to 140 characters in a single tweet seems to be the norm for both classes.","execution_count":null},{"metadata":{"id":"YHiDvaMq2C7r"},"cell_type":"markdown","source":"**2. Average word length in a tweet :**","execution_count":null},{"metadata":{"trusted":true,"id":"bt5p_flN2C7s"},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=train[train['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword=train[train['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","execution_count":null,"outputs":[]},{"metadata":{"id":"wGI3yKCJ2C7x"},"cell_type":"markdown","source":"**3. Common stopwords in tweets :**","execution_count":null},{"metadata":{"trusted":true,"id":"e26U_7BG2C7x"},"cell_type":"code","source":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom collections import  Counter\ncorpus1=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus1:\n    if word in corpus1:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \nx,y=zip(*top)\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{"id":"B9YFPh2n2C76"},"cell_type":"markdown","source":"* The most dominant word is \"THE\".","execution_count":null},{"metadata":{"id":"OrcyI38C2C76"},"cell_type":"markdown","source":"**4. Analyzing punctuations :**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"PXazeafc2C8H"},"cell_type":"markdown","source":"[](http://)","execution_count":null},{"metadata":{"id":"GsJ6n07_2C8M"},"cell_type":"markdown","source":"# 2-  Data Preprocessing","execution_count":null},{"metadata":{"id":"Fv1dz5w12C8N"},"cell_type":"markdown","source":"> * **Removing URL's**","execution_count":null},{"metadata":{"trusted":true,"id":"xuPBhih-2C8O"},"cell_type":"code","source":"import re\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"97U9JbqL2C8R"},"cell_type":"code","source":"def remove_URLs(text):\n    url = re.compile(r'http?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"2GPZ8-Qk2C8W"},"cell_type":"code","source":"train['text'] = train['text'].apply(remove_URL)\ntest['text'] = test['text'].apply(remove_URL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"_G5XEV7y2C8b"},"cell_type":"code","source":"train['text'] = train['text'].apply(remove_URLs)\ntest['text'] = test['text'].apply(remove_URLs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']","execution_count":null,"outputs":[]},{"metadata":{"id":"dgL3f9jx2C8g"},"cell_type":"markdown","source":"> * **Removing emojis**","execution_count":null},{"metadata":{"trusted":true,"id":"qqwJl3mD2C8g"},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"KNjErHBk2C8k"},"cell_type":"code","source":"train['text'] = train['text'].apply(remove_emoji)\ntest['text'] = test['text'].apply(remove_emoji)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']","execution_count":null,"outputs":[]},{"metadata":{"id":"-UH5qF1j2C8m"},"cell_type":"markdown","source":"> * **Removing contractions**","execution_count":null},{"metadata":{"trusted":true,"id":"g4Th9a6a2C8n"},"cell_type":"code","source":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\",\n\"thx\"   : \"thanks\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"BxSk8xmp2C8p"},"cell_type":"code","source":"def remove_contractions(text):\n    return contractions[text.lower()] if text.lower() in contractions.keys() else text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"58ou1skO2C8u"},"cell_type":"code","source":"train['text']=train['text'].apply(remove_contractions)\ntest['text']=test['text'].apply(remove_contractions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']","execution_count":null,"outputs":[]},{"metadata":{"id":"Q9gWzp-J2C8x"},"cell_type":"markdown","source":"> * **Removing Punctuation**","execution_count":null},{"metadata":{"trusted":true,"id":"lKNUKxYq2C80"},"cell_type":"code","source":"import string\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"NZbfw1Gb2C83"},"cell_type":"code","source":"train['text'] = train['text'].apply(remove_punct)\ntest['text'] = test['text'].apply(remove_punct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The most frequent words","execution_count":null},{"metadata":{"trusted":true,"id":"WgOfPKLL2C9I"},"cell_type":"code","source":"freq = pd.Series(' '.join(train['text']).split()).value_counts()[:20]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"id":"tKmi1GRk2C9M"},"cell_type":"markdown","source":"> Uncommon words","execution_count":null},{"metadata":{"trusted":true,"id":"VPBPOqCE2C9N"},"cell_type":"code","source":"freq1 =  pd.Series(' '.join(train \n         ['text']).split()).value_counts()[-20:]\nfreq1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"-uOeXVz52C9T"},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords \nstop_words = set(stopwords.words('english'))\n\nthislist = ['were', 'the', 'amp', 'dont', 'got', 'know', 'gon', 'na', 'wan', 'like', 'im', 'hers', 'why', 'over', \"'d\",'our', 'these', 'nevertheless', 'its', 'them', 'empty', 'how', 'whereas', 'whether', 'fifteen', 'about', 'four', 'give', 'otherwise', 'move', 'do', 'say', '‘ve', 'hence', 'n‘t', 'between', 'bottom', 'some', 'against', 'whole', 'i', 'into', 'they', 'already', 'she', 'either', 'an', 'both', 'him', 'due', 'using', 'five', 'across', 'front', 'in', 'off', 'only', 'really', 'twelve', 'twenty', 'show', 'whereupon', '‘m', 'n’t', 'himself', '’m', 'from', 'often', 'three', 'various', 'thereupon', 'should', 'put', 'take', 'who', 'above', 'their', 'been', 'towards', 'however', \"n't\", 'her', 'go', 'thereby', 'just', 'yourselves', 'become', 'thru', 'while', 'nowhere', 'neither', 'anyway', 'because', 'ca', 'which', 'moreover', 'forty', 'besides', 'us', 'more', 'third', 'wherein', 'whoever', 'used', 'every', 'whose', 'onto', 'your', 'hereafter', 'itself', 'sometimes', 'name', 'too', 'own', 'somewhere', 'there', 'we', 'you', '’ve', 'ourselves', 'sixty', 'would', 'first', 'must', 'whereafter', 'wherever', 'his', 'around', 'has', 'yours', 'became', 'doing','the', 'below', 'then', 'everyone', 'else', 'any', 'latterly', 'noone', 'part', 'might', \"'ve\", 'becoming', 'same', 'top', 'yourself', 'he', 'each', 'anyone', 'my', 'seeming', 'six', 'the', 'during', 'afterwards', 'throughout', 'formerly', 'seem', 'therefore', 'another', 'keep', 'without', 'being', 'can', 'had', 'per', \"'s\", 'other', 'side', '’s', 'also', 'herself', '’ll', 'eight', 'what', 'please', 'a', 'therein', 'back', 'me', 'never', 'not', 'does', 'enough', 'meanwhile', 'toward', 'even', 'get', 'and', 'it', 'perhaps', 'this', 'regarding', 'somehow', 'cannot', 'anyhow', 'through', 'whenever', 'thereafter', 'rather', 'by', 'still', 'where', 'than', 'made', 'of', 'will', 'within', 'are', 'amongst', 'although', 'former', 'full', 'nobody', 'was', 'to', 'is', 'at', 'hundred', 'all', 'on', 'such', 'after', 'almost', 'most', 'no', 'our', 'see', 'thus', 'upon', \"'ll\", 'whence', 'make', '‘s', 'could', 'quite', 'or', 'beyond', 'thence', 'mostly', 'though', 'alone', 'for', 'under', 'seemed', 'until', 'much', 'nine', 'least', 'that', 'nor', 'further', 'themselves', 'whatever', 'whom', 'anywhere', 'myself', 'eleven', 'none', 'with', 'as', 'have', '‘ll', \"'m\", 'up', 'if', 'several', 'whereby', 'now', 'always', 'amount', 'done', 'hereupon', 'others', 'may', 'one', 'everything', 'so', 'hereby', 'anything', 'fifty', 'last', 'am', 'beforehand', 'few', 'ever', 'together', 'unless', 'ten', 'behind', 'when', 'those', 'mine', 'everywhere', 'be', 'less', 'nothing', 'something', 'very', \"'re\", 'here', '‘re', 'since', 'seems', 'down', 'did', 'before', 'serious', '‘d', '’d', 'many', 'call', 'along', 'once', 'herein', 'out', 'namely', 'someone', 'becomes', 'whither', 're', 'two', 'but', 'again', 'elsewhere', 'well', 'next', 'sometime', 'indeed', 'ours', 'yet', '’re', 'via', 'latter', 'except', 'among', 'beside']\nstop_words.update(thislist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    '''Make text lower case , removw text in squre backets ,remove links, remove punctuation\n    and remove words containing numbers\n    '''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]','',text)\n    text = re.sub('https?://\\S+|www\\.\\S+','',text)\n    text = re.sub('<,*?>+','',text)\n    text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n    text = re.sub('\\n','',text)\n    text = re.sub('\\w*\\d\\w*','',text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * **Tokenzing the text**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * **Removing stopwords**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest1=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize,word_tokenize\n\ntweets_train = train1['text'].values\ntarget = train1['target'].values\n\ntweets_test = test1['text'].values\n","execution_count":null,"outputs":[]},{"metadata":{"id":"HSEx7ZeP2C9g"},"cell_type":"markdown","source":"> * **Lemmatizing and stemming the text**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer()\n\nfor i in range(len(tweets_train)):\n    \n    sentences = sent_tokenize(tweets_train[i])\n    \n    word_list = []\n    for sent in sentences:\n        \n        words = word_tokenize(sent)\n        \n        for word in words:\n            if words not in word_list:\n                word_list.append(word)\n                \n    \n    word_list = [lemmatizer.lemmatize(w) for w in word_list if w not in stop_words]\n    \n    tweets_train[i] = ' '.join(w for w in word_list)\n    \nfor i in range(len(tweets_test)):\n    \n    sentences = sent_tokenize(tweets_test[i])\n    \n    word_list = []\n    for sent in sentences:\n        \n        words = word_tokenize(sent)\n        \n        for word in words:\n            \n            if words not in word_list:\n                word_list.append(word)\n                \n    \n    word_list = [lemmatizer.lemmatize(w) for w in word_list if w not in stop_words]\n    \n    tweets_test[i] = ' '.join(w for w in word_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nporter = PorterStemmer()\n\nfor i in range(len(tweets_train)):\n    \n    sentences = sent_tokenize(tweets_train[i])\n    \n    word_list = []\n    for sent in sentences:\n        \n        words = word_tokenize(sent)\n        \n        for word in words:\n            \n            if words not in word_list:\n                word_list.append(word)\n                \n    \n    word_list = [porter.stem(w) for w in word_list if w not in stop_words]\n    \n    tweets_train[i] = ' '.join(w for w in word_list)\n\nfor i in range(len(tweets_test)):\n    \n    sentences = sent_tokenize(tweets_test[i])\n    \n    word_list = []\n    for sent in sentences:\n        \n        words = word_tokenize(sent)\n        \n        for word in words:\n            \n            if words not in word_list:\n                word_list.append(word)\n                \n    \n    word_list = [lemmatizer.lemmatize(w) for w in word_list if w not in stop_words]\n    \n    tweets_test[i] = ' '.join(w for w in word_list)    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"wuSTgiOQ2C-g"},"cell_type":"code","source":"# Combining results\ndef combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3- Modelisation","execution_count":null},{"metadata":{"trusted":true,"id":"lUOkhk4z2C-j"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[0].todense())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"EyrZpIOy2C-t"},"cell_type":"code","source":"# XG Boost\nimport  xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# Sklearn\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import  f1_score\nfrom sklearn import preprocessing ,decomposition, model_selection,metrics,pipeline\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"TkFdfgNa2C-v"},"cell_type":"code","source":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Q2u1g4Lq2C-0"},"cell_type":"code","source":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"naszJYPe2C-3"},"cell_type":"code","source":"clf.fit(train_vectors, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"X8ZJ6Qbq2C-5"},"cell_type":"code","source":"clf.predict(test_vectors[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"tFSdCvUj2C-7"},"cell_type":"code","source":"# Fitting a simple Logistic Regression on TFIDF\nclf_tfidf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf_tfidf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"DpwJWZ842C--"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"qQH4BbFv2C-_"},"cell_type":"code","source":"# Fitting a simple Naive Bayes on Counts\nclf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"PT0tqyRN2C_B"},"cell_type":"code","source":"clf_NB.fit(train_vectors, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"fK07oymW2C_D"},"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"BPk5Myxd2C_F"},"cell_type":"code","source":"clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"bxPim25D2C_M"},"cell_type":"code","source":"# Fitting XGBoost on Counts\nclf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"MsrCuRAM2C_P"},"cell_type":"code","source":"# Fitting XGBoost on TFIDF\nclf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"id":"jFFT4ohm2WWG","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, plot_confusion_matrix\nfrom sklearn.svm import LinearSVC, SVC, NuSVC\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nimport random","execution_count":null,"outputs":[]},{"metadata":{"id":"JLZ2zKHC2abK","trusted":true},"cell_type":"code","source":"X = train['text']  \ny = train['target']\ntest_x = test['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"BfIitiQg2xSX","trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer(stop_words='english')#, min_df = 0.05, max_df = 0.9)\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\ncount_train_sub = count_vectorizer.transform(X)\ncount_sub = count_vectorizer.transform(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxgb_classifier=xgb.XGBClassifier(objective='binary:logistic',learning_rate = 0.1,gamma=0.01,max_depth = 5,booster=\"gbtree\")\nxgb_classifier.fit(count_train ,y_train)\nxgb_pred = xgb_classifier.predict(count_test)\nxgb_score = accuracy_score(y_test,xgb_pred)\nprint('XGBoost Count Score: ',xgb_score)\nxgb_cm = confusion_matrix(y_test, xgb_pred)\nxgb_cm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\ndt_clf = tree.DecisionTreeClassifier()\ndt_clf.fit(count_train ,y_train)\ndt_pred = dt_clf.predict(count_test)\ndt_score = accuracy_score(y_test,dt_pred)\nprint('Decision Tree Count Score: ',dt_score)\ndt_cm = confusion_matrix(y_test, dt_pred)\ndt_cm\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada_clf = AdaBoostClassifier(n_estimators=1000,learning_rate=1,algorithm='SAMME.R')\nada_clf.fit(count_train ,y_train)\nada_pred = ada_clf.predict(count_test)\nada_score = accuracy_score(y_test,ada_pred)\nprint('AdaBoost Decision Tree Count Score: ',ada_score)\nada_cm = confusion_matrix(y_test, ada_pred)\nada_cm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(max_depth=2, random_state=0)\nrf_clf.fit(count_train ,y_train)\nrf_pred = rf_clf.predict(count_test)\nrf_score = accuracy_score(y_test,rf_pred)\nprint('Random Forest Count Score: ',rf_score)\nrf_cm = confusion_matrix(y_test, rf_pred)\nrf_cm","execution_count":null,"outputs":[]},{"metadata":{"id":"d6UOrHGr21wt","outputId":"834b61d2-c0f8-4b4c-c59a-0dd4874e7f15","trusted":true},"cell_type":"code","source":"count_nb = MultinomialNB()\ncount_nb.fit(count_train ,y_train)\ncount_nb_pred = count_nb.predict(count_test)\ncount_nb_score = accuracy_score(y_test,count_nb_pred)\nprint('MultinomialNaiveBayes Count Score: ', count_nb_score)\ncount_nb_cm = confusion_matrix(y_test, count_nb_pred)\ncount_nb_cm","execution_count":null,"outputs":[]},{"metadata":{"id":"wcNEGd_r22_a","outputId":"1d423b0a-0fec-40e8-f2fe-bbdc6258ebe7","trusted":true},"cell_type":"code","source":"count_bnb = BernoulliNB()\ncount_bnb.fit(count_train ,y_train)\ncount_bnb_pred = count_bnb.predict(count_test)\ncount_bnb_score = accuracy_score(y_test,count_bnb_pred)\nprint('BernoulliNaiveBayes Count Score: ', count_bnb_score)\ncount_bnb_cm = confusion_matrix(y_test, count_bnb_pred)\ncount_bnb_cm","execution_count":null,"outputs":[]},{"metadata":{"id":"xGxzPqdy28OS","outputId":"2d036541-2bc5-45f7-aeff-a83bd5cbb399","trusted":true},"cell_type":"code","source":"count_lsvc = LinearSVC()\ncount_lsvc.fit(count_train ,y_train)\ncount_lsvc_pred = count_lsvc.predict(count_test)\ncount_lsvc_score = accuracy_score(y_test,count_lsvc_pred)\nprint('LinearSVC Count Score: ', count_lsvc_score)\ncount_lsvc_cm = confusion_matrix(y_test, count_lsvc_pred)\ncount_lsvc_cm","execution_count":null,"outputs":[]},{"metadata":{"id":"URUm-k5T2-AJ","outputId":"a25be9b1-8a49-4408-b4de-61c2bab715e0","trusted":true},"cell_type":"code","source":"\ncount_svc = SVC()\ncount_svc.fit(count_train ,y_train)\ncount_svc_pred = count_svc.predict(count_test)\ncount_svc_score = accuracy_score(y_test,count_svc_pred)\nprint('SVC Count Score: ', count_svc_score)\ncount_svc_cm = confusion_matrix(y_test, count_svc_pred)\ncount_svc_cm","execution_count":null,"outputs":[]},{"metadata":{"id":"OFlbVljQ3ALi","outputId":"de6883c5-7b68-418c-dceb-22bec055b4d8","trusted":true},"cell_type":"code","source":"count_nusvc = NuSVC()\ncount_nusvc.fit(count_train ,y_train)\ncount_nusvc_pred = count_nusvc.predict(count_test)\ncount_nusvc_score = accuracy_score(y_test,count_nusvc_pred)\nprint('NuSVC Count Score: ', count_nusvc_score)\ncount_nusvc_cm = confusion_matrix(y_test, count_nusvc_pred)\ncount_nusvc_cm","execution_count":null,"outputs":[]},{"metadata":{"id":"SV5tb3aZ3CEx","outputId":"7ee7e81d-67e9-45a5-8230-38786ec25f48","trusted":true},"cell_type":"code","source":"count_sgd = SGDClassifier()\ncount_sgd.fit(count_train ,y_train)\ncount_sgd_pred = count_sgd.predict(count_test)\ncount_sgd_score = accuracy_score(y_test,count_sgd_pred)\nprint('SGD Count Score: ', count_sgd_score)\ncount_sgd_cm = confusion_matrix(y_test, count_sgd_pred)\ncount_sgd_cm","execution_count":null,"outputs":[]},{"metadata":{"id":"cbZpUI413DfX","outputId":"1bfbae77-4505-4a99-98c4-0ac50b3176b9","trusted":true},"cell_type":"code","source":"count_lr = LogisticRegression()\ncount_lr.fit(count_train ,y_train)\ncount_lr_pred = count_lr.predict(count_test)\ncount_lr_score = accuracy_score(y_test,count_lr_pred)\nprint('LogisticRegression Count Score: ', count_lr_score)\ncount_lr_cm = confusion_matrix(y_test, count_lr_pred)\ncount_lr_cm    ","execution_count":null,"outputs":[]},{"metadata":{"id":"r0iE38xk3G2n","outputId":"af0f3c0c-99f5-44a9-ffbd-60c82a523824","trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Naive Bayes Multinomiale', 'Naive Bayes Bernoulli',\n              'Linear Support Vector Classification', 'Support vector Classification', 'Nu-Support Vector Classification.', \n              'Stochastic Gradient Decent', 'Logistic Regression','XGBoost','AdaBoost'],\n    'Score': [count_nb_score, count_bnb_score, \n              count_lsvc_score, count_svc_score, count_nusvc_score, \n              count_sgd_score, count_lr_score,xgb_score,ada_score]})\nmodels.sort_values(by=\"Score\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"yEBlag2K4aCC","trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc","execution_count":null,"outputs":[]},{"metadata":{"id":"yji-kExw48kN","trusted":true},"cell_type":"code","source":"fpr1, tpr1, threshold1 = roc_curve(y_test, count_nb_pred) \nroc_auc1 = auc(fpr1, tpr1)\nfpr2, tpr2, threshold2 = roc_curve(y_test, count_bnb_pred) \nroc_auc2 = auc(fpr2, tpr2)\nfpr3, tpr3, threshold3 = roc_curve(y_test, count_lsvc_pred)\nroc_auc3 = auc(fpr3, tpr3)\nfpr4, tpr4, threshold4 = roc_curve(y_test, count_svc_pred) \nroc_auc4 = auc(fpr4, tpr4)\nfpr5, tpr5, threshold5 = roc_curve(y_test, count_nusvc_pred) \nroc_auc5 = auc(fpr5, tpr5)\nfpr6, tpr6, threshold6 = roc_curve(y_test, count_sgd_pred)\nroc_auc6 = auc(fpr6, tpr6)\nfpr7, tpr7, threshold7 = roc_curve(y_test, count_lr_pred)\nroc_auc7 = auc(fpr7, tpr7)\nfpr8, tpr8, threshold8 = roc_curve(y_test, xgb_pred)\nroc_auc8 = auc(fpr8, tpr8)\nfpr9, tpr9, threshold9 = roc_curve(y_test, ada_pred)\nroc_auc9 = auc(fpr9, tpr9)","execution_count":null,"outputs":[]},{"metadata":{"id":"NhOpxnKf5bRe","outputId":"2c30269a-fbff-4efe-f163-9777c222b844","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10)) \nplt.plot(fpr1, tpr1, color='red', lw=2, label='Naive Bayes Multinomiale (area = %0.2f)'% roc_auc1)\nplt.plot(fpr2, tpr2, color='green', lw=2, label='Naive Bayes Bernoulli (area = %0.2f)'% roc_auc2)\nplt.plot(fpr3, tpr3, color='cyan', lw=2, label='Linear Support Vector Classification (area = %0.2f)'% roc_auc3)\nplt.plot(fpr4, tpr4, color='magenta', lw=2, label='Support vector Classification (area = %0.2f)'% roc_auc4)\nplt.plot(fpr5, tpr5, color='yellow', lw=2, label='Nu-Support Vector Classification (area = %0.2f)'% roc_auc5)\nplt.plot(fpr6, tpr6, color='black', lw=2, label='Stochastic Gradient Decent (area = %0.2f)'% roc_auc6)\nplt.plot(fpr7, tpr7, color='#770080', lw=2, label='Logistic Regression (area = %0.2f)'% roc_auc7)\nplt.plot(fpr8, tpr8, color='pink', lw=2, label='XGBoost Binary:logistic (area = %0.2f)'% roc_auc8)\nplt.plot(fpr9, tpr9, color='blue', lw=2, label='AdaBoost Decision Tree (area = %0.2f)'% roc_auc9)\n\n\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--') \nplt.xlim([0.0, 1.0]) \nplt.ylim([0.0, 1.05]) \nplt.xlabel('False Positive Rate') \nplt.ylabel('True Positive Rate') \nplt.title('Classifiers ROC curves') \nplt.legend(loc = \"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SUBMISSION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#data pour la submission\nsample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission(model,test_vectors):\n    sample[\"target\"] = model.predict(test_vectors)\n    sample.to_csv(\"submission.csv\", index=False)\n    \ntest_vectors=test_tfidf\nsubmission(clf_NB_TFIDF,test_vectors)\n\nsubFinal=pd.read_csv('submission.csv')\nsubFinal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}