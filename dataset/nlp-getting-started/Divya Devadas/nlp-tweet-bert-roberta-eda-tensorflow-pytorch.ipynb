{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **NLP DISASTER TWEETS**: BERT, RoBERTa, EDA,TensorFlow 2.0, Pytorch ,Plotly VISUALIZATION","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n\n* [Introduction](#section-one)\n* [References](#section-ten)\n* [BERT Model]( #section-two)\n* [Libraries](#section-three)\n* [Loading Data](#section-four)\n* [Exploratory Data Analysis of Tweets](#section-five)\n    - [Show percentage of tweets marked as Disaster tweets](#subsection-1.1)      \n    - [sentence length analysis](#subsection-1.2)      \n    - [Tweet Word count analysis](#subsection-1.3)      \n    - [Tweet stop word analysis](#subsection-1.4)\n    - [Tweet Puntuation and space word analysis](#subsection-1.5)      \n    - [NGRAMS](#subsection-1.6)\n* [Data Cleaning](#section-six)\n    - [Remove URL from the tweet](#subsection-2.1)      \n    - [Remove # from the text](#subsection-2.2)      \n    - [Remove Emoji from the tweet](#subsection-2.3)      \n    - [Remove HTML Tags from Tweet](#subsection-2.4)\n    - [Remove Punctuations from the Tweet](#subsection-2.5)      \n    - [Spell correction in tweets](#subsection-2.6)      \n    - [Remove Stopwords from tweets](#subsection-2.7)      \n    - [Filling Missing values](#subsection-2.8)\n    - [WordCloud Map](#subsection-2.9)\n* [Modeling With BERT](#section-seven)\n    - [Building BERT Model With PyTorch](#subsection-3.1)      \n        - [Tokenization](#subsection-3.1.1)      \n        - [Map Tokens](#subsection-3.1.2)      \n        - [Create a TensorDataSet](#subsection-3.1.3)\n        - [Create Validation Dataset from TrainSet](#subsection-3.1.4)      \n        - [Create BERT Data Loader](#subsection-3.1.5)      \n        - [LOAD BERT Pretrained Sequence Classification](#subsection-3.1.6)      \n        - [Setup AdamW BERT Optimizer](#subsection-3.1.7)      \n        - [Create Learning rate scheduler with warmup](#subsection-3.1.8)\n    - [Training the BERT Model](#subsection-3.2)      \n    - [Validating the BERT Model](#subsection-3.3)      \n    - [Check Accuracy](#subsection-3.4) \n    - [Prediction with BERT Model](#subsection-3.5)     \n* [RoBERTa model with Tensorflow and HuggingFace](#section-eight)      \n    - [Create Tokenizer](#subsection-4.1)      \n    - [Map Token](#subsection-4.2)            \n    - [Build RoBERTa Model](#subsection-4.3)      \n    - [Fit RoBERTa Model](#subsection-4.4)      \n    - [Predict Using RoBERTa Model](#subsection-4.5)      \n    - [Plot the Prediction from RoBERTa](#subsection-4.6)      \n    - [RoBERTa Confusion Matrix](#subsection-4.7)\n* [Conclusion](#section-nine)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" <a id=\"section-one\"></a>\n # Introduction\n\nMy name is Divya Devadas. I’m currently doing my masters in Data Science. As a part of the course we came across many machine learning/ Data science aspects. I believe the Kaggle is the right place to practice with recent technologies in the area of data science and good place to refer all kinds of  data science related projects. And doing projects here will be definitely an achievement towards our carrier path. In this Kernal i have implemented BERT and RoBERTa model for classification and prediction.\n\nAs big data expands into AI and machine learning, its scope is very vast and, for any business data analysis is very important because it helps in decision making as well as provides explanation to important concepts and problems. Also we can find solution of various difficult real life problems. Every day the advantages of Data science is growing exponentially also NLP is a rapid growing field in AI.\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-ten\"></a>\n# References\n•\thttps://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/\n\n•\thttps://towardsdatascience.com/natural-language-processing-nlp-top-10-applications-to-know-b2c80bd428cb\n\n•\thttps://www.researchgate.net/figure/Example-for-tweet-text-preprocessing_fig2_322713146\n\n•\thttps://www.youtube.com/watch?v=FKlPCK1uFrc\n\n•\thttps://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX?fbclid=IwAR3k5k56sXpnbTc0z8AxctuigIZ4XhS7Zk0PTOpv6uqlU0WrXepJzssiM88#scrollTo=cKsH2sU0OCQA\n\n•\thttps://towardsdatascience.com/effectively-pre-processing-the-text-data-part-1-text-cleaning-9ecae119cb3e\n\n•\thttps://blog.camelot-group.com/2019/03/exploratory-data-analysis-an-important-step-in-data-science/\n\n•\thttps://huggingface.co/transformers/main_classes/tokenizer.html\n\n•\thttps://huggingface.co/transformers/main_classes/optimizer_schedules.html\n\n•\thttps://huggingface.co/transformers/model_doc/roberta.html?fbclid=IwAR0q4NiE-b1C8v4-Eusq6QhL3heYpfZa4XJXfpy0rlZkrqn819JNE-g78h4\n\n•\thttps://analyticsindiamag.com/bert-classifier-with-tensorflow-2-0/\n\n•\thttps://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta/notebook\n\n•\thttps://medium.com/analytics-vidhya/bert-in-keras-tensorflow-2-0-using-tfhub-huggingface-81c08c5f81d8\n\n•\thttps://www.infoq.com/news/2019/09/facebook-roberta-nlp/\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" <a id=\"section-two\"></a>\n # BERT Model\n \n## Implemented BERT model to do tokenization, classification and prediction with using transformers. \n \nBidirectional Encoder Representations from Transformers is a technique for NLP pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. Google is leveraging BERT to better understand user searches. [Wikipedia](http://).\nBERT is a “deeply bidirectional” model. Bidirectional means that BERT learns information from both the left and the right side of a token’s context during the training phase.\nThe bidirectionality of a model is important for truly understanding the meaning of a language.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" <a id=\"section-three\"></a>\n# Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install pyspellchecker","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport string # Library for string operations\n\nimport os\n\n# plotly library\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\npio.templates.default = \"plotly_dark\"\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\n\nimport matplotlib.pyplot as plt #Another plotting libraray\n\n# word cloud library\nfrom wordcloud import WordCloud\n\n#Regex library\nimport re\n\n#Spell Checker\n#from spellchecker import SpellChecker \n#spell = SpellChecker()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# Loading Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainDataSet= pd.read_csv('../input/nlp-getting-started/train.csv')\nTestDataSet=pd.read_csv('../input/nlp-getting-started/test.csv')\nTrainDataSet.head(3)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n# Exploratory Data Analysis of Tweets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-1.1\"></a>\n### ***Show percentage of tweets marked as Disaster tweets** ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Grouped_Disaster = TrainDataSet.groupby(['target'])['id'].count().reset_index()\nlabels = ['Disaster','Non-Disaster']\n\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=Grouped_Disaster['id'], hole=.4)])\nfig.update_layout(height=400,title_x=0.5,width=400, title_text='Disaster Tweet Percentage',\n                 annotations=[dict(text='#tweet', x=0.5, y=0.5, font_size=20, showarrow=False)])\n\nplotly.offline.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-1.2\"></a>\n### ***sentence length analysis**\nLoook at the number of characters present in each tweet. This can give us a idea about tweet length in our dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainDataSet['tweetlength'] = TrainDataSet['text'].apply(lambda x:  len(str(x)))\nTrainDataSet.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Top_20_Lengthy_Tweets = TrainDataSet.sort_values('tweetlength',ascending=False)[:20][::-1]\nBottom_20_Lengthy_Tweets = TrainDataSet.sort_values('tweetlength',ascending=True)[:20][::-1]\nTweetlength_Data =TrainDataSet['tweetlength'].describe()\n\n\nfig = make_subplots(\n    rows=3, cols=4,\n    specs=[[None,{\"type\": \"indicator\"},{\"type\": \"indicator\"},{\"type\": \"indicator\"}],\n           [{\"type\": \"bar\" ,\"colspan\": 2},None, {\"type\": \"bar\" ,\"colspan\": 2},None],\n           [{\"type\": \"bar\",\"colspan\": 4}, None,None,None]],\n    subplot_titles=(\"\",\"\",\"\",\"Top 20 Tweets by length\",\"Bottom 20 Tweet by length\")\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Tweetlength_Data[1],\n        title=\"Mean Tweet Length\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Tweetlength_Data[3],\n        title=\"Min Tweet Length\",\n    ),\n    row=1, col=3\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Tweetlength_Data[7],\n        title=\"Max Tweet Length\",\n    ),\n    row=1, col=4\n)\n\n\nfig.add_trace(go.Bar(name='id',text='id', x=list(range(len(Top_20_Lengthy_Tweets))), y=Top_20_Lengthy_Tweets['tweetlength']),\n              row=2, col=1)\n\n\nfig.add_trace(go.Bar(name='id',text='id', x=list(range(len(Bottom_20_Lengthy_Tweets))), y=Bottom_20_Lengthy_Tweets['tweetlength']),\n              row=2, col=3)\nfig.add_trace(go.Bar(name='id',text='id',  x=list(range(len(TrainDataSet.head(500)))), y=TrainDataSet['tweetlength']),\n              row=3, col=1)\n\nfig.update_layout(height=650,width=800,title_x=0.5,title_text=\"Tweets Length Analysis\", showlegend=False)\n\nplotly.offline.iplot(fig)\n\n\n# x=list(range(len(TrainDataSet)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-1.3\"></a>\n### ***Tweet Word count analysis**\nLoook at the number of words present in each tweet. This can give us a idea about tweet word count in our dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainDataSet['wordcount'] = TrainDataSet['text'].apply(lambda x:  len(str(x).split()))\nTrainDataSet.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Top_20_Lengthy_Tweets = TrainDataSet.sort_values('wordcount',ascending=False)[:20][::-1]\nBottom_20_Lengthy_Tweets = TrainDataSet.sort_values('wordcount',ascending=True)[:20][::-1]\nWordlength_Data =TrainDataSet['wordcount'].describe()\n\n\nfig = make_subplots(\n    rows=3, cols=4,\n    specs=[[None,{\"type\": \"indicator\"},{\"type\": \"indicator\"},{\"type\": \"indicator\"}],\n           [{\"type\": \"bar\" ,\"colspan\": 2},None, {\"type\": \"bar\" ,\"colspan\": 2},None],\n           [{\"type\": \"bar\",\"colspan\": 4}, None,None,None]],\n    subplot_titles=(\"\",\"\",\"\",\"Top 20 Tweets word count\",\"Bottom 20 Tweet by word count\")\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Wordlength_Data[1],\n        title=\"Mean Tweet word count\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Wordlength_Data[3],\n        title=\"Min Tweet word count\",\n    ),\n    row=1, col=3\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Wordlength_Data[7],\n        title=\"Max Tweet word count\",\n    ),\n    row=1, col=4\n)\n\n\nfig.add_trace(go.Bar(name='id',text='id', x=Top_20_Lengthy_Tweets['keyword'], y=Top_20_Lengthy_Tweets['wordcount']),\n              row=2, col=1)\n\n\nfig.add_trace(go.Bar(name='id',text='id', x=Bottom_20_Lengthy_Tweets['keyword'], y=Bottom_20_Lengthy_Tweets['wordcount']),\n              row=2, col=3)\nfig.add_trace(go.Bar(name='id',text='id', x=TrainDataSet['keyword'], y=TrainDataSet['wordcount']),\n              row=3, col=1)\n\nfig.update_layout(height=600,width=800,title_x=0.5, title_text=\"Tweets Word Count Analysis\", showlegend=False)\n\nplotly.offline.iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-1.4\"></a>\n### * **Tweet stop word analysis**\nLoook at the number of stop words present in each tweet. This can give us a idea about stop word usage in tweet.Stopwords are most commonly used in any language such as “the”,” a”,” an” etc. nltk library help us to find commonly used stopwords. Nltk contains stopwords for many languages so need to filter the English stopwords from the collection.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\nwordcollection=[]\nTempTextCol= TrainDataSet['text'].str.split()\nTempTextCol=TempTextCol.values.tolist()\nwordcollection=[word for i in TempTextCol for word in i]\n\nfrom collections import defaultdict\nstopwprddic=defaultdict(int)\nfor word in wordcollection:\n    if word in stop:\n        stopwprddic[word]+=1\n\nstopwprddf =  pd.DataFrame(stopwprddic.items(), columns=['word', 'count'])\n\nstopwprddf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Top_20_Lengthy_Tweets = stopwprddf.sort_values('count',ascending=False)[:20][::-1]\nBottom_20_Lengthy_Tweets = stopwprddf.sort_values('count',ascending=True)[:20][::-1]\nWordlength_Data =stopwprddf['count'].describe()\n\n\nfig = make_subplots(\n    rows=3, cols=4,\n    specs=[[None,{\"type\": \"indicator\"},{\"type\": \"indicator\"},{\"type\": \"indicator\"}],\n           [{\"type\": \"bar\" ,\"colspan\": 2},None, {\"type\": \"bar\" ,\"colspan\": 2},None],\n           [{\"type\": \"bar\",\"colspan\": 4}, None,None,None]],\n    subplot_titles=(\"\",\"\",\"\",\"Top 20 Tweets stopword count\",\"Bottom 20 Tweet by stopword count\")\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Wordlength_Data[1],\n        title=\"Mean stopwords\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Wordlength_Data[3],\n        title=\"Min stopwords\",\n    ),\n    row=1, col=3\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Wordlength_Data[7],\n        title=\"Max Tweet stopwords\",\n    ),\n    row=1, col=4\n)\n\n\nfig.add_trace(go.Bar(name='count',text='count', x=Top_20_Lengthy_Tweets['word'], y=Top_20_Lengthy_Tweets['count']),\n              row=2, col=1)\n\n\nfig.add_trace(go.Bar(name='count',text='count', x=Bottom_20_Lengthy_Tweets['word'], y=Bottom_20_Lengthy_Tweets['count']),\n              row=2, col=3)\nfig.add_trace(go.Bar(name='count',text='count', x=stopwprddf['word'], y=stopwprddf['count']),\n              row=3, col=1)\n\nfig.update_layout(height=500,width=800,title_text=\"Tweets StopWord Analysis\", title_x=0.5, showlegend=False)\n\nplotly.offline.iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-1.5\"></a>\n### * **Tweet Puntuation and space word analysis**\nLoook at the number of Puntuation/space words present in each tweet. This can give us a idea about Puntuation/space usage in tweet.Punctuations and space words also commonly used in any language. **spacy** library help us to find commonly used Puntuation/space words.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp = spacy.load('en')\n\nfrom collections import defaultdict\ndicspy=defaultdict(int)\n\n\ndocs = TrainDataSet['text'].tolist()\n\ndef token_filter(token):\n    return (token.is_punct | token.is_space )\n\nfiltered_tokens = []\nfor doc in nlp.pipe(docs):\n    tokens = [token.lemma_ for token in doc if token_filter(token)]\n    filtered_tokens.append(tokens)\n    for tk in tokens:\n        dicspy[tk]+=1\n\nPunctuationdf =  pd.DataFrame(dicspy.items(), columns=['word', 'count'])\n\nPunctuationdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Top_20_Lengthy_Tweets = Punctuationdf.sort_values('count',ascending=False)[:20][::-1]\nBottom_20_Lengthy_Tweets = Punctuationdf.sort_values('count',ascending=True)[:20][::-1]\nWordlength_Data =Punctuationdf['count'].describe()\n\n\nfig = make_subplots(\n    rows=3, cols=4,\n    specs=[[None,{\"type\": \"indicator\"},{\"type\": \"indicator\"},{\"type\": \"indicator\"}],\n           [{\"type\": \"bar\" ,\"colspan\": 2},None, {\"type\": \"bar\" ,\"colspan\": 2},None],\n           [{\"type\": \"bar\",\"colspan\": 4}, None,None,None]],\n    subplot_titles=(\"\",\"\",\"\",\"Top 20 Tweets Puntuation or Space count\",\"Bottom 20 Tweet by Puntuation or Space count\")\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Wordlength_Data[1],\n        title=\"Mean \",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Wordlength_Data[3],\n        title=\"Min\",\n    ),\n    row=1, col=3\n)\n\nfig.add_trace(\n    go.Indicator(\n        mode=\"number\",\n        value=Wordlength_Data[7],\n        title=\"Max\",\n    ),\n    row=1, col=4\n)\n\n\nfig.add_trace(go.Bar(name='count',text='count', x=Top_20_Lengthy_Tweets['word'], y=Top_20_Lengthy_Tweets['count']),\n              row=2, col=1)\n\n\nfig.add_trace(go.Bar(name='count',text='count', x=Bottom_20_Lengthy_Tweets['word'], y=Bottom_20_Lengthy_Tweets['count']),\n              row=2, col=3)\nfig.add_trace(go.Bar(name='count',text='count', x=Punctuationdf['word'], y=Punctuationdf['count']),\n              row=3, col=1)\n\nfig.update_layout(height=500,width=800,title_text=\"Tweets Punctuation and Space Analysis\", title_x=0.5, showlegend=False)\n\nplotly.offline.iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef getNgram(wordCollection, n=None):\n    vectorData = CountVectorizer(ngram_range=(n, n)).fit(wordCollection)\n    BagOfWords = vectorData.transform(wordCollection)\n    SumWords = BagOfWords.sum(axis=0) \n    WordsFq = [(word, SumWords[0, idx]) \n                  for word, idx in vectorData.vocabulary_.items()]\n    WordsFq =sorted(WordsFq, key = lambda x: x[1], reverse=True)\n    return WordsFq[:10]\n\ngetBigrams=getNgram(TrainDataSet['text'],2)[:10]\nx,y=map(list,zip(*getBigrams))\n\nimport plotly.express as px\nfig = px.bar(x=y,y=x)\nfig.update_layout(height=500,width=600,title_text=\"Ngram Analysis\", title_x=0.5, showlegend=False)\nplotly.offline.iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"triGrams=getNgram(TrainDataSet['text'],n=3)\nx,y=map(list,zip(*triGrams))\nfig = px.bar(x=y,y=x)\nfig.update_layout(height=500,width=600,title_text=\"TriGram Analysis\", title_x=0.5, showlegend=False)\nplotly.offline.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n# Data Cleaning\n\nWe need to clean the data to avoid errors and incorrect results\n\n### Data Cleaning Tasks\n\n\nRemove Url\n\nRemove # from the text\n\nRemove emoji's\n\nRemove HTML Tags\n\nRemove Punctuations\n\nspell corrections\n\nRemove stopwords \n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainDataSet['text'] = TrainDataSet['text'].apply(lambda x: re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', str(x)))\nTestDataSet['text'] = TestDataSet['text'].apply(lambda x: re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', str(x)))\nTrainDataSet.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2.1\"></a>\n### ***Remove URL from the tweet**\nURL'S some error during processing so we are using regex library to remove the urls","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nTrainDataSet['text'] = TrainDataSet['text'].apply(lambda x: re.sub(r'#', '', str(x)))\nTestDataSet['text'] = TestDataSet['text'].apply(lambda x: re.sub(r'#', '', str(x)))\nTrainDataSet.head(3)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2.2\"></a>\n### ***Remove # from the text**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nTrainDataSet['text'] = TrainDataSet['text'].apply(lambda x: re.sub(r'#', '', str(x)))\nTestDataSet['text'] = TestDataSet['text'].apply(lambda x: re.sub(r'#', '', str(x)))\nTrainDataSet.head(3)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Remove Emoji from the tweet**\nwe need to remove emoji from the tweet since people are using lot of emojies in there tweet to express emotions. We need to create function so we can specify different emoji patterns with range of unicode characters, the list is not complete but good for now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def EmojiCleanser(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TestDataSet['text'] = TestDataSet['text'].apply(lambda x: EmojiCleanser(str(x)))\nTrainDataSet['text'] = TrainDataSet['text'].apply(lambda x: EmojiCleanser(str(x)))\nTrainDataSet.tail(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2.4\"></a>\n### ***Remove HTML Tags from Tweet**\n\nwe need to remove html tags so we can avoid creating unncessary tokens. we can use a regex expression to remove those tags.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TestDataSet['text'] = TestDataSet['text'].apply(lambda x: re.sub(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', str(x)))\nTrainDataSet['text'] = TrainDataSet['text'].apply(lambda x: re.sub(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', str(x)))\nTrainDataSet.tail(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2.5\"></a>\n### ***Remove Punctuations from the Tweet**\nwe need to remove the puntuations from tweet so we are using string libraray to remove the punctuations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TestDataSet['text'] = TestDataSet['text'].apply(lambda x: str(x).translate(str.maketrans('','',string.punctuation)))\nTrainDataSet['text'] = TrainDataSet['text'].apply(lambda x: str(x).translate(str.maketrans('','',string.punctuation)))\nTrainDataSet.tail(3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2.6\"></a>\n### *** Spell correction in tweets**\nWe need to correct the spelling in tweets so we will get more accurate tokens. We can use SpellChecker in pyspellchecker library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#TestDataSet['text'] = TestDataSet['text'].apply(lambda x: \" \".join([spell.correction(i) for i in str(x).split()]))\n#TrainDataSet['text'] = TrainDataSet['text'].apply(lambda x: \" \".join([spell.correction(i) for i in str(x).split()]))\n#TrainDataSet.tail(3)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2.7\"></a>\n### * **Remove Stopwords from tweets**\nWe need to remove Stopwords in tweets so we will get more accurate tokens. We can use stopwords in nltk library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ndef StopWordCleanser(word):\n    if word in stop:\n        return \"\"\n    else:\n        return word\n\n\nTestDataSet['text'] = TestDataSet['text'].apply(lambda x: \" \".join([StopWordCleanser(i) for i in str(x).split()]))\nTrainDataSet['text'] = TrainDataSet['text'].apply(lambda x: \" \".join([StopWordCleanser(i) for i in str(x).split()]))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainDataSet.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nTrainDataSet = TrainDataSet[TrainDataSet.keyword.notnull()]\nTrainDataSet = TrainDataSet[TrainDataSet.location.notnull()]\n\n\nGrouped_Disaster = TrainDataSet.groupby(['keyword'])['id'].count().reset_index()\nGrouped_Location = TrainDataSet.groupby(['location'])['id'].count().reset_index()\n\nGrouped_Disaster = Grouped_Disaster.query('keyword !=\"Not Identified\"' )\nGrouped_Location = Grouped_Location.query('location !=\"Not Location\"' )\n\nGroup_Disaster_filter = Grouped_Disaster.sort_values('id',ascending=False)[:20][::-1]\nGrouped_Location_filter = Grouped_Location.sort_values('id',ascending=False)[:20][::-1]\n\nfig = make_subplots(\n    rows=1, cols=2,\n    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]],\n    subplot_titles=(\"Top 20 Disaster by Tweets\",\"Top 20 Tweet Location\")\n)\n\nfig.add_trace(go.Bar(name='id',text='id', x=Group_Disaster_filter['keyword'], y=Group_Disaster_filter['id']),\n              row=1, col=1)\n\n\nfig.add_trace(go.Bar(name='id',text='id', x=Grouped_Location_filter['location'], y=Grouped_Location_filter['id']),\n              row=1, col=2)\n\nfig.update_layout(height=500,width=600,title_text=\"Tweets Breakdown\", title_x=0.5, showlegend=False)\nplotly.offline.iplot(fig)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-2.9\"></a>\n## WordCloud Map","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = TrainDataSet.text.values\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"section-seven\"></a>\n# Modeling With BERT\n\nBidirectional Encoder Representations from Transformers is a technique for NLP pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google.\n\nBERT is a model with absolute position embeddings.\n\nBERT is trained with a masked language modeling (MLM) objective (Efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation). Models trained with a causal language modeling (CLM) objective are better in that regard.\n\nBERT is trained using a next sentence prediction (NSP) objective using the [CLS] token as a sequence approximate.\n\n\n![](https://miro.medium.com/max/1750/0*XSr4xRO5nhVpjuos.png)\n\n[Image source](https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Available Frame works (TensorFlow - Pytorch - Keras)\n\n* **Tensorflow** Created by Google, Version 2.3 avaialble now https://www.tensorflow.org/api_docs/python/tf.\n\n\n* **PyTorch** Created by Facebook , Version 1.6 availabe https://pytorch.org/\n\n\n* **Keras** (https://keras.io/)High level API to simplify the complexity of deep learning frameworks.Runs on top of other deep learning APIs — TensorFlow, Theano and CNTK\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.1\"></a>\n## BERT - Building Model With PyTorch and HuggingFace\nRef : https://huggingface.co/transformers/model_doc/bert.html ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If there's a GPU available...\n\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.  \n    \n    device = torch.device('cuda')    \n\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.1.1\"></a>\n### * **Tokenization**\n\nBERT expect data as token so our first step is to tokenize the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DisastweetTokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\nDisastweets = pd.concat([TrainDataSet, TestDataSet])\nDisastweets = Disastweets.text.values\n\nprint('Tokenized: ', DisastweetTokenizer.tokenize(Disastweets[0]))\nprint('Token IDs: ', DisastweetTokenizer.convert_tokens_to_ids(DisastweetTokenizer.tokenize(Disastweets[0])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.1.2\"></a>\n### * **MAP Tokens**\n\n![](http://www.mccormickml.com/assets/BERT/CLS_token_500x606.png)\n\n[Image Source](http://www.mccormickml.com/assets/BERT/CLS_token_500x606.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def MapTokens(tweet,labs='None'):\n    \n    \"\"\"A function for tokenize all of the sentences and map the tokens to their word IDs.\"\"\"\n    \n    global labels\n    \n    Tokenids = []\n    Textmasks = []\n\n    # For every sentence...\n    \n    for text in tweet:\n        #   \"encode_plus\" will:\n        \n        #   (1) Tokenize the sentence.\n        #   (2) Prepend the `[CLS]` token to the start.\n        #   (3) Append the `[SEP]` token to the end.\n        #   (4) Map tokens to their IDs.\n        #   (5) Pad or truncate the sentence to `max_length`\n        #   (6) Create attention masks for [PAD] tokens.\n        \n        encoded_dict = DisastweetTokenizer.encode_plus(\n                            text,                      # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            truncation='longest_first', # Activate and control truncation\n                            max_length = 84,           # Max length according to our text data.\n                            pad_to_max_length = True, # Pad & truncate all sentences.\n                            return_attention_mask = True,   # Construct attn. masks.\n                            return_tensors = 'pt',     # Return pytorch tensors.\n                       )\n\n        # Add the encoded sentence to the id list. \n        \n        Tokenids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        \n        Textmasks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists into tensors.\n    \n    Tokenids = torch.cat(Tokenids, dim=0)\n    Textmasks = torch.cat(Textmasks, dim=0)\n    if labs != 'None': \n        labels = torch.tensor(labs)\n    \n    return Tokenids,Textmasks,labels\n        \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_Tokenids, Train_Masks, Train_Labels = MapTokens(TrainDataSet['text'].values, TrainDataSet['target'].values)\nTest_Tokenids, Test_Masks,Test_Labels = MapTokens(TestDataSet['text'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.1.3\"></a>\n### * **Create a TensorDataSet**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TweetTensorDataset = TensorDataset(Train_Tokenids, Train_Masks, Train_Labels)\nPredict_TweetTensorDataset = TensorDataset(Test_Tokenids, Test_Masks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.1.4\"></a>\n### * **Create Validation Dataset from TrainSet**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Train_TweetTensorDataset, Validation_TweetTensorDataset = random_split(TweetTensorDataset, [int(0.8 * len(TweetTensorDataset)), (len(TweetTensorDataset) - (int(0.8 * len(TweetTensorDataset))))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.1.5\"></a>\n### ***Create BERT Data Loader**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\n# Train\nBERT_Train_Loader = DataLoader(\n            TweetTensorDataset,  # Train Data.\n            sampler = RandomSampler(TweetTensorDataset), # Random Batch\n            batch_size = batch_size \n        )\n\n# validation\n\nBERT_Validation_Loader = DataLoader(\n            Validation_TweetTensorDataset, # Validation Data.\n            sampler = SequentialSampler(Validation_TweetTensorDataset), # Sequential Batch.\n            batch_size = batch_size # Evaluate with this batch size.\n        )\n\n# Prediction.\n\nBERT_Predict_Loader = DataLoader(\n            Predict_TweetTensorDataset, # Prediction Data.\n            sampler = SequentialSampler(Predict_TweetTensorDataset), # Sequential Batch.\n            batch_size = batch_size \n        )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.1.6\"></a>\n### ***BERT -LOAD Pretrained Sequence Classification**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\n    'bert-large-uncased', \n    num_labels = 2, # binary classification   \n    output_attentions = False,\n    output_hidden_states = False, \n)\n\n# Tell pytorch to run this model.\n\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.1.7\"></a>\n### ***Setup AdamW BERT Optimizer**\nRefrenec https://huggingface.co/transformers/main_classes/optimizer_schedules.html","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_Tweet_Optimizer = AdamW(model.parameters(),\n                  lr = 6e-6, # args.learning_rate\n                  eps = 1e-8 # args.adam_epsilon\n                )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.1.8\"></a>\n### ***Create Learning rate scheduler with warmup**\n\nWarm up steps is a parameter which is used to lower the learning rate in order to reduce the impact of deviating the model from learning on sudden new data set exposure.By default, number of warm up steps is 0.\n\nTraining steps is number of batches * number of epochs, but not just number of epochs. So, basically num_training_steps = N_EPOCHS+1 is not correct, unless your batch_size is equal to the training set size.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of training epochs. The BERT authors recommend between 2 and 4. \n# We chose to run for 4, but we'll see later that this may be over-fitting the\n# training data.\nepochs = 4\n\nLinear_Scheduler = get_linear_schedule_with_warmup(BERT_Tweet_Optimizer, \n                                            num_warmup_steps = 0, #The number of steps for the warmup phase\n                                            num_training_steps = len(BERT_Train_Loader) * epochs) # The index of the last epoch when resuming training\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.2\"></a>\n## **BERT - Train the Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\nimport numpy as np\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\ntraining_stats = []\nBERT_train_predictions = []\nBERT_true_labels = []\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\nprint(\"Training Started\")\n\nfor epoch_i in range(0, epochs):\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('...')\n    \n     \n    t0 = time.time() # Measure how long the training epoch takes.\n    total_train_loss = 0     # Reset the total loss for this epoch.\n    \n    model.train() # Set the mode and iterate using the dataloader\n    \n    for step, batch in enumerate(BERT_Train_Loader):\n        \n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            \n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(BERT_Train_Loader), elapsed))\n\n        # Read three pytorch tensors (input ids, attention masks, labels) in each Batch Step\n        b_input_ids = batch[0].to(device).to(torch.int64)\n        b_input_mask = batch[1].to(device).to(torch.int64)\n        b_labels = batch[2].to(device).to(torch.int64)\n        \n        model.zero_grad()# Clear any previously calculated gradients        \n        # Evaluate the model on this training batch\n          \n        loss, logits = model(b_input_ids, \n                             token_type_ids=None, \n                             attention_mask=b_input_mask, \n                             labels=b_labels)\n               \n        total_train_loss += loss.item() # Accumulate the training loss  \n        \n        loss.backward()# Perform a backward pass to calculate the gradients.\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip the norm of the gradients to 1.0 -'exploding gradients' problem.\n        \n        BERT_Tweet_Optimizer.step() # Move to Next Step\n        \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        BERT_train_predictions.append(logits)\n        BERT_true_labels.append(label_ids)\n        Linear_Scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    \n        # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(BERT_Train_Loader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n      \n    \n    print(avg_train_loss)\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n        }\n    )\n        \nprint('')\nprint('Training complete!')   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\ndef flat_accuracy(preds, labels):\n    \n    \"\"\"A function for calculating accuracy scores\"\"\"\n    \n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    return accuracy_score(labels_flat, pred_flat)\n\ndef flat_f1(preds, labels):\n    \n    \"\"\"A function for calculating f1 scores\"\"\"\n    \n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    \n    return f1_score(labels_flat, pred_flat)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.3\"></a>\n## BERT -Validate the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_stats = []\n\npredictions = []\ntrue_labels = []\nprint('')\nprint('Start Validation...')\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\nfor epoch_i in range(0, 4):\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print(\"\")\n    t0 = time.time() # Measure how long the training epoch takes.\n    total_train_loss = 0     # Reset the total loss for this epoch.\n    \n    model.eval() # Set evaluation mode\n    \n    # Tracking variables:\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    total_eval_f1 = 0\n    nb_eval_steps = 0\n    \n    for batch in BERT_Validation_Loader:\n        # Calculate elapsed time in minutes.\n        elapsed = format_time(time.time() - t0)\n                    \n        # Report progress.\n        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(BERT_Train_Loader), elapsed))\n\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Disable constructing the compute graph.\n        with torch.no_grad():\n              (loss, logits) = model(b_input_ids, \n                                   token_type_ids=None, \n                                   attention_mask=b_input_mask,\n                                   labels=b_labels)\n            \n        total_eval_loss += loss.item() # Accumulate the validation loss.\n\n        # Move logits and labels to CPU:\n \n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches:\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        total_eval_f1 += flat_f1(logits, label_ids)\n    \n       \n    validation_time = format_time(time.time() - t0)  # Measure how long the validation run took.\n    avg_val_accuracy = total_eval_accuracy / len(BERT_Validation_Loader)\n    print(\"Accuracy : \" )\n    print(avg_val_accuracy)\n    avg_val_f1 = total_eval_f1 / len(BERT_Validation_Loader)  \n    print(\"F1 score : \" )\n    print(avg_val_f1)\n    avg_val_loss = total_eval_loss / len(BERT_Validation_Loader)\n    print(\"Validation Loss:\" )\n    print(avg_val_loss)\n    \n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    predictions.append(logits)\n    true_labels.append(label_ids)\n    validation_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Val_F1' : avg_val_f1,\n            'Validation Time': validation_time\n        }\n    )\n\nprint('')\nprint('Validation complete!')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.4\"></a>\n## BERT - Check Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import matthews_corrcoef\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nmatthews_set = []\n\n# Evaluate each test batch using Matthew's correlation coefficient\nprint('Calculating Matthews Corr. Coef. for each batch...')\n\n# For each input batch...\nfor i in range(len(true_labels)):\n    # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n    # and one column for \"1\"). Pick the label with the highest value and turn this\n    # in to a list of 0s and 1s.\n    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n    # Calculate and store the coef for this batch.\n    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n    matthews_set.append(matthews)\n    \n# Create a barplot showing the MCC score for each batch of test samples.\nax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n\nplt.title('MCC Score per Batch')\nplt.ylabel('MCC Score (-1 to +1)')\nplt.xlabel('Batch #')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import matthews_corrcoef\nmatthews_set = []\nfor i in range(len(true_labels)):\n  matthews = matthews_corrcoef(true_labels[i],\n                 np.argmax(predictions[i], axis=1).flatten())\n  matthews_set.append(matthews)\n  \n# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\nflat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\nflat_true_labels = [item for sublist in true_labels for item in sublist]\n\nprint('Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(matthews_corrcoef(flat_true_labels, flat_predictions)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-3.5\"></a>\n## Prediction with BERT Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('Starting Prediction.')\n# Put model in evaluation mode:\n\nmodel.eval()\n\n# Tracking variables :\n\npredictions = []\n\n# Predict:\n\nfor batch in BERT_Predict_Loader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask = batch\n    with torch.no_grad():\n        outputs = model(b_input_ids, token_type_ids=None, \n                      attention_mask=b_input_mask)\n\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    predictions.append(logits)\n\nprint('DONE.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\nsubmission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsubmission['target'] = flat_predictions\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Grouped_Disaster = submission.groupby(['target'])['id'].count().reset_index()\nlabels = ['Disaster','Non-Disaster']\n\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=Grouped_Disaster['id'], hole=.4)])\nfig.update_layout(width=600, height=400,title_text='BERT- Predicted Disaster Tweet Percentage',\n                 annotations=[dict(text='#tweet', x=0.5, y=0.5, font_size=20, showarrow=False)])\nplotly.offline.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Save the submission**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-4.7\"></a>\n## BERT Confusion Matrix\n\nReference - https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Starting Prediction.')\n# Put model in evaluation mode:\n\nmodel.eval()\n\n# Tracking variables :\n\npredictions = []\n\n# Predict:\n\nfor batch in BERT_Train_Loader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n    with torch.no_grad():\n        outputs = model(b_input_ids, token_type_ids=None, \n                      attention_mask=b_input_mask)\n\n    logits = outputs[0]\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    predictions.append(logits)\n\nprint('DONE.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"flat_predictions = [item for sublist in predictions for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport seaborn as sns\ndef plot_cm(y_true, y_pred, title, figsize=(5,5)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n    \nplot_cm(flat_predictions, TrainDataSet['target'].values, 'Confusion matrix for Roberta model', figsize=(7,7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-eight\"></a>\n# RoBERTa model with Tensorflow and HuggingFace\nRobustly optimised BERT is an optimised method for pretraining NLP systems, built on BERT’s language-masking strategy. The model is claimed to have surpassed the BERT-large as well as XLNet-large models in performance.\n\n**RoBERTa performs better than BERT By**\n\n* Bigger training data.\n\n* Using dynamic masking pattern.\n\n* Replacing the next sentence prediction training objective.\n\n* Training on longer sequences.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom transformers import TFRobertaModel, RobertaTokenizer, TFRobertaForSequenceClassification\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-4.1\"></a>\n### ***RoBERTa - Create Tokenizer**\n\nBERT:       [CLS] + tokens + [SEP] + padding\n\n\nRoBERTa:    [CLS] + prefix_space + tokens + [SEP] + padding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Roberta_Tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nDisastweets = pd.concat([TrainDataSet, TestDataSet])\nDisastweets = Disastweets.text.values\n\nprint('Tokenized: ', Roberta_Tokenizer.tokenize(Disastweets[0]))\nprint('Token IDs: ', Roberta_Tokenizer.convert_tokens_to_ids(Roberta_Tokenizer.tokenize(Disastweets[0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 0\n\n# For every sentence...\nfor text in Disastweets:\n\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n    input_ids = Roberta_Tokenizer.tokenize(text,is_pretokenized=True)\n\n    # Update the maximum sentence length.\n    max_len = max(max_len, len(input_ids))\n\nprint('Max sentence length: ', max_len)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-4.2\"></a>\n### * ** RoBERTa - MapToken**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\ndef MapTokens(texts):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = Roberta_Tokenizer.tokenize(text,is_pretokenized=True)\n        CLS = Roberta_Tokenizer.cls_token\n        SEP = Roberta_Tokenizer.sep_token    \n        text = text[:max_len-2]\n        input_sequence = [CLS] + text + [SEP]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = Roberta_Tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input = MapTokens(TrainDataSet.text.values)\ntest_input = MapTokens(TestDataSet.text.values)\ntrain_labels = TrainDataSet.target.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-4.3\"></a>\n## RoBERTa - Build Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ref : https://github.com/huggingface/transformers/issues/1350\n    \nclass _TFRobertaForSequenceClassification(TFRobertaForSequenceClassification):\n    \n    def __init__(self, config, *inputs, **kwargs):\n        super(_TFRobertaForSequenceClassification, self).__init__(config, *inputs, **kwargs)\n        self.roberta.call = tf.function(self.roberta.call)\n\n\n# Load model and collect encodings\nroberta = _TFRobertaForSequenceClassification.from_pretrained('roberta-large',num_labels=2)\n\n#Optimizer\nRoberta_Tweet_Optimizer = Adam(\n                  lr = 6e-6, # args.learning_rate\n                  epsilon = 1e-8 # args.adam_epsilon\n                )\n#Compile the Model\nroberta.compile(optimizer=Roberta_Tweet_Optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n#roberta.get_layer('predictions').activation=tf.compat.v1.keras.activations.linear\nroberta.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Roberta_Tweet_Optimizer = Adam(\n                  lr = 6e-6, # args.learning_rate\n                  epsilon = 1e-8 # args.adam_epsilon\n                )\nroberta.compile(optimizer=Roberta_Tweet_Optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n#roberta.get_layer('predictions').activation=tf.compat.v1.keras.activations.linear\nroberta.summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-4.4\"></a>\n## RoBERTa - Fit Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('model_roberta.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = roberta.fit(\n    train_input, TrainDataSet['target'].values,\n    validation_split = 0.2,\n    epochs = 4, # recomended 3-5 epochs\n    callbacks=[checkpoint],\n    batch_size = 32\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-4.5\"></a>\n## RoBERTa - Predict Using Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"roberta.load_weights('model_roberta.h5')\ntest_pred_roberta = roberta.predict(test_input)\nflat_predictions = [item for sublist in test_pred_roberta for item in sublist]\nflat_predictions = np.argmax(flat_predictions, axis=1).flatten()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-4.6\"></a>\n## RoBERTa - Plot the Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pd.DataFrame(flat_predictions, columns=['target'])\n#pred.plot.hist()\n\nvalues = [len(pred.query(\"target == 1\")), len(pred.query(\"target == 0\"))]\nlabels = ['Disaster','Non-Disaster']\n\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\nfig.update_layout(width=600, height=400,title_text='RoBERTa - Predicted Disaster Tweet Percentage',\n                 annotations=[dict(text='#tweet', x=0.5, y=0.5, font_size=20, showarrow=False)])\nplotly.offline.iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Create Prediction Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TestPrediction = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nTestPrediction['target'] = flat_predictions\nTestPrediction.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-4.7\"></a>\n## RoBERTa Confusion Matrix\n\nReference - https://www.kaggle.com/marcovasquez/basic-nlp-with-tensorflow-and-wordcloud**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* **Generate Prediction for training Data**\nUse the training input data and create a prediction so we can compare with actual training data targets using the confusion matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred_roberta = roberta.predict(train_input)\n\ntrain_predictions = [item for sublist in train_pred_roberta for item in sublist]\ntrain_predictions = np.argmax(train_predictions, axis=1).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nimport seaborn as sns\ndef plot_cm(y_true, y_pred, title, figsize=(5,5)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n\nplot_cm(train_predictions, TrainDataSet['target'].values, 'Confusion matrix for Roberta model', figsize=(7,7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Create the submission file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TestPrediction.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}