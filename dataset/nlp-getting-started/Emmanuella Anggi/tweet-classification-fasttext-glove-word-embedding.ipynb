{"cells":[{"metadata":{"id":"QyIHY0DU64yn"},"cell_type":"markdown","source":"# **Sentiment Analysis Using LSTM & CNN with fastText (and Gloe Word-Embedding!**","execution_count":null},{"metadata":{"id":"r_41kyTE7IWN"},"cell_type":"markdown","source":"My first notebook with kaggle dataset!\n\nThis time i'm trying to apply the method of word-embedding that i used on one of my paper in uni: fastText. i tried it to classify this interesting dataset from kaggle. The dataset consisted of tweets and classifies whether a tweet is using a disaster words as to inform a real disaster or merely just use it metaphorically.\n\nThis was tested/built with Google Colab, so it'll work fine and just the same if you try it on Colab.\n\nthank you to https://www.kaggle.com/vsmolyakov/keras-cnn-with-fasttext-embeddings!\n\nafter trying on LSTM, i will compare it with CNN. and then to fastText performance, i will also compare it with the performance of golve with lstm and cnn also.","execution_count":null},{"metadata":{"id":"39QHYVX9k0uJ"},"cell_type":"markdown","source":"# **Download Data from kaggle**\n\nFirst, the data. first things first is installing kaggle to my environment i dont have to download the data to my drive/local. It's super efficient.\n\nTo get the API, go to your kaggle profile and download the JSON file!","execution_count":null},{"metadata":{"id":"Jn74l2OE2RLf","trusted":true},"cell_type":"code","source":"#! pip install -q kaggle\n#! mkdir ~/.kaggle\n#! cp kaggle.json ~/.kaggle/\n#! chmod 600 ~/.kaggle/kaggle.json","execution_count":null,"outputs":[]},{"metadata":{"id":"tn_NXygs73AB"},"cell_type":"markdown","source":"download the dataset directly by copying the API command on the dataset page","execution_count":null},{"metadata":{"id":"NQRHwMsD3vF-","outputId":"2e7d2978-d25e-4317-9d32-87cd51e4e9db","trusted":true},"cell_type":"code","source":"! kaggle competitions download -c nlp-getting-started","execution_count":null,"outputs":[]},{"metadata":{"id":"2H7Ynha2lHj7"},"cell_type":"markdown","source":"#**Downloading pre-trained fastText, preparing datasets, and pre-processing**\n\nas i’ve mentioned, i am using fastText and i’m going to download the pre-trained model that fastText offered, directly to my session, and then unzip it to use.","execution_count":null},{"metadata":{"id":"DlC4m7alB44c","trusted":true},"cell_type":"code","source":"import requests, zipfile, io\nzip_file_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\nr = requests.get(zip_file_url)\nz = zipfile.ZipFile(io.BytesIO(r.content))\nz.extractall()","execution_count":null,"outputs":[]},{"metadata":{"id":"JEh0D8DSH1L4","outputId":"2fa2e332-e171-4938-d567-50e53b0f6468","trusted":true},"cell_type":"code","source":"#For Pre-Processing\nfrom tqdm import tqdm\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.tokenize import RegexpTokenizer \nfrom nltk.tokenize import word_tokenize\nimport os, re, csv, math, codecs\n\n\n# For Training\nimport keras\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten\nfrom keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom keras.utils import plot_model\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\n\n# For array, dataset, and visualizing\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"whitegrid\")\nnp.random.seed(0)\n\nMAX_NB_WORDS = 100000\ntokenizer = RegexpTokenizer(r'\\w+')\nstop_words = set(stopwords.words('english'))\nstop_words.update(['.', ',', '\"', \"'\", ':', ';', '(', ')', '[', ']', '{', '}'])","execution_count":null,"outputs":[]},{"metadata":{"id":"OryI3DKE8tr8"},"cell_type":"markdown","source":"\nConverting all the words to index in number, to the embedding index in pre-trained model and converted all the missing words to 0,","execution_count":null},{"metadata":{"id":"IPqS-8FcEN95","outputId":"2041b567-ab48-4a44-e3ce-e085272010d8","trusted":true},"cell_type":"code","source":"print('loading word embeddings...')\n\nembeddings_index = {}\nf = codecs.open('wiki-news-300d-1M.vec', encoding='utf-8')\n\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('found %s word vectors' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"id":"0f453DpY80zi"},"cell_type":"markdown","source":"Read both the train data and test data","execution_count":null},{"metadata":{"id":"gXof0wFo6Q8a","outputId":"d089cc36-71d5-4bf0-bfb5-45f0e88cf2b2","trusted":true},"cell_type":"code","source":"#load data\ntrain_df = pd.read_csv('../input/nlp-getting-started/train.csv', sep=',', header=0)\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv', sep=',', header=0)\ntest_df = test_df.fillna('_NA_')\n\nprint(\"Number of training data \", train_df.shape[0])\nprint(\"Number of testing data: \", test_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"id":"QQARD6HcIGYs","outputId":"4410e36d-48ad-4285-9167-3cd0bfa15109","trusted":true},"cell_type":"code","source":"label_names = [\"target\"]\ny_train = train_df[label_names].values\ntrain_df['doc_len'] = train_df['text'].apply(lambda words: len(words.split(\" \")))\nmax_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n\nsns.distplot(train_df['doc_len'], hist=True, kde=True, color='b', label='doc len')\nplt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\nplt.title('comment length'); plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"a_AOE1b5elSz"},"cell_type":"markdown","source":"tokenizing the data with tokenizer from tensorflow","execution_count":null},{"metadata":{"id":"Cc_9P1cEGg5J","outputId":"052e644a-9ce6-4cfe-98f3-74d81ca57b87","trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle \n\nraw_docs_train = train_df['text'].tolist()\nraw_docs_test = test_df['text'].tolist() \nnum_classes = len(label_names)\n\nprint(\"pre-processing train data...\")\n\nprocessed_docs_train = []\nfor doc in tqdm(raw_docs_train):\n    tokens = word_tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_train.append(\" \".join(filtered))\n#end for\n\nprocessed_docs_test = []\nfor doc in tqdm(raw_docs_test):\n    tokens = word_tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_test.append(\" \".join(filtered))\n#end for\n\nprint(\"tokenizing input data...\")\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\ntokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\nword_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\nword_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\nword_index = tokenizer.word_index\nprint(\"dictionary size: \", len(word_index))\n\n#pad sequences\nword_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\nword_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)","execution_count":null,"outputs":[]},{"metadata":{"id":"IpGENGlOtQs1"},"cell_type":"markdown","source":"#**Shape and Train with LSTM**","execution_count":null},{"metadata":{"id":"Nyj7gwQKexxy"},"cell_type":"markdown","source":"defining variables that used on training","execution_count":null},{"metadata":{"id":"PSZ3krGMG303","trusted":true},"cell_type":"code","source":"#training params\nbatch_size = 256 \nnum_epochs = 40\n\n#model parameters\nnum_filters = 64 \nembed_dim = 300 \nweight_decay = 1e-4","execution_count":null,"outputs":[]},{"metadata":{"id":"fZ6AlbhZe4Ky"},"cell_type":"markdown","source":"building the embedding matrix for the weights in Embedding Layer on training. more about embedding matrix: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n\nall words that aren't in the pre-trained model from fasttext would be changed to 0. the words are basically the ones with typos or names, the words mostly dont matter so much to the pattern. so it's nicer to just weights it 0.","execution_count":null},{"metadata":{"id":"8SJbfuJPJEDC","outputId":"372366a2-bfde-4f06-b30c-ff681c1ab76e","trusted":true},"cell_type":"code","source":"#embedding matrix\n\nprint('preparing embedding matrix...')\n\nwords_not_found = []\nnb_words = min(MAX_NB_WORDS, len(word_index)+1)\nembedding_matrix = np.zeros((nb_words, embed_dim))\n\nfor word, i in word_index.items():\n    if i >= nb_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","execution_count":null,"outputs":[]},{"metadata":{"id":"S_pjm-7BJGuP","outputId":"0a30624f-5503-40db-8719-deabdc82177c","trusted":true},"cell_type":"code","source":"print(\"sample words not found: \", np.random.choice(words_not_found, 10))","execution_count":null,"outputs":[]},{"metadata":{"id":"SUBAkn22ga48"},"cell_type":"markdown","source":"let's start training! here are the layers. i have been doing some test and modification on using the layer, unit cells, etc. and so far it works the best for me.","execution_count":null},{"metadata":{"id":"xpX2AFhKJJsh","outputId":"0919ecd8-6a35-421c-e63a-a2eb179db9d2","trusted":true},"cell_type":"code","source":"from keras.layers import BatchNormalization\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential()\n\nmodel.add(Embedding(nb_words,embed_dim,input_length=max_seq_len, weights=[embedding_matrix],trainable=False))\n\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"tCpYrqbaJUb2","trusted":true},"cell_type":"code","source":"from keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"o9tkbuRaJZ3d","outputId":"cb2f71b7-4aa1-435c-c80e-452e9d852252","trusted":true},"cell_type":"code","source":"es_callback = EarlyStopping(monitor='val_loss', patience=3)\n\nhistory = model.fit(word_seq_train, y_train, batch_size=256,\n          epochs=num_epochs, validation_split=0.3, callbacks=[es_callback], shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"aKSG_B_KtWZ-"},"cell_type":"markdown","source":"#**Evaluation**","execution_count":null},{"metadata":{"id":"kF_jtJSghCUl"},"cell_type":"markdown","source":"evaluation on the training for each epoch","execution_count":null},{"metadata":{"id":"mX6utTzSN2pB","outputId":"41e925c3-dd23-453e-d6b1-096d031577ea","trusted":true},"cell_type":"code","source":"#generate plots\nplt.figure()\nplt.plot(history.history['loss'], lw=2.0, color='b', label='train')\nplt.plot(history.history['val_loss'], lw=2.0, color='r', label='val')\nplt.title('LSTM sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Cross-Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"PM12-5wFN7tw","outputId":"da2f9e44-78bb-4fc0-c129-1091c145730b","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(history.history['accuracy'], lw=2.0, color='b', label='train')\nplt.plot(history.history['val_accuracy'], lw=2.0, color='r', label='val')\nplt.title('LSTM sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"H3yP7nBBN-mn","trusted":true},"cell_type":"code","source":"predictions = model.predict_classes(word_seq_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"zHYhQCI-OKRc","trusted":true},"cell_type":"code","source":"# sample=pd.read_csv('sample_submission.csv')\n# sample['target']= (predictions>0.5).astype(int)\n# sample.to_csv(\"submission.csv\",index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"JdPU8x1odx5u","trusted":true},"cell_type":"code","source":"# sample.head(20)","execution_count":null,"outputs":[]},{"metadata":{"id":"a8BPZ15_eLT-","trusted":true},"cell_type":"code","source":"# i'll try to submit this and know the accuracy if applied to real test\n# !kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"Using LSTM with fastText Word-Embedding\"","execution_count":null,"outputs":[]},{"metadata":{"id":"dmRux7Hjipr4"},"cell_type":"markdown","source":"it's **80%**! not so good but also not that bad, huh?","execution_count":null},{"metadata":{"id":"MOz59iBtVopX"},"cell_type":"markdown","source":"# **LET'S COMPARE!**","execution_count":null},{"metadata":{"id":"jwtHZxlVhXRA"},"cell_type":"markdown","source":"which one is more interesting to compare first? the using of lstm vs cnn? or the fasttext vs glove? let's try to compare the easiest one--for me at least dont judge me!--the model.\n\nnext up i'll show the comparation of fasttext and glove using the model that works better.","execution_count":null},{"metadata":{"id":"u9RfWN0ggR7l","trusted":true},"cell_type":"code","source":"# we don't want the model to overwrite, dont we?\nkeras.backend.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"id":"KOJvL8DDQBPU","outputId":"950c5b4d-0d83-4013-a2fa-d582ca056716","trusted":true},"cell_type":"code","source":"#CNN architecture\nprint(\"training CNN ...\")\nmodel = Sequential()\nmodel.add(Embedding(nb_words, embed_dim,\n          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(MaxPooling1D(2))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"uC0XjvhMjCDN","trusted":true},"cell_type":"code","source":"from keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"kW4_cUTOjCDd","outputId":"e5dbab58-d9d1-4986-867a-a745897b3af4","trusted":true},"cell_type":"code","source":"es_callback = EarlyStopping(monitor='val_loss', patience=3)\n\nhistory = model.fit(word_seq_train, y_train, batch_size=256,\n          epochs=num_epochs, validation_split=0.3, callbacks=[es_callback], shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"KpL3HNHhjCDn"},"cell_type":"markdown","source":"evaluation on the training for each epoch with this model","execution_count":null},{"metadata":{"id":"A394zVlKjCDp","outputId":"d14490a0-3cdf-47ff-c04d-707480a982ce","trusted":true},"cell_type":"code","source":"#generate plots\nplt.figure()\nplt.plot(history.history['loss'], lw=2.0, color='b', label='train')\nplt.plot(history.history['val_loss'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Cross-Entropy Loss')\nplt.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"3zh8IpX4jCDv","outputId":"00dbd76f-a701-4da0-bd75-73979a9c111e","trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(history.history['accuracy'], lw=2.0, color='b', label='train')\nplt.plot(history.history['val_accuracy'], lw=2.0, color='r', label='val')\nplt.title('CNN sentiment')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"HwnpxLgUjCD1","trusted":true},"cell_type":"code","source":"predictions = model.predict_classes(word_seq_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"vwcTkv9_jCD6","trusted":true},"cell_type":"code","source":"# sample=pd.read_csv('sample_submission.csv')\n# sample['target']= (predictions>0.5).astype(int)\n# sample.to_csv(\"submission.csv\",index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"wWpl2IMejCD-","trusted":true},"cell_type":"code","source":"# sample.head(20)","execution_count":null,"outputs":[]},{"metadata":{"id":"QNDxF_nOjCED","outputId":"763ba04e-dfdf-4131-96d7-2564eb032e65","trusted":true},"cell_type":"code","source":"# i'll try to submit this and know the accuracy if applied to real test\n# !kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"Using CNN with fastText Word-Embedding\"","execution_count":null,"outputs":[]},{"metadata":{"id":"dCUdxujRjj9F"},"cell_type":"markdown","source":"it's **79.4%**! but this one was using less epoch. what do you think?","execution_count":null},{"metadata":{"id":"KWByO6lQjxHu"},"cell_type":"markdown","source":"###### **next up is using glove! im excited! with what i promised, i'll use the better performance. with the time and result, i'll just use cnn again!**","execution_count":null},{"metadata":{"id":"b6A9w95FkhOk","trusted":true},"cell_type":"code","source":"import requests, zipfile, io\nzip_file_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\nr = requests.get(zip_file_url)\nz = zipfile.ZipFile(io.BytesIO(r.content))\nz.extractall()","execution_count":null,"outputs":[]},{"metadata":{"id":"4g57WxYskhO8"},"cell_type":"markdown","source":"\nConverting all the words to index in number, to the embedding index in pre-trained model and converted all the missing words to 0,","execution_count":null},{"metadata":{"id":"kRijAxEHkhO9","outputId":"313dac54-324c-4dbc-ed66-b66b6ef9b6b0","trusted":true},"cell_type":"code","source":"print('loading word embeddings...')\n\nembeddings_index = {}\nf = codecs.open('glove.6B.300d.txt', encoding='utf-8')\n\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('found %s word vectors' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"id":"BuYgR5SrkhPR"},"cell_type":"markdown","source":"tokenizing the data with tokenizer from tensorflow","execution_count":null},{"metadata":{"id":"s4iYtJ-akhPR","outputId":"388b60c0-ce0c-4bac-f4bc-9b6bbc5868e0","trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle \n\nraw_docs_train = train_df['text'].tolist()\nraw_docs_test = test_df['text'].tolist() \nnum_classes = len(label_names)\n\nprint(\"pre-processing train data...\")\n\nprocessed_docs_train = []\nfor doc in tqdm(raw_docs_train):\n    tokens = word_tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_train.append(\" \".join(filtered))\n#end for\n\nprocessed_docs_test = []\nfor doc in tqdm(raw_docs_test):\n    tokens = word_tokenize(doc)\n    filtered = [word for word in tokens if word not in stop_words]\n    processed_docs_test.append(\" \".join(filtered))\n#end for\n\nprint(\"tokenizing input data...\")\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\ntokenizer.fit_on_texts(processed_docs_train + processed_docs_test)  #leaky\nword_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\nword_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\nword_index = tokenizer.word_index\nprint(\"dictionary size: \", len(word_index))\n\n#pad sequences\nword_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\nword_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)","execution_count":null,"outputs":[]},{"metadata":{"id":"cRLkGEa9khPY"},"cell_type":"markdown","source":"defining variables that used on training","execution_count":null},{"metadata":{"id":"HMQrAl0vkhPY","trusted":true},"cell_type":"code","source":"#training params\nbatch_size = 256 \nnum_epochs = 40\n\n#model parameters\nnum_filters = 64 \nembed_dim = 300 \nweight_decay = 1e-4","execution_count":null,"outputs":[]},{"metadata":{"id":"Qgzbc9WgkhPb"},"cell_type":"markdown","source":"building the embedding matrix for the weights in Embedding Layer on training. more about embedding matrix: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/","execution_count":null},{"metadata":{"id":"DAWoLJkGkhPd","outputId":"ae9d6c28-e5d5-45a3-c99d-181bedb5dae7","trusted":true},"cell_type":"code","source":"#embedding matrix\n\nprint('preparing embedding matrix...')\n\nwords_not_found = []\nnb_words = min(MAX_NB_WORDS, len(word_index)+1)\nembedding_matrix = np.zeros((nb_words, embed_dim))\n\nfor word, i in word_index.items():\n    if i >= nb_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n    else:\n        words_not_found.append(word)\nprint('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))","execution_count":null,"outputs":[]},{"metadata":{"id":"bX5mIZFljT2S","outputId":"ea313bf2-46c8-4dc9-f802-d3012df3f325","trusted":true},"cell_type":"code","source":"print(\"sample words not found: \", np.random.choice(words_not_found, 10))","execution_count":null,"outputs":[]},{"metadata":{"id":"MwneiTphnQkP"},"cell_type":"markdown","source":"wait.. fastText has more null words?","execution_count":null},{"metadata":{"id":"uyxkTeconZGt","trusted":true},"cell_type":"code","source":"# we don't want the model to overwrite, dont we?\nkeras.backend.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"id":"q717a8qHnZG7","outputId":"0f6dda35-116f-4517-fcea-a92e82247502","trusted":true},"cell_type":"code","source":"#CNN architecture\nprint(\"training CNN ...\")\nmodel = Sequential()\nmodel.add(Embedding(nb_words, embed_dim,\n          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(MaxPooling1D(2))\nmodel.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\nmodel.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"jlFIjhKunZHI","trusted":true},"cell_type":"code","source":"from keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"8nqBct_dnZHN","outputId":"16a9c210-06c7-4e92-f008-3e9325db997d","trusted":true},"cell_type":"code","source":"es_callback = EarlyStopping(monitor='val_loss', patience=3)\n\nhistory = model.fit(word_seq_train, y_train, batch_size=256,\n          epochs=num_epochs, validation_split=0.3, callbacks=[es_callback], shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"mnP6q-uDnkzb","trusted":true},"cell_type":"code","source":"predictions = model.predict_classes(word_seq_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"per9hhklnkzs","trusted":true},"cell_type":"code","source":"# sample=pd.read_csv('sample_submission.csv')\n# sample['target']= (predictions>0.5).astype(int)\n# sample.to_csv(\"submission.csv\",index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"kyN8T9NQnkz2","trusted":true},"cell_type":"code","source":"# sample.head(20)","execution_count":null,"outputs":[]},{"metadata":{"id":"kFhbfoRJnkz8","outputId":"7166f1ab-805b-442c-f147-6b37085b4398","trusted":true},"cell_type":"code","source":"# i'll try to submit this and know the accuracy if applied to real test\n# !kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"Using CNN with Glove Word-Embedding\"","execution_count":null,"outputs":[]},{"metadata":{"id":"GzpJaZRGn6_l"},"cell_type":"markdown","source":"well, the result is **79.5%**! it's the almost the same with cnn using fastText. on training, it shows a higher number on accuracy tho..\n\n###**anyway i'm not satisfied yet so im just gonna tried LSTM using glove.**","execution_count":null},{"metadata":{"id":"-owyLu6mod8x","trusted":true},"cell_type":"code","source":"# we don't want the model to overwrite, dont we?\nkeras.backend.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"id":"QK19UWRroiwW","outputId":"d7787e76-3e26-49aa-bf99-2b7d71a48cab","trusted":true},"cell_type":"code","source":"from keras.layers import BatchNormalization\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential()\n\nmodel.add(Embedding(nb_words,embed_dim,input_length=max_seq_len, weights=[embedding_matrix],trainable=False))\n\nmodel.add(Dropout(0.3))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"TXHUgzURoiwq","trusted":true},"cell_type":"code","source":"from keras.optimizers import RMSprop\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"1IO5V9gioiwy","outputId":"6772a485-babb-4659-b53c-0433007f1ae8","trusted":true},"cell_type":"code","source":"es_callback = EarlyStopping(monitor='val_loss', patience=3)\n\nhistory = model.fit(word_seq_train, y_train, batch_size=256,\n          epochs=num_epochs, validation_split=0.3, callbacks=[es_callback], shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"oA1x6VaYowPQ","trusted":true},"cell_type":"code","source":"predictions = model.predict_classes(word_seq_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"tDiPNbKCowPi","trusted":true},"cell_type":"code","source":"# sample=pd.read_csv('sample_submission.csv')\n# sample['target']= (predictions>0.5).astype(int)\n# sample.to_csv(\"submission.csv\",index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"hOjEQ33OowPr","trusted":true},"cell_type":"code","source":"# sample.head(20)","execution_count":null,"outputs":[]},{"metadata":{"id":"BJTZPYGiowPw","outputId":"6e96e02f-bbaf-4467-c51f-b1e983c11195","trusted":true},"cell_type":"code","source":"# i'll try to submit this and know the accuracy if applied to real test\n# !kaggle competitions submit -c nlp-getting-started -f submission.csv -m \"Using LSTM with Glove Word-Embedding\"","execution_count":null,"outputs":[]},{"metadata":{"id":"NFU9xT6kpgIf"},"cell_type":"markdown","source":"**it's 79.6%!**","execution_count":null},{"metadata":{"id":"sJnyfTY2pvuv"},"cell_type":"markdown","source":"**well, so far from my experiments, fastText and LSTM showed the best performance. but it's still not very sure if it's really like that because validation splitting influence the performance too, also how i build the layers, choosing batch size, optimizer, and stuff.**\n\n**thank you if youre reading the comments too! i hope you have a great day!**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}