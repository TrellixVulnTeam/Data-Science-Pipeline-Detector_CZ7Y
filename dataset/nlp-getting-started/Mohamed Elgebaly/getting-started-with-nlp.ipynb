{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hey Kagglers!! Lets dive into the \"fancy\" NLP world with a simple yet illustrative problem","metadata":{}},{"cell_type":"markdown","source":"**As always ... Lets get our tool ready before we get started!**","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport numpy as np \nimport pandas as pd \nfrom keras.datasets import imdb\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.text import Tokenizer  \nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import models\nimport string\nimport seaborn as sns\nfrom nltk.stem.porter import PorterStemmer\nfrom keras import layers\nfrom keras import losses\nfrom keras import metrics\nimport html\nfrom tensorflow.keras import optimizers\nimport nltk\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport re\nfrom nltk.tokenize.casual import TweetTokenizer\nfrom tensorflow.keras.utils import plot_model\nimport unicodedata","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-12T11:46:07.603883Z","iopub.execute_input":"2022-04-12T11:46:07.604623Z","iopub.status.idle":"2022-04-12T11:46:15.599782Z","shell.execute_reply.started":"2022-04-12T11:46:07.604588Z","shell.execute_reply":"2022-04-12T11:46:15.598868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loading data set","metadata":{}},{"cell_type":"code","source":"train_df=pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df=pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:18:53.30571Z","iopub.execute_input":"2022-04-12T12:18:53.306028Z","iopub.status.idle":"2022-04-12T12:18:53.363288Z","shell.execute_reply.started":"2022-04-12T12:18:53.305981Z","shell.execute_reply":"2022-04-12T12:18:53.362356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:18:53.546085Z","iopub.execute_input":"2022-04-12T12:18:53.546382Z","iopub.status.idle":"2022-04-12T12:18:53.560468Z","shell.execute_reply.started":"2022-04-12T12:18:53.546351Z","shell.execute_reply":"2022-04-12T12:18:53.55962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*As we can see, the text needs alot of preprocessing to remove all unwanted and misleading parts*","metadata":{}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:18:53.949576Z","iopub.execute_input":"2022-04-12T12:18:53.950225Z","iopub.status.idle":"2022-04-12T12:18:53.969529Z","shell.execute_reply.started":"2022-04-12T12:18:53.950177Z","shell.execute_reply":"2022-04-12T12:18:53.968756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For now ... we are going to stick only with the text and ignore the other features**","metadata":{}},{"cell_type":"code","source":"train_df.drop(['id','keyword','location'],axis = 1, inplace = True)\n#we will need the test id column later for results submision \ntest_id = test_df['id']\ntest_df.drop(['id','keyword','location'], axis = 1 , inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:18:54.372616Z","iopub.execute_input":"2022-04-12T12:18:54.373392Z","iopub.status.idle":"2022-04-12T12:18:54.381033Z","shell.execute_reply.started":"2022-04-12T12:18:54.373347Z","shell.execute_reply":"2022-04-12T12:18:54.380298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:18:54.573494Z","iopub.execute_input":"2022-04-12T12:18:54.574377Z","iopub.status.idle":"2022-04-12T12:18:54.584513Z","shell.execute_reply.started":"2022-04-12T12:18:54.574337Z","shell.execute_reply":"2022-04-12T12:18:54.583862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"**Lets take a quick look at the distribution of the target**","metadata":{}},{"cell_type":"code","source":"train_df['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:18:55.259647Z","iopub.execute_input":"2022-04-12T12:18:55.260363Z","iopub.status.idle":"2022-04-12T12:18:55.267976Z","shell.execute_reply.started":"2022-04-12T12:18:55.260317Z","shell.execute_reply":"2022-04-12T12:18:55.267219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = sns.color_palette('Set2')\n\nplt.figure(figsize=(10,10))\nplt.title('Percent of disaster-related tweets')\nexplode = (0, 0.05)\ntrain_df['target'].value_counts().plot.pie(shadow = True,colors = colors,explode = explode,autopct='%.2f%%')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:18:55.546064Z","iopub.execute_input":"2022-04-12T12:18:55.546514Z","iopub.status.idle":"2022-04-12T12:18:55.747126Z","shell.execute_reply.started":"2022-04-12T12:18:55.546478Z","shell.execute_reply":"2022-04-12T12:18:55.74606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Number of characters in tweets","metadata":{}},{"cell_type":"code","source":"tweet = train_df","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:18:56.065031Z","iopub.execute_input":"2022-04-12T12:18:56.065464Z","iopub.status.idle":"2022-04-12T12:18:56.069767Z","shell.execute_reply.started":"2022-04-12T12:18:56.065431Z","shell.execute_reply":"2022-04-12T12:18:56.068587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:18:56.368963Z","iopub.execute_input":"2022-04-12T12:18:56.369292Z","iopub.status.idle":"2022-04-12T12:18:56.727081Z","shell.execute_reply.started":"2022-04-12T12:18:56.369261Z","shell.execute_reply":"2022-04-12T12:18:56.726209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Average word length in a tweet","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet');","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:18:56.960572Z","iopub.execute_input":"2022-04-12T12:18:56.960884Z","iopub.status.idle":"2022-04-12T12:18:58.143557Z","shell.execute_reply.started":"2022-04-12T12:18:56.96085Z","shell.execute_reply":"2022-04-12T12:18:58.142545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Now lets make the necessary preprocessing to get the data ready for modeling","metadata":{}},{"cell_type":"markdown","source":"## Text Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## All preprocessing functions used","metadata":{}},{"cell_type":"code","source":"def remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ').replace('.*', '.').replace('#', '')\n    return re1.sub(' ', html.unescape(x1))\n\ndef to_lowercase(text):\n    return text.lower()\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\ndef sequencing(text):\n    t = TweetTokenizer(preserve_case=False,strip_handles=True, reduce_len=True)\n    seq=t.tokenize(text)\n    return seq\n\ndef remove_punct(seq):\n    words = [w for w in seq if w not in string.punctuation[1:]] #remove all punc excluding exlimnation mark\n    return words\n\ndef stop_words_remove(seq):\n    stop_words = stopwords.words('english')\n    words = [w for w in seq if w not in stop_words]\n    return words\n\n# lemmetization\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\n#join all\ndef list_text(words):\n    return ''.join(words)\n\nURL_remover = lambda x: re.sub(r'http\\S+', '', x)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    text = URL_remover(text)\n    words = sequencing(text)\n    words = remove_punct(words)\n    words = stop_words_remove(words)\n    words=lemmatize_words(words)\n    words=lemmatize_verbs(words)\n    words=list_text(words)\n    return words\n\ndef normalize_corpus(corpus):\n      return [normalize_text(t) for t in corpus]","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:18:59.657723Z","iopub.execute_input":"2022-04-12T12:18:59.658062Z","iopub.status.idle":"2022-04-12T12:18:59.685525Z","shell.execute_reply.started":"2022-04-12T12:18:59.658025Z","shell.execute_reply":"2022-04-12T12:18:59.684482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_train=normalize_corpus(list(train_df.text))\ncorpus_test=normalize_corpus(list(test_df.text))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:19:00.33089Z","iopub.execute_input":"2022-04-12T12:19:00.331404Z","iopub.status.idle":"2022-04-12T12:19:05.831367Z","shell.execute_reply.started":"2022-04-12T12:19:00.331353Z","shell.execute_reply":"2022-04-12T12:19:05.830325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_train[:10]","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:19:21.969939Z","iopub.execute_input":"2022-04-12T12:19:21.970274Z","iopub.status.idle":"2022-04-12T12:19:21.976903Z","shell.execute_reply.started":"2022-04-12T12:19:21.97024Z","shell.execute_reply":"2022-04-12T12:19:21.976044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"markdown","source":"**Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms**","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntok = Tokenizer(oov_token='UNK')\ntexts=corpus_train\ntok.fit_on_texts(texts)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:34:56.517657Z","iopub.execute_input":"2022-04-12T12:34:56.517983Z","iopub.status.idle":"2022-04-12T12:34:56.668379Z","shell.execute_reply.started":"2022-04-12T12:34:56.51795Z","shell.execute_reply":"2022-04-12T12:34:56.667504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#token dictionary\ntok.word_index","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:34:57.063477Z","iopub.execute_input":"2022-04-12T12:34:57.063776Z","iopub.status.idle":"2022-04-12T12:34:57.104044Z","shell.execute_reply.started":"2022-04-12T12:34:57.063746Z","shell.execute_reply":"2022-04-12T12:34:57.103179Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# about 12k unique words and others extracted\nlen(tok.word_index.keys())","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:34:57.59146Z","iopub.execute_input":"2022-04-12T12:34:57.591761Z","iopub.status.idle":"2022-04-12T12:34:57.598417Z","shell.execute_reply.started":"2022-04-12T12:34:57.591729Z","shell.execute_reply":"2022-04-12T12:34:57.597432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Text features*\n### So far, we have tranformed the text into binary/digital form that can be understood by ML models.\n### However, we can further apply or extract different features from the vectorized form.\n#### In other words, we can represent the sequence of word indices we obtained in different forms.","metadata":{}},{"cell_type":"markdown","source":"## BoW with keras tokenizer\n> 3 BoW models for the following features:\n> - Binary       (default, is word present or not)\n> - Count + Freq (count of each word in text)\n> - TF-IDF       (frequency-inverse scoring for each word) ","metadata":{}},{"cell_type":"code","source":"bow = tok.texts_to_matrix(texts[:10], mode='count')\nbow.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:40:43.83431Z","iopub.execute_input":"2022-04-12T12:40:43.835254Z","iopub.status.idle":"2022-04-12T12:40:43.842412Z","shell.execute_reply.started":"2022-04-12T12:40:43.835203Z","shell.execute_reply":"2022-04-12T12:40:43.841574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#simple illustration for bow\nbow","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:40:48.470397Z","iopub.execute_input":"2022-04-12T12:40:48.470714Z","iopub.status.idle":"2022-04-12T12:40:48.477852Z","shell.execute_reply.started":"2022-04-12T12:40:48.470681Z","shell.execute_reply":"2022-04-12T12:40:48.477016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Binary","metadata":{}},{"cell_type":"code","source":"x_binary = tok.texts_to_matrix(texts, mode='binary')\ny_binary = train_df['target']\nx_binary.shape , x_binary.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:59:41.926758Z","iopub.execute_input":"2022-04-12T12:59:41.927726Z","iopub.status.idle":"2022-04-12T12:59:42.20617Z","shell.execute_reply.started":"2022-04-12T12:59:41.927681Z","shell.execute_reply":"2022-04-12T12:59:42.205083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(x_binary, y_binary, test_size=0.4, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:51:40.459663Z","iopub.execute_input":"2022-04-12T12:51:40.460434Z","iopub.status.idle":"2022-04-12T12:51:40.954299Z","shell.execute_reply.started":"2022-04-12T12:51:40.460391Z","shell.execute_reply":"2022-04-12T12:51:40.953263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:56:52.285611Z","iopub.execute_input":"2022-04-12T12:56:52.286065Z","iopub.status.idle":"2022-04-12T12:56:52.289381Z","shell.execute_reply.started":"2022-04-12T12:56:52.28603Z","shell.execute_reply":"2022-04-12T12:56:52.288508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(x_binary.shape[1],)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n\t\t\t  \nhistory = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\nhistory_dict = history.history\nhistory_dict.keys()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:58:30.424194Z","iopub.execute_input":"2022-04-12T12:58:30.424499Z","iopub.status.idle":"2022-04-12T12:58:35.720183Z","shell.execute_reply.started":"2022-04-12T12:58:30.424461Z","shell.execute_reply":"2022-04-12T12:58:35.719154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nacc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:58:35.721836Z","iopub.execute_input":"2022-04-12T12:58:35.722663Z","iopub.status.idle":"2022-04-12T12:58:35.956214Z","shell.execute_reply.started":"2022-04-12T12:58:35.722627Z","shell.execute_reply":"2022-04-12T12:58:35.955099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:58:35.957609Z","iopub.execute_input":"2022-04-12T12:58:35.957844Z","iopub.status.idle":"2022-04-12T12:58:36.188534Z","shell.execute_reply.started":"2022-04-12T12:58:35.957816Z","shell.execute_reply":"2022-04-12T12:58:36.187548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2- Count ","metadata":{}},{"cell_type":"code","source":"x_count = tok.texts_to_matrix(texts, mode='count')\ny_count = train_df['target']\nx_count.shape , x_count.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:30:29.519734Z","iopub.execute_input":"2022-04-12T13:30:29.520128Z","iopub.status.idle":"2022-04-12T13:30:29.8219Z","shell.execute_reply.started":"2022-04-12T13:30:29.520084Z","shell.execute_reply":"2022-04-12T13:30:29.820977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(x_count, y_count, test_size=0.4, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:30:30.105112Z","iopub.execute_input":"2022-04-12T13:30:30.105434Z","iopub.status.idle":"2022-04-12T13:30:30.545426Z","shell.execute_reply.started":"2022-04-12T13:30:30.105397Z","shell.execute_reply":"2022-04-12T13:30:30.5445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodelc = models.Sequential()\nmodelc.add(layers.Dense(16, activation='relu', input_shape=(x_binary.shape[1],)))\nmodelc.add(layers.Dense(16, activation='relu'))\nmodelc.add(layers.Dense(1, activation='sigmoid'))\n\nmodelc.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n\t\t\t  \nhistory = modelc.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\nhistory_dict = history.history\nhistory_dict.keys()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:32:02.061386Z","iopub.execute_input":"2022-04-12T13:32:02.06224Z","iopub.status.idle":"2022-04-12T13:32:08.138786Z","shell.execute_reply.started":"2022-04-12T13:32:02.062196Z","shell.execute_reply":"2022-04-12T13:32:08.137896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:02:06.120096Z","iopub.execute_input":"2022-04-12T13:02:06.120404Z","iopub.status.idle":"2022-04-12T13:02:06.381677Z","shell.execute_reply.started":"2022-04-12T13:02:06.120371Z","shell.execute_reply":"2022-04-12T13:02:06.380653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:02:19.68346Z","iopub.execute_input":"2022-04-12T13:02:19.68376Z","iopub.status.idle":"2022-04-12T13:02:19.919509Z","shell.execute_reply.started":"2022-04-12T13:02:19.683727Z","shell.execute_reply":"2022-04-12T13:02:19.918554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3- TF_IDF","metadata":{}},{"cell_type":"code","source":"x_idf = tok.texts_to_matrix(texts, mode='tfidf')\ny_idf = train_df['target']\nx_idf.shape , x_idf.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:05:39.330759Z","iopub.execute_input":"2022-04-12T13:05:39.331117Z","iopub.status.idle":"2022-04-12T13:05:39.898767Z","shell.execute_reply.started":"2022-04-12T13:05:39.331079Z","shell.execute_reply":"2022-04-12T13:05:39.897737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(x_idf, y_idf, test_size=0.4, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:12:06.171609Z","iopub.execute_input":"2022-04-12T13:12:06.171956Z","iopub.status.idle":"2022-04-12T13:12:06.362532Z","shell.execute_reply.started":"2022-04-12T13:12:06.171916Z","shell.execute_reply":"2022-04-12T13:12:06.361564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import Dropout","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:09:40.168751Z","iopub.execute_input":"2022-04-12T13:09:40.169065Z","iopub.status.idle":"2022-04-12T13:09:40.173635Z","shell.execute_reply.started":"2022-04-12T13:09:40.169025Z","shell.execute_reply":"2022-04-12T13:09:40.172732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(x_idf.shape[1],)))\n#model.add(Dropout(0.2))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n\t\t\t  \nhistory = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\nhistory_dict = history.history\nhistory_dict.keys()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:12:21.093386Z","iopub.execute_input":"2022-04-12T13:12:21.093736Z","iopub.status.idle":"2022-04-12T13:12:26.333713Z","shell.execute_reply.started":"2022-04-12T13:12:21.093698Z","shell.execute_reply":"2022-04-12T13:12:26.33277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:12:26.335511Z","iopub.execute_input":"2022-04-12T13:12:26.336091Z","iopub.status.idle":"2022-04-12T13:12:26.575291Z","shell.execute_reply.started":"2022-04-12T13:12:26.33604Z","shell.execute_reply":"2022-04-12T13:12:26.574399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:10:18.742375Z","iopub.execute_input":"2022-04-12T13:10:18.742668Z","iopub.status.idle":"2022-04-12T13:10:18.989788Z","shell.execute_reply.started":"2022-04-12T13:10:18.742637Z","shell.execute_reply":"2022-04-12T13:10:18.98896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> TF-IDF not normalized?\n> \n> In the above setting, TFIDF in keras texts_to_matrix produce non normalized values. This is because the IDF ~= 1/DF, but log(1+DF)\n> For this bug, it's better to use sklearn TfidfVectorizer","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=10000)\nvectorizer.fit(texts)\n\n\nx_idf = vectorizer.transform(texts)\n\nimport scipy \nx_idf =scipy.sparse.csr_matrix.todense(x_idf)\n\n\ny_idf = train_df['target']\nx_idf.shape , x_idf.shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:23:34.35222Z","iopub.execute_input":"2022-04-12T13:23:34.352515Z","iopub.status.idle":"2022-04-12T13:23:34.873481Z","shell.execute_reply.started":"2022-04-12T13:23:34.352486Z","shell.execute_reply":"2022-04-12T13:23:34.872873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(x_idf, y_idf, test_size=0.4, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:23:34.87488Z","iopub.execute_input":"2022-04-12T13:23:34.875437Z","iopub.status.idle":"2022-04-12T13:23:35.184578Z","shell.execute_reply.started":"2022-04-12T13:23:34.875345Z","shell.execute_reply":"2022-04-12T13:23:35.183526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.backend.clear_session()\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(x_idf.shape[1],)))\n#model.add(Dropout(0.2))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n\t\t\t  \nhistory = model.fit(x_train,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))\nhistory_dict = history.history\nhistory_dict.keys()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:23:35.186304Z","iopub.execute_input":"2022-04-12T13:23:35.186545Z","iopub.status.idle":"2022-04-12T13:23:39.953662Z","shell.execute_reply.started":"2022-04-12T13:23:35.186515Z","shell.execute_reply":"2022-04-12T13:23:39.952752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:23:53.15421Z","iopub.execute_input":"2022-04-12T13:23:53.154666Z","iopub.status.idle":"2022-04-12T13:23:53.378954Z","shell.execute_reply.started":"2022-04-12T13:23:53.154634Z","shell.execute_reply":"2022-04-12T13:23:53.378295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:24:08.125368Z","iopub.execute_input":"2022-04-12T13:24:08.125657Z","iopub.status.idle":"2022-04-12T13:24:08.367958Z","shell.execute_reply.started":"2022-04-12T13:24:08.125627Z","shell.execute_reply":"2022-04-12T13:24:08.366814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As we can see, the count bow model preformed best for the same NN dense model with the highest accuracy ; howerver the freq idf was least to overfit!","metadata":{}},{"cell_type":"code","source":"#submit","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test = tok.texts_to_matrix(corpus_test, mode='count')\ns=np.array(modelc.predict(x_test)>0.5).astype('int')\nsubm=pd.DataFrame(test_id ,columns=['id'])\nsubm['target']=s","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:34:37.739956Z","iopub.execute_input":"2022-04-12T13:34:37.740348Z","iopub.status.idle":"2022-04-12T13:34:38.319291Z","shell.execute_reply.started":"2022-04-12T13:34:37.740307Z","shell.execute_reply":"2022-04-12T13:34:38.318278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm.to_csv('submission1.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T13:35:18.833721Z","iopub.execute_input":"2022-04-12T13:35:18.834017Z","iopub.status.idle":"2022-04-12T13:35:18.851442Z","shell.execute_reply.started":"2022-04-12T13:35:18.833966Z","shell.execute_reply":"2022-04-12T13:35:18.850787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}