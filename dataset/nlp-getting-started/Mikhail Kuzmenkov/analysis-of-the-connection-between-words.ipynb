{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom nltk.stem.snowball import SnowballStemmer \nfrom nltk.corpus import stopwords\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport eli5\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nimport gensim\nfrom sklearn import manifold\nfrom gensim.models.word2vec import Word2Vec\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load the data and split it"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\nX = train['text']\ny = train['target']\ntest = test['text']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\nX_train, X_test, y_train, y_test = list(X_train), list(X_test), list(y_train), list(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make tokenizer and stemmer for cleaning data"},{"metadata":{"trusted":true},"cell_type":"code","source":"stop = stopwords.words('english')\ndef tokenizer(text):\n    tokenized = []\n    for string in text:\n        string = re.sub('[^a-z\\sA-Z]', '', string)\n        string = re.sub('http\\S+', '', string)\n        tokenized.append([w for w in string.split() if w not in stop])\n    return tokenized\n\nsnow_stemmer = SnowballStemmer(language='english') \ndef stemmer(text):\n    stem_string = []\n    for string in text: \n        stem_string.append([snow_stemmer.stem(word) for word in string])\n    return stem_string \n\nX_train = tokenizer(X_train)\nX_train = stemmer(X_train)\ntest = tokenizer(test)\ntest = stemmer(test)\nX_test = tokenizer(X_test)\nX_test = stemmer(X_test)\n\nX_train_corrected = [\" \".join(x) for x in X_train]\nX_test_corrected = [\" \".join(x) for x in X_test]\ntest_corrected = [\" \".join(x) for x in test]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compute Tfid "},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(lowercase=False, stop_words='english', preprocessor=None)\nX_tfidf_train = tfidf.fit_transform(X_train_corrected)\nX_tfidf_test = tfidf.transform(X_test_corrected )\ntest = tfidf.transform(test_corrected)\n\nX_tfidf_train.shape, X_tfidf_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find params for logistic regression based on Tfid features"},{"metadata":{"trusted":true},"cell_type":"code","source":"logit = LogisticRegression(random_state=0)\nparam = {'C': [0.1, 1, 2, 3],\n        'solver': ['lbfgs', 'liblinear']}\nlogit_grid = GridSearchCV(estimator = logit, param_grid = param, \n                          scoring = 'f1', n_jobs = -1,)\nlogit_grid.fit(X_tfidf_train, y_train)\nlogit_grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit = LogisticRegression(C = 4, solver = 'lbfgs',random_state=0)\nlogit.fit(X_tfidf_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Show F1 score and accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logit.predict(X_tfidf_test)\nprint('F1 = ', f1_score(y_true = y_test, y_pred = y_pred))\nprint('Accuracy = ', precision_score(y_true = y_test, y_pred = y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now let`s show weights of words importance "},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(estimator=logit, feature_names = list(tfidf.get_feature_names()),\n                 top=(20, 20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Interestingly that a word 'california' has quite big weight.\n### Now we will try to create words vectors using word2vec and vizualize it with 3D scatter plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = X_train\nnlp = gensim.models.word2vec.Word2Vec(corpus, size=200,   \n            window=6, min_count=1, sg=1, iter=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### First of all, let`s see how well our model understood the text and what words it considers to be close by meaning to the word 'fire'."},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp.most_similar(\"fire\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Not very good. But it still makes sense because it also shows 'forest', 'wild'. Model could work better if we better cleaned text and have played with parameters of word2vec. "},{"metadata":{},"cell_type":"markdown","source":"### Here we just reduce dimension of vector embeddings from 200 to 3. These 3 dimensions are our new axis for 3D scatter plot. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## choose a word 'fire' to compute its weights\nfig = plt.figure()\nword = \"fire\"\ntot_words = [word] + [tupla[0] for tupla in \n                 nlp.most_similar(word, topn=20)]\nX = nlp[tot_words]\n\n## pca to reduce dimensionality from 300 to 3\npca = manifold.TSNE(perplexity=40, n_components=3, init='pca')\nX = pca.fit_transform(X)\n\n## create data frame with 3 axis\ndtf_ = pd.DataFrame(X, index=tot_words, columns=[\"x\",\"y\",\"z\"])\ndtf_[\"input\"] = 0\ndtf_[\"input\"].iloc[0:1] = 1\n\n# plot 3d\nfig = plt.figure(figsize = (10,8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(dtf_[dtf_[\"input\"]==0]['x'], \n           dtf_[dtf_[\"input\"]==0]['y'], \n           dtf_[dtf_[\"input\"]==0]['z'], c=\"black\")\nax.scatter(dtf_[dtf_[\"input\"]==1]['x'], \n           dtf_[dtf_[\"input\"]==1]['y'], \n           dtf_[dtf_[\"input\"]==1]['z'], c=\"red\")\nax.set(xlabel=None, ylabel=None, zlabel=None, xticklabels=[], \n       yticklabels=[], zticklabels=[])\nfor label, row in dtf_[[\"x\",\"y\",\"z\"]].iterrows():\n    x, y, z = row\n    ax.text(x, y, z, s=label)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems to me that our vector embeddings reflects the meaning of words not bad if we take into account that it doesn`t have configured parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = logit.predict(test)\nnew_id = pd.read_csv('../input/nlp-getting-started/test.csv') \nids = list(new_id.id)\nfinal_submission = pd.DataFrame({'id': ids, 'target':y_pred})\nfinal_submission.to_csv('final_submission.csv', index = False) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}