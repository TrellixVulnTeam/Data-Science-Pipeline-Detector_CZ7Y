{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Table of Content\n1.  General Advice\n1.  Some important study material links related to this project\n1.  References of some other notebooks used in this project\n1.  Importing relevant Libraries\n1.  Loading the dataset\n1.  EDA\n     *      Check for the presence of imbalanced dataset.\n     *      Checking number of words in disastrous & non-disastrous tweets\n     *      Checking stop words for disastrous & Non-disastrous tweets\n     *      Checking common words in both types of tweets\n     *      Cleaning the dataset\n     *      Printing WordCloud\n1.  Word Embedding using GloVe  \n1.  Building an LSTM Model with GloVe results\n1.  Predicting train,cv,test outputs from the LSTM model\n1.  Plotting Confusion Matrix\n1.  Applying BERT\n1.  Testing the BERT model on train & test dataset\n1.  Comparing LSTM with GloVe vs BERT model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. General Advice\n\n* Ensure that the Internet option is turned \"ON\" in kaggle to download libraries,datasets,etc.\n \n* Ensure that you have enabled GPU accelerator for your notebook at the time of running your notebook for faster performance. You can enable it by clicking on the arrow appearing on the right of \"Save Version\" button. \n\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Some important study material links related to this project\n\nhttp://jalammar.github.io/illustrated-bert/\n\nhttp://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n\nhttps://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. References of some other notebooks used in this project\n\n* https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert\n\n* https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n\n* https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert\n\n* https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4. Importing relevant Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Loading the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train= pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_test=pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))\n\ndf_train.head()\nprint(df_train.columns)\nprint(df_train['text'].values[0])\nprint(\"=\"*50)\nprint(df_test['text'].values[150])\nprint(\"=\"*50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. EDA","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* ## Check for the presence of imbalanced dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# extracting the number of examples of each class\nReal_len = df_train[df_train['target'] == 1].shape[0]\nNot_len = df_train[df_train['target'] == 0].shape[0]\n# bar plot of the 3 classes\nplt.rcParams['figure.figsize'] = (7, 5)\nplt.bar(10,Real_len,3, label=\"Disastrous tweets counts\", color='red')\nplt.bar(15,Not_len,3, label=\"Non-Disastrous tweets counts\", color='green')\nplt.legend()\nplt.ylabel('Number of examples')\nplt.title('Propertion of examples')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### CONCLUSION : Both disastrous & non-disastrous tweets are almost equally probable to occur. So there is no problem of imbalanced dataset.\n\n*What if we had imbalanced dataset : We then had to implement techniques like upsampling, creating synthetic points(Data Augmentation),etc. Downsampling majority class is generally not recommended because it results into loss of dataset which ultimately results into loss of information.*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* ## Checking number of words in disatrous & non-disastrous tweets[](http://)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=df_train[df_train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=df_train[df_train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### CONCLUSION - We performed this check to see if number of characters appearing in disastrous tweets is very different from the number of characters appearing in non-disastrous tweets but ~120-140 character sentences are very common between disastrous & non-disastrous tweets. And the overall distribution also looks very similar so character count per sentence feature is not so important to classify between disastrous & non-disastrous tweets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* ## Checking stop words for disastrous & Non-disastrous tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target): # creating corpus of all the words in our dataset\n    corpus=[]\n    for x in df_train[df_train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop_0=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]\n\n####################################################################\n\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop_1=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx0,y0=zip(*top_0)\nx1,y1=zip(*top_1)\nplt.bar(x0,y0, color=['green'], label = \"Non-disaster\")                # indicates non-disaster tweets\nplt.bar(x1,y1, color=['red'], label = \"Disaster\")              # indicates disaster tweets\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### CONCLUSION : Both types of tweets have stop word \"the\" appearing most number of times. Stop words \"on\", \"by\", \"at\",  \"from\", \"are\", \"after\", \"as\" doesn't appear in non-disasterous tweets.But the count of these words are not enough to qualify them as a differentiating feature. In summary we cannot find much from here also.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* > ## Checking common words in both types of tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(1)\nplt.figure(figsize=(16,5))\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x).set_title('Words most frequently occuring in Non-disastrous tweets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### CONCLUSION : Words like \"-\",\"...\" are appearing in most of the sentences of non-disastrous tweets so we will eliminate such words appearing in the sentences because they don't add any information about the tweet.\n\n*But why are we removing unnecessary words? Why can't we just leave them as it is? : Some of the un-necessary words are occuring in almost every sentence so it will occupy so many space and doesn't make any sense to keep that word which is not adding any differentiating value for the classification*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = create_corpus(0)\nplt.figure(figsize=(16,5))\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x).set_title('Words most frequently occuring in Disastrous tweets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### CONCLUSION : Similar argument can be made for most frequently appearing words in disastrous tweets. Data cleaning is required in the whole dataset.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* ## Cleaning the dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Concatenating train & test data to apply cleaning on whole dataset*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([df_train,df_test])       \ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Removing URLs from tweets*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Removing HTML tags from tweets*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Romoving Emojis from tweets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake 😔😔\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Removing punctuations from tweets*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Applying all the data cleaning methods*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_URL(x))\ndf['text']=df['text'].apply(lambda x : remove_html(x))\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))\ndf['text']=df['text'].apply(lambda x : remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ## Printing WordCloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating seperate corpus for both disastrous & non-disastrous tweets\ncorpus_new1 = create_corpus(1)\ncorpus_new0 = create_corpus(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nword_cloud1 = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new1[:50]))\n\nword_cloud0 = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new0[:50]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(20, 5))\nax[0].imshow(word_cloud1)\nax[0].set_title('Most frequently appearing words in disastrous tweets',fontsize = 20)\nax[0].axis(\"off\")\n\nax[1].imshow(word_cloud0)\nax[1].set_title('Most frequently appearing words in non-disastrous tweets',fontsize = 20)\nax[1].axis(\"off\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## CONCLUSION - Words like \"Shelter\", \"evacuation\", \"Forest\", \"Reason\" appear a lot in disastrous tweets as we can see from the word cloud. Words like \"wonderful\", \"lovely\" and some words very irrelevant to a disastrous tweet appear in non-disastrous tweets, as expected. \n\n*Why are we drawing Word Clouds : Generally we draw word clouds to have an idea about the most common words appearing in the document(tweets in this case). In reference to this dataset, it also gives an idea about the words people are most commonly using in a disastrous & non-disastrous tweets. *","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 7. Word Embedding using GloVe","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Creating a new corpus*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus_new(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus_new(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Importing pretrained GloVe vector representation of words*\n\n\n*Please note that we are creating an embedding dictionary which will store each word representation in it. One more thing to note here is that these words are not the words from our dataset instead these are the words stored in the glove model *","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict={}\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Performing padding to ensure that each word vector representation is of same length because models coming up require inputs to be of same length.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Representation of first word in our corpus. Note that the size of tweet_pad[0][0:] is 50 dimension as MAX_LEN defined above is 50.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet_pad[0][0:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Total number of unique words in corpus(full datset):',len(word_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Here we are creating embedding matrix which will store vector representation of each word in our corpus. Note that this is the first time we are creating vector representation of words appearing in our dataset(corpus).embedding_matrix[i] gives the vector representaion of ith word in our dataset. \nPlease note that we are creating a 100 dimension vector representation of each word here.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Building an LSTM Model with GloVe results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Please note that we are providing embedding_matrix as embedding_initializer in embedding layer of this LSTM model.\nEmbedding layer in Keras also provides a way to vectorize the words. Instead of providing embedding_matrix specifically if you leave it as it is then it will perform word embedding on its own.*\n\n*People prefer to use trained word embedding models because they have been trained on a very large dataset and models like GloVe, W2V, BERT are some of the most advanced and accurate word embedding techniques out there.* ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Splitting train & test datasets. Please note that df stores all the dataset so we are splitting df into train and test*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train=tweet_pad[:df_train.shape[0]]\ntest=tweet_pad[df_train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Further splitting train set further into train and cv sets*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_cv,y_train,y_cv=train_test_split(train,df_train[\"target\"],test_size=0.1)\nprint('Shape of train  ->',X_train.shape)\nprint(\"Shape of Validation ->\",X_cv.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Fitting the model with the train dataset keeping cv dataset as validation_data*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Recomended 10-20 epochs\nhistory=model.fit(X_train,y_train,batch_size=64,epochs=10,validation_data=(X_cv,y_cv),verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## RESULTS : We are getting ~82% accuracy on cv dataset(unseen by model)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 9. Predicting train,cv,test outputs from the LSTM model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pred_GloVe = model.predict(X_train)\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')\n\ncv_pred_GloVe = model.predict(X_cv)\ncv_pred_GloVe_int = cv_pred_GloVe.round().astype('int')\n\ntest_pred_GloVe = model.predict(test)\ntest_pred_GloVe_int = test_pred_GloVe.round().astype('int')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10. Plotting Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm / cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Showing Confusion Matrix for LSTM+GloVe model on cv dataset\nplot_cm(cv_pred_GloVe_int, y_cv, 'Confusion matrix for LSTM+GloVe model on cv dataset', figsize=(7,7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## RESULTS : Our model is able to accurately classify ~82% of disastrous tweets as disastrous and ~79 of non-disastrous and non-disastrous.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Storing model's output for data in submission file*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = test_pred_GloVe_int\nsubmission.head(10)\n\nsubmission.to_csv(\"submission.csv\", index=False, header=True)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 11. Applying BERT","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*I would strongly recommend you to refer to the external reference links provided in the starting of this kernal. Following lines of codes will be very clear after that.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\n\n# We will use the official tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n    \nimport tokenization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    if Dropout_num == 0:\n        # Without Dropout\n        out = Dense(1, activation='sigmoid')(clf_output)\n    else:\n        # With Dropout(Dropout_num), Dropout_num > 0\n        x = Dropout(Dropout_num)(clf_output)\n        out = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load BERT from the Tensorflow Hub\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Please note that we are again loading the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load CSV files containing training data\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Concatenating train & test data to apply cleaning on whole dataset similar to what we done earlier*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.concat([train,test])       \ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_URL(x))\ndf['text']=df['text'].apply(lambda x : remove_html(x))\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))\ndf['text']=df['text'].apply(lambda x : remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Please note that df here includes train as well as test dataset.Therefore we have to split this dataset into train and test before applying BERT model in it.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df[:train.shape[0]]\ntest = df[train.shape[0]:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Splitting train further into train and cv datasets*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_BERT,X_cv_BERT,y_train_BERT,y_cv_BERT=train_test_split(train,train['target'].values,test_size=0.2)\nprint('Shape of train set ->',X_train_BERT.shape)\nprint(\"Shape of Validation set ->\",X_cv_BERT.shape)\nprint(\"Shape of test set ->\",test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Load tokenizer from the bert layer*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Encoding the text into tokens, masks, and segment flags which are taken as input to BERT model*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\ntrain_input = bert_encode(X_train_BERT.text.values, tokenizer, max_len=160)\ncv_input = bert_encode(X_cv_BERT.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = y_train_BERT\ncv_labels = y_cv_BERT","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Building BERT model with self tuning the parameters*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\nDropout_num = 0\nmodel_BERT = build_model(bert_layer, max_len=160)\nmodel_BERT.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\ncheckpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model_BERT.fit(\n    train_input, train_labels,\n    validation_split = 0.1,\n    epochs = 3, # recomended 3-5 epochs\n    callbacks=[checkpoint],\n    batch_size = 16\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 12. Testing the BERT model on train & test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Thanks to https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\n# Prediction by BERT model with my tuning\nmodel_BERT.load_weights('model_BERT.h5')\ntrain_pred_BERT = model_BERT.predict(train_input)\ntrain_pred_BERT_int = train_pred_BERT.round().astype('int')\n\ncv_pred_BERT = model_BERT.predict(cv_input)\ncv_pred_BERT_int = cv_pred_BERT.round().astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_BERT = model_BERT.predict(test_input)\ntest_pred_BERT_int = test_pred_BERT.round().astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = test_pred_BERT_int\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Printing Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cm(cv_pred_BERT_int, cv_labels, 'Confusion matrix for BERT model on cv dataset', figsize=(7,7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 13. Comparing LSTM with GloVe vs BERT model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> ## CONCLUSION : ~86% disastrous tweets are correctly predicted & ~85% non-disastrous are also correctly predicted. Whereas in LSTM+GloVe model this percentage was ~82% & ~79% respectively. So we have significantly improved from LSTM+GloVe model to BERT model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# ✔️PLEASE GIVE THIS NOTEBOOK AN UPVOTE IF YOU LIKED IT!!!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}