{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Description of Dataset\nUsed dataset is consist of relevant and irrelevant tweets to a weather disaster."},{"metadata":{},"cell_type":"markdown","source":"# Summary\nWe aim to distinguish relavant tweets to a weather disaster. As a result later we will be able to use those tweets to predict a weather disaster, maybe as a weather forcasting agency. \nIn this dataset we initially conducted a comprehensive EDA on the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import euclidean_distances\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading train data\ndf_tweet_train = pd.read_csv('../input/nlp-getting-started/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading test data\ndf_tweet_test = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Take a look at the dataset\ndf_tweet_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shape of train dataframe\ndf_tweet_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shape of test dataframe\ndf_tweet_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gartting number of relevant and irrelevent tweets out of total 7,613 tweets in train dataset\ndf_tweet_train[['text', 'target']].groupby('target').count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above it is an imbalanced dataset, as number f irrelevant tweets is considerably higher than relevant ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"#adding a new column to the dataframe for the length of each tweet\ndf_tweet_train['tweet_len'] = df_tweet_train.text.apply(len)\ndf_tweet_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution of tweets length based on relevant/irrelevant fact\nplt.figure(figsize=(10, 6))\n\ndf_tweet_train[df_tweet_train.target== 0].tweet_len.plot(bins=40, kind='hist', color='green', \n                                       label='irrelevant', alpha=0.6)\ndf_tweet_train[df_tweet_train.target==1].tweet_len.plot(bins=40,kind='hist', color='red', \n                                       label='relevant', alpha=0.6)\nplt.legend()\nplt.xlabel(\"text Length\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can notice in the above plot relevant tweets are usually longer than irrelevant tweets. "},{"metadata":{},"cell_type":"markdown","source":"#  Text Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#The following function will remove all stopwords (defined in the list of english stopwords in nltk) and punctuations from the text \nimport string\n\nfrom nltk.corpus import stopwords\n\ndef text_process(text):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    Vectorization1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    STOPWORDS = stopwords.words('english')\n    # making a list of characters of the text, excluding punctuations (!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~)\n    nopunc = [char for char in text if char not in string.punctuation]\n\n    # Join the characters with no space in between to form the text (excluding punctuations) again.\n    nopunc = ''.join(nopunc)\n    \n    # splitting string nopunc with spaces and making all words lowercase then\n    #check if the word exsists in STOPWORDS collection, if not join those words with space in between.\n    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making a new column in the dataframe applying the text-preprocessing function to the \"text\" column.\ndf_tweet_train['clean_txt'] = df_tweet_train.text.apply(text_process)\ndf_tweet_test['clean_txt'] = df_tweet_test.text.apply(text_process)\ndf_tweet_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorization"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# defining X (input) and y (label) from the dataframe columns for later use in COUNTVECTORIZER\nX_train = df_tweet_train.clean_txt.values\ny_train = df_tweet_train.target.values\nX_test = df_tweet_test.clean_txt.values\n# y_test = df_tweet_test.target.values\n#shape and dimension of X and y arrays\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we need to convert text documents to a matrix of token counts\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# instantiate the vectorizer object (content vectorizer) and training (fitting) that on the train dataset\nvect = CountVectorizer()\nvect.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at the “vocabulary” also called the “dictionary” for the whole representation\nvect.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learn training data vocabulary, then use it to create a document-term matrix\nX_train_dtm = vect.transform(X_train)\n# examine the document-term matrix\nX_train_dtm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This train dtm matrix contains 7613 train articles samples (rows) and 22,310 vocabs (columns). Its data type is integer, meaning all 0,1,2 (if two of a specific vocab in a text)..\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform testing data (using fitted vocabulary) into a document-term matrix\nX_test_dtm = vect.transform(X_test)\nX_test_dtm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This test dtm matrix contains 3263 test articles samples (rows) and 22,310 fitted vocabulary (columns). "},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import and instantiate a Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Accuracy based on cross validation.\naccuracy = model_selection.cross_val_score(nb, X_train_dtm, df_tweet_train[\"target\"], cv=3, scoring=\"accuracy\")\naccuracy","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Average accuracy of nb model in cross validation\nnp.average(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction based on cross validation.\nfrom sklearn.metrics import confusion_matrix\ny_pred = model_selection.cross_val_predict(nb, X_train_dtm, df_tweet_train[\"target\"], cv=3)\n# confusuin matrix based on cross validation.\nconf_mat = confusion_matrix(y_train, y_pred)\nconf_mat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit Naive Bayes classifier according to X, y\nnb.fit(X_train_dtm, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make class predictions for X_test_dtm\ny_pred_class = nb.predict(X_test_dtm)\n#The first 10 predictions\ny_pred_class[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate AUC\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_train, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission[\"target\"] = nb.predict(X_test_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.iloc[::5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"csc310","language":"python","name":"csc310"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"}},"nbformat":4,"nbformat_minor":4}