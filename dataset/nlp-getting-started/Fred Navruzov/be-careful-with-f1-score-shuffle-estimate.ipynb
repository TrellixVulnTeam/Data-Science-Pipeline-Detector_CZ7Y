{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi guys!\n<br>Please take into account F1-score specifics \n<br>and see the **Private/Public LB shift estimate** below"},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation\n---"},{"metadata":{},"cell_type":"markdown","source":"For simplicity we'll be using simple LGBM classifier atop of \n<br>[USE embeddings](https://tfhub.dev/google/universal-sentence-encoder-large/5) (or even on simple TF-IDF scores) - our purpose is \n<br>to estimate discrepancy between F1-score on Public and Private Leaderboards, under the following assumptions:\n- Public/Private LB split is done by 30% / 70%\n- This split was stratified by target (around **0.43 mean target** in both folds, according to [leaked data mean](https://www.kaggle.com/szelee/a-real-disaster-leaked-label))\n- F1-score, used for calculation, is averaged by `macro` strategy:\n```\nfrom sklearn.metrics import f1_score\nf1_score(y_true, y_pred, average='macro')\n```\n\nP.s. this [medium post about Macro-F1's](https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin) is highly recommended to read"},{"metadata":{},"cell_type":"markdown","source":"## Imports\n---"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T10:32:13.930277Z","start_time":"2020-01-04T10:32:13.892221Z"},"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Imports\nfrom functools import partial  # to mock arguments in functions, like f1_score\nfrom os.path import join as pjoin\n\nimport cufflinks as cf\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom plotly.offline import init_notebook_mode\nfrom sklearn.metrics import f1_score\n\ninit_notebook_mode(connected=False)\ncf.go_offline()\n\npd.options.display.max_rows = 200\npd.options.display.max_columns = 200\npd.options.display.max_colwidth = 200\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# F1-Score fundamentals\n---\n\n(Extracted from correspondent [WIKI page](https://en.wikipedia.org/wiki/F1_score))\n>In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the **precision p** and the **recall r** of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the **harmonic mean** of the precision and recall, where an F1 score **reaches its best value at 1 (perfect precision and recall)** and worst at 0. "},{"metadata":{},"cell_type":"markdown","source":"# F1-Score Surface visualization\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\n\n# f1-score data grid preparation\nsteps = 50\nprecision_grid = np.linspace(0, 1, num=steps)\nrecall_grid =    np.linspace(0, 1, num=steps)\npp, rr = np.meshgrid(precision_grid, recall_grid, sparse=True)\nf1_score_grid = 2*(pp*rr)/(pp + rr)\nf1_score_grid[np.isnan(f1_score_grid)] = 0\n\n# visualize it with plot.ly\nfig = go.Figure(data=[go.Surface(z=f1_score_grid, x=precision_grid, y=recall_grid)])\n\nfig.update_layout(\n    title='F1-Score surface (feel free to rotate/scale it as you wish)', \n#     autosize=True,\n    width=640,\n    height=640,\n    margin=dict(l=65, r=50, b=65, t=90),\n    scene_camera_eye=dict(x=2.2, y=0.78, z=0.64),\n    scene=dict(\n        xaxis_title='PRECISION',\n        yaxis_title='RECALL',\n        zaxis_title='F1-SCORE'\n    )\n)\n\nfig.update_traces(\n    contours_z=dict(\n        show=True, \n        usecolormap=True,\n        highlightcolor=\"limegreen\", \n        project_z=True)\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T10:38:16.355708Z","start_time":"2020-01-04T10:38:16.352712Z"},"trusted":true},"cell_type":"code","source":"# prepare correct metric\nf1_score_macro = partial(f1_score, average='macro')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loading\n---"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T10:19:54.185746Z","start_time":"2020-01-04T10:19:54.110112Z"},"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/nlp-getting-started/'\n# DATA_DIR = '../data'\ntrain = pd.read_csv(pjoin(DATA_DIR, 'train.csv'))\ntest = pd.read_csv(pjoin(DATA_DIR, 'test.csv'))\n\n# glue datasets together, for convenience\ntrain['is_train'] = True\ntest['is_train'] = False\ndf = pd.concat(\n    [train, test], \n    sort=False, ignore_index=True\n).set_index('id').sort_index()\n\nprint(train.shape, test.shape, df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare Features\n---"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T10:48:21.698188Z","start_time":"2020-01-04T10:48:21.182411Z"},"trusted":true},"cell_type":"code","source":"import re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_text = TfidfVectorizer(\n    stop_words='english',\n    max_df=0.33,\n    min_df=5,\n    dtype=np.float32,\n    max_features=500,\n)\n\n# fit transformer\ntfidf_text.fit(df['text'])\n\nNON_WORD_PATTERN = r\"[^A-Za-z0-9\\.\\'!\\?,\\$\\s]\"\n\ndf_tfidfs = pd.DataFrame(\n    np.array(tfidf_text.transform(df['text']).todense()),\n    columns=[\n        f'tfidf__{re.sub(NON_WORD_PATTERN, \"\", k)}'\n        for (k, v) in\n        sorted(tfidf_text.vocabulary_.items(), key=lambda item: item[1])\n    ],\n    index=df.index,\n)\nprint(df_tfidfs.shape)\ndf_tfidfs.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T11:21:06.416167Z","start_time":"2020-01-04T11:21:06.38412Z"},"trusted":true},"cell_type":"code","source":"# add keyword\ndf['keyword_cat_codes'] = df.keyword.fillna('missing').astype('category').cat.codes\n\ndf_features = pd.concat(\n    [\n       df_tfidfs,\n       df[['keyword_cat_codes']]\n    ],\n    axis=1\n)\n\nprint(df_features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make baseline"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T11:21:07.561962Z","start_time":"2020-01-04T11:21:07.558957Z"},"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross-validation"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T11:23:05.362275Z","start_time":"2020-01-04T11:22:34.828298Z"},"trusted":true},"cell_type":"code","source":"def f1_score_lgb(preds, dtrain):\n    labels = dtrain.get_label()\n    f_score = f1_score_macro(\n        np.round(preds),\n        labels,\n    )\n    return 'f1_score', f_score, True\n\n\nlgb_params = {\n    'num_leaves': 63,\n    'learning_rate': 0.015,\n    'max_depth': -1,\n    'subsample': 0.9,\n    'colsample_bytree': 0.33,\n}\n\nskf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n\n\ncv_res = lgb.cv(\n    params=lgb_params,\n    train_set=lgb.Dataset(\n        data=df_features[df.is_train],\n        label=df.loc[df.is_train, 'target'],\n        categorical_feature=['keyword_cat_codes'],\n    ),\n    folds=skf,\n#     metrics=['auc'],\n    feval=f1_score_lgb,\n    verbose_eval=50,\n    early_stopping_rounds=200,\n    #     eval_train_metric=True,\n    num_boost_round=1000,\n)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T11:23:55.056451Z","start_time":"2020-01-04T11:23:54.357408Z"},"trusted":true},"cell_type":"code","source":"# train simple model according to CV's boosting_rounds params\nmodel = lgb.LGBMClassifier(\n    **lgb_params, \n    n_esimators=int( len(cv_res['f1_score-mean']) * (skf.n_splits + 1)/skf.n_splits )\n)\n\nmodel.fit(\n    X=df_features[df.is_train],\n    y=df.loc[df.is_train, 'target'],\n    categorical_feature=['keyword_cat_codes'],\n)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T11:23:59.515091Z","start_time":"2020-01-04T11:23:58.934218Z"},"trusted":true},"cell_type":"code","source":"# check feature importance\nlgb.plot_importance(\n    model, \n    importance_type='gain', \n    figsize=(10, 10), \n    max_num_features=50\n)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T10:51:39.241506Z","start_time":"2020-01-04T10:51:39.238504Z"}},"cell_type":"markdown","source":"# F1-score calculations\n---\nAs far as we got leaked labels, for **educational purposes only** \n<br>let's see how F1-score varies across different private/public splits on **test data**"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T11:24:08.514506Z","start_time":"2020-01-04T11:24:08.502479Z"},"trusted":true},"cell_type":"code","source":"# load leaked data\nleaked_labels = pd.read_csv(\n    pjoin('../input/a-real-disaster-leaked-label', 'submission.csv')\n).set_index('id')\ndf.loc[~df.is_train, 'target'] = leaked_labels\ndf.loc[df.is_train, 'target'].mean(), df.loc[~df.is_train, 'target'].mean()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T11:30:36.549131Z","start_time":"2020-01-04T11:30:36.431965Z"},"trusted":true},"cell_type":"code","source":"# check total f1-score, on 100% test data\ny_pred = pd.Series(\n    model.predict(df_features[~df.is_train]), \n    index=df[~df.is_train].index\n)\nf1_total_test = np.round(\n    f1_score_macro(df.loc[~df.is_train, 'target'], y_pred), 4\n)\n\nf1_total_test","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T11:26:49.821398Z","start_time":"2020-01-04T11:26:40.046793Z"},"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\n\n# generate splits\nsampler = np.random.RandomState(911)\nn_trials = 1000\nrandom_seeds = sorted(set(sampler.randint(0, 10e7, n_trials)))\n\n# make private/public stratified splits\nf1_scores = []\n\nfor rs in tqdm(random_seeds):\n    public_ind, private_ind = train_test_split(\n        df[~df.is_train].index.values, \n        test_size=0.7, \n        random_state=rs,\n        stratify=df.loc[~df.is_train, 'target'],\n    )\n    f1_public = f1_score_macro(\n        y_true=df.loc[public_ind, 'target'],\n        y_pred=y_pred[public_ind]\n    )\n    f1_private = f1_score_macro(\n        y_true=df.loc[private_ind, 'target'],\n        y_pred=y_pred[private_ind]\n    )\n    \n    f1_scores.append((rs, f1_public, f1_private))\n    \ndf_f1 = pd.DataFrame(f1_scores, columns=['random_seed', 'f1_public', 'f1_private'])\ndf_f1['pub_pr_diff'] = df_f1['f1_public'] - df_f1['f1_private']\n\n# clip some extremes\ndf_f1['pub_pr_diff'] = df_f1['pub_pr_diff'].clip(\n    lower=df_f1['pub_pr_diff'].quantile(0.005),\n    upper=df_f1['pub_pr_diff'].quantile(0.995),\n)\n\n# check sample data\nprint(df_f1.shape)\ndf_f1.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T11:26:53.651446Z","start_time":"2020-01-04T11:26:53.643435Z"},"trusted":true},"cell_type":"code","source":"df_f1.pub_pr_diff.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check BOX plots of PUBLIC / PRIVATE F1-Scores\ndf_f1[['f1_public', 'f1_private']].iplot(\n    kind='box', \n    dimensions=(640, 320),\n    title='F1-Score range, public/private LB'\n)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true},"cell_type":"markdown","source":"Fortunately, private scores distribution is much more narrow and stable\n<br>However, the main thing to remember - **do not blindly trust public leaderboard results**"},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T11:31:41.00798Z","start_time":"2020-01-04T11:31:40.738584Z"},"trusted":true},"cell_type":"code","source":"# let's see hist of private f1-scores\nf1_mean_private = np.round(df_f1['f1_private'].mean(), 4)\ndf_f1['f1_private'].iplot(\n    kind='hist', \n    title=f'F1 mean_private: {f1_mean_private}<br>F1 total test:       {f1_total_test}',\n    dimensions=(640, 320)\n)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2020-01-04T11:26:56.949007Z","start_time":"2020-01-04T11:26:56.685598Z"},"trusted":true},"cell_type":"code","source":"# let's see hist of differences between public/private score\ndf_f1['pub_pr_diff'].iplot(\n    kind='hist', \n    title='Public/Private F1-score diff distribution', \n    dimensions=(640, 320)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"See, **how huge** discrepancy can be - you may (or may not) be lucky enough \n- to get score boost on private\n- as well as drastically drop\n\n<img src=\"https://media.giphy.com/media/Maz1la1GQaCPkxsODl/giphy.gif\" align=\"left\"/>"},{"metadata":{},"cell_type":"markdown","source":"---\nThat's all for now\n<br>Stay tuned, this notebook is going to be updated soon\n<br>Hope, you guys, like it and learn something new!\n<br>**As always, upvotes, comments, ideas are always welcome!**\n\n---\nP.s. Check my [EDA + Baseline notebook](https://www.kaggle.com/frednavruzov/starter-graph-based-eda-and-baseline-v1)"}],"metadata":{"kernelspec":{"display_name":"Python [conda env:graphs]","language":"python","name":"conda-env-graphs-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}