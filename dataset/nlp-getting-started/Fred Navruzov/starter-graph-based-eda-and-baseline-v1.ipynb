{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi guys!\n<br>Please find below detailed explanation of a given tweet dataset, \n<br>as well as ideas for baseline model construction\n\nThe notebook *is organized as follows*:\n\n# Table of Contents\n- [Preparation](#Preparation)\n    - [Additional packages installation](#Additional-Installations)\n    - [Imports](#Imports)\n    - [Data loading & merging](#Data-Loading)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n    - [Data Profiling](#Data-Profiling)\n    - [Target (`target` field)](#Target)\n    - [Keywords normalization (`keyword` field)](#Explore-keywords)\n    - [Tweet's entities (`text` field)](#Analyse-Tweet-Entities)\n        - [Hashtags](#Hashtags)\n        - [URLs](#URLs)\n        - [Users](#Users)\n    - [Location exploration (`location` field)](#Locations)\n    - [NLP-based stuff (WIP)](#NLP-based-stuff)\n- Baseline (WIP)"},{"metadata":{},"cell_type":"markdown","source":"# Preparation\n---"},{"metadata":{},"cell_type":"markdown","source":"## Additional Installations\n---"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# install additional dependencies\n\n# !pip install -U stellargraph[demos]  # graph-based embeddings, etc., for v2\n# !pip install geopy  # for direct/reverse geocoding, kaggle kernel already has it\n!pip install -U cufflinks  # interactive visualizations atop of Pandas and Plot.ly\n!pip install lemminflect  # spaCy add-on for lemmatization\n!pip install twitter-text-python  # for easier tweet entities extraction\n!pip install folium  # for neat geo-visualizations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Imports\n---"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Imports\nimport re\nfrom os.path import join as pjoin\n\nimport cufflinks as cf\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nfrom lemminflect import getInflection, getLemma\nfrom matplotlib import pyplot as plt\nfrom plotly.offline import init_notebook_mode\n\ninit_notebook_mode(connected=False)\ncf.go_offline()\n\npd.options.display.max_rows = 200\npd.options.display.max_columns = 200\npd.options.display.max_colwidth = 200\nplt.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loading\n---\nLet's load the data and create inline HTML report with `PandasProfiler` library \n<br>to see what's our data look like"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/nlp-getting-started/'\ntrain = pd.read_csv(pjoin(DATA_DIR, 'train.csv'))\ntest = pd.read_csv(pjoin(DATA_DIR, 'test.csv'))\n\n# glue datasets together, for convenience\ntrain['is_train'] = True\ntest['is_train'] = False\ndf = pd.concat(\n    [train, test], \n    sort=False, ignore_index=True\n).set_index('id').sort_index()\n\nprint(train.shape, test.shape, df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n---"},{"metadata":{},"cell_type":"markdown","source":"## Data Profiling\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get data description report (train)\npp.ProfileReport(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# do the same for test dataset\npp.ProfileReport(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target\n---\nLet's see how balanced our dataset is and whether there is a leak in data ordering\n<br> and/or train/test balancing within groups, ordered by `id` column\n<br>(to check if ordering matters)\n<br>P.s. However, such information (as explicit `row_id` data ordering in DWH) **shouldn't be used in a real ML pipeline**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# well, almost balanced\ndf.loc[df.is_train, 'target'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see how is train/test mixed within a step\nprint(f'train/test size ratio: {train.shape[0] / test.shape[0] :.2f}')\nprint(f'train share: {train.shape[0] / df.shape[0] :.2f}')\n\n# if balanced, this kpi should oscillate around 0.7\ngrid_step = 1000\ntr_share = df.sort_index().groupby(df.index // grid_step).agg({'is_train': 'mean'})\ntr_share.iplot(\n    dimensions=(640, 320), \n    title=f'Mean train share within groups, ordered by `id` column ({tr_share[\"is_train\"].mean():.2f})',\n    kind='bar'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check the same  kpi against mean target\n# well, not so uniform, even for bigger (1k) grid step\nprint(f'mean target: {train[\"target\"].mean() :.2f}')\ntarget_share = df.sort_index().groupby(df.index // grid_step).agg({'target': 'mean'})['target']\ntarget_share.iplot(\n    dimensions=(640, 320), \n    title=f'Mean target within groups, ordered by `id` column ({target_share.mean():.2f})',\n    kind='bar'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Explore keywords\n---\nOne may make fair assumption that there are different form of the same keyword, occuring in the dataset\n<br>Let's explore it and automatically map to the same lemma, based on installed [lemminflect](https://lemminflect.readthedocs.io/en/latest/) package"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare lemmatization schema (based on https://lemminflect.readthedocs.io/en/latest/)\ndef lemmatize(word, upos='VERB'):\n    try:\n        lemma = getLemma(word, upos=upos, lemmatize_oov=True)\n    except AssertionError:\n        lemma = getLemma(word, upos='VERB', lemmatize_oov=True)\n    if not lemma:  # empty tuple\n        lemma = getLemma(word, upos='VERB', lemmatize_oov=True)\n    return lemma[0].lower()\n\n\n# explore initial keywords, change space codes to underlines\ndf.keyword = df.keyword.fillna('NaN').astype(str).str.replace('(%20)+', '_')\nprint(df.keyword.nunique())\n\n# check top-N keywords stats\ntop_n = 10\ndf.groupby('keyword').agg(\n    tweet_cnt=pd.NamedAgg(column='target', aggfunc='count'),\n    mean_target=pd.NamedAgg(column='target', aggfunc='mean'),\n).sort_values(by=['tweet_cnt', 'mean_target'], ascending=[False, False]).head(top_n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare lemmatized mapping\nlemmatized = dict(\n    zip(\n        df.keyword.drop_duplicates(),\n        df.keyword.drop_duplicates().str.split('[_ ]+')\n        .apply(\n            lambda words: '_'.join(\n                lemmatize(w, upos='VERB') for w in sorted(words))  \n            # to get rid of the duplicates, like 'building_burning', 'building_burning'\n        )\n    )\n)\n\n# check reduction effect - 20% unique tag cnt drop\nprint(\n    len(set(lemmatized.keys())), \n    len(set(lemmatized.values())), \n    1 - len(set(lemmatized.values())) / len(set(lemmatized.keys()))\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check merge quality by visual inspection\n<br>Get only those lemmas with **2+ merged** candidates\n<br>Seems that merging was **valid**"},{"metadata":{"trusted":true},"cell_type":"code","source":"merged = pd.DataFrame(\n    data=lemmatized.items(), \n    columns=['init', 'new']\n).groupby('new')['init'].apply(list)\n\nmerged[merged.apply(len) > 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's substitute initial keywords with their merged lemmas \n# and see updated target mean\ndf['keyword_normalized'] = df.keyword.map(lemmatized)\nprint(df.keyword.nunique())\n\nkeywords_grouped = df.groupby('keyword_normalized').agg(\n    tweet_cnt=pd.NamedAgg(column='target', aggfunc='count'),\n    mean_target=pd.NamedAgg(column='target', aggfunc='mean'),\n).sort_values(by=['tweet_cnt', 'mean_target'], ascending=[False, False])\n\nkeywords_grouped.head(top_n*2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### WordCloud visualization\n---"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\n# generate 'disastrous' keywords\n\nwc_params = dict(\n    max_font_size=42,\n    background_color=None,\n    mode='RGBA',\n    max_words=200,\n    width=300,\n    height=300,\n    collocations=False,\n    relative_scaling=0.6,\n)\n\nwordcloud_disaster = WordCloud(**wc_params).generate(\n    ' '.join(\n        df.loc[\n            df['keyword_normalized'].replace({'nan': np.nan}).isin(\n                keywords_grouped[keywords_grouped['mean_target'] >= 0.5].index.tolist()\n            ), \n            'keyword_normalized'\n        ].values.tolist()\n    )\n)\n\nwordcloud_neutral = WordCloud(**wc_params).generate(\n    ' '.join(\n        df.loc[\n            df['keyword_normalized'].replace({'nan': np.nan}).isin(\n                keywords_grouped[keywords_grouped['mean_target'] <= 0.33].index.tolist()\n            ), \n            'keyword_normalized'\n        ].values.tolist()\n    )\n)\n\n# Display the generated image:\n# the matplotlib way:\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(7, 7))\nplt.title('`Disastrous` keywords')\nplt.imshow(wordcloud_disaster, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n\n\nplt.figure(figsize=(7, 7))\nplt.title('`Neutral` keywords')\nplt.imshow(wordcloud_neutral, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyse Tweet Entities\n---\n\nThis block is dedicated to extraction and analysis of tweet's entities:\n- #hashtags\n- @users\n- http://URLs\n\nFor that purpose we'll be using [this package by Edmond Burnett](https://github.com/edmondburnett/twitter-text-python)\n<br>It allows us to extract users/mentions, hashtags, follow the links, etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"from ttp import ttp\n\ntparser = ttp.Parser()\n\n\ndef extract_tweet_entities(tweet_text):\n    \"\"\"Extract entities from tweet given tweet's text\"\"\"\n    return tparser.parse(tweet_text)\n\n\ntweet_entities = {\n    tweet_id: extract_tweet_entities(text)\n    for (tweet_id, text) in df.reset_index()[['id', 'text']].values.tolist()\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hashtags\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['hashtags'] = df.index.map(tweet_entities)\ndf['hashtags'] = df['hashtags'].apply(lambda x: sorted(ht.lower() for ht in x.tags))\ndf['hashtag_cnt'] = df.hashtags.map(len)\n\n# check whether property 'has_hashtags' correlates with target\nprint(df[['target', 'hashtag_cnt']].clip(0, 1).corr())\n\npd.crosstab(\n    df.is_train.map({True: 'train', False: 'test'}),\n    df.hashtag_cnt.clip(0, 4),\n    normalize='index'\n).astype(str).rename(columns={4: '4+'}).iplot(\n    kind='bar',\n    title='Unique hashtags cnt (share) in train vs. test datasets',\n    dimensions=(640, 320)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check remaining hashtag candidates\n# it looks like garbage to me, ~2% of all tweets containing `#` sign\nprint(\n    df[(df.hashtag_cnt == 0) & df.text.str.contains('#')].shape[0] \n    / df[df.text.str.contains('#')].shape[0]\n)\n\ndf[(df.hashtag_cnt == 0) & df.text.str.contains('#')].sample(10, random_state=911)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see most popular hashtags as well as their **average target ratio**"},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'hashtags'\nhashtags_stats = df.loc[:, [col, 'target']].explode(col)\\\n    .reset_index(drop=True).groupby(col).agg(\n    tweet_cnt=pd.NamedAgg(column=col, aggfunc='count'),\n    mean_target=pd.NamedAgg(column='target', aggfunc='mean'),\n).sort_values(by=['tweet_cnt', 'mean_target'], ascending=[False, False])\n\nhashtags_stats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top \"disastrous\" hashtags\nhashtags_stats[\n    (hashtags_stats.tweet_cnt > 6)\n    & (hashtags_stats.mean_target > 0.75)\n].sort_values(by=['mean_target', 'tweet_cnt'], ascending=[False, False])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# top \"non-disastrous\" hashtags\nhashtags_stats[\n    (hashtags_stats.tweet_cnt > 6)\n    & (hashtags_stats.mean_target <= 0.25)\n].sort_values(by=['mean_target', 'tweet_cnt'], ascending=[True, False])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As far as there are tweets with **2+ hashtags within** we can try to adapt [graph-based](https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)) approaches\n<img src=\"http://www.buharainsaat.net/pics/b/56/565174_drawing-graph-python.png\" height=\"200\" width=\"300\" align=\"left\"/>\n\nto seek for hidden dependencies: let hashtags be **nodes** and we add an edge (connection) between 2 hashtags <-> they simultaneously occur within single tweet\n<br>Then we correspondingly update edge weigths for stronger connections to have bigger weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create hashtag graph\nfrom collections import defaultdict\nfrom itertools import combinations\n\nG_ht = nx.Graph()\n\n# add nodes\nG_ht.add_nodes_from(hashtags_stats.index.tolist())\n\n# add edges and set their weight\nd = defaultdict(int)\nfor taglist in df.hashtags:\n    for c in combinations(taglist, 2):\n        d[c] += 1\n        G_ht.add_edge((*c))\n\nfor n1, n2, attrs in G_ht.edges(data=True):\n    attrs['weight'] = np.log1p(d[(n1, n2)])\n\n# drop \"weak\" (ocassional) links\nprint(\n    f'Init stats:\\nNodes: {G_ht.number_of_nodes()}\\nEdges: {G_ht.number_of_edges()}')\nweak_edges = []\nfor n1, n2, attrs in G_ht.edges(data=True):\n    if attrs['weight'] <= np.log1p(1):\n        weak_edges.append((n1, n2))\nG_ht.remove_edges_from(weak_edges)\n\n# graph stats\nprint(\n    f'After weak edge removal:\\nNodes: {G_ht.number_of_nodes()}\\nEdges: {G_ht.number_of_edges()}')\n\n# get isolated tags and drop them from visualization\nisolates = list(nx.isolates(G_ht))\n\nG_ht.remove_nodes_from(isolates)\nprint(\n    f'After isolates removal:\\nNodes: {G_ht.number_of_nodes()}\\nEdges: {G_ht.number_of_edges()}')\n\n\nnode_degrees = pd.DataFrame(\n    [x for x in G_ht.degree()],\n    columns=['node', 'degree']\n).sort_values(by='degree', ascending=False).reset_index()\n\n# drop node with highest degree - tag #news\nG_ht.remove_node(node_degrees.iloc[0]['node'])\nprint(\n    f'After highest node degree removal:\\nNodes: {G_ht.number_of_nodes()}\\nEdges: {G_ht.number_of_edges()}')\n\ncc_ht = sorted(list(nx.connected_components(G_ht)),\n               key=lambda x: len(x), reverse=True)\n# remove very small components\nmin_size = 5\ncc_ht = [c for c in cc_ht if len(c) > min_size]\n\nprint(f'Top connected Components: {len(cc_ht)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's visualize top connected components as well as their mean targets + individual targets to see interconnection quality\n<br>Well, **almost all of them seems homogenous** among mean target, as well as **partially interpretable**\n\nUnfold 2 cells below to see visualization code"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\n\ndef draw_plotly_graph(G, pos, title, node_target_dict, node_degrees):\n    \"\"\"\n    Draw network data according to graph `G` and positions `pos`\n    https://plot.ly/python/network-graphs/\n    \"\"\"\n    edge_x = []\n    edge_y = []\n    for edge in G.edges():\n        x0, y0 = pos[edge[0]]\n        x1, y1 = pos[edge[1]]\n        edge_x.append(x0)\n        edge_x.append(x1)\n        edge_x.append(None)\n        edge_y.append(y0)\n        edge_y.append(y1)\n        edge_y.append(None)\n\n    edge_trace = go.Scatter(\n        x=edge_x,\n        y=edge_y,\n        line=dict(width=0.75, color='#888', dash='dot'),\n        hoverinfo='none',\n        mode='lines',\n    )\n\n    node_x = []\n    node_y = []\n    for node in G.nodes():\n        x, y = pos[node]\n        node_x.append(x)\n        node_y.append(y)\n\n    node_trace = go.Scatter(\n        x=node_x, \n        y=node_y,\n        mode='markers+text',\n#         hoverinfo='text',\n        marker=dict(\n            showscale=True,\n            colorscale='YlOrRd',\n            reversescale=False,\n            size=pd.Series(G.nodes).map(\n                np.power(node_degrees.set_index('node')['degree'], 1.2)\n            ).values.tolist(),\n            colorbar=dict(\n                thickness=15,\n                title='Node Connections',\n                xanchor='left',\n                titleside='right'\n            ),\n            line_width=2\n        ),\n        textposition='top center',\n        textfont=dict(family='arial', size=9.5, color='black', )\n    )\n\n    node_adjacencies = []\n    node_text = []\n    for node, adjacencies in enumerate(G.adjacency()):\n        node_adjacencies.append(len(adjacencies[1]))\n        node_text.append(f'<b>{adjacencies[0]}</b><br>({node_target_dict[adjacencies[0]]:.1f})')\n\n    node_trace.marker.color = node_adjacencies\n    node_trace.text = node_text\n\n    fig = go.Figure(\n        data=[edge_trace, node_trace],\n        layout=go.Layout(\n            title={\n                'text': title,\n                'xanchor': 'center',\n                'yanchor': 'top',\n                'y': 0.95,\n                'x': 0.5,\n            },\n            titlefont_size=16,\n            showlegend=False,\n            hovermode='closest',\n            margin=dict(b=20, l=5, r=5, t=40),\n            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n            width=600,\n            height=400,\n        )\n    )\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# plot main connected components and their mean target\nhashtags_flattened = df.loc[:, [col, 'target']]\\\n.explode(col).reset_index(drop=True)\n\nhashtags_agg_target = hashtags_flattened.groupby(col).agg({'target': 'mean'})['target']\n\n# uncomment all data instead of top-n\nfor (i, c) in enumerate(cc_ht[:4]):\n    mean_target = hashtags_flattened[hashtags_flattened[col].isin(c)]['target'].mean()\n#     ax[i].title.set_text(f'Mean target: {mean_target:.2f}')\n    sG = G_ht.subgraph(c)\n    pos = nx.spring_layout(sG, seed=42)\n    \n    draw_plotly_graph(\n        G=sG, \n        pos=pos, \n        title=f'Mean target: {mean_target:.2f}', \n        node_target_dict=hashtags_agg_target, \n        node_degrees=node_degrees\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add obtained user components as cluster features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\n\ndf['hashtag_cluster'] = [[] for i in range(len(df))]\nfor (ri, r) in tqdm(list(df.iterrows()), total=len(df)):\n    for (i, c) in enumerate(cc_ht):\n        if set(r['hashtags']).intersection(c):\n            df.loc[ri, 'hashtag_cluster'].append(i)\n    if not df.loc[ri, 'hashtag_cluster']:\n        df.loc[ri, 'hashtag_cluster'].append(-1)\n\n# add connected component mapping as a features\nt = df.explode('hashtag_cluster')['hashtag_cluster']\\\n.reset_index().rename(columns={'index': 'id'})\n\ndf_ht_clusters = pd.crosstab(t.id, t.hashtag_cluster)\ndf_ht_clusters.columns = [f'ht_cluster__{c}' for c in df_ht_clusters.columns]\n\nprint(df.shape)\ndf = df.drop(df.filter(regex='^ht_cluster').columns, axis=1)\ndf = pd.concat([df, df_ht_clusters], axis=1)\nprint(df.shape)\n\ndf_ht_clusters.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### URLs\n---\nMany tweets have URLs within its body -\n<br>let's see whether we can extract something meaningful from this data\n<br>It seems that URL presence **increases** chances of a tweet being `disastrous`\n(At least in this particular dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['urls'] = df.index.map(tweet_entities)\ndf['urls'] = df['urls'].apply(lambda x: sorted(ht.lower() for ht in x.urls))\ndf['url_cnt'] = df.urls.map(len)\n\n# check whether property 'has_url' correlates with target\nprint(df[['target', 'url_cnt']].clip(0, 1).corr())\n\npd.crosstab(\n    df.is_train.map({True: 'train', False: 'test'}),\n    df.url_cnt.clip(0, 3),\n    normalize='index'\n).astype(str).rename(columns={3: '3+'}).iplot(\n    kind='bar',\n    title='Unique URLs cnt (share) in train vs. test datasets',\n    dimensions=(640, 320)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get most popular links\ncol = 'urls'\nurl_stats = df.loc[:, [col, 'target']].explode(col)\\\n    .reset_index(drop=True).groupby(col).agg(\n    tweet_cnt=pd.NamedAgg(column=col, aggfunc='count'),\n    mean_target=pd.NamedAgg(column='target', aggfunc='mean'),\n).sort_values(by=['tweet_cnt', 'mean_target'], ascending=[False, False])\n\nprint(f'distinct urls mentions: {len(url_stats)}')\nprint(f'mentioned 2+ times: \\t{(url_stats.tweet_cnt > 2).sum()}')\n\nurl_stats[url_stats.tweet_cnt > 4].sort_values(by='mean_target', ascending=False)\n\n# one may try to follow the links with `utils` module\n# >>>import ttp\n# >>>tparser = ttp.Parser()\n# >>>result = tparser.parse(tweet)\n# >>>ttp.utils.follow_shortlinks(result.urls)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Users\n---\nMany tweets have user mentions within its body -\n<br>let's see whether we can extract something meaningful from this data and build user graph\n<br>It seems that user mention presence **decreases** chances of a tweet being `disastrous`\n(At least in this particular dataset)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['users'] = df.index.map(tweet_entities)\ndf['users'] = df['users'].apply(lambda x: sorted(ht.lower() for ht in x.users))\ndf['user_cnt'] = df.users.map(len)\n\n# check whether property 'has_user_ref' correlates with target\nprint(df[['target', 'user_cnt']].clip(0, 1).corr())\n\npd.crosstab(\n    df.is_train.map({True: 'train', False: 'test'}),\n    df.user_cnt.clip(0, 3),\n    normalize='index'\n).astype(str).rename(columns={3: '3+'}).iplot(\n    kind='bar',\n    title='Unique users cnt (share) in train vs. test datasets',\n    dimensions=(640, 320)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = 'users'\nuser_stats = df.loc[:, [col, 'target']].explode(col)\\\n    .reset_index(drop=True).groupby(col).agg(\n    tweet_cnt=pd.NamedAgg(column=col, aggfunc='count'),\n    mean_target=pd.NamedAgg(column='target', aggfunc='mean'),\n).sort_values(by=['tweet_cnt', 'mean_target'], ascending=[False, False])\n\nprint(f'distinct user mentions: {len(user_stats)}')\nprint(f'mentioned 2+ times: \\t{(user_stats.tweet_cnt > 2).sum()}')\n\nuser_stats[user_stats.tweet_cnt > 5].sort_values(by='mean_target', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfold cells below to see visualization code"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# create user graph\nG_u = nx.Graph()\n\n# add nodes\nG_u.add_nodes_from(user_stats.index.tolist())\n\n# add edges and set their weight\nd = defaultdict(int)\nfor ulist in df.users:\n    for c in combinations(ulist, 2):\n        d[c] += 1\n        G_u.add_edge((*c))\n\nfor n1,n2,attrs in G_u.edges(data=True):\n    attrs['weight'] = np.log1p(d[(n1, n2)])\n\n# drop \"weak\" (ocassional) links\nprint(f'Init stats:\\nNodes: {G_u.number_of_nodes()}\\nEdges: {G_u.number_of_edges()}')\nweak_edges = []\nfor n1,n2,attrs in G_u.edges(data=True):\n    if attrs['weight'] <= np.log1p(0):\n        weak_edges.append((n1, n2))\nG_u.remove_edges_from(weak_edges)\n    \n# graph stats\nprint(f'After weak edge removal:\\nNodes: {G_u.number_of_nodes()}\\nEdges: {G_u.number_of_edges()}')\n\n# get isolated tags and drop them from visualization\nisolates = list(nx.isolates(G_u))\n\nG_u.remove_nodes_from(isolates)\nprint(f'After isolates removal:\\nNodes: {G_u.number_of_nodes()}\\nEdges: {G_u.number_of_edges()}')\n\n\nnode_degrees = pd.DataFrame(\n    [x for x in G_u.degree()], \n    columns=['node', 'degree']\n).sort_values(by='degree', ascending=False).reset_index(drop=True)\n\n# drop node with highest degree - tag #news\nprint(node_degrees.iloc[0]['node'])\nG_u.remove_node(node_degrees.iloc[0]['node'])\nprint(f'After highest node degree removal:\\nNodes: {G_u.number_of_nodes()}\\nEdges: {G_u.number_of_edges()}')\n\ncc_u = sorted(list(nx.connected_components(G_u)), key=lambda x: len(x), reverse=True)\n# remove very small components\nmin_size = 5\ncc_u = [c for c in cc_u if len(c) > min_size]\n\nprint(f'Top connected Components: {len(cc_u)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what popular user's network consists of\n<br>We can clearly see groups with \n- perfect 0.00, like 4th group with porn-based stuff :) \n- or 1.00 in (observed in train) nodes ->\n\nOne may inference those group target mean to (observed in test) nodes -> dive into tweets level.\n<br>Be careful, some of the connected components exists only in **test dataset** (those with perfect `NaNs`)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# plot main connected components and their mean target\nusers_flattened = df.loc[:, [col, 'target']]\\\n.explode(col).reset_index(drop=True)\n\nusers_agg_target = users_flattened.groupby(col).agg({'target': 'mean'})['target']\n\n# uncomment all data instead of top-n\nfor (i, c) in enumerate(cc_u[:4]):\n    mean_target = users_flattened[users_flattened[col].isin(c)]['target'].mean()\n    sG = G_u.subgraph(c)\n    pos = nx.spring_layout(sG, seed=42)\n    \n    draw_plotly_graph(\n        G=sG, \n        pos=pos, \n        title=f'Mean target: {mean_target:.2f}', \n        node_target_dict=users_agg_target, \n        node_degrees=node_degrees\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Locations\n---\nLet's see what secrets are hidden in the `location` field.\n<br>We'll perform basic data cleaning, as well as direct/inverse geocoding with the help of [geopy package](https://geopy.readthedocs.io/en/stable/)\n<br>Then we plot the heatmap using [folium package](https://python-visualization.github.io/folium/)\n\n**UPD**: To prevent ArcGIS API from our kaggle-DDoSing, I've created **[separate dataset atop of geocoded location data](https://www.kaggle.com/frednavruzov/disaster-tweets-geodata)**\n<br>You can find collection details & scripts in [this starter notebook](https://www.kaggle.com/frednavruzov/starter-how-the-data-was-collected/edit)\n<br>Further contribution is always welcomed!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read parsed geodata\ngeodata = pd.read_csv('../input/disaster-tweets-geodata/geodata.csv').set_index('id')\ngeodata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# append geodata to the initial dataframe\nprint(df.shape)\ndf = df.merge(geodata, left_index=True, right_index=True, how='left')\n# drop possible duplicates\ndf = df[~df.index.duplicated(keep='first')]\nprint(df.shape)\ndf[['location', 'target'] + geodata.columns.tolist()].sample(5, random_state=911)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print location target (by country)\ndf.groupby(df.country.replace('', np.nan)).agg({'target': 'mean', 'location': 'count'})\\\n.sort_values(by=['location', 'target'], ascending=[False, False]).head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfold cells below to see visualization code"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from folium import plugins\nimport folium\n\n\n# inspired by https://alysivji.github.io/getting-started-with-folium.html\ndef map_points(df, lat_col='latitude', lon_col='longitude', zoom_start=11, \\\n                plot_points=False, pt_radius=15, \\\n                draw_heatmap=False, heat_map_weights_col=None, \\\n                heat_map_weights_normalize=True, heat_map_radius=15):\n    \"\"\"Creates a map given a dataframe of points. Can also produce a heatmap overlay\n\n    Arg:\n        df: dataframe containing points to maps\n        lat_col: Column containing latitude (string)\n        lon_col: Column containing longitude (string)\n        zoom_start: Integer representing the initial zoom of the map\n        plot_points: Add points to map (boolean)\n        pt_radius: Size of each point\n        draw_heatmap: Add heatmap to map (boolean)\n        heat_map_weights_col: Column containing heatmap weights\n        heat_map_weights_normalize: Normalize heatmap weights (boolean)\n        heat_map_radius: Size of heatmap point\n\n    Returns:\n        folium map object\n    \"\"\"\n\n    ## center map in the middle of points center in\n    middle_lat = df[lat_col].median()\n    middle_lon = df[lon_col].median()\n\n    curr_map = folium.Map(location=[middle_lat, middle_lon],\n                          zoom_start=zoom_start)\n\n    # add points to map\n    if plot_points:\n        for _, row in df.iterrows():\n            folium.CircleMarker([row[lat_col], row[lon_col]],\n                                radius=pt_radius,\n                                popup=row['location'],\n                                fill_color=\"#3db7e4\", # divvy color\n                               ).add_to(curr_map)\n\n    # add heatmap\n    if draw_heatmap:\n        # convert to (n, 2) or (n, 3) matrix format\n        if heat_map_weights_col is None:\n            cols_to_pull = [lat_col, lon_col]\n        else:\n            # if we have to normalize\n            if heat_map_weights_normalize:\n                df[heat_map_weights_col] = \\\n                    df[heat_map_weights_col] / df[heat_map_weights_col].sum()\n\n            cols_to_pull = [lat_col, lon_col, heat_map_weights_col]\n\n        stations = df[cols_to_pull].values\n        curr_map.add_child(plugins.HeatMap(stations, radius=heat_map_radius))\n\n    return curr_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_points(\n    df=df[\n        df.is_train \n        & ~df.lat.isnull()\n    ].sample(500, random_state=911) , # for memory constrains\n    lat_col='lat',\n    lon_col='lon', \n    zoom_start=2, # change to `1` to set global world view\n    plot_points=False, \n    pt_radius=0.75,\n    draw_heatmap=True, \n    heat_map_weights_col='target',\n    heat_map_weights_normalize=True,\n    heat_map_radius=15,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## NLP-based stuff\n---\nLet's see how natural language processing techniques can help us in solving this particular binary classification problem"},{"metadata":{"trusted":true},"cell_type":"code","source":"# borrowed some hot stuff from https://www.kaggle.com/jdparsons/tweet-cleaner\n\n# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\nEMOJI_REGEX = re.compile(\n    \"[\"\n    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n    u\"\\U00002702-\\U000027B0\"\n    u\"\\U000024C2-\\U0001F251\"\n    \"]+\",\n    flags=re.UNICODE\n)\n\nNON_WORD_PATTERN = r\"[^A-Za-z0-9\\.\\'!\\?,\\$\\s]\"\n\n# https://stackoverflow.com/a/35041925\n# replace multiple punctuation with single. Ex: !?!?!? would become ?\nPUNCT_DEDUPLICATION_REGEX = re.compile(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', re.I)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.parsing.preprocessing import STOPWORDS\nfrom ttp.ttp import HASHTAG_REGEX, URL_REGEX, USERNAME_REGEX\n\n# substitute links, urls, mentions, hashtags etc.\ndf['text_processed'] = df.text.str.replace(EMOJI_REGEX, ' EMOJI ')\\\n.str.replace(PUNCT_DEDUPLICATION_REGEX, '')\\\n.str.replace(URL_REGEX, ' URL ').str.replace(NON_WORD_PATTERN, '').str.replace('\\s+', ' ')\n\ndf['text_processed'].sample(10, random_state=911).values.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build word wectors from `text` field"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import FastText, Word2Vec\n\n\ndef compute_doc2vec(words, w2v, weight_dict, emb_size):\n    \"\"\"\n    given word embeddings, compute weighted (by term frequency) \n    doc embeddings\n    \"\"\"\n    doc_vector = np.zeros(emb_size, dtype=np.float32)\n    weights = 0\n    for w in words:\n        try:\n            doc_vector += w2v.wv[w]\n            weights += weight_dict.get(w, 0)\n\n        except KeyError:\n            pass\n\n    doc_vector /= max(0.01, weights)\n    return doc_vector / np.linalg.norm(doc_vector, ord=2)\n\ntext_field = 'text_processed'\nprepared_texts = df[text_field].str.lower().str.replace(\n#     '(\\\\b' + '\\\\b|\\\\b'.join(STOPWORDS) + ')', ''\n    '', ''\n).str.extractall(\n    '(?P<words>[A-Za-z]+)').groupby(level=0)['words'].apply(list).reindex(df.index).fillna('EMPTY')\n\nw2v_size = 16\n\nw2v = Word2Vec(\n    sentences=prepared_texts,\n    size=w2v_size,\n    window=32,\n    min_count=7,\n    iter=5,\n    negative=5,\n    sg=1,\n    batch_words=2**11,\n    #     min_n=3,\n    #     max_n=4,\n)\n\n\nword_dict = np.log1p(prepared_texts.explode().value_counts()).to_dict()\n\n\nw2v_df = pd.DataFrame(\n    data=np.concatenate(\n        prepared_texts.apply(\n            #             lambda x: np.array([w2v.wv[w] if w in w2v.wv else np.zeros(w2v_size) for w in x]).mean(axis=0)\n            lambda x: compute_doc2vec(\n                x, w2v, weight_dict=word_dict, emb_size=w2v_size)\n        ).values\n    ).reshape((len(df), w2v_size)),\n    columns=[f'w2v__{i}' for i in range(w2v_size)]\n)\n\n\n# join with the main dataset\ndf = df.drop(df.filter(regex='^w2v__').columns, axis=1)\nprint(df.shape)\ndf = pd.concat([df, w2v_df], axis=1)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encode tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"hashtags_sentences = df.explode('hashtags')['hashtags'].groupby(level=[0]).apply(\n#     lambda x: ' '.join(re.sub(r'[^\\x00-\\x7f]', r'', s) for s in list(x.astype(str)))\n    lambda x: ' '.join([re.sub(r'[^\\x00-\\x7f]', r'', s) for s in list(x.astype(str))])\n)\n\nhashtags_sentences.sample(10, random_state=911)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n\ntfidf = TfidfVectorizer(\n    max_df=1.0, # drop all words, whose document/sentence share > share (if float)\n    min_df=5,  # drop all words, whose count < x (if int)\n    dtype=np.float32,\n    max_features=200,\n)\n\ntfidf.fit(hashtags_sentences)\ntag_df = pd.DataFrame(\n    data=tfidf.transform(hashtags_sentences).todense(),\n    columns=[f'tag__{k}' for k, v in sorted(tfidf.vocabulary_.items(), key=lambda item: item[1])]\n)\n\nprint(tag_df.shape)\ntag_df.head()\n\n# join with the main dataset\ndf = df.drop(df.filter(regex='^tag__').columns, axis=1)\nprint(df.shape)\ndf = pd.concat([df, tag_df], axis=1)\n# df['tag__encoding'] = np.array(tfidf.transform(hashtags_sentences).todense().argmax(axis=1)).ravel()\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare Universal sentence encoder embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_hub as hub\n\nembedder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n\n# we have to wait a little :)\ndf_univ_embeddings = pd.DataFrame(\n    data=embedder(df[text_field].values).numpy(), \n    columns=[f'tf_emb__{i}' for i in range(512)]\n)\n\ndf_univ_embeddings.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# join with the main dataset\ndf = df.drop(df.filter(regex='^tf_emb__').columns, axis=1)\nprint(df.shape)\ndf = pd.concat([df, df_univ_embeddings], axis=1)\nprint(df.shape)\n\nprint(f'Memory usage, Mb: {df.memory_usage(deep=True).sum() // 2**20}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some basic NLP feature engineering\ndf['text_len'] = df.text.apply(len).astype(np.int16)\ndf['text_word_cnt'] = df.text.str.split('\\s+').map(len).astype(np.int16)\ndf['has_exclamations'] = df.text.str.contains(r'[?!]').astype(np.int8)\ndf['has_uppercased'] = df.text.str.contains(r'[A-Z]{2,}').astype(np.int8)\n\nnlp_features = [\n    'has_exclamations', \n    'has_uppercased',\n    'text_len',\n    'text_word_cnt',\n]\n\ndf[nlp_features + ['target']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"features_num = (\n    [\n        'hashtag_cnt',\n        'url_cnt',\n        'user_cnt',\n        'lat',\n        'lon'\n    ]\n#     + nlp_features\n    + tag_df.columns.tolist()\n    + w2v_df.columns.tolist()\n    + df_ht_clusters.columns.tolist()\n    + df_univ_embeddings.columns.tolist()\n)\n\nfeatures_cat = [\n    'country',\n    'city',\n    'keyword_normalized',\n#     'tag__encoding'\n]\n\ncat_cols = []\nfor col in features_cat:\n    df[f'{col}_code'] = df[col].astype('category').cat.codes\n    cat_cols.append(f'{col}_code')\n    \nfeatures = features_num + cat_cols\n    \nprint(df[features].dtypes)\n\ndf[features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross-validation"},{"metadata":{},"cell_type":"markdown","source":"We'll use LightGBM model (boosting) due to its **natural strengths**\n<br>(speed, scale-invariance, NaN handling, etc.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\n\ndef f1_score_lgb(preds, dtrain):\n    labels = dtrain.get_label()\n    f_score = f1_score(\n        np.round(preds), \n        labels, \n        average='macro'\n    )\n    return 'f1_score', f_score, True\n\n\nskf = StratifiedKFold(n_splits=5, random_state=911, shuffle=True)\n\nlgb_params = {\n#     'class_weight': 'balanced',\n    'num_leaves': 63,\n    'learning_rate': 0.01,\n    'max_depth': -1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.25,\n    'cat_l2': 10.,\n    'cat_smooth': 10.,\n    'min_data_per_group': 20,\n    'min_data_in_leaf': 20,\n    'reg_lambda': 0.5,\n    'boost_from_average': True,\n    \n    'max_cat_threshold': 64,\n    'max_bin': 255,\n    'min_data_in_bin': 5,\n#     'scale_pos_weight': 1 / df.loc[df.is_train, 'target'].mean()\n    #     ''\n}\n\nsample_weight = (\n    1 / df.loc[\n        df.is_train, \n        'target'\n    ].map(df.loc[df.is_train, 'target'].value_counts(normalize=True))\n)\n\n\ncv_res = lgb.cv(\n    params=lgb_params,\n    train_set=lgb.Dataset(\n        data=df.loc[df.is_train, features],\n        label=df.loc[df.is_train, 'target'],\n        categorical_feature=cat_cols,\n        weight=sample_weight\n    ),\n    folds=skf,\n    metrics=['binary_logloss'],\n    feval=f1_score_lgb,\n    verbose_eval=50,\n    early_stopping_rounds=200,\n    #     eval_train_metric=True,\n    num_boost_round=1000,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit simple model, based on cv rounds\n\nmodel = lgb.LGBMClassifier(\n    **lgb_params, \n    n_esimators=int( len(cv_res['binary_logloss-mean']) * (skf.n_splits + 1)/skf.n_splits )\n)\n\nmodel.fit(\n    X=df.loc[df.is_train, features], \n    y=df.loc[df.is_train, 'target'], \n    sample_weight=sample_weight,\n    categorical_feature=cat_cols,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check feature importance\nlgb.plot_importance(\n    model, \n    importance_type='gain', \n    figsize=(10, 10), \n    max_num_features=50\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get predictions\ny_pred = model.predict(df.loc[~df.is_train, features])\n\n# prepare submission\npd.DataFrame({'target': y_pred, 'id': df[~df.is_train].index}).astype(np.int32).to_csv(\n    'submission.csv', \n    index=False, \n    encoding='utf-8'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---\nThat's all for now\n<br>Stay tuned, this notebook is going to be updated soon\n<br>Hope, you guys, like it and learn something new!\n<br>**As always, upvotes, comments, ideas are always welcome!**\n\n---\nP.s. Check my [F1-Score metric analysis notebook](https://www.kaggle.com/frednavruzov/be-careful-with-f1-score-shuffle-estimate)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}