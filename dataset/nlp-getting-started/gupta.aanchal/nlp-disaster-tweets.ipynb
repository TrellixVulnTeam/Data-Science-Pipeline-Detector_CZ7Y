{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\",encoding=\"latin1\")\ntest_data=pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\",encoding=\"latin1\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Id=test_data.id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning "},{"metadata":{"trusted":true},"cell_type":"code","source":"import re \ntrain_data['text'] = [re.sub(\"[^a-zA-Z]\",\" \",text).lower() for text in train_data['text']]\ntest_data['text'] = [re.sub(\"[^a-zA-Z]\",\" \",text).lower() for text in test_data['text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\n\ndef polarity_check_label(dataframe):\n    polarity_list = []\n    \n    for text in dataframe['text']:\n        polarity_point = TextBlob(text).sentiment.polarity\n        if polarity_point < 0 : polarity_state= 'Negative'\n        elif polarity_point == 0 : polarity_state = 'Neutral'\n        else : polarity_state = 'Positive'\n        polarity_list.append(polarity_state)\n        \n    dataframe['polarity_state'] = polarity_list\n    \npolarity_check_label(train_data)\npolarity_check_label(test_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stopping Irrelavnt Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk # natural language tool kit for word_tokenize ...\nnltk.download(\"stopwords\")      # stopwords is download in corpus directory\nfrom nltk.corpus import stopwords  # import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['text'] = [nltk.word_tokenize(text) for text in train_data['text']]\ntest_data['text'] = [nltk.word_tokenize(text) for text in test_data['text']]\n\ntrain_data.text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemma_and_join(dataframe):\n    lemma = nltk.WordNetLemmatizer()\n    text_list = []\n    for text in dataframe['text']:\n        text = [ word for word in text if not word in set(stopwords.words(\"english\"))]\n        text = [lemma.lemmatize(word) for word in text]\n        text = \" \".join(text)\n        text_list.append(text)\n    return text_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['text'] = lemma_and_join(train_data)\ntest_data['text'] = lemma_and_join(test_data)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_loc = train_data.location.value_counts()\ntop_loc_disaster = list(raw_loc[raw_loc>=10].index)\ntop_only_disaster = train_data[train_data.location.isin(top_loc_disaster)]\n\ntop_location = top_only_disaster.groupby('location')['target'].mean().sort_values(ascending=False)\nsns.barplot(x=top_location.index, y=top_location)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(train_data['location'])) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_location(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    elif x in top_location:\n        return x\n    else: \n        return 'Others'\n    \ntrain_data['location'] = train_data['location'].apply(lambda x: clean_location(str(x)))\ntest_data['location'] = test_data['location'].apply(lambda x: clean_location(str(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_location = train_data.groupby('location')['target'].mean().sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_location.index, y=top_location)\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(train_data['location'])) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(test_data['location'])) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_df = train_data['text'].apply(lambda x:pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\nfreq_df.columns = ['words', 'frequences']\nfreq_df.sort_values('frequences',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nmax_features=5000\n\ncount_vectorizer = CountVectorizer(max_features=max_features)\n\nsparce_matrix_train=count_vectorizer.fit_transform(train_data['text'])\nsparce_matrix_test=count_vectorizer.fit_transform(test_data['text'])\n\nprint(\"{} most used words: {} \".format(max_features,count_vectorizer.get_feature_names()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from textblob import TextBlob\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\n\nfeatures = ['keyword', 'location']\nencoder = ce.TargetEncoder(cols=features)\nencoder.fit(train_data[features],train_data['target'])\n\ntrain_data = train_data.join(encoder.transform(train_data[features]).add_suffix('_target'))\ntest_data = test_data.join(encoder.transform(test_data[features]).add_suffix('_target'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \ntext_vec = vec_text.fit_transform(train_data['text'])\ntext_vec_test = vec_text.transform(test_data['text'])\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.join(X_train_text, rsuffix='_text')\ntest_data=test_data.join(X_test_text,rsuffix='_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,precision_score,recall_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_data.target\nx = train_data.drop(columns = ['id','keyword', 'location', 'text','polarity_state','target'])\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building and Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score_list=[]\ntrain_accuracy_list=[]\ntest_accuracy_list = []\nclassifier_list = []\ndef fit_and_predict(model,x_train,x_test,y_train,y_test):\n    \n    classifier = model\n    classifier.fit(x_train,y_train)\n    y_pred = classifier.predict(x_test)\n    cmatrix = confusion_matrix(y_test,y_pred)\n    \n    \n    f,ax = plt.subplots(figsize=(3,3))\n    sns.heatmap(cmatrix,annot=True,linewidths=0.5,cbar=False,linecolor=\"red\",fmt='.0f',ax=ax)\n    plt.xlabel(\"y_predict\")\n    plt.ylabel(\"y_true\")\n    ax.set(title=str(classifier))\n    plt.show()\n    \n    \n    f1score = f1_score(y_test,y_pred,average='weighted')\n    train_accuracy = round(classifier.score(x_train,y_train)*100)\n    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)\n    \n    classifier_list.append(str(classifier))\n    train_accuracy_list.append(str(train_accuracy))\n    test_accuracy_list.append(str(test_accuracy))\n    f1_score_list.append(str(round(f1score*100)))\n    \n    \n    print(classification_report(y_test,y_pred))\n    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))\n    print('-'*50)\n    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 29\nmodels=[\n        LogisticRegression(random_state=29),\n        SVC(random_state=random_state),\n        MultinomialNB(),\n        DecisionTreeClassifier(random_state = 29),\n        KNeighborsClassifier(),\n        RandomForestClassifier(random_state=29),\n       ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in models:\n    fit_and_predict(model,x_train,x_test,y_train,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"As above, RandomForestClassifier has the best accuracy so far. Hurrayy!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=x_train.columns\ncolumns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test_data.reindex(columns = columns, fill_value=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(test)\npred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submission = pd.DataFrame(data=pred,columns=['Traget'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"id\":Id, \"target\": pred})\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}