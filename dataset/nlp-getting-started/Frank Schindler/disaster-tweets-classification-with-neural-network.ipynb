{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-26T18:19:30.457383Z","iopub.execute_input":"2022-05-26T18:19:30.457699Z","iopub.status.idle":"2022-05-26T18:19:30.466649Z","shell.execute_reply.started":"2022-05-26T18:19:30.457666Z","shell.execute_reply":"2022-05-26T18:19:30.465701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:30.483377Z","iopub.execute_input":"2022-05-26T18:19:30.483808Z","iopub.status.idle":"2022-05-26T18:19:30.488691Z","shell.execute_reply.started":"2022-05-26T18:19:30.483776Z","shell.execute_reply":"2022-05-26T18:19:30.487699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the data, and separate the target\npath = \"/kaggle/input/nlp-getting-started/train.csv\"\ndtrain = pd.read_csv(path)\npath1 = \"/kaggle/input/nlp-getting-started/test.csv\"\ndtest = pd.read_csv(path1)\n\n\ndf = pd.concat((dtrain, dtest))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:30.505975Z","iopub.execute_input":"2022-05-26T18:19:30.506458Z","iopub.status.idle":"2022-05-26T18:19:30.544356Z","shell.execute_reply.started":"2022-05-26T18:19:30.506422Z","shell.execute_reply":"2022-05-26T18:19:30.543366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Exploration**","metadata":{}},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:30.545719Z","iopub.execute_input":"2022-05-26T18:19:30.545959Z","iopub.status.idle":"2022-05-26T18:19:30.561656Z","shell.execute_reply.started":"2022-05-26T18:19:30.545929Z","shell.execute_reply":"2022-05-26T18:19:30.561047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## How many are positive (1) and negative (0) in percent:\nprint(dtrain['target'].mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:30.562999Z","iopub.execute_input":"2022-05-26T18:19:30.563382Z","iopub.status.idle":"2022-05-26T18:19:30.579179Z","shell.execute_reply.started":"2022-05-26T18:19:30.563341Z","shell.execute_reply":"2022-05-26T18:19:30.578106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['keyword'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:30.580695Z","iopub.execute_input":"2022-05-26T18:19:30.581502Z","iopub.status.idle":"2022-05-26T18:19:30.594487Z","shell.execute_reply.started":"2022-05-26T18:19:30.581453Z","shell.execute_reply":"2022-05-26T18:19:30.5936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df['keyword'].isna().sum()/len(df['keyword']))\n\nprint(df['location'].isna().sum()/len(df['location']))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:30.603662Z","iopub.execute_input":"2022-05-26T18:19:30.604581Z","iopub.status.idle":"2022-05-26T18:19:30.611838Z","shell.execute_reply.started":"2022-05-26T18:19:30.604545Z","shell.execute_reply":"2022-05-26T18:19:30.611143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(['keyword'])['target'].mean()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:30.629024Z","iopub.execute_input":"2022-05-26T18:19:30.629826Z","iopub.status.idle":"2022-05-26T18:19:30.639353Z","shell.execute_reply.started":"2022-05-26T18:19:30.629787Z","shell.execute_reply":"2022-05-26T18:19:30.638488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['location'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:30.645054Z","iopub.execute_input":"2022-05-26T18:19:30.645516Z","iopub.status.idle":"2022-05-26T18:19:30.655402Z","shell.execute_reply.started":"2022-05-26T18:19:30.645471Z","shell.execute_reply":"2022-05-26T18:19:30.65469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature extraction**","metadata":{}},{"cell_type":"markdown","source":"1.) convert to lowercase","metadata":{}},{"cell_type":"code","source":"df['text'] = (df['text']).str.lower()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:30.667144Z","iopub.execute_input":"2022-05-26T18:19:30.667586Z","iopub.status.idle":"2022-05-26T18:19:30.678086Z","shell.execute_reply.started":"2022-05-26T18:19:30.667553Z","shell.execute_reply":"2022-05-26T18:19:30.677381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.) Handling location data","metadata":{}},{"cell_type":"code","source":"df['location'] = df['location'].fillna('None')\ndf['keyword'] = df['keyword'].fillna('None')\n\nloc = df.location.unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:30.685959Z","iopub.execute_input":"2022-05-26T18:19:30.686754Z","iopub.status.idle":"2022-05-26T18:19:30.695831Z","shell.execute_reply.started":"2022-05-26T18:19:30.686717Z","shell.execute_reply":"2022-05-26T18:19:30.695079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make new list for locations which appear at least 10 times, all other rows will get location 'Unknown'","metadata":{}},{"cell_type":"code","source":"listt = []\nfor l in loc:\n    if df.location.value_counts()[l] > 9:\n        listt.append(l)\n        \nprint(listt)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:30.709796Z","iopub.execute_input":"2022-05-26T18:19:30.710235Z","iopub.status.idle":"2022-05-26T18:19:40.862399Z","shell.execute_reply.started":"2022-05-26T18:19:30.7102Z","shell.execute_reply":"2022-05-26T18:19:40.861419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['location'] = [ll if ll in listt else 'Unknown' for ll in df['location']]\n\nprint(df.location.unique())\nprint(len(df.location.unique()))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:40.864348Z","iopub.execute_input":"2022-05-26T18:19:40.865191Z","iopub.status.idle":"2022-05-26T18:19:40.882042Z","shell.execute_reply.started":"2022-05-26T18:19:40.865121Z","shell.execute_reply":"2022-05-26T18:19:40.880967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Closer inspection of the locations kept shows that some locations appear multiple times and can be grouped like 'US' with 'USA' or 'New York' and 'New York, NY'","metadata":{}},{"cell_type":"code","source":"df['location'] = np.where(df['location'].isin(['USA', 'United States']),'US',df['location'])\ndf['location'] = np.where(df['location'].isin(['United Kingdom']),'UK',df['location'])\ndf['location'] = np.where(df['location'].isin(['London, UK', 'London, England']),'London',df['location'])\ndf['location'] = np.where(df['location'].isin(['San Francisco, CA']),'San Francisco',df['location'])\ndf['location'] = np.where(df['location'].isin(['Washington, D.C.']),'Washington, DC',df['location'])\ndf['location'] = np.where(df['location'].isin(['Los Angeles, CA']),'Los Angeles',df['location'])\ndf['location'] = np.where(df['location'].isin(['New York, NY', 'New York City', 'NYC']),'New York',df['location'])\ndf['location'] = np.where(df['location'].isin(['California, USA']),'California',df['location'])\ndf['location'] = np.where(df['location'].isin(['Chicago, IL']),'Chicago',df['location'])\ndf['location'] = np.where(df['location'].isin(['Denver, CO', 'Denver, Colorado']),'Denver',df['location'])\ndf['location'] = np.where(df['location'].isin(['Seattle, WA']),'Seattle',df['location'])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:40.883377Z","iopub.execute_input":"2022-05-26T18:19:40.883678Z","iopub.status.idle":"2022-05-26T18:19:40.907948Z","shell.execute_reply.started":"2022-05-26T18:19:40.883644Z","shell.execute_reply":"2022-05-26T18:19:40.907254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.groupby(['location'])['target'].mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:40.910105Z","iopub.execute_input":"2022-05-26T18:19:40.911196Z","iopub.status.idle":"2022-05-26T18:19:40.920727Z","shell.execute_reply.started":"2022-05-26T18:19:40.911112Z","shell.execute_reply":"2022-05-26T18:19:40.919722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the mean of the target value in the test set the location data seems to be useful","metadata":{}},{"cell_type":"markdown","source":"3.) Since both Location and Keyword are categorical features they need to be OneHotEncoded to be useful. This can be done using pd.get_dummies","metadata":{}},{"cell_type":"code","source":"dummies = pd.get_dummies(df['location'], drop_first=True)\ndummies1 = pd.get_dummies(df['keyword'], drop_first=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:40.922681Z","iopub.execute_input":"2022-05-26T18:19:40.922991Z","iopub.status.idle":"2022-05-26T18:19:40.951405Z","shell.execute_reply.started":"2022-05-26T18:19:40.922887Z","shell.execute_reply":"2022-05-26T18:19:40.950476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we define a function to get the length of a text","metadata":{}},{"cell_type":"code","source":"def remove_punc(text):\n    new = re.sub(r'[^\\w\\s]', '', text)\n    return new\n\ndef remove_mentions(text):\n    new = re.sub(\"@\\S+\", \"\", text)\n    return new\n\ndef remove_url(text):\n    new = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", text)\n    return new\n\ndef remove_hashtag(text):\n    new = re.sub(\"#\", \"\", text)\n    return new\n\ndef clean_text(text):\n    new = remove_url(text)\n    new = remove_hashtag(new)\n    new = remove_mentions(new)\n    new = remove_punc(new)\n    return new\n\ndef leng(col):\n    text = word_tokenize(col, language='english')#word_tokenize\n    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n    filtered = [w for w in text if nonPunct.match(w)]\n    return len(filtered)\n\ndef char(col):\n    #l1=[]\n    #for rew in col:\n    text = word_tokenize(col, language='english')#word_tokenize\n    nonPunct = re.compile('.*[A-Za-z0-9].*')  # must contain a letter or digit\n    filtered = [w for w in text if nonPunct.match(w)]\n    #print(text)\n    s = [len(u) for u in filtered]\n    #l1.append(sum(s))\n    return sum(s)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:40.952676Z","iopub.execute_input":"2022-05-26T18:19:40.952897Z","iopub.status.idle":"2022-05-26T18:19:40.962235Z","shell.execute_reply.started":"2022-05-26T18:19:40.952872Z","shell.execute_reply":"2022-05-26T18:19:40.961357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_text('#flood #disaster Hello World!!.? @ElonMusk, https://kaggle.com')","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:40.963916Z","iopub.execute_input":"2022-05-26T18:19:40.964574Z","iopub.status.idle":"2022-05-26T18:19:40.981242Z","shell.execute_reply.started":"2022-05-26T18:19:40.964523Z","shell.execute_reply":"2022-05-26T18:19:40.980379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using this function we can get the word count of a tweet. Furthermore we define a new column with the word count and standartize it afterwards.","metadata":{}},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:40.982859Z","iopub.execute_input":"2022-05-26T18:19:40.98338Z","iopub.status.idle":"2022-05-26T18:19:41.00366Z","shell.execute_reply.started":"2022-05-26T18:19:40.983337Z","shell.execute_reply":"2022-05-26T18:19:41.002983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['wordCount'] = df['text'].apply(lambda x: leng(x))\ndtrain['wordCount'] = dtrain['text'].apply(lambda x: leng(x))\n\ndf['charCount'] = df['text'].apply(lambda x: char(x))\ndtrain['charCount'] = dtrain['text'].apply(lambda x: char(x))\n\ndf['char/wrd'] = df['charCount']/df['wordCount']\n\nwrdcm = dtrain['wordCount'].mean()\nwrdcs = dtrain['wordCount'].std()\n\nchm = dtrain['charCount'].mean()\nchsd = dtrain['charCount'].std()\n\n\nprint(df[df['target']==1]['char/wrd'].mean())\nprint(df[df['target']==0]['char/wrd'].mean())\n\nprint(df[df['target']==1]['charCount'].mean())\nprint(df[df['target']==0]['charCount'].mean())\n\nprint(df[df['target']==1]['wordCount'].mean())\nprint(df[df['target']==0]['wordCount'].mean())\n\n#df['wordCount'] = (df['wordCount']-wrdcm)/wrdcs #standartised\n#df['charCount'] = (df['charCount']-chm)/chsd","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:41.004783Z","iopub.execute_input":"2022-05-26T18:19:41.00542Z","iopub.status.idle":"2022-05-26T18:19:48.753374Z","shell.execute_reply.started":"2022-05-26T18:19:41.005382Z","shell.execute_reply":"2022-05-26T18:19:48.752382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Average word count is very similiar for both disaster and non disaster tweets but disaster tweets have more characters per words on average.","metadata":{}},{"cell_type":"markdown","source":"Next apply lemmatization","metadata":{}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\n\nwl = WordNetLemmatizer()\n \n# single word lemmatization examples\nlist1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',\n         'driving', 'died', 'tried', 'feet']\nfor words in list1:\n    print(words + \" ---> \" + wl.lemmatize(words))\n    \nprint(wl.lemmatize('hands, birds cars'))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:48.756917Z","iopub.execute_input":"2022-05-26T18:19:48.757321Z","iopub.status.idle":"2022-05-26T18:19:48.764209Z","shell.execute_reply.started":"2022-05-26T18:19:48.757284Z","shell.execute_reply":"2022-05-26T18:19:48.763436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import wordnet\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\ndef lemmatizer(string):\n    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n    return \" \".join(a)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:48.765308Z","iopub.execute_input":"2022-05-26T18:19:48.765698Z","iopub.status.idle":"2022-05-26T18:19:48.780669Z","shell.execute_reply.started":"2022-05-26T18:19:48.765666Z","shell.execute_reply":"2022-05-26T18:19:48.779883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].apply(lambda x: clean_text(x))\ndf['text'] = df['text'].apply(lambda x: lemmatizer(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:19:48.782053Z","iopub.execute_input":"2022-05-26T18:19:48.782308Z","iopub.status.idle":"2022-05-26T18:20:00.183567Z","shell.execute_reply.started":"2022-05-26T18:19:48.782279Z","shell.execute_reply":"2022-05-26T18:20:00.182697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we can add the dummie columns","metadata":{}},{"cell_type":"code","source":"df = pd.concat((df, dummies), axis=1)\ndf = pd.concat((df, dummies1), axis=1)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:00.184817Z","iopub.execute_input":"2022-05-26T18:20:00.185062Z","iopub.status.idle":"2022-05-26T18:20:00.208326Z","shell.execute_reply.started":"2022-05-26T18:20:00.185031Z","shell.execute_reply":"2022-05-26T18:20:00.207488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For later use we define the feature columns as all columns other than 'id', 'keyword', 'target', 'text', 'location'","metadata":{}},{"cell_type":"code","source":"features = df.columns.drop(['id','keyword','target', 'text', 'location'])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:00.209815Z","iopub.execute_input":"2022-05-26T18:20:00.210027Z","iopub.status.idle":"2022-05-26T18:20:00.214456Z","shell.execute_reply.started":"2022-05-26T18:20:00.210001Z","shell.execute_reply":"2022-05-26T18:20:00.213604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4.) Text Vectorization: This will be done using TfidfVectorizer ","metadata":{}},{"cell_type":"code","source":"stpwrdlist=['the', 'a', 'an']#Custom list of stopwords \nvectorizer = TfidfVectorizer(max_df=0.9, min_df=0.001, stop_words=stpwrdlist, ngram_range=(1,3)) #stop_words=stpwrdlist","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:00.21593Z","iopub.execute_input":"2022-05-26T18:20:00.217027Z","iopub.status.idle":"2022-05-26T18:20:00.237544Z","shell.execute_reply.started":"2022-05-26T18:20:00.216983Z","shell.execute_reply":"2022-05-26T18:20:00.236541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split df into train and test set:","metadata":{}},{"cell_type":"code","source":"train = df[df['target'].notna()]\ntest =df[df['target'].isna()]\ny = train['target']\ntrain = train.drop('target', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:00.238897Z","iopub.execute_input":"2022-05-26T18:20:00.239114Z","iopub.status.idle":"2022-05-26T18:20:00.27758Z","shell.execute_reply.started":"2022-05-26T18:20:00.239088Z","shell.execute_reply":"2022-05-26T18:20:00.276732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fit vectorizer and transform test set","metadata":{}},{"cell_type":"code","source":"X = vectorizer.fit_transform(train['text'])\n\nX_test = vectorizer.transform(test['text'])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:00.278981Z","iopub.execute_input":"2022-05-26T18:20:00.279225Z","iopub.status.idle":"2022-05-26T18:20:00.900724Z","shell.execute_reply.started":"2022-05-26T18:20:00.279195Z","shell.execute_reply":"2022-05-26T18:20:00.899359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5.) Concatenate features from vectorizer with previous features","metadata":{}},{"cell_type":"code","source":"X = np.concatenate((X.toarray(), train[features]), axis=1)\n\nX_test = np.concatenate((X_test.toarray(), test[features]), axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:00.902013Z","iopub.execute_input":"2022-05-26T18:20:00.902263Z","iopub.status.idle":"2022-05-26T18:20:01.36515Z","shell.execute_reply.started":"2022-05-26T18:20:00.902235Z","shell.execute_reply":"2022-05-26T18:20:01.363803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split train data into train and validation data","metadata":{}},{"cell_type":"code","source":"X_t, X_v, y_t, y_v = train_test_split(X, y, test_size=0.2, random_state=10)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:01.367267Z","iopub.execute_input":"2022-05-26T18:20:01.367616Z","iopub.status.idle":"2022-05-26T18:20:01.471182Z","shell.execute_reply.started":"2022-05-26T18:20:01.367582Z","shell.execute_reply":"2022-05-26T18:20:01.470231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Build Neural Network**","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\n#from keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:01.472486Z","iopub.execute_input":"2022-05-26T18:20:01.472756Z","iopub.status.idle":"2022-05-26T18:20:01.478335Z","shell.execute_reply.started":"2022-05-26T18:20:01.472723Z","shell.execute_reply":"2022-05-26T18:20:01.476894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:01.480376Z","iopub.execute_input":"2022-05-26T18:20:01.480862Z","iopub.status.idle":"2022-05-26T18:20:01.495234Z","shell.execute_reply.started":"2022-05-26T18:20:01.480827Z","shell.execute_reply":"2022-05-26T18:20:01.4944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\n#early_stopping_monitor = EarlyStopping(patience=1)\nmodel.add(Dense(5, input_shape=(3376,)))\nmodel.add(Dense(32, activation='sigmoid'))\nmodel.add(Dense(32, activation='sigmoid'))\n#model.add(Dense(32, activation='leaky_relu'))\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:01.496841Z","iopub.execute_input":"2022-05-26T18:20:01.497458Z","iopub.status.idle":"2022-05-26T18:20:01.551478Z","shell.execute_reply.started":"2022-05-26T18:20:01.49735Z","shell.execute_reply":"2022-05-26T18:20:01.550248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:01.553241Z","iopub.execute_input":"2022-05-26T18:20:01.553604Z","iopub.status.idle":"2022-05-26T18:20:01.563792Z","shell.execute_reply.started":"2022-05-26T18:20:01.553571Z","shell.execute_reply":"2022-05-26T18:20:01.562446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = tf.keras.optimizers.Adam(0.001) #tf.keras.optimizers.SGD(learning_rate=0.1, momentum=1) \n\nmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:01.565917Z","iopub.execute_input":"2022-05-26T18:20:01.566619Z","iopub.status.idle":"2022-05-26T18:20:01.586999Z","shell.execute_reply.started":"2022-05-26T18:20:01.566568Z","shell.execute_reply":"2022-05-26T18:20:01.586279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h_callback = model.fit(X_t, y_t, epochs=7, batch_size=50)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:01.588056Z","iopub.execute_input":"2022-05-26T18:20:01.588499Z","iopub.status.idle":"2022-05-26T18:20:04.918631Z","shell.execute_reply.started":"2022-05-26T18:20:01.588464Z","shell.execute_reply":"2022-05-26T18:20:04.918039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot( h_callback.history['accuracy'])\n#plt.plot( h_callback.history['val_accuracy'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:04.919879Z","iopub.execute_input":"2022-05-26T18:20:04.920106Z","iopub.status.idle":"2022-05-26T18:20:05.093111Z","shell.execute_reply.started":"2022-05-26T18:20:04.920078Z","shell.execute_reply":"2022-05-26T18:20:05.092179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_v, y_v)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:05.094343Z","iopub.execute_input":"2022-05-26T18:20:05.094561Z","iopub.status.idle":"2022-05-26T18:20:05.384763Z","shell.execute_reply.started":"2022-05-26T18:20:05.094535Z","shell.execute_reply":"2022-05-26T18:20:05.383819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model seems to perform reasonably well. To get a final prediction we will fit it again to the whole testing data","metadata":{}},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:05.389323Z","iopub.execute_input":"2022-05-26T18:20:05.390115Z","iopub.status.idle":"2022-05-26T18:20:05.395674Z","shell.execute_reply.started":"2022-05-26T18:20:05.390076Z","shell.execute_reply":"2022-05-26T18:20:05.394863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = Sequential()\n\n#early_stopping_monitor = EarlyStopping(patience=1)\nmodel1.add(Dense(5, input_shape=(3376,)))\nmodel1.add(Dense(32, activation='sigmoid'))\nmodel1.add(Dense(32, activation='sigmoid'))\nmodel1.add(Dense(1, activation='sigmoid'))\n\nmodel1.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:05.397044Z","iopub.execute_input":"2022-05-26T18:20:05.397288Z","iopub.status.idle":"2022-05-26T18:20:05.445999Z","shell.execute_reply.started":"2022-05-26T18:20:05.39726Z","shell.execute_reply":"2022-05-26T18:20:05.445339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.fit(X, dtrain['target'], epochs=7, batch_size=50)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:05.447048Z","iopub.execute_input":"2022-05-26T18:20:05.44736Z","iopub.status.idle":"2022-05-26T18:20:09.18532Z","shell.execute_reply.started":"2022-05-26T18:20:05.447319Z","shell.execute_reply":"2022-05-26T18:20:09.184392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1 = np.round(model1.predict(X))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:09.186824Z","iopub.execute_input":"2022-05-26T18:20:09.187115Z","iopub.status.idle":"2022-05-26T18:20:09.704404Z","shell.execute_reply.started":"2022-05-26T18:20:09.187081Z","shell.execute_reply":"2022-05-26T18:20:09.703507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred1","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:09.705893Z","iopub.execute_input":"2022-05-26T18:20:09.706199Z","iopub.status.idle":"2022-05-26T18:20:09.712016Z","shell.execute_reply.started":"2022-05-26T18:20:09.706148Z","shell.execute_reply":"2022-05-26T18:20:09.711426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score, confusion_matrix\n\nprint(f1_score(dtrain['target'], pred1))\nprint(confusion_matrix(dtrain['target'], pred1))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:09.712965Z","iopub.execute_input":"2022-05-26T18:20:09.713702Z","iopub.status.idle":"2022-05-26T18:20:09.732142Z","shell.execute_reply.started":"2022-05-26T18:20:09.713666Z","shell.execute_reply":"2022-05-26T18:20:09.731436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On the train set the model reaches an accuracy of about 86% and f1_score of about 0.83. However it is trained to overfit slightly as seen from the 0.79 accuracy on the validation set, which is closer to what to expect when applied to the test set.","metadata":{}},{"cell_type":"markdown","source":"**Predict and submit**","metadata":{}},{"cell_type":"code","source":"predictions = np.round(model1.predict(X_test)).astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:09.733314Z","iopub.execute_input":"2022-05-26T18:20:09.734076Z","iopub.status.idle":"2022-05-26T18:20:09.978141Z","shell.execute_reply.started":"2022-05-26T18:20:09.734041Z","shell.execute_reply":"2022-05-26T18:20:09.977135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.DataFrame({'id':dtest['id'].values.tolist(),'target':predictions.ravel()})\nsub.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:09.979332Z","iopub.execute_input":"2022-05-26T18:20:09.979567Z","iopub.status.idle":"2022-05-26T18:20:09.990144Z","shell.execute_reply.started":"2022-05-26T18:20:09.979539Z","shell.execute_reply":"2022-05-26T18:20:09.989423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2022-05-26T18:20:09.992294Z","iopub.execute_input":"2022-05-26T18:20:09.992641Z","iopub.status.idle":"2022-05-26T18:20:10.009023Z","shell.execute_reply.started":"2022-05-26T18:20:09.992601Z","shell.execute_reply":"2022-05-26T18:20:10.008156Z"},"trusted":true},"execution_count":null,"outputs":[]}]}