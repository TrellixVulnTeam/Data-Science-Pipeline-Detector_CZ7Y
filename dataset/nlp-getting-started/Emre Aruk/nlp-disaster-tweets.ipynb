{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T10:09:21.18364Z","iopub.execute_input":"2021-07-29T10:09:21.184045Z","iopub.status.idle":"2021-07-29T10:09:21.195718Z","shell.execute_reply.started":"2021-07-29T10:09:21.184015Z","shell.execute_reply":"2021-07-29T10:09:21.193861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader,TensorDataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix,plot_confusion_matrix,classification_report\nimport time","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:21.199375Z","iopub.execute_input":"2021-07-29T10:09:21.199945Z","iopub.status.idle":"2021-07-29T10:09:21.20862Z","shell.execute_reply.started":"2021-07-29T10:09:21.19979Z","shell.execute_reply":"2021-07-29T10:09:21.207017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu' )\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:21.211617Z","iopub.execute_input":"2021-07-29T10:09:21.212628Z","iopub.status.idle":"2021-07-29T10:09:21.223458Z","shell.execute_reply.started":"2021-07-29T10:09:21.212578Z","shell.execute_reply":"2021-07-29T10:09:21.22175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain.info()\ntrain","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:21.226531Z","iopub.execute_input":"2021-07-29T10:09:21.227043Z","iopub.status.idle":"2021-07-29T10:09:21.278266Z","shell.execute_reply.started":"2021-07-29T10:09:21.226998Z","shell.execute_reply":"2021-07-29T10:09:21.276749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest.info()\ntest","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:21.28083Z","iopub.execute_input":"2021-07-29T10:09:21.281252Z","iopub.status.idle":"2021-07-29T10:09:21.31961Z","shell.execute_reply.started":"2021-07-29T10:09:21.28121Z","shell.execute_reply":"2021-07-29T10:09:21.318238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_train=train['text'].values\nlabel=train['target'].values\nx_train,x_val,label_train,label_val=train_test_split(data_train,label,test_size=0.10,random_state=42,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:21.322166Z","iopub.execute_input":"2021-07-29T10:09:21.322681Z","iopub.status.idle":"2021-07-29T10:09:21.331359Z","shell.execute_reply.started":"2021-07-29T10:09:21.322639Z","shell.execute_reply":"2021-07-29T10:09:21.330131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(x_train))\nprint(len(x_val))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:21.333739Z","iopub.execute_input":"2021-07-29T10:09:21.334539Z","iopub.status.idle":"2021-07-29T10:09:21.344049Z","shell.execute_reply.started":"2021-07-29T10:09:21.334493Z","shell.execute_reply":"2021-07-29T10:09:21.342688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(label);","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:21.345678Z","iopub.execute_input":"2021-07-29T10:09:21.346388Z","iopub.status.idle":"2021-07-29T10:09:21.468763Z","shell.execute_reply.started":"2021-07-29T10:09:21.346342Z","shell.execute_reply":"2021-07-29T10:09:21.467335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef preprocessing(s):\n    # Remove all non-word characters (everything except numbers and letters)\n    s = re.sub(r\"[^\\w\\s]\", '', s)\n    # Replace all runs of whitespaces with no space\n    s = re.sub(r\"\\s+\", '', s)\n    # replace digits with no space\n    s = re.sub(r\"\\d\", '', s)\n    \n    return s","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:21.470872Z","iopub.execute_input":"2021-07-29T10:09:21.471352Z","iopub.status.idle":"2021-07-29T10:09:21.478469Z","shell.execute_reply.started":"2021-07-29T10:09:21.471285Z","shell.execute_reply":"2021-07-29T10:09:21.477287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\ndef tokenizer(x_train,x_val):\n    word_list=[]\n    stop_word=set(stopwords.words('english'))\n    for sent in x_train:\n        sent=sent.lower().split()\n        for word in sent:\n            word=preprocessing(word)\n           # if word not in stop_word and word != '' : \n            word_list.append(word)\n                \n                \n    corpus=Counter(word_list)     \n    corpus_=sorted(corpus,key=corpus.get,reverse=True)[:1500]\n    one_hot_dict={w:i+1 for i,w in enumerate(corpus_)}\n    \n    train_list=[]\n    val_list=[] \n    for  sent in  x_train:   \n         train_list.append( [one_hot_dict[preprocessing(word)] for word in sent.lower().split()\n                           if preprocessing(word) in one_hot_dict.keys()])\n        \n    for sent in x_val :\n        val_list.append( [one_hot_dict[preprocessing(word)]  for word in sent.lower().split()\n                            if preprocessing(word) in one_hot_dict.keys()])\n    \n    \n    \n    return  train_list,val_list,one_hot_dict\n        \nx_tr,x_ts,vocab=tokenizer(x_train,x_val)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:21.480198Z","iopub.execute_input":"2021-07-29T10:09:21.481004Z","iopub.status.idle":"2021-07-29T10:09:23.001046Z","shell.execute_reply.started":"2021-07-29T10:09:21.480956Z","shell.execute_reply":"2021-07-29T10:09:22.999889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Determine max length from expressions\na=[len(x) for x in x_tr]\nmax_len_index=a.index(max(a))\nmax_len_seq=len(x_tr[max_len_index])\nmax_len_seq","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:23.005153Z","iopub.execute_input":"2021-07-29T10:09:23.005674Z","iopub.status.idle":"2021-07-29T10:09:23.014985Z","shell.execute_reply.started":"2021-07-29T10:09:23.005626Z","shell.execute_reply":"2021-07-29T10:09:23.013301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#padding used to same size tensors\npad=max_len_seq\ndef padding(data,padd):\n    for i,sent in enumerate(data):\n        feature=np.zeros((1,padd),dtype=int) \n        if len(np.array(sent)) != 0:\n            feature[:,-len(sent):]=np.array(sent)\n            data[i]=feature\n        else :\n            data[i]=feature\n    return data\n\nX_train=padding(x_tr,pad) \nX_val=padding(x_ts,pad)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:23.017987Z","iopub.execute_input":"2021-07-29T10:09:23.018537Z","iopub.status.idle":"2021-07-29T10:09:23.127623Z","shell.execute_reply.started":"2021-07-29T10:09:23.018493Z","shell.execute_reply":"2021-07-29T10:09:23.126562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class sentimentLSTM(nn.Module):\n    def __init__(self,num_layers,batch_size,hidden_size,vocab_size,embed_size,p,pad):\n        super(sentimentLSTM,self).__init__()\n        \n        self.hidden=hidden_size\n        self.embed=nn.Embedding(vocab_size,embed_size)\n        self.embed_size=embed_size\n        self.num_layers=num_layers\n        self.p=p\n        self.pad=pad\n        self.lstm=nn.LSTM(input_size=self.embed_size,\n                         hidden_size=self.hidden,\n                         num_layers=self.num_layers,\n                         batch_first=True)\n        \n\n        \n        \n        self.linear=nn.Linear(self.hidden,512)\n        self.linear2=nn.Linear(512,1)\n\n        self.drop=nn.Dropout(self.p)\n        self.relu=nn.ReLU()\n      #  self.sigmoid=nn.Sigmoid()\n    \n\n    \n    def forward(self,x,hidden):\n        \n        batch=x.shape[0]\n        x=x.view(batch,-1)\n        x=self.embed(x)\n        x,hidden=self.lstm(x)\n        self.x=x.contiguous().view(-1, self.hidden)\n        x=self.drop(self.x)\n        x=self.relu(x)\n        x=self.linear(x) \n        \n        x=self.drop(x)\n        x=self.relu(x)\n        x=self.linear2(x)  \n        \n        \n       # x=self.sigmoid(x)\n        out=x.view(batch,self.pad, -1)   \n        \n        return out[:,-1] ,h\n    \n    \n    def get_x(self):\n        return self.x\n\n    def init_hidden(self,b):\n\n        h0 = torch.zeros((self.num_layers,batch_size,self.hidden)).to(device)\n        c0 = torch.zeros((self.num_layers,batch_size,self.hidden)).to(device)\n        hidden = (h0,c0)\n        return hidden","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:23.129269Z","iopub.execute_input":"2021-07-29T10:09:23.129801Z","iopub.status.idle":"2021-07-29T10:09:23.145776Z","shell.execute_reply.started":"2021-07-29T10:09:23.129758Z","shell.execute_reply":"2021-07-29T10:09:23.144528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=20\nlr=0.01\nhidden_size=512\nvocab_size=len(vocab)+1\nembed_size=250\nbatch_size=64\nnum_layers=1\np=0.5\nmodel=sentimentLSTM(num_layers,batch_size,hidden_size,vocab_size,embed_size,p,pad).to(device)\ncriterion=nn.BCEWithLogitsLoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\n\ntrain_data=TensorDataset(torch.tensor(X_train),torch.tensor(label_train))\nval_data=TensorDataset(torch.tensor(X_val),torch.tensor(label_val))\n\ntrain_loader=DataLoader(train_data,batch_size=batch_size,shuffle=True)\nval_loader=DataLoader(val_data,batch_size=batch_size,shuffle=True)\n\n\n\ntrain_losses=[]\nval_losses=[]\nfor epoch in range(epochs):\n    \n    train_loss=0\n    val_loss=0\n    model.train()\n    train_preds=[]\n    train_labels=[]\n    h = model.init_hidden(batch_size)\n    for e,(train_inputs,train_label) in enumerate(train_loader):\n        \n        train_inputs=train_inputs.type(torch.LongTensor).to(device)\n        train_label=train_label.type(torch.LongTensor).to(device)\n        \n        h = tuple([each.data for each in h])\n        train_pred,h=model(train_inputs,h)\n\n        loss=criterion(train_pred.float(),train_label.reshape(-1,1).float())\n        train_loss+=loss.item()\n        \n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_labels.append(np.array(train_label.cpu()))\n        train_preds.append(np.array([1 if i>0.5 else 0 for i in train_pred]))\n        \n    train_losses.append(train_loss/len(train_data)) \n    train_labels=np.concatenate(train_labels)\n    train_preds=np.concatenate(train_preds)\n    acc_train=accuracy_score(train_labels,train_preds)    \n\n    model.eval() \n    val_preds=[]\n    val_labels=[]\n    for t,(input_val,l_val) in enumerate(val_loader):\n        \n        input_val=input_val.type(torch.LongTensor).to(device)\n        l_val=l_val.type(torch.LongTensor).to(device)\n        \n        h = tuple([each.data for each in h])\n        pred_val,h=model(input_val,h)\n\n        loss_val=criterion(pred_val.float(),l_val.reshape(-1,1).float())\n        val_loss+=loss_val.item()\n        \n        \n        val_labels.append(np.array(l_val.cpu()))\n        val_preds.append(np.array([1 if i>0.5 else 0 for i in pred_val ]))\n        \n    \n    val_losses.append(val_loss/len(val_data))\n    val_labels=np.concatenate(val_labels)\n    val_preds=np.concatenate(val_preds)\n    acc_val=accuracy_score(val_labels,val_preds)\n    \n    print('Epochs {}, Train_accurary {:.3f},Train error {:.5f} --- Val_accuracy {:.3f}'.\n          format(epoch,acc_train,(train_loss/len(train_data)),acc_val))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:23.147816Z","iopub.execute_input":"2021-07-29T10:09:23.148566Z","iopub.status.idle":"2021-07-29T10:09:57.969576Z","shell.execute_reply.started":"2021-07-29T10:09:23.148517Z","shell.execute_reply":"2021-07-29T10:09:57.968369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest_text=test['text']\ntest_text","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:57.971023Z","iopub.execute_input":"2021-07-29T10:09:57.971402Z","iopub.status.idle":"2021-07-29T10:09:57.99735Z","shell.execute_reply.started":"2021-07-29T10:09:57.971372Z","shell.execute_reply":"2021-07-29T10:09:57.99633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#test text preprocessing \ntest_list=[]\nfor text in test_text:\n    word_seq = np.array([vocab[preprocessing(word)] for word in text.lower().split() \n                     if preprocessing(word)  in vocab.keys()])\n    feature=np.zeros((1,max_len_seq),dtype=int)\n\n    if len(word_seq)==0 :\n        inputs =  torch.from_numpy(feature).type(torch.LongTensor).to(device)\n        test_list.append(inputs)\n\n    else:\n        feature[:,-len(word_seq):]=np.array(word_seq)\n        inputs =  torch.from_numpy(feature).type(torch.LongTensor).to(device)\n        test_list.append(inputs)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:57.998821Z","iopub.execute_input":"2021-07-29T10:09:57.999269Z","iopub.status.idle":"2021-07-29T10:09:58.689264Z","shell.execute_reply.started":"2021-07-29T10:09:57.999225Z","shell.execute_reply":"2021-07-29T10:09:58.688049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs=[]\nfor seq in test_list:\n    model.eval()\n    with torch.no_grad():\n        h = model.init_hidden(batch_size)\n        h = tuple([each.data for each in h])\n        output, h = model(seq, h)\n        outputs.append(1 if output>0.5 else 0  )\n        ","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:09:58.69086Z","iopub.execute_input":"2021-07-29T10:09:58.692594Z","iopub.status.idle":"2021-07-29T10:10:03.845028Z","shell.execute_reply.started":"2021-07-29T10:09:58.69183Z","shell.execute_reply":"2021-07-29T10:10:03.843788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:10:03.84686Z","iopub.execute_input":"2021-07-29T10:10:03.847641Z","iopub.status.idle":"2021-07-29T10:10:03.860133Z","shell.execute_reply.started":"2021-07-29T10:10:03.847593Z","shell.execute_reply":"2021-07-29T10:10:03.858843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc_scr=accuracy_score(np.array(outputs),np.array(submission['target']))\nacc_scr","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:10:03.862407Z","iopub.execute_input":"2021-07-29T10:10:03.863452Z","iopub.status.idle":"2021-07-29T10:10:03.882528Z","shell.execute_reply.started":"2021-07-29T10:10:03.863304Z","shell.execute_reply":"2021-07-29T10:10:03.880768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(np.array(outputs),np.array(submission['target']))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:10:03.88731Z","iopub.execute_input":"2021-07-29T10:10:03.887774Z","iopub.status.idle":"2021-07-29T10:10:03.906973Z","shell.execute_reply.started":"2021-07-29T10:10:03.887732Z","shell.execute_reply":"2021-07-29T10:10:03.905678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_new=pd.DataFrame()\nsubmission_new['target']=np.array(outputs)\nsubmission_new.index=submission['id']\nsubmission_new.to_csv('/kaggle/working/submission.csv')\nsubmission_new","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:10:03.911861Z","iopub.execute_input":"2021-07-29T10:10:03.914425Z","iopub.status.idle":"2021-07-29T10:10:03.951948Z","shell.execute_reply.started":"2021-07-29T10:10:03.914348Z","shell.execute_reply":"2021-07-29T10:10:03.950932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(outputs,list(submission['target'])))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:10:03.956703Z","iopub.execute_input":"2021-07-29T10:10:03.959183Z","iopub.status.idle":"2021-07-29T10:10:03.992559Z","shell.execute_reply.started":"2021-07-29T10:10:03.959139Z","shell.execute_reply":"2021-07-29T10:10:03.991486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(submission['target'])","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:10:03.9977Z","iopub.execute_input":"2021-07-29T10:10:04.000436Z","iopub.status.idle":"2021-07-29T10:10:04.172629Z","shell.execute_reply.started":"2021-07-29T10:10:04.000379Z","shell.execute_reply":"2021-07-29T10:10:04.171493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(np.array(outputs))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:10:04.176138Z","iopub.execute_input":"2021-07-29T10:10:04.176476Z","iopub.status.idle":"2021-07-29T10:10:04.304982Z","shell.execute_reply.started":"2021-07-29T10:10:04.176443Z","shell.execute_reply":"2021-07-29T10:10:04.303245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}