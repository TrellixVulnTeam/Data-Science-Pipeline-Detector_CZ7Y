{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-16T15:43:21.770822Z","iopub.execute_input":"2022-06-16T15:43:21.771268Z","iopub.status.idle":"2022-06-16T15:43:21.779186Z","shell.execute_reply.started":"2022-06-16T15:43:21.77123Z","shell.execute_reply":"2022-06-16T15:43:21.778345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In this we will be Fine tuning a BERT model to classify natural disaster tweets using HuggingFace**\n\n\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:43:23.099345Z","iopub.execute_input":"2022-06-16T15:43:23.100148Z","iopub.status.idle":"2022-06-16T15:43:42.866249Z","shell.execute_reply.started":"2022-06-16T15:43:23.100108Z","shell.execute_reply":"2022-06-16T15:43:42.865241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use **datasets** library by huggingface for data streamlining as well as for metrics.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, load_metric","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:43:42.869787Z","iopub.execute_input":"2022-06-16T15:43:42.870122Z","iopub.status.idle":"2022-06-16T15:43:48.771687Z","shell.execute_reply.started":"2022-06-16T15:43:42.870092Z","shell.execute_reply":"2022-06-16T15:43:48.770819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will laod the csv using load_dataset function","metadata":{}},{"cell_type":"code","source":"train_dataset = load_dataset('csv', data_files='../input/nlp-getting-started/train.csv',split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:43:48.77389Z","iopub.execute_input":"2022-06-16T15:43:48.77481Z","iopub.status.idle":"2022-06-16T15:43:49.428571Z","shell.execute_reply.started":"2022-06-16T15:43:48.774768Z","shell.execute_reply":"2022-06-16T15:43:49.427737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will load the metrics required for the evalution. \n**GLUE** is a benchmark which consists of lot of NLP taks and their evalution method for scoring how well the model is generalized across the task.\nsentiment classification is one of them, as ours is also a binary classification method. we will use that.\n\nIn the code below the sst2 indicates sentiment classification.","metadata":{}},{"cell_type":"code","source":"metric = load_metric('glue', 'sst2')","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-06-16T15:43:49.430209Z","iopub.execute_input":"2022-06-16T15:43:49.430645Z","iopub.status.idle":"2022-06-16T15:43:50.643658Z","shell.execute_reply.started":"2022-06-16T15:43:49.430605Z","shell.execute_reply":"2022-06-16T15:43:50.642643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metric","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:43:50.647804Z","iopub.execute_input":"2022-06-16T15:43:50.648178Z","iopub.status.idle":"2022-06-16T15:43:50.66407Z","shell.execute_reply.started":"2022-06-16T15:43:50.648137Z","shell.execute_reply":"2022-06-16T15:43:50.663392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now for evalution we will split the train data into train and eval , for that we will use **train_test_split** function. The 0.1 means we are spliting 10% of the data as testing.\n\nAnd the method we are using expects sentence and target, so we rename the coloumns likewise using **rename_columns** function which is same syntax as pandas.\n\n","metadata":{}},{"cell_type":"code","source":"train_dataset = train_dataset.rename_columns({\"target\" : \"label\",\"text\" : \"sentence\"})\ntrain_dataset = train_dataset.train_test_split(test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:43:50.802451Z","iopub.execute_input":"2022-06-16T15:43:50.802752Z","iopub.status.idle":"2022-06-16T15:43:50.819677Z","shell.execute_reply.started":"2022-06-16T15:43:50.802724Z","shell.execute_reply":"2022-06-16T15:43:50.819023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:43:51.548192Z","iopub.execute_input":"2022-06-16T15:43:51.548876Z","iopub.status.idle":"2022-06-16T15:43:51.556421Z","shell.execute_reply.started":"2022-06-16T15:43:51.54882Z","shell.execute_reply":"2022-06-16T15:43:51.55551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use **bert-base-uncased** and fine-tune with batch size of 32","metadata":{}},{"cell_type":"code","source":"model_checkpoint = \"bert-base-uncased\"\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:43:55.190572Z","iopub.execute_input":"2022-06-16T15:43:55.191225Z","iopub.status.idle":"2022-06-16T15:43:55.195554Z","shell.execute_reply.started":"2022-06-16T15:43:55.191188Z","shell.execute_reply":"2022-06-16T15:43:55.194395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A **tokenizer** is in charge of preparing the inputs for a model. The library contains tokenizers for all the models.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n    \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:43:57.225829Z","iopub.execute_input":"2022-06-16T15:43:57.226489Z","iopub.status.idle":"2022-06-16T15:44:04.351186Z","shell.execute_reply.started":"2022-06-16T15:43:57.226454Z","shell.execute_reply":"2022-06-16T15:44:04.350363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will create a function which takes sentence as input and returns the tokenized output.\n\n**truncation=true** tells that if a lenght of string is more then certain lenght its truncated to that max lenght.","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n  return tokenizer(examples['sentence'], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:44:04.353016Z","iopub.execute_input":"2022-06-16T15:44:04.353525Z","iopub.status.idle":"2022-06-16T15:44:04.358627Z","shell.execute_reply.started":"2022-06-16T15:44:04.353487Z","shell.execute_reply":"2022-06-16T15:44:04.357888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are tokenzing the whole train and eval dataset. As you can see its easy to do that. This is one of the advatange of using **datasets**, where you are not applying any process differently for train and eval.\n","metadata":{}},{"cell_type":"code","source":"encoded_dataset = train_dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:44:04.359825Z","iopub.execute_input":"2022-06-16T15:44:04.360622Z","iopub.status.idle":"2022-06-16T15:44:05.350137Z","shell.execute_reply.started":"2022-06-16T15:44:04.360585Z","shell.execute_reply":"2022-06-16T15:44:05.349256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By default model will not have architecture for the specific task, so we load the model with a task specific function which does the work. In below we are using **AutoModelForSequenceClassification** which takes bert_based_uncased and makes it ready it for classification.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer \n\nnum_labels = 2\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:44:09.152446Z","iopub.execute_input":"2022-06-16T15:44:09.153338Z","iopub.status.idle":"2022-06-16T15:44:21.037336Z","shell.execute_reply.started":"2022-06-16T15:44:09.153288Z","shell.execute_reply":"2022-06-16T15:44:21.036543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we pass some hyperparmeters for the model. \n","metadata":{}},{"cell_type":"code","source":"model_name = model_checkpoint.split(\"/\")[-1]\nmetric_name = 'accuracy'\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-dst_clf\",\n    per_device_train_batch_size=batch_size,\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    metric_for_best_model =metric_name,\n    learning_rate=2e-5\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:44:21.039191Z","iopub.execute_input":"2022-06-16T15:44:21.039944Z","iopub.status.idle":"2022-06-16T15:44:21.105899Z","shell.execute_reply.started":"2022-06-16T15:44:21.039903Z","shell.execute_reply":"2022-06-16T15:44:21.105163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create a function which calculates accuracy. This we will be running after some iteration to check how well our model is generalising.","metadata":{}},{"cell_type":"code","source":"import numpy as np\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:44:21.10708Z","iopub.execute_input":"2022-06-16T15:44:21.107645Z","iopub.status.idle":"2022-06-16T15:44:21.113219Z","shell.execute_reply.started":"2022-06-16T15:44:21.107608Z","shell.execute_reply":"2022-06-16T15:44:21.112458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset['test'],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:44:21.115131Z","iopub.execute_input":"2022-06-16T15:44:21.115483Z","iopub.status.idle":"2022-06-16T15:44:26.5596Z","shell.execute_reply.started":"2022-06-16T15:44:21.115447Z","shell.execute_reply":"2022-06-16T15:44:26.558786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By default the train logs are reported to wandb,we will disable it for now. **Wandb** used to get details on loss and other model metrics which are usefull while making model versioning.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:44:31.573605Z","iopub.execute_input":"2022-06-16T15:44:31.574329Z","iopub.status.idle":"2022-06-16T15:44:31.579701Z","shell.execute_reply.started":"2022-06-16T15:44:31.574282Z","shell.execute_reply":"2022-06-16T15:44:31.57834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:44:32.626734Z","iopub.execute_input":"2022-06-16T15:44:32.627438Z","iopub.status.idle":"2022-06-16T15:47:00.170905Z","shell.execute_reply.started":"2022-06-16T15:44:32.6274Z","shell.execute_reply":"2022-06-16T15:47:00.166732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will get the final metrics on our evalution dataset.","metadata":{}},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:47:00.189559Z","iopub.execute_input":"2022-06-16T15:47:00.19213Z","iopub.status.idle":"2022-06-16T15:47:01.838015Z","shell.execute_reply.started":"2022-06-16T15:47:00.192078Z","shell.execute_reply":"2022-06-16T15:47:01.836821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will move the model to CPU for prediction","metadata":{}},{"cell_type":"code","source":"temp_output = model.cpu()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:47:01.842343Z","iopub.execute_input":"2022-06-16T15:47:01.843158Z","iopub.status.idle":"2022-06-16T15:47:02.576561Z","shell.execute_reply.started":"2022-06-16T15:47:01.84312Z","shell.execute_reply":"2022-06-16T15:47:02.575554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef predict(text):\n  token_output = tokenizer(text, truncation=True,return_tensors='pt')\n  output = model.forward(input_ids=token_output['input_ids'],attention_mask=token_output['attention_mask'])\n  return np.argmax(output['logits'].detach().numpy(), axis=1)[0]\n  \n\neval_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\neval_df['target'] = eval_df['text'].apply(predict)\neval_df[['id','target']].to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T15:49:01.29079Z","iopub.execute_input":"2022-06-16T15:49:01.291327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}