{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing Data and Dependencies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Libraries\nimport numpy as np\nimport pandas as pd\nimport regex as re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as ms\n\n#Data\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\nss = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target value :\n\n### 1. Target Value Missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing Value\nprint('Number of Missing Values in Target feature: {}'.format(train.target.isnull().sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Target Value Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Distribution\ncanv, axs = plt.subplots(1,2,figsize=(22,8))\ncolor = ['darkgreen','darkslategrey']\n\nplt.sca(axs[0])\nplt.pie(train.groupby('target').count()['id'],explode=(0.1,0),startangle=120,colors=color,\n    textprops={'fontsize':15},labels=['Not Disaster (57%)', 'Disaster (43%)'])\n\nplt.sca(axs[1])\nbars = plt.bar([0,0.5],train.groupby('target').count()['id'],width=0.3,color=color)\nplt.xticks([0,0.5],['Not Disaster','Disaster'])\nplt.tick_params(axis='both',labelsize=15,size=0,labelleft=False)\n\nfor sp in plt.gca().spines.values():\n    sp.set_visible(False)\n    \nfor bar,val in zip(bars,train.groupby('target').count()['id']):\n    plt.text(bar.get_x()+0.113,bar.get_height()-250,val,color='w',fontdict={'fontsize':18,'fontweight':'bold'})\n\ncanv.suptitle('Target Value Distribution in Training Data',fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train Data\ntrain_na = (train.isnull().sum() / len(train)) * 100\ntrain_na = train_na.drop(train_na[train_na==0].index).sort_values(ascending=False)\n\npd.DataFrame({'Train Missing Ratio' :train_na}).head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test Data\ntest_na = (test.isnull().sum() / len(test)) * 100\ntest_na = test_na.drop(test_na[test_na==0].index).sort_values(ascending=False)\n\npd.DataFrame({'Test Missing Ratio' :test_na}).head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing Mssing Values\ntitle = 'Train'\ndata = [train_na,test_na]\ncanv, axs = plt.subplots(1,2)\ncanv.set_size_inches(18,5)\nfor ax, dat in zip(axs,data):\n    plt.sca(ax)\n    sns.barplot(x=dat.index, y=dat,dodge=False)  \n    plt.xlabel('Features', fontsize=15,labelpad=10)\n    plt.ylabel('Percent of missing values', fontsize=15,labelpad=13)\n    plt.title('Percent missing data by feature in {} Data'.format(title), fontsize=15,pad=20)\n    plt.tick_params(axis='both',labelsize=12)\n    \n    sp = plt.gca().spines\n    sp['top'].set_visible(False)\n    sp['right'].set_visible(False)\n    \n    title = 'Test'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Handling Missing Data:\nBoth training and test set have same ratio of missing values in `keyword` and `location`.\n* **0.8%** of `keyword` is missing in both training and test set\n* **33%** of `location` is missing in both training and test set\n\nSince missing value ratios between training and test set are too close, **they are most probably taken from the same sample**. Missing values in those features are filled with `None` and `None` respectively."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filling Missing Data\nfor df in [train,test]:\n    for col in ['keyword','location']:\n        df[col].fillna('None',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering:"},{"metadata":{},"cell_type":"markdown","source":"### Helper Function to add Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function\ndef add_feature(X, feature_to_add):\n    \"\"\"\n    Returns sparse feature matrix with added feature.\n    \"\"\"\n    from scipy.sparse import csr_matrix, hstack\n    return hstack([X, csr_matrix(feature_to_add).T], 'csr')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### List of Features:\n- TFIDF with 2-5 ngrams and Min DF of 5\n- Word Count\n- Unique Word Count\n- Character Count\n- Number of Hastags\n- Number of Tagged People('@')\n- Number of Outlinks\n- Number of Non-WordCharacters\n- Number of Digits(Stats)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Target Value\ny = train.target\nX = train.text\n\n#Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. TFIDF\n\nIn information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.[1] It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the words.\n\n![TFIDF](https://www.researchgate.net/profile/Haider_Al-Khateeb2/publication/291950178/figure/fig1/AS:330186932408324@1455734107458/Term-Frequency-Inverse-Document-Frequency-TF-IDF.png)\n\n- We will be using a min docmuent frequency of 5 that means a ngram needs to be in atleast 5 document or instances in the data to be able to get in the vacabulary of the vectorizer \n- And we will using ngrams of 2 to 5 words "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(min_df=5,ngram_range=(1,5)).fit(X_train)\n\n#train\nX_train_vect = tfidf.transform(X_train)\n\n#test\nX_test_vect = tfidf.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Word Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : len(str(x).split())))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : len(str(x).split())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Unique Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : len(set(str(x).split()))))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : len(set(str(x).split()))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Character Count"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : len(str(x))))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : len(str(x))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Number of Hastags"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : x.count('#')))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : x.count('#')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Number of Tagged People('@')"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : x.count('@')))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : x.count('@')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Number of Out Links"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vect = add_feature(X_train_vect,X_train.apply(lambda x : x.count('http')))\nX_test_vect = add_feature(X_test_vect,X_test.apply(lambda x : x.count('http')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8. Number of Non-WordCharacters"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vect = add_feature(X_train_vect,X_train.str.count(r'[\\\\/!?,\\.:=<>^-]'))\nX_test_vect = add_feature(X_test_vect,X_test.str.count(r'[\\\\/!?,\\.:=<>^-]'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9. Number of Digits"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vect = add_feature(X_train_vect,X_train.str.count(r'\\d'))\nX_test_vect = add_feature(X_test_vect,X_test.str.count(r'\\d'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looking at the final Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of Features\nprint('The Number of Features in the Processed Data: {}'.format(X_train_vect.shape[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at some of the Vacabulary from the Tfidf\nprint(tfidf.get_feature_names()[350:420])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building\n\n1. Multilayer Perceptron Classifier"},{"metadata":{},"cell_type":"markdown","source":"### 1. MLPClassifier\n\nMLPClassifier stands for Multi-layer Perceptron classifier which in the name itself connects to a Neural Network. Unlike other classification algorithms such as Support Vectors or Naive Bayes Classifier, MLPClassifier relies on an underlying Neural Network to perform the task of classification.\n\n![MLP](https://s3.amazonaws.com/stackabuse/media/intro-to-neural-networks-scikit-learn-3.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Metric and Model\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score,make_scorer,accuracy_score,roc_auc_score\n\n#Creating Log Loss Scorer\nLogLoss = make_scorer(f1_score, greater_is_better=True, needs_proba=True)\n\n#Two Classifier\nmlp1 = MLPClassifier(max_iter=200,verbose=False,solver='sgd',activation='relu',learning_rate='adaptive')\nmlp2 = MLPClassifier(max_iter=200,verbose=False,solver='adam',activation='relu',learning_rate='adaptive')\n\n#Parameters for tunning\nparameter_space = {\n      'hidden_layer_sizes': [(50,50,50), (50,100,50),(100,100,100),(100,100),(500,500)],\n      'alpha': [0.0001, 0.05],\n  }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### HyperParameter Tunning"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tunning the First Classifier\nclf1 = GridSearchCV(mlp1, parameter_space,verbose=2,cv=3,scoring='f1')\nclf1.fit(X_train_vect,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Tunning the Second Classifier\nclf2 = GridSearchCV(mlp2, parameter_space,verbose=2,cv=3,scoring='f1')\nclf2.fit(X_train_vect,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Results from the first GridSearch\npd.DataFrame(data=clf1.cv_results_,columns=['param_hidden_layer_sizes','param_alpha','mean_test_score','std_test_score']).sort_values('mean_test_score',\n                                                                                                     ascending=False).reset_index(drop=True).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Results from the Second GridSearch\npd.DataFrame(data=clf2.cv_results_,columns=['param_hidden_layer_sizes','param_alpha','mean_test_score','std_test_score']).sort_values('mean_test_score',\n                                                                                                     ascending=False).reset_index(drop=True).iloc[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we can see that the best esitmator has the following parameters\nprm = {'alpha':0.05,'hidden_layer_sizes':(100,100,100),'max_iter':200,'solver':'adam','activation':'relu','learning_rate':'adaptive'}\nprint('The Best Parameters are:\\n')\nfor p,c in zip(prm.items(),range(1,7)):\n    print('{}. {} = {}'.format(c,p[0],p[1]))\n\n#Final F1 Score of test data\nprint('\\n\\nAnd the Accuracy of the Final Model on Test Data: {:.3f}'.format(accuracy_score(y_test,clf2.predict(X_test_vect))))\nprint('\\n\\nAnd the F1 score of the Final Model on Test Data: {:.3f}'.format(f1_score(y_test,clf2.predict(X_test_vect))))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}