{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is a code-along note book that follows the following notebook by Pei-Yi Hong with additional comments to clarify each step for beginner learners like me :-)\n\nhttps://www.kaggle.com/hongpeiyi/bert-with-pytorch-and-fastai ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-01T18:44:39.848945Z","iopub.execute_input":"2021-06-01T18:44:39.849469Z","iopub.status.idle":"2021-06-01T18:44:39.861547Z","shell.execute_reply.started":"2021-06-01T18:44:39.849385Z","shell.execute_reply":"2021-06-01T18:44:39.859577Z"}}},{"cell_type":"code","source":"#Importing the libraries\nimport numpy as np\nimport pandas as pd\nfrom fastai.text.all import *\nimport re","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:19:38.081834Z","iopub.execute_input":"2021-06-02T08:19:38.08224Z","iopub.status.idle":"2021-06-02T08:19:42.559714Z","shell.execute_reply.started":"2021-06-02T08:19:38.082158Z","shell.execute_reply":"2021-06-02T08:19:42.558875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the training and testing data into dataframes\ndir_path = \"/kaggle/input/nlp-getting-started/\"\ntrain_df = pd.read_csv(dir_path + \"train.csv\")\ntest_df = pd.read_csv(dir_path + \"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:19:44.062647Z","iopub.execute_input":"2021-06-02T08:19:44.062965Z","iopub.status.idle":"2021-06-02T08:19:44.250908Z","shell.execute_reply.started":"2021-06-02T08:19:44.062936Z","shell.execute_reply":"2021-06-02T08:19:44.250129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keep only the text and target columnds\ntraind_df = train_df.drop(columns = [\"id\", \"keyword\", \"location\"])\ntraind_df[\"target\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:19:45.171614Z","iopub.execute_input":"2021-06-02T08:19:45.171942Z","iopub.status.idle":"2021-06-02T08:19:45.1889Z","shell.execute_reply.started":"2021-06-02T08:19:45.171913Z","shell.execute_reply":"2021-06-02T08:19:45.187976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning the text data: removing URLs, html code and emoji\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_URL)\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_html)\ntrain_df[\"text\"] = train_df[\"text\"].apply(remove_emoji)\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_URL)\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_html)\ntest_df[\"text\"] = test_df[\"text\"].apply(remove_emoji)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:19:57.352815Z","iopub.execute_input":"2021-06-02T08:19:57.353128Z","iopub.status.idle":"2021-06-02T08:19:57.489992Z","shell.execute_reply.started":"2021-06-02T08:19:57.353097Z","shell.execute_reply":"2021-06-02T08:19:57.489136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"text\"].apply(lambda x:len(x.split())).plot(kind=\"hist\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:20:01.690391Z","iopub.execute_input":"2021-06-02T08:20:01.690713Z","iopub.status.idle":"2021-06-02T08:20:01.885447Z","shell.execute_reply.started":"2021-06-02T08:20:01.690685Z","shell.execute_reply":"2021-06-02T08:20:01.884655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:20:05.276591Z","iopub.execute_input":"2021-06-02T08:20:05.276921Z","iopub.status.idle":"2021-06-02T08:20:05.965119Z","shell.execute_reply.started":"2021-06-02T08:20:05.276892Z","shell.execute_reply":"2021-06-02T08:20:05.964262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate a tokenizer based on the Bert case sensitive model \ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:20:07.229733Z","iopub.execute_input":"2021-06-02T08:20:07.230067Z","iopub.status.idle":"2021-06-02T08:20:12.194844Z","shell.execute_reply.started":"2021-06-02T08:20:07.230037Z","shell.execute_reply":"2021-06-02T08:20:12.194067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert text sequences to numerical tokens (vector of numbers which can be fed into the model)\n\ntrain_tensor = tokenizer(list(train_df[\"text\"]), padding=\"max_length\", \n                         truncation=True, max_length=30, \n                         return_tensors=\"pt\")[\"input_ids\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:20:15.56776Z","iopub.execute_input":"2021-06-02T08:20:15.568092Z","iopub.status.idle":"2021-06-02T08:20:16.257768Z","shell.execute_reply.started":"2021-06-02T08:20:15.568062Z","shell.execute_reply":"2021-06-02T08:20:16.256918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a custom class to prepare the training data to be \n# in model input form (tuple of tokenized text sequence and tensor of target)\n\nclass TweetDataset:\n    def __init__(self, tensors, targ, ids):\n        self.text = tensors[ids]\n        self.targ = targ[ids].reset_index(drop=True)\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        t = self.text[idx]\n        y = self.targ[idx]\n        return t, tensor(y)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:20:19.044935Z","iopub.execute_input":"2021-06-02T08:20:19.045253Z","iopub.status.idle":"2021-06-02T08:20:19.051202Z","shell.execute_reply.started":"2021-06-02T08:20:19.045225Z","shell.execute_reply":"2021-06-02T08:20:19.049997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data in a trainig and validation set\ntrain_ids, valid_ids = RandomSplitter()(train_df)\n\n# Separate the y / target into a variable\ntarget = train_df[\"target\"]\n\n# create the input dataset based on the randomsplitter ids and utiliing the pre-processed tokens \ntrain_ds = TweetDataset(train_tensor, target, train_ids)\nvalid_ds = TweetDataset(train_tensor, target, valid_ids)\n\ntrain_dl = DataLoader(train_ds, bs=64)\nvalid_dl = DataLoader(valid_ds, bs=512)\n\ndls = DataLoaders(train_dl, valid_dl).to(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:20:56.699539Z","iopub.execute_input":"2021-06-02T08:20:56.699863Z","iopub.status.idle":"2021-06-02T08:20:56.726943Z","shell.execute_reply.started":"2021-06-02T08:20:56.699836Z","shell.execute_reply":"2021-06-02T08:20:56.726027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate model object (Bert model with classification output)\n\nbert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\").train().to(\"cuda\")\n\nclassifier = nn.Sequential(\n    nn.Linear(768, 300),\n    nn.ReLU(),\n    nn.BatchNorm1d(300),\n    nn.Dropout(0.5),\n    nn.Linear(300, 2)\n)\n\nbert.classifier = classifier\n\nclass BertClassifier(Module):\n    def __init__(self, bert):\n        self.bert = bert\n    def forward(self, x):\n        return self.bert(x).logits\n\nmodel = BertClassifier(bert)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:21:33.097863Z","iopub.execute_input":"2021-06-02T08:21:33.098176Z","iopub.status.idle":"2021-06-02T08:21:37.590195Z","shell.execute_reply.started":"2021-06-02T08:21:33.098146Z","shell.execute_reply":"2021-06-02T08:21:37.589375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up the fastai learner using the model insantiated in previous step and find optimal learning rate\n\nlearn = Learner(dls, model, loss_func=nn.CrossEntropyLoss(), metrics=[accuracy, F1Score()])\nlearn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:21:42.001684Z","iopub.execute_input":"2021-06-02T08:21:42.002004Z","iopub.status.idle":"2021-06-02T08:22:04.42804Z","shell.execute_reply.started":"2021-06-02T08:21:42.001974Z","shell.execute_reply":"2021-06-02T08:22:04.427302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model\nlearn.fit_one_cycle(4, lr_max=5e-5, wd=0.8)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:22:48.88355Z","iopub.execute_input":"2021-06-02T08:22:48.883984Z","iopub.status.idle":"2021-06-02T08:24:16.99132Z","shell.execute_reply.started":"2021-06-02T08:22:48.883948Z","shell.execute_reply":"2021-06-02T08:24:16.990393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check f1 scores if threshold for for the probability of the logit \n# with the highest probability is above a minimum threshold \n# (if logits is below this threshold, I believe the prediction is zero: \"not a disaster tweet\") \n\nfrom sklearn.metrics import f1_score\n\npreds, targs = learn.get_preds()\n\nmin_threshold = None\nmax_f1 = -float(\"inf\")\nthresholds = np.linspace(0.3, 0.7, 50)\nfor threshold in thresholds:\n    f1 = f1_score(targs, F.softmax(preds, dim=1)[:, 1]>threshold)\n    if f1 > max_f1:\n        min_threshold = threshold\n        min_f1 = f1\n    print(f\"thresholds:{threshold:.4f} - f1:{f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:24:28.556188Z","iopub.execute_input":"2021-06-02T08:24:28.556534Z","iopub.status.idle":"2021-06-02T08:24:29.998133Z","shell.execute_reply.started":"2021-06-02T08:24:28.556506Z","shell.execute_reply":"2021-06-02T08:24:29.997374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert text sequences to numerical tokens (vector of numbers which can be fed into the model)\n\ntest_tensor = tokenizer(list(test_df[\"text\"]),\n                        padding=\"max_length\",\n                        truncation=True,\n                        max_length=30,\n                        return_tensors=\"pt\")[\"input_ids\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:25:09.393988Z","iopub.execute_input":"2021-06-02T08:25:09.394342Z","iopub.status.idle":"2021-06-02T08:25:09.640638Z","shell.execute_reply.started":"2021-06-02T08:25:09.394301Z","shell.execute_reply":"2021-06-02T08:25:09.639732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a custom class to prepare the test data in the input form required for the model: a tuple\n# of text sequence and a tensor of zero\n\nclass TestDS:\n    def __init__(self, tensors):\n        self.tensors = tensors\n    \n    def __len__(self):\n        return len(self.tensors)\n    \n    def __getitem__(self, idx):\n        t = self.tensors[idx]\n        return t, tensor(0)\n\ntest_dl = DataLoader(TestDS(test_tensor), bs=128)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:25:12.109995Z","iopub.execute_input":"2021-06-02T08:25:12.110348Z","iopub.status.idle":"2021-06-02T08:25:12.116733Z","shell.execute_reply.started":"2021-06-02T08:25:12.11031Z","shell.execute_reply":"2021-06-02T08:25:12.11578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get test prediction from the learner model\n\ntest_preds = learn.get_preds(dl=test_dl)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:25:14.957033Z","iopub.execute_input":"2021-06-02T08:25:14.957384Z","iopub.status.idle":"2021-06-02T08:25:18.120989Z","shell.execute_reply.started":"2021-06-02T08:25:14.957352Z","shell.execute_reply":"2021-06-02T08:25:18.120206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submit output to Kaggle\nprediction = (F.softmax(test_preds[0], dim=1)[:, 1]>min_threshold).int()\nsub = pd.read_csv(dir_path + \"sample_submission.csv\")\nsub[\"target\"] = prediction\nsub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T08:25:20.891986Z","iopub.execute_input":"2021-06-02T08:25:20.892333Z","iopub.status.idle":"2021-06-02T08:25:21.041154Z","shell.execute_reply.started":"2021-06-02T08:25:20.892276Z","shell.execute_reply":"2021-06-02T08:25:21.040329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}