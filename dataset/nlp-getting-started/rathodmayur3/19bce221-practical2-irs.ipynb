{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Text Preprocessing Using `NLTK` On given Dataset","metadata":{}},{"cell_type":"markdown","source":"## 19BCE221 || `IRS` || Practical 2\n### Text Preprocessing using NLTK.\n#### Visualization \n#### Word Cloud \n#### Histogram of top N frequent terms","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport string\nimport numpy as np\nimport pandas as pd \nimport os\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer","metadata":{"execution":{"iopub.status.busy":"2022-03-12T03:20:04.456409Z","iopub.execute_input":"2022-03-12T03:20:04.456675Z","iopub.status.idle":"2022-03-12T03:20:05.962822Z","shell.execute_reply.started":"2022-03-12T03:20:04.456648Z","shell.execute_reply":"2022-03-12T03:20:05.961902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.DataFrame()\ndf=pd.read_csv('../input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-12T03:20:14.610086Z","iopub.execute_input":"2022-03-12T03:20:14.610383Z","iopub.status.idle":"2022-03-12T03:20:14.680825Z","shell.execute_reply.started":"2022-03-12T03:20:14.610346Z","shell.execute_reply":"2022-03-12T03:20:14.680016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:22.883826Z","iopub.execute_input":"2022-03-11T18:27:22.884145Z","iopub.status.idle":"2022-03-11T18:27:22.893826Z","shell.execute_reply.started":"2022-03-11T18:27:22.884115Z","shell.execute_reply":"2022-03-11T18:27:22.89308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:22.90079Z","iopub.execute_input":"2022-03-11T18:27:22.901357Z","iopub.status.idle":"2022-03-11T18:27:22.915591Z","shell.execute_reply.started":"2022-03-11T18:27:22.901316Z","shell.execute_reply":"2022-03-11T18:27:22.914834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['text']=df[:200]['text'].astype('str')\n\nsampledata=''\nfor x in range(7):\n    sampledata+=' '+data['text'][x]\n\ndata","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:22.917157Z","iopub.execute_input":"2022-03-11T18:27:22.917607Z","iopub.status.idle":"2022-03-11T18:27:22.930246Z","shell.execute_reply.started":"2022-03-11T18:27:22.917572Z","shell.execute_reply":"2022-03-11T18:27:22.929321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences=sent_tokenize(sampledata)\ndata1=pd.DataFrame()\n\nstop_words = set(stopwords.words(\"english\"))\npunc = string.punctuation\nno_sw=[]\n\nfor sentence in sentences: \n    no_sw+=list(set(word_tokenize(sentence)) - stop_words -set(punc) )\nno_sw\n\ndata1['Tokens']=no_sw\nporter = PorterStemmer()\nlancaster=LancasterStemmer()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:22.963807Z","iopub.execute_input":"2022-03-11T18:27:22.964258Z","iopub.status.idle":"2022-03-11T18:27:22.975033Z","shell.execute_reply.started":"2022-03-11T18:27:22.964225Z","shell.execute_reply":"2022-03-11T18:27:22.974107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# i use two type of stemming.\n# 1) porter 2) lancaster\nps=[]\nlc=[]\nfor token in no_sw:\n    ps.append(porter.stem(token))\n    lc.append(lancaster.stem(token))\n\ndata1['Porter']=ps\ndata1['Lancaster']=lc\ndata1.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:22.976962Z","iopub.execute_input":"2022-03-11T18:27:22.977761Z","iopub.status.idle":"2022-03-11T18:27:22.994467Z","shell.execute_reply.started":"2022-03-11T18:27:22.977721Z","shell.execute_reply":"2022-03-11T18:27:22.993223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Punctuation\n\nno_puncts=[]\nfor x in data[:]['text']:\n    no_puncts.append(x.translate(str.maketrans('', '', punc)))\n    \ndata[\"no_punc\"] = pd.DataFrame(no_puncts)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:22.99612Z","iopub.execute_input":"2022-03-11T18:27:22.996668Z","iopub.status.idle":"2022-03-11T18:27:23.011419Z","shell.execute_reply.started":"2022-03-11T18:27:22.996571Z","shell.execute_reply":"2022-03-11T18:27:23.01034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove stopword\nno_stops=[]\nfor x in data[:]['no_punc']:\n    no_stops.append(' '.join([word for word in str(x).split() if word not in stop_words]))\n    \ndata['no_punc_stops'] = pd.DataFrame(no_stops)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.043776Z","iopub.execute_input":"2022-03-11T18:27:23.044344Z","iopub.status.idle":"2022-03-11T18:27:23.062467Z","shell.execute_reply.started":"2022-03-11T18:27:23.044307Z","shell.execute_reply":"2022-03-11T18:27:23.061783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"por=[]\nlan=[]\nfor x in data[:]['no_punc_stops']:\n    por.append(' '.join([porter.stem(word) for word in str(x).split() ]))\n    lan.append(' '.join([lancaster.stem(word) for word in str(x).split()]))\n    \ndata['porter'] = pd.DataFrame(por)\ndata['lancaster'] = pd.DataFrame(lan)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.063925Z","iopub.execute_input":"2022-03-11T18:27:23.064316Z","iopub.status.idle":"2022-03-11T18:27:23.170417Z","shell.execute_reply.started":"2022-03-11T18:27:23.064282Z","shell.execute_reply":"2022-03-11T18:27:23.169737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# More towards text processing","metadata":{}},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"# One simpe example of Tokenizer\nfrom nltk.tokenize import word_tokenize\nstr2 = \"Rathod mayur 19bce221\"\nquotes_tokens1 = word_tokenize(str2)\nquotes_tokens1","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.171508Z","iopub.execute_input":"2022-03-11T18:27:23.172327Z","iopub.status.idle":"2022-03-11T18:27:23.178235Z","shell.execute_reply.started":"2022-03-11T18:27:23.172291Z","shell.execute_reply":"2022-03-11T18:27:23.177554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"str1 = \"\"\"According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, \nespecially intelligent computer programs”.Artificial Intelligence is a way of making a computer, a computer-controlled robot, \nor a software think intelligently, in the similar manner the intelligent humans think. \n\nAI is accomplished by studying how human brain thinks, and how humans learn, decide, and work while trying to solve a problem, \nand then using the outcomes of this study as a basis of developing intelligent software and systems.\"\"\"\n\ntype(str1)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.180343Z","iopub.execute_input":"2022-03-11T18:27:23.180805Z","iopub.status.idle":"2022-03-11T18:27:23.189781Z","shell.execute_reply.started":"2022-03-11T18:27:23.180768Z","shell.execute_reply":"2022-03-11T18:27:23.18897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nstr1_tokens = word_tokenize(str1)\nstr1_tokens","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.191453Z","iopub.execute_input":"2022-03-11T18:27:23.191962Z","iopub.status.idle":"2022-03-11T18:27:23.2013Z","shell.execute_reply.started":"2022-03-11T18:27:23.191925Z","shell.execute_reply":"2022-03-11T18:27:23.200506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(str1_tokens)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.20294Z","iopub.execute_input":"2022-03-11T18:27:23.203453Z","iopub.status.idle":"2022-03-11T18:27:23.210479Z","shell.execute_reply.started":"2022-03-11T18:27:23.203418Z","shell.execute_reply":"2022-03-11T18:27:23.209801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.probability import FreqDist\nfdist = FreqDist()\n\nfor word in str1_tokens:\n    fdist[word.lower()]+=1\nfdist","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.211604Z","iopub.execute_input":"2022-03-11T18:27:23.212239Z","iopub.status.idle":"2022-03-11T18:27:23.221125Z","shell.execute_reply.started":"2022-03-11T18:27:23.212201Z","shell.execute_reply":"2022-03-11T18:27:23.220293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(fdist['humans'])\n\nprint(len(fdist))\n\n# Output of len(fdist) is 61. At the starting time it 101.","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.243632Z","iopub.execute_input":"2022-03-11T18:27:23.243941Z","iopub.status.idle":"2022-03-11T18:27:23.249772Z","shell.execute_reply.started":"2022-03-11T18:27:23.24391Z","shell.execute_reply":"2022-03-11T18:27:23.248917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can also use any partitular paragraph from given data using blankline_tokenize.\nfrom nltk.tokenize import blankline_tokenize\nstr_blank = blankline_tokenize(str1)\nprint(len(str_blank))\n\nstr_blank[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.255014Z","iopub.execute_input":"2022-03-11T18:27:23.255565Z","iopub.status.idle":"2022-03-11T18:27:23.262498Z","shell.execute_reply.started":"2022-03-11T18:27:23.255535Z","shell.execute_reply":"2022-03-11T18:27:23.261656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.util import bigrams, trigrams, ngrams\nstr3 = \"The best and the most beautiful things in the world cannot be seen or even touched, they must be felt with the heart\"\nquotes_tokens = word_tokenize(str3)\nquotes_tokens","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.264445Z","iopub.execute_input":"2022-03-11T18:27:23.264879Z","iopub.status.idle":"2022-03-11T18:27:23.272975Z","shell.execute_reply.started":"2022-03-11T18:27:23.264834Z","shell.execute_reply":"2022-03-11T18:27:23.272167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quotes_bigrams = list(bigrams(quotes_tokens))\nquotes_bigrams","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.274536Z","iopub.execute_input":"2022-03-11T18:27:23.274988Z","iopub.status.idle":"2022-03-11T18:27:23.285036Z","shell.execute_reply.started":"2022-03-11T18:27:23.27495Z","shell.execute_reply":"2022-03-11T18:27:23.284288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quotes_trigrams = list(trigrams(quotes_tokens))\nquotes_trigrams","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.286657Z","iopub.execute_input":"2022-03-11T18:27:23.28694Z","iopub.status.idle":"2022-03-11T18:27:23.295994Z","shell.execute_reply.started":"2022-03-11T18:27:23.286908Z","shell.execute_reply":"2022-03-11T18:27:23.295046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here i give N = 5\nquotes_ngrams = list(nltk.ngrams(quotes_tokens, 5))\nquotes_ngrams","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.318886Z","iopub.execute_input":"2022-03-11T18:27:23.319074Z","iopub.status.idle":"2022-03-11T18:27:23.328153Z","shell.execute_reply.started":"2022-03-11T18:27:23.319052Z","shell.execute_reply":"2022-03-11T18:27:23.327227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stemming","metadata":{}},{"cell_type":"code","source":"#  Stamming : Normalize words into its base form or root form.  => stem\n#  Ex : Affectation, Affets, Affections, Affected ====> Affect \n\n#  Three Type : \n#      PorterStemmer\n#      LancasterStemmer\n#      SnowballStemmer (In this we tell language name)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.330287Z","iopub.execute_input":"2022-03-11T18:27:23.330929Z","iopub.status.idle":"2022-03-11T18:27:23.334737Z","shell.execute_reply.started":"2022-03-11T18:27:23.330892Z","shell.execute_reply":"2022-03-11T18:27:23.33376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\nfrom nltk.stem import SnowballStemmer\n\npst = PorterStemmer()\nlst = LancasterStemmer()\nsbts = SnowballStemmer('english')\n\nprint(pst.stem(\"having\"))\nprint(lst.stem(\"having\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.336251Z","iopub.execute_input":"2022-03-11T18:27:23.336753Z","iopub.status.idle":"2022-03-11T18:27:23.344983Z","shell.execute_reply.started":"2022-03-11T18:27:23.336706Z","shell.execute_reply":"2022-03-11T18:27:23.344021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_to_stem = [\"give\", \"giving\", \"given\", \"gave\"]\nfor words in word_to_stem:\n    print(words+ \":\" + pst.stem(words))\nprint(\"------------------\")\nfor words in word_to_stem:\n    print(words + \":\" + lst.stem(words))\n    \nprint(\"------------------\")\nfor words in word_to_stem:\n    print(words + \":\" + sbts.stem(words))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.34733Z","iopub.execute_input":"2022-03-11T18:27:23.347945Z","iopub.status.idle":"2022-03-11T18:27:23.358273Z","shell.execute_reply.started":"2022-03-11T18:27:23.347908Z","shell.execute_reply":"2022-03-11T18:27:23.357542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lemmatization","metadata":{}},{"cell_type":"code","source":"# Lemmatisation in linguistics is the process of grouping together the inflected forms of a word \n# so they can be analysed as a single item, identified by the word's lemma.\n\n# Somehow similar to Stemming, as it maps several words into one common root.\n# Output of Lemmatisation is a proper word\n# Ex : gone, going => go.","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.359548Z","iopub.execute_input":"2022-03-11T18:27:23.360585Z","iopub.status.idle":"2022-03-11T18:27:23.364492Z","shell.execute_reply.started":"2022-03-11T18:27:23.360543Z","shell.execute_reply":"2022-03-11T18:27:23.363786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import wordnet\nfrom nltk.stem import WordNetLemmatizer\nword_len = WordNetLemmatizer()\nword_len.lemmatize(\"corpora\")","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:23.365748Z","iopub.execute_input":"2022-03-11T18:27:23.366583Z","iopub.status.idle":"2022-03-11T18:27:25.174749Z","shell.execute_reply.started":"2022-03-11T18:27:23.366544Z","shell.execute_reply":"2022-03-11T18:27:25.173987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for words in word_to_stem:\n    print(words + \":\" + word_len.lemmatize(words))\n    \n# We get same output as word because we not apply POS = \"Parts Of Speech\".","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:25.176863Z","iopub.execute_input":"2022-03-11T18:27:25.177333Z","iopub.status.idle":"2022-03-11T18:27:25.183317Z","shell.execute_reply.started":"2022-03-11T18:27:25.177287Z","shell.execute_reply":"2022-03-11T18:27:25.182526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stop Words","metadata":{}},{"cell_type":"code","source":"# Stop words are useful in english language but not useful in natural process language (NLP)\nfrom nltk.corpus import stopwords\nprint(len(stopwords.words('english')))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:25.184861Z","iopub.execute_input":"2022-03-11T18:27:25.185184Z","iopub.status.idle":"2022-03-11T18:27:25.19342Z","shell.execute_reply.started":"2022-03-11T18:27:25.185151Z","shell.execute_reply":"2022-03-11T18:27:25.192297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:25.194958Z","iopub.execute_input":"2022-03-11T18:27:25.195457Z","iopub.status.idle":"2022-03-11T18:27:25.214629Z","shell.execute_reply.started":"2022-03-11T18:27:25.195419Z","shell.execute_reply":"2022-03-11T18:27:25.213771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I have to remove the stopwords form over string\nimport re\npunctuation = re.compile(r'[-.?!,:;()[0-9]')","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:25.216368Z","iopub.execute_input":"2022-03-11T18:27:25.216789Z","iopub.status.idle":"2022-03-11T18:27:25.221065Z","shell.execute_reply.started":"2022-03-11T18:27:25.216754Z","shell.execute_reply":"2022-03-11T18:27:25.220279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"after_puncturation = []\nfor words in str1_tokens:\n    word = punctuation.sub(\"\",words)\n    if len(word)>0 :\n        after_puncturation.append(word)\n        \nafter_puncturation","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:25.222601Z","iopub.execute_input":"2022-03-11T18:27:25.223162Z","iopub.status.idle":"2022-03-11T18:27:25.233957Z","shell.execute_reply.started":"2022-03-11T18:27:25.223125Z","shell.execute_reply":"2022-03-11T18:27:25.23319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### POS = Parts of Speech\n","metadata":{}},{"cell_type":"code","source":"# grammer, verbs, prepositions, nouns, articles, abjectives, conditionals\nnltk.help.upenn_tagset()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:25.235997Z","iopub.execute_input":"2022-03-11T18:27:25.23664Z","iopub.status.idle":"2022-03-11T18:27:25.280377Z","shell.execute_reply.started":"2022-03-11T18:27:25.236604Z","shell.execute_reply":"2022-03-11T18:27:25.279591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentance = \"Mayur is a honest when it comes to coding\"\nsentance_tokens = word_tokenize(sentance)\nsentance_tokens","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:25.285099Z","iopub.execute_input":"2022-03-11T18:27:25.285564Z","iopub.status.idle":"2022-03-11T18:27:25.297809Z","shell.execute_reply.started":"2022-03-11T18:27:25.285522Z","shell.execute_reply":"2022-03-11T18:27:25.296712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for token in sentance_tokens:\n    print(nltk.pos_tag([token]))","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:25.300069Z","iopub.execute_input":"2022-03-11T18:27:25.302436Z","iopub.status.idle":"2022-03-11T18:27:25.488702Z","shell.execute_reply.started":"2022-03-11T18:27:25.302392Z","shell.execute_reply":"2022-03-11T18:27:25.487982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Chunking","metadata":{}},{"cell_type":"markdown","source":"Picking up individual pieces of Information and Grouping then into bigger Pieces.","metadata":{}},{"cell_type":"markdown","source":"# Word Clouds","metadata":{}},{"cell_type":"markdown","source":"Hi! this is Rathod Mayur | `19bce221` || This is the part of my 2nd practical of IRS Course\"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image\nfrom wordcloud import WordCloud, ImageColorGenerator\n\n\nbackground_image = np.array(Image.open('/kaggle/input/image/masks-wordclouds/upvote.png'))\nplt.imshow(background_image)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:25.489887Z","iopub.execute_input":"2022-03-11T18:27:25.490614Z","iopub.status.idle":"2022-03-11T18:27:27.521817Z","shell.execute_reply.started":"2022-03-11T18:27:25.490574Z","shell.execute_reply":"2022-03-11T18:27:27.521108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_cloud2 = WordCloud(background_color = 'white',mask = background_image, \n               width = 2048, height = 1080).generate(\" \".join(data1['Porter']))\n# font color matching the masked image\nimg_colors = ImageColorGenerator(background_image)\nword_cloud2.recolor(color_func = img_colors)\n\n#saving the image\nword_cloud2.to_file('photo.png')\n\n# Show the word_cloud\nplt.imshow(word_cloud2, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:27.523159Z","iopub.execute_input":"2022-03-11T18:27:27.523646Z","iopub.status.idle":"2022-03-11T18:27:47.148723Z","shell.execute_reply.started":"2022-03-11T18:27:27.523607Z","shell.execute_reply":"2022-03-11T18:27:47.148062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"background_image2 = np.array(Image.open('/kaggle/input/image/masks-wordclouds/user.png'))\nplt.imshow(background_image2)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:47.149924Z","iopub.execute_input":"2022-03-11T18:27:47.151433Z","iopub.status.idle":"2022-03-11T18:27:48.12169Z","shell.execute_reply.started":"2022-03-11T18:27:47.15139Z","shell.execute_reply":"2022-03-11T18:27:48.121036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first read the data file\ntextfile = open('../input/txtfile/Week1Summary.txt', 'r').read()\n\nword_cloud2 = WordCloud(background_color = 'white',mask = background_image2, \n               width = 2048, height = 1080).generate(textfile)\n# font color matching the masked image\nimg_colors = ImageColorGenerator(background_image2)\nword_cloud2.recolor(color_func = img_colors)\n\n#saving the image\nword_cloud2.to_file('photo.png')\n\n# Show the word_cloud\nplt.imshow(word_cloud2, interpolation='bilinear')\ntitle = 'Most common words in this file'\nplt.title(title, fontdict={'size': 25, 'color': 'green', \n                                  'verticalalignment': 'bottom'})\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:48.123029Z","iopub.execute_input":"2022-03-11T18:27:48.12329Z","iopub.status.idle":"2022-03-11T18:27:52.001691Z","shell.execute_reply.started":"2022-03-11T18:27:48.12324Z","shell.execute_reply":"2022-03-11T18:27:52.001043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Advantages of Word Clouds : \n#     Analyzing customer and employee feedback.\n#     Identifying new SEO keywords to target.\n\n# Drawbacks of Word Clouds : \n#     Word Clouds are not perfect for every situation.\n#     Data should be optimized for context.","metadata":{"execution":{"iopub.status.busy":"2022-03-11T18:27:52.003052Z","iopub.execute_input":"2022-03-11T18:27:52.003523Z","iopub.status.idle":"2022-03-11T18:27:52.007019Z","shell.execute_reply.started":"2022-03-11T18:27:52.003485Z","shell.execute_reply":"2022-03-11T18:27:52.006379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}