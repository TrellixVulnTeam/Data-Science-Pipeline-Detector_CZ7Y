{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Sentence Classification\n\n\nIn this notebook, We will be classifying text (The data-set used here contains tweets, but the process shown here can be adapted for other text classification tasks too.)\nThe content is arranged as follows:\n* Cleaninig and basic pre-processing of text\n* Building a vocabulary, and creating iterators using TorchText\n* Building a sequence model - LSTM using Pytorch to predict labels\n\n**_Notebook is still under construction...._**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the files in Data\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport time\n\nimport torch\nfrom torchtext import data\nimport torch.nn as nn\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"847ec12886b0a3bbe7edc2ed9644fe72a91e8d36"},"cell_type":"code","source":"# Import Data\n\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Data Pre Processing**\nCleaning the text data "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of dataset\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7c0c16f029480aedfe950f2069b722f3e5c973b"},"cell_type":"markdown","source":"#### Let us get a glimpse at the data table"},{"metadata":{"trusted":true,"_uuid":"6594b5897b28f5836fd5690e808526e9190a3bf2"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15d18321c816f4dae82ef83272bfea53d2569e6d"},"cell_type":"markdown","source":"#### The `target` column marks the label of the text:\n* **\n* **label==1** : If the Tweet is about Disasters.\n* **label==0** : If the Tweet is not about disasters. \n\nWe are only interested in the `text` and `target` columns. So we drop the rest. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop 'id' , 'keyword' and 'location' columns.\ntrain.drop(columns=['id','keyword','location'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39a768b75c4b70cc71be47968831217a82759e7f"},"cell_type":"markdown","source":"### Next we clean and modify the texts, so that the classification algorithm does not get confused with irrelevant information. "},{"metadata":{"trusted":true,"_uuid":"bfb5962baab7fd0deefcb8943849872d92418fd9"},"cell_type":"code","source":"# to clean data\ndef normalise_text (text):\n    text = text.str.lower() # lowercase\n    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n    text = text.str.replace(r\"@\",\"\")\n    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n    text = text.str.replace(\"\\s{2,}\", \" \")\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a05812772c695c58461339f1f541690219957f28"},"cell_type":"code","source":"train[\"text\"]=normalise_text(train[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3eb51c856ca81fc6db51a7de4a8c34a5243d23d7"},"cell_type":"markdown","source":"Let us look at the cleaned text once"},{"metadata":{"trusted":true,"_uuid":"06df411fc085383179e72f8d5236492be57c6715"},"cell_type":"code","source":"train['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the data into training and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data into train and validation \ntrain_df, valid_df = train_test_split(train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following will help make the results reproducible later.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\n\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to create `Field` objects to process the text data. These field objects will contain information for converting the texts to Tensors.\nWe will set two parameters:\n* `tokenize=spacy` and \n* `include_arguments=True`\nWhich implies that SpaCy will be used to tokenize the texts and that the field objects should include length of the texts - which will be needed to pad the texts. \nWe will later use methods of these objects to create a vocabulary, which will help us create a numerical representation for every token.\n\nThe `LabelField` is a shallow wrappper around field, useful for data labels. "},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\nLABEL = data.LabelField(dtype = torch.float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we create a `DataFrameDataset` class which will allow us to load the data and the target-labels as a `DataSet` using a DataFrame as a source of data.\nWe will create a vocabulary using the training dataset and then pass the training and validation datasets to the iterator later. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# source : https://gist.github.com/lextoumbourou/8f90313cbc3598ffbabeeaa1741a11c8\n# to use DataFrame as a Data source\n\nclass DataFrameDataset(data.Dataset):\n\n    def __init__(self, df, fields, is_test=False, **kwargs):\n        examples = []\n        for i, row in df.iterrows():\n            label = row.target if not is_test else None\n            text = row.text\n            examples.append(data.Example.fromlist([text, label], fields))\n\n        super().__init__(examples, fields, **kwargs)\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n        train_data, val_data, test_data = (None, None, None)\n        data_field = fields\n\n        if train_df is not None:\n            train_data = cls(train_df.copy(), data_field, **kwargs)\n        if val_df is not None:\n            val_data = cls(val_df.copy(), data_field, **kwargs)\n        if test_df is not None:\n            test_data = cls(test_df.copy(), data_field, True, **kwargs)\n\n        return tuple(d for d in (train_data, val_data, test_data) if d is not None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We will first create a list called _field_, where the elements will be a tuple of string (name) and `Field` object. The `Field` object for the text should be placed with name 'text'  and the object for label should be placed with name 'label'\n\nThen we will use the `splits` method of `DataFrameDataset`, which will return the training and validation datasets, which will be composed of Examples of the tokenized texts and labels. The texts and labels will have the name that we provide to the _field_.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"fields = [('text',TEXT), ('label',LABEL)]\n\ntrain_ds, val_ds = DataFrameDataset.splits(fields, train_df=train_df, val_df=valid_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at a random example\nprint(vars(train_ds[15]))\n\n# Check the type \nprint(type(train_ds[15]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now build the vocabulary using only the training dataset. This can be accessed through `TEXT.vocab` and will be shared by the validation dataset.\n\nWe will use pretrainied 200 dimensional vectors to represent the tokens. Any unknown token will have a zero vector. These vectors will be later loaded as the embedding layer. "},{"metadata":{"trusted":true,"_uuid":"137eaa9eadbf47da7362794a2d0152a4855db04b"},"cell_type":"code","source":"MAX_VOCAB_SIZE = 25000\n\nTEXT.build_vocab(train_ds, \n                 max_size = MAX_VOCAB_SIZE, \n                 vectors = 'glove.6B.200d',\n                 unk_init = torch.Tensor.zero_)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LABEL.build_vocab(train_ds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We build the iterators. "},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 128\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator = data.BucketIterator.splits(\n    (train_ds, val_ds), \n    batch_size = BATCH_SIZE,\n    sort_within_batch = True,\n    device = device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM architecture"},{"metadata":{"_uuid":"651e4c67928760a49098a5d536b3ecab6ae1314d"},"cell_type":"markdown","source":"### Declare Hyperparameters"},{"metadata":{"trusted":true,"_uuid":"0057c657aa1fe65fca683ac97df2cede9a4f7325"},"cell_type":"code","source":"# Hyperparameters\nnum_epochs = 25\nlearning_rate = 0.001\n\nINPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 200\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.2\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # padding","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75fdbeccfc961d0b3f93bedab1bb1b86a80ac2a6"},"cell_type":"markdown","source":"### Setting up the LSTM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM_net(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 bidirectional, dropout, pad_idx):\n        \n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n        \n        self.rnn = nn.LSTM(embedding_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n                           bidirectional=bidirectional, \n                           dropout=dropout)\n        \n        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n        \n        self.fc2 = nn.Linear(hidden_dim, 1)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text, text_lengths):\n        \n        # text = [sent len, batch size]\n        \n        embedded = self.embedding(text)\n        \n        # embedded = [sent len, batch size, emb dim]\n        \n        #pack sequence\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n        \n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        \n        #unpack sequence\n        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n\n        # output = [sent len, batch size, hid dim * num directions]\n        # output over padding tokens are zero tensors\n        \n        # hidden = [num layers * num directions, batch size, hid dim]\n        # cell = [num layers * num directions, batch size, hid dim]\n        \n        # concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n        # and apply dropout\n        \n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        output = self.fc1(hidden)\n        output = self.dropout(self.fc2(output))\n                \n        #hidden = [batch size, hid dim * num directions]\n            \n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating instance of our LSTM_net class\n\nmodel = LSTM_net(INPUT_DIM, \n            EMBEDDING_DIM, \n            HIDDEN_DIM, \n            OUTPUT_DIM, \n            N_LAYERS, \n            BIDIRECTIONAL, \n            DROPOUT, \n            PAD_IDX)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"loading the pretrained vectors into the embedding matrix. "},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_embeddings = TEXT.vocab.vectors\n\nprint(pretrained_embeddings.shape)\nmodel.embedding.weight.data.copy_(pretrained_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  to initiaise padded to zeros\nmodel.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n\nprint(model.embedding.weight.data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03e87004b8803019eb1ab08ac8435b01162e69c4"},"cell_type":"code","source":"model.to(device) #CNN to GPU\n\n\n# Loss and optimizer\ncriterion = nn.BCEWithLogitsLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binary_accuracy(preds, y):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"\n\n    #round predictions to the closest integer\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float() #convert into float for division \n    acc = correct.sum() / len(correct)\n    return acc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07072e1e57251e9133e6762f642b42d9f53948d4"},"cell_type":"markdown","source":"### Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# training function \ndef train(model, iterator):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        text, text_lengths = batch.text\n        \n        optimizer.zero_grad()\n        predictions = model(text, text_lengths).squeeze(1)\n        loss = criterion(predictions, batch.label)\n        acc = binary_accuracy(predictions, batch.label)\n\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, iterator):\n    \n    epoch_acc = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for batch in iterator:\n            text, text_lengths = batch.text\n            predictions = model(text, text_lengths).squeeze(1)\n            acc = binary_accuracy(predictions, batch.label)\n            \n            epoch_acc += acc.item()\n        \n    return epoch_acc / len(iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time.time()\nloss=[]\nacc=[]\nval_acc=[]\n\nfor epoch in range(num_epochs):\n    \n    train_loss, train_acc = train(model, train_iterator)\n    valid_acc = evaluate(model, valid_iterator)\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n    \n    loss.append(train_loss)\n    acc.append(train_acc)\n    val_acc.append(valid_acc)\n    \nprint(f'time:{time.time()-t:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b347c0182fc06e547c25ef2ee3a9f2c35c8e928"},"cell_type":"markdown","source":"### Plot a graph to trace model performance"},{"metadata":{"trusted":true,"_uuid":"a3299a7f04480735cda7d18a3b5875d2ca3d62d0","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2fdc188dc5754b2b6898f8c8fb7d5af9d32afb1","_kg_hide-output":false,"_kg_hide-input":true},"cell_type":"code","source":"plt.xlabel(\"runs\")\nplt.ylabel(\"normalised measure of loss/accuracy\")\nx_len=list(range(len(acc)))\nplt.axis([0, max(x_len), 0, 1])\nplt.title('result of LSTM')\nloss=np.asarray(loss)/max(loss)\nplt.plot(x_len, loss, 'r.',label=\"loss\")\nplt.plot(x_len, acc, 'b.', label=\"accuracy\")\nplt.plot(x_len, val_acc, 'g.', label=\"val_accuracy\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.2)\nplt.show\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}