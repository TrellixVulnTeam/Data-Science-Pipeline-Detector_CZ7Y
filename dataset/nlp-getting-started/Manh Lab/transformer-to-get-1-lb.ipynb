{"cells":[{"metadata":{"id":"4dgGvHUE1UL6"},"cell_type":"markdown","source":"## TPU use in this notebook to speed up training time.\n![XLA](https://xla.rocks/_xlawpx/wp-content/uploads/2019/02/logo-xla-2019.svg)\nTransformers to finetune\n\nPyTorch/XLA is a Python package that uses the XLA deep learning compiler to connect the PyTorch deep learning framework and Cloud TPUs. You can try it right now, for free, on a single Cloud TPU with Google Colab, and use it in production and on Cloud TPU Pods with Google Cloud.\nNice to use it in this notebook!\n\n\n* Note:\n     Install XLA must the first shell you run to success install it.\nI cannot train model on Kaggle because memory and RAM requirement. So i trainning it on colab. You can visit this [link](https://colab.research.google.com/drive/16zHlALeStz-2vWBgE5Zol4wguPCz2Avz?usp=sharing) for details: \n\nPlease give me a upvote if it helpfully!!!!!!!!!!!!\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I got top 1% accuracy with ensemble of BERT+ROBERTA+ELECTRA. ","execution_count":null},{"metadata":{"id":"UK1_SuCu1Nb9","outputId":"96af69a0-d4b0-4254-d340-a18264677d1a","trusted":false},"cell_type":"code","source":"VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version $VERSION","execution_count":null,"outputs":[]},{"metadata":{"id":"w5KHHh0Q1Sy1","outputId":"d20dc7c7-b2c2-4273-e9f0-f8b2a0b72573","trusted":false},"cell_type":"code","source":"!pip install transformers==3.0.0","execution_count":null,"outputs":[]},{"metadata":{"id":"_ZyCNtjy5j7G"},"cell_type":"markdown","source":"## Data Processing\nPreprocessing: Not in 2020\n* Just split in 2 dataset. \n","execution_count":null},{"metadata":{"id":"KFRCVDsox0SG","trusted":false},"cell_type":"code","source":"from sklearn.metrics import  accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('data/train.csv')\ntrain = train.fillna('')\ntrain['text'] = train['keyword'] + ' ' + train['location'] + ' '+ train['text']\ntrain = train[['text','target']]\ndf_train, df_valid = train_test_split(train, test_size=0.2, random_state=42)\ndf_train.to_csv('train.csv')\ndf_valid.to_csv('valid.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model config ","execution_count":null},{"metadata":{"id":"F6SQguh4exqs","trusted":false},"cell_type":"code","source":"from transformers import BertConfig, RobertaConfig, DistilBertConfig, BertModel, RobertaModel, DistilBertModel\nfrom torch import nn\nimport torch\nclass BertStyleModel(torch.nn.Module):\n    \n    def __init__(self, model_type):\n        super().__init__()\n        \n        self.model_type = model_type\n        if (model_type == 'roberta'):\n            config_path = 'roberta-base'\n            model_path = 'roberta-base'\n            config = RobertaConfig.from_pretrained(config_path)\n            config.output_hidden_states = True\n            self.bert = RobertaModel.from_pretrained(model_path, config=config)\n        elif (model_type == 'distilbert'):\n            config_path = 'distilbert-base-uncased'\n            config = DistilBertConfig.from_json_file(config_path)\n            model_path = 'distilbert-base-uncased'\n            config.output_hidden_states = True\n            self.bert = DistilBertModel.from_pretrained(model_path, config=config)\n        elif (model_type == 'bert'):\n            config_path = 'bert-base-uncased'\n            config = BertConfig.from_pretrained(config_path)\n            config.output_hidden_states = True\n            model_path = 'bert-base-uncased'\n            self.bert = BertModel.from_pretrained(model_path, config=config)\n            \n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)   \n        self.cls_token_head = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(768 * 4, 768),\n            nn.ReLU(inplace=True),\n        )\n        self.classifier = nn.Linear(768, 1)\n        \n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n        \n        if (self.model_type == 'roberta'):\n            outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            hidden_layers = outputs[2]\n        elif (self.model_type == 'distilbert'):\n            outputs = self.bert(input_ids, attention_mask=attention_mask)\n            hidden_layers = outputs[1]\n        elif (self.model_type == 'bert'):     \n            outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            hidden_layers = outputs[2]\n        \n        hidden_states_cls_embeddings = [x[:, 0] for x in hidden_layers[-4:]]\n        x = torch.cat(hidden_states_cls_embeddings, dim=-1)\n        cls_output = self.cls_token_head(x)\n        logits = torch.mean(torch.stack([\n            #Multi Sample Dropout takes place here\n            self.classifier(self.high_dropout(cls_output))\n            for _ in range(5)\n        ], dim=0), dim=0)\n        outputs = logits\n        return outputs","execution_count":null,"outputs":[]},{"metadata":{"id":"T_HBz-W46v6w"},"cell_type":"markdown","source":"## Training","execution_count":null},{"metadata":{"id":"AycyM8skhFqI","trusted":false},"cell_type":"code","source":"from torch import nn\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom transformers import T5Tokenizer, T5Model\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef loss_fn(ypred, label):\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\ndef seed_all(seed):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)","execution_count":null,"outputs":[]},{"metadata":{"id":"BNJ-lQcW5xDi","trusted":false},"cell_type":"code","source":"\nimport os\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom collections import OrderedDict, namedtuple\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport joblib\n\nimport logging\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\nimport sys\nfrom sklearn import metrics, model_selection\n\nimport warnings\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nclass BERTDatasetTraining:\n    def __init__(self, comment_text, targets, tokenizer, max_length):\n        self.comment_text = comment_text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.comment_text)\n\n    def __getitem__(self, item):\n        comment_text = str(self.comment_text[item])\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            comment_text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        padding_length = self.max_length - len(ids)\n        \n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"id":"5x_hECjONtBy"},"cell_type":"markdown","source":"## BERT","execution_count":null},{"metadata":{"id":"GZhQNiO16uHi","trusted":false},"cell_type":"code","source":"import torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\nfrom sklearn import metrics\nimport torch\nfrom  transformers import AdamW\nfrom transformers import  get_linear_schedule_with_warmup\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport transformers\nmx = BertStyleModel('bert')\ndef _run():\n    def loss_fn(outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\n    def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n        model.train()\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            optimizer.zero_grad()\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n            loss = loss_fn(outputs, targets)\n            if bi % 10 == 0:\n                xm.master_print(f'bi={bi}, loss={loss}')\n\n            loss.backward()\n            xm.optimizer_step(optimizer)\n            if scheduler is not None:\n                scheduler.step()\n\n    def eval_loop_fn(data_loader, model, device):\n        model.eval()\n        fin_targets = []\n        fin_outputs = []\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n            targets_np = targets.cpu().detach().numpy().tolist()\n            outputs_np = outputs.cpu().detach().numpy().tolist()\n            fin_targets.extend(targets_np)\n            fin_outputs.extend(outputs_np)    \n\n        return fin_outputs, fin_targets\n\n    df_train = pd.read_csv('train.csv')\n    df_valid = pd.read_csv('valid.csv')\n    MAX_LEN = 192\n    TRAIN_BATCH_SIZE = 4\n    EPOCHS = 3\n\n    tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n\n    train_targets = df_train.target.values\n    valid_targets = df_valid.target.values\n\n    train_dataset = BERTDatasetTraining(\n        comment_text=df_train.text.values,\n        targets=train_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=True)\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=1\n    )\n\n    valid_dataset = BERTDatasetTraining(\n        comment_text=df_valid.text.values,\n        targets=valid_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=16,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n    \n    device = xm.xla_device()\n    model = mx.to(device)\n    \n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    lr = 0.4 * 1e-5 * xm.xrt_world_size()\n    num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n        xm.save(model.state_dict(), \"bert.bin\")\n        np.save('bert', np.array(o))\n        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n        xm.master_print(f'AUC = {auc}')","execution_count":null,"outputs":[]},{"metadata":{"id":"Jb1YQ81_h-88","outputId":"070e01b2-9548-484d-ad02-11fccf640bb6","trusted":false},"cell_type":"code","source":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"id":"9qd0HBJIiFVQ","trusted":false},"cell_type":"code","source":"class BERTDatasetTest:\n    def __init__(self, comment_text, tokenizer, max_length):\n        self.comment_text = comment_text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.comment_text)\n\n    def __getitem__(self, item):\n        comment_text = str(self.comment_text[item])\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            comment_text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        padding_length = self.max_length - len(ids)\n        \n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n           \n        }\ndef test_loop_fn(data_loader, model, device):\n        model.eval()\n        fin_targets = []\n        fin_outputs = []\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            \n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n          \n            outputs_np = outputs.cpu().detach().numpy().tolist()\n            fin_outputs.extend(outputs_np)    \n\n        return fin_outputs\ndef test_model():\n  MAX_LEN = 192\n  device = xm.xla_device()\n  model = BertStyleModel('bert')\n  model.load_state_dict(torch.load(\"bert.bin\"))\n  model.to(device)\n  df_test = pd.read_csv('data/test.csv')\n  df_test = df_test.fillna('')\n  df_test['text'] = df_test['keyword'] + ' ' + df_test['location'] + ' '+ df_test['text']\n  df_test = df_test[['text']]\n  tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n\n  test_dataset = BERTDatasetTest(\n        comment_text=df_test.text.values,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n  test_sampler = torch.utils.data.distributed.DistributedSampler(\n          test_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n  test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=16,\n        sampler=test_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n  para_loader = pl.ParallelLoader(test_loader, [device])\n  ypred = test_loop_fn(para_loader.per_device_loader(device), model, device)\n  ypred= np.array(ypred)\n  np.save('result.npy', ypred)\n  return ypred\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"-XH2brxUrSvb","outputId":"ec357e73-d425-4af8-d55f-e9a026cb38f2","trusted":false},"cell_type":"code","source":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = test_model()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"id":"OBAWVejMuAY8","outputId":"4bae9c17-27a5-4eaa-d14f-52be75326dc1","trusted":false},"cell_type":"code","source":"submit = pd.read_csv('data/sample_submission.csv')\nypred = np.load('result.npy')\nsubmit['target'] = ypred\nsubmit['target'].hist()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"v-AyUNJ0vDfw","outputId":"a157031a-7aff-4873-bb58-a3c1dee366bd","trusted":false},"cell_type":"code","source":"pred = [int(i> 0.5) for i in ypred] \nplt.hist(pred)\nplt.show()\nsubmit['target'] = pred\nsubmit.to_csv('submit.csv', index=False)\n### get 81.918 acccuracy","execution_count":null,"outputs":[]},{"metadata":{"id":"1FE4y6BaNl-u"},"cell_type":"markdown","source":"## ROBERTA","execution_count":null},{"metadata":{"id":"BqylfObIPESa","trusted":false},"cell_type":"code","source":"class BERTDatasetTraining:\n    def __init__(self, comment_text, targets, tokenizer, max_length):\n        self.comment_text = comment_text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.comment_text)\n\n    def __getitem__(self, item):\n        comment_text = str(self.comment_text[item])\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            comment_text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        padding_length = self.max_length - len(ids)\n        \n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"id":"5v7mWcpRxN1x","trusted":false},"cell_type":"code","source":"import torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\nfrom sklearn import metrics\nimport torch\nfrom  transformers import AdamW\nfrom transformers import  get_linear_schedule_with_warmup\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport transformers\n\nmx = BertStyleModel('roberta')\ndef _run():\n    def loss_fn(outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\n    def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n        model.train()\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            optimizer.zero_grad()\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n            )\n\n            loss = loss_fn(outputs, targets)\n            if bi % 10 == 0:\n                xm.master_print(f'bi={bi}, loss={loss}')\n\n            loss.backward()\n            xm.optimizer_step(optimizer)\n            if scheduler is not None:\n                scheduler.step()\n\n    def eval_loop_fn(data_loader, model, device):\n        model.eval()\n        fin_targets = []\n        fin_outputs = []\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n            )\n\n            targets_np = targets.cpu().detach().numpy().tolist()\n            outputs_np = outputs.cpu().detach().numpy().tolist()\n            fin_targets.extend(targets_np)\n            fin_outputs.extend(outputs_np)    \n\n        return fin_outputs, fin_targets\n\n    df_train = pd.read_csv('train.csv')\n    df_valid = pd.read_csv('valid.csv')\n    MAX_LEN = 192\n    TRAIN_BATCH_SIZE = 4\n    EPOCHS = 3\n\n    tokenizer = transformers.RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n\n    train_targets = df_train.target.values\n    valid_targets = df_valid.target.values\n\n    train_dataset = BERTDatasetTraining(\n        comment_text=df_train.text.values,\n        targets=train_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=True)\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=1\n    )\n\n    valid_dataset = BERTDatasetTraining(\n        comment_text=df_valid.text.values,\n        targets=valid_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=16,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n    \n    device = xm.xla_device()\n    model = mx.to(device)\n\n    print(\"%s: traning\"%device)\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    lr = 0.4 * 1e-5 * xm.xrt_world_size()\n    num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)\n    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n        xm.save(model.state_dict(), \"roberta.bin\")\n        np.save('roberta', np.array(t))\n        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n        xm.master_print(f'AUC = {auc}')","execution_count":null,"outputs":[]},{"metadata":{"id":"uThbrp4N2ik3","outputId":"73c7b856-9c25-4fd7-df94-0729c762980b","trusted":false},"cell_type":"code","source":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"id":"sbe1xWayycai","trusted":false},"cell_type":"code","source":"class BERTDatasetTest:\n    def __init__(self, comment_text, tokenizer, max_length):\n        self.comment_text = comment_text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.comment_text)\n\n    def __getitem__(self, item):\n        comment_text = str(self.comment_text[item])\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            comment_text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        padding_length = self.max_length - len(ids)\n        \n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),           \n        }\ndef test_loop_fn(data_loader, model, device):\n        model.eval()\n        fin_targets = []\n        fin_outputs = []\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            \n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n            )\n\n          \n            outputs_np = outputs.cpu().detach().numpy().tolist()\n            fin_outputs.extend(outputs_np)    \n\n        return fin_outputs\ndef test_model():\n  MAX_LEN = 192\n  device = xm.xla_device()\n  model = BertStyleModel('roberta')\n  model.load_state_dict(torch.load(\"roberta.bin\"))\n  model.to(device)\n  df_test = pd.read_csv('data/test.csv')\n  df_test = df_test.fillna('')\n  df_test['text'] = df_test['keyword'] + ' ' + df_test['location'] + ' '+ df_test['text']\n  df_test = df_test[['text']]\n  tokenizer = transformers.RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n\n  test_dataset = BERTDatasetTest(\n        comment_text=df_test.text.values,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n  test_sampler = torch.utils.data.distributed.DistributedSampler(\n          test_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n  test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=16,\n        sampler=test_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n  para_loader = pl.ParallelLoader(test_loader, [device])\n  ypred = test_loop_fn(para_loader.per_device_loader(device), model, device)\n  ypred= np.array(ypred)\n  np.save('result2.npy', ypred)\n  return ypred\n","execution_count":null,"outputs":[]},{"metadata":{"id":"IBWahjfmyp8a","trusted":false},"cell_type":"code","source":"# Start test processes XLA\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = test_model()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"id":"GkRM1LBTyvQT","outputId":"88ffa836-1e68-40cf-b8ba-012b1543eb8a","trusted":false},"cell_type":"code","source":"submit = pd.read_csv('data/sample_submission.csv')\nypred = np.load('result2.npy')\nsubmit['target'] = ypred\nplt.hist(ypred)\nplt.show()\npred = [int(i> 0.5) for i in ypred] \nplt.hist(pred)\nplt.show()\nsubmit['target'] = pred\nsubmit.to_csv('submit2.csv', index=False)\n### get 0.83389 acccuracy","execution_count":null,"outputs":[]},{"metadata":{"id":"f6KESTfaQKDI"},"cell_type":"markdown","source":"## Postprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Try to concat 2 model.\nI really think: ROBERTA is better for this task. So try to multiply Roberta result with greatter 1 number. Get result better. \n    Summary: roberta*1.3 and get top 15%","execution_count":null,"outputs":[]},{"metadata":{"id":"263JbTJpQJgS","trusted":false},"cell_type":"code","source":"bert = np.load('result.npy')\nroberta= np.load('result2.npy')","execution_count":null,"outputs":[]},{"metadata":{"id":"Q9Hznul5R75K","trusted":false},"cell_type":"code","source":"bert_ = np.array([int(b>0.5) for b in bert])\nroberta_ = np.array([int(b>0.5) for b in roberta])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"hV8V5RWMSIzG","outputId":"23235dfd-fad0-4b47-cb71-a189ebfda6c3","trusted":false},"cell_type":"code","source":"bert_.sum() , roberta_.sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"wc2vwut6SOh8","outputId":"36487d11-b7f0-4449-a400-637aa4e9a6c3","trusted":false},"cell_type":"code","source":"roberta2 = roberta*1.3 #  Best today is 0.83512\nroberta_ = np.array([int(b>0.5) for b in roberta2])\nprint(roberta_.sum())","execution_count":null,"outputs":[]},{"metadata":{"id":"q3sYF0nJSc9-","outputId":"26302b86-7118-4e78-8277-da9635d6da00","trusted":false},"cell_type":"code","source":"submit = pd.read_csv('sample_submission.csv')\nypred = roberta2\nsubmit['target'] = ypred\nplt.hist(ypred)\nplt.show()\npred = [int(i> 0.5) for i in ypred] \nplt.hist(pred)\nplt.show()\nsubmit['target'] = pred\nsubmit.to_csv('submit2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Submit the file\nimport pandas as pd\nsubmit = pd.read_csv('/kaggle/input/nlpstarted/submit2 (1).csv')\nsubmit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}