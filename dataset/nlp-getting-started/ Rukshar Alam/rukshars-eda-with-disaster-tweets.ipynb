{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTigQWzoYCNiDyrz1BN4WTf2X2k9OZ_yvW-FsmcIMsdS9fppNmh)\n","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"competition-description\"></a><span style=\"color:#328c4f\">Competition Description</span>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">\n    Twitter has become an important communication channel in times of emergency.\n    The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).<br>\n    But, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:<br>\n    \n</p>\n<img src=\"https://storage.googleapis.com/kaggle-media/competitions/tweet_screenshot.png\" alt=\"drawing\" width=\"250\"/><br>\n\n<p style=\"font-size:19px\">\n    The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.\n    <br>\nIn this competition,we have to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. The dataset has 10,000 tweets that were hand classified.\n    <br>\n<b>Disclaimer</b> : The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n    \n</p>","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"data-description\"></a><span style=\"color:#328c4f\">Data Description</span>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:22px; color:#04661e\">Data Format</p>\n<p style=\"font-size:19px\">Each sample in the train and test set has the following information:</p>\n\n- <span style=\"font-size:19px\">The text of a tweet </span>\n- <span style=\"font-size:19px\">A keyword from that tweet (although this may be blank!)</span>\n- <span style=\"font-size:19px\">The location the tweet was sent from (may also be blank)</span>\n\n<p style=\"font-size:19px\">You are predicting whether a given tweet is about a real disaster or not. If so, predict a <b>1</b>. If not, predict a <b>0</b></p>\n\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:22px; color:#04661e\">Columns</p>\n<ol>\n  <li style=\"font-size:19px\">id: a unique identifier for each tweet</li>\n  <li style=\"font-size:19px\">text: the text of the tweet</li>\n  <li style=\"font-size:19px\">location: the location the tweet was sent from (may be blank)</li>\n  <li style=\"font-size:19px\">keyword: a particular keyword from the tweet (may be blank)</li>\n  <li style=\"font-size:19px\">target: in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)</li>\n</ol>\n","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"import-library\"></a><span style=\"color:#328c4f\">Importing Libraries</span>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import Adam\nimport os\nimport plotly\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nimport plotly.graph_objs as go\nfrom sklearn.decomposition import PCA\nfrom gensim.test.utils import datapath, get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nimport pickle\nfrom wordcloud import WordCloud\noffline.init_notebook_mode(connected = True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-15T09:23:59.364812Z","iopub.execute_input":"2022-05-15T09:23:59.365272Z","iopub.status.idle":"2022-05-15T09:23:59.418541Z","shell.execute_reply.started":"2022-05-15T09:23:59.365235Z","shell.execute_reply":"2022-05-15T09:23:59.41782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"data-load\"></a><span style=\"color:#328c4f\">Loading data and Basic Idea </span>","metadata":{}},{"cell_type":"code","source":"tweet= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\ntweet.head(3)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-15T08:51:10.171623Z","iopub.execute_input":"2022-05-15T08:51:10.172102Z","iopub.status.idle":"2022-05-15T08:51:10.245365Z","shell.execute_reply.started":"2022-05-15T08:51:10.172062Z","shell.execute_reply":"2022-05-15T08:51:10.244667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-15T08:51:14.959263Z","iopub.execute_input":"2022-05-15T08:51:14.959655Z","iopub.status.idle":"2022-05-15T08:51:14.966928Z","shell.execute_reply.started":"2022-05-15T08:51:14.959497Z","shell.execute_reply":"2022-05-15T08:51:14.966005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"data-load\"></a><span style=\"color:#328c4f\">Class distribution</span>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1.</p>","metadata":{}},{"cell_type":"code","source":"x=tweet.target.value_counts() #Create a dataframe counting the frequency of each class\nx_df = x.to_frame()\nx_df.index = ['No-Disaster', 'Disaster']\nx_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-15T08:51:18.715942Z","iopub.execute_input":"2022-05-15T08:51:18.716478Z","iopub.status.idle":"2022-05-15T08:51:18.731391Z","shell.execute_reply.started":"2022-05-15T08:51:18.716441Z","shell.execute_reply":"2022-05-15T08:51:18.729952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = px.bar(x_df, y = \"target\", color=x_df.index)\nfig.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-15T08:51:23.136648Z","iopub.execute_input":"2022-05-15T08:51:23.137621Z","iopub.status.idle":"2022-05-15T08:51:23.90336Z","shell.execute_reply.started":"2022-05-15T08:51:23.137575Z","shell.execute_reply":"2022-05-15T08:51:23.902627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)</p>","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Missing Value Analysis</span>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Here, we analyse the number of missing values in the training tweet dataset</p>","metadata":{}},{"cell_type":"code","source":"\nnull_value_df = tweet.isnull().sum(axis = 0).to_frame()\nnull_value_df.columns = ['Number of Null Values']\nnull_value_df['Percentage Null Values'] = null_value_df['Number of Null Values']*100/tweet.shape[0]\nprint(null_value_df)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T09:08:22.544798Z","iopub.execute_input":"2022-05-15T09:08:22.545125Z","iopub.status.idle":"2022-05-15T09:08:22.565633Z","shell.execute_reply.started":"2022-05-15T09:08:22.545086Z","shell.execute_reply":"2022-05-15T09:08:22.564904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = null_value_df.plot.bar( y='Percentage Null Values', rot=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T09:10:47.333912Z","iopub.execute_input":"2022-05-15T09:10:47.33484Z","iopub.status.idle":"2022-05-15T09:10:47.561259Z","shell.execute_reply.started":"2022-05-15T09:10:47.334777Z","shell.execute_reply":"2022-05-15T09:10:47.560579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">So, a lot of location column data is missing (more than 33%)</p>","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Location Analysis of Tweets</span>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Here, we analyse the location column of training tweets data</p>","metadata":{}},{"cell_type":"code","source":"location_df = tweet[['location']]\nlocation_df.dropna(inplace=True) ##remove rows with null values\nlocation_df[:5]","metadata":{"execution":{"iopub.status.busy":"2022-05-15T09:22:36.452002Z","iopub.execute_input":"2022-05-15T09:22:36.452261Z","iopub.status.idle":"2022-05-15T09:22:36.468024Z","shell.execute_reply.started":"2022-05-15T09:22:36.452233Z","shell.execute_reply":"2022-05-15T09:22:36.467286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">We create a wordcloud of the locations</p>","metadata":{}},{"cell_type":"code","source":"location_str = ''\nfor i in range(location_df.shape[0]):\n    location = location_df['location'].iloc[i]\n    location_str += location + ' '\n#print(location_str)\n\n# Generate a word cloud image\nwordcloud = WordCloud().generate(location_str)\n\nfig, ax = plt.subplots(figsize = (10,10))\nax.imshow(wordcloud, interpolation=\"bilinear\")\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-15T09:58:01.502054Z","iopub.execute_input":"2022-05-15T09:58:01.502315Z","iopub.status.idle":"2022-05-15T09:58:02.192116Z","shell.execute_reply.started":"2022-05-15T09:58:01.502287Z","shell.execute_reply":"2022-05-15T09:58:02.191497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">A lot of the locations are from North America.</p>","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Keyword Analysis of Tweets</span>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Here, we analyse the location column of training tweets data</p>","metadata":{}},{"cell_type":"code","source":"keyword_df = tweet[['keyword']]\nkeyword_df.dropna(inplace=True) ##remove rows with null values\nkeyword_df[:5]","metadata":{"execution":{"iopub.status.busy":"2022-05-15T10:03:25.497555Z","iopub.execute_input":"2022-05-15T10:03:25.497831Z","iopub.status.idle":"2022-05-15T10:03:25.512198Z","shell.execute_reply.started":"2022-05-15T10:03:25.497802Z","shell.execute_reply":"2022-05-15T10:03:25.511397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">We create a wordcloud of the keywords</p>","metadata":{}},{"cell_type":"code","source":"keyword_str = ''\nfor i in range(keyword_df.shape[0]):\n    kw = keyword_df['keyword'].iloc[i]\n    keyword_str += kw + ' '\n#print(location_str)\n\n# Generate a word cloud image\nwordcloud = WordCloud().generate(keyword_str)\n\nfig, ax = plt.subplots(figsize = (10,10))\nax.imshow(wordcloud, interpolation=\"bilinear\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T10:06:27.331969Z","iopub.execute_input":"2022-05-15T10:06:27.332218Z","iopub.status.idle":"2022-05-15T10:06:27.905622Z","shell.execute_reply.started":"2022-05-15T10:06:27.332192Z","shell.execute_reply":"2022-05-15T10:06:27.904967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">The wordcloud for keywords show words like fatalities, armageddon, collide, siren appear quite a few times. </p>","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Character Analysis of Tweets</span>","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='cyan')\nax1.set_title('Disaster tweets')\nax1.set_xlabel('No. of Char./Tweet')\nax1.set_ylabel('No. of Tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='magenta')\nax2.set_title('Not Disaster tweets')\nax2.set_xlabel('No. of Char./Tweet')\nax2.set_ylabel('No. of Tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-15T06:52:28.278652Z","iopub.execute_input":"2022-05-15T06:52:28.278909Z","iopub.status.idle":"2022-05-15T06:52:28.630499Z","shell.execute_reply.started":"2022-05-15T06:52:28.278873Z","shell.execute_reply":"2022-05-15T06:52:28.629823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">The distribution of both seems to be almost same.120 to 140 characters in a tweet are the most common among both.</p>","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Basic Word Analysis of Tweets</span>","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('Disaster tweets')\nax1.set_xlabel('No. of Words/Tweet')\nax1.set_ylabel('No. of Tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='red')\nax2.set_title('Not Disaster tweets')\nax2.set_xlabel('No. of Words/Tweet')\nax2.set_ylabel('No. of Tweets')\nfig.suptitle('Words in a tweet')\nplt.show()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-15T06:52:28.631724Z","iopub.execute_input":"2022-05-15T06:52:28.632459Z","iopub.status.idle":"2022-05-15T06:52:28.981501Z","shell.execute_reply.started":"2022-05-15T06:52:28.632421Z","shell.execute_reply":"2022-05-15T06:52:28.980804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Average word length in a tweet</span>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:22px; color:#04661e\">Kernel density estimation</p>\n<p style=\"font-size:19px\">A histogram aims to approximate the underlying probability density function that generated the data by binning and counting observations. Kernel density estimation (KDE) presents a different solution to the same problem. Rather than using discrete bins, a KDE plot smooths the observations with a Gaussian kernel, producing a continuous density estimate</p>\n<p style=\"font-size:22px; color:#04661e\">Rug Plot</p>\n<p style=\"font-size:19px\">Plot marginal distributions by drawing ticks along the x and y axes. This function is intended to complement other plots by showing the location of individual observations in an unobstrusive way.</p>","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red', rug=True)\nax1.set_title('Disaster')\nword=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green', rug=True)\nax2.set_title('Not Disaster')\nfig.suptitle('Average Word Length in each Tweet')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-15T06:52:28.98272Z","iopub.execute_input":"2022-05-15T06:52:28.983249Z","iopub.status.idle":"2022-05-15T06:52:30.175529Z","shell.execute_reply.started":"2022-05-15T06:52:28.983208Z","shell.execute_reply":"2022-05-15T06:52:30.174776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Common Stopwords in Tweets</span>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">We count the frequencies of stop words for both classes of tweets. The list of stop words is loaded from <i>nltk</i> library</p>\n","metadata":{}},{"cell_type":"code","source":"def create_corpus(target): #create a list of words from tweets under a class\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:30.176632Z","iopub.execute_input":"2022-05-15T06:52:30.179848Z","iopub.status.idle":"2022-05-15T06:52:30.185029Z","shell.execute_reply.started":"2022-05-15T06:52:30.179818Z","shell.execute_reply":"2022-05-15T06:52:30.183982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_no_disaster=create_corpus(0) #create a corpus for No-Disaster tweets\ncorpus_disaster = create_corpus(1) #create a corpus for Disaster tweets\n\n#loop to create a dict. with stopwords and their frequencies\ndic_no_disaster=defaultdict(int)\nfor word in corpus_no_disaster:\n    if word in stop:\n        dic_no_disaster[word]+=1\n\ndic_disaster=defaultdict(int)\nfor word in corpus_disaster:\n    if word in stop:\n        dic_disaster[word]+=1\n\n        \n#take the top 10 stop words\ntop_no_disaster=sorted(dic_no_disaster.items(), key=lambda x:x[1],reverse=True)[:10] \ntop_disaster = sorted(dic_disaster.items(), key=lambda x:x[1],reverse=True)[:10] \n#plot the frequencies in bar plots\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nx_no_disaster,y_no_disaster=zip(*top_no_disaster)\nx_disaster, y_disaster = zip(*top_disaster)\nax1.bar(x_no_disaster,y_no_disaster, color='green')\nax1.set_xlabel('No-Disaster')\n#plt.show()\nax2.bar(x_disaster, y_disaster, color = 'blue')\nax2.set_xlabel('Disaster')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:30.188801Z","iopub.execute_input":"2022-05-15T06:52:30.189102Z","iopub.status.idle":"2022-05-15T06:52:30.55566Z","shell.execute_reply.started":"2022-05-15T06:52:30.189073Z","shell.execute_reply":"2022-05-15T06:52:30.555002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.</p>\n","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Analyzing punctuations</span>","metadata":{}},{"cell_type":"code","source":"import string\nspecial = string.punctuation\n\n#loop to create a dict. with stopwords and their frequencies\ndic_no_disaster=defaultdict(int)\nfor word in corpus_no_disaster:\n    if word in special:\n        dic_no_disaster[word]+=1\n\ndic_disaster=defaultdict(int)\nfor word in corpus_disaster:\n    if word in special:\n        dic_disaster[word]+=1\n        \n#take the top 10 punctuations\ntop_no_disaster=sorted(dic_no_disaster.items(), key=lambda x:x[1],reverse=True)[:10] \ntop_disaster = sorted(dic_disaster.items(), key=lambda x:x[1],reverse=True)[:10] \n\nx_no_disaster,y_no_disaster=zip(*top_no_disaster)\nx_disaster, y_disaster = zip(*top_disaster)\n\n#plot the frequencies in bar plots\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n\nax1.bar(x_no_disaster,y_no_disaster, color='green')\nax1.set_xlabel('No-Disaster')\n#plt.show()\nax2.bar(x_disaster, y_disaster, color = 'blue')\nax2.set_xlabel('Disaster')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:30.557881Z","iopub.execute_input":"2022-05-15T06:52:30.558416Z","iopub.status.idle":"2022-05-15T06:52:30.890499Z","shell.execute_reply.started":"2022-05-15T06:52:30.558362Z","shell.execute_reply":"2022-05-15T06:52:30.889762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">In both of them, the punctuation distributions are somewhat similar</p>\n","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Common Words</span>","metadata":{}},{"cell_type":"code","source":"counter=Counter(corpus_no_disaster)\nmost=counter.most_common() #count the frequencies of most common words\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:30.891686Z","iopub.execute_input":"2022-05-15T06:52:30.893124Z","iopub.status.idle":"2022-05-15T06:52:30.909331Z","shell.execute_reply.started":"2022-05-15T06:52:30.893082Z","shell.execute_reply":"2022-05-15T06:52:30.908599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10,5))\nsns.barplot(x=y,y=x,ax = ax)\nax.set_xlabel('Frequency')\nax.set_ylabel('Words')\nax.set_title('Frequency of Most Common Words')","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:30.910929Z","iopub.execute_input":"2022-05-15T06:52:30.911221Z","iopub.status.idle":"2022-05-15T06:52:31.140042Z","shell.execute_reply.started":"2022-05-15T06:52:30.911183Z","shell.execute_reply":"2022-05-15T06:52:31.139329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">The data requires lots of cleaning.</p>","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Ngram analysis</span>\n","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">We will do a bigram <b>(n=2)</b> analysis over the tweets.Let's check the most common bigrams in tweets.</p>","metadata":{}},{"cell_type":"code","source":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:31.141132Z","iopub.execute_input":"2022-05-15T06:52:31.141866Z","iopub.status.idle":"2022-05-15T06:52:31.148246Z","shell.execute_reply.started":"2022-05-15T06:52:31.141828Z","shell.execute_reply":"2022-05-15T06:52:31.147405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\nplt.figure(figsize=(10,5))\nx,y=map(list,zip(*top_tweet_bigrams))\nfig,ax = plt.subplots(figsize=(10,5))\nsns.barplot(x=y,y=x, ax = ax)\nax.set_ylabel('Bi-grams')\nax.set_xlabel('Frequency')\nax.set_title('Bi-gram Frequency Analysis')","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:31.149846Z","iopub.execute_input":"2022-05-15T06:52:31.150129Z","iopub.status.idle":"2022-05-15T06:52:32.023921Z","shell.execute_reply.started":"2022-05-15T06:52:31.150093Z","shell.execute_reply":"2022-05-15T06:52:32.023265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Given the prevalence of uninformative bi-grams a lot of data cleaning is required.</p>","metadata":{}},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Data Cleaning</span>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Twitter tweets have to be cleaned before working with them.So we do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.</p>","metadata":{}},{"cell_type":"code","source":"df=pd.concat([tweet,test])\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:32.025251Z","iopub.execute_input":"2022-05-15T06:52:32.025491Z","iopub.status.idle":"2022-05-15T06:52:32.03768Z","shell.execute_reply.started":"2022-05-15T06:52:32.025457Z","shell.execute_reply":"2022-05-15T06:52:32.037123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Removing Urls</span>","metadata":{}},{"cell_type":"code","source":"def remove_URL(text): #remove urls from text \n    url = re.compile(r'https?://\\S+|www\\.\\S+') #regular expression for detecting urls\n    return url.sub(r'',text)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:32.038905Z","iopub.execute_input":"2022-05-15T06:52:32.039367Z","iopub.status.idle":"2022-05-15T06:52:32.049532Z","shell.execute_reply.started":"2022-05-15T06:52:32.039332Z","shell.execute_reply":"2022-05-15T06:52:32.048675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_URL(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:32.05063Z","iopub.execute_input":"2022-05-15T06:52:32.050857Z","iopub.status.idle":"2022-05-15T06:52:32.106896Z","shell.execute_reply.started":"2022-05-15T06:52:32.050829Z","shell.execute_reply":"2022-05-15T06:52:32.106102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Removing HTML tags</span>","metadata":{}},{"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:32.10821Z","iopub.execute_input":"2022-05-15T06:52:32.108564Z","iopub.status.idle":"2022-05-15T06:52:32.113695Z","shell.execute_reply.started":"2022-05-15T06:52:32.108527Z","shell.execute_reply":"2022-05-15T06:52:32.112786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_html(x))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:32.115267Z","iopub.execute_input":"2022-05-15T06:52:32.11568Z","iopub.status.idle":"2022-05-15T06:52:32.141552Z","shell.execute_reply.started":"2022-05-15T06:52:32.115639Z","shell.execute_reply":"2022-05-15T06:52:32.140807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Removing Emojis</span>","metadata":{}},{"cell_type":"code","source":"# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:32.142783Z","iopub.execute_input":"2022-05-15T06:52:32.143268Z","iopub.status.idle":"2022-05-15T06:52:32.14844Z","shell.execute_reply.started":"2022-05-15T06:52:32.14323Z","shell.execute_reply":"2022-05-15T06:52:32.147734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x: remove_emoji(x))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:32.149885Z","iopub.execute_input":"2022-05-15T06:52:32.150407Z","iopub.status.idle":"2022-05-15T06:52:32.239083Z","shell.execute_reply.started":"2022-05-15T06:52:32.150371Z","shell.execute_reply":"2022-05-15T06:52:32.238356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Removing punctuations</span>","metadata":{}},{"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:32.240415Z","iopub.execute_input":"2022-05-15T06:52:32.240658Z","iopub.status.idle":"2022-05-15T06:52:32.244984Z","shell.execute_reply.started":"2022-05-15T06:52:32.240625Z","shell.execute_reply":"2022-05-15T06:52:32.244324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_punct(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:32.246594Z","iopub.execute_input":"2022-05-15T06:52:32.247196Z","iopub.status.idle":"2022-05-15T06:52:32.310424Z","shell.execute_reply.started":"2022-05-15T06:52:32.247161Z","shell.execute_reply":"2022-05-15T06:52:32.309785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">Spelling Correction</span>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Even if I'm not good at spelling I can correct it with python :) I will use <i>pyspellcheker</i> to do that.</p>\n","metadata":{}},{"cell_type":"code","source":"!pip install pyspellchecker","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-15T06:52:32.311627Z","iopub.execute_input":"2022-05-15T06:52:32.311857Z","iopub.status.idle":"2022-05-15T06:52:41.951753Z","shell.execute_reply.started":"2022-05-15T06:52:32.311824Z","shell.execute_reply":"2022-05-15T06:52:41.950931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:41.953525Z","iopub.execute_input":"2022-05-15T06:52:41.953823Z","iopub.status.idle":"2022-05-15T06:52:42.093949Z","shell.execute_reply.started":"2022-05-15T06:52:41.953784Z","shell.execute_reply":"2022-05-15T06:52:42.093203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df['text']=df['text'].apply(lambda x : correct_spellings(x)) #takes a long time to execute","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:42.095213Z","iopub.execute_input":"2022-05-15T06:52:42.095543Z","iopub.status.idle":"2022-05-15T06:52:42.099723Z","shell.execute_reply.started":"2022-05-15T06:52:42.095506Z","shell.execute_reply":"2022-05-15T06:52:42.098893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">GloVe for Vectorization of Tweet Corpus</span>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">We use GloVe pretrained corpus model to represent our words. It's available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here. Using these GloVe vectors we are going to represent the corupus from tweets in a vector format</p>\n","metadata":{}},{"cell_type":"code","source":"def create_corpus(df): #method to tokenize tweets \n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:42.105558Z","iopub.execute_input":"2022-05-15T06:52:42.106113Z","iopub.status.idle":"2022-05-15T06:52:42.112826Z","shell.execute_reply.started":"2022-05-15T06:52:42.106084Z","shell.execute_reply":"2022-05-15T06:52:42.111974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Let's create a corpus from the input tweets by tokenizing the words from tweets. Stop words are not included</p>\n","metadata":{}},{"cell_type":"code","source":"corpus=create_corpus(df)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:42.114292Z","iopub.execute_input":"2022-05-15T06:52:42.114666Z","iopub.status.idle":"2022-05-15T06:52:44.311544Z","shell.execute_reply.started":"2022-05-15T06:52:42.114635Z","shell.execute_reply":"2022-05-15T06:52:44.310599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Let's load the word vectors from a pretrained GloVe file</p>\n","metadata":{}},{"cell_type":"code","source":"embedding_dict={} #load glove vectors in a dictionary\nwith open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt','r') as f:\n    for line in tqdm(f):\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:44.316075Z","iopub.execute_input":"2022-05-15T06:52:44.316475Z","iopub.status.idle":"2022-05-15T06:52:57.533717Z","shell.execute_reply.started":"2022-05-15T06:52:44.316437Z","shell.execute_reply":"2022-05-15T06:52:57.533027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Let's create a dictionary of words available in the corpus with each word assigned a unique index</p>","metadata":{}},{"cell_type":"code","source":"#Create arrays of token indices for tweets\nMAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\nword_index=tokenizer_obj.word_index #<word, word_index>\nprint('Number of unique words:',len(word_index))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:57.535186Z","iopub.execute_input":"2022-05-15T06:52:57.535457Z","iopub.status.idle":"2022-05-15T06:52:57.769724Z","shell.execute_reply.started":"2022-05-15T06:52:57.535418Z","shell.execute_reply":"2022-05-15T06:52:57.769013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Extract GloVe vectors for the words available in the corpus</p>","metadata":{}},{"cell_type":"code","source":"num_words=len(word_index)+1 #Assign glove vectors for the words obtained from tokenization\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:57.771017Z","iopub.execute_input":"2022-05-15T06:52:57.772684Z","iopub.status.idle":"2022-05-15T06:52:57.835258Z","shell.execute_reply.started":"2022-05-15T06:52:57.772651Z","shell.execute_reply":"2022-05-15T06:52:57.834396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index.keys()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-15T06:52:57.836735Z","iopub.execute_input":"2022-05-15T06:52:57.837004Z","iopub.status.idle":"2022-05-15T06:52:57.854401Z","shell.execute_reply.started":"2022-05-15T06:52:57.836956Z","shell.execute_reply":"2022-05-15T06:52:57.851026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <a id=\"character-analysis\"></a><span style=\"color:#328c4f\">GloVe Visualization Using PCA</span>","metadata":{}},{"cell_type":"code","source":"if os.path.exists('../working/glove2word2vec_model.sav') == False:\n    print('Glove Model loading and saving....')\n    glove_model = KeyedVectors.load_word2vec_format('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt', binary=False, no_header=True)\n    filename = '../working/glove2word2vec_model.sav'\n    pickle.dump(glove_model, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:52:57.855605Z","iopub.execute_input":"2022-05-15T06:52:57.855918Z","iopub.status.idle":"2022-05-15T06:53:29.969144Z","shell.execute_reply.started":"2022-05-15T06:52:57.85588Z","shell.execute_reply":"2022-05-15T06:53:29.968255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove_model = pickle.load(open(filename, 'rb'))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:53:29.970688Z","iopub.execute_input":"2022-05-15T06:53:29.970945Z","iopub.status.idle":"2022-05-15T06:53:30.411766Z","shell.execute_reply.started":"2022-05-15T06:53:29.970909Z","shell.execute_reply":"2022-05-15T06:53:30.410914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def append_list(sim_words, words):\n    \n    list_of_words = []\n    \n    for i in range(len(sim_words)):\n        \n        sim_words_list = list(sim_words[i])\n        sim_words_list.append(words)\n        sim_words_tuple = tuple(sim_words_list)\n        list_of_words.append(sim_words_tuple)\n        \n    return list_of_words","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:53:30.413804Z","iopub.execute_input":"2022-05-15T06:53:30.414219Z","iopub.status.idle":"2022-05-15T06:53:30.425971Z","shell.execute_reply.started":"2022-05-15T06:53:30.414181Z","shell.execute_reply":"2022-05-15T06:53:30.425033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">Let's choose 3 random words from the dataset vocabulary and observe the most similar words</p>","metadata":{}},{"cell_type":"code","source":"input_word = 'pollution,iceberg,sanctuary'\nuser_input = [x.strip() for x in input_word.split(',')]\nresult_word = []\nfor words in user_input:\n    sim_words = glove_model.most_similar(words, topn = 5)\n    sim_words = append_list(sim_words, words)\n    #print(sim_words)\n    result_word.extend(sim_words)\n\n#print(result_word)\nsimilar_word = [word[0] for word in result_word]\nsimilarity = [word[1] for word in result_word] \nsimilar_word.extend(user_input)\nlabels = [word[2] for word in result_word]\nlabel_dict = dict([(y,x+1) for x,y in enumerate(set(labels))])\ncolor_map = [label_dict[x] for x in labels]    \n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:58:32.710866Z","iopub.execute_input":"2022-05-15T06:58:32.711431Z","iopub.status.idle":"2022-05-15T06:58:32.783838Z","shell.execute_reply.started":"2022-05-15T06:58:32.711392Z","shell.execute_reply":"2022-05-15T06:58:32.783037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef display_pca_scatterplot_2D(model, user_input=None, words=None, label=None, color_map=None, topn=5, sample=10):\n\n    if words == None:\n        if sample > 0:\n            words = np.random.choice(list(model.vocab.keys()), sample)\n        else:\n            words = [ word for word in model.vocab ]\n    \n    word_vectors = np.array([model[w] for w in words])\n    \n    #three_dim = PCA(random_state=0).fit_transform(word_vectors)[:,:3]\n    # For 2D, change the three_dim variable into something like two_dim like the following:\n    two_dim = PCA(random_state=0).fit_transform(word_vectors)[:,:2]\n\n    data = []\n    count = 0\n    \n    for i in range (len(user_input)):\n\n                trace = go.Scatter(\n                    x = two_dim[count:count+topn,0], \n                    y = two_dim[count:count+topn,1],  \n                    text = words[count:count+topn],\n                    name = user_input[i],\n                    textposition = \"top center\",\n                    textfont_size = 20,\n                    mode = 'markers+text',\n                    marker = {\n                        'size': 10,\n                        'opacity': 0.8,\n                        'color': 2\n                    }\n       \n                )\n                \n                # For 2D, instead of using go.Scatter3d, we need to use go.Scatter and delete the z variable. Also, instead of using\n                # variable three_dim, use the variable that we have declared earlier (e.g two_dim)\n            \n                data.append(trace)\n                count = count+topn\n\n    trace_input = go.Scatter(\n                    x = two_dim[count:,0], \n                    y = two_dim[count:,1],  \n                    text = words[count:],\n                    name = 'input words',\n                    textposition = \"top center\",\n                    textfont_size = 20,\n                    mode = 'markers+text',\n                    marker = {\n                        'size': 10,\n                        'opacity': 1,\n                        'color': 'black'\n                    }\n                    )\n\n    # For 2D, instead of using go.Scatter3d, we need to use go.Scatter and delete the z variable.  Also, instead of using\n    # variable three_dim, use the variable that we have declared earlier (e.g two_dim)\n            \n    data.append(trace_input)\n    \n# Configure the layout\n\n    layout = go.Layout(\n        margin = {'l': 0, 'r': 0, 'b': 0, 't': 0},\n        showlegend=True,\n        legend=dict(\n        x=1,\n        y=0.5,\n        font=dict(\n            family=\"Courier New\",\n            size=25,\n            color=\"black\"\n        )),\n        font = dict(\n            family = \" Courier New \",\n            size = 15),\n        autosize = False,\n        width = 1000,\n        height = 1000\n        )\n\n\n    plot_figure = go.Figure(data = data, layout = layout)\n    plot_figure.show()\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:53:30.587956Z","iopub.execute_input":"2022-05-15T06:53:30.590607Z","iopub.status.idle":"2022-05-15T06:53:30.617828Z","shell.execute_reply.started":"2022-05-15T06:53:30.590527Z","shell.execute_reply":"2022-05-15T06:53:30.616856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef display_pca_scatterplot_3D(model, user_input=None, words=None, label=None, color_map=None, topn=5, sample=10):\n\n    if words == None:\n        if sample > 0:\n            words = np.random.choice(list(model.vocab.keys()), sample)\n        else:\n            words = [ word for word in model.vocab ]\n    \n    word_vectors = np.array([model[w] for w in words])\n    \n    three_dim = PCA(random_state=0).fit_transform(word_vectors)[:,:3]\n    # For 2D, change the three_dim variable into something like two_dim like the following:\n    # two_dim = PCA(random_state=0).fit_transform(word_vectors)[:,:2]\n\n    data = []\n    count = 0\n    \n    for i in range (len(user_input)):\n\n                trace = go.Scatter3d(\n                    x = three_dim[count:count+topn,0], \n                    y = three_dim[count:count+topn,1],  \n                    z = three_dim[count:count+topn,2],\n                    text = words[count:count+topn],\n                    name = user_input[i],\n                    textposition = \"top center\",\n                    textfont_size = 20,\n                    mode = 'markers+text',\n                    marker = {\n                        'size': 10,\n                        'opacity': 0.8,\n                        'color': 2\n                    }\n       \n                )\n                \n                # For 2D, instead of using go.Scatter3d, we need to use go.Scatter and delete the z variable. Also, instead of using\n                # variable three_dim, use the variable that we have declared earlier (e.g two_dim)\n            \n                data.append(trace)\n                count = count+topn\n\n    trace_input = go.Scatter3d(\n                    x = three_dim[count:,0], \n                    y = three_dim[count:,1],  \n                    z = three_dim[count:,2],\n                    text = words[count:],\n                    name = 'input words',\n                    textposition = \"top center\",\n                    textfont_size = 20,\n                    mode = 'markers+text',\n                    marker = {\n                        'size': 10,\n                        'opacity': 1,\n                        'color': 'black'\n                    }\n                    )\n\n    # For 2D, instead of using go.Scatter3d, we need to use go.Scatter and delete the z variable.  Also, instead of using\n    # variable three_dim, use the variable that we have declared earlier (e.g two_dim)\n            \n    data.append(trace_input)\n    \n# Configure the layout\n\n    layout = go.Layout(\n        margin = {'l': 0, 'r': 0, 'b': 0, 't': 0},\n        showlegend=True,\n        legend=dict(\n        x=1,\n        y=0.5,\n        font=dict(\n            family=\"Courier New\",\n            size=25,\n            color=\"black\"\n        )),\n        font = dict(\n            family = \" Courier New \",\n            size = 15),\n        autosize = False,\n        width = 1000,\n        height = 1000\n        )\n\n\n    plot_figure = go.Figure(data = data, layout = layout)\n    plot_figure.show()\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:53:30.624879Z","iopub.execute_input":"2022-05-15T06:53:30.628509Z","iopub.status.idle":"2022-05-15T06:53:30.654637Z","shell.execute_reply.started":"2022-05-15T06:53:30.628421Z","shell.execute_reply":"2022-05-15T06:53:30.653864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_pca_scatterplot_2D(glove_model, user_input, similar_word, labels, color_map)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:58:50.452367Z","iopub.execute_input":"2022-05-15T06:58:50.452617Z","iopub.status.idle":"2022-05-15T06:58:50.472726Z","shell.execute_reply.started":"2022-05-15T06:58:50.452588Z","shell.execute_reply":"2022-05-15T06:58:50.471932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_pca_scatterplot_3D(glove_model, user_input, similar_word, labels, color_map)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T06:59:05.29214Z","iopub.execute_input":"2022-05-15T06:59:05.292672Z","iopub.status.idle":"2022-05-15T06:59:05.320715Z","shell.execute_reply.started":"2022-05-15T06:59:05.292634Z","shell.execute_reply":"2022-05-15T06:59:05.31993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style=\"font-size:19px\">So for the 3 words chosen we can visualize the closest words in meaning in the axis system. Iceberg is related to Titanic because the Titanic sank after getting brushing by an iceberg. Pollution is related to smog, emission, etc. Santuary is similarly related to caves, refuges, shelters, etc.</p>\n\n<p style=\"font-size:19px\">PCA can be a great tool for visualizing relation between related words within 2D or 3D as can be observed from the above example.</p>","metadata":{}},{"cell_type":"markdown","source":"*This notebook is for research/surveying, learning, experimenting, and reproducing existing literature found online*\n\n**Reference:**\n- https://www.kaggle.com/code/shahules/basic-eda-cleaning-and-glove","metadata":{}}]}