{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nimport geocoder\nimport tensorflow_hub as hub","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train=pd.read_csv(r\"C:\\Data Science for Telecom\\NLP\\Twitter Prediction\\train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test=pd.read_csv(r\"C:\\Data Science for Telecom\\NLP\\Twitter Prediction\\test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Class distribution\n\nx=train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Character length distribution","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len_prob =train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len_prob,color='red')\nax1.set_title(\"Disaster_tweets\")\ntweet_len_noprob=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len_noprob,color='green')\nax2.set_title(\"Non_disastor_tweets\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word Length Distribution","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len_word_dis = train[train['target']==1]['text'].str.split().map(lambda x:len(x))\nax1.hist(tweet_len_word_dis,color='red')\nax1.set_title(\"Disastor_tweets\")\ntweet_len_word_nodis = train[train['target']==0]['text'].str.split().map(lambda x:len(x))\nax2.hist(tweet_len_word_nodis,color='green')\nax2.set_title(\"No_Disastor_tweets\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Perc distribution of word length per tweet","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"fig,(ax1,ax2)= plt.subplots(1,2,figsize=(10,5))\nword=train[train['target']==1]['text'].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word.map(lambda x:np.mean(x)),ax=ax1,color='red')\nax1.set_title(\"disastor_tweet\")\nword_nodis =train[train['target']==0]['text'].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word.map(lambda x:np.mean(x)),ax=ax2,color='green')\nax2.set_title(\"no_disastor_tweet\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def create_corpus(target):\n    corpus=[]\n    for x in train[train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corpus=create_corpus(0)\nlen(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\ndic","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"top=sorted(dic.items(),key=lambda x:x[1],reverse=True)[:10]\ntop","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x,y=zip(*top)\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corpus1=create_corpus(1)\ndic1=defaultdict(int)\nfor word in corpus1:\n    if word in stop:\n        dic1[word]+=1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"top1 = sorted(dic1.items(),key=lambda x:x[1],reverse=True)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"top1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x1,y1=zip(*top1)\nplt.bar(x1,y1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spread of Punctuations","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(1)\ndic=defaultdict(int)\nimport string\nspecial=string.punctuation\nfor i in corpus:\n    if i in special:\n        dic[i]+=1\nx,y=zip(*dic.items())\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"corpus=create_corpus(0)\ndic=defaultdict(int)\nfor i in corpus:\n    if i in special:\n        dic[i]+=1\nx,y=zip(*dic.items())\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"counter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.barplot(x=x,y=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df=train.append(test).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"example=\"New competition launched :https://www.kaggle.com/c/nlp-getting-started\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def remove_url(text):\n    url = re.compile(r'https://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\nremove_url(example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_url(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def remove_html(text):\n    html=re.compile('<.*?>')\n    return html.sub(r'',text)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"example = \"\"\"<div>\n<h1>Real or Fake</h1>\n<p>Kaggle </p>\n<a href=\"https://www.kaggle.com/c/nlp-getting-started\">getting started</a>\n</div>\"\"\"\n\nprint(remove_html(example))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : remove_html(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"remove_emoji(\"Omg #another Earthquake ðŸ˜”ðŸ˜”\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def remove_punct(text):\n    try:\n        table =str.maketrans(\"\",\"\",string.punctuation)\n        return text.translate(table)\n    except:\n        return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(remove_punct(\"is , an the value\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x: remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from spellchecker import SpellChecker","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"spell=SpellChecker()\ndef spell_correction(text):\n    try:\n        spelled =[]\n        uncorrect =spell.unknown(text.split())\n        for word in text.split():\n            if word in uncorrect:\n                spelled.append(spell.correction(word))\n            else:\n                spelled.append(word)\n        return(\" \".join(spelled))\n    except:\n        return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"example=\"kiss me plese\"\nspell_correction(example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def ascii_remover(text):\n    try:\n        all_ascii = ''.join(char for char in text if ord(char) < 128)\n        return(all_ascii)\n    except:\n        return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":" ascii_remover(\"45ÃƒÂ¥Ã‚Â¡ 5'12.53N   14ÃƒÂ¥Ã‚Â¡ 7'24.93E\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x:ascii_remover(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['location']=df['location'].apply(lambda x:ascii_remover(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def replace(text):\n    try:\n        \n        text_replace=text.replace(\"T: \", \"\")\n        return(text_replace)\n    except:\n        return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['location']=df['location'].apply(lambda x:replace(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#df['location']=df['location'].apply(lambda x: remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from geopy.geocoders import Nominatim","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def reverse_geocoder(text):\n    \n    geolocator = Nominatim()\n    try:\n        location = geolocator.reverse(text)\n        address=location.address\n        if address is None:\n            return(text)\n        else:\n            return(address)\n    except:\n        return(text)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1=df[df['id']==9662]\ndf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1['location']=df1['location'].apply(lambda x:reverse_geocoder(x))\ndf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['location']=df['location'].apply(lambda x:reverse_geocoder(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['location']=df['location'].apply(lambda x:spell_correction(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def string_capitalize(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(word.capitalize())\n        return(\" \".join(word1))\n        \n    except:\n        return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['location']=df['location'].apply(lambda x:string_capitalize(x))\ndf['text']=df['text'].apply(lambda x:string_capitalize(x))\ndf['text']=df['text'].apply(lambda x:spell_correction(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from geotext import GeoText","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def geo_cities(text):\n    try:\n        places=GeoText(text)\n        city=places.cities\n        country=places.countries\n\n        if len(city)>0:\n            \n            return(city[0])\n        elif len(country)>0:\n            return(country[0])\n        else :\n            return(text)\n    except:\n        return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['location']=df['location'].apply(lambda x: geo_cities(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['location']=df['location'].apply(lambda x: remove_punct(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['keyword']=df['keyword'].apply(lambda x:spell_correction(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#nltk.download('averaged_perceptron_tagger')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from pattern.text.en import singularize\ndef singular(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(singularize(word))\n        return(\" \".join(word1))\n        \n    except:\n        return(text)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x:singular(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def string_lower(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(word.lower())\n        return(\" \".join(word1))\n        \n    except:\n        return(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x:string_lower(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\ndef keyword(text):\n    disastor=['flooding','wildfire','bombed','bagging','kill','dead','apocalypse','calamity','catastrophe','collapse','crash','debacle','defeat','emergency','failure','fiasco','flood','harm','hazard','holocaust','mishap','setback','tragedy','woe','adversity','affliction','bale','bane','blight','blow','bust','casualty','cataclysm','collision','depression','exigency','fall','flop','grief','misadventure','mischance','misfortune','reverse','rock','rough','ruin','ruination','slip','stroke','undoing','upset','washout','act of God','bad luck','bad news','fell stroke','hard luck','hot water','ill luck','the worst','crack-up','disaster','fender-bender','fluke','pileup','rear ender','smash','smashup','stack-up','total','wrack-up','bad break','bummer','can of worms','clutch','contretemps','crunch','difficulty','distress','downer','drag','evil eye','hard knocks','hard times','hardship','hurting','ill fortune','jam','jinx','kiss of death','misery','on the skids','pain in the neck','poison','sorrow','suffering','tough luck','trial','trouble','annoyance','bad trip','disappointment','irritation','bum trip','depressing experience','raw deal','rotten hand','unhappy situation','unpleasant experience','unpleasant situation','unpleasent','burden','bÃªte noir','curse','despair','destruction','downfall','fatal attraction','nuisance','pest','plague','scourge','torment','venom','balk','bolt from the blue','bombshell','chagrin','comedown','disgruntlement','frustration','jolt','letdown','shock','chance','contingency','accident','alluvion','culmination','curtains','denouement','desolation','devastation','end','fatality','finale','havoc','ill','infliction','meltdown','termination','upshot','waterloo','wreck','embarrassment','predicament','agitation','clamor','commotion','ferment','furor','outcry','quaking','rocking','seism','shaking','tottering','trembling','tumult','turbulence','upheaval','upturn','big trouble','change','climacteric','climax','confrontation','corner','crossroad','crux','deadlock','dilemma','dire straits','entanglement','extremity','height','hot potato','hour of decision','imbroglio','impasse','juncture','mess','moment of truth','necessity','pass','perplexity','pickle','pinch','plight','point of no return','pressure','puzzle','quandary','situation','stew','strait','trauma','turning point','urgency','Cancer','cross','evil','hydra','ordeal','pestilence','tribulation','vexation','voodoo','beating','blue ruin','breakdown','defeasance','dissolution','drubbing','licking','overthrow','reversal','rout','shellacking','trouncing','vanquishment','bitter pill','blind alley','blunder','bringdown','discouragement','dud','error','false alarm','faux pas','fizzle','flash in the pan','inefficacy','lemon','miscalculation','mistake','obstacle','old one-two','destitution','hard time','holy mess','indigence','need','poverty','privation','rigor','rotten luck','scrape','straits','throe','ticklish spot','tough break','unholy mess','vicissitude','want','Judgment Day','Moira','annihilation','circumstance','conclusion','condemnation','death','decree','destination','destiny','fixed future','foreordination','fortune','handwriting on wall','judgment','karma','kismet','lap of the gods','lot','opinion','portion','predestination','predetermination','sentence','verdict','way the ball bounces','way the cookie crumbles','acme','acuteness','apex','apogee','border','bound','boundary','brim','brink','butt','consummation','crisis','depth','edge','excess','extreme','extremes','frontier','last','margin','maximum','nadir','outside','pinnacle','pole','remote','rim','terminal','terminus','tip','top','verge','vertex','zenith','abasement','capitulation','degradation','diminution','dive','drop','humiliation','loss','resignation','surrender','tumble','deadliness','destructiveness','dying','inevitability','lethality','lethalness','mortality','necrosis','noxiousness','poisonousness','virulence','abortion','botched situation','dumb thing to do','dumb trick','farce','flap','miscarriage','route','screwup','stunt','bomb','loser','nonstarter','Herculean task','asperity','austerity','case','danger','discomfort','drudgery','fatigue','grievance','injury','labor','oppression','peril','persecution','rainy day','toil','travail','uphill battle','worry','ache','black and blue','boo-boo','bruise','chop','detriment','disadvantage','disservice','down','gash','ill-treatment','mark','mischief','nick','ouch','outrage','pain','pang','prejudice','scratch','sore','soreness','wound','wrong','bereavement','cost','damage','debit','debt','deficiency','depletion','deprivation','disappearance','dispossession','forfeiture','hurt','impairment','losing','mislaying','misplacing','perdition','retardation','sacrifice','shrinkage','squandering','waste','wreckage','lapse','ill-fortune','rear-ender','crime','crying shame','regret','shame','sin','clobbering','confusion','flight','hiding','retreat','romp','shambles','shutout','thrashing','trashing','walkover','waxing','whipping','about-face','alteration','convulsion','disorder','disruption','disturbance','eruption','explosion','flip-flop','new ball-game','new deal','outbreak','outburst','revolution','shakeout','stirring','switch','temblor','tremor','turmoil','turnaround','punishment','retribution','downpour','erosion','gully','agony','anguish','bemoaning','blues','care','dejection','deploring','dole','gloom','grieving','headache','heartache','heartbreak','lamentation','melancholy','rain','rue','sadness','unhappiness','wretchedness','attack','armageddon','aftershock','typhoon','asteroid','tsunami','natural disasters','volcano','tornado','avalanche','earthquake','blizzard','drought','bushfire','dust storm','magma','twister','windstorm','heat wave','cyclone','forest fire','fire','hailstorm','lava','lightning','high-pressure','hail','hurricane','seismic','whirlpool','Richter scale','whirlwind','cloud','thunderstorm','barometer','gale','blackout','gust','force','low-pressure','volt','snowstorm','rainstorm','storm','nimbus','violent storm','sandstorm','Beaufort scale','fatal','cumulonimbus','lost','money','tension','uproot','underground','destroy','arsonist','wind scale','arson','rescue','permafrost','fault','shelter','ablaze'\n]\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            if word in disastor:\n                word1.append(word)\n        return(\",\".join(word1))\n    \n    except:\n        return(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y1 = df.loc[df['keyword']=='']\nindex_y1=list(y1.index.values)\nprint(index_y1)\n\nfor i in index_y1:\n    df['keyword'].iat[i]=keyword(df['text'].iat[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['keyword'].iat[3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x:string_capitalize(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To Capture those cities/geographical areas which are made up of 2 words with a space between them e.g. South Tampa , North Wales etc..","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from nltk import word_tokenize, pos_tag, ne_chunk\nfrom nltk import Tree\ndef get_continuous_chunks(text, label):\n    try:\n        chunked = ne_chunk(pos_tag(word_tokenize(text)))\n        prev = None\n        continuous_chunk = []\n        current_chunk = []\n\n        for subtree in chunked:\n            if type(subtree) == Tree and subtree.label() == label:\n                current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n            elif current_chunk:\n                named_entity = \" \".join(current_chunk)\n                if named_entity not in continuous_chunk:\n                    continuous_chunk.append(named_entity)\n                    current_chunk = []\n            else:\n                continue\n\n        return continuous_chunk\n    except:\n        return(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"get_continuous_chunks(\"Haha South Tampa Is Getting Flooded Hah Wait A Second I Live In South Tampa What Am I Gonna Do What Am I Gonna Do Fvck Flooding\",'GPE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y = df.loc[df['location']==\"\"]\nindex_y=list(y.index.values)\nprint(index_y)\n\nfor i in index_y:\n    df['location'].iat[i]=get_continuous_chunks(df['text'].iat[i],'LOCATION')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y = df.loc[df['location']==\"\"]\nindex_y=list(y.index.values)\nprint(index_y)\n\nfor i in index_y:\n    df['location'].iat[i]=get_continuous_chunks(df['text'].iat[i],'GPE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def remove_ing(text):\n    try:\n        words=text.split()\n        word1=[]\n        for word in words:\n            word1.append(word.replace(\"ing\",\"\"))\n        return(\" \".join(word1))\n    \n    except:\n        return(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x:remove_ing(x))\ny1 = df.loc[df['keyword']=='']\nindex_y1=list(y1.index.values)\nprint(index_y1)\n\nfor i in index_y1:\n    df['keyword'].iat[i]=keyword(df['text'].iat[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.to_csv(r'C:\\Data Science for Telecom\\NLP\\Twitter Prediction\\output_datafile.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}