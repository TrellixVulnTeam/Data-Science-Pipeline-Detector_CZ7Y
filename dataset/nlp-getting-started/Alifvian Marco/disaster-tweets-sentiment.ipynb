{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.target.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = df_train.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(['keyword', 'location'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = nltk.corpus.stopwords.words(['english'])\nprint(stop_words)\n\nlem = WordNetLemmatizer()\n\ndef cleaning(data):\n  #remove urls\n  tweet_without_url = re.sub(r'http\\S+',' ', data)\n\n  #remove hashtags\n  tweet_without_hashtag = re.sub(r'#\\w+', ' ', tweet_without_url)\n\n  #3. Remove mentions and characters that not in the English alphabets\n  tweet_without_mentions = re.sub(r'@\\w+',' ', tweet_without_hashtag)\n  precleaned_tweet = re.sub('[^A-Za-z]+', ' ', tweet_without_mentions)\n\n    #2. Tokenize\n  tweet_tokens = TweetTokenizer().tokenize(precleaned_tweet)\n    \n    #3. Remove Puncs\n  tokens_without_punc = [w for w in tweet_tokens if w.isalpha()]\n    \n    #4. Removing Stopwords\n  tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n    \n    #5. lemma\n  text_cleaned = [lem.lemmatize(t) for t in tokens_without_sw]\n    \n    #6. Joining\n  return \" \".join(text_cleaned)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['cleaned_text'] = train['text'].apply(cleaning)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from textblob import TextBlob\n\ndef getSubjectivity(tweet):\n  return TextBlob(tweet).sentiment.subjectivity\n\ndef getPolarity(tweet):\n  return TextBlob(tweet).sentiment.polarity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['subjectivity'] = train['cleaned_text'].apply(getSubjectivity)\ntrain['polarity'] = train['cleaned_text'].apply(getPolarity)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getSentiment(score):\n  if score < 0:\n    return 'negative'\n  elif score == 0:\n    return 'neutral'\n  else:\n    return 'positive'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['sentiment'] = train['polarity'].apply(getSentiment)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.drop(['keyword', 'location'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['cleaned_text'] = test['text'].apply(cleaning)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['subjectivity'] = test['cleaned_text'].apply(getSubjectivity)\ntest['polarity'] = test['cleaned_text'].apply(getPolarity)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['sentiment'] = test['polarity'].apply(getSentiment)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding = {\n    'negative':0,\n    'neutral':1,\n    'positive':2,\n}\ntrain.sentiment.replace(encoding, inplace=True)\ntest.sentiment.replace(encoding, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.head())\nprint(test.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as Layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D\nfrom keras.models import load_model\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train['cleaned_text'])\n\nX_train = tokenizer.texts_to_sequences(train['cleaned_text'].values)\ny_train = train[['sentiment']].values\n\nX_test = tokenizer.texts_to_sequences(test['cleaned_text'].values)\ny_test = test[['sentiment']].values\n\nvocab_size = len(tokenizer.word_index)+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Vocabulary size: {}\".format(vocab_size))\nprint(\"\\n----------Example----------\\n\")\nprint(\"Sentence:\\n{}\".format(train[\"cleaned_text\"][6]))\nprint(\"\\nAfter tokenizing :\\n{}\".format(X_train[6]))\n\nX_train = pad_sequences(X_train, padding='post')  # adding padding of zeros to obtain uniform length for all sequences\nX_test = pad_sequences(X_test, padding='post')\nprint(\"\\nAfter padding :\\n{}\".format(X_train[6]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 100\nBATCH_SIZE = 32\nembedding_dim = 16\nunits = 256\n\nmodel = Sequential()\nmodel.add(\n    Embedding(\n        vocab_size, \n        embedding_dim, \n        input_length=X_train.shape[1]\n        ))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(units, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(3, activation='softmax')),\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = pd.get_dummies(train['sentiment']).values\n\nprint(X_train.shape, Y.shape)\n\nY_test = pd.get_dummies(test['sentiment']).values\nprint(X_test.shape, Y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    X_train,\n    Y, \n    epochs=10, \n    validation_split=0.25, \n#     validation_data=(X_test, y_test),\n    batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test =  np.argmax(model.predict(X_test), axis=1)\nprint('Accuracy:\\t{:0.1f}%'.format(accuracy_score(np.argmax(Y_test,axis=1),y_pred_test)*100))\nprint(classification_report(np.argmax(Y_test,axis=1), y_pred_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}