{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-06T20:34:39.889981Z","iopub.execute_input":"2021-08-06T20:34:39.890707Z","iopub.status.idle":"2021-08-06T20:34:39.903738Z","shell.execute_reply.started":"2021-08-06T20:34:39.890587Z","shell.execute_reply":"2021-08-06T20:34:39.902724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom geopy.geocoders import Nominatim\nfrom geopy.extra.rate_limiter import RateLimiter\nimport folium \nfrom folium import plugins \n\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:34:40.393805Z","iopub.execute_input":"2021-08-06T20:34:40.394356Z","iopub.status.idle":"2021-08-06T20:34:41.996275Z","shell.execute_reply.started":"2021-08-06T20:34:40.394289Z","shell.execute_reply":"2021-08-06T20:34:41.995321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:40:30.466706Z","iopub.execute_input":"2021-08-06T20:40:30.467046Z","iopub.status.idle":"2021-08-06T20:40:30.501934Z","shell.execute_reply.started":"2021-08-06T20:40:30.467016Z","shell.execute_reply":"2021-08-06T20:40:30.501142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:34:42.093123Z","iopub.execute_input":"2021-08-06T20:34:42.093437Z","iopub.status.idle":"2021-08-06T20:34:42.11856Z","shell.execute_reply.started":"2021-08-06T20:34:42.093395Z","shell.execute_reply":"2021-08-06T20:34:42.117581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:34:42.119942Z","iopub.execute_input":"2021-08-06T20:34:42.120217Z","iopub.status.idle":"2021-08-06T20:34:42.138739Z","shell.execute_reply.started":"2021-08-06T20:34:42.120191Z","shell.execute_reply":"2021-08-06T20:34:42.137368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,9))\n\nax1 = fig.add_subplot(121)\nsns.barplot(df_train['keyword'].isnull().value_counts().index, df_train['keyword'].isnull().value_counts().values, palette='mako', ax=ax1)\nax1.set_title('Missing Values in Keyword')\n\nax2 = fig.add_subplot(122)\nsns.barplot(df_train['location'].isnull().value_counts().index, df_train['location'].isnull().value_counts().values, palette='mako', ax=ax2)\nax2.set_title('Missing Values in Location')\n\nfig.suptitle('Missing Values')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:34:42.140127Z","iopub.execute_input":"2021-08-06T20:34:42.140525Z","iopub.status.idle":"2021-08-06T20:34:42.526911Z","shell.execute_reply.started":"2021-08-06T20:34:42.140494Z","shell.execute_reply":"2021-08-06T20:34:42.526205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,9))\nsns.barplot(df_train['target'].value_counts().index, df_train['target'].value_counts().values)\nplt.title('Target Values')\nplt.xlabel('0:not disaster|1:disaster')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:34:42.527929Z","iopub.execute_input":"2021-08-06T20:34:42.528378Z","iopub.status.idle":"2021-08-06T20:34:42.661513Z","shell.execute_reply.started":"2021-08-06T20:34:42.528336Z","shell.execute_reply":"2021-08-06T20:34:42.66041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tgroup = df_train.groupby('target').size()\n\ndf_tgroup.plot(kind='pie', subplots=True, figsize=(10, 8), autopct = \"%.2f%%\", colors=['blue','green'])\nplt.title(\"Pie chart of Target\",fontsize=16)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:34:42.662785Z","iopub.execute_input":"2021-08-06T20:34:42.663081Z","iopub.status.idle":"2021-08-06T20:34:42.846254Z","shell.execute_reply.started":"2021-08-06T20:34:42.663053Z","shell.execute_reply":"2021-08-06T20:34:42.845553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df_train.location.value_counts()[:20]\ndata = pd.DataFrame(data)\ndata = data.reset_index()\ndata.columns = ['location', 'counts']\n\ngeolocator = Nominatim(user_agent='Location Map')\ngeocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n\ndict_lat = {}\ndict_long = {}\nfor i in data.location.values:\n    print(i)\n    location = geocode(i)\n    dict_lat[i] = location.latitude\n    dict_long[i] = location.longitude\n\ndata['latitude'] = data.location.map(dict_lat)\ndata['longitude'] = data.location.map(dict_long)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:34:42.847813Z","iopub.execute_input":"2021-08-06T20:34:42.848201Z","iopub.status.idle":"2021-08-06T20:35:02.052872Z","shell.execute_reply.started":"2021-08-06T20:34:42.848161Z","shell.execute_reply":"2021-08-06T20:35:02.051964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"location_map = folium.Map(location=[7.0,7.0], zoom_start=2)\nmarkers=2\n\nfor i,row in data.iterrows():\n  loss = row['counts']\n  if row['counts']>0:\n    count = row['counts']*0.4\n  folium.CircleMarker([float(row['latitude']), float(row['longitude'])], radius=float(count), color='red', fill=True).add_to(location_map)\n\nlocation_map","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:35:02.054629Z","iopub.execute_input":"2021-08-06T20:35:02.055014Z","iopub.status.idle":"2021-08-06T20:35:02.096758Z","shell.execute_reply.started":"2021-08-06T20:35:02.054973Z","shell.execute_reply":"2021-08-06T20:35:02.095941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"â‚¬\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:40:36.413502Z","iopub.execute_input":"2021-08-06T20:40:36.41384Z","iopub.status.idle":"2021-08-06T20:40:36.434071Z","shell.execute_reply.started":"2021-08-06T20:40:36.41381Z","shell.execute_reply":"2021-08-06T20:40:36.433114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def word_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\n# Replace all abbreviations\ndef replace_abbrev(text):\n    string = \"\"\n    for word in text.split():\n        string += word_abbrev(word) + \" \"        \n    return string\ndf_train['cleaned_text'] = df_train['text'].apply(replace_abbrev)\ndf_test['cleaned_text'] = df_test['text'].apply(replace_abbrev)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:40:38.403444Z","iopub.execute_input":"2021-08-06T20:40:38.403801Z","iopub.status.idle":"2021-08-06T20:40:38.488749Z","shell.execute_reply.started":"2021-08-06T20:40:38.403772Z","shell.execute_reply":"2021-08-06T20:40:38.487838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = nltk.corpus.stopwords.words(['english'])\nlem = WordNetLemmatizer()\n\nprint(stop_words)\n\ndef cleaning(data):\n    #remove urls\n    tweet_without_url = re.sub(r'http\\S+',' ', data)\n\n    #remove hashtags\n    tweet_without_hashtag = re.sub(r'#\\w+', ' ', tweet_without_url)\n\n    #3. Remove mentions and characters that not in the English alphabets\n    tweet_without_mentions = re.sub(r'@\\w+',' ', tweet_without_hashtag)\n    precleaned_tweet = re.sub('[^A-Za-z]+', ' ', tweet_without_mentions)\n\n    #2. Tokenize\n    tweet_tokens = TweetTokenizer().tokenize(precleaned_tweet)\n    \n    #3. Remove Puncs\n    tokens_without_punc = [w for w in tweet_tokens if w.isalpha()]\n    \n    #4. Removing Stopwords\n    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n    \n    #5. lemma\n    text_cleaned = [lem.lemmatize(t) for t in tokens_without_sw]\n    \n    #6. Joining\n    return \" \".join(text_cleaned)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:40:39.824677Z","iopub.execute_input":"2021-08-06T20:40:39.825188Z","iopub.status.idle":"2021-08-06T20:40:39.835726Z","shell.execute_reply.started":"2021-08-06T20:40:39.825157Z","shell.execute_reply":"2021-08-06T20:40:39.834767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['cleaned_text'] = df_train['cleaned_text'].apply(cleaning)\ndf_test['cleaned_text'] = df_test['cleaned_text'].apply(cleaning)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:40:40.851189Z","iopub.execute_input":"2021-08-06T20:40:40.85166Z","iopub.status.idle":"2021-08-06T20:40:42.126803Z","shell.execute_reply.started":"2021-08-06T20:40:40.851629Z","shell.execute_reply":"2021-08-06T20:40:42.126073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_train['cleaned_text'] = df_train['cleaned_text'].apply(stemming)\n# df_test['cleaned_text'] = df_test['cleaned_text'].apply(stemming)\n\ndf_train['cleaned_text'] = df_train['cleaned_text'].apply(lambda x : x.lower())\ndf_test['cleaned_text'] = df_test['cleaned_text'].apply(lambda x : x.lower())","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:40:46.666093Z","iopub.execute_input":"2021-08-06T20:40:46.668044Z","iopub.status.idle":"2021-08-06T20:40:46.67713Z","shell.execute_reply.started":"2021-08-06T20:40:46.668001Z","shell.execute_reply":"2021-08-06T20:40:46.676499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.drop_duplicates(subset=['cleaned_text'], inplace=True)\ndf_test.drop_duplicates(subset=['cleaned_text'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:42:32.687695Z","iopub.execute_input":"2021-08-06T20:42:32.688037Z","iopub.status.idle":"2021-08-06T20:42:32.702785Z","shell.execute_reply.started":"2021-08-06T20:42:32.688004Z","shell.execute_reply":"2021-08-06T20:42:32.70184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.dropna(how='any', inplace=True, axis=1)\ndf_test.dropna(how='any', inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:42:55.935517Z","iopub.execute_input":"2021-08-06T20:42:55.935895Z","iopub.status.idle":"2021-08-06T20:42:55.948068Z","shell.execute_reply.started":"2021-08-06T20:42:55.935864Z","shell.execute_reply":"2021-08-06T20:42:55.9472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collect_tokens(data, target):\n    tokens = []\n    \n    for i in data[data['target'] == target]['cleaned_text'].str.split():\n        for j in i:\n            tokens.append(j)\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:43:00.497146Z","iopub.execute_input":"2021-08-06T20:43:00.497507Z","iopub.status.idle":"2021-08-06T20:43:00.503804Z","shell.execute_reply.started":"2021-08-06T20:43:00.497471Z","shell.execute_reply":"2021-08-06T20:43:00.502847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster_tokens = collect_tokens(df_train, 1)\nnon_disaster_tokens = collect_tokens(df_train, 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:43:00.766296Z","iopub.execute_input":"2021-08-06T20:43:00.766657Z","iopub.status.idle":"2021-08-06T20:43:00.788102Z","shell.execute_reply.started":"2021-08-06T20:43:00.766628Z","shell.execute_reply":"2021-08-06T20:43:00.787272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,8))\nword_cloud = WordCloud(background_color=\"white\",max_font_size=60).generate(\" \".join(disaster_tokens[:50]))\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most words in disaster ', fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:43:01.250189Z","iopub.execute_input":"2021-08-06T20:43:01.250536Z","iopub.status.idle":"2021-08-06T20:43:01.501063Z","shell.execute_reply.started":"2021-08-06T20:43:01.250505Z","shell.execute_reply":"2021-08-06T20:43:01.5004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,8))\nword_cloud = WordCloud(background_color=\"white\",max_font_size=60).generate(\" \".join(non_disaster_tokens[:50]))\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Most words in non disaster ', fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:43:01.983086Z","iopub.execute_input":"2021-08-06T20:43:01.983601Z","iopub.status.idle":"2021-08-06T20:43:02.209983Z","shell.execute_reply.started":"2021-08-06T20:43:01.983559Z","shell.execute_reply":"2021-08-06T20:43:02.209035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1 Way","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nmax_features = 5000\ncount_vectorizer = CountVectorizer(max_features=max_features)\nsparce_matrix_train=count_vectorizer.fit_transform(df_train['cleaned_text'])\nsparce_matrix_test=count_vectorizer.fit_transform(df_test['cleaned_text'])\n\ndef count_vector(data):\n    count = CountVectorizer()\n    vector = count.fit_transform(data)\n    return vector, count_vectorizer\n\ndef tfidf_vector(data):\n    tfidf = TfidfVectorizer()\n    vector_tfidf = tfidf.fit_transform(data)\n    return vector_tfidf, tfidf\n\nX_train_count, count_vectorizer = count_vector(df_train['cleaned_text'])\nX_train_tfidf, tfidf_vectorizer = tfidf_vector(df_train['cleaned_text'])\n\nX_test_count = count_vectorizer.transform(df_test['cleaned_text'])                                                     \nX_test_tfidf = tfidf_vectorizer.transform(df_test['cleaned_text'])","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:55:59.269662Z","iopub.execute_input":"2021-08-06T20:55:59.27017Z","iopub.status.idle":"2021-08-06T20:55:59.634495Z","shell.execute_reply.started":"2021-08-06T20:55:59.270123Z","shell.execute_reply":"2021-08-06T20:55:59.633547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score, classification_report\n\nnp.random.seed(0)\nrandom_state = 29","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:12:00.098454Z","iopub.execute_input":"2021-08-06T21:12:00.098796Z","iopub.status.idle":"2021-08-06T21:12:00.104743Z","shell.execute_reply.started":"2021-08-06T21:12:00.098766Z","shell.execute_reply":"2021-08-06T21:12:00.103602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_pred(model, X_train,X_test,y_train,y_test):\n    clf = model\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    cmatx = confusion_matrix(y_test, y_pred)\n    \n    f,ax = plt.subplots(figsize=(3,3))\n    sns.heatmap(cmatx,annot=True,linewidths=0.5,cbar=False,linecolor=\"red\",fmt='.0f',ax=ax)\n    plt.xlabel(\"y_predict\")\n    plt.ylabel(\"y_true\")\n    ax.set(title=str(clf))\n    plt.show()\n    \n    train_accuracy = round(clf.score(X_train,y_train)*100)\n    test_accuracy =  round(accuracy_score(y_test,y_pred)*100)\n    \n    print(classification_report(y_test,y_pred))    \n    print('Accuracy of classifier on training set:{}%'.format(train_accuracy))\n    print('Accuracy of classifier on test set:{}%' .format(test_accuracy))","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:11:35.181669Z","iopub.execute_input":"2021-08-06T21:11:35.181995Z","iopub.status.idle":"2021-08-06T21:11:35.188029Z","shell.execute_reply.started":"2021-08-06T21:11:35.181965Z","shell.execute_reply":"2021-08-06T21:11:35.18742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models=[\n        XGBClassifier(max_depth=6, n_estimators=1000),\n        LogisticRegression(random_state=random_state),\n        SVC(random_state=random_state),\n        MultinomialNB(),\n        DecisionTreeClassifier(random_state = random_state),\n        KNeighborsClassifier(),\n        RandomForestClassifier(random_state=random_state),\n       ]","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:11:36.056654Z","iopub.execute_input":"2021-08-06T21:11:36.057117Z","iopub.status.idle":"2021-08-06T21:11:36.06107Z","shell.execute_reply.started":"2021-08-06T21:11:36.057086Z","shell.execute_reply":"2021-08-06T21:11:36.060426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for m in models:\n    y = df_train['target']\n    print('COUNTVECTOR')\n    \n    X = X_train_count\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    fit_pred(m, X_train, X_test, y_train, y_test)\n    \n    print('TFIDFVECTOR')\n    X = X_train_tfidf\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    fit_pred(m, X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:12:04.492287Z","iopub.execute_input":"2021-08-06T21:12:04.49264Z","iopub.status.idle":"2021-08-06T21:13:03.374353Z","shell.execute_reply.started":"2021-08-06T21:12:04.49261Z","shell.execute_reply":"2021-08-06T21:13:03.373396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2 Way","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as Layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D, MaxPooling1D, GRU\nfrom keras.models import load_model\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:18:42.608916Z","iopub.execute_input":"2021-08-06T21:18:42.609282Z","iopub.status.idle":"2021-08-06T21:18:42.614589Z","shell.execute_reply.started":"2021-08-06T21:18:42.609247Z","shell.execute_reply":"2021-08-06T21:18:42.613673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train['cleaned_text']\ny = pd.get_dummies(df_train['target']).values\nnum_classes = df_train['target'].nunique()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:18:43.648375Z","iopub.execute_input":"2021-08-06T21:18:43.648923Z","iopub.status.idle":"2021-08-06T21:18:43.657836Z","shell.execute_reply.started":"2021-08-06T21:18:43.648874Z","shell.execute_reply":"2021-08-06T21:18:43.656844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 101 # fix random seed for reproducibility\nnp.random.seed(seed)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2,\n                                                    stratify=y,\n                                                    random_state=seed)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:18:44.369551Z","iopub.execute_input":"2021-08-06T21:18:44.369869Z","iopub.status.idle":"2021-08-06T21:18:44.412103Z","shell.execute_reply.started":"2021-08-06T21:18:44.369841Z","shell.execute_reply":"2021-08-06T21:18:44.411246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:18:45.068469Z","iopub.execute_input":"2021-08-06T21:18:45.068967Z","iopub.status.idle":"2021-08-06T21:18:45.240655Z","shell.execute_reply.started":"2021-08-06T21:18:45.068916Z","shell.execute_reply":"2021-08-06T21:18:45.239876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import sequence\nmax_words = 30\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)\nprint(X_train.shape,X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:18:48.859754Z","iopub.execute_input":"2021-08-06T21:18:48.860091Z","iopub.status.idle":"2021-08-06T21:18:48.897524Z","shell.execute_reply.started":"2021-08-06T21:18:48.860059Z","shell.execute_reply":"2021-08-06T21:18:48.896529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM, Dropout\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\nbatch_size = 128\nepochs = 20\n\nmax_features = 20000\nembed_dim = 100\n\nnp.random.seed(seed)\nK.clear_session()\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))    \n# model.add(Dropout(0.5))\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:22:35.19652Z","iopub.execute_input":"2021-08-06T21:22:35.19684Z","iopub.status.idle":"2021-08-06T21:22:35.412083Z","shell.execute_reply.started":"2021-08-06T21:22:35.196811Z","shell.execute_reply":"2021-08-06T21:22:35.411203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n\ndef callbacks():\n  cb =[]\n  reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss',  \n                                       factor=0.5, patience=1, \n                                       verbose=1, mode='min', \n                                       min_delta=0.0001, min_lr=0,\n                                       restore_best_weights=True)\n  cb.append(reduceLROnPlat)\n  log = CSVLogger('log.csv')\n  cb.append(log)\n\n  es = EarlyStopping(monitor='val_loss', patience=5, verbose=0,\n                       mode='min', restore_best_weights=True)\n  cb.append(es)\n\n  return cb","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:22:36.49354Z","iopub.execute_input":"2021-08-06T21:22:36.49416Z","iopub.status.idle":"2021-08-06T21:22:36.500595Z","shell.execute_reply.started":"2021-08-06T21:22:36.494114Z","shell.execute_reply":"2021-08-06T21:22:36.499856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    X_train, \n    y_train, \n    validation_data=(X_test, y_test),\n    epochs=epochs, \n    batch_size=batch_size, \n    verbose=2,\n    callbacks = callbacks()\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:22:38.043547Z","iopub.execute_input":"2021-08-06T21:22:38.044181Z","iopub.status.idle":"2021-08-06T21:22:54.074952Z","shell.execute_reply.started":"2021-08-06T21:22:38.044133Z","shell.execute_reply":"2021-08-06T21:22:54.073954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:23:00.685162Z","iopub.execute_input":"2021-08-06T21:23:00.685516Z","iopub.status.idle":"2021-08-06T21:23:00.874255Z","shell.execute_reply.started":"2021-08-06T21:23:00.685487Z","shell.execute_reply":"2021-08-06T21:23:00.873312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test['cleaned_text']\ntest_data = tokenizer.texts_to_sequences(df_test['cleaned_text'])\ntest_data = sequence.pad_sequences(test_data, maxlen=max_words)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:23:09.842037Z","iopub.execute_input":"2021-08-06T21:23:09.842421Z","iopub.status.idle":"2021-08-06T21:23:09.920243Z","shell.execute_reply.started":"2021-08-06T21:23:09.842384Z","shell.execute_reply":"2021-08-06T21:23:09.919508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(test_data)\npred","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:23:10.727932Z","iopub.execute_input":"2021-08-06T21:23:10.72854Z","iopub.status.idle":"2021-08-06T21:23:11.283318Z","shell.execute_reply.started":"2021-08-06T21:23:10.72849Z","shell.execute_reply":"2021-08-06T21:23:11.282386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = [x for x in df_test['id']]\ntarget = [x for x in np.argmax(pred,axis=1)]","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:23:13.042834Z","iopub.execute_input":"2021-08-06T21:23:13.04316Z","iopub.status.idle":"2021-08-06T21:23:13.048192Z","shell.execute_reply.started":"2021-08-06T21:23:13.04313Z","shell.execute_reply":"2021-08-06T21:23:13.047497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.DataFrame({\n    'id':idx,\n    'target':target\n})","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:23:13.738604Z","iopub.execute_input":"2021-08-06T21:23:13.738968Z","iopub.status.idle":"2021-08-06T21:23:13.745847Z","shell.execute_reply.started":"2021-08-06T21:23:13.738918Z","shell.execute_reply":"2021-08-06T21:23:13.744658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:23:15.029112Z","iopub.execute_input":"2021-08-06T21:23:15.029463Z","iopub.status.idle":"2021-08-06T21:23:15.041688Z","shell.execute_reply.started":"2021-08-06T21:23:15.029429Z","shell.execute_reply":"2021-08-06T21:23:15.040696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T21:23:17.088093Z","iopub.execute_input":"2021-08-06T21:23:17.088426Z","iopub.status.idle":"2021-08-06T21:23:17.099237Z","shell.execute_reply.started":"2021-08-06T21:23:17.088396Z","shell.execute_reply":"2021-08-06T21:23:17.098474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}