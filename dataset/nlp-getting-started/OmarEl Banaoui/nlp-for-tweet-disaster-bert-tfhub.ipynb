{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# downloading a tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:29:54.758892Z","iopub.execute_input":"2021-08-31T13:29:54.759239Z","iopub.status.idle":"2021-08-31T13:29:55.925203Z","shell.execute_reply.started":"2021-08-31T13:29:54.759166Z","shell.execute_reply":"2021-08-31T13:29:55.92373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_colwidth', None)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport nltk, re, string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ntf.config.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\nimport tokenization","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:29:55.927276Z","iopub.execute_input":"2021-08-31T13:29:55.927573Z","iopub.status.idle":"2021-08-31T13:30:03.58907Z","shell.execute_reply.started":"2021-08-31T13:29:55.927542Z","shell.execute_reply":"2021-08-31T13:30:03.587765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the data\ntrain_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:03.591561Z","iopub.execute_input":"2021-08-31T13:30:03.592286Z","iopub.status.idle":"2021-08-31T13:30:03.669309Z","shell.execute_reply.started":"2021-08-31T13:30:03.59224Z","shell.execute_reply":"2021-08-31T13:30:03.668202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train and text shapes\ntrain_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:03.671358Z","iopub.execute_input":"2021-08-31T13:30:03.671869Z","iopub.status.idle":"2021-08-31T13:30:03.682533Z","shell.execute_reply.started":"2021-08-31T13:30:03.671835Z","shell.execute_reply":"2021-08-31T13:30:03.681012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the duplicated tweets\ndup_train = train_df['text'].duplicated().sum()\nprint(f'there are {dup_train} tweets duplicated in train_df.')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:03.684374Z","iopub.execute_input":"2021-08-31T13:30:03.685144Z","iopub.status.idle":"2021-08-31T13:30:03.707881Z","shell.execute_reply.started":"2021-08-31T13:30:03.685099Z","shell.execute_reply":"2021-08-31T13:30:03.706438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"it seems that we have 110 duplicated tweets based on text column","metadata":{}},{"cell_type":"code","source":"# drop duplictes\ntrain_df = train_df.drop_duplicates(subset=['text'], keep='first')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:03.709962Z","iopub.execute_input":"2021-08-31T13:30:03.710504Z","iopub.status.idle":"2021-08-31T13:30:03.731354Z","shell.execute_reply.started":"2021-08-31T13:30:03.710454Z","shell.execute_reply":"2021-08-31T13:30:03.729917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new shape for train data\ntrain_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:03.732662Z","iopub.execute_input":"2021-08-31T13:30:03.733026Z","iopub.status.idle":"2021-08-31T13:30:03.744433Z","shell.execute_reply.started":"2021-08-31T13:30:03.732996Z","shell.execute_reply":"2021-08-31T13:30:03.743057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:03.748989Z","iopub.execute_input":"2021-08-31T13:30:03.749344Z","iopub.status.idle":"2021-08-31T13:30:03.770434Z","shell.execute_reply.started":"2021-08-31T13:30:03.749316Z","shell.execute_reply":"2021-08-31T13:30:03.769168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the distribution of the disaster and no-disaster tweets\ncount = train_df['target'].value_counts()\nsns.barplot(count.index, count)\ncount","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:03.773104Z","iopub.execute_input":"2021-08-31T13:30:03.77354Z","iopub.status.idle":"2021-08-31T13:30:03.950213Z","shell.execute_reply.started":"2021-08-31T13:30:03.773495Z","shell.execute_reply":"2021-08-31T13:30:03.948871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First 15 disaster tweets\nfor x in range(15):\n    ex = train_df[train_df['target'] == 0]['text'][0:15].tolist()\n    print(ex[x])","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:03.952064Z","iopub.execute_input":"2021-08-31T13:30:03.952536Z","iopub.status.idle":"2021-08-31T13:30:04.003218Z","shell.execute_reply.started":"2021-08-31T13:30:03.952449Z","shell.execute_reply":"2021-08-31T13:30:04.002058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First 15 non-disaster tweets\nfor x in range(15):\n    ex = train_df[train_df['target'] == 1]['text'][0:15].tolist()\n    print(ex[x])","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:04.004524Z","iopub.execute_input":"2021-08-31T13:30:04.004976Z","iopub.status.idle":"2021-08-31T13:30:04.039492Z","shell.execute_reply.started":"2021-08-31T13:30:04.004945Z","shell.execute_reply":"2021-08-31T13:30:04.03837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning the data and removing the stopwords\ndef Data_Cleaning(text):\n    text = text.lower()\n    text = re.sub(\"won\\'t\", \"will not\", text)\n    text = re.sub(\"can\\'t\", \"can not\", text)\n    text = re.sub(\"don\\'t\", \"do not\", text)\n    \n    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+',' ', text)\n    text = re.sub(r'&amp?;',' ', text)\n    text = re.sub(r'&lt;',' ', text)\n    text = re.sub(r'&gt;',' ', text)\n    \n    text = re.sub(r'\\d{2}:\\d{2}:\\d{2}', ' ', text)\n    text = re.sub(r'UTC', ' ', text)\n    text = re.sub(r'\\d{2}km', ' ', text)\n    text = re.sub(r\"\\b\\d+\\b\", \" \", text) # removing the numbers\n\n    text = re.sub(r\"#\",\"\",text) \n    text = re.sub(r\"(?:\\@)\\w+\", ' ', text)\n    text = re.sub(r'\\n', ' ', text)\n    \n    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n    text = re.sub(' +', ' ', text) # remove multiple spaces\n    \n    text = [word for word in word_tokenize(text) if not word in stopwords.words('english')]\n    text = ' '.join(text)\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:04.0428Z","iopub.execute_input":"2021-08-31T13:30:04.043175Z","iopub.status.idle":"2021-08-31T13:30:04.056072Z","shell.execute_reply.started":"2021-08-31T13:30:04.043137Z","shell.execute_reply":"2021-08-31T13:30:04.054835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply the cleaning function to the dataset and creating a new column of the cleaned data\ntrain_df['cleaned'] = train_df['text'].apply(lambda x: Data_Cleaning(x))\ntest_df['cleaned'] = test_df['text'].apply(lambda x: Data_Cleaning(x))","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:04.0576Z","iopub.execute_input":"2021-08-31T13:30:04.057968Z","iopub.status.idle":"2021-08-31T13:30:30.211002Z","shell.execute_reply.started":"2021-08-31T13:30:04.057925Z","shell.execute_reply":"2021-08-31T13:30:30.209878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.tail(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:30.212521Z","iopub.execute_input":"2021-08-31T13:30:30.213052Z","iopub.status.idle":"2021-08-31T13:30:30.229972Z","shell.execute_reply.started":"2021-08-31T13:30:30.213009Z","shell.execute_reply":"2021-08-31T13:30:30.22867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:30.231546Z","iopub.execute_input":"2021-08-31T13:30:30.232243Z","iopub.status.idle":"2021-08-31T13:30:30.253051Z","shell.execute_reply.started":"2021-08-31T13:30:30.232187Z","shell.execute_reply":"2021-08-31T13:30:30.251501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    masks = []\n    segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        masks.append(pad_masks)\n        segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(masks), np.array(segments)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:30.254998Z","iopub.execute_input":"2021-08-31T13:30:30.255569Z","iopub.status.idle":"2021-08-31T13:30:30.266664Z","shell.execute_reply.started":"2021-08-31T13:30:30.255524Z","shell.execute_reply":"2021-08-31T13:30:30.265092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer from tokenization script\nF_tokenizer = tokenization.FullTokenizer","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:30.268549Z","iopub.execute_input":"2021-08-31T13:30:30.269228Z","iopub.status.idle":"2021-08-31T13:30:30.283698Z","shell.execute_reply.started":"2021-08-31T13:30:30.269156Z","shell.execute_reply":"2021-08-31T13:30:30.282608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_layer = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1', trainable=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:30:30.28558Z","iopub.execute_input":"2021-08-31T13:30:30.28642Z","iopub.status.idle":"2021-08-31T13:31:11.447254Z","shell.execute_reply.started":"2021-08-31T13:30:30.286376Z","shell.execute_reply":"2021-08-31T13:31:11.44614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\nvocabulary = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n\ntokenizer = F_tokenizer(vocabulary, to_lower_case)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:31:11.450104Z","iopub.execute_input":"2021-08-31T13:31:11.450906Z","iopub.status.idle":"2021-08-31T13:31:11.586937Z","shell.execute_reply.started":"2021-08-31T13:31:11.450843Z","shell.execute_reply":"2021-08-31T13:31:11.585913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(bert_layer, max_len=512):\n    \n    input_word_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_word_ids\")\n    input_mask = Input(shape = (max_len,), dtype = tf.int32, name = \"input_mask\")\n    segment_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"segment_ids\")\n\n    pooled_sequence, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    output = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs = output)\n    model.compile(Adam(lr=1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:31:11.588835Z","iopub.execute_input":"2021-08-31T13:31:11.589305Z","iopub.status.idle":"2021-08-31T13:31:11.598248Z","shell.execute_reply.started":"2021-08-31T13:31:11.589262Z","shell.execute_reply":"2021-08-31T13:31:11.597118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\nmax_length = len(max(df.cleaned, key=len))\nmax_length","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:31:11.600089Z","iopub.execute_input":"2021-08-31T13:31:11.600863Z","iopub.status.idle":"2021-08-31T13:31:11.624455Z","shell.execute_reply.started":"2021-08-31T13:31:11.600816Z","shell.execute_reply":"2021-08-31T13:31:11.623454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like the maximum lentgh of the cleaned tweets is 138 therefore we are going to use max len of 140","metadata":{}},{"cell_type":"code","source":"train_input = bert_encode(train_df.cleaned.values, tokenizer, max_len=140)\ntest_input = bert_encode(test_df.cleaned.values, tokenizer, max_len=140)\ntrain_labels = train_df['target'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:31:11.625968Z","iopub.execute_input":"2021-08-31T13:31:11.626396Z","iopub.status.idle":"2021-08-31T13:31:15.395983Z","shell.execute_reply.started":"2021-08-31T13:31:11.626355Z","shell.execute_reply":"2021-08-31T13:31:15.394917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model(bert_layer, max_len=140)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:31:15.401608Z","iopub.execute_input":"2021-08-31T13:31:15.402004Z","iopub.status.idle":"2021-08-31T13:31:16.812383Z","shell.execute_reply.started":"2021-08-31T13:31:15.401957Z","shell.execute_reply":"2021-08-31T13:31:16.811256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint('model.h5', monitor = 'val_loss', save_best_only = True)\ntrain_history = model.fit(train_input, train_labels, validation_split = 0.25, epochs = 5, callbacks = [checkpoint], batch_size = 16)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T13:31:16.814465Z","iopub.execute_input":"2021-08-31T13:31:16.814952Z","iopub.status.idle":"2021-08-31T13:59:50.971779Z","shell.execute_reply.started":"2021-08-31T13:31:16.814905Z","shell.execute_reply":"2021-08-31T13:59:50.970542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T14:07:11.773735Z","iopub.execute_input":"2021-08-31T14:07:11.774154Z","iopub.status.idle":"2021-08-31T14:08:12.172822Z","shell.execute_reply.started":"2021-08-31T14:07:11.774124Z","shell.execute_reply":"2021-08-31T14:08:12.171639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-31T14:13:30.906448Z","iopub.execute_input":"2021-08-31T14:13:30.90683Z","iopub.status.idle":"2021-08-31T14:13:30.919432Z","shell.execute_reply.started":"2021-08-31T14:13:30.906789Z","shell.execute_reply":"2021-08-31T14:13:30.918259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-31T14:13:58.625141Z","iopub.execute_input":"2021-08-31T14:13:58.625479Z","iopub.status.idle":"2021-08-31T14:13:58.643094Z","shell.execute_reply.started":"2021-08-31T14:13:58.625451Z","shell.execute_reply":"2021-08-31T14:13:58.641859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}