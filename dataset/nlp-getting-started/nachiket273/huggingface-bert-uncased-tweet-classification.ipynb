{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Huggingface Transformer Pretrained Model\n\nThis is very basic introductory repository which uses pretrained huggingface tokenizer and bert model.<br>\nThis uses on 'text' field.<br>\n<br>\nRepository can be found: https://github.com/nachiket273/Tweet_Classification_Huggingface_Wandb <br>\n<br>\n\nFollowing steps are performed: <br>\n<br>\n1) Pre-processing:<br>\n    i) Remove urls , non-ascii characters, emoji's,  punctuations, contractions (standard preprocessing <br>\n    most of the notebooks :) <br>\n    ii) to-lower <br>\n<br>   \n2) Class - Balancing : Sampling from class with more training example to balance class.<br>\n<br>\n3) Model, Optimizer and Other configurations:<br>\ntokenizer and bert model = 'bert-base-uncased' <br>\noptimizer= AdamW <br>\nscheduler linear with warmup <br>\nstart_lr = 2e-5 <br>\ntrain_bs = 8 <br>\nvalid_bs = 8 <br>\nepochs = 5 <br>\nmax_len = 160 <br>\ndropout_ratio = 0.1 <br>\nwarmup_epochs = 0 <br>\ntest_size = 0.2 <br>\n<br>\n4) Ran for 3 different seed - 42, 11, 2020 and averaged the predictions.<br>\n<br>\n5)Repository also contains basic visualizations and tracking using 'weights and biases'.<br>\n<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n%reload_ext autoreload\n%autoreload 2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nimport time\nimport torch\nimport transformers\nfrom transformers import BertConfig\nfrom transformers import get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.insert(1, '../input/tweet-classification-huggingface-wandb1/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import config\nimport dataset\nimport model\nimport train\nimport util","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warmup_epochs = 0\nepochs = 5\nmodel_name = 'bert-base-uncased'\ntest_size = 0.3\ndropout = 0.1\nnum_classes = 2\nlinear_in = 768\nmax_len = 160\ntrain_bs = 16\nvalid_bs = 8\nstart_lr = 2e-5\n\nSEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"util.set_seed(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Preprocessing**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import preprocess\nimport re\nimport string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain_df = preprocess.fix_erraneous(train_df, ids_with_target_error, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_tabs_newlines(text):\n    text = re.sub(r'\\t', ' ', text) # remove tabs\n    text = re.sub(r'\\n', ' ', text) # remove line jump\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text'] = train_df['text'].apply(lambda k : remove_tabs_newlines(k))\ntest_df['text'] = test_df['text'].apply(lambda k : remove_tabs_newlines(k))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_url(text):\n    return re.sub(r\"http\\S+\", \"URL\", text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_non_ascii(text):\n    return ''.join([x for x in text if x in string.printable])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_usrname(text):\n    text = re.sub(r'@\\S{0,}', ' USER ', text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = re.sub(r'\\b(USER)( \\1\\b)+', r'\\1', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A list of contractions from \n# http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\ncontractions = { \n    \"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"i would\",\n    \"i'll\": \"i will\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'll\": \"it will\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"needn't\": \"need not\",\n    \"oughtn't\": \"ought not\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"she'd\": \"she would\",\n    \"she'll\": \"she will\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"that'd\": \"that would\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there had\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'll\": \"they will\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'll\": \"we will\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"who'll\": \"who will\",\n    \"who's\": \"who is\",\n    \"won't\": \"will not\",\n    \"wouldn't\": \"would not\",\n    \"you'd\": \"you would\",\n    \"you'll\": \"you will\",\n    \"you're\": \"you are\",\n    \"thx\"   : \"thanks\"\n}\n\ndef remove_contractions(text):\n    return contractions[text.lower()] if text.lower() in contractions.keys() else text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punctuations(text):\n    return text.translate(str.maketrans('', '', string.punctuation))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text'] = train_df['text'].apply(lambda k: remove_usrname(k))\ntrain_df['text'] = train_df['text'].apply(lambda k: remove_contractions(k))\ntrain_df['text'] = train_df['text'].apply(lambda k: remove_url(k))\ntrain_df['text'] = train_df['text'].apply(lambda k: remove_emoji(k))\ntrain_df['text'] = train_df['text'].apply(lambda k: remove_non_ascii(k))\ntrain_df['text'] = train_df['text'].apply(lambda k: remove_punctuations(k))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['text'] = test_df['text'].apply(lambda k: remove_usrname(k))\ntest_df['text'] = test_df['text'].apply(lambda k: remove_contractions(k))\ntest_df['text'] = test_df['text'].apply(lambda k: remove_url(k))\ntest_df['text'] = test_df['text'].apply(lambda k: remove_emoji(k))\ntest_df['text'] = test_df['text'].apply(lambda k: remove_non_ascii(k))\ntest_df['text'] = test_df['text'].apply(lambda k: remove_punctuations(k))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleanup(text):\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = text.strip()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_nums(text):\n    text = re.sub(r'^\\d\\S{0,}| \\d\\S{0,}| \\d\\S{0,}$', ' NUMBER ', text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = re.sub(r'\\b(NUMBER)( \\1\\b)+', r'\\1', text)\n    text = re.sub(r\"[0-9]\", \" \", text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text'] = train_df['text'].apply(lambda k: replace_nums(k))\ntest_df['text'] = test_df['text'].apply(lambda k: replace_nums(k))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text'] = train_df['text'].apply(lambda k: cleanup(k))\ntest_df['text'] = test_df['text'].apply(lambda k: cleanup(k))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text'][:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = train_df['target']\ntrain_df.drop(['target'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test \\\n        = train_test_split(train_df['text'], train_y, random_state=SEED, test_size=test_size, stratify=train_y.values) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Encode and get dataloaders**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = transformers.BertTokenizer.from_pretrained(\n        model_name,\n        do_lower_case=True\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = dataset.BertDataset(\n        text=X_train.values,\n        tokenizer= tokenizer,\n        max_len= max_len,\n        target=y_train.values\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_dataset = dataset.BertDataset(\n        text=X_test.values,\n        tokenizer= tokenizer,\n        max_len= max_len,\n        target=y_test.values\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dl = torch.utils.data.DataLoader(\n        train_dataset,\n        train_bs,\n        shuffle=True,\n        num_workers=4\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_dl = torch.utils.data.DataLoader(\n        valid_dataset,\n        valid_bs,\n        shuffle=True,\n        num_workers=1\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bert = model.BertHf(model_name, dp=dropout, num_classes=num_classes, linear_in=linear_in)\nbert = bert.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in bert.parameters():\n    param.requires_grad = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\nparams = list(bert.named_parameters())\nmodified_params = [\n    {'params': [p for n, p in params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n    {'params': [p for n, p in params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optim = torch.optim.AdamW(modified_params, lr=start_lr, eps=1e-8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_steps = int(len(train_df) * epochs / train_bs)\nwarmup_steps = int(len(train_df) * warmup_epochs / train_bs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sched = get_linear_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stat = util.AvgStats()\nvalid_stat = util.AvgStats()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_acc = 0.0\nbest_model_file = str(model_name) + '_best.pth.tar'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\nEpoch\\tTrain Acc\\tTrain Loss\\tTrain F1\\tValid Acc\\tValid Loss\\tValid F1\")\nfor i in range(epochs):\n    start = time.time()\n    losses, ops, targs = train.train(train_dl, bert, criterion, optim, sched, device)\n    duration = time.time() - start\n    train_acc = accuracy_score(targs, ops)\n    train_f1_score = f1_score(targs, ops)\n    train_loss = sum(losses)/len(losses)\n    train_prec = precision_score(targs, ops)\n    train_rec = recall_score(targs, ops)\n    train_roc_auc = roc_auc_score(targs, ops)\n    train_stat.append(train_loss, train_acc, train_f1_score, train_prec, train_rec, train_roc_auc, duration)\n    start = time.time()\n    lossesv, opsv, targsv = train.test(valid_dl, bert, criterion, device)\n    duration = time.time() - start\n    valid_acc = accuracy_score(targsv, opsv)\n    valid_f1_score = f1_score(targsv, opsv)\n    valid_loss = sum(lossesv)/len(lossesv)\n    valid_prec = precision_score(targsv, opsv)\n    valid_rec = recall_score(targsv, opsv)\n    valid_roc_auc = roc_auc_score(targsv, opsv)\n    valid_stat.append(valid_loss, valid_acc, valid_f1_score, valid_prec, valid_rec, valid_roc_auc, duration)\n\n    if valid_acc > best_acc:\n        best_acc = valid_acc\n        util.save_checkpoint(bert, True, best_model_file)\n        tfpr, ttpr, _ = roc_curve(targs, ops)\n        train_stat.update_best(ttpr, tfpr, train_acc, i)\n        vfpr, vtpr, _ = roc_curve(targsv, opsv)\n        valid_stat.update_best(vtpr, vfpr, best_acc, i)\n\n    print(\"\\n{}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\".format(i+1, train_acc*100, train_loss, \n                                                train_f1_score, valid_acc*100, \n                                                valid_loss, valid_f1_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Summary of best run::\")\nprint(\"Best Accuracy: {}\".format(valid_stat.best_acc))\nprint(\"Roc Auc score: {}\".format(valid_stat.roc_aucs[valid_stat.best_epoch]))\nprint(\"Loss: {}\".format(valid_stat.losses[valid_stat.best_epoch]))\nprint(\"Area Under Curve: {}\".format(auc(valid_stat.fprs, valid_stat.tprs)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now make predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dataset = dataset.BertDataset(\n        text=test_df.text.values,\n        tokenizer= tokenizer,\n        max_len= max_len\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_dl = torch.utils.data.DataLoader(\n        test_dataset,\n        1,\n        shuffle=False,\n        num_workers=1\n    )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"util.load_checkpoint(bert, best_model_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, opst, _ = train.test(test_dl, bert, criterion, device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_csv = pd.DataFrame(columns=['id', 'target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_csv['id'] = test_df['id']\nsub_csv['target'] = opst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_csv.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}