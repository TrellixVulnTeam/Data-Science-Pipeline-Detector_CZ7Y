{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"train= pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest= pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"validation_size=0.2\ntraining_size= int((1-validation_size)*len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nimport string\nimport re\nimport spacy\nsp = spacy.load('en_core_web_sm')\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\nlemma= WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def clean_data(text):\n    #remove emails\n    text = ' '.join([i for i in text.split() if '@' not in i])\n    \n    #remove web address\n    text = re.sub('http[s]?://\\S+', '', text)\n    \n    #Filter to allow only alphabets\n    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n    \n    #Remove Unicode characters\n    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    \n    #Convert to lowercase to maintain consistency\n    text = text.lower()\n    \n    #Remove stopwords\n    all_stopwords = sp.Defaults.stop_words\n    text_tokens = word_tokenize(text)\n    tokens_without_sw= [word for word in text_tokens if not word in all_stopwords]\n      \n    #lemmatization\n    text= [lemma.lemmatize(w) for w in tokens_without_sw]\n    text= ' '.join(text)\n  \n    #remove double spaces \n    text = re.sub('\\s+', ' ',text)\n    return (text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print(train['text'][0])\nprint(train['text'][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train['text']= train['text'].apply(clean_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print(train['text'][0])\nprint(train['text'][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train=train.reindex(np.random.permutation(train.index))\ntrain= train.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"corpus=[]\nlabels=[]\nfor i in range(len(train)):\n    corpus.append(train['text'][i])\n    labels.append(train['target'][i])\n\nprint(corpus[2])\nprint(labels[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#vocab_size=20000\noov_token= \"<oov>\"\npadding_type='post'\ntrunc_type='post'\nembedding_dim=100\nmax_len= max([len(x) for x in corpus])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_data= corpus[0:training_size]\nvalidation_data= corpus[training_size:]\n\ntraining_labels=labels[0:training_size]\nvalidation_labels= labels[training_size:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"tokenizer=Tokenizer()\ntokenizer.fit_on_texts(corpus)\nword_index= tokenizer.word_index\nvocab_size= len(word_index)\nprint(vocab_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"training_sequences=tokenizer.texts_to_sequences(train_data)\npadded_training= pad_sequences(training_sequences,padding=padding_type,maxlen=max_len)\n\nvalidation_sequences= tokenizer.texts_to_sequences(validation_data)\npadded_validation= pad_sequences(validation_sequences,padding=padding_type,truncating=trunc_type,maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print(test['text'][0])\nprint(test['text'][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"test['text']=test['text'].apply(clean_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"print(test['text'][0])\nprint(test['text'][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"testing_sentences=[]\n\nfor i in range(len(test)):\n    testing_sentences.append(test['text'][i])\n    \ntesting_sequences= tokenizer.texts_to_sequences(testing_sentences)\npadded_testing= pad_sequences(testing_sequences,padding=padding_type,maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false,"scrolled":true},"cell_type":"code","source":"padded_training= np.array(padded_training)\ntraining_labels= np.array(training_labels)\n\npadded_validation= np.array(padded_validation)\nvalidation_labels= np.array(validation_labels)\n\npadded_testing= np.array(padded_testing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"embeddings_index = {};\nGLOVE_DIR= '../input/glove-global-vectors-for-word-representation'\nwith open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"model= Sequential()\nmodel.add(Embedding(vocab_size+1,embedding_dim,input_length=max_len, weights=[embeddings_matrix],trainable=False))\nmodel.add(Dropout(0.3))\nmodel.add(Bidirectional(LSTM(64,return_sequences= True)))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"adam= Adam(0.0003)\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"history=model.fit(padded_training,training_labels,epochs=20,validation_data=(padded_validation,validation_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"predictions= model.predict_classes(padded_testing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sample=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sample['target']= (predictions>0.5).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sample.to_csv(\"new_submission.csv\",index=False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}