{"cells":[{"metadata":{},"cell_type":"markdown","source":"Refrences : <b>\n    \nhttps://www.kaggle.com/parulpandey/getting-started-with-nlp-a-general-intro\nhttps://www.kaggle.com/parulpandey/getting-started-with-nlp-feature-vectors"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_description(df):\n    print(df.info())\n    print(\"*\"* 40)\n    print(df.describe())\n    print(\"*\"* 40)\n    print(df.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_description(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_description(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train_df.target.value_counts()\nsns.barplot(x.index,x)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can conclude that no of fake disasters are more than the genuine ones"},{"metadata":{},"cell_type":"markdown","source":"Now we know that the twitter has increased the character length of a tweet from 140 characters to 280.Let's make some graphical distribution for characters and size of graph in a tweet."},{"metadata":{"trusted":true},"cell_type":"code","source":"def no_of_characters_graphs(df):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n    tweet_len=df[df['target']==1]['text'].str.len()\n    ax1.hist(tweet_len,color='red')\n    ax1.set_title('Disaster tweets')\n    tweet_len=df[df['target']==0]['text'].str.len()\n    ax2.hist(tweet_len,color='green')\n    ax2.set_title('Not disaster tweets')\n    fig.suptitle('Characters in tweets')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_of_characters_graphs(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def no_words_graphs(df):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n    tweet_len=df[df['target']==1]['text'].str.split().map(lambda x: len(x))\n    ax1.hist(tweet_len,color='red')\n    ax1.set_title('disaster tweets')\n    tweet_len=df[df['target']==0]['text'].str.split().map(lambda x: len(x))\n    ax2.hist(tweet_len,color='green')\n    ax2.set_title('Not disaster tweets')\n    fig.suptitle('Words in a tweet')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_words_graphs(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\nstop=set(stopwords.words('english'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom collections import  Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train_df[train_df['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Common stopwords in tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \nx,y=zip(*top)\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\n\nx,y=zip(*top)\nplt.bar(x,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"message =\"my number is 510-123-4567\"\nmyregex = re.compile(r'\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d')\nmatch = myregex.search(message)\nprint(match.group())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ntxt = \"my number is 990-445-4836 and 844-096-1968\"\nmyregex = re.compile(r'\\d\\d\\d-\\d\\d\\d-\\d\\d\\d\\d')\nprint(myregex.findall(txt))\n\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basic EDA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing values in test set\ntest_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploring the Target Column\nDistribution of the Target Column.<br>\nWe have to predict whether a given tweet is about a real disaster or not. - If so, predict a 1. If not, predict a 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train_df['target'].value_counts().index,train_df['target'].value_counts(),palette='rocket')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A disaster tweet\ndisaster_tweets = train_df[train_df['target']==1]['text']\ndisaster_tweets.values[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_disaster_tweets = train_df[train_df['target']==0]['text']\nnon_disaster_tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['keyword'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['keyword'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"1. Data Cleaning\n**Before we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Some of the basic text pre-processing techniques includes:**<br>\n\nMake text all lower case or uppercase so that the algorithm does not treat the same words in different cases as different<br>\n<br>\n\n**Removing Noise** i.e everything that isn’t in a standard number or letter i.e Punctuation, Numerical values, common non-sensical text <br>\n<br>\n\n**Tokenization: **Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n<br>\n<br>\n\n**Stopword Removal:** Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying a first round of text cleaning techniques","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying a first round of text cleaning techniques\nimport string\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain_df['text'] = train_df['text'].apply(lambda x: clean_text(x))\ntest_df['text'] = test_df['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain_df['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Tokenization <br>\nTokenization is a process that splits an input sequence into so-called tokens where the tokens\ncan be a word, sentence, paragraph etc. <br>\nBase upon the type of tokens we want, tokenization can be of various types, for instance"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\ntext = \"Are you coming , aren't you\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \",text)\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain_df['text'] = train_df['text'].apply(lambda x: tokenizer.tokenize(x))\ntest_df['text'] = test_df['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain_df['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Stopwords Removal <br>\nNow, let's get rid of the stopwords i.e words which occur very frequently but have no possible value like a, an, the, are etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\ntrain_df['text'] = train_df['text'].apply(lambda x : remove_stopwords(x))\ntest_df['text'] = test_df['text'].apply(lambda x : remove_stopwords(x))\ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stemming and Lemmatization examples\ntext = \"feet cats wolves talked\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer\nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer=nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is important to note here that stemming and lemmatization sometimes donot necessarily improve results as at times we donot want to trim words but rather preserve their original form. Hence their usage actually differs from problem to problem. For this problem, I will not use these techniques."},{"metadata":{"trusted":true},"cell_type":"code","source":"# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain_df['text'] = train_df['text'].apply(lambda x : combine_text(x))\ntest_df['text'] = test_df['text'].apply(lambda x : combine_text(x))\ntrain_df['text']\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting it all together- A Text Preprocessing Function<br>\nThis concludes the pre-processing part. It will be prudent to convert all the steps undertaken into a function for better reusability."},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#5. Transforming tokens to a vector¶ <br>\nAfter the initial preprocessing phase, we need to transform text into a meaningful vector (or array) of numbers. This can be done by a number of tecniques:\n<br>\n<br>\n\n"},{"metadata":{},"cell_type":"markdown","source":"Bag of Words - Countvectorizer Features\nCountvectorizer converts a collection of text documents to a matrix of token counts. <br>\nIt is important to note here that CountVectorizer comes with a lot of options to\nautomatically do preprocessing, tokenization, and stop word removal.<br>\nHowever, i did all the process manually above to just get a better understanding.\nLet's use a vanilla implementation of the countvectorizer without specifying any parameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\ncount_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train_df['text'])\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Short intro to countVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = ['The weather is sunny', 'The weather is partly sunny and partly cloudy.']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvectorizer.fit(sentences)\nvectorizer.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.transform(sentences).toarray()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By default, a scikit learn Count vectorizer can perform the following opertions over a text corpus:\n\n* Encoding via utf-8\n* converts text to lowercase\n* Tokenizes text using word level tokenization<br>\n\nCountVectorizer has a number of parameters. Let's look at some of them :"},{"metadata":{"trusted":true},"cell_type":"code","source":"# StopWord removal using countvectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords = stopwords.words('english')\n\ncount_vectorizer = CountVectorizer(stop_words = stopwords)\ncount_vectorizer.fit(train_df['text'])\n\ntrain_vectors = count_vectorizer.fit_transform(train_df['text'])\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])\n\n## Keeping only non-zero elements to preserve space \ntrain_vectors.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transforming tokens to a vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train_df['text'])\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[0].todense())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(train_vectors, train_df[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission\nsample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"] = clf.predict(test_vectors)\nsample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Well I got a score of .078527","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**TFIDF** <br>\nA problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content”. Also, it will give more weight to longer documents than shorter documents.\n\nOne approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like “the” that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency-Inverse Document Frequency, or TF-IDF for short, where:\n\n**Term Frequency: is a scoring of the frequency of the word in the current document.**\n\nTF = (Number of times term t appears in a document)/(Number of terms in the document)\n\n**Inverse Document Frequency: is a scoring of how rare the word is across documents.**\n\nIDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n\n"},{"metadata":{},"cell_type":"markdown","source":"**Creating a Baseline Model using TFIDF**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word level\ntfidf = TfidfVectorizer(analyzer='word',token_pattern=r'\\w{1,}',max_features=5000)\ntrain_tfidf = tfidf.fit_transform(train_df['text'])\ntest_tfidf = tfidf.transform(test_df[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer( min_df=3,  max_features=None,analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = stopwords)\n\ntrain_tfidf = tfidf.fit_transform(train_df['text'])\ntest_tfidf = tfidf.transform(test_df[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(train_tfidf, train_df[\"target\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submission\nsample_submission_1 = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission_1[\"target\"] = clf.predict(test_tfidf)\nsample_submission_1.to_csv(\"submission_1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Advance Algorithms"},{"metadata":{},"cell_type":"markdown","source":"Well, this is a decent score. Let's try with another model that is said to work well with text data : Naive Bayes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naives Bayes Clssifiers\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Naive Bayes on Counts\nclf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_tfidf, train_df[\"target\"], cv=5, scoring=\"f1\")\nscores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_NB.fit(train_tfidf, train_df[\"target\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_2 = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission_2[\"target\"] = clf_NB.predict(test_tfidf)\nsample_submission_2.to_csv(\"submission_2.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}