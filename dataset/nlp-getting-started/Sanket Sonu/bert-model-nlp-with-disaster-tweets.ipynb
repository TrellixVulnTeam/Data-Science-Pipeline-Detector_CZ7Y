{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U tensorflow-text\n!pip install -q tf-models-official\n!pip install tensorflow_hub","metadata":{"id":"jj0LPMF13xl4","outputId":"cf674505-7523-4f09-a06d-e6c40c88c45e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Note:** \nIt is not recommended to remove features like stopwords, numbers, repeating characters, Stem & Lemm, and punctuations for the BERT model. Removing these can be a bad idea for the BERT model because BERT is a pre-trained model and these features can give a negative impact on the accuracy. Because single punctuation or repeating characters or words also shows some emotions. You can always do experiments to check which features give what impact, it will vary based on the dataset.","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nfrom tensorflow.keras.layers import Input, Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nimport wordcloud\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom keras import backend as K\nfrom transformers import AutoTokenizer,TFBertModel\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import CategoricalAccuracy, BinaryAccuracy\nfrom tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy\n\n\nsns.set_style(\"whitegrid\")","metadata":{"id":"M00HMxyi3ASB","outputId":"7123a371-5f97-4d3b-dad7-46c7ff09746a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\ntrain.head()","metadata":{"id":"QRm2Lhja3kDG","outputId":"667a954f-644c-40cc-88b4-7a08a9dc6eb7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(['keyword','location'], axis = 1)\ntest = test.drop(['keyword','location'], axis = 1)\ntrain.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking Shape of Train and Test sets:\nprint(\"Shape of Train set:\", train.shape)\nprint(\"Shape of Test set:\", test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Labels are as follows:\nlabel '1' ---> racist/sexist tweet           \nlabel '0' ---> not racist/sexist tweet","metadata":{"id":"Gw2tIJ_C3i-C"}},{"cell_type":"code","source":"df = train.copy()\ndf['target'].value_counts()","metadata":{"id":"fleUSdtI3kOZ","outputId":"a8196139-0e5d-4bf4-e691-0de9b672c36a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Model without removing any feature:","metadata":{"id":"R338ymKtGmI8"}},{"cell_type":"markdown","source":"### Splitting data into Train and Test sets","metadata":{"id":"c4DR_rqRGyOT"}},{"cell_type":"code","source":"y = tf.keras.utils.to_categorical(df['target'], num_classes=2)\nX_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=3)","metadata":{"id":"eXZiaKYnGuAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT \n### Base Model with Neural Networks:","metadata":{"id":"oV4EGmipG-1y"}},{"cell_type":"code","source":"bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3\")\nbert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/4\")","metadata":{"id":"_2eseBvNGt8U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking array created using BERT:\ndef get_sentence_embedding(sentences):\n  preprocessed_text = bert_preprocess(sentences)\n  return bert_encoder(preprocessed_text)['pooled_output']\n\nget_sentence_embedding([\"You are noob.\",\"What are you looking at?\"])","metadata":{"id":"WhbITOFaGt6C","outputId":"3e3e40b0-7908-404a-9cdc-4ec5d3da36bb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bert layers:\nnum_classes = 2\ntext_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\npreprocessed_text = bert_preprocess(text_input)\noutputs = bert_encoder(preprocessed_text)\n\n# Neural network layers:\nl = tf.keras.layers.Dropout(0.2, name='dropout')(outputs['pooled_output'])\nl = tf.keras.layers.Dense(num_classes, activation='sigmoid', name='output')(l)\n\n# Construct final model:\nmodel = tf.keras.Model(inputs=[text_input], outputs=[l])\n\nmodel.summary()\n\n# You can use these METRICS as well. If you are using this then change 'metrics=METRICS' in 'model.compile' section.\n# METRICS = [\n#            tf.keras.metrics.BinaryCrossentropy(name='accuracy'),\n#            tf.keras.metrics.Precision(name='precision'),\n#            tf.keras.metrics.Recall(name='recall')\n# ]\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","metadata":{"id":"CdQ0GPmbGt25","outputId":"553c8161-2566-4907-c334-4264cf135750"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ploting Model Architecture:\ntf.keras.utils.plot_model(model)","metadata":{"id":"SMh9FauXHV8-","outputId":"0da8485c-12c4-4f79-fe1b-7d8f9fc9f076"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training model:","metadata":{"id":"8DRYvpP9JCXh"}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=3, validation_split=0.1)\n\n# Evaluating results with test set:\nmodel.evaluate(X_test, y_test, verbose=1)","metadata":{"id":"GtjGaPN9HV__","outputId":"9f1e6814-f109-4581-9ba7-a3d72acdeb75"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{"id":"M4EBwwBsHbHo"}},{"cell_type":"code","source":"y_test_arg = np.argmax(y_test, axis=1)\ny_test_arg[1]\ny_pred = np.argmax(model.predict(X_test),axis=1)\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test_arg, y_pred))\nprint(metrics.classification_report(y_test_arg, y_pred))","metadata":{"id":"B7D6dPi0HWFL","outputId":"869001b7-0bd5-4c7a-de52-698964569dc2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy vs Loss:","metadata":{}},{"cell_type":"code","source":"# This builds a graph with all the available metrics of the history.\n\npd.DataFrame(history.history).plot(figsize=(10,6))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model after removing stopwords:","metadata":{"id":"yz9aU8MRIjZe"}},{"cell_type":"code","source":"df = train.copy()","metadata":{"id":"judFCs9VIm9-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Stopwords:","metadata":{"id":"Xm7rcox7IsaH"}},{"cell_type":"code","source":"sw = stopwords.words('english')\ndf['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))","metadata":{"id":"XVw75OFyIwxo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test sets","metadata":{"id":"_767Q4kPI28l"}},{"cell_type":"code","source":"y = tf.keras.utils.to_categorical(df['target'], num_classes=2)\nX_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=3)","metadata":{"id":"Rgho-xB7Iwvc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training model:","metadata":{"id":"eXwzufACI_Q8"}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=3, validation_split=0.1)\n\n# Evaluating results with test set:\nmodel.evaluate(X_test, y_test, verbose=1)","metadata":{"id":"855Z6G95IwqY","outputId":"24bb5d07-c3a8-4f15-f2e8-0a580a72aea9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{"id":"u-b02s4aJRjb"}},{"cell_type":"code","source":"y_test_arg = np.argmax(y_test, axis=1)\ny_test_arg[1]\ny_pred = np.argmax(model.predict(X_test),axis=1)\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test_arg, y_pred))\nprint(metrics.classification_report(y_test_arg, y_pred))","metadata":{"id":"UkY86LqKIwoP","outputId":"a7cbffc5-f7eb-4c41-fb2e-53f3cf719cd2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy vs Loss:","metadata":{}},{"cell_type":"code","source":"# This builds a graph with all the available metrics of the history.\n\npd.DataFrame(history.history).plot(figsize=(10,6))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model after removing repeating characters:","metadata":{"id":"dELBCFu8JUwd"}},{"cell_type":"code","source":"df = train.copy()","metadata":{"id":"5VT6YGXpJdiM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing repeating characteres:","metadata":{"id":"yEjPIcV7JUs_"}},{"cell_type":"code","source":"tokens = (word_tokenize(i) for i in df.text)\ndf['text'] = df['text'].apply(nltk.word_tokenize)\n\npattern = re.compile(r'(.)\\1*')\n\ndef reduce_sequence_word(word):\n    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n\ndef reduce_sequence_tweet(tweet):\n    return [reduce_sequence_word(word) for word in tweet]\n\ndf.text = df.text.apply(lambda tweet: reduce_sequence_tweet(tweet))\n\n# Detokenizing tweets:\n\ndef listToString(s): \n    \n    # initialize an empty string\n    str1 = \" \" \n    \n    # return string  \n    return (str1.join(s))\n\ndf['text'] = df['text'].apply(lambda x: listToString(x))","metadata":{"id":"4A9TEHapIwmE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test sets","metadata":{"id":"QRU61clEJnO7"}},{"cell_type":"code","source":"y = tf.keras.utils.to_categorical(df['target'], num_classes=2)\nX_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=3)","metadata":{"id":"1Ap7UDZfIwhz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training model:","metadata":{"id":"hRSOFwRgJuHe"}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=3, validation_split=0.1)\n\n# Evaluating results with test set:\nmodel.evaluate(X_test, y_test, verbose=1)","metadata":{"id":"zfXgZs8TJq8D","outputId":"2fd6a561-edfa-452c-85a1-5a048354b03c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{"id":"2QRues6VJwkf"}},{"cell_type":"code","source":"y_test_arg = np.argmax(y_test, axis=1)\ny_test_arg[1]\ny_pred = np.argmax(model.predict(X_test),axis=1)\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test_arg, y_pred))\nprint(metrics.classification_report(y_test_arg, y_pred))","metadata":{"id":"Pvwj-Tm7Jq-c","outputId":"a6db0f71-a7c3-4269-bc32-c4ef0642911f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy vs Loss:","metadata":{}},{"cell_type":"code","source":"# This builds a graph with all the available metrics of the history.\n\npd.DataFrame(history.history).plot(figsize=(10,6))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Model after removing Punctuations:","metadata":{"id":"TXECfDyLJzT7"}},{"cell_type":"code","source":"df = train.copy()","metadata":{"id":"qK0EU7EeJrAl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Punctuations:","metadata":{"id":"CszDScGqJ7Y1"}},{"cell_type":"code","source":"import string\nstring.punctuation","metadata":{"id":"xxs9pUT0JrCu","outputId":"56018a58-b9ed-4b14-9c0f-6f943ca8d75b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punctuations_list = string.punctuation\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)\n\ndf['text'] = df['text'].apply(lambda x: cleaning_punctuations(x))","metadata":{"id":"aH2u3qqgJrFT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test sets","metadata":{"id":"iGLTnY9NJ-P2"}},{"cell_type":"code","source":"y = tf.keras.utils.to_categorical(df['target'], num_classes=2)\nX_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=3)","metadata":{"id":"NaHjtYwCJrHv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training model:","metadata":{"id":"2OK1yotwKUu8"}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=3, validation_split=0.1)\n\n# Evaluating results with test set:\nmodel.evaluate(X_test, y_test, verbose=1)","metadata":{"id":"j8TP1D-dJrMC","outputId":"7a607ece-fcb4-40ce-c14e-0fdd1b0452da"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{"id":"RLluY7b5Kait"}},{"cell_type":"code","source":"y_test_arg = np.argmax(y_test, axis=1)\ny_test_arg[1]\ny_pred = np.argmax(model.predict(X_test),axis=1)\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test_arg, y_pred))\nprint(metrics.classification_report(y_test_arg, y_pred))","metadata":{"id":"Y0253zglJrOG","outputId":"6b408487-cc86-4e1d-dcc2-f6e7b88ff74d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy vs Loss:","metadata":{}},{"cell_type":"code","source":"# This builds a graph with all the available metrics of the history.\n\npd.DataFrame(history.history).plot(figsize=(10,6))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Model after removing numbers:","metadata":{"id":"GvMEqQdHKdDg"}},{"cell_type":"code","source":"df = train.copy()","metadata":{"id":"23VpbPYSKid3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing numbers:","metadata":{"id":"O6F8QjQTKc84"}},{"cell_type":"code","source":"def cleaning_numbers(text):\n    return re.sub('[0-9]+', '', text)\n\ndf['text'] = df['text'].apply(lambda text: cleaning_numbers(text))","metadata":{"id":"QtMPakeBKoAF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test sets","metadata":{"id":"EYzzt9PyKp0Z"}},{"cell_type":"code","source":"y = tf.keras.utils.to_categorical(df['target'], num_classes=2)\nX_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=3)","metadata":{"id":"M0aJGiFwKvjM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training model:","metadata":{"id":"JYVbwsseK5GE"}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=3, validation_split=0.1)\n\n# Evaluating results with test set:\nmodel.evaluate(X_test, y_test, verbose=1)","metadata":{"id":"n_wVKVmmKvno","outputId":"ab2ccaae-cd3b-4a2d-e8ba-a616bafb31f4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{"id":"0USOcWEgK6IP"}},{"cell_type":"code","source":"y_test_arg = np.argmax(y_test, axis=1)\ny_test_arg[1]\ny_pred = np.argmax(model.predict(X_test),axis=1)\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test_arg, y_pred))\nprint(metrics.classification_report(y_test_arg, y_pred))","metadata":{"id":"15Vmft4CKvra","outputId":"fc3ba2b6-9e49-4da7-cda3-b76f4e55a71a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy vs Loss:","metadata":{}},{"cell_type":"code","source":"# This builds a graph with all the available metrics of the history.\n\npd.DataFrame(history.history).plot(figsize=(10,6))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup 6: Applying Stemming and Lemmatization:","metadata":{}},{"cell_type":"code","source":"df = train.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying Stemming: ","metadata":{}},{"cell_type":"code","source":"# Tokenizing tweets:\ntokens = (word_tokenize(i) for i in df.text)\ndf['text'] = df['text'].apply(nltk.word_tokenize)\n\nstemm = SnowballStemmer('english')\ndf['text'] = df['text'].apply(lambda x: [stemm.stem(y) for y in x])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test :","metadata":{}},{"cell_type":"code","source":"y = tf.keras.utils.to_categorical(df['target'].astype(str), num_classes=2)\nX_train, X_test, y_train, y_test = train_test_split(df['text'].astype(str), y, test_size=0.2, random_state=3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training model:","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=3, validation_split=0.1)\n\n# Evaluating results with test set:\nmodel.evaluate(X_test, y_test, verbose=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results:","metadata":{}},{"cell_type":"code","source":"y_test_arg = np.argmax(y_test, axis=1)\ny_test_arg[1]\ny_pred = np.argmax(model.predict(X_test),axis=1)\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test_arg, y_pred))\nprint(metrics.classification_report(y_test_arg, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy vs Loss:","metadata":{}},{"cell_type":"code","source":"# This builds a graph with all the available metrics of the history.\n\npd.DataFrame(history.history).plot(figsize=(10,6))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Models after removing all the features:","metadata":{"id":"8AHYcK_tICb3"}},{"cell_type":"code","source":"df = train.copy()","metadata":{"id":"gH0-x3_OID8C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Punctuations:","metadata":{"id":"qRieS82V3uxn"}},{"cell_type":"code","source":"import string\nstring.punctuation","metadata":{"id":"NVrSUteG3iWi","outputId":"806ce6b2-0e65-4939-efa0-0fa280f12e49"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punctuations_list = string.punctuation\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)\n\ndf['text'] = df['text'].apply(lambda x: cleaning_punctuations(x))","metadata":{"id":"IGkijhQ84Ycq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Stopwords:","metadata":{"id":"qhQgMvaP31XO"}},{"cell_type":"code","source":"sw = stopwords.words('english')\ndf['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))","metadata":{"id":"Up4kO1Qw4bPE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Numbers:","metadata":{"id":"7CGqcOfV367y"}},{"cell_type":"code","source":"def cleaning_numbers(text):\n    return re.sub('[0-9]+', '', text)\n\ndf['text'] = df['text'].apply(lambda text: cleaning_numbers(text))","metadata":{"id":"6VLYSul94dId"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing repeating characters:","metadata":{"id":"syTrKFQNCHtj"}},{"cell_type":"code","source":"tokens = (word_tokenize(i) for i in df.text)\ndf['text'] = df['text'].apply(nltk.word_tokenize)\n\npattern = re.compile(r'(.)\\1*')\n\ndef reduce_sequence_word(word):\n    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n\ndef reduce_sequence_tweet(tweet):\n    return [reduce_sequence_word(word) for word in tweet]\n\ndf.text = df.text.apply(lambda tweet: reduce_sequence_tweet(tweet))\n\n# Detokenizing tweets:\n\ndef listToString(s): \n    \n    # initialize an empty string\n    str1 = \" \" \n    \n    # return string  \n    return (str1.join(s))\n\ndf['text'] = df['text'].apply(lambda x: listToString(x))","metadata":{"id":"qYgP8wXzCKZ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying Stemming: ","metadata":{}},{"cell_type":"code","source":"# Tokenizing tweets:\ntokens = (word_tokenize(i) for i in df.text)\ndf['text'] = df['text'].apply(nltk.word_tokenize)\n\nstemm = SnowballStemmer('english')\ndf['text'] = df['text'].apply(lambda x: [stemm.stem(y) for y in x])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test sets","metadata":{"id":"2a3qKrLa4UP6"}},{"cell_type":"code","source":"y = tf.keras.utils.to_categorical(df['target'].astype(str), num_classes=2)\nX_train, X_test, y_train, y_test = train_test_split(df['text'].astype(str), y, test_size=0.2, random_state=3)","metadata":{"id":"ggZvcwG0l6q7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training model:","metadata":{"id":"uf_4sAyuJLIq"}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs=3, validation_split=0.1)\n\n# Evaluating results with test set:\nmodel.evaluate(X_test, y_test, verbose=1)","metadata":{"id":"xJTri_ZlhGA4","outputId":"08e429e1-343f-4a3d-ea68-e3596ab0cb70"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{"id":"0HqeR3Q8Ict7"}},{"cell_type":"code","source":"y_test_arg = np.argmax(y_test, axis=1)\ny_test_arg[1]\ny_pred = np.argmax(model.predict(X_test),axis=1)\nprint('Confusion Matrix')\nprint(confusion_matrix(y_test_arg, y_pred))\nprint(metrics.classification_report(y_test_arg, y_pred))","metadata":{"id":"6WKfiGDNLWaN","outputId":"b75cb037-3fb1-4a8e-8f43-fc08a066ba03"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Accuracy vs Loss:","metadata":{"id":"ZwAqjLaPAPy7"}},{"cell_type":"code","source":"# This builds a graph with all the available metrics of the history.\n\npd.DataFrame(history.history).plot(figsize=(10,6))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}