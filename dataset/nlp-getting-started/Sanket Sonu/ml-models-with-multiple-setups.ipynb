{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing libraries:\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\n%matplotlib inline\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:31.861774Z","iopub.execute_input":"2022-03-31T11:38:31.862444Z","iopub.status.idle":"2022-03-31T11:38:33.818679Z","shell.execute_reply.started":"2022-03-31T11:38:31.862344Z","shell.execute_reply":"2022-03-31T11:38:33.817909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing:","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:33.820382Z","iopub.execute_input":"2022-03-31T11:38:33.820818Z","iopub.status.idle":"2022-03-31T11:38:33.896403Z","shell.execute_reply.started":"2022-03-31T11:38:33.820779Z","shell.execute_reply":"2022-03-31T11:38:33.895703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Removing Keyword and Location variables from both train and test:","metadata":{}},{"cell_type":"code","source":"train = train.drop(['keyword','location'], axis = 1)\ntest = test.drop(['keyword','location'], axis = 1)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:33.897441Z","iopub.execute_input":"2022-03-31T11:38:33.897721Z","iopub.status.idle":"2022-03-31T11:38:33.915042Z","shell.execute_reply.started":"2022-03-31T11:38:33.897685Z","shell.execute_reply":"2022-03-31T11:38:33.914118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking Shape of Train and Test sets:\nprint(\"Shape of Train set:\", train.shape)\nprint(\"Shape of Test set:\", test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:33.917517Z","iopub.execute_input":"2022-03-31T11:38:33.918086Z","iopub.status.idle":"2022-03-31T11:38:33.92461Z","shell.execute_reply.started":"2022-03-31T11:38:33.918048Z","shell.execute_reply":"2022-03-31T11:38:33.923562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing duplicates of Train set. There are few duplicates in Test set as well,\n# however, duplicates of Test set can'b be removed because the final test with target has to be uploaded as a submission file\n\ntrain = train.drop_duplicates(subset=['text'], keep='last')\nprint(\"Shape of Train set after removing duplicates:\", train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:33.925944Z","iopub.execute_input":"2022-03-31T11:38:33.926592Z","iopub.status.idle":"2022-03-31T11:38:33.940229Z","shell.execute_reply.started":"2022-03-31T11:38:33.926539Z","shell.execute_reply":"2022-03-31T11:38:33.939516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[train['text'].map(lambda x: x.isascii())]\ntest[test['text'].map(lambda x: x.isascii())]\n\n# Cleaning Tweets\ndef clean_tweets(text):\n    text = re.sub(r'@[A-Za-z0-9_]+','',text)    # Removing @mentions\n    text = re.sub(r'#','',text)                 # Removing #tag symbol\n    text = re.sub(r'RT[\\s]+',' ',text)          # Remvoing RT\n    text = re.sub(r'\\n','',text) \n    text = re.sub(r',','',text) \n    text = re.sub(r'.[.]+','',text) \n    text = re.sub(r'\\w+:\\/\\/\\S+','',text) \n    text = re.sub(r'https?:\\/\\/\\S+','',text)    # Removing hyperlinks\n    text = re.sub(r'/',' ',text)\n    text = re.sub(r'-',' ',text)\n    text = re.sub(r'_',' ',text)\n    text = re.sub(r'!','',text)\n    text = re.sub(r':',' ',text)\n    text = re.sub(r'$','',text)\n    text = re.sub(r'%','',text)\n    text = re.sub(r'^','',text)\n    text = re.sub(r'&','',text)\n    text = re.sub(r'=',' ',text)\n    text = re.sub(r' +',' ',text)               # Removing extra whitespaces\n\n    return text\n\n# Removing Emojis\ndef clean_emoji(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\n\ntrain['text'] = train['text'].apply(clean_tweets)    # Applying function to clean tweets\ntrain['text'] = train['text'].apply(clean_emoji)     # Applying function to remove emojis\ntrain['text'] = train.text.str.lower()               # Making all texts to lower case\ntrain['text'] = train['text'].str.strip()            # Removing leading and trailing whitespaces\n\ntest['text'] = test['text'].apply(clean_tweets)      # Applying function to clean tweets\ntest['text'] = test['text'].apply(clean_emoji)       # Applying function to remove emojis\ntest['text'] = test.text.str.lower()                 # Making all texts to lower case\ntest['text'] = test['text'].str.strip()              # Removing leading and trailing whitespaces\n#pd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', -1)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:33.941492Z","iopub.execute_input":"2022-03-31T11:38:33.941836Z","iopub.status.idle":"2022-03-31T11:38:34.394794Z","shell.execute_reply.started":"2022-03-31T11:38:33.941797Z","shell.execute_reply":"2022-03-31T11:38:34.394116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Labels are as follows:\n'target' -> This denotes whether a tweet is about a real disaster (1) or not (0)","metadata":{}},{"cell_type":"code","source":"train['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:34.396049Z","iopub.execute_input":"2022-03-31T11:38:34.396302Z","iopub.status.idle":"2022-03-31T11:38:34.406382Z","shell.execute_reply.started":"2022-03-31T11:38:34.39627Z","shell.execute_reply":"2022-03-31T11:38:34.405528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setups:\n\nEach of our classification models (SVM, Naive Bayes, Logistic Regression, and Random Forest) were\ntested on the following setups:\n\n1. **Setup 1: Removing Punctuation:** All the models are trained and tested after removing punctuations from the corpus.\n2. **Setup 2: Removing Stop-words:** All the models are trained and tested after removing stop-words from the corpus.\n3. **Setup 3: Removing Numbers:** All the models are trained and tested after removing numbers from the corpus.\n4. **Setup 4: Removing Repeating Characters:** All the models are trained and tested after removing repeating characters.\n5. **Setup 5: Stemming and Lemmatization:** All the models are trained and tested after applying stemming and lemmatization.\n6. **Setup 6: Setup 1â€“5:** All the models are trained and tested after removing punctuation, stop-words, numbers, repeating words, stemming and lemmatization.\n7. **Setup 7: Keeping all above features:** All the models are trained and tested without eliminating any of the above special features.","metadata":{}},{"cell_type":"markdown","source":"# Models:\n### These models with hyperparameters will be used by all setups, to find the best setup and best model:","metadata":{}},{"cell_type":"code","source":"# making a dictionary with four models with some parameters:\n\nmodel_params = {\n    \n    'SVC' :{\n        'model' : SVC(),\n        'params' : {\n            'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01], 'kernel': ['rbf','linear','poly','sigmoid']\n        }\n    },\n    \n    'MultinomialNB' :{\n        'model' : MultinomialNB(),\n        'params' : {\n            'alpha' : np.linspace(0.5, 1.5, 6), 'fit_prior' : [True, False]\n        }\n    },\n    \n    'logistics_regression' :{\n        'model' : LogisticRegression(solver = 'lbfgs', multi_class = 'auto'),\n        'params' : {\n            'C' : [0.1, 1, 20, 40, 60, 80, 100], 'solver' : ['lbfgs', 'liblinear']\n        }\n    },\n    \n    'random_forest' :{\n        'model' : RandomForestClassifier(),\n        'params' : {\n            'n_estimators' : [80,85,90,95,100], \n            'max_depth':[20,30,None], 'criterion':['gini','entropy']\n        }\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:34.408064Z","iopub.execute_input":"2022-03-31T11:38:34.408458Z","iopub.status.idle":"2022-03-31T11:38:34.419196Z","shell.execute_reply.started":"2022-03-31T11:38:34.408355Z","shell.execute_reply":"2022-03-31T11:38:34.418524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup 1: Models after removing Punctuations:","metadata":{}},{"cell_type":"code","source":"# Creating a df that is copy of the train set.\ndf = train.copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:34.422023Z","iopub.execute_input":"2022-03-31T11:38:34.422256Z","iopub.status.idle":"2022-03-31T11:38:34.428143Z","shell.execute_reply.started":"2022-03-31T11:38:34.42222Z","shell.execute_reply":"2022-03-31T11:38:34.427432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Punctuations:","metadata":{}},{"cell_type":"code","source":"import string\nstring.punctuation","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:34.431965Z","iopub.execute_input":"2022-03-31T11:38:34.432207Z","iopub.status.idle":"2022-03-31T11:38:34.437147Z","shell.execute_reply.started":"2022-03-31T11:38:34.432177Z","shell.execute_reply":"2022-03-31T11:38:34.436432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"punctuations_list = string.punctuation\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)\n\ndf['text'] = df['text'].apply(lambda x: cleaning_punctuations(x))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:34.438553Z","iopub.execute_input":"2022-03-31T11:38:34.439122Z","iopub.status.idle":"2022-03-31T11:38:34.474479Z","shell.execute_reply.started":"2022-03-31T11:38:34.439087Z","shell.execute_reply":"2022-03-31T11:38:34.473883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test :","metadata":{}},{"cell_type":"code","source":"# Splitting data into Train and Test sets:\nX = df['text']\ny = df['target']\nX_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:34.475624Z","iopub.execute_input":"2022-03-31T11:38:34.475888Z","iopub.status.idle":"2022-03-31T11:38:34.483349Z","shell.execute_reply.started":"2022-03-31T11:38:34.475839Z","shell.execute_reply":"2022-03-31T11:38:34.48263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforming dataset using TF-IDF Vectorizer:","metadata":{}},{"cell_type":"code","source":"# Extracting features using TF-IDF (1,2) - unigrams and bigrams\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))\n\n# Transforming the data using TD-IDF Vectorizer\nX_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:34.484736Z","iopub.execute_input":"2022-03-31T11:38:34.485058Z","iopub.status.idle":"2022-03-31T11:38:35.079546Z","shell.execute_reply.started":"2022-03-31T11:38:34.485026Z","shell.execute_reply":"2022-03-31T11:38:35.078884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{}},{"cell_type":"code","source":"%%time\n\n# implemented GridSearchCV for four models using a loop and a previously created dictionary\n# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n    print(mp['model'])\n    print('\\nTraining the model...')\n    best_model = clf.fit(X_train, y_train)                      # Training the model\n    clf_pred = best_model.predict(X_test)                       # Predicting the results\n    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n    scores.append({                                             # Appending results to 'scores' list\n        'model' : model_name,\n        'best_score' : best_model.score(X_test, y_test),\n        'best_params' : clf.best_params_\n    })\n    print('\\nScore is appended.\\n')\n    \n# Creating data frame with model, best scores and best params:\nres = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\nres","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:38:35.080831Z","iopub.execute_input":"2022-03-31T11:38:35.081111Z","iopub.status.idle":"2022-03-31T11:42:25.70184Z","shell.execute_reply.started":"2022-03-31T11:38:35.081079Z","shell.execute_reply":"2022-03-31T11:42:25.701137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup 2: Models after removing Stop-words:","metadata":{}},{"cell_type":"code","source":"# Creating a df that is copy of the train set.\ndf = train.copy()","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:42:25.703154Z","iopub.execute_input":"2022-03-31T11:42:25.704018Z","iopub.status.idle":"2022-03-31T11:42:25.709151Z","shell.execute_reply.started":"2022-03-31T11:42:25.703971Z","shell.execute_reply":"2022-03-31T11:42:25.708215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Stop-words:","metadata":{}},{"cell_type":"code","source":"sw = stopwords.words('english')\ndf['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:42:25.7106Z","iopub.execute_input":"2022-03-31T11:42:25.711098Z","iopub.status.idle":"2022-03-31T11:42:25.894763Z","shell.execute_reply.started":"2022-03-31T11:42:25.711063Z","shell.execute_reply":"2022-03-31T11:42:25.894154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test :","metadata":{}},{"cell_type":"code","source":"# Splitting data into Train and Test sets:\nX = df['text']\ny = df['target']\nX_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:42:25.895996Z","iopub.execute_input":"2022-03-31T11:42:25.896248Z","iopub.status.idle":"2022-03-31T11:42:25.90411Z","shell.execute_reply.started":"2022-03-31T11:42:25.896215Z","shell.execute_reply":"2022-03-31T11:42:25.903383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforming dataset using TF-IDF Vectorizer:","metadata":{}},{"cell_type":"code","source":"# Extracting features using TF-IDF (1,2) - unigrams and bigrams\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))\n\n# Transforming the data using TD-IDF Vectorizer\nX_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:42:25.905552Z","iopub.execute_input":"2022-03-31T11:42:25.905805Z","iopub.status.idle":"2022-03-31T11:42:26.358791Z","shell.execute_reply.started":"2022-03-31T11:42:25.905771Z","shell.execute_reply":"2022-03-31T11:42:26.358046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{}},{"cell_type":"code","source":"%%time\n\n# implemented GridSearchCV for four models using a loop and a previously created dictionary\n# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n    print(mp['model'])\n    print('\\nTraining the model...')\n    best_model = clf.fit(X_train, y_train)                      # Training the model\n    clf_pred = best_model.predict(X_test)                       # Predicting the results\n    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n    scores.append({                                             # Appending results to 'scores' list\n        'model' : model_name,\n        'best_score' : best_model.score(X_test, y_test),\n        'best_params' : clf.best_params_\n    })\n    print('\\nScore is appended.\\n')\n    \n# Creating data frame with model, best scores and best params:\nres = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\nres","metadata":{"execution":{"iopub.status.busy":"2022-03-31T11:42:26.359933Z","iopub.execute_input":"2022-03-31T11:42:26.360485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup 3: Models after removing numbers:","metadata":{}},{"cell_type":"code","source":"# Creating a df that is copy of the train set.\ndf = train.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing numbers:","metadata":{}},{"cell_type":"code","source":"def cleaning_numbers(text):\n    return re.sub('[0-9]+', '', text)\n\ndf['text'] = df['text'].apply(lambda text: cleaning_numbers(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test :","metadata":{}},{"cell_type":"code","source":"# Splitting data into Train and Test sets:\nX = df['text']\ny = df['target']\nX_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforming dataset using TF-IDF Vectorizer:","metadata":{}},{"cell_type":"code","source":"# Extracting features using TF-IDF (1,2) - unigrams and bigrams\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))\n\n# Transforming the data using TD-IDF Vectorizer\nX_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{}},{"cell_type":"code","source":"%%time\n\n# implemented GridSearchCV for four models using a loop and a previously created dictionary\n# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n    print(mp['model'])\n    print('\\nTraining the model...')\n    best_model = clf.fit(X_train, y_train)                      # Training the model\n    clf_pred = best_model.predict(X_test)                       # Predicting the results\n    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n    scores.append({                                             # Appending results to 'scores' list\n        'model' : model_name,\n        'best_score' : best_model.score(X_test, y_test),\n        'best_params' : clf.best_params_\n    })\n    print('\\nScore is appended.\\n')\n    \n# Creating data frame with model, best scores and best params:\nres = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\nres","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup 4: Models after removing repeating characters:","metadata":{}},{"cell_type":"code","source":"# Creating a df that is copy of the train set.\ndf = train.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing repeating characteres:","metadata":{}},{"cell_type":"code","source":"tokens = (word_tokenize(i) for i in df.text)\ndf['text'] = df['text'].apply(nltk.word_tokenize)\n\npattern = re.compile(r'(.)\\1*')\n\ndef reduce_sequence_word(word):\n    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n\ndef reduce_sequence_tweet(tweet):\n    return [reduce_sequence_word(word) for word in tweet]\n\ndf.text = df.text.apply(lambda tweet: reduce_sequence_tweet(tweet))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test :","metadata":{}},{"cell_type":"code","source":"# Splitting data into Train and Test sets:\nX = df['text'].astype(str)\ny = df['target'].astype(str)\nX_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforming dataset using TF-IDF Vectorizer:","metadata":{}},{"cell_type":"code","source":"# Extracting features using TF-IDF (1,2) - unigrams and bigrams\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))\n\n# Transforming the data using TD-IDF Vectorizer\nX_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{}},{"cell_type":"code","source":"%%time\n\n# implemented GridSearchCV for four models using a loop and a previously created dictionary\n# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n    print(mp['model'])\n    print('\\nTraining the model...')\n    best_model = clf.fit(X_train, y_train)                      # Training the model\n    clf_pred = best_model.predict(X_test)                       # Predicting the results\n    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n    scores.append({                                             # Appending results to 'scores' list\n        'model' : model_name,\n        'best_score' : best_model.score(X_test, y_test),\n        'best_params' : clf.best_params_\n    })\n    print('\\nScore is appended.\\n')\n    \n# Creating data frame with model, best scores and best params:\nres = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\nres","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup 5: Applying Stemming and Lemmatization:","metadata":{}},{"cell_type":"code","source":"# Creating a df that is copy of the train set.\ndf = train.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying Stemming: ","metadata":{}},{"cell_type":"code","source":"# Tokenizing tweets:\ntokens = (word_tokenize(i) for i in df.text)\ndf['text'] = df['text'].apply(nltk.word_tokenize)\n\nstemm = SnowballStemmer('english')\ndf['text'] = df['text'].apply(lambda x: [stemm.stem(y) for y in x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test :","metadata":{}},{"cell_type":"code","source":"# Splitting data into Train and Test sets:\nX = df['text'].astype(str)\ny = df['target'].astype(str)\nX_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforming dataset using TF-IDF Vectorizer:","metadata":{}},{"cell_type":"code","source":"# Extracting features using TF-IDF (1,2) - unigrams and bigrams\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))\n\n# Transforming the data using TD-IDF Vectorizer\nX_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{}},{"cell_type":"code","source":"%%time\n\n# implemented GridSearchCV for four models using a loop and a previously created dictionary\n# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n    print(mp['model'])\n    print('\\nTraining the model...')\n    best_model = clf.fit(X_train, y_train)                      # Training the model\n    clf_pred = best_model.predict(X_test)                       # Predicting the results\n    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n    scores.append({                                             # Appending results to 'scores' list\n        'model' : model_name,\n        'best_score' : best_model.score(X_test, y_test),\n        'best_params' : clf.best_params_\n    })\n    print('\\nScore is appended.\\n')\n    \n# Creating data frame with model, best scores and best params:\nres = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\nres","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup 6: Models after removing all the features:","metadata":{}},{"cell_type":"code","source":"# Creating a df that is copy of the train set.\ndf = train.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Punctuation:","metadata":{}},{"cell_type":"code","source":"import string\nstring.punctuation\n\npunctuations_list = string.punctuation\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)\n\ndf['text'] = df['text'].apply(lambda x: cleaning_punctuations(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Stop-words: ","metadata":{}},{"cell_type":"code","source":"sw = stopwords.words('english')\ndf['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing Numbers:","metadata":{}},{"cell_type":"code","source":"def cleaning_numbers(text):\n    return re.sub('[0-9]+', '', text)\n\ndf['text'] = df['text'].apply(lambda text: cleaning_numbers(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removing repeating characters:","metadata":{}},{"cell_type":"code","source":"tokens = (word_tokenize(i) for i in df.text)\ndf['text'] = df['text'].apply(nltk.word_tokenize)\n\npattern = re.compile(r'(.)\\1*')\n\ndef reduce_sequence_word(word):\n    return ''.join([match.group()[:2] if len(match.group()) > 2 else match.group() for match in pattern.finditer(word)])\n\ndef reduce_sequence_tweet(tweet):\n    return [reduce_sequence_word(word) for word in tweet]\n\ndf.text = df.text.apply(lambda tweet: reduce_sequence_tweet(tweet))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Applying Stemming and Lemmatization:","metadata":{}},{"cell_type":"code","source":"stemm = SnowballStemmer('english')\ndf['text'] = df['text'].apply(lambda x: [stemm.stem(y) for y in x])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test :","metadata":{}},{"cell_type":"code","source":"# Splitting data into Train and Test sets:\nX = df['text'].astype(str)\ny = df['target'].astype(str)\nX_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforming dataset using TF-IDF Vectorizer:","metadata":{}},{"cell_type":"code","source":"# Extracting features using TF-IDF (1,2) - unigrams and bigrams\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))\n\n# Transforming the data using TD-IDF Vectorizer\nX_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{}},{"cell_type":"code","source":"%%time\n\n# implemented GridSearchCV for four models using a loop and a previously created dictionary\n# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n    print(mp['model'])\n    print('\\nTraining the model...')\n    best_model = clf.fit(X_train, y_train)                      # Training the model\n    clf_pred = best_model.predict(X_test)                       # Predicting the results\n    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n    scores.append({                                             # Appending results to 'scores' list\n        'model' : model_name,\n        'best_score' : best_model.score(X_test, y_test),\n        'best_params' : clf.best_params_\n    })\n    print('\\nScore is appended.\\n')\n    \n# Creating data frame with model, best scores and best params:\nres = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\nres","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup 7: Models without removing any setup:","metadata":{}},{"cell_type":"code","source":"# Creating a df that is copy of the train set.\ndf = train.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test :","metadata":{}},{"cell_type":"code","source":"# Splitting data into Train and Test sets:\nX = df['text']\ny = df['target']\nX_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.2, random_state = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforming dataset using TF-IDF Vectorizer:","metadata":{}},{"cell_type":"code","source":"# Extracting features using TF-IDF (1,2) - unigrams and bigrams\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))\n\n# Transforming the data using TD-IDF Vectorizer\nX_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Results:","metadata":{}},{"cell_type":"code","source":"%%time\n\n# implemented GridSearchCV for four models using a loop and a previously created dictionary\n# in the created variable 'scores', results are stored for each model such as: model, best_score and best_params.\n\n\nscores = []\n\nfor model_name, mp in model_params.items():\n    clf = GridSearchCV(mp['model'], mp['params'], cv=5, n_jobs=-1, verbose=1) # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\n    print(mp['model'])\n    print('\\nTraining the model...')\n    best_model = clf.fit(X_train, y_train)                      # Training the model\n    clf_pred = best_model.predict(X_test)                       # Predicting the results\n    print(confusion_matrix(y_test,clf_pred))                    # Printing Confusion Matrix\n    print(metrics.classification_report(y_test, clf_pred))      # Printing Classification Report\n    scores.append({                                             # Appending results to 'scores' list\n        'model' : model_name,\n        'best_score' : best_model.score(X_test, y_test),\n        'best_params' : clf.best_params_\n    })\n    print('\\nScore is appended.\\n')\n    \n# Creating data frame with model, best scores and best params:\nres = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\nres","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Submission file:\nIt can be observed that **Setup-1 and 7** is performing best for SVM model. **Setup 1** will be used. Let's just train this model with 100% training data. This model will be used for predicting test file.","metadata":{}},{"cell_type":"code","source":"# Creating a df that is copy of the train set.\ndf = train.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nstring.punctuation\n\npunctuations_list = string.punctuation\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)\n\ndf['text'] = df['text'].apply(lambda x: cleaning_punctuations(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting data into Train and Test :","metadata":{}},{"cell_type":"code","source":"# Not spliiting, Creating X_train and y_train.\n# Using 100% data for training SVC model to get better training. Because from Step - 7,\n# it can be concluded that SVC model with 'TF-IDF Vectorizer (1,2) - unigrams and bigrams' performs best for this dataset\n\n\nX_train = df['text']\ny_train = df['target']    \nX_test = test['text']   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforming dataset using TF-IDF Vectorizer:","metadata":{}},{"cell_type":"code","source":"# Extracting features using TF-IDF (1,2) - unigrams and bigrams\nvectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))\n\n# Transforming the data using TD-IDF Vectorizer\nX_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVC model:","metadata":{}},{"cell_type":"code","source":"svc = SVC()\nhyperParam = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01], 'kernel': ['rbf','linear','poly','sigmoid']}\n\ngsv = GridSearchCV(svc,hyperParam,cv=5,verbose=1,n_jobs=-1)  # Using Cross Validation of 5 and n_jobs=-1 for fast training by using all the processors\nbest_model = gsv.fit(X_train, y_train)                       # Training model with X_train and y_train\nsvc_pred = best_model.predict(X_test)                        # Predicting the results\n\nprint(\"Best HyperParameter: \", gsv.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission file:","metadata":{}},{"cell_type":"code","source":"print(svc_pred)\nprint(type(svc_pred))\n\nmy_array = svc_pred\nprint(len(my_array))\n\nsubmission = pd.DataFrame(my_array,columns = ['target'])\nsubmission['id'] = test['id']\nsubmission = submission[['id','target']]\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}