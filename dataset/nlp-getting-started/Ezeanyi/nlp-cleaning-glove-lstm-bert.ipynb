{"cells":[{"metadata":{"papermill":{"duration":0.050486,"end_time":"2020-10-28T23:31:43.455893","exception":false,"start_time":"2020-10-28T23:31:43.405407","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Introduction\nThe challenge in this kernel is to build a model to predict which tweets are disaster tweets and which ones are not given a train set of tweets with their labels. The model will be evaluated with the given test set."},{"metadata":{"papermill":{"duration":0.044618,"end_time":"2020-10-28T23:31:43.547006","exception":false,"start_time":"2020-10-28T23:31:43.502388","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 1.0 Data exploration\nLoad and explore data"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:43.647694Z","iopub.status.busy":"2020-10-28T23:31:43.646908Z","iopub.status.idle":"2020-10-28T23:31:45.186571Z","shell.execute_reply":"2020-10-28T23:31:45.185851Z"},"papermill":{"duration":1.593805,"end_time":"2020-10-28T23:31:45.186712","exception":false,"start_time":"2020-10-28T23:31:43.592907","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"#load libraries for data manipulation and visualization\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n# text/file processing libraries\nimport string\nimport re\nimport sys\nfrom nltk.corpus import stopwords\nfrom itertools import chain\n# warnings\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.046327,"end_time":"2020-10-28T23:31:45.280702","exception":false,"start_time":"2020-10-28T23:31:45.234375","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The dataset for this kernel is the kaggle Real or Not? NLP with  Disater Tweets"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:45.390494Z","iopub.status.busy":"2020-10-28T23:31:45.389744Z","iopub.status.idle":"2020-10-28T23:31:45.443491Z","shell.execute_reply":"2020-10-28T23:31:45.444048Z"},"papermill":{"duration":0.117815,"end_time":"2020-10-28T23:31:45.444216","exception":false,"start_time":"2020-10-28T23:31:45.326401","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# load the train and test data sets\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nprint('Number of Training Samples = {}'.format(train_df.shape[0]))\nprint('Number of Test Samples = {}\\n'.format(test_df.shape[0]))\nprint('Training X Shape = {}'.format(train_df.shape))\nprint('Training y Shape = {}\\n'.format(train_df['target'].shape[0]))\nprint('Test X Shape = {}'.format(test_df.shape))\n\nprint('Test y Shape = {}\\n'.format(test_df.shape[0]))\nprint('Index of Train Set:\\n', train_df.columns)\nprint('Index of Test Set:\\n', test_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:45.650734Z","iopub.status.busy":"2020-10-28T23:31:45.650032Z","iopub.status.idle":"2020-10-28T23:31:45.797377Z","shell.execute_reply":"2020-10-28T23:31:45.797926Z"},"papermill":{"duration":0.199571,"end_time":"2020-10-28T23:31:45.798066","exception":false,"start_time":"2020-10-28T23:31:45.598495","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# class distribution of train set\npl = sb.countplot(train_df['target'])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.049022,"end_time":"2020-10-28T23:31:45.902062","exception":false,"start_time":"2020-10-28T23:31:45.85304","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Non disaster tweets represented with 0 are more than disaster tweets represented with 1"},{"metadata":{"papermill":{"duration":0.047267,"end_time":"2020-10-28T23:31:45.997721","exception":false,"start_time":"2020-10-28T23:31:45.950454","status":"completed"},"tags":[]},"cell_type":"markdown","source":"####  Sample data\nExplore sample disaster and non disaster tweet"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:46.10392Z","iopub.status.busy":"2020-10-28T23:31:46.103247Z","iopub.status.idle":"2020-10-28T23:31:46.112893Z","shell.execute_reply":"2020-10-28T23:31:46.1124Z"},"papermill":{"duration":0.066588,"end_time":"2020-10-28T23:31:46.112996","exception":false,"start_time":"2020-10-28T23:31:46.046408","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# display sample train data\ntrain_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:46.217633Z","iopub.status.busy":"2020-10-28T23:31:46.216803Z","iopub.status.idle":"2020-10-28T23:31:46.221733Z","shell.execute_reply":"2020-10-28T23:31:46.222315Z"},"papermill":{"duration":0.060293,"end_time":"2020-10-28T23:31:46.222448","exception":false,"start_time":"2020-10-28T23:31:46.162155","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# sample train disaster tweet\ntrain_df.loc[1241]['text']","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.048442,"end_time":"2020-10-28T23:31:46.320122","exception":false,"start_time":"2020-10-28T23:31:46.27168","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The keywords 'buildings on fire' in the above tweet are in the text. Though, the keywords do not appear in that order in the text."},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:46.425593Z","iopub.status.busy":"2020-10-28T23:31:46.424715Z","iopub.status.idle":"2020-10-28T23:31:46.428368Z","shell.execute_reply":"2020-10-28T23:31:46.428861Z"},"papermill":{"duration":0.05817,"end_time":"2020-10-28T23:31:46.428973","exception":false,"start_time":"2020-10-28T23:31:46.370803","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# sample train non disaster tweet\ntrain_df.loc[2301]['text']","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.048359,"end_time":"2020-10-28T23:31:46.526189","exception":false,"start_time":"2020-10-28T23:31:46.47783","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The keyword 'demolish' in the above tweet may literally mean disaster but reading the text indicates no disaster."},{"metadata":{"papermill":{"duration":0.048197,"end_time":"2020-10-28T23:31:46.623048","exception":false,"start_time":"2020-10-28T23:31:46.574851","status":"completed"},"tags":[]},"cell_type":"markdown","source":"An analysis of the sample disaster and non disaster tweet indicate that the text column is the most important as it contains the  keywords and the context of the text is important in determining a disaster and non disaster tweet."},{"metadata":{"papermill":{"duration":0.050336,"end_time":"2020-10-28T23:31:46.722789","exception":false,"start_time":"2020-10-28T23:31:46.672453","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Mislabelled tweets\nThis idea was adapted from  [disaster nlp: keras bert using tfhub](https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert)"},{"metadata":{"papermill":{"duration":0.050509,"end_time":"2020-10-28T23:31:46.82298","exception":false,"start_time":"2020-10-28T23:31:46.772471","status":"completed"},"tags":[]},"cell_type":"markdown","source":"\"There are **18** unique tweets in training set which are labeled differently in their duplicates. Those tweets are probably labeled by different people and they interpreted the meaning differently because some of them are not very clear. Tweets with two unique `target` values are relabeled since they can affect the training score.\""},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:46.943835Z","iopub.status.busy":"2020-10-28T23:31:46.942927Z","iopub.status.idle":"2020-10-28T23:31:46.981777Z","shell.execute_reply":"2020-10-28T23:31:46.982322Z"},"papermill":{"duration":0.098936,"end_time":"2020-10-28T23:31:46.982485","exception":false,"start_time":"2020-10-28T23:31:46.883549","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_df.groupby(['text']).nunique().sort_values(by='target', ascending=False)[0:18]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:47.094347Z","iopub.status.busy":"2020-10-28T23:31:47.093461Z","iopub.status.idle":"2020-10-28T23:31:47.142614Z","shell.execute_reply":"2020-10-28T23:31:47.141733Z"},"papermill":{"duration":0.108704,"end_time":"2020-10-28T23:31:47.142739","exception":false,"start_time":"2020-10-28T23:31:47.034035","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"df_mislabeled = train_df.groupby(['text']).nunique().sort_values(by='target', ascending=False)\ndf_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\ndf_mislabeled_all = df_mislabeled.index.tolist()\nprint(f'Number of repeated tweets(after preprocessing): {len(df_mislabeled_all)}')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:47.260162Z","iopub.status.busy":"2020-10-28T23:31:47.259308Z","iopub.status.idle":"2020-10-28T23:31:47.307655Z","shell.execute_reply":"2020-10-28T23:31:47.308245Z"},"papermill":{"duration":0.113128,"end_time":"2020-10-28T23:31:47.308379","exception":false,"start_time":"2020-10-28T23:31:47.195251","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"train_df['target_relabeled'] = train_df['target'].copy() \n\ntarget_1_list = [   \n   \n    \"CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97/Georgia Ave Silver Spring\",\n    \".POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https://t.co/rqWuoy1fm4\",\n    \"Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE\",\n    \"RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http://t.co/JlzK2HdeTG\",\n    \"Caution: breathing may be hazardous to your health.\" ]\n    \nfor mislabeled_sample in df_mislabeled_all:\n    if mislabeled_sample in target_1_list:\n        train_df.loc[train_df['text'] == mislabeled_sample, 'target_relabeled'] = 1\n    else:\n        train_df.loc[train_df['text'] == mislabeled_sample, 'target_relabeled'] = 0\n\nfilter_mislabel = (train_df['target'] != train_df['target_relabeled'])\nprint(f'Number of relabeled: {len(train_df[filter_mislabel])}')\ntrain_df[filter_mislabel][:12]  ","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.053762,"end_time":"2020-10-28T23:31:47.416289","exception":false,"start_time":"2020-10-28T23:31:47.362527","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Exploring text\nExplore text column for data cleaning"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:47.527002Z","iopub.status.busy":"2020-10-28T23:31:47.52609Z","iopub.status.idle":"2020-10-28T23:31:47.529662Z","shell.execute_reply":"2020-10-28T23:31:47.530142Z"},"papermill":{"duration":0.061676,"end_time":"2020-10-28T23:31:47.53026","exception":false,"start_time":"2020-10-28T23:31:47.468584","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_df['text'].sample(20).tolist()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.051853,"end_time":"2020-10-28T23:31:47.633642","exception":false,"start_time":"2020-10-28T23:31:47.581789","status":"completed"},"tags":[]},"cell_type":"markdown","source":"There is need to remove or filter out characters that may not be relevant in predicting disaster or non disaster tweets such as: punctuations, contractions, stop words, short words, urls, html tags, emojis, mentions, hashtags, and bad spellings "},{"metadata":{"papermill":{"duration":0.052905,"end_time":"2020-10-28T23:31:47.73938","exception":false,"start_time":"2020-10-28T23:31:47.686475","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 2.0 Data cleaning\nCleaning text means converting it to a list of words or tokens, different cleaning task will be performed on the dataset."},{"metadata":{"papermill":{"duration":0.051679,"end_time":"2020-10-28T23:31:47.842362","exception":false,"start_time":"2020-10-28T23:31:47.790683","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Remove html links and entity references"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:47.977214Z","iopub.status.busy":"2020-10-28T23:31:47.972069Z","iopub.status.idle":"2020-10-28T23:31:48.075914Z","shell.execute_reply":"2020-10-28T23:31:48.075408Z"},"papermill":{"duration":0.1765,"end_time":"2020-10-28T23:31:48.076042","exception":false,"start_time":"2020-10-28T23:31:47.899542","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def html_references(tweets):\n    texts = tweets\n    # remove url - references to websites\n    url_remove  = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n    texts  = re.sub(url_remove, '', texts)\n    # remove common html entity references in utf-8 as '&lt;', '&gt;', '&amp;'\n    entities_remove = r'&amp;|&gt;|&lt'\n    texts = re.sub(entities_remove, \"\", texts)\n    # split into words by white space\n    words = texts.split()\n    #convert to lower case\n    words = [word.lower() for word in words]\n    return \" \".join(words)\ntrain_df['clean_text'] = train_df['text'].apply(lambda x : html_references(x))\ntest_df['clean_text'] = test_df['text'].apply(lambda x : html_references(x))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.051717,"end_time":"2020-10-28T23:31:48.180346","exception":false,"start_time":"2020-10-28T23:31:48.128629","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Remove apostrophes/contractions"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:48.323888Z","iopub.status.busy":"2020-10-28T23:31:48.313627Z","iopub.status.idle":"2020-10-28T23:31:48.717947Z","shell.execute_reply":"2020-10-28T23:31:48.717335Z"},"papermill":{"duration":0.484519,"end_time":"2020-10-28T23:31:48.71807","exception":false,"start_time":"2020-10-28T23:31:48.233551","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def decontraction(tweet):\n    # specific\n    tweet = re.sub(r\"won\\'t\", \" will not\", tweet)\n    tweet = re.sub(r\"won\\'t've\", \" will not have\", tweet)\n    tweet = re.sub(r\"can\\'t\", \" can not\", tweet)\n    tweet = re.sub(r\"don\\'t\", \" do not\", tweet)\n    \n    tweet = re.sub(r\"can\\'t've\", \" can not have\", tweet)\n    tweet = re.sub(r\"ma\\'am\", \" madam\", tweet)\n    tweet = re.sub(r\"let\\'s\", \" let us\", tweet)\n    tweet = re.sub(r\"ain\\'t\", \" am not\", tweet)\n    tweet = re.sub(r\"shan\\'t\", \" shall not\", tweet)\n    tweet = re.sub(r\"sha\\n't\", \" shall not\", tweet)\n    tweet = re.sub(r\"o\\'clock\", \" of the clock\", tweet)\n    tweet = re.sub(r\"y\\'all\", \" you all\", tweet)\n    # general\n    tweet = re.sub(r\"n\\'t\", \" not\", tweet)\n    tweet = re.sub(r\"n\\'t've\", \" not have\", tweet)\n    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n    tweet = re.sub(r\"\\'s\", \" is\", tweet)\n    tweet = re.sub(r\"\\'d\", \" would\", tweet)\n    tweet = re.sub(r\"\\'d've\", \" would have\", tweet)\n    tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n    tweet = re.sub(r\"\\'ll've\", \" will have\", tweet)\n    tweet = re.sub(r\"\\'t\", \" not\", tweet)\n    tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n    tweet = re.sub(r\"\\'m\", \" am\", tweet)\n    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n    return tweet \ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x : decontraction(x))\ntest_df['clean_text'] = test_df['clean_text'].apply(lambda x : decontraction(x))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.052442,"end_time":"2020-10-28T23:31:48.823294","exception":false,"start_time":"2020-10-28T23:31:48.770852","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Remove punctuations and unprintable characters\nRemove punctuations will remove the characters specified by string.punctuation while the inverse of string.printable will remove non ascii characters."},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:48.977736Z","iopub.status.busy":"2020-10-28T23:31:48.976458Z","iopub.status.idle":"2020-10-28T23:31:48.982251Z","shell.execute_reply":"2020-10-28T23:31:48.982988Z"},"papermill":{"duration":0.097141,"end_time":"2020-10-28T23:31:48.983206","exception":false,"start_time":"2020-10-28T23:31:48.886065","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# print puntuation characters\nstring.punctuation","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:49.160513Z","iopub.status.busy":"2020-10-28T23:31:49.158567Z","iopub.status.idle":"2020-10-28T23:31:49.163026Z","shell.execute_reply":"2020-10-28T23:31:49.159528Z"},"papermill":{"duration":0.094133,"end_time":"2020-10-28T23:31:49.163181","exception":false,"start_time":"2020-10-28T23:31:49.069048","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# print printable characters\nstring.printable","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.108725,"end_time":"2020-10-28T23:31:49.355998","exception":false,"start_time":"2020-10-28T23:31:49.247273","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The puntuation characters will be removed with non english and unicode characters not in string.printable"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:49.543213Z","iopub.status.busy":"2020-10-28T23:31:49.542404Z","iopub.status.idle":"2020-10-28T23:31:50.069832Z","shell.execute_reply":"2020-10-28T23:31:50.07051Z"},"papermill":{"duration":0.631205,"end_time":"2020-10-28T23:31:50.070705","exception":false,"start_time":"2020-10-28T23:31:49.4395","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"def filter_punctuations_etc(tweets):\n    words = tweets.split()\n    # prepare regex for char filtering\n    re_punc = re.compile( '[%s]' % re.escape(string.punctuation))\n    # remove punctuation from each word\n    words = [re_punc.sub('', w) for w in words]\n    # filter out non-printable characters\n    re_print = re.compile( '[^%s]' % re.escape(string.printable))\n    words = [re_print.sub(' ', w) for w in words]\n    return \" \".join(words)\ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x : filter_punctuations_etc(x))\ntest_df['clean_text'] = test_df['clean_text'].apply(lambda x : filter_punctuations_etc(x))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.065482,"end_time":"2020-10-28T23:31:50.218601","exception":false,"start_time":"2020-10-28T23:31:50.153119","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Separate alphanumeric characters"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:50.364742Z","iopub.status.busy":"2020-10-28T23:31:50.344219Z","iopub.status.idle":"2020-10-28T23:31:50.507663Z","shell.execute_reply":"2020-10-28T23:31:50.508179Z"},"papermill":{"duration":0.237234,"end_time":"2020-10-28T23:31:50.508321","exception":false,"start_time":"2020-10-28T23:31:50.271087","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def separate_alphanumeric(tweets):\n    words = tweets\n    # separate alphanumeric\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x : separate_alphanumeric(x))\ntest_df['clean_text'] = test_df['clean_text'].apply(lambda x : separate_alphanumeric(x))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.055029,"end_time":"2020-10-28T23:31:50.617077","exception":false,"start_time":"2020-10-28T23:31:50.562048","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Change repetitive characters\nChange repetitive characters e.g.goooooooaaaal to gooaal, so the spell checker can try correcting it. An english word cannot have more than 2 consecutive same letter."},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:50.823026Z","iopub.status.busy":"2020-10-28T23:31:50.792341Z","iopub.status.idle":"2020-10-28T23:31:50.958598Z","shell.execute_reply":"2020-10-28T23:31:50.958043Z"},"papermill":{"duration":0.251296,"end_time":"2020-10-28T23:31:50.958708","exception":false,"start_time":"2020-10-28T23:31:50.707412","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def cont_rep_char(text):\n    tchr = text.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] # take max of 2 consecutive letters\ndef unique_char(rep, tweets):\n    substitute = re.sub(r'(\\w)\\1+', rep, tweets)\n    return substitute\ntrain_df['clean_text'] = (train_df['clean_text'].astype('str').apply(lambda x : unique_char(cont_rep_char, x)))\ntest_df['clean_text'] = (test_df['clean_text'].astype('str').apply(lambda x : unique_char(cont_rep_char, x)))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.053734,"end_time":"2020-10-28T23:31:51.066682","exception":false,"start_time":"2020-10-28T23:31:51.012948","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Spell checking\nCheck spellings and make corrections where possible, this a computational expensive exercise."},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:31:51.179638Z","iopub.status.busy":"2020-10-28T23:31:51.178798Z","iopub.status.idle":"2020-10-28T23:32:02.203798Z","shell.execute_reply":"2020-10-28T23:32:02.203181Z"},"papermill":{"duration":11.083056,"end_time":"2020-10-28T23:32:02.203946","exception":false,"start_time":"2020-10-28T23:31:51.12089","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!pip install pyspellchecker","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:32:02.348677Z","iopub.status.busy":"2020-10-28T23:32:02.347701Z","iopub.status.idle":"2020-10-28T23:32:02.516016Z","shell.execute_reply":"2020-10-28T23:32:02.515354Z"},"papermill":{"duration":0.245804,"end_time":"2020-10-28T23:32:02.516168","exception":false,"start_time":"2020-10-28T23:32:02.270364","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n#train_df['clean_text'] = train_df['clean_text'].apply(lambda x : correct_spellings(x))\n#test_df['clean_text'] = test_df['clean_text'].apply(lambda x : correct_spellings(x))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.06196,"end_time":"2020-10-28T23:32:02.649004","exception":false,"start_time":"2020-10-28T23:32:02.587044","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Split attached words\nSplit attached words especially common with twitter hashtags like \"caraccidentlawyer\" into \"car\", \"accident\", and \"lawyer\". Some desirable words may be split, but the gain may be more than the loss."},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:32:02.783312Z","iopub.status.busy":"2020-10-28T23:32:02.782348Z","iopub.status.idle":"2020-10-28T23:32:14.334679Z","shell.execute_reply":"2020-10-28T23:32:14.33373Z"},"papermill":{"duration":11.622028,"end_time":"2020-10-28T23:32:14.334801","exception":false,"start_time":"2020-10-28T23:32:02.712773","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"!pip install wordninja","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:32:14.480234Z","iopub.status.busy":"2020-10-28T23:32:14.479327Z","iopub.status.idle":"2020-10-28T23:32:21.197542Z","shell.execute_reply":"2020-10-28T23:32:21.1969Z"},"papermill":{"duration":6.793229,"end_time":"2020-10-28T23:32:21.197672","exception":false,"start_time":"2020-10-28T23:32:14.404443","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import wordninja # !pip install wordninja\ndef split_attached_words(tweet):\n    words = wordninja.split(tweet)\n    return\" \".join(words)\ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x : split_attached_words(x))\ntest_df['clean_text'] = test_df['clean_text'].apply(lambda x : split_attached_words(x))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.069631,"end_time":"2020-10-28T23:32:21.337533","exception":false,"start_time":"2020-10-28T23:32:21.267902","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Remove stopwords and short words\nStopwords are common words that may not add to keywords. Stopwords and single letter words will be removed to reduce vocabularity and sparsity  of a bag of words model."},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:32:21.488148Z","iopub.status.busy":"2020-10-28T23:32:21.487248Z","iopub.status.idle":"2020-10-28T23:32:23.922604Z","shell.execute_reply":"2020-10-28T23:32:23.921585Z"},"papermill":{"duration":2.515387,"end_time":"2020-10-28T23:32:23.92273","exception":false,"start_time":"2020-10-28T23:32:21.407343","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def stopwords_shortwords(tweet):\n    # filter out stop words\n    words = tweet.split()\n    stop_words = set(stopwords.words( 'english' ))\n    words = [w for w in words if not w in stop_words]\n    # filter out short tokens\n    for word in words:\n        if word.isalpha():\n            words = [word for word in words if len(word) > 1 ]\n        else:\n            words = [word for word in words]\n    return\" \".join(words)\ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x : stopwords_shortwords(x))\ntest_df['clean_text'] = test_df['clean_text'].apply(lambda x : stopwords_shortwords(x))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.070285,"end_time":"2020-10-28T23:32:24.063908","exception":false,"start_time":"2020-10-28T23:32:23.993623","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## 3.0 GloVe Sentiment Analysis\nPrepare data for a word embedding sentiment analysis model. A word embedding is a context based dense vector representation of texts. There are 2 approaches of using word embeddings, the 1st approach is learn word embedding for a specific task or to be reused in another project,  the 2nd approach is the use of pretrained word embeddings like Word2Vec, GloVe, BERT etc."},{"metadata":{"papermill":{"duration":0.069382,"end_time":"2020-10-28T23:32:24.203991","exception":false,"start_time":"2020-10-28T23:32:24.134609","status":"completed"},"tags":[]},"cell_type":"markdown","source":"#### Pre-processing\nPrepare data for processing"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:32:24.35082Z","iopub.status.busy":"2020-10-28T23:32:24.349654Z","iopub.status.idle":"2020-10-28T23:32:24.356271Z","shell.execute_reply":"2020-10-28T23:32:24.355709Z"},"papermill":{"duration":0.082989,"end_time":"2020-10-28T23:32:24.356395","exception":false,"start_time":"2020-10-28T23:32:24.273406","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# split train set into train/validate \ntrain_df2, validate_df = train_test_split(train_df, test_size=0.075, random_state=0)\ntrain_df2 = train_df2.reset_index(drop=True)\nvalidate_df = validate_df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.06728,"end_time":"2020-10-28T23:32:24.493256","exception":false,"start_time":"2020-10-28T23:32:24.425976","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Separating texts and targets for modeling"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:32:24.638751Z","iopub.status.busy":"2020-10-28T23:32:24.637645Z","iopub.status.idle":"2020-10-28T23:32:24.643255Z","shell.execute_reply":"2020-10-28T23:32:24.642716Z"},"papermill":{"duration":0.081221,"end_time":"2020-10-28T23:32:24.643361","exception":false,"start_time":"2020-10-28T23:32:24.56214","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# train and test sets\nall_df=pd.concat([train_df,test_df])\nX_all = all_df['clean_text']\n# training set\nX_train = train_df2['clean_text']\ny_train = train_df2['target_relabeled'].astype(int)\n# validation set\nX_validate= validate_df['clean_text']\ny_validate = validate_df['target_relabeled'].astype(int)\n# test set\nX_test = test_df['clean_text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Tokenization"},{"metadata":{},"cell_type":"markdown","source":"Tokenizing the text"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n# create a tokenizer for encoding texts as digits\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n# create the tokenizer - mapping data to integer values\ntokenizer = create_tokenizer(X_all)\nword_index=tokenizer.word_index\nmax_words = len(word_index) + 1\nprint( 'unique words are : %d' % max_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tweet with maximum length\nmax_length = max([len(s.split()) for s in X_all])\nprint( ' Maximum length: %d ' % max_length)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform preprocessed text into padded sequences of word ids to get a feature matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# integer encode and pad tweets\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ndef encode_data(tokenizer, max_length, data):\n    # integer encode\n    encoded = tokenizer.texts_to_sequences(data)\n    # pad sequences\n    padded = pad_sequences(encoded, maxlen=max_length, padding= 'post' )\n    return padded\nXtrain = encode_data(tokenizer, max_length, X_train)\nXvalidate = encode_data(tokenizer, max_length, X_validate)\nXtest = encode_data(tokenizer, max_length, X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.heatmap(Xtrain==0, vmin=0, cbar=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GloVe Embedding\nCreate an embedding matrix with GloVe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# parsing the GloVe word-embeddings file\nimport os\nglove_dir = '../input/glove6b'\nembeddings_index = {}\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preparing the GloVe word-embeddings matrix\nembedding_dim = 100\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.074515,"end_time":"2020-10-28T23:32:32.318488","exception":false,"start_time":"2020-10-28T23:32:32.243973","status":"completed"},"tags":[]},"cell_type":"markdown","source":"####  GloVe Embedding and LSTM"},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:32:32.564814Z","iopub.status.busy":"2020-10-28T23:32:32.563757Z","iopub.status.idle":"2020-10-28T23:32:32.572907Z","shell.execute_reply":"2020-10-28T23:32:32.574317Z"},"papermill":{"duration":0.136775,"end_time":"2020-10-28T23:32:32.574514","exception":false,"start_time":"2020-10-28T23:32:32.437739","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding\nimport  tensorflow.keras.optimizers as optimizers\nfrom tensorflow.keras.layers import LSTM, Bidirectional\nmodel = Sequential()\nmodel.add(Embedding(max_words, 100, input_length=max_length))\n# lstm layer\nmodel.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))   \nmodel.add(Bidirectional(LSTM(64,  dropout=0.2, recurrent_dropout=0.2,))) \n# densely connected classifier\nmodel.add(Dense(64, activation= 'relu' ))\nmodel.add(Dense(1, activation='sigmoid'))\n# summarize\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load pretrained word embeddings into the Embedding layer\nmodel.layers[0].set_weights([embedding_matrix])\nmodel.layers[0].trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compile\nmodel.compile(loss= 'binary_crossentropy',  optimizer=optimizers.Adam(lr=.0001), metrics=[ 'accuracy' ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\ncallbacks = [\n    EarlyStopping(patience=3, verbose=1),\n    ReduceLROnPlateau(factor=0.25, patience=2, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('model_lstm.h5', verbose=1, save_best_only=True, save_weights_only=True)\n]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:33:48.880985Z","iopub.status.busy":"2020-10-28T23:33:48.880073Z","iopub.status.idle":"2020-10-28T23:48:19.092034Z","shell.execute_reply":"2020-10-28T23:48:19.092809Z"},"papermill":{"duration":870.702636,"end_time":"2020-10-28T23:48:19.093003","exception":false,"start_time":"2020-10-28T23:33:48.390367","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# fit network\nmodel.fit(Xtrain, y_train, epochs=10, callbacks=callbacks, validation_data=(Xvalidate,y_validate))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-10-28T23:48:21.36639Z","iopub.status.busy":"2020-10-28T23:48:21.365459Z","iopub.status.idle":"2020-10-28T23:48:24.736248Z","shell.execute_reply":"2020-10-28T23:48:24.735314Z"},"papermill":{"duration":4.437142,"end_time":"2020-10-28T23:48:24.736433","exception":false,"start_time":"2020-10-28T23:48:20.299291","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"sample_submission=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\ny_pre=model.predict(Xtest)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_submission['id'].values.tolist(),'target':y_pre})\nsub.to_csv('tweet5a.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.0 BERT Sentiment Analysis\nBERT(Bidirectional Encoded Representation from Transformers) is a language model that can recognize context and semantics in a sentence. It takes input text transformed into 3 vectors ids, masks and segments. It can be used in a variety of NLP including sentiment analysis and is known to perform well with minimal or no text preprocessing. This is because the tokenizer needs to capture the context of each sentence which can be lost in a text cleaning exercise. This kernel will process text for BERT without text cleaning."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will use the official tokenization script created by the Google team\n!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tokenization\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nimport  tensorflow.keras.optimizers as optimizers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"A pretrained BERT model can be used where the BERT layers are frozen during training preserving the model parameters or the prefered method where the BERT  layers are finetuned and trained on the new data . Running BERT the first time can be a hassle, gained insight on implementation from [disaster nlp: keras bert using tfhub](https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# create BERT embedding layer\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create BERT tokenizer\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define function to encode input as 3 matrices of tokens or ids, masks and segments"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoding input data\ntrain_input = bert_encode(train_df.text.values, tokenizer, max_len=128)\ntest_input = bert_encode(test_df.text.values, tokenizer, max_len=128)\ntrain_labels = train_df.target_relabeled.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define create model function\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n   \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create model\nmodel = build_model(bert_layer, max_len=128)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\ncallbacks2 = [\n    ModelCheckpoint('model_bert.h5', monitor='val_loss', save_best_only=True)\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_input, train_labels, validation_split=0.075, epochs=3, \n          callbacks=callbacks2, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\ny_pre2=model.predict(test_input)\ny_pre2=np.round(y_pre2).astype(int).reshape(3263)\nsub2=pd.DataFrame({'id':sample_submission['id'].values.tolist(),'target':y_pre2})\nsub2.to_csv('tweet5b.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":1.073478,"end_time":"2020-10-28T23:48:26.918083","exception":false,"start_time":"2020-10-28T23:48:25.844605","status":"completed"},"tags":[]},"cell_type":"markdown","source":"## If you like this kernel, please upvote, corrections are welcome."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}