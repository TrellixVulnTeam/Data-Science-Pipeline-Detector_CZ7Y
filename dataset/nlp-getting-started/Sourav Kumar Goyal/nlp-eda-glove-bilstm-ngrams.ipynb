{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Problem Statement","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We have to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t.","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom wordcloud import WordCloud, STOPWORDS\nimport missingno as msno\nimport nltk\nimport re\nimport string\nfrom keras.preprocessing.sequence import pad_sequences\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(r'/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv(r'/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking the missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Percentage of Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Percentage of keywords missing in train data:\", (len(train[train['keyword'].isnull()==True])/train.shape[0])*100,\"%\")\nprint(\"Percentage of keywords missing in test data:\", (len(test[test['keyword'].isnull()==True])/test.shape[0])*100,\"%\")\nprint(\"Percentage of location missing in train data:\", (len(train[train['location'].isnull()==True])/train.shape[0])*100,\"%\")\nprint(\"Percentage of location missing in test data:\", (len(test[test['location'].isnull()==True])/test.shape[0])*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As percentage of missing values for both keyword and location are equal for train and test data. So, it may be concluded that both are taken from the same sample","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Visualizing Most Repeated Words using WordCloud","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)\ndef word_cloud(data, title=None):\n    cloud = WordCloud(background_color = 'black',\n                     stopwords = stopwords,\n                     max_words = 200,\n                     max_font_size = 40,\n                     scale = 3).generate(str(data))\n    fig = plt.figure(figsize=(15,15))\n    plt.axis('off')\n    if title:\n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.25)\n        plt.imshow(cloud)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_cloud(train['text'],'Most Repeated words in train data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_cloud(test['text'],'Most Repeated words in test data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_cloud(train[train['target']==1]['text'],'Most Repeated words in real disaster tweets in train data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_cloud(train[train['target']==0]['text'],'Most Repeated words in fake disaster tweets in train data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Counts for each categories of tweet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(7,6))\nsns.countplot(train['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like a Balanced Dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Number of characters in a tweet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Non-disaster tweets')\nfig.suptitle('Characters in tweets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Number of words in a tweet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_words = train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_words, color='red')\nax1.set_title('disaster tweets')\ntweet_words = train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_words, color='green')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Words in a tweet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Average Word Length in a tweet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\ntweet_words = train[train['target']==1]['text'].str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(tweet_words.map(lambda x: np.mean(x)), color='red')\nplt.title('disaster tweets')\nplt.subplot(1,2,2)\ntweet_words = train[train['target']==0]['text'].str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(tweet_words.map(lambda x: np.mean(x)), color='green')\nplt.title('Non disaster tweets')\nplt.suptitle('Average word length in each tweet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Removing emojis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: remove_emoji(x))\ntest['text'] = test['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemma = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(data):\n    # Removing urls\n    data = re.sub(r'https?://\\S+|www\\.\\S+',' ', data)\n    # Remove Punctutaions\n    t = [w for w in data if w not in string.punctuation]\n    data = ''.join(t)\n    # Remove Stopwords\n    t = [w for w in data.split() if w not in stopwords]\n    data = ' '.join(t)\n    # Remove numbers from text'\n    data = re.sub(r'\\d+',' ', data)\n    # Lowercasing the data\n    data = data.lower()\n    # Removing everthing other than alphabets\n    data = re.sub('[^a-zA-Z]',' ', data)\n    # Remove html tags\n    data = re.sub(r'<.*?>',' ', data)\n    # Removing whitespace characters\n    data = re.sub(r'\\s',' ',data)\n    data = re.sub(r' +',' ',data)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: clean_data(x))\ntest['text'] = test['text'].apply(lambda x: clean_data(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train['text'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_tweets = train[train['target']==1]\nnon_disaster_tweets = train[train['target']==0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## N-gram exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_words(corpus, ngram_range = (1,1), n= None):\n    vec = CountVectorizer(ngram_range = ngram_range, stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0)\n    word_freq = [(word, sum_words[0,idx]) for word,idx in vec.vocabulary_.items()]\n    word_freq = sorted(word_freq, key = lambda x: x[1], reverse = True)\n    return word_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of top unigrams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_unigram = get_top_n_words(disaster_tweets['text'],(1,1),40)\npos_unigram = get_top_n_words(non_disaster_tweets['text'],(1,1),40)\n\ndf1 = pd.DataFrame(pos_unigram, columns = ['word','count'])\ndf2 = pd.DataFrame(neg_unigram, columns = ['word','count'])\n\nplt.tight_layout()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(14,11))\nsns.barplot(x = 'count' , y = 'word', data = df1, orient = 'h',ax = ax1)\nax1.set_title('Most repititve words in non-disaster tweets')\nax1.spines[\"right\"].set_visible(False)\nax1.spines[\"top\"].set_visible(False)\nax1.grid(False)\nsns.barplot(x = 'count' , y = 'word', data = df2, orient = 'h',ax = ax2)\nax2.set_title('Most repititve words in disaster tweets')\nax2.spines[\"right\"].set_visible(False)\nax2.spines[\"top\"].set_visible(False)\nax2.grid(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of top bigrams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_unigram = get_top_n_words(disaster_tweets['text'],(2,2),40)\npos_unigram = get_top_n_words(non_disaster_tweets['text'],(2,2),40)\n\ndf1 = pd.DataFrame(pos_unigram, columns = ['word','count'])\ndf2 = pd.DataFrame(neg_unigram, columns = ['word','count'])\n\nplt.tight_layout()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(14,11))\nsns.barplot(x = 'count' , y = 'word', data = df1, orient = 'h',ax = ax1)\nax1.set_title('Most repititve words in non-disaster tweets')\nax1.spines[\"right\"].set_visible(False)\nax1.spines[\"top\"].set_visible(False)\nax1.grid(False)\nsns.barplot(x = 'count' , y = 'word', data = df2, orient = 'h',ax = ax2)\nax2.set_title('Most repititve words in disaster tweets')\nax2.spines[\"right\"].set_visible(False)\nax2.spines[\"top\"].set_visible(False)\nax2.grid(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Distribution of top trigrams","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_unigram = get_top_n_words(disaster_tweets['text'],(3,3),40)\npos_unigram = get_top_n_words(non_disaster_tweets['text'],(3,3),40)\n\ndf1 = pd.DataFrame(pos_unigram, columns = ['word','count'])\ndf2 = pd.DataFrame(neg_unigram, columns = ['word','count'])\n\nplt.tight_layout()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(14,11))\nsns.barplot(x = 'count' , y = 'word', data = df1, orient = 'h',ax = ax1)\nax1.set_title('Most repititve words in non-disaster tweets')\nax1.spines[\"right\"].set_visible(False)\nax1.spines[\"top\"].set_visible(False)\nax1.grid(False)\nsns.barplot(x = 'count' , y = 'word', data = df2, orient = 'h',ax = ax2)\nax2.set_title('Most repititve words in disaster tweets')\nax2.spines[\"right\"].set_visible(False)\nax2.spines[\"top\"].set_visible(False)\nax2.grid(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Number of words in a tweet after data cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_words = train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_words, color='red')\nax1.set_title('disaster tweets')\ntweet_words = train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_words, color='green')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Words in a tweet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Taking out the text and labels out of train and test dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = []\ntrain_labels = []\nfor i in range(len(train['text'])):\n    train_text.append(str(train.iloc[i,3]))\n    train_labels.append(train.iloc[i,4])\ntest_text = []\nfor i in range(len(test['text'])):\n    test_text.append(str(test.iloc[i,3]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initializing parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 84\nmax_length = 20\nvocab_size = 15000\npadding_type = 'post'\ntrunc_type = 'post'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenizing the text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(oov_token='<oov>', num_words = vocab_size)\ntokenizer.fit_on_texts(train_text)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(train_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(word_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Padding text to make it of same length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"padded_train = pad_sequences(sequences, maxlen = max_length , padding = padding_type, truncating = trunc_type)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying padding to test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tokenizer.texts_to_sequences(test['text'])\npadded_test = pad_sequences(test_sequences, maxlen = max_length , padding = padding_type, truncating = trunc_type)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make training labels as numpy array as it is expected for the training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.array(train_labels)\ny.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting training data into train and validation sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val=train_test_split(padded_train,y,test_size=0.05,random_state=12)\nprint(\"Shape of X_train: \" + str(X_train.shape))\nprint(\"Shape of y_train: \" + str(len(y_train)))\nprint(\"Shape of X_val: \" + str(X_val.shape))\nprint(\"Shape of y_val: \" + str(len(y_val)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using Pretrained Word Embeddings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadEmbeddingMatrix(emb_file):\n    EMBEDDING_FILE = emb_file\n    embed_size = 200\n    embedding_index = dict()\n    f = open(EMBEDDING_FILE)\n    for line in f:\n        value = line.split() # Split line tnto indexed Array\n        word = value[0] # First index is word\n        coef = np.asarray(value[1:],dtype = 'float32') # Rest all are word vectors\n        embedding_index[word] = coef\n    f.close()\n    print('Loaded {} word vectors.'.format(len(embedding_index)))\n    \n    del(embedding_index['-0.29736'])\n    all_emb = np.stack(list(embedding_index.values()))\n    emb_mean, emb_std = all_emb.mean(), all_emb.std()\n    \n    nb_words = len(tokenizer.word_index) # Number of words in our dataset\n    \n    # We get the mean and standard deviation of embedding weights so that we could maintain the same statistics for random generated weights \n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    # We will fill the embedding matrix with the words common in both \n    embed_count = 0\n    \n    for word,i in tokenizer.word_index.items():\n        i-=1\n        # We will see if the word in glove's dictionary, if yes, get corresponding weights\n        embedding_vector = embedding_index.get(word)\n        # store this in embedding matrix which will be used for training later\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            embed_count += 1\n        \n    print('Total embedded', embed_count, 'common words')\n    \n    del(embedding_index)\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix = loadEmbeddingMatrix('/kaggle/input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initializing the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(len(word_index), embedding_matrix.shape[1], input_length = max_length, weights = [embedding_matrix],\n                             trainable = False),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences = True, dropout = 0.2, recurrent_dropout = 0.2)),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(units=8, activation=\"relu\"),\n    tf.keras.layers.Dense(units=1, activation=\"sigmoid\")    \n])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initializing the optimizer and metric ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss = 'binary_crossentropy', optimizer = tf.keras.optimizers.RMSprop(lr=4*0.0001), metrics = ['acc'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initializing Callback","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.3, verbose=1,\n                              patience=2, min_lr=0.00000001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X_train, y_train, epochs = 75, validation_data = (X_val, y_val),callbacks=[reduce_lr], verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing Training and validation sets loss and accuracy score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,7))\nax1 = plt.subplot(1,2,1)\nax1.plot(history.history['loss'], color='b', label='Training Loss') \nax1.plot(history.history['val_loss'], color='r', label = 'Validation Loss',axes=ax1)\nlegend = ax1.legend(loc='best', shadow=True)\nax2 = plt.subplot(1,2,2)\nax2.plot(history.history['acc'], color='b', label='Training Accuracy') \nax2.plot(history.history['val_acc'], color='r', label = 'Validation Accuracy')\nlegend = ax2.legend(loc='best', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predicting test data to output the tweet labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(padded_test)\nmodel_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nmodel_submission['target'] = np.round(y_pred).astype('int')\nmodel_submission.to_csv('model_submission_new.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Please UPVOTE if you find this Kernal Informative !!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}