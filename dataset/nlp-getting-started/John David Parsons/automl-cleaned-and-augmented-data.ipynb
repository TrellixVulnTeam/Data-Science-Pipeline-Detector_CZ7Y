{"cells":[{"metadata":{},"cell_type":"markdown","source":"My methodology is as follows:\n 1. Clean the text using my [Tweet Cleaner](https://www.kaggle.com/jdparsons/tweet-cleaner) notebook\n 2. Send the clean text to GPT-2 using my [GPT-2: fake real disasters](https://www.kaggle.com/jdparsons/gpt-2-fake-real-disasters-data-augmentation) notebook. This generates similar tweets with the same label, which I used to double the size of the training data.\n  * The original training data has 7612 rows, while my augmented version has 14612 rows. My hypothesis is that GPT-2 adds useful signal to the training data that AutoML can learn.\n 3. The current notebook is a fork of the official [AutoML Getting Started Notebook](https://www.kaggle.com/yufengg/automl-getting-started-notebook) submitted by Google/@yufengg. Here, I replace the original training data with my GPT-2 augmented version, and replace the test data with my cleaned version. A previous run with the original data took around 4 hours to complete, and cost $26 of GCP usage. I will post a comment at the bottom with the run time and cost when using the augmented data.\n\nCheck this notebook's score to see if AutoML can beat my previous best score of 0.82413 from the notebook [USE + LGB + Grid Search + KFold CV](https://www.kaggle.com/jdparsons/use-lgb-grid-search-kfold-cv)!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nfrom datetime import datetime\n\nfrom sklearn.model_selection import train_test_split\n\nfrom google.cloud import storage\nfrom google.cloud import automl_v1beta1 as automl\n\nfrom automlwrapper import AutoMLWrapper","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# https://cloud.google.com/natural-language/automl/docs/quickstart\n\n# Set your own values for these. bucket_name should be the project_id + '-lcm'.\nPROJECT_ID = 'kaggle-real-or-not'\nbucket_name = PROJECT_ID + '-lcm'\n\nregion = 'us-central1' # Region must be us-central1\ndataset_display_name = 'kaggle_tweets_aug'\nmodel_display_name = 'kaggle_starter_model_aug'\n\nstorage_client = storage.Client(project=PROJECT_ID)\nclient = automl.AutoMlClient()\n\nprint('successfully connected to GCP AutoML')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nlp_train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n#nlp_test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n\n# this is the only change as compared to the original AutoML Getting Started notebook\nnlp_train_df = pd.read_csv('/kaggle/input/offline-download-of-gpt2-augmented/train_df_combined.csv')\nnlp_test_df = pd.read_csv('/kaggle/input/tweet-cleaner/test_df_clean.csv')\n\ndef callback(operation_future):\n    result = operation_future.result()\n    \nprint('loaded data')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp_test_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https://cloud.google.com/storage/docs/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}'.format(\n        source_file_name,\n        'gs://' + bucket_name + '/' + destination_blob_name))\n    \ndef download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bucket = storage.Bucket(storage_client, name=bucket_name)\nif not bucket.exists():\n    bucket.create(location=region)\n\nprint('GCP bucket created')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select the text body and the target value, for sending to AutoML NL\nnlp_train_df[['text','target']].to_csv('train.csv', index=False, header=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('starting data upload')\n\ntraining_gcs_path = 'uploads/kaggle_getstarted/full_train.csv'\nupload_blob(bucket_name, 'train.csv', training_gcs_path)\n\nprint('data upload completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amw = AutoMLWrapper(client=client, \n                    project_id=PROJECT_ID, \n                    bucket_name=bucket_name, \n                    region='us-central1', \n                    dataset_display_name=dataset_display_name, \n                    model_display_name=model_display_name)\n\nprint('AutoML wrapper created')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not amw.get_dataset_by_display_name(dataset_display_name):\n    print('dataset not found, creating new one')\n    amw.create_dataset()\n    # this part took me around 30 minutes\n    amw.import_gcs_data(training_gcs_path)\n\namw.dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('starting model train')\n# started at 5:27pm - finished around 9pm\nif not amw.get_model_by_display_name():\n    # took me around 3-4 hours to train the model\n    amw.train_model()\n    \nprint('train complete')\nprint('starting model deploy')\namw.deploy_model() # took me around 10 min\nprint('model deploy complete')\namw.model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"amw.model_full_path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create client for prediction service.\nprediction_client = automl.PredictionServiceClient()\namw.set_prediction_client(prediction_client)\n\nprint('starting predictions')\n\n# takes about 20-30 min\npredictions_df = amw.get_predictions(nlp_test_df, \n                                     input_col_name='text', \n#                                      ground_truth_col_name='target', # we don't have ground truth in our test set\n                                     limit=None, \n                                     threshold=0.5,\n                                     verbose=False)\nprint('predictions complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('starting model undeploy')\namw.undeploy_model()\nprint('undeploy complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.concat([nlp_test_df['id'], predictions_df['class']], axis=1)\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = submission_df.rename(columns={'class':'target'})\nsubmission_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False, header=True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}