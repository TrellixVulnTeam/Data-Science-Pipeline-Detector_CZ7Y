{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# we need to install HuggingFace datasets library\n!pip install datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-29T12:03:21.096355Z","iopub.execute_input":"2022-04-29T12:03:21.096944Z","iopub.status.idle":"2022-04-29T12:03:37.226519Z","shell.execute_reply.started":"2022-04-29T12:03:21.096844Z","shell.execute_reply":"2022-04-29T12:03:37.22521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# to disable wandb logging\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport torch\n\n# Transformers\n# installed with pip command above\nfrom datasets import Dataset\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:03:38.770792Z","iopub.execute_input":"2022-04-29T12:03:38.771087Z","iopub.status.idle":"2022-04-29T12:03:47.273875Z","shell.execute_reply.started":"2022-04-29T12:03:38.771055Z","shell.execute_reply":"2022-04-29T12:03:47.272781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### reading the entire training dataset","metadata":{}},{"cell_type":"code","source":"TRAIN_FILE = \"/kaggle/input/nlp-getting-started/train.csv\"\n\norig_train = pd.read_csv(TRAIN_FILE)\n\n# num of distinct labels in target\nNUM_LABELS = orig_train['target'].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:04:23.624176Z","iopub.execute_input":"2022-04-29T12:04:23.624499Z","iopub.status.idle":"2022-04-29T12:04:23.682934Z","shell.execute_reply.started":"2022-04-29T12:04:23.624464Z","shell.execute_reply":"2022-04-29T12:04:23.682003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### split in train/validation set and keep only useful columns","metadata":{}},{"cell_type":"code","source":"# do the train/validation split\n\nSEED = 1432\nVALID_FRAC = 0.2\nUSED_COLUMNS = ['text', 'target']\n\n\ntrain_df, valid_df = train_test_split(orig_train, test_size=VALID_FRAC, random_state=42)\n\ntrain_df = train_df[USED_COLUMNS]\nvalid_df = valid_df[USED_COLUMNS]\n\nprint(f\"There are {train_df.shape[0]} samples in train set\")\nprint(f\"There are {valid_df.shape[0]} samples in valid set\")\n\n# rename rating to label\ntrain_df = train_df.rename(columns={\"target\": \"label\"})\nvalid_df = valid_df.rename(columns={\"target\": \"label\"})","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:04:26.662068Z","iopub.execute_input":"2022-04-29T12:04:26.662716Z","iopub.status.idle":"2022-04-29T12:04:26.683395Z","shell.execute_reply.started":"2022-04-29T12:04:26.662674Z","shell.execute_reply":"2022-04-29T12:04:26.682338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### create HF datasets\n\nto prepare data fro the training we need a train and a validation dataset where text has been transformed in token and encoded","metadata":{}},{"cell_type":"code","source":"# start building the dataset objects expected from transformers\nds_train = Dataset.from_pandas(train_df.reset_index(drop=True))\nds_valid = Dataset.from_pandas(valid_df.reset_index(drop=True))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:05:29.488442Z","iopub.execute_input":"2022-04-29T12:05:29.488819Z","iopub.status.idle":"2022-04-29T12:05:29.521876Z","shell.execute_reply.started":"2022-04-29T12:05:29.488776Z","shell.execute_reply":"2022-04-29T12:05:29.520921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train.features","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:05:30.826863Z","iopub.execute_input":"2022-04-29T12:05:30.827134Z","iopub.status.idle":"2022-04-29T12:05:30.836535Z","shell.execute_reply.started":"2022-04-29T12:05:30.827105Z","shell.execute_reply":"2022-04-29T12:05:30.835502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"# here we define the pre-trained transformer we are using. In this NB we will be using roberta-large and corresponding tokenizer\nMODEL_CKPT = \"roberta-large\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:05:53.656005Z","iopub.execute_input":"2022-04-29T12:05:53.656361Z","iopub.status.idle":"2022-04-29T12:06:01.498882Z","shell.execute_reply.started":"2022-04-29T12:05:53.65633Z","shell.execute_reply":"2022-04-29T12:06:01.497806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this function will be applied to both set for tokenization to add columns with token encoded\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=50)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:06:08.63511Z","iopub.execute_input":"2022-04-29T12:06:08.63547Z","iopub.status.idle":"2022-04-29T12:06:08.641389Z","shell.execute_reply.started":"2022-04-29T12:06:08.635382Z","shell.execute_reply":"2022-04-29T12:06:08.640419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# and here we have the final HF datasets\nds_train_encoded = ds_train.map(tokenize, batched=True, batch_size=None)\nds_valid_encoded = ds_valid.map(tokenize, batched=True, batch_size=None)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:06:27.441652Z","iopub.execute_input":"2022-04-29T12:06:27.441958Z","iopub.status.idle":"2022-04-29T12:06:28.933191Z","shell.execute_reply.started":"2022-04-29T12:06:27.441926Z","shell.execute_reply":"2022-04-29T12:06:28.929278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# have a look\nds_train_encoded","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:06:45.992743Z","iopub.execute_input":"2022-04-29T12:06:45.99349Z","iopub.status.idle":"2022-04-29T12:06:46.00177Z","shell.execute_reply.started":"2022-04-29T12:06:45.993423Z","shell.execute_reply":"2022-04-29T12:06:46.000675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we can see we have two columns added: input_ids and attention mask, that will be used during the training","metadata":{}},{"cell_type":"code","source":"# prepare the training on GPU (if available)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = (AutoModelForSequenceClassification.from_pretrained(MODEL_CKPT, num_labels=NUM_LABELS).to(device))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:07:48.848113Z","iopub.execute_input":"2022-04-29T12:07:48.848828Z","iopub.status.idle":"2022-04-29T12:08:51.108346Z","shell.execute_reply.started":"2022-04-29T12:07:48.848762Z","shell.execute_reply":"2022-04-29T12:08:51.107374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this function is used to compute the metrics (accuracy, f1-score) that will be computed during validation phases\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    \n    f1 = f1_score(labels, preds, average=\"weighted\")\n    acc = accuracy_score(labels, preds)\n    \n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:08:51.110245Z","iopub.execute_input":"2022-04-29T12:08:51.110612Z","iopub.status.idle":"2022-04-29T12:08:51.118124Z","shell.execute_reply.started":"2022-04-29T12:08:51.110581Z","shell.execute_reply":"2022-04-29T12:08:51.116519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# params for training\nBATCH_SIZE = 32\n\n# be careful, with 5 epochs local storage will be filled and you run out of space\nEPOCHS = 3\nLR = 1e-5\nW_DECAY = 0.01\n\n# to disable wandb logging ---> best, see report_to\n# os.environ[\"WANDB_DISABLED\"] = \"true\"\n\nlogging_steps = len(ds_train_encoded) // BATCH_SIZE\n\nmodel_name = f\"{MODEL_CKPT}-finetuned-tweets\"\n\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=EPOCHS,\n                                  # changed\n                                  learning_rate=LR,\n                                  per_device_train_batch_size=BATCH_SIZE,\n                                  per_device_eval_batch_size=BATCH_SIZE,\n                                  weight_decay=W_DECAY,\n                                  evaluation_strategy=\"epoch\",\n                                  save_strategy=\"epoch\",\n                                  disable_tqdm=False,\n                                  logging_steps=logging_steps,\n                                  push_to_hub=False, \n                                  log_level=\"error\",\n                                  load_best_model_at_end=True,\n                                  # to disable wandb logging\n                                  report_to=\"none\"\n                                 )","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:09:30.41647Z","iopub.execute_input":"2022-04-29T12:09:30.416742Z","iopub.status.idle":"2022-04-29T12:09:30.42617Z","shell.execute_reply.started":"2022-04-29T12:09:30.416712Z","shell.execute_reply":"2022-04-29T12:09:30.425134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#\n# and here we will do the training\n#\ntrainer = Trainer(model=model, args=training_args, \n                  compute_metrics=compute_metrics,\n                  train_dataset=ds_train_encoded,\n                  eval_dataset=ds_valid_encoded,\n                  tokenizer=tokenizer)\ntrainer.train();","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:09:47.858959Z","iopub.execute_input":"2022-04-29T12:09:47.859244Z","iopub.status.idle":"2022-04-29T12:16:38.498216Z","shell.execute_reply.started":"2022-04-29T12:09:47.859214Z","shell.execute_reply":"2022-04-29T12:16:38.497186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute f1 score on the best model (chosen on valid_loss)\nf1 = trainer.predict(ds_valid_encoded).metrics['test_f1']\n    \nprint(f\"F1 score is: {round(f1, 4)}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-29T12:16:45.436578Z","iopub.execute_input":"2022-04-29T12:16:45.436874Z","iopub.status.idle":"2022-04-29T12:16:53.539636Z","shell.execute_reply.started":"2022-04-29T12:16:45.436843Z","shell.execute_reply":"2022-04-29T12:16:53.538331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### final remarks:\n* as you can see, the score is good: F1 = 0.84; Obviously here it depends on the train/valid split done\n* a better result could be obtained, for example, using k-fold split; The final result is a set of k models and you need to avg predictions\n\nI have not put here the code to do predictions on the test set. It is not difficult. Only one thing: you need to do it in batches (to avoid OOM on GPU). It is left as an exercise.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}