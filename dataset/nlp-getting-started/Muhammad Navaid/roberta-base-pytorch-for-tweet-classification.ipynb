{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nfrom textwrap import wrap\n\nimport os\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\nimport tensorflow as tf\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\n\nRANDOM_SEED = 63\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-03T17:26:47.488573Z","iopub.execute_input":"2021-07-03T17:26:47.488954Z","iopub.status.idle":"2021-07-03T17:26:47.523214Z","shell.execute_reply.started":"2021-07-03T17:26:47.488921Z","shell.execute_reply":"2021-07-03T17:26:47.519664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\ncombined = pd.concat([train,test], axis=0)\ncombined.drop('target',inplace=True, axis=1)\ncombined.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:47.524834Z","iopub.execute_input":"2021-07-03T17:26:47.525196Z","iopub.status.idle":"2021-07-03T17:26:47.574938Z","shell.execute_reply.started":"2021-07-03T17:26:47.525162Z","shell.execute_reply":"2021-07-03T17:26:47.573951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* d - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n* \n* No missing values in the most important feature, text, but...\n* 87 missing values in keyword\n* 3638 missing values in location","metadata":{}},{"cell_type":"code","source":"y = train.target.copy()\nX = train.drop('target',axis=1)\npath = '../input/roberta-transformers-pytorch/roberta-base'","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:47.576748Z","iopub.execute_input":"2021-07-03T17:26:47.577141Z","iopub.status.idle":"2021-07-03T17:26:47.584268Z","shell.execute_reply.started":"2021-07-03T17:26:47.577102Z","shell.execute_reply":"2021-07-03T17:26:47.583137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Class Imbalance?","metadata":{}},{"cell_type":"code","source":"sns.countplot(y)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:47.586191Z","iopub.execute_input":"2021-07-03T17:26:47.586836Z","iopub.status.idle":"2021-07-03T17:26:47.773094Z","shell.execute_reply.started":"2021-07-03T17:26:47.586793Z","shell.execute_reply":"2021-07-03T17:26:47.772035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nNegatives outnumber positives by ~1000","metadata":{}},{"cell_type":"markdown","source":"# First, we have to sort our data by sequence length. We'll tokenize our data and plot the length of these tokens first.Negatives outnumber positives by ~1000\n","metadata":{}},{"cell_type":"markdown","source":"First, we have to sort our data by sequence length. We'll tokenize our data and plot the length of these tokens first.","metadata":{}},{"cell_type":"code","source":"train_targets = train.target.values.tolist()\nmax_len = 90\n\ntokenizer = AutoTokenizer.from_pretrained('../input/roberta-transformers-pytorch/roberta-base')\n\ninput_ids = [tokenizer.encode(\n        text=i,           \n        add_special_tokens=True, \n        max_length=max_len,  \n        truncation=True,     \n        padding=False\n    ) for i in train.text]","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:47.77465Z","iopub.execute_input":"2021-07-03T17:26:47.775028Z","iopub.status.idle":"2021-07-03T17:26:49.009017Z","shell.execute_reply.started":"2021-07-03T17:26:47.77499Z","shell.execute_reply":"2021-07-03T17:26:49.008111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unsorted_lengths = [len(x) for x in input_ids]\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style='darkgrid')\n\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nplt.scatter(range(0, len(unsorted_lengths)), unsorted_lengths, marker=\"|\")\n\nplt.xlabel('Sample Number')\nplt.ylabel('Sequence Length')\nplt.title('Samples BEFORE Sorting')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:49.010287Z","iopub.execute_input":"2021-07-03T17:26:49.01063Z","iopub.status.idle":"2021-07-03T17:26:49.331424Z","shell.execute_reply.started":"2021-07-03T17:26:49.010595Z","shell.execute_reply":"2021-07-03T17:26:49.330665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nWhat a mess. Now we'll sort the tokens by length","metadata":{}},{"cell_type":"code","source":"sorted_input_ids = sorted(zip(input_ids, train_targets), key=lambda x: len(x[0]))\nprint('Shortest sample:', len(sorted_input_ids[0][0]))\nprint('Longest sample:', len(sorted_input_ids[-1][0]))\nsorted_lengths = [len(s[0]) for s in sorted_input_ids]","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:49.333659Z","iopub.execute_input":"2021-07-03T17:26:49.334165Z","iopub.status.idle":"2021-07-03T17:26:49.346415Z","shell.execute_reply.started":"2021-07-03T17:26:49.334124Z","shell.execute_reply":"2021-07-03T17:26:49.345429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nplt.plot(range(0, len(sorted_lengths)), sorted_lengths)\n\nplt.xlabel('Sample Number')\nplt.ylabel('Sequence Length')\nplt.title('Samples after Sorting')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:49.348128Z","iopub.execute_input":"2021-07-03T17:26:49.348624Z","iopub.status.idle":"2021-07-03T17:26:49.611631Z","shell.execute_reply.started":"2021-07-03T17:26:49.348587Z","shell.execute_reply":"2021-07-03T17:26:49.610686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Batch Selection","metadata":{}},{"cell_type":"code","source":"batch_size = 32\nimport random\n\nbatch_ordered_sentences = []\nbatch_ordered_labels = []\n\nprint('Creating training batches of size {:}'.format(batch_size))\n\nwhile len(sorted_input_ids) > 0:  \n    if ((len(batch_ordered_sentences) % 50) == 0):\n        print('  Selected {:,} batches.'.format(len(batch_ordered_sentences)))\n\n    to_take = min(batch_size, len(sorted_input_ids))\n    select = random.randint(0, len(sorted_input_ids) - to_take)\n    batch = sorted_input_ids[select:(select + to_take)]\n    batch_ordered_sentences.append([s[0] for s in batch])\n    batch_ordered_labels.append([s[1] for s in batch])\n    del sorted_input_ids[select:select + to_take]\n\nprint('\\n  DONE - {:,} batches.'.format(len(batch_ordered_sentences)))","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:49.613041Z","iopub.execute_input":"2021-07-03T17:26:49.613385Z","iopub.status.idle":"2021-07-03T17:26:49.633459Z","shell.execute_reply.started":"2021-07-03T17:26:49.613349Z","shell.execute_reply":"2021-07-03T17:26:49.632377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nRemember, the batches are still not ordered according to length\n","metadata":{}},{"cell_type":"markdown","source":"# Padding","metadata":{}},{"cell_type":"code","source":"inputs = []\nattn_masks = []\ntargets = []\n\nfor (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n    batch_padded_inputs = []\n    batch_attn_masks = []\n    max_size = max([len(sen) for sen in batch_inputs])\n    for sen in batch_inputs:\n        num_pads = max_size - len(sen)\n        padded_input = sen + [tokenizer.pad_token_id]*num_pads\n        attn_mask = [1] * len(sen) + [0] * num_pads\n        batch_padded_inputs.append(padded_input)\n        batch_attn_masks.append(attn_mask)\n    inputs.append(torch.tensor(batch_padded_inputs))\n    attn_masks.append(torch.tensor(batch_attn_masks))\n    targets.append(torch.tensor(batch_labels))","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:49.635035Z","iopub.execute_input":"2021-07-03T17:26:49.635585Z","iopub.status.idle":"2021-07-03T17:26:49.730178Z","shell.execute_reply.started":"2021-07-03T17:26:49.635546Z","shell.execute_reply":"2021-07-03T17:26:49.729419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison","metadata":{}},{"cell_type":"code","source":"train_text = train.text.values.tolist()\npadded_lengths = [len(s) for batch in inputs for s in batch]\nsmart_token_count = np.sum(padded_lengths)\nfixed_token_count = len(train_text) * max_len\n\nprcnt_reduced = (fixed_token_count - smart_token_count) / float(fixed_token_count) \n\nprint('Total tokens:')\nprint('   Fixed Padding: {:,}'.format(fixed_token_count))\nprint('  Smart Batching: {:,}  ({:.2%} less)'.format(smart_token_count, prcnt_reduced))","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:49.731382Z","iopub.execute_input":"2021-07-03T17:26:49.731718Z","iopub.status.idle":"2021-07-03T17:26:49.767587Z","shell.execute_reply.started":"2021-07-03T17:26:49.731682Z","shell.execute_reply":"2021-07-03T17:26:49.766651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Putting it all together\n\n\nSmartBatchingDataset stores samples by tokenizing text and converting to sequences\n","metadata":{}},{"cell_type":"code","source":"# Essential Imports\nimport random\nimport numpy as np\nimport multiprocessing\nimport more_itertools\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Sampler, Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:49.768947Z","iopub.execute_input":"2021-07-03T17:26:49.769285Z","iopub.status.idle":"2021-07-03T17:26:49.774863Z","shell.execute_reply.started":"2021-07-03T17:26:49.769251Z","shell.execute_reply":"2021-07-03T17:26:49.774009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SmartBatchingDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        super(SmartBatchingDataset, self).__init__()\n        self._data = (\n            f\"{tokenizer.bos_token} \" + df.text + f\" {tokenizer.eos_token}\" \n        ).apply(tokenizer.tokenize).apply(tokenizer.convert_tokens_to_ids).to_list()\n        self._targets = None\n        if 'target' in df.columns:\n            self._targets = df.target.tolist()\n        self.sampler = None\n\n    def __len__(self):\n        return len(self._data)\n\n    def __getitem__(self, item):\n        if self._targets is not None:\n            return self._data[item], self._targets[item]\n        else:\n            return self._data[item]\n\n    def get_dataloader(self, batch_size, max_len, pad_id):\n        self.sampler = SmartBatchingSampler(\n            data_source=self._data,\n            batch_size=batch_size\n        )\n        collate_fn = SmartBatchingCollate(\n            targets=self._targets,\n            max_length=max_len,\n            pad_token_id=pad_id\n        )\n        dataloader = DataLoader(\n            dataset=self,\n            batch_size=batch_size,\n            sampler=self.sampler,\n            collate_fn=collate_fn,\n            num_workers=(multiprocessing.cpu_count()-1),\n            pin_memory=True\n        )\n        return dataloader","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:49.775996Z","iopub.execute_input":"2021-07-03T17:26:49.776485Z","iopub.status.idle":"2021-07-03T17:26:49.78776Z","shell.execute_reply.started":"2021-07-03T17:26:49.776435Z","shell.execute_reply":"2021-07-03T17:26:49.787041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nSmartBatchingSampler sorts sequences by length, make batches of specified size, shuffle the batch, then return indices\n","metadata":{}},{"cell_type":"code","source":"class SmartBatchingSampler(Sampler):\n    def __init__(self, data_source, batch_size):\n        super(SmartBatchingSampler, self).__init__(data_source)\n        sample_lengths = [len(seq) for seq in data_source]\n        argsort_inds = np.argsort(sample_lengths)\n        batches = list(more_itertools.chunked(argsort_inds, n=batch_size))\n        if batches:\n            last_batch = batches.pop(-1)\n            np.random.shuffle(batches)\n            batches.append(last_batch)\n        self._inds = list(more_itertools.flatten(batches))\n        self._backsort_inds = None\n    \n    def __iter__(self):\n        it = iter(self._inds)\n        return it\n\n    def __len__(self):\n        return len(self._inds)\n    \n    @property\n    def backsort_inds(self):\n        if self._backsort_inds is None:\n            self._backsort_inds = np.argsort(self._inds)\n        return self._backsort_inds","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:49.788998Z","iopub.execute_input":"2021-07-03T17:26:49.789444Z","iopub.status.idle":"2021-07-03T17:26:49.798963Z","shell.execute_reply.started":"2021-07-03T17:26:49.78941Z","shell.execute_reply":"2021-07-03T17:26:49.797874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nSmartBatchingCollate adds padding up to max_length, make attention masks, and targets for each sample batch\n","metadata":{}},{"cell_type":"code","source":"class SmartBatchingCollate:\n    def __init__(self, targets, max_length, pad_token_id):\n        self._targets = targets\n        self._max_length = max_length\n        self._pad_token_id = pad_token_id\n        \n    def __call__(self, batch):\n        if self._targets is not None:\n            sequences, targets = list(zip(*batch))\n        else:\n            sequences = list(batch)\n        \n        input_ids, attention_mask = self.pad_sequence(\n            sequences,\n            max_sequence_length=self._max_length,\n            pad_token_id=self._pad_token_id\n        )\n        \n        if self._targets is not None:\n            output = input_ids, attention_mask, torch.tensor(targets)\n        else:\n            output = input_ids, attention_mask\n        return output\n    \n    def pad_sequence(self, sequence_batch, max_sequence_length, pad_token_id):\n        max_batch_len = max(len(sequence) for sequence in sequence_batch)\n        max_len = min(max_batch_len, max_sequence_length)\n        padded_sequences, attention_masks = [[] for i in range(2)]\n        attend, no_attend = 1, 0\n        for sequence in sequence_batch:\n            # As discussed above, truncate if exceeds max_len\n            new_sequence = list(sequence[:max_len])\n            \n            attention_mask = [attend] * len(new_sequence)\n            pad_length = max_len - len(new_sequence)\n            \n            new_sequence.extend([pad_token_id] * pad_length)\n            attention_mask.extend([no_attend] * pad_length)\n            \n            padded_sequences.append(new_sequence)\n            attention_masks.append(attention_mask)\n        \n        padded_sequences = torch.tensor(padded_sequences)\n        attention_masks = torch.tensor(attention_masks)\n        return padded_sequences, attention_masks","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:49.800154Z","iopub.execute_input":"2021-07-03T17:26:49.800585Z","iopub.status.idle":"2021-07-03T17:26:49.813238Z","shell.execute_reply.started":"2021-07-03T17:26:49.800521Z","shell.execute_reply":"2021-07-03T17:26:49.81239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = SmartBatchingDataset(train, tokenizer)\ntrain_data_loader = dataset.get_dataloader(batch_size=24, max_len=max_len, pad_id=tokenizer.pad_token_id)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:49.814368Z","iopub.execute_input":"2021-07-03T17:26:49.814772Z","iopub.status.idle":"2021-07-03T17:26:51.965786Z","shell.execute_reply.started":"2021-07-03T17:26:49.814715Z","shell.execute_reply":"2021-07-03T17:26:51.964897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padded_lengths = []\nfor batch_idx, (input_ids, attention_mask, targets) in enumerate(train_data_loader):\n    for s in input_ids:\n        padded_lengths.append(len(s))\n\nsmart_token_count = np.sum(padded_lengths)\nfixed_token_count = len(train_text) * max_len\n\nprcnt_reduced = (fixed_token_count - smart_token_count) / float(fixed_token_count) \n\nprint('Total tokens:')\nprint('   Fixed Padding: {:,}'.format(fixed_token_count))\nprint('  Smart Batching: {:,}  ({:.2%} less)'.format(smart_token_count, prcnt_reduced))","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:51.967068Z","iopub.execute_input":"2021-07-03T17:26:51.967406Z","iopub.status.idle":"2021-07-03T17:26:52.978079Z","shell.execute_reply.started":"2021-07-03T17:26:51.967372Z","shell.execute_reply":"2021-07-03T17:26:52.977051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nPytorch dataloader implementation saved a little more memory\n","metadata":{}},{"cell_type":"markdown","source":"# Model Creation\n","metadata":{}},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.roberta = AutoModel.from_pretrained(path)\n        self.config = self.roberta.config\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.config.hidden_size, n_classes)\n        \n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n        output = self.drop(sequence_output)\n        return self.out(output)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:52.98175Z","iopub.execute_input":"2021-07-03T17:26:52.982044Z","iopub.status.idle":"2021-07-03T17:26:52.989236Z","shell.execute_reply.started":"2021-07-03T17:26:52.982014Z","shell.execute_reply":"2021-07-03T17:26:52.988293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentimentClassifier(2) # 2 classes 1 for disaster and 0 for not\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:52.990896Z","iopub.execute_input":"2021-07-03T17:26:52.991284Z","iopub.status.idle":"2021-07-03T17:26:55.717746Z","shell.execute_reply.started":"2021-07-03T17:26:52.991244Z","shell.execute_reply":"2021-07-03T17:26:55.716875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.nn.functional.softmax(model(input_ids, attention_mask), dim=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:55.719065Z","iopub.execute_input":"2021-07-03T17:26:55.719432Z","iopub.status.idle":"2021-07-03T17:26:55.723374Z","shell.execute_reply.started":"2021-07-03T17:26:55.719394Z","shell.execute_reply":"2021-07-03T17:26:55.722429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"EPOCHS = 10\noptimizer = AdamW(model.parameters(), betas = (0.99, 0.98), lr=2e-5)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:55.724785Z","iopub.execute_input":"2021-07-03T17:26:55.725139Z","iopub.status.idle":"2021-07-03T17:26:55.739105Z","shell.execute_reply.started":"2021-07-03T17:26:55.725102Z","shell.execute_reply":"2021-07-03T17:26:55.73828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Some recommendations for fine tuning from the BERT paper\n* Batch size: 16, 32\n* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n* Number of epochs: 2, 3, 4","metadata":{}},{"cell_type":"markdown","source":"\n\ntrain_data_loader orders its data like this: input_ids, attention_mask, targets\n","metadata":{}},{"cell_type":"code","source":"def train_epoch(\n  model,\n  data_loader,\n  loss_fn,\n  optimizer,\n  device,\n  scheduler,\n  n_examples\n):\n  model = model.train()\n  losses = []\n  correct_predictions = 0\n  for d in data_loader:\n    input_ids = d[0].to(device)\n    attention_mask = d[1].to(device)\n    targets = d[2].to(device)\n    outputs = model(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n    _, preds = torch.max(outputs, dim=1)\n    loss = loss_fn(outputs, targets)\n    correct_predictions += torch.sum(preds == targets)\n    losses.append(loss.item())\n    loss.backward()\n    # prevents exploding gradients\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n  return correct_predictions.double() / n_examples, np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:55.740317Z","iopub.execute_input":"2021-07-03T17:26:55.740674Z","iopub.status.idle":"2021-07-03T17:26:55.750989Z","shell.execute_reply.started":"2021-07-03T17:26:55.740638Z","shell.execute_reply":"2021-07-03T17:26:55.750161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n  model = model.eval()\n  losses = []\n  correct_predictions = 0\n  with torch.no_grad():\n    for d in data_loader:\n        input_ids = d[0].to(device)\n        attention_mask = d[1].to(device)\n        targets = d[2].to(device)\n        outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n  return correct_predictions.double() / n_examples, np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:55.751958Z","iopub.execute_input":"2021-07-03T17:26:55.752209Z","iopub.status.idle":"2021-07-03T17:26:55.766514Z","shell.execute_reply.started":"2021-07-03T17:26:55.752176Z","shell.execute_reply":"2021-07-03T17:26:55.765639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val = train_test_split(train, test_size=0.10, random_state=RANDOM_SEED)\n\ntrain_dataset = SmartBatchingDataset(X_train, tokenizer)\ntrain_data_loader = train_dataset.get_dataloader(batch_size=24, max_len=max_len, pad_id=tokenizer.pad_token_id)\n\nval_dataset = SmartBatchingDataset(X_val, tokenizer)\nval_data_loader = val_dataset.get_dataloader(batch_size=24, max_len=max_len, pad_id=tokenizer.pad_token_id)\n\nhistory = defaultdict(list)\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(X_train)\n    )\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n\n    val_acc, val_loss = eval_model(\n      model,\n      val_data_loader,\n      loss_fn,\n      device,\n      len(X_val)\n    )\n    print(f'Val loss {val_loss} accuracy {val_acc}')\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)","metadata":{"execution":{"iopub.status.busy":"2021-07-03T17:26:55.769606Z","iopub.execute_input":"2021-07-03T17:26:55.769904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.plot(history['train_acc'], label='train accuracy')\n# plt.plot(history['val_acc'], label = 'val accurracy')\n# plt.title('Training history')\n# plt.ylabel('Accuracy')\n# plt.xlabel('Epoch')\n# plt.legend()\n# plt.ylim([0, 1]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing\n","metadata":{}},{"cell_type":"code","source":"encodes = test.text.apply(lambda x: tokenizer.encode_plus(\n            x, \n            add_special_tokens=True,\n            max_length = max_len,\n            truncation=True,\n            padding='max_length',\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            return_tensors='pt'\n        ))\ninput_ids = [i['input_ids'] for i in encodes]\nattention_mask = [i['attention_mask'] for i in encodes]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor i, j in zip(input_ids, attention_mask):\n    i = i.to(device)\n    j = j.to(device)\n    output = model(i, j)\n    _, prediction = torch.max(output, dim=1)\n    predictions.append(prediction.item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"predictions = []\nfor i, j in zip(input_ids, attention_mask):\n    i = i.to(device)\n    j = j.to(device)\n    output = model(i, j)\n    _, prediction = torch.max(output, dim=1)\n    predictions.append(prediction.item())","metadata":{}},{"cell_type":"code","source":"submission = pd.concat([test.id, pd.Series(predictions)], axis=1)\nsubmission.rename(columns = {0:'target'}, inplace=True)\nsubmission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}