{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"My attempt at a sequence classification task using BERT via Hugging Face, as part of the [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started) getting started competition. The task is to classify whether tweets are about a real disaster or not. This is just a simple project to familiarize myself with these libraries.\n\nMuch is owed to the Hugging Face fine-tuning tutorial [here](https://huggingface.co/docs/transformers/training), as well as [this notebook](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert#7.-Model) for some text cleaning and information plotting techniques.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n#libraries for data exploration and display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#libraries for text preprocessing\n\nimport string\nimport re\nimport emoji\nfrom bs4 import BeautifulSoup\n\n#machine learning tools\n\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import EarlyStoppingCallback\nfrom transformers import BertTokenizer, BertForSequenceClassification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We begin with some exploratory data analysis, examining the tweets and related data.","metadata":{}},{"cell_type":"code","source":"#take a look at training data\ntrain_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntrain_data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#take a look at test data\ntest_data = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_data.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#examine the distribution of disaster and non-disaster tweets\ntrue_count = train_data[train_data['target'] == 1].shape[0]\nfalse_count = train_data[train_data['target'] == 0].shape[0]\n\nprint(f'Disaster tweets: {true_count}')\nprint(f'Non-disaster tweets: {false_count}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check data types and look for null values\nprint('Disaster tweet information:')\nprint(train_data.info())\nprint('\\nNon-disaster tweet information:')\nprint(test_data.info())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems we are missing some keywords, plus a significant portion of locations.","metadata":{}},{"cell_type":"code","source":"#examine the contents of keyword\ntrain_data['keyword'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#examine the contents of location\ntrain_data['location'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#seeing a trail of several unique values, check total unique count\ntrain_data['location'].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With so many unique values, it seems best to drop location.","metadata":{}},{"cell_type":"code","source":"train_data.drop('location', axis=1)\ntest_data.drop('location', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we work on the keyword column:","metadata":{}},{"cell_type":"code","source":"#remove null values\ntrain_data['keyword'] = train_data['keyword'].fillna('') \ntest_data['keyword'] = test_data['keyword'].fillna('') ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#associate each train keyword with its mean percentage of real disaster tweets\ntrain_data['target_mean'] = train_data.groupby('keyword')['target'].transform('mean')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the keywords to observe their association with real disasters\nplt.figure(figsize=(8, 60))\n\nsns.countplot(y=train_data.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=train_data.sort_values(by='target_mean', ascending=False)['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some keywords are significantly more associated with real disasters than others, suggesting it may be a good idea to incorporate keyword information. For now, I experiment with simply appending the keywords to the text.","metadata":{}},{"cell_type":"code","source":"#experiment: simply append the keywords to the text\nfor keyword, text in zip(train_data['keyword'], train_data['text']):\n    re.sub(r'%20', ' ', keyword) #minor text cleaning\n    text = text + \" \" + keyword\n    \nfor keyword, text in zip(test_data['keyword'], test_data['text']):\n    re.sub(r'%20', ' ', keyword)\n    text = text + \" \" + keyword","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now examine the text contents of the tweets and determine how to preprocess them.","metadata":{}},{"cell_type":"code","source":"#create corpora for each target.\ndisaster_corpus = []\nfor tweet in train_data[train_data['target']==1]['text'].str.split():\n    for word in tweet:\n        disaster_corpus.append(word)\n        \nnondisaster_corpus = []\nfor tweet in train_data[train_data['target']==0]['text'].str.split():\n    for word in tweet:\n        nondisaster_corpus.append(word)\n        \ndisaster_corpus","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can look at these corpora to determine how to clean the text, finding a variety of misspelled words, links, usernames, and so on which can negatively impact our model's ability to handle the text.\n\nWe now clean the text, removing URLs, html, emojis, and other undesirable elements:","metadata":{}},{"cell_type":"code","source":"#cleaning function to be mapped onto all tweets\ndef clean_text(text):\n    '''Performs various text cleaning operations, returning a processed string'''\n    #make all text lowercase\n    text = text.lower()\n    \n    #handle sequences like '\\x89ÛÏ'\n    text = text.encode('ascii', 'ignore').decode('utf-8') \n    \n    #remove links\n    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text) \n    \n    #remove html\n    soup = BeautifulSoup(text, 'lxml')\n    text = soup.get_text(strip=True)\n    \n    #remove emoji\n    text = emoji.replace_emoji(text, replace='') \n    \n    #remove mentions including user name\n    text = re.sub(r'@\\S+', '', text) \n    \n    #remove numbers and mixed alphanumeric words (often user names, l33t, etc.)\n    text = re.sub(r'\\w*\\d+\\w*', '', text) \n    \n    #remove punctuation\n    table = str.maketrans('', '', string.punctuation)\n    text = text.translate(table)\n    \n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply text cleaning\ntrain_data['cleaned_text'] = train_data['text'].apply(lambda text: clean_text(text))\ntest_data['cleaned_text'] = test_data['text'].apply(lambda text: clean_text(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#new corpora of cleaned words, used during text cleaning experiments\ndisaster_corpus = []\nfor tweet in train_data[train_data['target']==1]['cleaned_text'].str.split():\n    for word in tweet:\n        disaster_corpus.append(word)\n        \nnondisaster_corpus = []\nfor tweet in train_data[train_data['target']==0]['cleaned_text'].str.split():\n    for word in tweet:\n        nondisaster_corpus.append(word)\n\ndisaster_corpus","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Satisfied with how the text has been cleaned, we now begin tokenizing and encoding the text.","metadata":{}},{"cell_type":"code","source":"#create tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#extract text from series and split into training and validation set\nX_train, X_val = train_test_split(train_data, test_size=0.2)\nX_test = test_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#examine how many tokens the tokenizer makes for our data, and set max_length accordingly:\ntemp_tokenized = train_data['cleaned_text'].apply(lambda text : tokenizer.tokenize(text))\nsns.histplot(x=temp_tokenized.apply(lambda text : len(text)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on this, we set the max_length to 64, in case there are some slightly longer tweets in the test set.\n\nWe then obtain encoding data (input ids and attention masks) for all our tweets:","metadata":{}},{"cell_type":"code","source":"#function to directly create a list of dictonaries to be loaded into the model:\ndef tokenize_df(tokenizer, df, has_label=True):\n        #convert to list\n        tweet_list = df['cleaned_text'].tolist()\n        #returns dictionary with keys: input_ids, attention_mask)\n        encoding_dict = tokenizer(tweet_list, padding=\"max_length\", max_length=64,\n                                  truncation=True, return_token_type_ids=False)\n        #add 'label' key if we are making train or validation data\n        if has_label: \n            encoding_dict['label'] = df['target'].tolist()\n        #convert dictionary of lists into list of dictionaries\n        return [dict(zip(encoding_dict, t)) for t in zip(*encoding_dict.values())]\n\n#apply above function to create lists to use for our data:\nX_train_tokenized = tokenize_df(tokenizer, X_train)\nX_val_tokenized = tokenize_df(tokenizer, X_val)\nX_test_tokenized = tokenize_df(tokenizer, X_test, has_label=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The tokenized training data is now a list of dictionaries containing encodings and attention masks for each example:","metadata":{}},{"cell_type":"code","source":"X_train_tokenized[0]","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset approach - I am currently experimenting with not using a dataset object \n#to better understand what is passed into and out of the model\n'''#Create torch datasets for our dictionaries:\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\ntrain_dataset = Dataset(X_train_tokenized, y_train)\neval_dataset = Dataset(X_eval_tokenized, y_val)\ntest_dataset = Dataset(X_test_tokenized)'''\n\n#dataset object free method:\ntrain_dataset = X_train_tokenized\neval_dataset = X_val_tokenized\ntest_dataset = X_test_tokenized","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have chosen to use a very standard BERT classification model for simplicity, and because it is easy to find examples and discussions about the performace of BERT, since it has been widely used and well-studied.","metadata":{}},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use gpu if available\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\nprint(f'Using {device}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#establish metrics to use when training:\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {'accuracy': acc,\n            'f1': f1,\n            'precision': precision,\n            'recall': recall}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n                output_dir=\"output\",\n                learning_rate=2e-5,\n                num_train_epochs=2, \n                report_to=\"none\",\n                evaluation_strategy=\"steps\", \n                eval_steps=100,\n                save_steps=100,\n                logging_steps=100,\n                load_best_model_at_end=True\n                )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#run the model\ntrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics=trainer.evaluate()\nprint(metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at some simple charts for our training, in addition to the Trainer.Train() output above:","metadata":{}},{"cell_type":"code","source":"log_df = pd.DataFrame(trainer.state.log_history)\nsns.lineplot(data=log_df[['loss', 'eval_loss', 'eval_accuracy', 'eval_f1']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that our model fits to the data and decides to early-stop at around the 1 epoch point.\n\nWe can now make predictions on the test data:","metadata":{}},{"cell_type":"code","source":"#make predictions:\npredictions = trainer.predict(test_dataset)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"predictions is a PredictionOutput with several keys, so we extract the predictions themselves:","metadata":{}},{"cell_type":"code","source":"y_pred = predictions.predictions\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two values per example, representing the two classes. We use argmax to identify which is higher for each and create a list of the results:","metadata":{}},{"cell_type":"code","source":"y_pred = [np.argmax(pred) for pred in y_pred]\ny_pred[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then load that data into a submission file:","metadata":{}},{"cell_type":"code","source":"#submit file (by replacing a column in the sample file)\nsample_submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsample_submission[\"target\"] = y_pred\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}