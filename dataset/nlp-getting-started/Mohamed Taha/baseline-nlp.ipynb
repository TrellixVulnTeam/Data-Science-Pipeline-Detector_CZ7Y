{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Variable Exploration","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntrain_data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntest_data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train_data['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"## Text Cleaning and Preprocessing","metadata":{}},{"cell_type":"code","source":"!pip install BeautifulSoup4","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup # Text Cleaning\nimport re, string # Regular Expressions, String\nfrom nltk.corpus import stopwords # stopwords\nfrom nltk.stem.porter import PorterStemmer # for word stemming\nfrom nltk.stem import WordNetLemmatizer # for word lemmatization\nimport unicodedata\nimport html\n\n# set of stopwords to be removed from text\nstop = set(stopwords.words('english'))\n\n# update stopwords to have punctuation too\nstop.update(list(string.punctuation))\n\ndef clean_tweets(text):\n    \n    # Remove unwanted html characters\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n    'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n    '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n    ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    text = re1.sub(' ', html.unescape(x1))\n    \n    # remove non-ascii characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n    # strip html\n    soup = BeautifulSoup(text, 'html.parser')\n    text = soup.get_text()\n    \n    # remove between square brackets\n    text = re.sub('\\[[^]]*\\]', '', text)\n    \n    # remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    \n    # remove twitter tags\n    text = text.replace(\"@\", \"\")\n    \n    # remove hashtags\n    text = text.replace(\"#\", \"\")\n    \n    # remove all non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    \n    # remove stopwords from text\n    final_text = []\n    for word in text.split():\n        if word.strip().lower() not in stop:\n            final_text.append(word.strip().lower())\n    \n    text = \" \".join(final_text)\n    \n    # lemmatize words\n    lemmatizer = WordNetLemmatizer()    \n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n    text = \" \".join([lemmatizer.lemmatize(word, pos = 'v') for word in text.split()])\n    \n    # replace all numbers with \"num\"\n    text = re.sub(\"\\d\", \"num\", text)\n    \n    return text.lower()\n\ntrain_data['prep_text'] = train_data['text'].apply(clean_tweets)\ntrain_data['prep_text'].head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['text'] = test_data['text'].apply(clean_tweets)\ntest_data['text'].head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Representation","metadata":{}},{"cell_type":"markdown","source":"## Text One-Hot Encoding","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer # Text tokenization\n\n# Setting up the tokenizer\nvocab_size = 1000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_data['prep_text']) + list(test_data['text']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Representing texts as one hot encoded sequence\n\nX_train_ohe = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'binary')\nX_test_ohe = tokenizer.texts_to_matrix(test_data['text'], mode = 'binary')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_ohe.shape}\")\nprint(f\"X_test shape: {X_test_ohe.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling on a simple Neural Network","metadata":{}},{"cell_type":"markdown","source":"## Train Validation Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_ohe, X_val_ohe, y_train, y_val = train_test_split(X_train_ohe, y_train, random_state = 42, test_size = 0.2)\n\nprint(f\"X_train shape: {X_train_ohe.shape}\")\nprint(f\"X_val shape: {X_val_ohe.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting up the model","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers, metrics, optimizers, losses\n\ndef setup_model():\n    \n    model = Sequential()\n#     model.add(layers.Dense(16, activation='relu', input_shape=(vocab_size,)))\n#     model.add(layers.Dense(16, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid', input_shape=(vocab_size,)))\n    \n    model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])\n    \n    return model\n\nmodel = setup_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train_ohe, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_ohe, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, accuracy = model.evaluate(X_val_ohe, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning Curves","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_history(history): \n\n    history_dict = history.history\n    history_dict.keys()\n\n\n    acc = history.history['binary_accuracy']\n    val_acc = history.history['val_binary_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n\n    # \"bo\" is for \"blue dot\"\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    # b is for \"solid blue line\"\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.show()\n    \nplot_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word-Count Representation","metadata":{}},{"cell_type":"code","source":"X_train_wc = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'count')\nX_test_wc = tokenizer.texts_to_matrix(test_data['text'], mode = 'count')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_test shape: {X_test_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"code","source":"X_train_wc, X_val_wc, y_train, y_val = train_test_split(X_train_wc, y_train, random_state = 42, test_size = 0.2)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_val shape: {X_val_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning on the Same Architecture","metadata":{}},{"cell_type":"code","source":"model = setup_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train_wc, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_wc, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, accuracy = model.evaluate(X_val_wc, y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning Curves","metadata":{}},{"cell_type":"code","source":"plot_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Term Frequency Representation","metadata":{}},{"cell_type":"code","source":"X_train_freq = tokenizer.texts_to_matrix(train_data['prep_text'], mode = 'freq')\nX_test_freq = tokenizer.texts_to_matrix(test_data['text'], mode = 'freq')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_freq.shape}\")\nprint(f\"X_test shape: {X_test_freq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_freq, X_val_freq, y_train, y_val = train_test_split(X_train_freq, y_train, test_size = 0.2, random_state = 42)\nprint(f\"X_train shape: {X_train_freq.shape}\")\nprint(f\"X_val shape: {X_val_freq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training on the same architecture","metadata":{}},{"cell_type":"code","source":"model = setup_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train_freq, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_freq, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning Curves","metadata":{}},{"cell_type":"code","source":"plot_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using TF-IDF Vectorization","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer # Term Frequency - Inverse Document Frequency\n\nvectorizer = TfidfVectorizer(max_features = vocab_size)\nvectorizer.fit(list(train_data['prep_text']) + list(test_data['text']))\n\n# Fitting on training and testing data\nX_train_tfidf = vectorizer.transform(list(train_data['prep_text'])).toarray() \nX_test_tfidf = vectorizer.transform(list(test_data['text'])).toarray()\n\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape {X_train_tfidf.shape}\")\nprint(f\"X_test shape {X_test_tfidf.shape}\")\nprint(f\"y_train shape {y_train.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Validation Split","metadata":{}},{"cell_type":"code","source":"X_train_tfidf, X_val_tfidf, y_train, y_val = train_test_split(X_train_tfidf, y_train, test_size = 0.2, random_state = 42)\nprint(f\"X_train shape: {X_train_tfidf.shape}\")\nprint(f\"X_val shape: {X_val_tfidf.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training on the same architecture","metadata":{}},{"cell_type":"code","source":"model = setup_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train_tfidf, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_tfidf, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning Curves","metadata":{}},{"cell_type":"code","source":"plot_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using twitter GloVE embeddings","metadata":{}},{"cell_type":"markdown","source":"## Sequence Length Analysis","metadata":{}},{"cell_type":"code","source":"plt.hist(list(train_data['prep_text'].str.split().map(lambda x: len(x))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the embedding dictionary from file\n\nembedding_dict={}\nwith open('../input/glovetwitter27b100dtxt/glove.twitter.27B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sequences creation, truncation and padding\n\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Setting up the tokenizer\nvocab_size = 10000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_data['prep_text']) + list(test_data['text']))\n\nmax_len = 15\nX_train_seq = tokenizer.texts_to_sequences(train_data['prep_text'])\nX_test_seq = tokenizer.texts_to_sequences(test_data['text'])\n\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\nX_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_test shape: {X_test_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Validation Split","metadata":{}},{"cell_type":"code","source":"X_train_seq, X_val_seq, y_train, y_val = train_test_split(X_train_seq, y_train, test_size = 0.2, random_state = 42)\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_val shape: {X_val_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words = len(tokenizer.word_index)\nprint(f\"Number of unique words: {num_words}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying GloVE representations on our corpus\n\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tokenizer.word_index.items():\n    if i < num_words:\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting up a model with the embeddings layer","metadata":{}},{"cell_type":"code","source":"# Setting up the model\n\nn_latent_factors = 100\nmodel_glove = Sequential()\nmodel_glove.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n                           input_length = max_len, trainable=True))\nmodel_glove.add(layers.Flatten())\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dropout(0.5))\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dense(1, activation='sigmoid'))\nmodel_glove.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_glove.compile(optimizer = optimizers.RMSprop(lr=0.001),\n              loss = losses.binary_crossentropy,\n              metrics = [metrics.binary_accuracy])\n\nhistory = model_glove.fit(X_train_seq,\n                    y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(X_val_seq, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Learning Curves","metadata":{}},{"cell_type":"code","source":"plot_history(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training for Submission","metadata":{}},{"cell_type":"code","source":"max_len = 15\nX_train_seq = tokenizer.texts_to_sequences(train_data['prep_text'])\nX_test_seq = tokenizer.texts_to_sequences(test_data['text'])\n\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\nX_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_test shape: {X_test_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\\n\")\n\n# Setting up the model\n\nn_latent_factors = 100\nmodel_glove = Sequential()\nmodel_glove.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n                           input_length = max_len, trainable=True))\nmodel_glove.add(layers.Flatten())\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dropout(0.5))\n# model_glove.add(layers.Dense(16, activation='relu'))\nmodel_glove.add(layers.Dense(1, activation='sigmoid'))\nprint(f\"{model_glove.summary()}\\n\")\n\n\nmodel_glove.compile(optimizer = optimizers.RMSprop(lr=0.001),\n              loss = losses.binary_crossentropy,\n              metrics = [metrics.binary_accuracy])\n\nhistory = model_glove.fit(X_train_seq,\n                    y_train,\n                    epochs=20,\n                    batch_size=512)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The effect of text preprocessing","metadata":{}},{"cell_type":"code","source":"# Setting up the tokenizer\nvocab_size = 1000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_data['text']) + list(test_data['text']))\n\n# Word count representation\nX_train_wc = tokenizer.texts_to_matrix(train_data['text'], mode = 'count')\nX_test_wc = tokenizer.texts_to_matrix(test_data['text'], mode = 'count')\ny_train = np.array(train_data['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_test shape: {X_test_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n\n# Train Validation Split\nX_train_wc, X_val_wc, y_train, y_val = train_test_split(X_train_wc, y_train, test_size = 0.2, random_state = 42)\n\nprint(f\"X_train shape: {X_train_wc.shape}\")\nprint(f\"X_val shape: {X_val_wc.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\\n\")\n\n# Setting up the model\nmodel = setup_model()\n\n# Fitting the model on un-preprocessed text\nhistory = model.fit(X_train_wc, y_train, epochs = 20, batch_size = 512, validation_data = (X_val_wc, y_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It turns out the model overfits because of the noise of the text like stopwords, punctuation, un-stemmed words, etc.","metadata":{}},{"cell_type":"markdown","source":"# Final Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\ntest_pred = model_glove.predict(X_test_seq)\ntest_pred_int = test_pred.round().astype('int')\nsubmission['target'] = test_pred_int\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}