{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Name:** Disaster Tweet Classifier\n\n**Author:** Sharome Burton\n\n**Date:** 07/19/2021\n\n**Description:** Machine learning model used to predict whether a tweet is about a real disaster or not.\n\n## 1. Problem definition\n> How well can we predict whether a tweet is about a disaster or not?\n\n## 2. Data\n   * `train.csv` - the training set\n   * `test.csv` - the test set\n    \n source: https://www.kaggle.com/c/nlp-getting-started/data\n\n   \n## 3. Evaluation \n\n> **Goal:** Predict the whether a tweet is about a disaster or not with >80% accuracy.\n\n## 4. Features\n\n   * id - a unique identifier for each tweet\n   * text - the text of the tweet\n   * location - the location the tweet was sent from (may be blank)\n   * keyword - a particular keyword from the tweet (may be blank)\n   * target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n","metadata":{}},{"cell_type":"code","source":"### Import libraries\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n## Packages for basic text processing\nimport re\nimport string \n\n## Visualization tools\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\n## Import Logistic Regression function for model training from linear model module of sklearn package\nfrom sklearn.linear_model import LogisticRegression\n\n## Import functions for evaluation from  model selection module of sklearn package\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n## Import functions for model selection from model selection module of sklearn package\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n## Import functions for text vectorization from feature extraction module of sklearn package\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n## Import function from NLTK package for Text tokenization and Normalization\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\n## Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-20T06:55:47.205089Z","iopub.execute_input":"2021-07-20T06:55:47.205714Z","iopub.status.idle":"2021-07-20T06:55:47.218447Z","shell.execute_reply.started":"2021-07-20T06:55:47.205679Z","shell.execute_reply":"2021-07-20T06:55:47.217654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading data","metadata":{"execution":{"iopub.status.busy":"2021-07-20T05:01:09.912588Z","iopub.execute_input":"2021-07-20T05:01:09.912973Z","iopub.status.idle":"2021-07-20T05:01:09.916906Z","shell.execute_reply.started":"2021-07-20T05:01:09.912943Z","shell.execute_reply":"2021-07-20T05:01:09.915848Z"}}},{"cell_type":"code","source":"tweets_raw = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntweets_raw_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.219795Z","iopub.execute_input":"2021-07-20T06:55:47.220323Z","iopub.status.idle":"2021-07-20T06:55:47.267002Z","shell.execute_reply.started":"2021-07-20T06:55:47.220265Z","shell.execute_reply":"2021-07-20T06:55:47.265966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T05:05:19.267254Z","iopub.execute_input":"2021-07-20T05:05:19.267672Z","iopub.status.idle":"2021-07-20T05:05:19.272045Z","shell.execute_reply.started":"2021-07-20T05:05:19.267641Z","shell.execute_reply":"2021-07-20T05:05:19.270992Z"}}},{"cell_type":"code","source":"tweets_raw","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.268647Z","iopub.execute_input":"2021-07-20T06:55:47.269007Z","iopub.status.idle":"2021-07-20T06:55:47.286365Z","shell.execute_reply.started":"2021-07-20T06:55:47.268966Z","shell.execute_reply":"2021-07-20T06:55:47.285204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tweets about real disaster\ntweets_raw[tweets_raw[\"target\"] == 1][\"text\"].values[:5]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.288366Z","iopub.execute_input":"2021-07-20T06:55:47.288715Z","iopub.status.idle":"2021-07-20T06:55:47.303876Z","shell.execute_reply.started":"2021-07-20T06:55:47.288683Z","shell.execute_reply":"2021-07-20T06:55:47.302451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tweets not about real disaster\ntweets_raw[tweets_raw[\"target\"] == 0][\"text\"].values[:5]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.305685Z","iopub.execute_input":"2021-07-20T06:55:47.30613Z","iopub.status.idle":"2021-07-20T06:55:47.31722Z","shell.execute_reply.started":"2021-07-20T06:55:47.306095Z","shell.execute_reply":"2021-07-20T06:55:47.316165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing data\ntweets_raw.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.319165Z","iopub.execute_input":"2021-07-20T06:55:47.319601Z","iopub.status.idle":"2021-07-20T06:55:47.338094Z","shell.execute_reply.started":"2021-07-20T06:55:47.31957Z","shell.execute_reply":"2021-07-20T06:55:47.337058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Discarding columns except text and target\ntweets=tweets_raw[['text','target']]","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.339714Z","iopub.execute_input":"2021-07-20T06:55:47.340063Z","iopub.status.idle":"2021-07-20T06:55:47.354813Z","shell.execute_reply.started":"2021-07-20T06:55:47.340032Z","shell.execute_reply":"2021-07-20T06:55:47.353509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of occurences of real disasters\nsns.countplot(x=tweets[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.356426Z","iopub.execute_input":"2021-07-20T06:55:47.356927Z","iopub.status.idle":"2021-07-20T06:55:47.494661Z","shell.execute_reply.started":"2021-07-20T06:55:47.356852Z","shell.execute_reply":"2021-07-20T06:55:47.493945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'{tweets.target[tweets.target==1].count()/tweets.target.count()*100:.2f} % of tweets are labeled as disaster tweets in data')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.496174Z","iopub.execute_input":"2021-07-20T06:55:47.496591Z","iopub.status.idle":"2021-07-20T06:55:47.502686Z","shell.execute_reply.started":"2021-07-20T06:55:47.496539Z","shell.execute_reply":"2021-07-20T06:55:47.501636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing data\ntweets.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.50452Z","iopub.execute_input":"2021-07-20T06:55:47.50482Z","iopub.status.idle":"2021-07-20T06:55:47.526174Z","shell.execute_reply.started":"2021-07-20T06:55:47.504791Z","shell.execute_reply":"2021-07-20T06:55:47.524938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting dataset into training and validation sets","metadata":{}},{"cell_type":"code","source":"# Splitting data set into 80:20 ratio\ntrain, test = train_test_split(tweets,test_size=0.25,random_state=8)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.527699Z","iopub.execute_input":"2021-07-20T06:55:47.528203Z","iopub.status.idle":"2021-07-20T06:55:47.540775Z","shell.execute_reply.started":"2021-07-20T06:55:47.52817Z","shell.execute_reply":"2021-07-20T06:55:47.539732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text Cleaning and Normalization\n\nWe will:\n\n *  Lowercase all words in text\n *  Remove newline characters if any in text\n *  Remove punctuations\n *  Remove url's and links from text\n *   Remove tags from text\n *  Remove multiple spaces from text\n *   Remove special characters\n *  Remove stop words from text\n *   Apply stemming to normalize the text\n\n\n","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nstemmer = PorterStemmer()\ndef lower_text(text):\n    \"\"\"\n        function to convert text into lowercase\n        input: text\n        output: cleaned text\n    \"\"\"\n    text = text.lower() # lowering\n    return text\n    \ndef remove_newline(text):\n    \"\"\"\n        function to remove new line characters in text\n        input: text\n        output: cleaned text\n    \"\"\"\n    text = re.sub(r'\\n',' ', text)\n    return text\n\ndef remove_punctuations(text):\n    \"\"\"\n        function to remove punctuations from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    return text\n\n\ndef remove_links(text):\n    \"\"\"\n        function to links and urls from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n    \n    return text\n\ndef remove_tags(text):\n    \"\"\"\n        function to remove references and hashtags from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\"\",text)\n    return text\n    \ndef remove_multiplespaces(text):\n    \"\"\"\n        function to remove multiple spaces from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    return text\n\ndef remove_specialchars(text):\n    \"\"\"\n        function to remove special characters from text\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = re.sub(r'\\W', ' ', text)\n    return text\n\ndef remove_stopwords(text):\n    \"\"\"\n        function to tokenize the words using nltk word tokenizer and remove the stop words using nltk package's english stop words\n        input: text\n        output: cleaned text\n    \"\"\"    \n    text = ' '.join([word for word in word_tokenize(text) if word not in stop_words])\n    return text\n\ndef word_stemming(text):\n    \"\"\"\n        function to perform stemming using porter stemmer from nltk package\n        input: text\n        output: cleaned text\n    \"\"\"        \n    text=' '.join([stemmer.stem(word) for word in word_tokenize(text)])\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.542345Z","iopub.execute_input":"2021-07-20T06:55:47.54283Z","iopub.status.idle":"2021-07-20T06:55:47.556769Z","shell.execute_reply.started":"2021-07-20T06:55:47.542778Z","shell.execute_reply":"2021-07-20T06:55:47.556022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning and normalizing test dataset","metadata":{}},{"cell_type":"code","source":"# Covert text to lowercase\ntrain.text=train.text.apply(lambda text: lower_text(text))\n\n# Remove newlines\ntrain.text=train.text.apply(lambda text: remove_newline(text))\n\n# Remove punctuations\ntrain.text=train.text.apply(lambda text: remove_punctuations(text))\n\n# Remove links\ntrain.text=train.text.apply(lambda text: remove_links(text))\n\n# Remove tags\ntrain.text=train.text.apply(lambda text: remove_tags(text))\n\n# Remove multiple spaces\ntrain.text=train.text.apply(lambda text: remove_multiplespaces(text))\n\n# Remove special characters\ntrain.text=train.text.apply(lambda text: remove_specialchars(text))\n\n# Remove stopwords\ntrain.text=train.text.apply(lambda text: remove_stopwords(text))\n\n# Apply Stemming\ntrain.text=train.text.apply(lambda text: word_stemming(text))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:47.558029Z","iopub.execute_input":"2021-07-20T06:55:47.558605Z","iopub.status.idle":"2021-07-20T06:55:52.236746Z","shell.execute_reply.started":"2021-07-20T06:55:47.55856Z","shell.execute_reply":"2021-07-20T06:55:52.23594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning and normalizing validation dataset","metadata":{}},{"cell_type":"code","source":"# Covert text to lowercase\ntest.text=test.text.apply(lambda text: lower_text(text))\n\n# Remove newlines\ntest.text=test.text.apply(lambda text: remove_newline(text))\n\n# Remove punctuations\ntest.text=test.text.apply(lambda text: remove_punctuations(text))\n\n# Remove links\ntest.text=test.text.apply(lambda text: remove_links(text))\n\n# Remove tags\ntest.text=test.text.apply(lambda text: remove_tags(text))\n\n# Remove multiple spaces\ntest.text=test.text.apply(lambda text: remove_multiplespaces(text))\n\n# Remove special characters\ntest.text=test.text.apply(lambda text: remove_specialchars(text))\n\n# Remove stopwords\ntest.text=test.text.apply(lambda text: remove_stopwords(text))\n\n# Apply Stemming\ntest.text=test.text.apply(lambda text: word_stemming(text))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:52.238086Z","iopub.execute_input":"2021-07-20T06:55:52.238715Z","iopub.status.idle":"2021-07-20T06:55:53.817875Z","shell.execute_reply.started":"2021-07-20T06:55:52.238667Z","shell.execute_reply":"2021-07-20T06:55:53.816849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text visualization","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20, 12))\n\nwc_disaster = WordCloud(\n        width=800, height=600,\n        background_color='white',\n        stopwords=STOPWORDS\n    ).generate(' '.join(train[train.target==1]['text']))\n\nwc_nondisaster = WordCloud(\n        width=800, height=600,\n        background_color='white',\n        stopwords=STOPWORDS\n    ).generate(' '.join(train[train.target==0]['text']))\n\nax1.imshow(wc_disaster)\nax1.set_title(\"Word cloud of disaster tweets\", fontsize=20)\nax1.axis(\"off\")\n\nax2.imshow(wc_nondisaster)\nax2.set_title(\"Word cloud of non disaster tweets\", fontsize=20)\nax2.axis(\"off\")\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:53.819563Z","iopub.execute_input":"2021-07-20T06:55:53.820007Z","iopub.status.idle":"2021-07-20T06:55:56.979337Z","shell.execute_reply.started":"2021-07-20T06:55:53.819954Z","shell.execute_reply":"2021-07-20T06:55:56.978397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Text vectorization\n\nTo process the text data first we need to convert text data into numerical representation for systems to learn further from data. Vectorization is the process of converting a word ito a vector of numbers that contains the information in the word.\n\nWe will try:\n* Bag-of-words vectorization\n* TF-IDF vectorization","metadata":{}},{"cell_type":"code","source":"# Bag-of-words\ncount_vectorizer = CountVectorizer()\ntrain_vectors_bow = count_vectorizer.fit_transform(train[\"text\"])\ntest_vectors_bow = count_vectorizer.transform(test[\"text\"])\n\n# TF-IDF\ntfidf_vectorizer = TfidfVectorizer()\ntrain_vectors_tf = tfidf_vectorizer.fit_transform(train[\"text\"])\ntest_vectors_tf = tfidf_vectorizer.transform(test[\"text\"])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:56.982479Z","iopub.execute_input":"2021-07-20T06:55:56.98314Z","iopub.status.idle":"2021-07-20T06:55:57.332408Z","shell.execute_reply.started":"2021-07-20T06:55:56.983087Z","shell.execute_reply":"2021-07-20T06:55:57.331603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training model\n\nWe will use logistic regression to train this model\n.","metadata":{}},{"cell_type":"code","source":"# Bag-of-words\nclf = LogisticRegression()\n\nprint(\"Bag-of-words:\\n\")\nscores = cross_val_score(clf, train_vectors_bow, train[\"target\"], cv=5, scoring=\"f1\")\nfor k, score in zip(range(len(scores)),scores):\n    print(\"F1 Score for fold %d is %.2f \" % (k+1,score))\n    \nclf.fit(train_vectors_bow, train[\"target\"])\n\n# TF-IDF\nclf = LogisticRegression()\n\nprint(\"\\nTF-IDF:\\n\")\nscores = cross_val_score(clf, train_vectors_tf, train[\"target\"], cv=5, scoring=\"f1\")\nfor k, score in zip(range(len(scores)),scores):\n    print(\"F1 Score for fold %d is %.2f \" % (k+1,score))\n    \nclf.fit(train_vectors_tf, train[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:55:57.333698Z","iopub.execute_input":"2021-07-20T06:55:57.334165Z","iopub.status.idle":"2021-07-20T06:56:02.323068Z","shell.execute_reply.started":"2021-07-20T06:55:57.334128Z","shell.execute_reply":"2021-07-20T06:56:02.322125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making predictions using model\n","metadata":{}},{"cell_type":"code","source":"# Bag-of-words\ntest[\"pred\"] = clf.predict(test_vectors_bow)\n\nprint(\"We got %.1f%% accuracy on our test dataset\" % (float(accuracy_score(test.target, test.pred))*100))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:56:02.324334Z","iopub.execute_input":"2021-07-20T06:56:02.324664Z","iopub.status.idle":"2021-07-20T06:56:02.334517Z","shell.execute_reply.started":"2021-07-20T06:56:02.324633Z","shell.execute_reply":"2021-07-20T06:56:02.333475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bag-of-words\ntn, fp, fn, tp = confusion_matrix(test.target, test.pred).ravel()\ntot = confusion_matrix(test.target, test.pred).sum()\n\nprint(\"True Negative Rate: %.1f%%\" % ((tn/tot)*100))\nprint(\"False Positive Rate: %.1f%%\" % ((fp/tot)*100))\nprint(\"False Negative Rate: %.1f%%\" % ((fn/tot)*100))\nprint(\"True Positive Rate: %.1f%%\" % ((tp/tot)*100))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:56:02.33623Z","iopub.execute_input":"2021-07-20T06:56:02.337172Z","iopub.status.idle":"2021-07-20T06:56:02.363388Z","shell.execute_reply.started":"2021-07-20T06:56:02.337117Z","shell.execute_reply":"2021-07-20T06:56:02.362227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF-IDF\ntest[\"pred\"] = clf.predict(test_vectors_tf)\n\nprint(\"We got %.1f%% accuracy on our test dataset\" % (float(accuracy_score(test.target, test.pred))*100))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:56:02.365282Z","iopub.execute_input":"2021-07-20T06:56:02.366125Z","iopub.status.idle":"2021-07-20T06:56:02.376219Z","shell.execute_reply.started":"2021-07-20T06:56:02.366067Z","shell.execute_reply":"2021-07-20T06:56:02.375073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TF_IDF\ntn, fp, fn, tp = confusion_matrix(test.target, test.pred).ravel()\ntot = confusion_matrix(test.target, test.pred).sum()\n\nprint(\"True Negative Rate: %.1f%%\" % ((tn/tot)*100))\nprint(\"False Positive Rate: %.1f%%\" % ((fp/tot)*100))\nprint(\"False Negative Rate: %.1f%%\" % ((fn/tot)*100))\nprint(\"True Positive Rate: %.1f%%\" % ((tp/tot)*100))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:56:02.378095Z","iopub.execute_input":"2021-07-20T06:56:02.378881Z","iopub.status.idle":"2021-07-20T06:56:02.402931Z","shell.execute_reply.started":"2021-07-20T06:56:02.378829Z","shell.execute_reply":"2021-07-20T06:56:02.401747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Making predictions from competition test data","metadata":{}},{"cell_type":"code","source":"# Import data\ncomp_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n# Check for missing data\ncomp_test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:56:02.404861Z","iopub.execute_input":"2021-07-20T06:56:02.405717Z","iopub.status.idle":"2021-07-20T06:56:02.435258Z","shell.execute_reply.started":"2021-07-20T06:56:02.405663Z","shell.execute_reply":"2021-07-20T06:56:02.434254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp_test","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:56:02.4366Z","iopub.execute_input":"2021-07-20T06:56:02.437333Z","iopub.status.idle":"2021-07-20T06:56:02.452106Z","shell.execute_reply.started":"2021-07-20T06:56:02.437295Z","shell.execute_reply":"2021-07-20T06:56:02.451002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Isolating text column\ntest=comp_test[['text','id']]","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:56:02.453439Z","iopub.execute_input":"2021-07-20T06:56:02.454145Z","iopub.status.idle":"2021-07-20T06:56:02.465467Z","shell.execute_reply.started":"2021-07-20T06:56:02.45411Z","shell.execute_reply":"2021-07-20T06:56:02.464619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Cleaning and normalizing data\n\n# Covert text to lowercase\ntest.text=test.text.apply(lambda text: lower_text(text))\n\n# Remove newlines\ntest.text=test.text.apply(lambda text: remove_newline(text))\n\n# Remove punctuations\ntest.text=test.text.apply(lambda text: remove_punctuations(text))\n\n# Remove links\ntest.text=test.text.apply(lambda text: remove_links(text))\n\n# Remove tags\ntest.text=test.text.apply(lambda text: remove_tags(text))\n\n# Remove multiple spaces\ntest.text=test.text.apply(lambda text: remove_multiplespaces(text))\n\n# Remove special characters\ntest.text=test.text.apply(lambda text: remove_specialchars(text))\n\n# Remove stopwords\ntest.text=test.text.apply(lambda text: remove_stopwords(text))\n\n# Apply Stemming\ntest.text=test.text.apply(lambda text: word_stemming(text))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:56:02.466586Z","iopub.execute_input":"2021-07-20T06:56:02.46711Z","iopub.status.idle":"2021-07-20T06:56:05.17759Z","shell.execute_reply.started":"2021-07-20T06:56:02.467078Z","shell.execute_reply":"2021-07-20T06:56:05.176828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Predictions\n\n\ntest_vectors_tf = tfidf_vectorizer.transform(test[\"text\"])\n\ntest[\"pred\"] = clf.predict(test_vectors_tf)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:58:54.297596Z","iopub.execute_input":"2021-07-20T06:58:54.298015Z","iopub.status.idle":"2021-07-20T06:58:54.364484Z","shell.execute_reply.started":"2021-07-20T06:58:54.297976Z","shell.execute_reply":"2021-07-20T06:58:54.363382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exporting predictions","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\nsample_submission[\"target\"] = test[\"pred\"]\n\nsample_submission.to_csv(\"submission.csv\", index=False)\n\nsample_submission[:25]\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-20T06:58:55.82541Z","iopub.execute_input":"2021-07-20T06:58:55.825777Z","iopub.status.idle":"2021-07-20T06:58:55.85372Z","shell.execute_reply.started":"2021-07-20T06:58:55.825743Z","shell.execute_reply":"2021-07-20T06:58:55.852792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2021-07-20T07:00:01.412865Z","iopub.execute_input":"2021-07-20T07:00:01.413277Z","iopub.status.idle":"2021-07-20T07:00:01.426047Z","shell.execute_reply.started":"2021-07-20T07:00:01.413241Z","shell.execute_reply":"2021-07-20T07:00:01.424558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}