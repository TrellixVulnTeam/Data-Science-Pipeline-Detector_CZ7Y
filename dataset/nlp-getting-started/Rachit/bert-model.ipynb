{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"89q_obOYZiX9","outputId":"305f5e98-72d9-452a-efec-ec3e5cf47c9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"tV9Ckt0Tk8Ln","outputId":"7326585d-c1ce-455e-f614-515e96cb052c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils import data\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport re\n\nimport transformers\nfrom transformers import BertTokenizer, BertModel","metadata":{"id":"lJi5PiFt4aqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"zSTu3VZqmkP0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('train.csv')","metadata":{"id":"0cJuxHcK5Woa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"id":"J_cJLi7CF1yU","outputId":"6109cd06-1479-4f1c-bab4-f918860735a3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.text = train.text.str.lower()","metadata":{"id":"dB0-W27mlsfX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')","metadata":{"id":"PgMLaCT3mliE","outputId":"3c4d63ac-671b-4ad7-bce6-7397c416fb5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\n\ntknzr = TweetTokenizer(strip_handles=True)\nstop_words = set(stopwords.words('english'))\ncorpus = []\n\ndef clean_data(text):\n    # special characters\n    text = re.sub(r\"\\x89Û_\", \"\", text)\n    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n    text = re.sub(r\"\\x89Û÷\", \"\", text)\n    text = re.sub(r\"\\x89Ûª\", \"\", text)\n    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n    text = re.sub(r\"å_\", \"\", text)\n    text = re.sub(r\"\\x89Û¢\", \"\", text)\n    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n    text = re.sub(r\"åÊ\", \"\", text)\n    text = re.sub(r\"åÈ\", \"\", text)\n    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n    text = re.sub(r\"Ì©\", \"e\", text)\n    text = re.sub(r\"å¨\", \"\", text)\n    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n    text = re.sub(r\"åÇ\", \"\", text)\n    text = re.sub(r\"å£3million\", \"3 million\", text)\n    text = re.sub(r\"åÀ\", \"\", text)\n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    # remove numbers\n    text = re.sub(r'[0-9]', '', text)\n    \n    # remove punctuation and special chars (keep '!')\n    for p in string.punctuation.replace('!', ''):\n        text = text.replace(p, '')\n        \n    # remove urls\n    text = re.sub(r'http\\S+', '', text)\n    \n    # tokenize\n    text = tknzr.tokenize(text)\n    \n    # remove stopwords\n#     text = [w.lower() for w in text if not w in stop_words]\n#     corpus.append(text)\n    \n    # join back\n    text = ' '.join(text)\n    \n    return text","metadata":{"id":"IWxQx7DXl2Nt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Thanks to https://www.kaggle.com/rftexas/text-only-kfold-bert\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"€\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n     \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w/\" : \"with\",\n    \"w/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","metadata":{"id":"2EfQ-vdRmpdT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_abbrev_in_text(text):\n    t=[]\n    words=text.split()\n    t = [abbreviations[w.lower()] if w.lower() in abbreviations.keys() else w for w in words]\n    return ' '.join(t) ","metadata":{"id":"bZheqY-YmvEL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text']=train['text'].apply(clean_data)\ntrain['text']=train['text'].apply(convert_abbrev_in_text)","metadata":{"id":"B0txMjqHmvGq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text_len'] = train.text.apply(lambda x : len(x))","metadata":{"id":"TYPMGR4vmvJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.displot(train.text_len)","metadata":{"id":"EZpWeCqxmvMJ","outputId":"75a1fa4d-47da-4862-be6f-310b9e6bd720"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train, valid = train_test_split(train,test_size=0.2,stratify=train.target)","metadata":{"id":"VtDBQe-aE25c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.target.value_counts(), train.shape","metadata":{"id":"ome2YZIbF5s_","outputId":"48ef1d3d-4514-49e5-9b18-dc7ff4599c1c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid.target.value_counts(), valid.shape","metadata":{"id":"KLlC3cm9F80d","outputId":"16fdbfc0-9ff4-487c-a387-d6e35ac36065"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(data.Dataset):\n    \n    def __init__(self,texts,targets,max_len):\n        self.texts = texts\n        self.targets = targets\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self,item):\n        \n        text = str(self.texts[item])\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            max_length = self.max_len,\n            add_special_tokens = True,\n            return_attention_mask = True,\n            padding = 'max_length',\n            truncation = True,\n            return_token_type_ids = False,\n            return_tensors = 'pt'\n        )\n        \n        return {\n            'attention_mask': encoding['attention_mask'],\n            'ids': encoding['input_ids'],\n            'targets': torch.tensor(self.targets[item],dtype=torch.float)\n        }","metadata":{"id":"Dj8GfYNl5e3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyBertModel(nn.Module):\n    \n    def __init__(self):\n        super(MyBertModel,self).__init__()\n        self.model = BertModel.from_pretrained('bert-base-uncased',return_dict=False)\n        self.dropout = nn.Dropout(0.3)\n        self.linear = nn.Linear(768,1)\n        \n    def forward(self,input_ids,attention_mask):\n        _,polled_output = self.model(input_ids = input_ids,attention_mask = attention_mask)\n        output = self.dropout(polled_output)\n        return self.linear(output)        ","metadata":{"id":"XDJdoxT_52o0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_function(data_loader,model,optimizer,device):\n    \n    model.train()\n    losses = []\n    for batch, data in enumerate(data_loader):\n\n        dim = data['ids'].shape[0]\n        ids = data['ids'].view(dim,-1).to(device)\n        mask = data['attention_mask'].view(dim,-1).to(device)\n        targets = data['targets'].view(dim,-1).to(device)\n\n        optimizer.zero_grad()\n        \n        outputs = model(\n                    input_ids = ids,\n                    attention_mask = mask\n                )\n        \n        loss = nn.BCEWithLogitsLoss()\n        loss_output = loss(outputs,targets).to(device)\n        losses.append(loss_output.item())\n        loss_output.backward()\n        optimizer.step()\n\n    print('mean train loss', np.mean(losses))","metadata":{"id":"6NmRvqgm56Ds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def valid_function(data_loader,model,device):\n    \n    model.eval()\n    \n    val_targets = []\n    val_outputs = []\n    \n    with torch.no_grad():\n\n      for batch, data in enumerate(data_loader):\n\n          dim = data['ids'].shape[0]\n          ids = data['ids'].view(dim,-1).to(device)\n          mask = data['attention_mask'].view(dim,-1).to(device)\n          targets = data['targets'].view(dim,-1).to(device)\n\n          outputs = model(\n                      input_ids = ids,\n                      attention_mask = mask\n                  )\n          \n          val_outputs.extend(nn.Sigmoid()(outputs).cpu().detach().numpy().tolist())\n          val_targets.extend(targets.cpu().detach().numpy().tolist())\n    \n    return val_targets, val_outputs","metadata":{"id":"Xcrji1VrPrVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = MyDataset(train.text.values,train.target.values,256)\n\ntrain_data_loader = torch.utils.data.DataLoader(\n                        dataset=train_dataset,\n                        batch_size=128,\n                        num_workers=1\n                    )\n\nvalid_dataset = MyDataset(valid.text.values,valid.target.values,256)\n\nvalid_data_loader = torch.utils.data.DataLoader(\n                        dataset=valid_dataset,\n                        batch_size=128,\n                        num_workers=1\n                    )\n","metadata":{"id":"p-r6fl9L5-Qk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MyBertModel()","metadata":{"id":"CYD672Fx5-St","outputId":"bf4c0bd2-7be0-49bf-a962-2ee812a5c075"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i,j in model.model.named_parameters():\n  \n  if re.search('encoder\\.layer\\.[0-9][^1]*\\.',i):\n    j.requires_grad = False\n\n  if re.search('embeddings\\.',i):\n    j.requires_grad = False\n  # j.requires_grad_ = False\n  print(j.requires_grad)\n  print(i)","metadata":{"id":"5M3WfTjDlhA_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(list(model.parameters()))","metadata":{"id":"PVD5LcoF5-Uq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","metadata":{"id":"QkTfdWLKLa-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"id":"1NITwgRxJ5yt","outputId":"d9c43a86-1f95-4937-f574-ebacdfbbe0b0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"TzsFZel9aPW_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)\n\nmax_f1 = 0\n\nfor epoch in range(5):\n  \n  train_function(train_data_loader,model,optimizer,device)\n  targets,outputs = valid_function(valid_data_loader,model,device)\n\n  outputs = np.array(outputs)>=0.5\n  f1_score = sklearn.metrics.f1_score(targets,outputs,average=\"macro\")\n  print(f'for epoch {epoch} validation f1 score is {f1_score}')\n\n  if max_f1<f1_score:\n    max_f1 = f1_score\n    torch.save(model.state_dict(), 'my_model')","metadata":{"id":"nmXMPgZZ7W7B","outputId":"62bab2f6-f592-4479-941d-dc5c57116c61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_f1","metadata":{"id":"R8FIMOXDsiXg","outputId":"6369eaba-8d52-474d-8a53-2389ebd29110"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('test.csv')","metadata":{"id":"e_XlOv3df6Eh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"id":"cynXZLUQf6JC","outputId":"0dfe61f7-ec4b-410c-97ce-67a39ff6c1b7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.text = test.text.str.lower()","metadata":{"id":"WNjjP6rjlk36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['text']=test['text'].apply(clean_data)\ntest['text']=test['text'].apply(convert_abbrev_in_text)","metadata":{"id":"W_XkPS0VnU91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nfrom collections import defaultdict\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"id":"Gu6g4IB1jc6r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MyBertModel()","metadata":{"id":"xv5rMeasxuej","outputId":"ae460dc0-c4c7-4dd9-bceb-78eed48d7028"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('my_model'))","metadata":{"id":"TWHFFLlXxz50","outputId":"6a368931-a2a5-43e4-dffa-7f85b8e27ebc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)\nmodel.eval()","metadata":{"id":"a4aH9e0dyF2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l = defaultdict(list)\n\nwith torch.no_grad():\n\n  for idx in test.index:\n    \n    data = tokenizer.encode_plus(\n        str(test.loc[idx,'text']),\n        max_length = 256,\n        add_special_tokens = True,\n        return_attention_mask = True,\n        padding = 'max_length',\n        truncation = True,\n        return_token_type_ids = False,\n        return_tensors = 'pt'\n    )\n    \n    output = model(\n        input_ids = data['input_ids'].to(device),\n        attention_mask = data['attention_mask'].to(device)\n    )\n\n    l['id'].append(test.loc[idx,'id'])\n\n    if nn.Sigmoid()(output)>=0.5:\n      l['target'].append(1)\n    else:\n        l['target'].append(0)\n    ","metadata":{"id":"PNCDH_2Cf6LC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub = pd.DataFrame(l)","metadata":{"id":"EGDP223ZkXf3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.to_csv('submission.csv',index=False)","metadata":{"id":"BkUsMor1kXvr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('submission.csv')","metadata":{"id":"LOKgqyIykpaa","outputId":"e1f70f6c-3186-4df4-fff6-86a947a1420c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\n","metadata":{"id":"ggV_mP7X7XSx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Current Score: 0.82500 F1 score \n\n## Future Work:\n1. Will add Roberta model.\n2. Test the model with different thresholds.","metadata":{}},{"cell_type":"code","source":"","metadata":{"id":"ebza1duS7XXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"IvsfC6wH5-Yx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"Xe7CWDUp5-b2"},"execution_count":null,"outputs":[]}]}