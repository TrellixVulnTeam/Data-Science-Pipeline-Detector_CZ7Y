{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aim\n#### Aim of this notebook is to create model for classifying the tweets into disaster and non disaster using LSTM","metadata":{}},{"cell_type":"markdown","source":"#### So, let's start our code by importing libraries required","metadata":{}},{"cell_type":"code","source":"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's import the Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape, test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plotting the number of  disaster and non disaster tweets","metadata":{}},{"cell_type":"code","source":"sns.countplot(df[\"target\"]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"target\"].value_counts(normalize = True) #normalized value counts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Creating a function for plotting a histogram (for length of tweets)","metadata":{}},{"cell_type":"code","source":"def len_plot(data, name):\n  length = [len(sent.split()) for sent in data]\n  plt.hist(length)\n  plt.title(name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_plot(df[df[\"target\"]==0][\"text\"], \"Not Disaster\") #passing non disaster tweets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_plot(df[df[\"target\"]==1][\"text\"], \"Disaster\") #passing disaster tweets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Separating input and output features","metadata":{}},{"cell_type":"code","source":"X = df[\"text\"] # indpendent\ny = df[\"target\"] # dependent\ny = np.array(y) # converting into array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Function for finding the number of unique words in our dataset","metadata":{}},{"cell_type":"code","source":"def unique_words(text):\n  unique_words_list = []\n  for sent in tqdm(text):\n    for word in sent.split():\n      if word.lower() not in unique_words_list:\n        unique_words_list.append(word.lower())\n      else:\n        pass\n  return unique_words_list\nun_words = unique_words(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total number of unique words :\",len(un_words))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"un_words[:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As it is a twitter dataset, it contains several words starting with \"@\" and \"#\". Let's find this words","metadata":{}},{"cell_type":"markdown","source":"#### words with starting letter \"#\"","metadata":{}},{"cell_type":"code","source":"SYMBOL1 = \"#\"\nwords_sym1 = [word for word in un_words if word.startswith(SYMBOL1)]\nlen(words_sym1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words_sym1[:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### words with starting letter \"@\"","metadata":{}},{"cell_type":"code","source":"# words with starting letter \"@\"\nSYMBOL2 = \"@\"\nwords_sym2 = [word for word in un_words if word.startswith(SYMBOL2)]\nlen(words_sym2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words_sym2[:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since many of the words starting with \"@\" doesn't give any impact to our model accuracy, so we need to remove it","metadata":{}},{"cell_type":"markdown","source":"#### Function for url removing","metadata":{}},{"cell_type":"code","source":"def remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lemmatizer","metadata":{}},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nwl = WordNetLemmatizer()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Preprocessing function","metadata":{}},{"cell_type":"code","source":"def preprocessing(text):\n  \n  tweets = []\n  for tweet in tqdm(text):\n    tweet = tweet.lower() # converting to lower case\n    tweet =  remove_urls(tweet) # url removing\n    tweet = re.sub(r'@\\w+',  '', tweet).strip() # removing the words start with \"@\"\n    tweet = re.sub(\"[^a-zA-Z0-9 ']\", \"\", tweet) # removing unwanted symbols\n    tweet = tweet.split()\n    tweet1 = [wl.lemmatize(word) for word in tweet if word not in set(stopwords.words(\"english\"))] #lemmatization and stopwrds removal\n    tweet1 = \" \".join(tweet1)\n    tweets.append(tweet1)\n  return tweets\n\ntweets = preprocessing(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets[:50]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM","metadata":{}},{"cell_type":"code","source":"# importing libraries\nimport tensorflow as tf\ntf.__version__\n\nfrom tensorflow.keras.layers import (Embedding,\n                                     LSTM,\n                                     Dense,\n                                     Dropout,\n                                     GlobalMaxPool1D,\n                                     BatchNormalization)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import one_hot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Performing onehot encoding","metadata":{}},{"cell_type":"code","source":"VOC_SIZE = 30000\nonehot_repr = [one_hot(words, VOC_SIZE) for words in tweets]\nonehot_repr[100:110]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding sentence length for each tweets\nword_length = []\nfor i in onehot_repr:\n  word_length.append(len(i))\n\nlen(word_length)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_length[1100:1150]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting graph (length of the tweets vs Numbers)\nplt.hist(word_length)\nplt.xlabel(\"Length of Words\")\nplt.ylabel(\"Nos\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(word_length) # lenth of the longest tweet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Padding the Sequence","metadata":{}},{"cell_type":"code","source":"SENT_LENGTH = 15\nembedded_docs = pad_sequences(onehot_repr, padding=\"post\", maxlen=SENT_LENGTH)\nembedded_docs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Function for model creation","metadata":{}},{"cell_type":"code","source":"def create_model():\n  VECT_FEATURES = 32\n  model = Sequential()\n  model.add(Embedding(VOC_SIZE,\n                      VECT_FEATURES,\n                      input_length=SENT_LENGTH))\n  model.add(LSTM(100, return_sequences = True))\n  model.add(GlobalMaxPool1D())\n  model.add(BatchNormalization())\n  model.add(Dropout(0.5))\n  model.add(Dense(10, activation=\"relu\"))\n  model.add(Dropout(0.2))\n  model.add(Dense(1, activation = \"sigmoid\"))\n  return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model()\nmodel.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"]) # compiling\nmodel.summary() #summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training the Model","metadata":{}},{"cell_type":"code","source":"history = model.fit(embedded_docs, y, epochs=6, batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Plotting the graph of model accuracy and loss","metadata":{}},{"cell_type":"code","source":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\nax1.plot(history.history[\"accuracy\"])\nax1.set_title(\"Accuracy\")\nax1.set_xlabel(\"Epochs\")\nax1.set_ylabel(\"Accuracy\")\n\nax2.plot(history.history[\"loss\"])\nax2.set_title(\"Loss\")\nax2.set_xlabel(\"Epochs\")\nax2.set_ylabel(\"Loss\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}