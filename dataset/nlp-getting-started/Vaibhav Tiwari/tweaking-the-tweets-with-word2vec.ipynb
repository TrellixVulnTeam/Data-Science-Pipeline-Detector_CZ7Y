{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"### This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"tweet= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest= pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet[tweet['target']==1]['text'].values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x= tweet.target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x.index,x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y= tweet[tweet['target']==1]['text'].str.split()\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_len=tweet[tweet['target']==1]['text'].str.split().str.len()\nprint(word_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(word_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus= []\n    \n    for l in y:\n        for q in l:\n            corpus.append(q)\n    return(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus= create_corpus(1)\n\ndic= defaultdict(int)\n\nfor i in corpus:\n    if i in stop:\n        dic[i]= dic[i]+1\ntop = sorted(dic.items(), key= lambda x: x[1], reverse= True)[:10]\nm,n =zip(*top)\nplt.bar(m,n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.concat([tweet, test])\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sterilization(data):\n    \n    data = re.sub('https?://\\S+|www\\.\\S+', '', data)\n    data = re.sub('<.*?>', '', data)\n    emoj = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    emoj.sub(r'', data)\n    data = data.lower()\n    data = data.translate(str.maketrans('','', string.punctuation))\n    data = re.sub(r'\\[.*?\\]', '', data)\n    data = re.sub(r'\\w*\\d\\w*','', data)\n    \n\n    return data\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['text']=df['text'].apply(lambda x : sterilization(x))\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\n\ntokenizer = RegexpTokenizer(r'\\w+')\n\ndf[\"tokens\"] = df[\"text\"].apply(tokenizer.tokenize)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**IMPORTING WORD2VEC pretrained model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"word2vec_path = \"../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**'filtered' function is for removing stop words**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def filtered(text):\n    words= [w for w in text if w not in stop]\n    \n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['tokens']= df['tokens'].apply(lambda x: filtered(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Taking average of all the vector weights of words present in a single tweet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n    if len(tokens_list)<1:\n        return np.zeros(k)\n    if generate_missing:\n        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n    else:\n        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n   \n    length = len(vectorized)\n    summed = np.sum(vectorized, axis=0)\n    averaged = np.divide(summed, length)\n    return averaged\n\ndef get_word2vec_embeddings(vectors, df, generate_missing=False):\n    embeddings = df['tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n    return list(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweet.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train= df[:7613]\ny_train= X_train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test= df[7613:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings = get_word2vec_embeddings(word2vec, X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, y_train, test_size= 0.2, random_state=40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf_w2v = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n                         multi_class='multinomial', random_state=40)\nclf_w2v.fit(X_train_word2vec, y_train_word2vec)\ny_predicted_word2vec = clf_w2v.predict(X_test_word2vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n\ndef get_metrics(y_test, y_predicted):  \n    # true positives / (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    # true positives / (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    \n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    \n    # true positives + true negatives/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1\n\naccuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n                                                                       recall_word2vec, f1_word2vec))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_embeds= get_word2vec_embeddings(word2vec, X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = XGBClassifier(colsample_bytree=0.7, learning_rate= 0.03, max_depth= 10,\n                    min_child_weight=11, missing= -999, n_estimators= 1200,\n                    nthread= 4, objective='binary:logistic', seed=1337, silent=1, subsample=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XTa= np.array(X_train_word2vec)\nyTa= np.array(y_train_word2vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(XTa,yTa)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predicted_word2vec = clf.predict(X_test_word2vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predicted_word2vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_word2vec, precision_word2vec, \n                                                                       recall_word2vec, f1_word2vec))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n\nsubmission[\"target\"] = clf.predict(test_embeds)\n\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Download the generated CSV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}