{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom sklearn.model_selection import train_test_split\n\nimport re\nfrom nltk.corpus import stopwords\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[1] Load dataset, both train & test","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n\ndf_train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[2] Preprocessing","metadata":{}},{"cell_type":"markdown","source":"[2-1] Drop unnecessary columns","metadata":{}},{"cell_type":"code","source":"df_train.drop(['keyword','location'],axis=1,inplace=True)\ndf_test.drop(['keyword','location'],axis=1,inplace=True)\n\nprint(f\"train shape >> {df_train.shape}\")\nprint(f\"test shape >> {df_test.shape}\")\n\n\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[2-2] Check if imbalanced","metadata":{}},{"cell_type":"code","source":"sns.set_style('darkgrid')\nsns.countplot(x=df_train.target)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-> I think the balance is ok","metadata":{}},{"cell_type":"markdown","source":"[2-3] Text Preprocessing","metadata":{}},{"cell_type":"markdown","source":"(1) Remove #, @chunk, urls, shortwords(length is either 1 or 2), stopwords","metadata":{}},{"cell_type":"code","source":"df_train.text.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"at_chunk = re.compile(r\"@[a-zA-Z0-9]*\")\nurl = re.compile(r\"https?:/+[a-zA-Z0-9./]*\")\nshortword = re.compile(r\"\\b\\w{1,2}\\b\")\n\nstop_words = set(stopwords.words('english'))\n\ndef clean(text):\n    text = re.sub('#','',text)\n    text = re.sub(at_chunk,'',text)\n    text = re.sub(url,'',text)\n    text = re.sub(shortword,'',text)\n    \n    text = text.split()\n    text = [w for w in text if w not in stop_words]\n    \n    text = \" \".join(text)\n    text = text.strip()\n    \n    return text\n\ndf_train.text = df_train.text.apply(clean)\ndf_test.text = df_test.text.apply(clean)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.text.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(2) Check NULL","metadata":{}},{"cell_type":"code","source":"df_train.text.isnull().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(3) Tokenizer","metadata":{}},{"cell_type":"code","source":"tok = Tokenizer()\n\ntrain_text = df_train.text\ntrain_label = df_train.target\ntest_text = df_test.text\n\ntok.fit_on_texts(train_text)\n\nword_size = len(tok.index_word)\nvocab_size = word_size+1\n\nprint(f\"{word_size} words are used!\")\n\nprint(\"Tokenizing train texts\\n\")\ntrain_text = tok.texts_to_sequences(train_text)\nprint(\"Tokenizing train texts finished!\\n\")\n\nprint(\"Tokenizng test texts with the same tokenizer\\n\")\ntest_text = tok.texts_to_sequences(test_text)\nprint(\"Tokenizing test texts finished!\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(4) Padding","metadata":{}},{"cell_type":"code","source":"lengths = [len(s) for s in train_text]\nprint(f\"Max of sequence size >> {np.max(lengths)}\")\nprint(f\"Average of sequence size >> {int(np.round(np.mean(lengths)))}\")\n\nplt.hist(lengths,bins=100)\nplt.show()\n\nsequence_size=21","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_text = pad_sequences(train_text,maxlen=sequence_size,padding='post',truncating='post')\ntest_text = pad_sequences(test_text,maxlen=sequence_size,padding='post',truncating='post')\n\nprint(f\"train text shape >> {train_text.shape}\")\nprint(f\"test text shape >> {test_text.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_text\ntest_data = test_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[3] Modelling","metadata":{}},{"cell_type":"markdown","source":"(1-1) Bidirectional stacked LSTM without pre-trained Embedding Vectors from GLOVE : return only last hidden cell on the last lstm layer","metadata":{}},{"cell_type":"code","source":"from keras.layers import Input,Embedding,Bidirectional,LSTM,TimeDistributed,Dense,Dropout,BatchNormalization,GlobalMaxPool1D,GlobalAveragePooling1D\nfrom keras.utils import plot_model\nfrom keras.models import Model\n\n#vocab_size\n#sequence_size\nword_vec_size = 128\nhidden_size = 128\n\ndef create_lstm1():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,mask_zero=True)(X)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=False))(H)\n    H = BatchNormalization()(H)\n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    \n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm1 = create_lstm1()\nhist = lstm1.fit(train_data,train_label,validation_split=0.1,epochs=10,batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(lstm1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(1-2) Bidirectional Stacked LSTM model without using pre-trained Embedding Vectors : return all the hidden cells and use global average pooling","metadata":{}},{"cell_type":"code","source":"def create_lstm2():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,mask_zero=True)(X)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = GlobalAveragePooling1D()(H)\n    H = BatchNormalization()(H)\n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    \n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm2 = create_lstm2()\nhist = lstm2.fit(train_data,train_label,validation_split=0.1,epochs=10,batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(lstm2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(1-3) Bidirectional Stacked LSTM model without using pre-trained Embedding Vectors : return all the hidden cells and use global max pooling","metadata":{}},{"cell_type":"code","source":"def create_lstm3():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,mask_zero=True)(X)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = GlobalMaxPool1D()(H)\n    H = BatchNormalization()(H)\n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    \n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm3 = create_lstm3()\nhist = lstm3.fit(train_data,train_label,validation_split=0.1,epochs=7,batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(lstm3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(2-1) Bidirectional Stacked LSTM model with pre-trained Embedding vectors","metadata":{}},{"cell_type":"code","source":"import os\n\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove*.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dict = dict()\n\nf = open(os.path.join('glove.6B.200d.txt'),encoding='utf-8')\n\nfor line in f:\n    tokens = line.split()\n    word = tokens[0]\n    vector = tokens[1:]\n    vector =  np.asarray(vector,dtype='float32')\n    embedding_dict[word] = vector\n    \nf.close()\n\nembedding_size = len(embedding_dict['world'])\nprint(f\"There are {len(embedding_dict)} embedding vectors in total\")\nprint(f\"The size of embedding vector here >> {embedding_size}\")\n\nembedding_matrix =  np.zeros((vocab_size,embedding_size))\nfor word,idx in tok.word_index.items():\n    vector = embedding_dict.get(word)\n    if vector is not None:\n        embedding_matrix[idx] = np.asarray(vector,dtype='float32')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#embedding_size\ndef create_lstm_glove1():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,embedding_size,input_length=sequence_size,weights=[embedding_matrix],trainable=False,mask_zero=True)(X)\n    H = Dropout(0.2)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = GlobalMaxPool1D()(H)\n    H = BatchNormalization()(H)\n    \n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_glove1 = create_lstm_glove1()\nhist = lstm_glove1.fit(train_data,train_label,validation_split=0.1,epochs=6,batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(lstm_glove1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#embedding_size\ndef create_lstm_glove2():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,embedding_size,input_length=sequence_size,weights=[embedding_matrix],trainable=False,mask_zero=True)(X)\n    H = Dropout(0.2)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Dropout(0.1)(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = GlobalAveragePooling1D()(H)\n    H = BatchNormalization()(H)\n    \n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lstm_glove2 = create_lstm_glove2()\nhist = lstm_glove2.fit(train_data,train_label,validation_split=0.1,epochs=7,batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(lstm_glove2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(3-1) Multi-kernel Conv1D model using pre-trained Embedding vectors","metadata":{}},{"cell_type":"code","source":"from keras.layers import Conv1D,Concatenate,LeakyReLU,Flatten\n\ndef create_conv1():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,embedding_size,input_length=sequence_size,mask_zero=True,weights=[embedding_matrix],trainable=True)(X)\n    H = Dropout(0.1)(H)\n    \n    conv_blocks=[]\n    kernel_filters=[256,256,128,128]\n    kernel_sizes=[3,4,5,6]\n    \n    for i in range(len(kernel_sizes)):\n        conv = Conv1D(filters=kernel_filters[i],kernel_size=kernel_sizes[i])(H)\n        conv = GlobalMaxPool1D()(conv)\n        conv = Flatten()(conv)\n        conv_blocks.append(conv)\n    \n    H = Concatenate()(conv_blocks)\n    H = Dropout(0.1)(H)\n    \n    H = Dense(256)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    H = Dense(32)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    Y = Dense(1,activation='sigmoid')(H)\n        \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv1 = create_conv1()\nhist = conv1.fit(train_data,train_label,validation_split=0.1,epochs=7,batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(conv1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(3-2) Multi-kernel Stacked Conv1D model using pre-trained Embedding vectors","metadata":{}},{"cell_type":"code","source":"def create_conv2():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,embedding_size,input_length=sequence_size,mask_zero=True,weights=[embedding_matrix],trainable=True)(X)\n    H = Dropout(0.1)(H)\n    \n    conv_blocks=[]\n    kernel_filters=[256,256,128,128]\n    kernel_sizes=[3,4,5,6]\n    \n    for i in range(len(kernel_sizes)):\n        conv = Conv1D(filters=kernel_filters[i],kernel_size=kernel_sizes[i])(H)\n        conv = Conv1D(filters=kernel_filters[i],kernel_size=kernel_sizes[i])(conv)\n        conv = GlobalMaxPool1D()(conv)\n        conv = Flatten()(conv)\n        conv_blocks.append(conv)\n    \n    H = Concatenate()(conv_blocks)\n    H = Dropout(0.1)(H)\n    \n    H = Dense(256)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    H = Dense(32)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    Y = Dense(1,activation='sigmoid')(H)\n        \n    model = Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv2 = create_conv2()\nhist = conv2.fit(train_data,train_label,validation_split=0.1,epochs=7,batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(conv2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(4) BERT","metadata":{}},{"cell_type":"markdown","source":"(5) Submission","metadata":{}},{"cell_type":"code","source":"#lstm2, lstm_glove1, conv1\n#test_data\n\ntest_id = df_test.id\n\ndef get_submission(model,filename):\n    pred = model.predict(test_data)\n    pred = pred.reshape(-1)\n    submission = pd.DataFrame({\n        'id':test_id,\n        'target':pred\n    })\n    submission.target = submission.target.apply(lambda x:1 if x>0.5 else 0)\n    print(\"Making submission DataFrame Finished!\")\n    submission.to_csv(filename+\".csv\",index=False)\n    print(\"Making CSV file Finished!\\n\\n\")\n    return submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_submission(lstm2,\"lstm2\")\nget_submission(lstm_glove1,\"lstm_glove1\")\nget_submission(conv1,\"conv1\")\n\nprint(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}