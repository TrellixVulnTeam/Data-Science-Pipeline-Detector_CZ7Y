{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#libraries\nimport pandas as pd\nimport spacy\nfrom spacy import displacy\nimport seaborn as sns\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-17T19:09:53.160168Z","iopub.execute_input":"2022-03-17T19:09:53.160621Z","iopub.status.idle":"2022-03-17T19:09:55.118242Z","shell.execute_reply.started":"2022-03-17T19:09:53.160577Z","shell.execute_reply":"2022-03-17T19:09:55.116915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1-Inspection of the datasets.  ","metadata":{}},{"cell_type":"code","source":"dtrain=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\nsample=pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\ndtest=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-03-17T19:09:55.120551Z","iopub.execute_input":"2022-03-17T19:09:55.120888Z","iopub.status.idle":"2022-03-17T19:09:55.200699Z","shell.execute_reply.started":"2022-03-17T19:09:55.120859Z","shell.execute_reply":"2022-03-17T19:09:55.199541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inspection of train data\ndtrain.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:55.201925Z","iopub.execute_input":"2022-03-17T19:09:55.202224Z","iopub.status.idle":"2022-03-17T19:09:55.22381Z","shell.execute_reply.started":"2022-03-17T19:09:55.202196Z","shell.execute_reply":"2022-03-17T19:09:55.222626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(y='target',data=dtrain,palette='Set3')#are there more tweets classified as 0 or as 1?","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:55.225286Z","iopub.execute_input":"2022-03-17T19:09:55.22565Z","iopub.status.idle":"2022-03-17T19:09:55.49833Z","shell.execute_reply.started":"2022-03-17T19:09:55.225618Z","shell.execute_reply":"2022-03-17T19:09:55.497215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are more tweets classified as non disaster(0). ","metadata":{}},{"cell_type":"code","source":"#from where are these tweets?\nsns.countplot(y='location',data=dtrain,palette='Set3',order=dtrain['location'].value_counts().iloc[:6].index)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:55.501838Z","iopub.execute_input":"2022-03-17T19:09:55.502213Z","iopub.status.idle":"2022-03-17T19:09:55.668374Z","shell.execute_reply.started":"2022-03-17T19:09:55.502177Z","shell.execute_reply":"2022-03-17T19:09:55.667503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the top-5 of locations where someone sent a tweet. We can improve this graph.","metadata":{}},{"cell_type":"code","source":"#this is for replace the cities by their country\ndtrain['location']=dtrain['location'].replace(['United States','New York','Los Angeles','Los Angeles, CA', 'Washington, DC'],'USA')\ndtrain['location']=dtrain['location'].replace(['London'],'UK')\ndtrain['location']=dtrain['location'].replace(['Mumbai'],'India')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:55.670625Z","iopub.execute_input":"2022-03-17T19:09:55.671145Z","iopub.status.idle":"2022-03-17T19:09:55.682996Z","shell.execute_reply.started":"2022-03-17T19:09:55.671104Z","shell.execute_reply":"2022-03-17T19:09:55.682135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(y='location',data=dtrain,palette='Set3',order=dtrain['location'].value_counts().iloc[:6].index)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:55.684095Z","iopub.execute_input":"2022-03-17T19:09:55.684521Z","iopub.status.idle":"2022-03-17T19:09:55.859672Z","shell.execute_reply.started":"2022-03-17T19:09:55.684489Z","shell.execute_reply":"2022-03-17T19:09:55.858742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(y='keyword',data=dtrain,palette='Set2',order=dtrain['keyword'].value_counts().iloc[:3].index)#Top 3 of keywords more used","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:55.860786Z","iopub.execute_input":"2022-03-17T19:09:55.861209Z","iopub.status.idle":"2022-03-17T19:09:55.994729Z","shell.execute_reply.started":"2022-03-17T19:09:55.861178Z","shell.execute_reply":"2022-03-17T19:09:55.993601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2-Introduction to spaCy.","metadata":{}},{"cell_type":"markdown","source":"SpaCy is an alternative to using NLTK. They are pretty similar but spaCy has cool utilities like these ones:","metadata":{}},{"cell_type":"markdown","source":"First we have to load the english model for spaCy.","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:55.996345Z","iopub.execute_input":"2022-03-17T19:09:55.997018Z","iopub.status.idle":"2022-03-17T19:09:57.120206Z","shell.execute_reply.started":"2022-03-17T19:09:55.996949Z","shell.execute_reply":"2022-03-17T19:09:57.119232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#First one: Entity Recognition\ndoc=nlp(dtrain['text'][58])\ndisplacy.render(doc,style='ent')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.121644Z","iopub.execute_input":"2022-03-17T19:09:57.121944Z","iopub.status.idle":"2022-03-17T19:09:57.161137Z","shell.execute_reply.started":"2022-03-17T19:09:57.121916Z","shell.execute_reply":"2022-03-17T19:09:57.16018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"doc=nlp(dtrain['text'][10])\ndisplacy.render(doc,style='ent')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.162771Z","iopub.execute_input":"2022-03-17T19:09:57.163175Z","iopub.status.idle":"2022-03-17T19:09:57.185318Z","shell.execute_reply.started":"2022-03-17T19:09:57.163129Z","shell.execute_reply":"2022-03-17T19:09:57.184188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#linguistic annotations\ntokenized_text = pd.DataFrame()\n#describe the words in the sentence before\nfor i, token in enumerate(doc):\n    tokenized_text.loc[i, 'text'] = token.text\n    tokenized_text.loc[i, 'type'] = token.pos_\n    tokenized_text.loc[i, 'lemma'] = token.lemma_,\n    tokenized_text.loc[i, 'is_alphabetic'] = token.is_alpha\n    tokenized_text.loc[i, 'is_stop'] = token.is_stop\n    tokenized_text.loc[i, 'is_punctuation'] = token.is_punct\n    tokenized_text.loc[i, 'sentiment'] = token.sentiment\n    \n    \n\ntokenized_text[:30]","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.186788Z","iopub.execute_input":"2022-03-17T19:09:57.187426Z","iopub.status.idle":"2022-03-17T19:09:57.25829Z","shell.execute_reply.started":"2022-03-17T19:09:57.187348Z","shell.execute_reply":"2022-03-17T19:09:57.257315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* text: the text of the word\n\n* type: type of the word. Is it an adverb? Is it a preposition?\n\n* lemma: the base form of the word.\n\n* is_alpha: does the word consist of alphabetic characters? \n\n* is_stop: is the word part of a stop list?\n\n* is_puntuaction: is the word puntuaction?\n\n* sentiment: A scalar value indicating the positivity or negativity of the token.\n\n","metadata":{}},{"cell_type":"code","source":"#dependency parser- see the relations between the words \ndisplacy.render(doc,style='dep',jupyter='true')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.259504Z","iopub.execute_input":"2022-03-17T19:09:57.259799Z","iopub.status.idle":"2022-03-17T19:09:57.268594Z","shell.execute_reply.started":"2022-03-17T19:09:57.259771Z","shell.execute_reply":"2022-03-17T19:09:57.267497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#if you don't understand a tag displayed\nspacy.explain('ADP')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.270334Z","iopub.execute_input":"2022-03-17T19:09:57.270978Z","iopub.status.idle":"2022-03-17T19:09:57.28208Z","shell.execute_reply.started":"2022-03-17T19:09:57.27092Z","shell.execute_reply":"2022-03-17T19:09:57.28099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3-Build the model.","metadata":{}},{"cell_type":"markdown","source":"For building the model I am following the guide made by Susan Li: https://towardsdatascience.com/machine-learning-for-text-classification-using-spacy-in-python-b276b4051a49. ","metadata":{}},{"cell_type":"code","source":"import string\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.283703Z","iopub.execute_input":"2022-03-17T19:09:57.284406Z","iopub.status.idle":"2022-03-17T19:09:57.298972Z","shell.execute_reply.started":"2022-03-17T19:09:57.284339Z","shell.execute_reply":"2022-03-17T19:09:57.297794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#A-FIRST STEP: TOKEN THE DATA. We are going to remove stopwords and puntuaction from each sentence.\n\n# Create a list of punctuation marks\npunctuations = string.punctuation\n\n# Create a list of stopwords\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n\n\n# Load English tokenizer\ntokenizer = English()\n\n# Creating a tokenizer function with the ones defined before\ndef text_tokenizer(sentence):\n    # Creating the token object\n    tokens = tokenizer(sentence)\n\n    # Lemmatizing each token if it is not a pronoun and converting each token into lowercase\n    tokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n    \n    # Remove stop words\n    tokens = [ word for word in tokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return tokens\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.300465Z","iopub.execute_input":"2022-03-17T19:09:57.301094Z","iopub.status.idle":"2022-03-17T19:09:57.648829Z","shell.execute_reply.started":"2022-03-17T19:09:57.30104Z","shell.execute_reply":"2022-03-17T19:09:57.647905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we want to clean more our data. For that, we will be creating a class predictors which inherits from sklearn TransformerMixin\nfrom sklearn.base import TransformerMixin","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.650596Z","iopub.execute_input":"2022-03-17T19:09:57.651004Z","iopub.status.idle":"2022-03-17T19:09:57.690989Z","shell.execute_reply.started":"2022-03-17T19:09:57.650973Z","shell.execute_reply":"2022-03-17T19:09:57.690157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Custom transformer using spaCy\nclass CleanTextTransformer(TransformerMixin):\n    def transform(self, X, **transform_params):\n        # Cleaning Text\n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n# Basic function to clean the text\ndef clean_text(text):\n    # Removing spaces and converting text into lowercase\n    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n    text = text.lower()\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.692651Z","iopub.execute_input":"2022-03-17T19:09:57.693091Z","iopub.status.idle":"2022-03-17T19:09:57.704145Z","shell.execute_reply.started":"2022-03-17T19:09:57.693043Z","shell.execute_reply":"2022-03-17T19:09:57.7029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CountVectorizer converts a collection of text documents to a matrix of token counts\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nvectorizer = CountVectorizer(tokenizer = text_tokenizer, ngram_range=(1,1))","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.70608Z","iopub.execute_input":"2022-03-17T19:09:57.70641Z","iopub.status.idle":"2022-03-17T19:09:57.766338Z","shell.execute_reply.started":"2022-03-17T19:09:57.706366Z","shell.execute_reply":"2022-03-17T19:09:57.765444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now we need to split our train dataset into train and validation data\nX = dtrain['text'] \ny = dtrain['target'] ","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.767564Z","iopub.execute_input":"2022-03-17T19:09:57.768051Z","iopub.status.idle":"2022-03-17T19:09:57.772561Z","shell.execute_reply.started":"2022-03-17T19:09:57.767999Z","shell.execute_reply":"2022-03-17T19:09:57.771458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_vald, y_train, y_vald = train_test_split(X, y, test_size=0.15)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.77367Z","iopub.execute_input":"2022-03-17T19:09:57.774093Z","iopub.status.idle":"2022-03-17T19:09:57.945265Z","shell.execute_reply.started":"2022-03-17T19:09:57.774048Z","shell.execute_reply":"2022-03-17T19:09:57.944269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n# we are going to use Linear Support Vector Classification\nfrom sklearn.svm import LinearSVC\nclassifier = LinearSVC()\n\n# Create a pipeline\npipeline = Pipeline([(\"cleaner\", CleanTextTransformer()),\n                 ('vectorizer', vectorizer),\n                 ('classifier', classifier)])\n\n# model generation\npipeline.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:09:57.946761Z","iopub.execute_input":"2022-03-17T19:09:57.947057Z","iopub.status.idle":"2022-03-17T19:10:00.873869Z","shell.execute_reply.started":"2022-03-17T19:09:57.947028Z","shell.execute_reply":"2022-03-17T19:10:00.872906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\n# predict the X_vald\npredictions = pipeline.predict(X_vald)\n\n# model Accuracy\nprint(\"Linear Support Vector Classification Accuracy:\",metrics.accuracy_score(y_vald, predictions))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:10:00.8752Z","iopub.execute_input":"2022-03-17T19:10:00.875527Z","iopub.status.idle":"2022-03-17T19:10:01.31258Z","shell.execute_reply.started":"2022-03-17T19:10:00.875496Z","shell.execute_reply":"2022-03-17T19:10:01.311239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the code bellow is to create the submission file with the predictions made using the test dataset","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:10:01.313961Z","iopub.execute_input":"2022-03-17T19:10:01.31443Z","iopub.status.idle":"2022-03-17T19:10:01.318836Z","shell.execute_reply.started":"2022-03-17T19:10:01.314365Z","shell.execute_reply":"2022-03-17T19:10:01.31753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictionsFinal=pipeline.predict(dtest['text'])","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:10:01.320223Z","iopub.execute_input":"2022-03-17T19:10:01.320562Z","iopub.status.idle":"2022-03-17T19:10:02.556773Z","shell.execute_reply.started":"2022-03-17T19:10:01.32053Z","shell.execute_reply":"2022-03-17T19:10:02.555839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample['target'] = predictionsFinal","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:10:02.558004Z","iopub.execute_input":"2022-03-17T19:10:02.558462Z","iopub.status.idle":"2022-03-17T19:10:02.56304Z","shell.execute_reply.started":"2022-03-17T19:10:02.55843Z","shell.execute_reply":"2022-03-17T19:10:02.561887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:10:02.564467Z","iopub.execute_input":"2022-03-17T19:10:02.564787Z","iopub.status.idle":"2022-03-17T19:10:02.58522Z","shell.execute_reply.started":"2022-03-17T19:10:02.564755Z","shell.execute_reply":"2022-03-17T19:10:02.584083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.to_csv(\"submissionNLP.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T19:10:02.586675Z","iopub.execute_input":"2022-03-17T19:10:02.587212Z","iopub.status.idle":"2022-03-17T19:10:02.95076Z","shell.execute_reply.started":"2022-03-17T19:10:02.587178Z","shell.execute_reply":"2022-03-17T19:10:02.949818Z"},"trusted":true},"execution_count":null,"outputs":[]}]}