{"cells":[{"metadata":{},"cell_type":"markdown","source":"## loading libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\npd.set_option('display.max_colwidth', -1)\nimport os\nimport re\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nnp.random.seed(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## import inputs and randomize","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntest = test.sample(frac=1,random_state = 1)\ntrain = train.sample(frac=1,random_state = 1)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### glove embeddings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nwith open('/kaggle/input/glove100d/glove.6B.50d.txt','r',encoding = 'utf8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.array(values[1:]).astype(np.float)\n        embeddings_index[word] = coefs\n        \nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.info())\n\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## drop unnecessary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['keyword','location'],axis =1, inplace = True)\n\nprint(train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop(['keyword','location'], axis = 1, inplace = True)\n\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## clean the text column","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. remove http links\n2. removing unkonwn characters \\x89U0 etc.\n3. remove #,@ =>\n4. remove special characters ',:;. etc ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    #2. remove unkonwn characrters\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n   \n    #1. remove http links\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    text = url.sub(r'',text)\n    \n    #3,4. remove #,@ and othet symbols\n    text = text.replace('#',' ')\n    text = text.replace('@',' ')\n    symbols = re.compile(r'[^A-Za-z0-9 ]')\n    text = symbols.sub(r'',text)\n    \n    #5. lowercase\n    text = text.lower()\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## seperate id column ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_id = train['id']\ntrain.drop(['id'],axis=1, inplace = True)\n\ntest_id = test['id']\ntest.drop(['id'],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word2idx and embedding dicts for LSTM model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx = {}\nnew_embedding_index = {}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### instead of using all the 400000 word vectors lets use only vectors form words present in the train and test data. Next codes mean that we are giving each unique word a index(number) and storing in word2idx dictionary and also creating a new embedding dictionary which maps those numbers to a coeff from glove embeddings. If the word does not exist in the glove embedding then we give them a random coeffs of same dimension.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X_list = []\nindex = 1\n\nembed_keys = embeddings_index.keys()\nfor x in train['text']:\n        list1 = x.split(' ')\n        new_list = []\n        for i in list1:\n            if((i in embed_keys)  and (i not in word2idx.keys())):\n                new_embedding_index[index] = embeddings_index[i]\n                word2idx[i] = index\n                new_list.append(index)\n                index=index+1   \n                \n            elif(i not in word2idx.keys()):\n                new_embedding_index[index] = np.random.normal(scale=0.4, size=(50, )).astype(np.float)\n                word2idx[i] = index\n                new_list.append(index)\n                index=index+1   \n\n            else:\n                new_list.append(word2idx[i])\n\n        train_X_list.append(new_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X_list = []\nindex = len(word2idx)+1\n\nembed_keys = embeddings_index.keys()\nfor x in test['text']:\n        list1 = x.split(' ')\n        new_list = []\n        for i in list1:\n            if((i in embed_keys)  and (i not in word2idx.keys())):\n                new_embedding_index[index] = embeddings_index[i]\n                word2idx[i] = index\n                new_list.append(index)\n                index=index+1   \n                \n            elif(i not in word2idx.keys()):\n                new_embedding_index[index] = np.random.normal(scale=0.4, size=(50, )).astype(np.float)\n                word2idx[i] = index\n                new_list.append(index)\n                index=index+1   \n\n            else:\n                new_list.append(word2idx[i])\n\n        test_X_list.append(new_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### lest see total no of unique words in train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(new_embedding_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### its just 22652 unique words including words which may not be in the glove vector.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Padding","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### first lets find the maximum length or no of words in a text column of train and test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max(map(len, train_X_list)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(map(len, test_X_list)) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. #### so we will pad for length of 64","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_features(reviews_int, seq_length):\n    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n    for i, review in enumerate(reviews_int):\n        review_len = len(review)\n        \n        if review_len <= seq_length:\n            zeroes = list(np.zeros(seq_length-review_len))\n            new = zeroes+review \n        \n        elif review_len > seq_length:\n            new = review[0:seq_length]\n        \n        features[i,:] = np.array(new)\n    \n    return features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### lest pad train and test lists","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"train list total length is 7613. but for batch size we need 7616 elements so that all of them will go in batches or else last batch will be left over.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X_list = pad_features(train_X_list,55)\n\nfor i in range(3):\n    extra_list =[np.array(np.zeros(55).astype(int))]\n    train_X_list =  np.append(train_X_list,extra_list, axis=0)\n    \nprint(len(train_X_list))   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we added three new rows in train_X_list we need to add three new rows in train_y_list. all three will be zeros.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y_list=[]\nfor i in train['target']:\n    train_y_list.append(i)\n    \nfor i in range(3):\n    train_y_list.append(0)\nprint(len(train_y_list))\n\ntrain_y_list=np.array(train_y_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the test_X_list has 3263 elements i am considering 64 as batchsize. so in the training or testing it happens in batches format, but in the test set last 63 elements will be left over because of batch size. so to make them also to go through the LSTM i am adding  another row in test list which contains only zeors. so that 3264 can be divided by 64.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X_list = pad_features(test_X_list,55)\n\n\nextra_list =[np.array(np.zeros(55).astype(int))]\n\n\ntest_X_list =  np.append(test_X_list,extra_list, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"embedding dictionary starts with 1 so at 0 index nothing will be there. i am placing all zeors in 0 index.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"new_embedding_index[0] = np.array(np.zeros(50)).astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data loading and batching","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\ntrain_data = TensorDataset(torch.from_numpy(train_X_list),torch.from_numpy(train_y_list))\n\n\nbatch_size = 16\ntrain_loader = DataLoader(train_data, batch_size = batch_size, drop_last = True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM model with Pytorch utilizes GPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass BiLSTM(nn.Module):\n    #rnn for sentiment analysis\n    \n    def __init__(self,weights_matrix, output_size, hidden_dim,hidden_dim2, n_layers, drop_prob=0.5):\n        #initialize model by setting up the layers\n        super(BiLSTM, self).__init__()\n        \n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        #embedding and lstm layers and embedding from the glove\n        num_embeddings, embedding_dim = weights_matrix.shape\n        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n       \n        #getting values or parameters for embedding layer \n        self.embedding.weight = nn.Parameter(weights_matrix)\n        \n        self.lstm = nn.LSTM(embedding_dim,hidden_dim, n_layers, dropout = drop_prob, bidirectional=True, batch_first=True)\n        \n        \n        #dropoutlayer\n        self.dropout = nn.Dropout(0.3)\n        \n        #linear and sigmoid layers\n        self.fullyconnect1 = nn.Linear(hidden_dim,hidden_dim2)\n        \n        self.fullyconnect2 = nn.Linear(hidden_dim2, output_size)\n\n        #self.fullyconnect3 = nn.Linear(hidden_dim3, output_size)\n        \n        self.sig = nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        #forward pass of our model \n        batch_size = x.size(0)\n         \n        #embedding and lstm out\n        embeds = self.embedding(x)\n        lstm_outs, hidden = self.lstm(embeds, hidden)\n        \n        # stack up lstm outputs\n        lstm_outs = lstm_outs.contiguous().view(-1, self.hidden_dim)\n        \n        \n        #dropout and fully connected layer\n        out = self.dropout(lstm_outs)\n        out = self.fullyconnect1(out)\n        out = self.dropout(out)\n        out = self.fullyconnect2(out)\n        #sigmoid function\n        sig_out = self.sig(out)\n        \n         # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        #return last sigmoid output and hidden state\n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size,train_on_gpu=False):\n        # initialize hidden state\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n            \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers*2, batch_size, self.hidden_dim).zero_())\n        \n        return hidden ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### initialize bilstm","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vals = np.array(list(new_embedding_index.values()))\nvals = torch.from_numpy(vals)\n\noutput_size = 1\nhidden_dim = 200\nhidden_dim2 = 50\n#hidden_dim3 = 50\nn_layers = 2\n\nnet = BiLSTM(vals, output_size, hidden_dim,hidden_dim2, n_layers)\n\nprint(net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_gpu = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### training loop","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n\n# training params\n\nepochs =12\n\ncounter = 0\nprint_every = 64\nclip=5 # gradient clipping\n\nnet = net.float()\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n\n\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        inputs = inputs.type(torch.LongTensor)\n        inputs = inputs.cuda() \n        labels = labels.cuda()\n        output, h = net(inputs, h)\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n       \n    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                \"Loss: {:.6f}...\".format(loss.item()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntest_data = torch.from_numpy(test_X_list)\n\ntest_loader = DataLoader(test_data,batch_size=batch_size)\n\nh = net.init_hidden(batch_size)\n\npred = []\n\nnet.eval()\n# iterate over test data\nfor inputs in test_loader:\n\n    # Creating new variables for the hidden state, otherwise\n    # we'd backprop through the entire training history\n    h = tuple([each.data for each in h])\n    \n    # get predicted outputs\n    inputs = inputs.type(torch.LongTensor)\n    if(train_on_gpu):\n        inputs = inputs.cuda()\n        \n    output, h = net(inputs, h)\n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred.append(torch.round(output.squeeze()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = []\nfor i in pred:\n    prediction.append(i.tolist())\n\npred = []\n\npred = [item for sublist in prediction for item in sublist]\n\npred = pred[:-1] # because in test we added extra row in the last for batch size matching.\n\npred = [int(i) for i in pred]\nprint(len(pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'id': test_id,'target': pred})\n\noutput.sort_values([\"id\"], axis=0, \n                 ascending=True, inplace=True)\n\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}