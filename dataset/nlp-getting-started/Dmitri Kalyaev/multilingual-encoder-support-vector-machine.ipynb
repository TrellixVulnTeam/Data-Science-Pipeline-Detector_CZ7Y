{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import time\nstart_time = time.time()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nimport re\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow-text==2.0.0 --user","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_text as text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#it helps to print full tweet , not a part\npd.set_option('display.max_colwidth', -1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results from :\nhttps://www.kaggle.com/ihelon/starter-nlp-svm-tf-idf ;\nhttps://www.kaggle.com/dmitri9149/svm-expm-v0/edit/run/27808847 ;\nhttps://www.kaggle.com/rerere/disaster-tweets-svm ;\nshow the Support Vector Machine works quite well for the Real or Not ? (disaster) Tweets classification with with TF-ID for tokenization.<br>\nIn https://www.kaggle.com/gibrano/disaster-universal-sentences-encoder-svm the Multilingual Universal Sentence Encoder is used for sentence encoding. Here I follow the work in using the Multilingual Universal Sentence Encoder (from tensorflow_hub).<br>\nThe approach from https://www.kaggle.com/bandits/using-keywords-for-prediction-improvement is applied for final filtering of the results basing on the 'keywords'.<br>\n\nThe resulting model is quite simple and relativelly fast (700....900 seconds execution time without GPU). This makes the model suitable for experiments with different parameters and text preprocessing."},{"metadata":{},"cell_type":"markdown","source":"### Data loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data preprosessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean(text):\n    text = re.sub(r\"http\\S+\", \" \", text) # remove urls\n    text = re.sub(r\"RT \", \" \", text) # remove RT\n    # remove all characters if not in the list [a-zA-Z#@\\d\\s]\n    text = re.sub(r\"[^a-zA-Z#@\\d\\s]\", \" \", text)\n    text = re.sub(r\"[0-9]\", \" \", text) # remove numbers\n    text = re.sub(r\"\\s+\", \" \", text) # remove extra spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text = train.text.apply(clean)\ntest.text = test.text.apply(clean)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How the text looks like after the cleaning."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'][50:70]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load the multilingual encoder module."},{"metadata":{"trusted":true},"cell_type":"code","source":"use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some words about Universal Sentence Encoders and Transformer"},{"metadata":{},"cell_type":"markdown","source":"A Universal Sentence Encoders encode sentencies to fixed length vectors (The size is 512 in the case of the Multilingual Encoder). The encoders are pre trained on several different tasks: (research article) https://arxiv.org/pdf/1803.11175.pdf. And a use case: https://towardsdatascience.com/use-cases-of-googles-universal-sentence-encoder-in-production-dd5aaab4fc15 <br>\nTwo architectures are in use in the encoders: Transformer and Deep Averaging Networks.\nTransformer use \"self attention mechanism\" that learns contextual relations between words and (depending on model) even subwords in a sentence. Not only a word , but it position in a sentence is also taking into account (like positions of other words). There are different ways to implement the intuitive notion of \"contextual relation between words in a sentence\" ( so, different ways to construct \"representation space\" for the contextual words relation). If the several \"ways\" are implemented in a model in the same time: the term \"multi head attention mechanism\" is used.<br>\nTransformers have 2 steps. Encoding: read the text and transform it in vector of fixed length, and decoding: decode the vector (produce prediction for the task). For example: take sentence in English, encode, and translate (decode) in sentence in German.<br>\nFor our model we need only encoding mechanism: sentencies are encoded in vectors and supplied for classification to Support Vector Machine.<br>\nGood and intuitive explanation of the Transformer: http://jalammar.github.io/illustrated-transformer/ ; The original and quite famous now paper \"Attention is all you need\": (research article)\nhttps://arxiv.org/pdf/1706.03762.pdf. More about multi head attention: (research article)\nhttps://arxiv.org/pdf/1810.10183.pdf. How Transformer is used in BERT: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270.<br>\n\nThe Multilingual Universal Sentence Encoder:(research articles) https://arxiv.org/pdf/1810.12836.pdf; https://arxiv.org/pdf/1810.12836.pdf;\nExample code: https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\nThe Multilingual Encoder uses very interesting Sentence Piece tokenization to make a pretrained vocabulary: (research articles) https://www.aclweb.org/anthology/D18-2012.pdf; https://www.aclweb.org/anthology/P18-1007.pdf.<br>\n\nAbout the text preprocessing and importance of its coherence with the text preprocessing that is conducted for pretraining + about the different models of text tokeniation:\n\nvery good article:\nhttps://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/.<br>\n\nBelow the encoding is applied to every sentence in train.text and test.text columns and the resulting vectors are saved to lists.<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = []\nfor r in tqdm(train.text.values):\n  emb = use(r)\n  review_emb = tf.reshape(emb, [-1]).numpy()\n  X_train.append(review_emb)\n\nX_train = np.array(X_train)\ny_train = train.target.values\n\nX_test = []\nfor r in tqdm(test.text.values):\n  emb = use(r)\n  review_emb = tf.reshape(emb, [-1]).numpy()\n  X_test.append(review_emb)\n\nX_test = np.array(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training and Evaluating"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_arrays, test_arrays, train_labels, test_labels = train_test_split(X_train,\n                                                                        y_train,\n                                                                        random_state =42,\n                                                                        test_size=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def svc_param_selection(X, y, nfolds):\n    Cs = [1.07]\n    gammas = [2.075]\n    param_grid = {'C': Cs, 'gamma' : gammas}\n    grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=nfolds, n_jobs=8)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search\n\nmodel = svc_param_selection(train_arrays,train_labels, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(test_arrays)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Accuracy and confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(test_labels,pred)\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = accuracy_score(test_labels,pred)\naccuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make Support Vector Machine prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model.predict(X_test)\nsubmission['target'] = test_pred.round().astype(int)\n#submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using keywords for better prediction."},{"metadata":{},"cell_type":"markdown","source":"Here I follow https://www.kaggle.com/bandits/using-keywords-for-prediction-improvement The idea is that some keywords with very high probability (sometimes = 1) signal about disaster (or usual) tweets. It is possible to add the extra 'keyword' feature to the model, but the simple approach also works. I make correction for the disaster tweets prediction to the model basing on the \"disaster\" keywords."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_copy = train\ntrain_df_copy = train_df_copy.fillna('None')\nag = train_df_copy.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\n\nag.sort_values('Disaster Probability', ascending=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count = 2\nprob_disaster = 0.9\nkeyword_list_disaster = list(ag[(ag['Count']>count) & (ag['Disaster Probability']>=prob_disaster)].index)\n#we print the list of keywords which will be used for prediction correction \nkeyword_list_disaster","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The keywords are used for the corretion."},{"metadata":{"trusted":true},"cell_type":"code","source":"ids_disaster = test['id'][test.keyword.isin(keyword_list_disaster)].values\nsubmission['target'][submission['id'].isin(ids_disaster)] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I did experiments with different parameters and text preprocessing within the model configuration. It gives many \"clever\" variants with a good score. But the very model is attractive by its simplicity."},{"metadata":{},"cell_type":"markdown","source":"Please, upvote, if you like"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}