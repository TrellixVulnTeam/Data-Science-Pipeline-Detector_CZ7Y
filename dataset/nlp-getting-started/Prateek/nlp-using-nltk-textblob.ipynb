{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nimport numpy as np\nfrom collections import defaultdict\nfrom collections import  Counter\nfrom textblob import TextBlob \nfrom nltk.corpus import words\nfrom nltk.tokenize import TweetTokenizer,word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer \nimport nltk\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import words\nfrom string import punctuation\nfrom scipy.stats import norm\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom html import unescape\nfrom wordcloud import WordCloud, STOPWORDS\n\nnp.random.seed(0);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install wordsegment -q\nfrom wordsegment import load, segment\nload()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"style.use('fivethirtyeight')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"TRAIN = '/kaggle/input/nlp-getting-started/train.csv'\nTEST = '/kaggle/input/nlp-getting-started/test.csv'\nSAMPLE_SUBMISSION = '/kaggle/input/nlp-getting-started/sample_submission.csv'\n\nticker = ticker.EngFormatter(unit='')\nwords = set(nltk.corpus.words.words())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_polarity(text):\n    try:\n        pol = TextBlob(text).sentiment.polarity\n    except:\n        pol = 0.0\n    return pol\n\ndef statistical_features(df,text_col):\n    df['word_count'] = df[text_col].apply(lambda x : len(x.split()))\n    df['char_count'] = df[text_col].apply(lambda x : len(x.replace(\" \",\"\")))\n    df['word_density'] = df['word_count'] / (df['char_count'] + 1)\n    df['punc_count'] = df[text_col].apply(lambda x : len([a for a in x if a in punctuation]))\n    df['stop_count'] = df[text_col].apply(lambda x : len([a for a in x if a in STOPWORDS]))\n    df['polarity'] = df[text_col].apply(get_polarity)\n    df[\"has_hashtag\"] = df[text_col].apply(lambda text: 1 if \"#\" in text else 0)\n    df[\"has_mention\"] = df[text_col].apply(lambda text: 1 if \"@\" in text else 0)\n    df[\"has_exclamation\"] = df[text_col].apply(lambda text: 1 if \"!\" in text else 0)\n    df[\"has_question\"] = df[text_col].apply(lambda text: 1 if \"?\" in text else 0)\n    df[\"has_url\"] = df[text_col].apply(lambda text: 1 if \"http\" in text else 0)\n    df[\"has_RT\"] = df[text_col].apply(lambda text: 1 if \"RT\" in text else 0)\n    return df\n\ndef get_data(df,filter_val):\n    return df[df['target']==filter_val]\n\ndef create_corpus(df,target):\n    corpus=[]\n    \n    for x in df[df['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i.lower())\n    return corpus\n\ndef get_Distribution(corpus,values):\n    dic=defaultdict(int)\n    for word in corpus:\n        if word in values:\n            dic[word]+=1\n    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)\n    return top\n\ndef most_common_words(corpus):\n    counter=Counter(corpus)\n    most=counter.most_common()\n    x=[]\n    y=[]\n    for word,count in most[:100]:\n        if (word not in STOPWORDS and word not in punctuation) :\n            x.append(word)\n            y.append(count)\n    return x,y\n\ndef get_top_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df  = pd.read_csv(TRAIN,encoding='utf-8-sig')\ntest_df  = pd.read_csv(TEST,encoding='utf-8-sig')\n\nprint('There are {} rows and {} columns in train'.format(train_df.shape[0],train_df.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test_df.shape[0],test_df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nwith plt.xkcd():\n    fig = plt.figure(figsize = (7, 5))\n    ax = fig.add_subplot(111)\n    ax = sns.countplot(data=train_df,x='target')\n    ax.set_ylabel('Sample Size')\n    ax.set_xlabel('Target Class')\n    ax.yaxis.set_major_formatter(ticker)\n    ax.set_title('Target Class Population Distribution')\n    ax= plt.xticks([0,1], ['Fake Tweet', 'Disaster Tweet'])\n    plt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nwith plt.xkcd():\n    fig =plt.figure(figsize=(15,8))\n    ax = fig.add_subplot(111)\n    ax = train_df.keyword.value_counts()[:20].plot(kind='barh', color='#1B86BA')\n    ax.set_xlabel('Sample Size')\n    ax.set_ylabel('Keywords')\n    ax.set_title('Top 20 keywords in text')\n    plt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nwith plt.xkcd():\n    fig =plt.figure(figsize=(15,8))\n    ax = fig.add_subplot(111)\n    ax = train_df.location.value_counts()[:20].plot(kind='barh', color='#1B86BA')\n    ax.set_xlabel('Sample Size')\n    ax.set_ylabel('Location')\n    ax.set_title('Top 20 Location in text')\n    plt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = statistical_features(train_df,'text')\ntest_df = statistical_features(test_df,'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.xkcd():\n    fig =plt.figure(figsize=(15,8))\n    ax1 = plt.subplot(121,frameon=True)\n    ax1 = sns.distplot(get_data(train_df,0)['word_count'], fit=norm, kde=False,color=\"#1B86BA\")\n    ax1.set_title('Fake Tweet')\n    ax2 = plt.subplot(122,frameon=True)\n    ax2 = sns.distplot(get_data(train_df,1)['word_count'], fit=norm, kde=False,color=\"#E36149\")\n    ax2.set_title('Disaster Tweet')\n    ax1.grid()\n    ax2.grid()\n    fig.suptitle('Word Count Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.xkcd():\n    fig =plt.figure(figsize=(15,8))\n    ax1 = plt.subplot(121,frameon=True)\n    ax1 = sns.distplot(get_data(train_df,0)['char_count'], fit=norm, kde=False,color=\"#1B86BA\")\n    ax1.set_title('Fake Tweet')\n    ax2 = plt.subplot(122,frameon=True)\n    ax2 = sns.distplot(get_data(train_df,1)['char_count'], fit=norm, kde=False,color=\"#E36149\")\n    ax2.set_title('Disaster Tweet')\n    ax1.grid()\n    ax2.grid()\n    fig.suptitle('Character Count Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.xkcd():\n    fig =plt.figure(figsize=(15,8))\n    ax1 = plt.subplot(121,frameon=True)\n    ax1 = sns.distplot(get_data(train_df,0)['word_density'], fit=norm, kde=False,color=\"#1B86BA\")\n    ax1.set_title('Fake Tweet')\n    ax2 = plt.subplot(122,frameon=True)\n    ax2 = sns.distplot(get_data(train_df,1)['word_density'], fit=norm, kde=False,color=\"#E36149\")\n    ax2.set_title('Disaster Tweet')\n    ax1.grid()\n    ax2.grid()\n    fig.suptitle('Word Density Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.xkcd():\n    fig =plt.figure(figsize=(15,8))\n    ax1 = plt.subplot(121,frameon=True)\n    ax1 = sns.distplot(get_data(train_df,0)['punc_count'], fit=norm, kde=False,color=\"#1B86BA\")\n    ax1.set_title('Fake Tweet')\n    ax2 = plt.subplot(122,frameon=True)\n    ax2 = sns.distplot(get_data(train_df,1)['punc_count'], fit=norm, kde=False,color=\"#E36149\")\n    ax2.set_title('Disaster Tweet')\n    ax1.grid()\n    ax2.grid()\n    fig.suptitle('Punctuation Count Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.xkcd():\n    fig =plt.figure(figsize=(15,8))\n    ax1 = plt.subplot(121,frameon=True)\n    ax1 = sns.distplot(get_data(train_df,0)['stop_count'], fit=norm, kde=False,color=\"#1B86BA\")\n    ax1.set_title('Fake Tweet')\n    ax2 = plt.subplot(122,frameon=True)\n    ax2 = sns.distplot(get_data(train_df,1)['stop_count'], fit=norm, kde=False,color=\"#E36149\")\n    ax2.set_title('Disaster Tweet')\n    ax1.grid()\n    ax2.grid()\n    fig.suptitle('Stopwords Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.xkcd():\n    fig =plt.figure(figsize=(15,8))\n    ax1 = plt.subplot(121,frameon=True)\n    ax1 = sns.distplot(get_data(train_df,0)['polarity'], fit=norm, kde=False,color=\"#1B86BA\")\n    ax1.set_title('Fake Tweet')\n    ax2 = plt.subplot(122,frameon=True)\n    ax2 = sns.distplot(get_data(train_df,1)['polarity'], fit=norm, kde=False,color=\"#E36149\")\n    ax2.set_title('Disaster Tweet')\n    ax1.grid()\n    ax2.grid()\n    fig.suptitle('Polarity Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_0 = create_corpus(train_df,0)\ncorpus_1 =create_corpus(train_df,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_fake_x, top_fake_y = zip(*get_Distribution(corpus_0,STOPWORDS)[:30])\ntop_disaster_x,top_disaster_y = zip(*get_Distribution(corpus_1,STOPWORDS)[:30])\n\nwith plt.xkcd():\n    fig =plt.figure(figsize=(15,8))\n    ax1 = plt.subplot(121,frameon=True)\n    ax1.barh(top_fake_x,top_fake_y,color=\"#1B86BA\")\n    ax1.set_title('Fake Tweet')\n    ax2 = plt.subplot(122,frameon=True)\n    ax2.barh(top_disaster_x,top_disaster_y,color=\"#E36149\")\n    ax2.set_title('Disaster Tweet')\n    ax1.grid()\n    ax2.grid()\n    fig.suptitle('Top 30 Stop Word')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_fake_x, top_fake_y = zip(*get_Distribution(corpus_0,punctuation)[:30])\ntop_disaster_x,top_disaster_y = zip(*get_Distribution(corpus_1,punctuation)[:30])\n\nwith plt.xkcd():\n    fig =plt.figure(figsize=(15,8))\n    ax1 = plt.subplot(121,frameon=True)\n    ax1.barh(top_fake_x,top_fake_y,color=\"#1B86BA\")\n    ax1.set_title('Fake Tweet')\n    ax2 = plt.subplot(122,frameon=True)\n    ax2.barh(top_disaster_x,top_disaster_y,color=\"#E36149\")\n    ax2.set_title('Disaster Tweet')\n    ax1.grid()\n    ax2.grid()\n    fig.suptitle('Top 30 Punctions')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_fake_x, top_fake_y = most_common_words(corpus_0)\ntop_disaster_x,top_disaster_y = most_common_words(corpus_1)\n\nwith plt.xkcd():\n    fig =plt.figure(figsize=(20,12))\n    ax1 = plt.subplot(121,frameon=True)\n    ax1.barh(top_fake_x,top_fake_y,color=\"#1B86BA\")\n    ax1.set_title('Fake Tweet')\n    ax2 = plt.subplot(122,frameon=True)\n    ax2.barh(top_disaster_x,top_disaster_y,color=\"#E36149\")\n    ax2.set_title('Disaster Tweet')\n    ax1.grid()\n    ax2.grid()\n    fig.suptitle('Most Common Word\\n(Exclusing Punctuation & StopWords)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_fake_x, top_fake_y = map(list,zip(*get_top_bigrams(corpus_0)[:30]))\ntop_disaster_x,top_disaster_y = map(list,zip(*get_top_bigrams(corpus_1)[:30]))\n\nwith plt.xkcd():\n    fig =plt.figure(figsize=(30,18))\n    ax1 = plt.subplot(121,frameon=True)\n    ax1.barh(top_fake_x,top_fake_y,color=\"#1B86BA\")\n    ax1.set_title('Fake Tweet')\n    ax2 = plt.subplot(122,frameon=True)\n    ax2.barh(top_disaster_x,top_disaster_y,color=\"#E36149\")\n    ax2.set_title('Disaster Tweet')\n    ax1.grid()\n    ax2.grid()\n    fig.suptitle('Most Common Bi-Gram')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_mention_hashtag(text):\n    \"\"\"https://medium.com/analytics-vidhya/working-with-twitter-data-b0aa5419532\"\"\"\n#     cleantext = re.sub(r\"#(\\w+)\", '', text, flags=re.MULTILINE).strip()\n    cleantext = re.sub(r\"@(\\w+)\", '', text, flags=re.MULTILINE).strip()\n    return cleantext\n\ndef html_escape(text):\n    \"\"\"https://docs.python.org/3/library/html.html\"\"\"\n    return unescape(text)\n\ndef web_urls(text):\n    \"\"\"https://stackoverflow.com/a/11332543\"\"\"\n    return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text).strip()\n\ndef clean_text(text):\n    \"\"\"https://stackoverflow.com/a/57030875\"\"\"\n    text_nonum = re.sub(r'\\d+', '', text)\n    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in punctuation])\n    return text_nopunct\n\nadd_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"]\n\ndef stopWords(text):\n    \"\"\"https://medium.com/analytics-vidhya/working-with-twitter-data-b0aa5419532\"\"\"\n    return ' '.join(token.lower() for token in word_tokenize(text) if \n                    ((token.lower() not in STOPWORDS) and (token.lower() not in add_stopwords)\n                    ))\ndef removeNonAscii(s): \n    \"\"\"https://stackoverflow.com/a/1342373\"\"\"\n    return \"\".join(i for i in s if ord(i)<128)\n\ndef remove_double_space(text):\n    \"\"\"https://stackoverflow.com/a/57030875\"\"\"\n    return re.sub('\\s+', ' ', text).strip()\n\ndef remove_len_words(text):\n    return re.sub(r'\\b\\w{1,2}\\b', '', text)\n\ndef corrections(text):\n    return TextBlob(text).correct()\n\ndef clean_dataframe(df,clean_field):\n    df['cleaned_text'] = df[clean_field].apply(lambda x: html_escape(x))\n    df['cleaned_text'] = df.cleaned_text.apply(lambda x: remove_mention_hashtag(x))\n    df['cleaned_text'] = df.cleaned_text.apply(lambda x: web_urls(x))\n    df['cleaned_text'] = df.cleaned_text.apply(lambda x: removeNonAscii(x))\n    df['cleaned_text'] = df.cleaned_text.apply(lambda x: stopWords(x))\n    df['cleaned_text'] = df.cleaned_text.apply(lambda x: clean_text(x))\n    df['cleaned_text'] = df.cleaned_text.apply(lambda x: remove_len_words(x))\n    df['cleaned_text'] = df.cleaned_text.apply(lambda x: remove_double_space(x))\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_train_df = clean_dataframe(train_df,'text')\ncleaned_test_df = clean_dataframe(test_df,'text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pprint import pprint\nfrom time import time\nimport logging\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainIndex, testIndex = list(), list()\nfor i in range(cleaned_train_df.shape[0]):\n    if np.random.uniform(0, 1) < 0.75:\n        trainIndex += [i]\n    else:\n        testIndex += [i]\n        \ntrainData = cleaned_train_df.loc[trainIndex]\ntestData = cleaned_train_df.loc[testIndex]\n\ncv = CountVectorizer(stop_words = 'english',strip_accents='ascii')\nvect = cv.fit(cleaned_train_df.cleaned_text)\n\n# #############################################################################\n# Define a pipeline combining a text feature extractor with a simple\n# classifier\npipeline = Pipeline([\n    ('vect', CountVectorizer(stop_words = 'english',strip_accents='ascii')),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier(random_state=0)),\n])\npipeline.fit(trainData.cleaned_text,trainData.target);\ny_pred = pipeline.predict(testData.cleaned_text)\nf1_score(testData.target, y_pred, average='micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def wordCloud(df1,tex1,df2,tex2,title,feature='cleaned_text'):\n    all_words_1 = ' '.join([text for text in df1[feature]])\n    wordcloud1 = WordCloud(width=1000, height=900, random_state=21, max_font_size=110).generate(all_words_1)\n    all_words_2 = ' '.join([text for text in df2[feature]])\n    wordcloud2 = WordCloud(width=1000, height=900, random_state=21, max_font_size=110).generate(all_words_2)\n    \n    fig =plt.figure(figsize=(30,12))\n    ax1 = plt.subplot(121,frameon=True)\n    ax1 = plt.imshow(wordcloud1, interpolation=\"bilinear\")\n    ax1 = plt.axis('off')\n    ax1 = plt.title(tex1)\n    ax2 = plt.subplot(122,frameon=True)\n    ax2 = plt.imshow(wordcloud2, interpolation=\"bilinear\")\n    ax2 = plt.title(tex2)\n    ax2 = plt.axis('off')\n    fig.suptitle(f'Word Cloud\\n{title} Set')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordCloud(cleaned_train_df,'Train Tweet',cleaned_test_df,'Test Tweet','Train vs Validation','cleaned_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordCloud(cleaned_train_df[cleaned_train_df.target==0],'Fake Tweet',cleaned_train_df[cleaned_train_df.target==1],'Disaster Tweet','Fake vs Disaster from Training Set','cleaned_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\"\"\"\ndef get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)\n\nlemmatizer = WordNetLemmatizer()\n\ncleaned_train_df['lemmatize_text'] = cleaned_train_df['cleaned_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]))\ncleaned_test_df['lemmatize_text'] = cleaned_test_df['cleaned_text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(x)]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordCloud(cleaned_train_df,'Train Tweet(Lemmatized)',cleaned_test_df,'Test Tweet(Lemmatized)','Train vs Validation','lemmatize_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordCloud(cleaned_train_df[cleaned_train_df.target==0],'Fake Tweet(Lemmatized)',cleaned_train_df[cleaned_train_df.target==1],'Disaster Tweet(Lemmatized)','Fake vs Disaster from Training Set','lemmatize_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalization(df,clean_field):\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bmom\\b', 'mother', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bomg\\b', 'oh my god', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\blol\\b', 'laugh out loud', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bhaha\\b', 'laugh', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bgovt\\b', 'government', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\blmao\\b', 'laughing my ass off', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bmilitants\\b', 'militant', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bguys\\b', 'guy', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\binfo\\b', 'information', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bphotos\\b', 'photo', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bwomens\\b', 'women', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bbusinesses\\b', 'business', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bppl\\b', 'people', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\btraumatised\\b', 'trauma', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bmadhya pradesh\\b', 'madhya_pradesh', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\byearold\\b', 'year old', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\bpanicking\\b', 'panic old', x))\n    df['cleaned_text'] = df[clean_field].apply(lambda x: re.sub(r'\\busa|libya|pamela geller|bob apocalypse wither|mod|tonto|faux leather|enugu|calgary|nws|apc|refugio|cameroon\\b', '', x))\n    return df['cleaned_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_train_df['cleaned_text'] = normalization(cleaned_train_df,'cleaned_text')\ncleaned_test_df['cleaned_text'] = normalization(cleaned_test_df,'cleaned_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData = cleaned_train_df.loc[trainIndex]\ntestData = cleaned_train_df.loc[testIndex]\n\ncv = CountVectorizer(stop_words = 'english',strip_accents='ascii')\nvect = cv.fit(cleaned_train_df.cleaned_text)\n\n# #############################################################################\n# Define a pipeline combining a text feature extractor with a simple\n# classifier\npipeline = Pipeline([\n    ('vect', CountVectorizer(stop_words = 'english',strip_accents='ascii')),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier(random_state=0)),\n])\npipeline.fit(trainData.cleaned_text,trainData.target);\ny_pred = pipeline.predict(testData.cleaned_text)\nf1_score(testData.target, y_pred, average='micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_train_df['lemmatize_text'] = normalization(cleaned_train_df,'lemmatize_text')\ncleaned_test_df['lemmatize_text'] = normalization(cleaned_test_df,'lemmatize_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainData = cleaned_train_df.loc[trainIndex]\ntestData = cleaned_train_df.loc[testIndex]\n\ncv = CountVectorizer(stop_words = 'english',strip_accents='ascii')\nvect = cv.fit(cleaned_train_df.cleaned_text)\n\n# #############################################################################\n# Define a pipeline combining a text feature extractor with a simple\n# classifier\npipeline = Pipeline([\n    ('vect', CountVectorizer(stop_words = 'english',strip_accents='ascii')),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier(random_state=0)),\n])\npipeline.fit(trainData.lemmatize_text,trainData.target);\ny_pred = pipeline.predict(testData.lemmatize_text)\nf1_score(testData.target, y_pred, average='micro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nembedding_dict = {}\nnum_lines = sum(1 for line in open('../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt','r'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"../input/glove-global-vectors-for-word-representation/glove.twitter.27B.200d.txt\",\"r\") as f:\n    for line in tqdm(f, total=num_lines):\n        values = line.split()\n        word = values[0].replace(\"<\", \"\").replace(\">\", \"\")\n        vectors = np.asarray(values[1:],'float32')\n        embedding_dict[word] = vectors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_text_features(df,text):\n    \"\"\"https://www.kaggle.com/ykskks/lightgbm-starter\"\"\"\n    text_feature_df = pd.DataFrame()\n    \n    # get word vector for each word and average them\n    for i, text in enumerate(df[text].values):\n        word_vecs = []\n        for word in text.split():\n            if word in embedding_dict:\n                word_vecs.append(embedding_dict[word])\n        tweet_vec = np.mean(np.array(word_vecs), axis=0)\n        text_feature_df[i] = pd.Series(tweet_vec)\n        \n    text_feature_df = text_feature_df.T\n    text_feature_df.columns = [f\"GloVe_{j+1}\" for j in range(200)]\n    \n    return text_feature_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text_feat = get_text_features(cleaned_train_df,'lemmatize_text')\ntest_text_feat = get_text_features(cleaned_test_df,'lemmatize_text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"basic_feat = ['word_count','char_count','word_density','punc_count','stop_count','polarity','has_hashtag','has_mention','has_exclamation','has_question','has_url','has_RT']\n# combine two types of features\ntrain_feat = pd.concat([cleaned_train_df[basic_feat], train_text_feat], axis=1)\ntarget = cleaned_train_df.target\ntest_feat = pd.concat([cleaned_test_df[basic_feat], test_text_feat], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\noof = np.zeros(len(train_feat))\npredictions = np.zeros(len(test_feat))\nfeature_importance_df = pd.DataFrame()\n\nparams = {\"learning_rate\": 0.01,\n          \"max_depth\": 8,\n         \"boosting\": \"gbdt\",\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.8,\n          \"colsample_bytree\": 0.5,\n         \"min_data_in_leaf\": 50,\n         \"bagging_seed\": 42,\n          \"lambda_l2\": 0.0001,\n         \"metric\": \"binary_logloss\",\n         \"random_state\": 42}\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_feat, target)):\n    print(f\"Fold {fold+1}\")\n    train_data = lgb.Dataset(train_feat.iloc[train_idx], label=target.iloc[train_idx])\n    val_data = lgb.Dataset(train_feat.iloc[val_idx], label=target.iloc[val_idx])\n    num_round = 10000\n    clf = lgb.train(params, train_data, num_round, valid_sets = [train_data, val_data], verbose_eval=100, early_stopping_rounds=100)\n    oof[val_idx] = clf.predict(train_feat.iloc[val_idx], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = train_feat.columns\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type=\"gain\")\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    feature_importance_df = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False).head(50)\n    \n    predictions += clf.predict(test_feat, num_iteration=clf.best_iteration) / skf.n_splits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\nwith plt.xkcd():\n    fig = plt.figure(figsize=(22, 18))\n    ax = fig.add_subplot(111)\n    ax = sns.barplot(feature_importance_df[\"importance\"], feature_importance_df.index,color=\"#1B86BA\")\n    ax.set_ylabel('Feature')\n    ax.set_xlabel('Importance')\n    ax.set_title('Feature Importance (LGBM)')\n    ax.xaxis.set_major_formatter(ticker)\n    plt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresholds = [0.35, 0.375, 0.40, 0.425, 0.45, 0.475, 0.50, 0.525, 0.55]\nf1s = []\n\nfor threshold in thresholds:\n    pred_bin = np.where(oof>threshold, 1, 0)\n    f1 = f1_score(target, pred_bin)\n    f1s.append(f1)\n\nprint(f1s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit\nspsbm = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nspsbm[\"target\"] = np.where(predictions>0.45, 1, 0)\nspsbm.to_csv(\"submission_lgbm.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}