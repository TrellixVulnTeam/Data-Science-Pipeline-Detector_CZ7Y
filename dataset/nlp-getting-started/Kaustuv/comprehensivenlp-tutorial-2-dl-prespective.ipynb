{"cells":[{"metadata":{},"cell_type":"markdown","source":"This tutorial is part-2 of [Comprehensive tutorial on NLP](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-1-ml-perspective) series. In this part we will learn about word embeddings and see how Deep learning has simplified NLP. \n\nPre-requisite: Basic Deep learning understanding would be helpful though not mandatory."},{"metadata":{},"cell_type":"markdown","source":"<a class=\"kk\" id=\"0.1\"></a>\n## Contents\n\n1. [Introduction to Word Embedding](#1)\n    1. [Dimensionality](#1.1) \n    1. [Padding](#1.2)\n    1. [Euclidean Distance](#1.3)\n    1. [Cosine Similarity](#1.4)\n1. [Word Embedding Techniques](#2)\n    1. [Word2Vec](#2.1)\n        1. [Skip-Gram](#2.1.1)\n        1. [CBOW (Continuous Bag of Words)](#2.1.2)\n    1. [GloVe](#2.2) \n    1. [FastText](#2.3)\n1. [Text to Numeric Convertion Using Word Vectors](#3)\n    1. [Vector Averaging](#3.1)\n        1. [Vector Averaging With Word2Vec](#3.1.1)\n        1. [Vector Averaging With GloVe](#3.1.2)\n        1. [Vector Averaging With FastText](#3.1.3) \n    1. [Embedding Matrix & Keras Embedding layer](#3.2)\n        1. [Word2Vec Embedding layers](#3.2.1)\n        1. [GloVe Embedding layers](#3.2.2)\n        1. [FastText Embedding layers](#3.2.3)      \n1. [Deep Learning models](#4)\n    1. [Basic-DNN](#4.1)\n    1. [CNN](#4.2)\n    1. [RNN](#4.3)\n    1. [Recurrent Neural Network -LSTM](#4.4)\n    1. [Recurrent Neural Network – GRU](#4.5)\n "},{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction to Word Embedding  <a class=\"kk\" id=\"1\"></a>\n[Back to Contents](#0.1)\n\nWord Embedding is also known as Word Vectorization. It means converting word into vector. Vectors are numeric representation of a point in space. Mathematically vectors are 1D array or sequence of numbers.  \n\n\n<B>Why we need Word Embedding? </B>\n\nA problem with our previous text to numeric conversion techniques (countvectorizer & tfidf) was that, they ignore synonyms. For example, word 'measure' and ‘calculate’ were represented differently however, in most sentences they can be used interchangeably. In Word Embedding, similar words are spatially close to each other in vector space. Word Embedding is also capable of preserving semantic and syntactic similarity and relation with other words. The vector representation are such that geometric transformation adopts syntax and semantic. For instance, by adding a “female” vector to the vector \"king\", we obtain the vector \"queen\" and by adding a \"plural\" vector to the vector \"king\", we obtain \"kings\". \n\nAnother problem we observe in [part-1](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-1-ml-perspective), was production of high dimensionality sparse matrix. Word Embedding solve this also, as it produces low dimensional dense matrix.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/images/textproc.jpg\",  width=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before applying Word Embedding techniques lets look into into few common NLP vocabulary terms."},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Dimensionality  <a class=\"kk\" id=\"1.1\"></a>\n\nDimensionality refers to the length of vectors and equals to number of columns of vector representation.\nFor example, in the above figure each token is encoded into vector of dimesionality 3."},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Padding <a class=\"kk\" id=\"1.2\"></a>\n\nPadding is task of appending string up to given specific length with whitespaces. Padding is used to represent all records as fixed length."},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Euclidean Distance <a class=\"kk\" id=\"1.3\"></a>\n\nEuclidean distance is the shortest distance between two points in (Euclidean) space.\n\n "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"/kaggle/input/images/eucldeandistance.png\",  width=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Cosine Similarity  <a class=\"kk\" id=\"1.4\"></a>\nCosine similarity is a measure of similarity between two nonzero vectors of an inner product space. It measures the cosine of the angle between them. \n "},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"Image(\"/kaggle/input/images/cosineSimilarity.png\",  width=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Word Embedding Techniques <a class=\"kk\" id=\"2\"></a>\n[Back to Contents](#0.1)\n\nNow we will dive deep into word Embedding techniques. But first let's fetch our [Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) dataset and clean it, as we did in [part 1](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-1-ml-perspective)."},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Fetch & clean dataset \n!pip install pyspellchecker\nfrom spellchecker import SpellChecker\nimport pandas as pd\nfrom nltk.corpus import stopwords \nfrom nltk.corpus import wordnet\n\nfrom nltk.stem import WordNetLemmatizer \nimport nltk \nimport re\nimport numpy as np  \nimport pandas as pd \ntrain_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n\n\ndef convert_to_antonym(sentence):\n    words = nltk.word_tokenize(sentence)\n    new_words = []\n    temp_word = ''\n    for word in words:\n        antonyms = []\n        if word == 'not':\n            temp_word = 'not_'\n        elif temp_word == 'not_':\n            for syn in wordnet.synsets(word):\n                for s in syn.lemmas():\n                    for a in s.antonyms():\n                        antonyms.append(a.name())\n            if len(antonyms) >= 1:\n                word = antonyms[0]\n            else:\n                word = temp_word + word # when antonym is not found, it will\n                                    # remain not_happy\n            \n            temp_word = ''\n        if word != 'not':\n            new_words.append(word)\n    return ' '.join(new_words)\n\n\n# def correct_spellings(text):\n#     spell = SpellChecker()\n#     corrected_words = []\n#     misspelled_words = spell.unknown(text.split())\n#     for word in text.split():\n#         if word in misspelled_words:\n#             corrected_words.append(spell.correction(word))\n#         else:\n#             corrected_words.append(word)\n#     return \" \".join(corrected_words)\n\n        \nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV }\ndef lemma_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word ,wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\n \ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n  \"\"\"\n    text = text.lower() # lowercase text\n    text= re.sub(r'[^\\w\\s#]',' ',text) #Removing every thing other than space, word and hash\n    text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n    text= re.sub(r'[0-9]',' ',text)\n    #text = correct_spellings(text)\n    text = lemma_words(text)\n    text = convert_to_antonym(text)\n    text = re.sub(' +', ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text    \n    return text\n\n\ntrain_df['text'] = train_df['text'].apply(clean_text)\ntest_df['text'] = test_df['text'].apply(clean_text)\n\nsentences= pd.DataFrame(columns=['text'])\nsentences['text']= pd.concat([train_df[\"text\"], test_df[\"text\"]])\n\nfrom collections import defaultdict\ntokens_list = [row.split() for row in sentences['text']]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Word2Vec    <a class=\"kk\" id=\"2.1\"></a>\n\n Word2Vec is group of related models that are used to produce Word Embeddings. It was created & patented by Tomas Mikolov and a group of a research team from Google in 2013. Each unique word in the corpus is assigned a corresponding vector in the space. Word2Vec relies only on local information of language hence the semantics learnt for a given word is only affected by the surrounding words. Underlying assumption of Word2Vec is that two words sharing similar contexts also share a similar meaning, this at times results into similar vector representation (cosine similarity) of multiple words. One more drawback of word2vec is it's unablity to take care of OutofVocabulary(OOV) word.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"/kaggle/input/images/word2vec.png\",  width=450)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Word2Vec comes in two flavours,\n - Skip-Gram and \n - Continuous Bag of Words (CBOW)\n\nUnderneath, Word2Vec uses neural network algorithms that can be trained on any type of sequential data. Fortunately we have libraries available that have already implemented these algorithms and we have to just call the method with proper arguments. A popular one among such libraries is <B>gensim</B>. It provides the [Word2Vec Class](https://radimrehurek.com/gensim/models/word2vec.htm) for working with a Word2Vec model.\n\nFor more details of Gensim implementation of Wrod2Vec refer their [official documentation](https://radimrehurek.com/gensim/models/word2vec.html)\n"},{"metadata":{},"cell_type":"markdown","source":"###  2.1.1 Skip-Gram  <a class=\"kk\" id=\"2.1.1\"></a>\n \nSkip-Gram is designed to predict the context from base word. From a given word, Skip-gram model tries to predict its neighbouring words.   \n\nSkip-gram is a [(target, context), relevancy] generator. Skip-gram generator gives us pair of words and their relevance (a float value). Let's generate Word2Vec skip-gram embedding for our cleaned-up text corpus using gensim."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"/kaggle/input/images/skipgram.png\",  width=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Building Skipgram  WordVectors using gensim"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\nfrom time import time\nt = time()\n# initialize skipgram model\nsg_model = Word2Vec(min_count=2,\n                    window=2,size=300, sg = 1,\n                    sample=5e-5, alpha=0.05, \n                    min_alpha=0.0005,negative=20 )\n\n# build model vocabulary\nsg_model.build_vocab(tokens_list)\n\n# train the model\nsg_model.train(tokens_list, total_examples=sg_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to build Skip gram model vocab: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have just build our first word-embedding model and that also with only 3 lines of code. Lets play with this model."},{"metadata":{},"cell_type":"markdown","source":"##### Vector representation of word"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"sg_model.wv.__getitem__('hope')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Validate dimension of our word vector"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"len(sg_model.wv.__getitem__('hope'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Measure similarity   b/w two words "},{"metadata":{"scrolled":false,"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"sg_model.wv.similarity('people','saint' )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sg_model.wv.similarity('people', 'terrorist')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Intrestingly, there's higher similarity b/w people & terrorists than people & saints :()\n"},{"metadata":{},"cell_type":"markdown","source":"##### Fetch most similar words for any given word "},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# top 5 similar words\nsg_model.wv.most_similar('fire')[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Fetch list of word vocabulary "},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"print(list(sg_model.wv.vocab))\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1.2 CBOW (Continuous Bag of Words)  <a class=\"kk\" id=\"2.1.2\"></a>\n\nCBOW is designed to predict the base(target) word from context. CBOW is faster to train than the skip-gram and gives slightly better accuracy for frequent words.\n"},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"/kaggle/input/images/cbow.png\", width=450)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#### Building CBOW wordvectors\nfrom gensim.models import Word2Vec\nfrom time import time\nt = time()\n# initialize\ncbow_model = Word2Vec(min_count=2,window=2,size=300, sg = 0,sample=5e-5, alpha=0.05, min_alpha=0.0005, \n                     negative=20 )\n# build model vocabulary\ncbow_model.build_vocab(tokens_list)\n\n# train the model\ncbow_model.train(tokens_list, total_examples=cbow_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to build CBOW model vocab: {} mins'.format(round((time() - t) / 60, 2)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Did you notice CBOW trained faster than Skipgram ?\n"},{"metadata":{},"cell_type":"markdown","source":"####  Pretrain Word2Vec\n\nGoogle has made available pretrained word embedding which includes word vectors for a vocabulary of 3 million words and phrases that they have trained on roughly 100 billion words from Google News dataset using Word2Vec."},{"metadata":{"trusted":true},"cell_type":"code","source":"#fetching  pretrain wordvector\nfrom gensim.models.keyedvectors import KeyedVectors\nt = time()\npretrained_w2vec_embedding = KeyedVectors.load_word2vec_format('../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin', binary=True)\nprint('Time to fetch  pretrain  Word2Vec model vocab: {} mins'.format(round((time() - t) / 60, 2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### vector representation of Word2Vec pretrained word "},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"pretrained_w2vec_embedding['people']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of pretrain w2vec embedding\npretrained_w2vec_embedding.vectors.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del pretrained_w2vec_embedding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 GloVe  <a class=\"kk\" id=\"2.2\"></a>\n\n[GloVe](https://nlp.stanford.edu/pubs/glove.pdf) stands for \"Global Vectors\". It is a Word Embedding [project](https://nlp.stanford.edu/projects/glove/)  written in C language and developed by Stanford university researchers in 2014. Glove embedding technique is based on \nfirstly, construction of a co-occurrence matrix from a training corpus and then \nsecondly, factorization of co-occurrence matrix in order to yield word vector.\n\nUnlike word2vec, which captures only local statistics of token, Glove captures both global statistics and local statistics of a text tokens. GloVe embeddings relate to the probabilities that two words appear together. [glove_python](https://github.com/maciejkula/glove-python) library provides glove implementation.\n\nFor more details of GloVe implementation of  glove_python refer their [official documentation](https://pypi.org/project/glove/)\n"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":false},"cell_type":"code","source":"!pip install glove_python","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"#importing the glove library\nfrom glove import Corpus, Glove\n\n# creating a corpus object\ncorpus = Corpus() \n\n#training the corpus to generate the co occurence matrix which is used in GloVe\ncorpus.fit(tokens_list, window=3)\n\n#creating a Glove object which will use the matrix created in the above lines to create embeddings\n#we can set the learning rate as glove uses Gradient Descent\nglove = Glove(no_components=300, learning_rate=0.05)\nglove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\nglove.add_dictionary(corpus.dictionary)\nglove.save('glove.model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  Displaying Glove WordVector of a word "},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"glove.word_vectors[glove.dictionary['people']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Pretrain Glove\n\nGlove developers have also made available pre-computed embeddings for millions of English tokens, obtained from training Wikipedia data and Common crawl data."},{"metadata":{},"cell_type":"markdown","source":"<B>Due to notebook memory constrains, I have commented out Pretrained Glove and Pretrained Fast-text.However, there usage is similar to pre-trained Word2Vec. You can uncomment and run on separate notebook </B>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Fetch pretrain glove word vectors \n# import numpy as np \n# pretrained_glove_embedding={}\n# with open('../input/nlpword2vecembeddingspretrained/glove.6B.300d.txt','r') as f:\n#     for line in f:\n#         values=line.split()\n#         word=values[0]\n#         vectors=np.asarray(values[1:],'float32')\n#         pretrained_glove_embedding[word]=vectors\n# f.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####   Glove pretrained WordVector of a word "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#pretrained_glove_embedding['hello']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no of  word in pretrained glove embedding\n#len(pretrained_glove_embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del pretrained_glove_embedding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3. Fast-Text <a class=\"kk\" id=\"2.3\"></a>\n\n[FastText](https://fasttext.cc/) is a library for learning of word embeddings and text classification. The Facebook Research Team created fastText in Nov 2015. Fast-Text is an extension of word2vec library. It builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. FastText assumes a word to be formed by a n-grams of character. For example, sunny is composed of [sun, sunn,sunny],[sunny,unny,nny]... etc, where n could range from 1 to the length of the word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. Thus even for previously unseen words, typo errors, and OOV (Out Of Vocabulary) words the model can make an educated guess towards its meaning.Obvious trade off is processing time. Gensim provides the [FastText implementation](https://radimrehurek.com/gensim/models/fasttext.html).\n\nFor more details of FastText implementation of  Gensim refer their [official documentation](https://radimrehurek.com/gensim/models/fasttext.html)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dimension =300\n# from gensim.models import FastText\n# fasttext_model = FastText(tokens_list, size=dimension, window=5, min_count=5, workers=4, sg=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  Displaying FastText WordVector of given word "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# fasttext_model.wv.__getitem__('people')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# # fasttext word similarity measure\n# fasttext_model.wv.similarity('evacuation','shelter' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# # most similar words\n# fasttext_model.wv.most_similar('fire')[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### PreTrained Fasttext\n\nFastText developers have also made available pre-computed embeddings for millions of english tokens, obtained from training Wikipedia data and common crawl data. \n\nDisclaimer: Loading the fastText pretrain will consume some serious memory."},{"metadata":{},"cell_type":"markdown","source":"As mention earlier, due to notebook memory constrains, I have commented out Pretrained-Glove and Pretrained-Fasttext here.The usage is quite similar to pre-trained Word2Vec. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_coefs(word, *arr): \n#     return word, np.asarray(arr, dtype='float32')\n\n# EMBEDDING_FILE = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n# pretrained_fasttext_embedding = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in (open(EMBEDDING_FILE)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"## no of words in pretrained fasttext embedding\n#len(pretrained_fasttext_embedding)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#####  Displaying FastText pretrained WordVector of a word "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#pretrained_fasttext_embedding['fire']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#len(pretrained_fasttext_embedding['fire'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Text to Numeric Convertion Using Word Vectors <a class=\"kk\" id=\"3\"></a>\n[Back to Contents](#0.1)\n\nWe learned about word embedding techniques and created word vectors for our corpus.  Now we will convert our textual data into numerical using these word vectors. I will explain about two popular texts to numerical conversion techniques using word vectors, \n\n1. Vector Averaging  \n1. Embedding Matrix and Keras Embedding layer\n"},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Vector Averaging  <a class=\"kk\" id=\"3.1\"></a>\nIn Vector Averaging approach we average all word embeddings (of words) in the text. Final length remains equal to word vector dimension. This is go to technique when we are planning to use machine learning models such a logistic regression, naïve-bayes, svm etc.  \n "},{"metadata":{},"cell_type":"markdown","source":"### Vector Averaging With Word2Vec <a class=\"kk\" id=\"3.1.1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# functions for Vector Averaging with word2Vec\nimport numpy as np\ndef w2v_embeddings(text,w2v_model,dimension):\n    if len(text) < 1:\n        return np.zeros(dimension)\n    else:\n        vectorized = [w2v_model.wv[word] if word in w2v_model.wv else np.random.rand(dimension) for word in text] \n    \n    sum = np.sum(vectorized,axis=0)\n    ## return the average\n    return sum/len(vectorized)     \n\ndef get_w2v_embeddings(text,w2v_model,dimension):\n        embeddings = text.apply(lambda x: w2v_embeddings(x, w2v_model,dimension))\n        return list(embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SkipGram Model"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Text to Numeric Vector Averaging using sgmodel \ntrain_embeddings_sg_model  = get_w2v_embeddings(train_df['text'],sg_model,dimension=300)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Validate train set size\nlen(train_embeddings_sg_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# validate dimension\nlen(train_embeddings_sg_model[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### CBOW model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text to numeric Vector Averaging using cbow model\ntrain_embeddings_cbow_model_  = get_w2v_embeddings(train_df['text'],cbow_model,dimension=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vector Averaging With Glove <a class=\"kk\" id=\"3.1.2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"## functions  for Vector Averaging with GloVe\nimport numpy as np\ndef glove_embeddings(text, glove_model, dim ):\n    dic=glove_model.dictionary\n    if len(text) < 1:\n        return np.zeros(dim)\n    else:\n        vectorized = [glove_model.word_vectors[dic[word]] if word in dic else np.random.rand(dim) for word in text]  \n    sum = np.sum(vectorized,axis=0)\n    ## return the average\n    return sum/len(vectorized)     \n\ndef get_glove_embeddings(text,glove_model,dimension):\n        embeddings = text.apply(lambda x: glove_embeddings(x,glove_model, dimension))\n        return list(embeddings)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Text to numeric Vector Averaging using glove\nimport numpy as np\ntrain_embeddings_glove = get_glove_embeddings(train_df['text'],glove,dimension=300)\ntest_embeddings_glove = get_glove_embeddings(test_df['text'],glove,dimension=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vector Averaging With Fasttext  <a class=\"kk\" id=\"3.1.3\"></a>\n\nAs Fastext is an extension of word2vec hence the same averaging function of w2vec i.e.`get_ w2v_embeddings` will work for fasttext model too. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## Text to numeric using Averaging with Fasttext\n# import numpy as np\n# fasttext_train_embeddings = w2v_embeddings(train_df['text'], fasttext_model,dimension=300)\n# fasttext_test_embeddings = w2v_embeddings(test_df['text'],  fasttext_model,dimension=300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Vector Averaging With pretrained Embeddings\n\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"def pretrained_embeddings(text,model,dimension):\n    if len(text) < 1:\n        return np.zeros(dimension)\n    else:\n        vectorized = [model[word] if word in model else np.random.rand(dimension) for word in text] \n    \n    sum = np.sum(vectorized,axis=0)\n    ## return the average\n    return sum/len(vectorized)     \n\ndef get_pretrained_embeddings(text,model,dimension):\n        embeddings = text.apply(lambda x: pretrained_embeddings(x, model,dimension))\n        return list(embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# uncomment section in below code in order to perfrom vector averaging using pretrained word embeeding\n\n#Text to numeric Vector Averaging  using  pretrianed word2vec\ntrain_embeddings_w2vec_pretrained_  = get_pretrained_embeddings(train_df['text'],pretrained_w2vec_embedding,dimension=300)\n\n## Text to numeric Vector Averaging  using  pretrianed glove\n# train_embeddings_glove_pretrained_  = get_pretrained_embeddings(train_df['text'],pretrained_glove_embedding,dimension=300)\n\n## Text to numeric Vector Averaging  using  pretrianed fasttext\n# train_embeddings_fasttext_pretrained_  = get_pretrained_embeddings(train_df['text'],pretrained_fasttext_embedding,dimension=300)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Embedded Matrix & Keras Embedding layer <a class=\"kk\" id=\"3.2\"></a>\n\nAveraging is preferred choice when we intend to use Machine Learning models such as lr, svm, gbm etc. but our purpose here is to utilise Deep-learning algorithms. Deep Learning is a layer bases learning where each layer passes its learning to the next layer.   Few libraries have implement deep learning algorithms. A popular one among them is Keras. We will use Keras for our deep learning modelling purpose.\n\nFor text processing Keras offers an embedding layer. This is the first layer of deep learning algorithm. Weights of the Embedding layer are of the shape (vocabulary_size, embedding_dimension), this weight matrix is also called as Embedding matrix. We will first generate this embedding matrix from our word vectors and then initialize Keras embedding layer for each of our word embeddings. \n\nMoreover, Keras has built-in utilities for doing tokenization and encoding of text. We will use these utilities as they take care of a number of important features such as stripping special characters from strings, padding, fetching N most common words in dataset etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"# using keras built in utilities\n# tokenizing using keras  tokenizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.layers import Embedding\ntokenizer_obj=Tokenizer()\n# to builds the word index\ntokenizer_obj.fit_on_texts(tokens_list)\n# to turns strings into lists of integer indices.\nsequences=tokenizer_obj.texts_to_sequences(tokens_list)\n# defining maximum length of sequence \nMAX_LEN= 50\n# pad_sequences is used to ensure that all sequences in a list have the same length\ntweet_pad= pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n\n# segregating text & train from corpu\nx_train = tweet_pad[:7613]\nx_test = tweet_pad[7613:]\n\ntargets =  [target for target in train_df['target']]\n\n# set of all word and their sequence no\nword_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))\nvocab_size = len(word_index)+1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to generate embeeding layer weights i.e. embeeding_matrix\n\n\ndef generate_word2vec_embeeding_matrix(word_vector_model, dimension, vocab_size= vocab_size, word_index =word_index):\n    embedding_matrix=np.zeros((vocab_size,dimension))\n    for word,i in tqdm(word_index.items()):\n        if i > vocab_size:\n            continue\n        if word in word_vector_model.wv:  \n            emb_vec=word_vector_model.wv.__getitem__(word)\n            embedding_matrix[i]=emb_vec\n    return embedding_matrix\n\ndef generate_pretrained_embeeding_matrix(word_vector_model, dimension, vocab_size= vocab_size, word_index =word_index):\n    embedding_matrix=np.zeros((vocab_size,dimension))\n    for word,i in tqdm(word_index.items()):\n        if i > vocab_size:\n            continue\n        if word in word_vector_model:  \n            emb_vec=word_vector_model[word]\n            embedding_matrix[i]=emb_vec\n    return embedding_matrix\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets create Keras embedding layers for our word embeddings. As we have created 4 trained word embedding model (skipgram, cbow, glove and fasttext) and 3 pretrained model (one each for word2vec, glove and fasttext) for all these seven we will create a Keras embedding layer.\n\n### Word2Vec Embedding layers  <a class=\"kk\" id=\"3.2.1\"></a>"},{"metadata":{},"cell_type":"markdown","source":"##### Trained skipgram"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"from keras.layers import Embedding\n\nembedding_matrix_sg_trained = generate_word2vec_embeeding_matrix(sg_model, dimension = 300)\n\nembedding_layer_sg_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_sg_trained], \n                                     input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Pre-Trained  Word2Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pre trainde word2vec  \nembedding_matrix_w2v_pretrained = generate_pretrained_embeeding_matrix(pretrained_w2vec_embedding, dimension =300)    \n\nembedding_layer_w2v_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_w2v_pretrained], \n                                     input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Trained  CBOW"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix_cbow_trained = generate_word2vec_embeeding_matrix(cbow_model, dimension = 300)\n\nembedding_layer_cbow_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_cbow_trained], \n                                     input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GloVe Embedding Layers   <a class=\"kk\" id=\"3.2.2\"></a>"},{"metadata":{},"cell_type":"markdown","source":"##### Trained Glove"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nembedding_matrix_glove_trained=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    if i > vocab_size:\n        continue\n    \n    emb_vec=glove.word_vectors[glove.dictionary[word]]\n    if emb_vec is not None:\n        embedding_matrix_glove_trained[i]=emb_vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_layer_glove_trained = Embedding(vocab_size, 300 , weights=[embedding_matrix_glove_trained], \n                                     input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### PreTrained Glove"},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding_matrix_glove_pretrained = generate_pretrained_embeeding_matrix(pretrained_glove_embedding, dimension =300)    \n\n# embedding_layer_glove_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_glove_pretrained], \n#                                      input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fasttext  Embedding layers  <a class=\"kk\" id=\"3.2.3\"></a>"},{"metadata":{},"cell_type":"markdown","source":"##### Trained FastText"},{"metadata":{"trusted":true},"cell_type":"code","source":" \n# embedding_matrix_fasttext_trained = generate_word2vec_embeeding_matrix(fasttext_model, dimension =300)    \n\n# embedding_layer_fasttext_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_fasttext_trained], \n#                                      input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Pre-Trained FastText"},{"metadata":{"trusted":true},"cell_type":"code","source":"# embedding_matrix_fasttext_pretrained = generate_pretrained_embeeding_matrix(pretrained_fasttext_embedding, dimension =300)    \n\n# embedding_layer_fasttext_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_fasttext_pretrained], \n#                                      input_length=MAX_LEN, trainable=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Deep Learning Models <a class=\"kk\" id=\"4\"></a>\n[Back to Contents](#0.1)\n\n We have initialized Keras embedding layer for our various word embedding models. Now it’s time to train using deep learning models. I will demonstrate how to train for glove pertained layer. You can test with other six embedding layer also (by just reassigning embedding_layer). One point you will notice that pretrained embedding layers performs much better than their trained counter parts. Again the purpose here is to depict basic Deep Learning model performance and not to obtain high score. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declare embeeding layer of your choics \nembedding_layer = embedding_layer_sg_trained\n\n# try with other embedding layes too\n#embedding_layer_glove_pretrained\n# embedding_layer_fasttext_pretrained\n# embedding_layer_fasttext_trained\n# embedding_layer_cbow_trained\n# embedding_layer_sg_trained\n# embedding_layer_w2vec_pretrained\n# embedding_layer_glove_trained","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nmy_callbacks = [\n    keras.callbacks.EarlyStopping(patience=5),\n    \n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Basic DNN <a class=\"kk\" id=\"4.1\"></a>\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Flatten, Dense\ndnn_model = Sequential()\ndnn_model.add(embedding_layer)\ndnn_model.add(Flatten())\ndnn_model.add(Dense(1, activation='sigmoid'))\n\ndnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\ndnn_model.summary()\n\nhistory = dnn_model.fit(x_train,  y = targets,\n                    epochs=10,\n                    batch_size=32,\n                    validation_split=0.2, callbacks= my_callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  4.2 CNN <a class=\"kk\" id=\"4.2\"></a>"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#CNN is suitable for image processing\n# import keras\n# cnn_model = Sequential()\n# # note : below we add embedding layer\n# cnn_model.add(embedding_layer)\n# cnn_model.add(keras.layers.Dropout(0.2))\n# cnn_model.add(keras.layers.Conv1D(3,3, padding='valid',activation='relu', strides=1))\n# cnn_model.add(keras.layers.GlobalMaxPooling1D())\n# cnn_model.add(keras.layers.Dense(20))\n# cnn_model.add(keras.layers.Dropout(0.2))\n# cnn_model.add(keras.layers.Activation('relu'))\n# cnn_model.add(keras.layers.Dense(1))\n# cnn_model.add(keras.layers.Activation('sigmoid'))\n\n# # Get model summary\n# cnn_model.summary()\n# cnn_model.compile(optimizer='rmsprop',\n#               loss='binary_crossentropy',\n#               metrics=['acc'])\n\n# # compile the model\n# history = cnn_model.fit(x_train,  y = targets,\n#                     epochs=10,\n#                     batch_size=32,\n#                     validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 RNN <a class=\"kk\" id=\"4.3\"></a>"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN\nrnn_model = Sequential()\n# note : below we add embedding layer\nrnn_model.add(embedding_layer)\nrnn_model.add(SimpleRNN(32))\nrnn_model.add(Dense(1, activation='sigmoid'))\nrnn_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = rnn_model.fit(x_train, y = targets,epochs=10, batch_size=32,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4  Recurrent Neural Network -LSTM <a class=\"kk\" id=\"4.4\"></a>"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from keras.layers import LSTM\nlstm_model = Sequential()\n# note : below we add embedding layer\nlstm_model.add(embedding_layer)\nlstm_model.add(LSTM(32))\nlstm_model.add(Dense(1, activation='sigmoid'))\n\nlstm_model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = lstm_model.fit(x_train, y = targets,epochs=10, batch_size=32,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 Recurrent Neural Network – GRU <a class=\"kk\" id=\"4.5\"></a>"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from keras.layers import GRU\ngru_model = Sequential()\n# note : below we add embedding layer\ngru_model.add(embedding_layer)\ngru_model.add(GRU(32))\ngru_model.add(Dense(1, activation='sigmoid'))\n\ngru_model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = gru_model.fit(x_train, y = targets,epochs=10, batch_size=32,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Target Prediction  "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# lets predict target values for test set\nmodel =  dnn_model\nraw_preds = model.predict(x_test)\npreds = raw_preds.round().astype(int)\npreds","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Image(\"/kaggle/input/images/too much.png\",  width=450)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" \n\n\nThanks for reading ! \n\nIf you like this kernel, <b>  Do Upvote</b>!!! Its at the top right near Copy&Edit and will not cost you:)\n\n\nIn the next [part-3](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-3-bert-others) we will read about state-of-the-art 'BERT Embedding'.\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"## References\n- Deep Learning with Python by FRANÇOIS CHOLLET  http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf\n- https://en.wikipedia.org/\n- https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/eb9cd609-e44a-40a2-9c3a-f16fc4f5289a.xhtml\n- https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html\n- https://www.kaggle.com/c/quora-question-pairs/discussion/31257#177483\n- https://www.kaggle.com/slatawa/simple-implementation-of-word2vec\n- https://becominghuman.ai/how-does-word2vecs-skip-gram-work-f92e0525def4\n- https://www.thinkinfi.com/2019/06/single-word-cbow.html(image)\n- https://www.kaggle.com/rajmehra03/a-detailed-explanation-of-keras-embedding-layer\n- https://medium.com/@japneet121/word-vectorization-using-glove-76919685ee0b\n- https://www.kaggle.com/christofhenkel/fasttext-starter-description-only\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}