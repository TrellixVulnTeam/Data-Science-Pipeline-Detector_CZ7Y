{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is comprehensive tutorial about Natural Language Processing for beginners and intermediate level users. The tutorial spans across 3 parts.\n[Part-1](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-1-ml-perspective/) discusses NLP with respect to traditional Machine Learning perspective. \n[Part 2](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-2-dl-perspective) explains NLP with respect to Deep Learning perspective and \n[Part-3](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-3-bert) demonstrates useful NLP state-of-art BERT embedding. I will be using [Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) dataset for our text modeling. ","metadata":{}},{"cell_type":"markdown","source":"<a class=\"kk\" id=\"0.1\"></a>\n## Contents\n\n1. [NLP Introduction](#1)\n1. [Text Cleaning](#2)\n    1. [Text Standardization](#2.1)\n        1. [Convert to Lower Case](#2.1.1)\n        1. [Spelling Correction](#2.1.2)    \n    1.  [Eliminate Undesirable Items From Text](#2.2)\n         1. [Removing Additional Spaces](#2.2.1)\n         1. [Removing Punctuations](#2.2.2)\n         1. [Removing URLs](#2.2.3)\n         1. [Removing Digits](#2.2.4)\n         1. [Removing Stopwords](#2.2.5) \n    1. [Convert Non-Words to Words](#2.3)\n        1. [Convert Emoji Into Words](#2.3.1)  \n    1. [Convert Negative Word to its Antonyms](#2.4)\n    1. [Dealing With Base and Derived Words](#2.5)\n         1. [Stemming](#2.5.1)\n         1. [Lemmatization](#2.5.2)\n    1. [Extract Text Using BeautifulSoup](#2.6)    \n1. [Text to Numeric Conversion](#3)\n    1. [Pre Conversion](#3.1)\n        1. [Corpus](#3.1.1)\n        1. [Tokenization](#3.1.2)\n        1. [Bag of Words](#3.1.3)\n        1. [N-Gram](#3.1.4)\n    1.  [Conversion](#3.2)\n         1. [Count Vectorization](#3.2.1)\n         1. [TF-IDF Vectorization](#3.2.2)    \n    1. [Post Conversion Dimesionality Reduction](#3.3)\n        1. [SVD(TruncatedSVD)](#3.3.1)         \n1. [ML-Modeling](#4)\n    1. [Naive Bayes Classifer](#4.1)\n        1. [Gaussian Classifier](#4.1.1)\n        1. [Bernoulli  Classifier](#4.1.2)\n    1. [Logistic Regression](#4.2)\n    1. [SVM](#4.3)\n    1. [XGBoost](#4.4)","metadata":{}},{"cell_type":"markdown","source":"## 1. NLP Introduction <a class=\"kk\" id=\"1\"></a>\n[Back to Contents](#0.1)\n\nNatural Language processing enables computer to understand and process human languages which include both text and audio. In this tutorial we will look into text processing.  \n\npre-requsite:  Python and its libraries.\n ","metadata":{}},{"cell_type":"markdown","source":"## 2. Text Cleaning <a class=\"kk\" id=\"2\"></a>\n\n[Back to Contents](#0.1)\n\nPrimary step of any Machine-Learning project is data cleaning. In text processing also, first we need to clean and standardise text. Lets look into some of the ways of cleaning textual data.","metadata":{}},{"cell_type":"markdown","source":"### 2.1. Text Standardization <a class=\"kk\" id=\"2.1\"></a>\n\n","metadata":{}},{"cell_type":"markdown","source":"#### 2.1.1. Convert to Lower Case <a class=\"kk\" id=\"2.1.1\"></a>\n\nWords with different cases are intercepted differently such as 'The' and 'the'. Hence all words should be converted into same case, preferably lower case.","metadata":{}},{"cell_type":"code","source":"text =\"Welcome to NLP Tutorial\"\ntext = text.lower()\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1.2 Spelling Correction <a class=\"kk\" id=\"2.1.2\"></a>\n\nAt times textual data such as social media data is prone to spelling errors. Spelling errors should be rectified early during the clean-up phase. Fortunately we have libraries available for spelling correction.","metadata":{}},{"cell_type":"code","source":"!pip install pyspellchecker","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from spellchecker import SpellChecker\ndef correct_spellings(text):\n    spell = SpellChecker()\n    corrected_words = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_words.append(spell.correction(word))\n        else:\n            corrected_words.append(word)\n    return \" \".join(corrected_words)\n        \n\n\ntext = \"Spelling correctin is proprly perfrmed\"\ntext = correct_spellings(text)\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Eliminate Undesirable Items From Text <a class=\"kk\" id=\"2.2\"></a>\n\nTexts contains many items that are not useful with respect to text processing and it is better to eliminate them before modelling. Lets look into items that should be considered for removal and repective codes to remove them.","metadata":{}},{"cell_type":"markdown","source":"#### 2.2.1  Removing Additional Spaces <a class=\"kk\" id=\"2.2.1\"></a>","metadata":{}},{"cell_type":"code","source":"import re\ntext = \"Correcting   double  space  text \"\ntext = re.sub(' +', ' ', text)\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.2. Removing Punctuations <a class=\"kk\" id=\"2.2.2\"></a>","metadata":{}},{"cell_type":"code","source":"import string\n\ntext = \"This! sentence, contains so: many - punctuations.\"\ntext = text.translate(str.maketrans('', '', string.punctuation))\nprint(text)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.3. Removing URLs  <a class=\"kk\" id=\"2.2.3\"></a>","metadata":{}},{"cell_type":"code","source":"text = 'Shall I search the answer in www.google.com ?'\ntext  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\nprint(text)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.4.  Removing Digits <a class=\"kk\" id=\"2.2.4\"></a>","metadata":{}},{"cell_type":"code","source":"text =\"Being no 1 team is more important or being no 3 but with fair play \"\ntext= re.sub(r'[0-9]','',text)\nprint (text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2.5. Removing Stopwords  <a class=\"kk\" id=\"2.2.5\"></a>\n\nStopwords are the most common words in a language. For example 'is', 'the', 'that' etc. are stopwords in English language. Stopwords shall be removed during text clean-up phase. However removing stop word can change the meaning of sentence. For instance 'I didn't love politics' will get converted to 'I love politics' after removing stopword.  ","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords \n\ntext = \"This is not the most important topic\"\n\ndef remove_stopwords(text):\n    stop_words = set(stopwords.words('english'))\n    # stop_words will contain  set all english stopwords\n    filtered_sentence = []   \n    for word in text.split(): \n        if word not in stop_words: \n            filtered_sentence.append(word) \n    return \" \".join(filtered_sentence)\n\ntext = remove_stopwords(text)\nprint(text) \n \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Convert Non-Words to Words <a class=\"kk\" id=\"2.3\"></a>\n\nSpecial symbols such as emoticon, emojis etc. are example of non-words.  These non-words should be either converted into words or removed from text. Emoji library converts emojis to equivalent words as shown below. ","metadata":{}},{"cell_type":"markdown","source":"#### 2.3.1 Convert Emoji Into Words  <a class=\"kk\" id=\"2.3.1\"></a>","metadata":{}},{"cell_type":"code","source":"import emoji\ntext = 'Python is üëç'\nprint(emoji.demojize(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Convert Negative Word to its Antonyms <a class=\"kk\" id=\"2.4\"></a>\n\nNegative words should be replaced with their antonym for efficient processing. For instance 'not good‚Äô should be replaced with bad.","metadata":{}},{"cell_type":"code","source":"text = \"He was not happy with the score of team\"\nfrom nltk.corpus import wordnet\nimport nltk\n\ndef convert_to_antonym(sentence):\n    words = nltk.word_tokenize(sentence)\n    new_words = []\n    temp_word = ''\n    for word in words:\n        antonyms = []\n        if word == 'not':\n            temp_word = 'not_'\n        elif temp_word == 'not_':\n            for syn in wordnet.synsets(word):\n                for s in syn.lemmas():\n                    for a in s.antonyms():\n                        antonyms.append(a.name())\n            if len(antonyms) >= 1:\n                word = antonyms[0]\n            else:\n                word = temp_word + word # when antonym is not found, it will\n                                    # remain not_happy\n            \n            temp_word = ''\n        if word != 'not':\n            new_words.append(word)\n    return ' '.join(new_words)\n    \ntext = convert_to_antonym(text)\nprint(text)   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5 Dealing With Base and Derived Words <a class=\"kk\" id=\"2.5\"></a>\n\nWords can be classified into Base word and Derived word. For example 'go' is a base word and  'going', 'gone' and 'went' are its derived words. During data cleaning phase derived word shall be converted to their base counterparts. There are two ways of finding base word of any word - Stemming and Lemmatization.\n","metadata":{}},{"cell_type":"markdown","source":"#### 2.5.1 Stemming  <a class=\"kk\" id=\"2.5.1\"></a>\n\nStemming is a rule base technique. In Stemming Base word is identified by chopping the word at end. For instance 'going' and 'gone' will get converted to 'go' but 'went' will not.","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\ntext = \" David wanted to go with Alfa but Alfa went with Charli so David is going with Bravo\"\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ntext = stem_words(text)\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.5.2 Lemmatization   <a class=\"kk\" id=\"2.5.2\"></a>\nLemmatization is a dictionary based technique and more accurate than stemming. It looks up to the dictionary to fetch Base word (called as lemma). The obvious downside is, it is slow in processing because has to store and look up the dictionary.\n\nAdditionally, lemmatization requires Parts of speech tagging. Lets understand POS tagging first.","metadata":{}},{"cell_type":"markdown","source":"####  Parts of Speech (POS) tagging \n\nParts of Speech Tagger processes a sequence of words and attaches a part of speech tag to each word. 'nltks poc_tag' library is used for POS tagging. Some POS tags examples are VBZ -> Verb, NN-> Noun, PRP -> preposition, IN -> Interjection.\n","metadata":{}},{"cell_type":"code","source":"import nltk\ntext = \"This is very good observation by you.\"\nnltk.pos_tag(text.split()) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you want to know what exactly each tag specify use nltk help function as below","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('tagsets')\nnltk.help.upenn_tagset('DT')","metadata":{"scrolled":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now coming back to Lemmatization,we need to define a wordnet_map (as in below code) and specify for which all parts of speech we need to find Base words otherwise by default it will fetch base words for nouns only.  ","metadata":{}},{"cell_type":"code","source":"# import these modules \nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer \nimport nltk \nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV }\n\n# without wordnet map it takes evey word as noun\ntext = \"David wanted to go with Alfa but Alfa went with Charli so David is going with Bravo \"\n \ndef lemma_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word ,wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\n\nlemma_words(text) \n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.6 Extract Text Using BeautifulSoup <a class=\"kk\" id=\"2.6\"></a>\nBeautiful Soup is a very useful Python library used for extracting text data from HTML and XML files.","metadata":{}},{"cell_type":"code","source":"text = \"\"\"\n<html><head><title>The NLP story</title></head>\n<body>\n<p class=\"title\"><b>The NLP story</b></p>\n<p class=\"story\">Once upon a time there were three little  techniques; and their names were\n<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\nand they lived till the next conference.</p>\n<p class=\"story\">...</p>\n\"\"\"\n\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\ntext = BeautifulSoup(text, \"html\").text# for HTML decoding\n# for lxml  decoding use as below\n#text = BeautifulSoup(text, \"lxml\").text\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, as we know possible ways to clean our text data we will write a clean-up function for our Disaster Tweets dataset.\nNote that: choice of clean up approaches mainly depend upon problem domain, dataset and individual perception. For instance, for newspaper and journal articles we might not need any spelling correction. Clean-up function is an important factor in deciding overall classification outcome.","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n  \"\"\"\n    text = text.lower() # lowercase text\n    text= re.sub(r'[^\\w\\s#]',' ',text) #Removing every thing other than space, word and hash\n    text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n    text= re.sub(r'[0-9]',' ',text)\n    #text = correct_spellings(text)\n    text = convert_to_antonym(text)\n    text = re.sub(' +', ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text    \n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np  \nimport pandas as pd \ntrain_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Text to Numeric Conversion <a class=\"kk\" id=\"3\"></a>\n\n[Back to Contents](#0.1) \n\nOnce text is cleaned we need to feed it into learning Algorithm. However, Learning algorithms understand numbers and not words hence, we will convert text into numbers before feeding into any modeling algorithm. \n\n ","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/images/texttonumber.jpg\",  width=400)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1 Pre Conversion <a class=\"kk\" id=\"3.1\"></a>\n\nBefore performing text to numeric conversion we will look into few common NLP vocabulary terms.","metadata":{}},{"cell_type":"markdown","source":"#### 3.1.1 Corpus <a class=\"kk\" id=\"3.1.1\"></a>\nCollection of all available textual data is known as corpus.","metadata":{}},{"cell_type":"code","source":"corpus= pd.DataFrame(columns=['text'])\ncorpus['text']= pd.concat([train_df[\"text\"], test_df[\"text\"]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.2 Tokenization<a class=\"kk\" id=\"3.1.2\"></a>\n\nText is segmented into its smaller parts called tokens. This segmentation process is known as tokenization. A common example of tokens we can think of is words. However tokens can be something else also such as combination of 2 words. \n ","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/images/tokenization.png\",  width=400)\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.1.3 Bag of Words (BOW) <a class=\"kk\" id=\"3.1.3\"></a>\n\nBOW approach is a way of representing text data. BOW approach recognizes text as just a bag of words and keeps a count of the total occurrences of words in text. It does not consider meaning or context of the word in the document. For example,\n\nText -> \"Now tell me BOW approach is good or not so good?\"\n\nBOW -> \"Now\",\"tell\",\"me\",\"BOW\",\"approach\",is\",\"good\",\"or\",\"not\",\"so\",\"good\"\n","metadata":{}},{"cell_type":"markdown","source":"### 3.1.4 N-Gram <a class=\"kk\" id=\"3.1.4\"></a>\n\nN-grams is a contiguous sequence of n words from a given sample of source text. For Example: \n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/images/ngram.png\",  width=400)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Conversion <a class=\"kk\" id=\"3.2\"></a>\nIn this section we will look into text to numerical conversion techniques.","metadata":{}},{"cell_type":"markdown","source":"### 3.2.1 CountVectorization  <a class=\"kk\" id=\"3.2.1\"></a>\n\nCountVectorization converts a collection of text documents to a matrix of token counts. It counts the tokens in text.  Total no of tokens i.e. vocabulary size will equal to matrix column length (no of columns) where each column represents a token. A row array represents unique tokens of the record and the number at a cell will indicate count of that token in the record.\n ","metadata":{}},{"cell_type":"code","source":"Image(\"/kaggle/input/images/countvectorization.png\",  width=400)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n# analyzer should be word/ character\n#ngram_range lower and upper boundary of the range of n-values\n\n## let'sconvert text to vectors\ntrain_countvectors = count_vectorizer.fit_transform(train_df[\"text\"])\n\n# generating test CountVectorizer matrix\ntest_countvectors = count_vectorizer.transform(test_df[\"text\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting sparce to dense vector \nprint(train_countvectors.shape)\nprint(train_countvectors[0])\nprint(train_countvectors[0].todense())","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2 TF-IDF Vectorization <a class=\"kk\" id=\"3.2.2\"></a>\n\nTF-IDF stands for 'Term frequency-inverse document frequency'. It measures importance of a token (called as term) with respect to its record (called as document) in a corpus. Every term of a document is assigned a weight after multiplying its term frequency (tf) and inverse document frequency (idf). Internally, different libraries use slightly different formula to calculate 'idf' value though underlying idea remains same.\n\n \n","metadata":{}},{"cell_type":"code","source":"Image(\"/kaggle/input/images/tfidf.png\",  width=450)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words='english', ngram_range=(1, 1))\n# training tfidf on corpus\ntfidf_vectorizer.fit(corpus['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tfidfvectors = tfidf_vectorizer.transform(train_df['text'])\ntest_tfidfvectors = tfidf_vectorizer.transform(test_df['text'])\n\nprint(train_tfidfvectors.shape)\nprint(test_tfidfvectors.shape)\nprint(train_tfidfvectors.todense().shape)\nprint(train_tfidfvectors[0].todense())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Post Conversion <a class=\"kk\" id=\"3.3\"></a>\n\nBoth Countvectorization and tf-idf technique produce large size matrix. Post text-to-numerical conversion dimensionality reduction techniques can be tried out to reduce matrix size though keeping their relative importance intact.","metadata":{}},{"cell_type":"markdown","source":"#### 3.3.1 SVD ( TruncatedSVD)   <a class=\"kk\" id=\"3.3.1\"></a>\n\nTruncatedSVD transformer performs linear dimensionality reduction by means of truncated Singular Value Decomposition (SVD). It efficiently works on sparse matrices. ","metadata":{}},{"cell_type":"code","source":"train_countvectors.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import TruncatedSVD   \ntsv = TruncatedSVD(n_components=100)\ntrain_countvectors_svd = tsv.fit_transform(train_countvectors) \ntrain_tfidfvectors_svd = tsv.fit_transform(train_tfidfvectors)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_countvectors_svd.shape)\nprint(train_tfidfvectors_svd.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. ML Modeling <a class=\"kk\" id=\"4\"></a>\n[Back to Contents](#0.1)\n\nAs our text data is in numerical form, it is ready to feed into ML Algorithm. Now we will train classifier models on our numerical data. Purpose here is to depict basic model performance and not to obtain high score.","metadata":{}},{"cell_type":"code","source":"\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# base function to train and compare various models performance\nfrom sklearn import model_selection\ndef text_modeling( model):\n    print( \"Model :\"+ str(model))\n    print('***** F1 Scores *******')\n    scores=model_selection.cross_val_score(model, train_countvectors.toarray(), train_df[\"target\"], cv=3, scoring=\"f1\")\n    print(\"CountVectorized dataset :\"+str(scores.mean()))\n    scores= model_selection.cross_val_score(model, train_tfidfvectors.toarray(), train_df[\"target\"], cv=3, scoring=\"f1\")\n    print(\"TF-IDF Vectorized dataset :\"+str(scores.mean()))\n    scores = model_selection.cross_val_score(model, train_tfidfvectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\")\n    print(\"TF-IDF Vectorized + SVD dataset \"+str(scores.mean()))\n    scores = model_selection.cross_val_score(model, train_countvectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\")\n    print(\"CountVectorized + SVD dataset \"+str(scores.mean()))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1 Naive Bayes Classifer <a class=\"kk\" id=\"4.1\"></a>\n","metadata":{}},{"cell_type":"markdown","source":"#### 4.1.1 Gaussian Classifier <a class=\"kk\" id=\"4.1.1\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\ngn = GaussianNB()\ntext_modeling(gn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4.1.2 Bernoulli  Classifier <a class=\"kk\" id=\"4.1.2\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\n#Create a Gaussian Classifier\n \nbr = BernoulliNB()\ntext_modeling( br)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Logistic Regression <a class=\"kk\" id=\"4.2\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='liblinear')\ntext_modeling( lr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3 Support Vector Machine <a class=\"kk\" id=\"4.3\"></a>","metadata":{}},{"cell_type":"code","source":" from sklearn.svm import SVC\nsvm = SVC()\n# text_modeling(svm)\nmodel_selection.cross_val_score(svm, train_tfidfvectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\").mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4 XGBoost <a class=\"kk\" id=\"4.4\"></a>","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nxgb_clf = xgb.XGBClassifier()\n# text_modeling(xgb_clf)\nmodel_selection.cross_val_score(xgb_clf, train_tfidfvectors_svd, train_df[\"target\"], cv=3, scoring=\"f1\").mean()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks for reading. In this part we saw various ways to clean text data and convert text into numerical representations so that text shall be processed by Machine learning algorithms.  \n\n\nIn the next [part](https://www.kaggle.com/kksienc/comprehensive-nlp-tutorial-2-dl-perspective) we will see how to take care of synonyms and language specific aspects while doing numeric conversion.We will learn about one of the important concepts of NLP that is 'Word Embeddings' and see how Deep Learning has simplified NLP. \n\nIf you found this notebook usefull Please Upvote!\n","metadata":{}}]}