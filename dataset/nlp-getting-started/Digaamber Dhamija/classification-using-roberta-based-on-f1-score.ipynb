{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Please upvote if you find this notebook useful.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:30:23.310478Z","iopub.execute_input":"2021-08-23T21:30:23.310808Z","iopub.status.idle":"2021-08-23T21:30:23.320859Z","shell.execute_reply.started":"2021-08-23T21:30:23.31073Z","shell.execute_reply":"2021-08-23T21:30:23.319911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Analysis**","metadata":{}},{"cell_type":"markdown","source":"*Train Data*","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:30:23.32327Z","iopub.execute_input":"2021-08-23T21:30:23.323767Z","iopub.status.idle":"2021-08-23T21:30:23.467527Z","shell.execute_reply.started":"2021-08-23T21:30:23.32373Z","shell.execute_reply":"2021-08-23T21:30:23.466567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Test Data*","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntest_data","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:30:23.468924Z","iopub.execute_input":"2021-08-23T21:30:23.469297Z","iopub.status.idle":"2021-08-23T21:30:23.503157Z","shell.execute_reply.started":"2021-08-23T21:30:23.469259Z","shell.execute_reply":"2021-08-23T21:30:23.502263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Plotting % missing values*","metadata":{}},{"cell_type":"code","source":"missing_cols = [\"keyword\", \"location\"]\ntrain_missing_vals = [train_data[col].isna().sum() / train_data.shape[0] * 100 for col in missing_cols]\ntest_missing_vals = [test_data[col].isna().sum() / test_data.shape[0] * 100 for col in missing_cols]\n\nplt.figure(figsize = (12, 4))\n\nplt.subplot(121)\nplt.bar(missing_cols, train_missing_vals)\nplt.title(\"% missing values in training set\")\nplt.xlabel(\"Column\")\nplt.ylabel(\"Missing values %\")\n\nplt.subplot(122)\nplt.bar(missing_cols, test_missing_vals)\nplt.title(\"% missing values in test set\")\nplt.xlabel(\"Column\")\nplt.ylabel(\"Missing values %\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:30:23.504956Z","iopub.execute_input":"2021-08-23T21:30:23.505289Z","iopub.status.idle":"2021-08-23T21:30:23.75517Z","shell.execute_reply.started":"2021-08-23T21:30:23.505253Z","shell.execute_reply":"2021-08-23T21:30:23.754205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Transformer Model**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import PolynomialDecay\nfrom transformers import TFAutoModel, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:30:23.756866Z","iopub.execute_input":"2021-08-23T21:30:23.757218Z","iopub.status.idle":"2021-08-23T21:30:30.24406Z","shell.execute_reply.started":"2021-08-23T21:30:23.757182Z","shell.execute_reply":"2021-08-23T21:30:30.243198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Helper Functions and Classes*","metadata":{}},{"cell_type":"code","source":"def tokenize_dataset(tokenizer):\n    \"\"\"\n    Returns a dict of the train and test datasets based on the given tokenizer.\n    \"\"\"\n    return {\n        \"train\": {\n            \"data\": tokenizer(list(train_data[\"text\"].values), padding = \"max_length\", max_length = 84, truncation = True, return_tensors = \"tf\").data,\n            \"labels\": train_data[\"target\"].values,\n        },\n        \"test\": {\n            \"data\": tokenizer(list(test_data[\"text\"].values), padding = \"max_length\", max_length = 84, truncation = True, return_tensors = \"tf\").data\n        }\n    }","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:30:30.245424Z","iopub.execute_input":"2021-08-23T21:30:30.245752Z","iopub.status.idle":"2021-08-23T21:30:30.251585Z","shell.execute_reply.started":"2021-08-23T21:30:30.245716Z","shell.execute_reply":"2021-08-23T21:30:30.250498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class disasterClassificationModel(tf.keras.Model):\n    \"\"\"\n    Adds a classification head to the transformers base model.\n    \"\"\"\n    def __init__(self, checkpoint):\n        super(disasterClassificationModel, self).__init__()\n        self.dropout_rate = 0.7\n        \n        self.base_model = TFAutoModel.from_pretrained(checkpoint)\n        self.flatten = layers.Flatten()\n        \n        self.dropout1 = layers.Dropout(rate = self.dropout_rate)\n        self.dense1 = layers.Dense(units = 768, kernel_regularizer = \"l1_l2\")\n        self.batchNorm1 = layers.BatchNormalization()\n        self.activation1 = layers.Activation(\"relu\")\n        \n        self.dropout2 = layers.Dropout(rate = self.dropout_rate)\n        self.dense2 = layers.Dense(units = 32, kernel_regularizer = \"l1_l2\")\n        self.batchNorm2 = layers.BatchNormalization()\n        self.activation2 = layers.Activation(\"relu\")\n        \n        self.dropout3 = layers.Dropout(rate = self.dropout_rate)\n        self.dense3 = layers.Dense(units = 1, activation = \"sigmoid\")\n\n    def call(self, inputs, training = False):\n        x = self.base_model(inputs).last_hidden_state\n        x = self.flatten(x)\n        \n        x = self.dropout1(x) if training else x\n        x = self.dense1(x)\n        x = self.batchNorm1(x)\n        x = self.activation1(x)\n        \n        x = self.dropout2(x) if training else x\n        x = self.dense2(x)\n        x = self.batchNorm2(x)\n        x = self.activation2(x) \n        \n        x = self.dropout3(x) if training else x\n        x = self.dense3(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:30:30.253067Z","iopub.execute_input":"2021-08-23T21:30:30.25352Z","iopub.status.idle":"2021-08-23T21:30:30.266986Z","shell.execute_reply.started":"2021-08-23T21:30:30.253458Z","shell.execute_reply":"2021-08-23T21:30:30.266113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Custom F1 Score Metric*","metadata":{}},{"cell_type":"code","source":"class F1_score(tf.keras.metrics.Metric):\n    \"\"\"\n    F1 score metric based on Keras Precision and Recall metrics.\n    \"\"\"\n    def __init__(self, name = \"f1_score\", **kwargs):\n        super(F1_score, self).__init__(name = name, **kwargs)\n        \n        self.precision = tf.keras.metrics.Precision()\n        self.recall = tf.keras.metrics.Recall()\n        \n    def update_state(self, y_true, y_pred, sample_weight = None):\n        self.precision.update_state(y_true, y_pred, sample_weight)\n        self.recall.update_state(y_true, y_pred, sample_weight)\n        \n    def reset_states(self):\n        self.precision.reset_states()\n        self.recall.reset_states()\n        \n    def result(self):\n        return 2 / ((1 / self.precision.result()) + (1 / self.recall.result()))","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:30:30.268615Z","iopub.execute_input":"2021-08-23T21:30:30.269362Z","iopub.status.idle":"2021-08-23T21:30:30.279101Z","shell.execute_reply.started":"2021-08-23T21:30:30.269319Z","shell.execute_reply":"2021-08-23T21:30:30.278161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compile_model(model, batch_size, epochs, tokenized_dataset):\n    \"\"\"\n    Compiles the given model by adding a learning rate scheduler to the Adam optimizer.\n    \"\"\"\n    train_steps = (len(tokenized_dataset[\"train\"][\"data\"][\"input_ids\"]) // batch_size) * epochs\n\n    # Learning rate scheduler to linearly reduce the learning rate from an initial value to an end value\n    lr_scheduler = PolynomialDecay(\n        initial_learning_rate = 5e-5,\n        end_learning_rate = 0,\n        decay_steps = train_steps,\n    )\n\n    optimizer = Adam(learning_rate = lr_scheduler)\n    loss = BinaryCrossentropy()\n    \n    model.compile(loss = loss, optimizer = optimizer, metrics = [\"accuracy\", F1_score()])","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:30:30.281778Z","iopub.execute_input":"2021-08-23T21:30:30.282373Z","iopub.status.idle":"2021-08-23T21:30:30.291265Z","shell.execute_reply.started":"2021-08-23T21:30:30.282285Z","shell.execute_reply":"2021-08-23T21:30:30.290439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**ROBERTa base model**","metadata":{}},{"cell_type":"markdown","source":"*This particular model uses a Roberta base and has been finetuned for sentiment-analysis using 58M tweets.*","metadata":{}},{"cell_type":"code","source":"checkpoint = \"cardiffnlp/twitter-roberta-base-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = disasterClassificationModel(checkpoint)\ntokenized_dataset = tokenize_dataset(tokenizer)\ncompile_model(model, batch_size = 10, epochs = 10, tokenized_dataset = tokenized_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:34:47.867134Z","iopub.execute_input":"2021-08-23T21:34:47.867463Z","iopub.status.idle":"2021-08-23T21:34:54.395051Z","shell.execute_reply.started":"2021-08-23T21:34:47.86743Z","shell.execute_reply":"2021-08-23T21:34:54.394183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    x = tokenized_dataset[\"train\"][\"data\"],\n    y = tokenized_dataset[\"train\"][\"labels\"],\n    batch_size = 10,\n    epochs = 10,\n    validation_split = 0.1,\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:34:54.396544Z","iopub.execute_input":"2021-08-23T21:34:54.39686Z","iopub.status.idle":"2021-08-23T21:53:32.856304Z","shell.execute_reply.started":"2021-08-23T21:34:54.396825Z","shell.execute_reply":"2021-08-23T21:53:32.855466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Plotting model history*","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (14, 4))\n\nplt.subplot(131)\nplt.plot(history.history[\"loss\"], label = \"loss\")\nplt.plot(history.history[\"val_loss\"], label = \"val_loss\")\nplt.title(\"Loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend(loc = \"best\")\n\nplt.subplot(132)\nplt.plot(history.history[\"accuracy\"], label = \"accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label = \"val_accuracy\")\nplt.title(\"Accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend(loc = \"best\")\n\nplt.subplot(133)\nplt.plot(history.history[\"f1_score\"], label = \"f1_score\")\nplt.plot(history.history[\"val_f1_score\"], label = \"val_f1_score\")\nplt.title(\"F1 Score\")\nplt.ylabel(\"F1 Score\")\nplt.xlabel(\"Epoch\")\nplt.legend(loc = \"best\")","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:53:32.858732Z","iopub.execute_input":"2021-08-23T21:53:32.859111Z","iopub.status.idle":"2021-08-23T21:53:33.278905Z","shell.execute_reply.started":"2021-08-23T21:53:32.859069Z","shell.execute_reply":"2021-08-23T21:53:33.278139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Submission**","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(tokenized_dataset[\"test\"][\"data\"], verbose = True)\npredictions = np.where(predictions >= 0.5, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:53:33.280274Z","iopub.execute_input":"2021-08-23T21:53:33.280611Z","iopub.status.idle":"2021-08-23T21:53:48.721846Z","shell.execute_reply.started":"2021-08-23T21:53:33.280576Z","shell.execute_reply":"2021-08-23T21:53:48.721079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissions = test_data.drop(labels = [\"keyword\", \"location\", \"text\"], axis = 1)\nsubmissions[\"target\"] = predictions\nsubmissions.to_csv(\"submissions.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-23T21:53:48.723099Z","iopub.execute_input":"2021-08-23T21:53:48.72342Z","iopub.status.idle":"2021-08-23T21:53:48.740714Z","shell.execute_reply.started":"2021-08-23T21:53:48.723384Z","shell.execute_reply":"2021-08-23T21:53:48.73999Z"},"trusted":true},"execution_count":null,"outputs":[]}]}