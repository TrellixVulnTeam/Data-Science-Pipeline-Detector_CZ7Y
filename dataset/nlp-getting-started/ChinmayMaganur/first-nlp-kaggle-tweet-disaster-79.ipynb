{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport plotly\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import stopwords\nimport nltk\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train',train.shape)\nprint('test',train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'][1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train,null%\\n',train.isnull().mean())\nprint()\nprint('test,null%\\n',test.isnull().mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In train,  keyword has 0.8% and loc has 33% missing values\n#In test,  keyword has 0.7% and loc has 33% missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster=train[train['target']==1]\nnon_dist=train[train['target']==0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=disaster['keyword'].value_counts()[:20]\nnd=non_dist['keyword'].value_counts()[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.subplots import make_subplots\nfig=make_subplots(rows=1,cols=2)\nfig.add_traces(go.Bar(y=d.index,x=d.values,orientation='h',name='disaster keywords'),1,1)\nfig.add_traces(go.Bar(y=nd.index,x=nd.values,orientation='h',name='non_dist kewords'),1,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loc_d=disaster['location'].value_counts()[:10]\nloc_nd=non_dist['location'].value_counts()[:10]\nfig=make_subplots(rows=1,cols=2)\nfig.add_traces(go.Bar(y=loc_d.index,x=loc_d.values,orientation='h',name='Most disaster loc'),1,1)\nfig.add_traces(go.Bar(y=loc_nd.index,x=loc_nd.values,orientation='h',name='non_dist Loc '),1,2)\n\n#USA records more disaster teeets\n#Newyork less disaster tweets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.figure_factory as ff\n#calculate len of tweets in both disaster and non\nd1=disaster['text'].apply(len)\nnd1=non_dist['text'].apply(len)\nff.create_distplot([d1,nd1],['len_d1','len_nd1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('mean_len_d1',d1.mean())\nprint('mean_len_nd1',nd1.mean())\n\n#disaster tweets len is more comp to non diasster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_dist['text'].str.len().sort_values(ascending=False)[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#to remove url,punc,#,@,stopwords\nprint(non_dist['text'][1270])\nprint(non_dist['text'][4801])\nprint(non_dist['text'][261])\nprint(non_dist['text'][5379])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster['text'].str.len().sort_values(ascending=False)[:10]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(disaster['text'][614])\nprint(disaster['text'][635])\nprint(disaster['text'][2718])\nprint(disaster['text'][1111])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nimport re\na='http://t.co/FYJWjDkM5I this is it'\na=re.sub('http://[a-z]+\\.[a-z]+/[a-zA-Z]+','',non_dist['text'][6555],)\n#re.findall('http://[a-z]+\\.[a-z]+/[a-zA-Z]+',a)\n\nre.sub('[^\\w]',' ',a)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n'''a='@IcyMagistrate ÛÓher upper armÛÒ those /friggin/ icicle projectilesÛÒ and leg from various other wounds the girl looks like a miniature moreÛÓ'\na.encode(\"ascii\", errors=\"ignore\").decode()\n#output=='@IcyMagistrate her upper arm those /friggin/ icicle projectiles and leg from various other wounds the girl looks like a miniature'http.?://[a-z]+\\.[a-z]+/[a-zA-Z0-9]+ more'\n''''http.?://[a-z]+\\.[a-z]+/[a-zA-Z0-9]+'''\n\n#a= train['text'][614]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text= emoji_pattern.sub(r'', text)\n    text=text.encode('ascii',errors='ignore').decode()\n    #print(text)\n    text=text.lower()\n    text=re.sub('http.?://[a-z]+\\.[a-z]+/[a-zA-Z0-9]+', '',text)\n    text=re.sub('&amp',' and',text)\n    text=re.sub('gt','greater than',text)\n    text=re.sub('lt','lesser than',text)\n    text=re.sub('rt','retweet',text)\n    text=re.sub('n\\'t',' not',text)\n    text=re.sub('\\'s',' is',text)\n    text=re.sub('\\'ll', 'will',text)\n    text=re.sub('\\'ve','have',text)\n    text=re.sub('i\\'m','i am',text)\n    text=re.sub('\\'re',' are',text)\n    #print(text)\n    #print(text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    #print(text)\n    #print(text)\n    text = re.sub('\\w*\\d\\w*','', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['new_text']=train['text'].apply(lambda x: clean_text(x))\ntest['new_text']=train['text'].apply(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['new_text'][1270]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import worldcloud\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nw=WordCloud().generate(train['new_text'][1])\nplt.imshow(w, interpolation='bilinear')\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_train=train[train['target']==1]\nnondisaster=train[train['target']==0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax=plt.subplots(1,2,figsize=[30,10])\nw1=WordCloud(background_color='white').generate(''.join(disaster_train['new_text']))\nax[0].imshow(w1)\nax[0].axis('off')\nax[0].set_title('Disaster Tweets',fontsize=40);\n\nw2=WordCloud(background_color='white').generate(''.join(nondisaster['new_text']))\nax[1].imshow(w2)\nax[1].axis('off')\nax[1].set_title('Non_Disaster Tweets',fontsize=40);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens=word_tokenize(train['text'][0])\nprint('normal tokens',tokens)\ntokens=[t for t in tokens if t not in stopwords.words('english')]\nprint('no stipwords',tokens)\nporter = nltk.PorterStemmer()\nlancaster = nltk.LancasterStemmer()\nsnowball=nltk.SnowballStemmer(language='english')\nprint()\nporter=[porter.stem(t)for t in tokens]\nprint('porter',porter)\nprint()\nlan=[lancaster.stem(t)for t in tokens]\nprint('lancaster',lan)\nprint()\nsnowball=[snowball.stem(t)for t in tokens]\nprint('snowball',snowball)\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text2(text):\n    \n    text = re.sub('<.*?>+', '', text)\n    token=word_tokenize(text)\n    lemm=[lemmatizer.lemmatize(t) for t in token]\n    #token=[t for t in token if t not in stopwords.words('english')]\n    snowball=nltk.SnowballStemmer(language='english')\n    snb=[snowball.stem(t) for t in lemm]\n    text=' '.join(snb)\n    text=re.sub('via|wa|ha|tt|ve','',text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer() \ntrain['new_text2']=train['new_text'].apply(lambda x: clean_text2(x))\ntest['new_text2']=test['new_text'].apply(lambda x: clean_text2(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfrom nltk.stem import WordNetLemmatizer \n  \nlemmatizer = WordNetLemmatizer() \ntoken=word_tokenize(train['new_text'][1270])\nlemm=[lemmatizer.lemmatize(t) for t in token]\nprint(lemm)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_train=train[train['target']==1]\nnondisaster=train[train['target']==0]\nfig,ax=plt.subplots(1,2,figsize=[30,10])\nw1=WordCloud(background_color='white').generate(''.join(disaster_train['new_text2']))\nax[0].imshow(w1)\nax[0].axis('off')\nax[0].set_title('Disaster Tweets',fontsize=40);\n\nw2=WordCloud(background_color='white').generate(''.join(nondisaster['new_text2']))\nax[1].imshow(w2)\nax[1].axis('off')\nax[1].set_title('Non_Disaster Tweets',fontsize=40);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#info extract\n\n#Bag of Words:describe the occurance of the words within documents\n#CountVectorizer():converts the text document into matrix form(count  of each token)\n#tfidf Vectorizer=it also converts the text into matrix format, but here the frequent tokens are given less weightage, and rara tokens are giiven more","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv=CountVectorizer(token_pattern='\\w{2,}',ngram_range=(1,1),analyzer='word',)\ncv.fit(train['new_text2'])\n#print(cv.get_feature_names())\ntrain_vec=cv.transform(train['new_text2'])\ntest_vec=cv.transform(test['new_text2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb=MultinomialNB()\ncv_nb=cross_val_score(nb,X=train_vec,y=train['target'],scoring='f1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_nb.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#TFIDF\n\ntfidf=TfidfVectorizer(analyzer='word',token_pattern=r'\\w{2,}',ngram_range=(1,2),min_df=3)\ntrain_tfidf=tfidf.fit_transform(train['new_text2'])\ntest_tfidf=tfidf.transform(test['new_text2'])\n#cross_val_score(lr,train_tfidf,train['target'],cv=3,scoring='f1').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection  import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(train.text, train.target, test_size=0.2, random_state=22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf=TfidfVectorizer(min_df=0,max_df=0.8,use_idf=True,ngram_range=(1,1))\ntrain_tfidf=tfidf.fit_transform(x_train)\nval_tfidf=tfidf.transform(x_val)\ntest_tfidf=tfidf.transform(test.text)\n\nprint('tfidf_train:',train_tfidf.shape)\nprint('tfidf_validation:',val_tfidf.shape)\nprint('tfidf_test:',test_tfidf.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB().fit(train_tfidf, y_train)\npred=nb.predict(val_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_val,pred))\nprint(confusion_matrix(y_val,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nCounter(pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsubmission['target']=pd.Series(nb.predict(test_tfidf))\nsubmission.to_csv(\"submission11.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic\nlr=LogisticRegression().fit(train_tfidf,y_train)\npred=lr.predict(val_tfidf)\nprint(classification_report(y_val,pred))\nprint(confusion_matrix(y_val,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc=RandomForestClassifier().fit(train_tfidf,y_train)\npred=rfc.predict(val_tfidf)\nprint(classification_report(y_val,pred))\nprint(confusion_matrix(y_val,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb=XGBClassifier().fit(train_tfidf,y_train)\npred=xgb.predict(val_tfidf)\nprint(classification_report(y_val,pred))\nprint(confusion_matrix(y_val,pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(XGBClassifier())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nxgb=XGBClassifier()\nparams={'max_depth':[2,3,4,6,8],'learning_rate':[0.0010,0.005,0.01,0.05,0.1,0.5],'n_estimator':[100,200,300]}\ngrid = GridSearchCV(estimator=xgb, param_grid=params, cv = 3, n_jobs=-1)\ngrid.fit(train_tfidf,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_xgb=XGBClassifier(learning_rate=0.5,max_depth=8,n_estimator=100).fit(train_tfidf,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=rs_xgb.predict(val_tfidf)\nprint(classification_report(pred,y_val))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}