{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n* A LSTM based model is used to make a prediction using the 'text' of the tweet\n* An ANN based model is used to make a prediction using the 'keyword' of the tweet\n* The two models are emsembled to the make the final prediction\n* 200 dimensional GloVe word vectors are used for embedding the 'text' ;  25 dimensional GloVe word vectors are used for embedding the 'keywords'\n* GloVe reference : https://nlp.stanford.edu/projects/glove/","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport os\nimport plotly.graph_objects as go\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(103)\ntorch.cuda.manual_seed(103)\nnp.random.seed(103)\n\ndeviceCount = torch.cuda.device_count()\nprint(deviceCount)\n\ncuda0 = None\nif deviceCount > 0:\n  print(torch.cuda.get_device_name(0))\n  cuda0 = torch.device('cuda:0')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Input","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='target', data=df)\nplt.gca().set_ylabel('tweets')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Downloading and Processing GloVe Files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\n\n%cd '/kaggle'\n!wget -q http://nlp.stanford.edu/data/glove.twitter.27B.zip\n!unzip -q glove.twitter.27B.zip\n\n!ls\n%cd '/kaggle/working'\n\nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_embedding_dimension = 200\nkey_embedding_dimension = 25\n\n\npath_to_glove_file = '/kaggle/glove.twitter.27B.{}d.txt'.format(text_embedding_dimension)\n\nembeddings_index_200 = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index_200[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index_200))\n\n\n\npath_to_glove_file = '/kaggle/glove.twitter.27B.{}d.txt'.format(key_embedding_dimension)\n\nembeddings_index_25 = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index_25[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index_25))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Primary Text Cleaning : remove links and split text into individual words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    \n    # lower case characters only\n    text = text.lower() \n    \n    # remove urls\n    text = re.sub('http\\S+', ' ', text)\n    \n    # only alphabets, spaces and apostrophes \n    text = re.sub(\"[^a-z' ]+\", ' ', text)\n    \n    # remove all apostrophes which are not used in word contractions\n    text = ' ' + text + ' '\n    text = re.sub(\"[^a-z]'|'[^a-z]\", ' ', text)\n    \n    return text.split()\n\ndf['text'] = df['text'].apply(lambda x: clean_text(x))\n\ndf.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis and Further Text Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Finding the Words which are Unknown to GloVe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"unknown_words = []\ntotal_words = 0\n\ndef find_unknown_words(words):\n    \n    global total_words\n    total_words = total_words + len(words)\n    \n    for word in words:\n        if not (word in embeddings_index_200):\n            unknown_words.append(word)\n    \n    return words\n\n\ndf['text'].apply(lambda words: find_unknown_words(words))\n\nprint( f'{len(unknown_words)/total_words*100:5.2} % of words are unknown' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def analyze_unknown_words(unknown_words):\n    \n    unknown_words = np.array(unknown_words)\n    (word, count) = np.unique(unknown_words, return_counts=True)\n    \n    word_freq = pd.DataFrame({'word': word, 'count': count}).sort_values('count', ascending=False)\n\n    fig = go.Figure(data=[go.Table(\n          header=dict(values=list(word_freq.columns),\n                    fill_color='paleturquoise',\n                    align='left'),\n          cells=dict(values=[word_freq['word'], word_freq['count']],\n                    fill_color='lavender',\n                    align='left'))\n          ])\n    fig.update_layout(width=300, height=300, margin={'b':0, 'l':0, 'r':0, 't':0, 'pad':0})\n    fig.show()\n        \nanalyze_unknown_words(unknown_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Looks like a lot of the unknown words are contractions. Let's expand the most common ones.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"contractions  = { \"i'm\" : \"i am\", \"it's\" : \"it is\", \"don't\" : \"do not\", \"can't\" : \"cannot\", \n                  \"you're\" : \"you are\", \"that's\" : \"that is\", \"we're\" : \"we are\", \"i've\" : \"i have\", \n                  \"he's\" : \"he is\", \"there's\" : \"there is\", \"i'll\" : \"i will\", \"i'd\" : \"i would\", \n                  \"doesn't\" : \"does not\", \"what's\" : \"what is\", \"didn't\" : \"did not\", \n                  \"wasn't\" : \"was not\", \"hasn't\" : \"has not\", \"they're\" : \"they are\", \n                  \"let's\" : \"let us\", \"she's\" : \"she is\", \"isn't\" : \"is not\", \"ain't\" : \"not\", \n                  \"aren't\" : \"are not\", \"haven't\" : \"have not\", \"you'll\" : \"you will\", \n                  \"we've\" : \"we have\", \"you've\" : \"you have\", \"y'all\" : \"you all\", \n                  \"weren't\" : \"were not\", \"couldn't\" : \"could not\", \"would've\" : \"would have\", \n                  \"they've\" : \"they have\", \"they'll\" : \"they will\", \"you'd\" : \"you would\", \n                  \"they'd\" : \"they would\", \"it'll\" : \"it will\", \"where's\" : \"where is\", \n                  \"we'll\" : \"we will\", \"we'd\" : \"we would\", \"he'll\" : \"he will\", \n                  \"gov't\" : \"government\", \"shouldn't\" : \"should not\", \"bioterror\" : \"biological terror\", \n                  \"bioterrorism\" : \"biological terrorism\", \"wouldn't\" : \"would not\", \n                  \"won't\" : \"will not\" }\n\n\ndef expand_contractions(words):\n    \n    for i in range(len(words)):\n        if words[i] in contractions:\n            words[i] = contractions[words[i]]\n            \n    return (' '.join(words)).split()\n\n\n# precautionary cleaning for any remaing apostrophes\ndef remove_apostrophes(words):\n    words = ' '.join(words)\n    words = re.sub(\"'\", '', words)\n    return words.split()\n\n\ndf['text'] = df['text'].apply(lambda words: expand_contractions(words))\n\ndf['text'] = df['text'].apply(lambda words: remove_apostrophes(words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unknown_words = []\ntotal_words = 0\n\ndf['text'].apply(lambda words: find_unknown_words(words))\n\nprint( f'{len(unknown_words)/total_words*100:5.2} % of words are unknown' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finding the most frequent stop words and removing them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"words_freq = {}\n\ndef word_frequency(words):\n  for word in words:\n    if word in words_freq:\n      words_freq[word] += 1\n    else:\n      words_freq[word] = 1\n\ndf['text'].apply(lambda words: word_frequency(words))\n\nword = []\ncount = []\nfor w in words_freq:\n  word.append(w)\n  count.append( words_freq[w] )\n\nword = np.array(word)\ncount = np.array(count)\n\nword_freq = pd.DataFrame({'word': word, 'count': count}).sort_values('count', ascending=False)\n\nfig = go.Figure(data=[go.Table(\n      header=dict(values=list(word_freq.columns),\n                fill_color='paleturquoise',\n                align='left'),\n      cells=dict(values=[word_freq['word'], word_freq['count']],\n                fill_color='lavender',\n                align='left'))\n      ])\nfig.update_layout(width=300, height=300, margin={'b':0, 'l':0, 'r':0, 't':0, 'pad':0})\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_words = [ 'the', 'a', 'in', 'to', 'of', 'i', 'and', 'is', 'you', 'for', 'on', 'it', 'my', 'that',\n               'with', 'are', 'at', 'by', 'this', 'have', 'from', 'be', 'was', 'do', 'will', 'as', 'up', \n               'me', 'am', 'so', 'we', 'your', 'has', 'when', 'an', 's', 'they', 'about', 'been', 'there',\n               'who', 'would', 'into', 'his', 'them', 'did', 'w', 'their', 'm', 'its', 'does', 'where', 'th',\n               'b', 'd', 'x', 'p', 'o', 'r', 'c', 'n', 'e', 'g', 'v', 'k', 'l', 'f', 'j', 'z', 'us', 'our',\n               'all', 'can', 'may' ] \n\ndef remove_stop_words(words):\n  result = []\n  for word in words:\n    if not (word in stop_words):\n      result.append(word)\n  return result\n\ndf['text'] = df['text'].apply(lambda words: remove_stop_words(words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Embedding the Text and Keyword","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_embed(words):\n    \n    unknown_indices = []\n    mean = np.zeros(text_embedding_dimension)\n    \n    for i in range(len(words)):\n        if words[i] in embeddings_index_200:\n            words[i] = embeddings_index_200[ words[i] ]\n            mean += words[i]\n        else:\n            unknown_indices.append(i)\n            \n    mean /= len(words)-len(unknown_indices)\n    \n    # unknown words in the text are represented using the mean of the known words\n    for i in unknown_indices:\n        words[i] = mean\n    \n    return np.array(words)\n\ndf['text'] = df['text'].apply(lambda words: text_embed(words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def keyword_embed(keyword, text):\n    \n    if pd.isna(keyword):\n        keyword = np.zeros(25)\n    else:\n        keyword = keyword.lower()\n        keyword = re.sub(\"[^a-z ]+\", ' ', keyword)\n        keywords = keyword.split()\n\n        if len(keywords) == 0:\n            keyword = np.zeros(key_embedding_dimension)\n        else:\n            keyword = np.zeros(key_embedding_dimension)\n            word_count = 0\n            for word in keywords:\n                if word in embeddings_index_25:\n                    keyword += embeddings_index_25[word]\n                    word_count += 1\n\n            if word_count > 0:\n                keyword = keyword / word_count\n \n    return keyword\n\ndf['keyword'] = df.apply(lambda x: keyword_embed(x['keyword'], x['text']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('location', axis=1).sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train and Cross Validation Split\n* The greater cross validation ratio of 0.2 was used during hyperparamter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross_validation_ratio = 0.2\ncross_validation_ratio = 0.05\n\nmask = np.random.rand(len(df)) > cross_validation_ratio\n\ntrain_df = df[mask]\n\nval_df = df[~mask]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_text = train_df['text'].values\nx_train_key = train_df['keyword'].values\n\nx_val_text = val_df['text'].values\nx_val_key = val_df['keyword'].values\n\ny_train = train_df['target'].values\ny_val = val_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train_key = np.array( [i for i in x_train_key] ).reshape(-1, key_embedding_dimension)\nx_val_key = np.array( [i for i in x_val_key] ).reshape(-1, key_embedding_dimension)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ANN Model For Prediction using Keywords","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class ANN_Model(nn.Module):\n    def __init__(self):\n        super().__init__()                          \n        self.fc1 = nn.Linear(key_embedding_dimension, 10)\n        self.fc2 = nn.Linear(10, 1)\n        self.bn1 = nn.BatchNorm1d(10)\n        self.dropout1 = nn.Dropout(p=0.1)\n\n    def forward(self, X):\n        X = self.fc1(X)\n        X = self.bn1(X)\n        X = F.relu(X)\n        X = self.dropout1(X)\n        X = self.fc2(X)\n        X = torch.sigmoid(X)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_model = ANN_Model()\n\nif cuda0 != None:\n  ann_model.to(cuda0)\n\ncriterion_key = nn.BCELoss()\noptimizer_key = torch.optim.Adam(ann_model.parameters(), lr=0.01)\n# scheduler_key = torch.optim.lr_scheduler.ExponentialLR(optimizer_key, gamma=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\n\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nfor epoch in range(300):  \n    \n    ann_model.train()\n\n    tweet = torch.FloatTensor(x_train_key)\n    label = torch.FloatTensor(y_train)\n\n    if cuda0 != None:\n        tweet = tweet.cuda()\n        label = label.cuda()\n\n    pred = ann_model(tweet)\n    pred = pred.reshape(-1)\n\n    loss = criterion_key(pred, label)\n\n    optimizer_key.zero_grad()\n    loss.backward()\n    optimizer_key.step()\n\n    train_losses.append(loss.item())\n    train_accuracies.append( ( (pred>0.5) == (label==1) ).sum().item() / len(x_train_key) )\n\n\n    ann_model.eval()\n\n    with torch.no_grad():\n\n        tweet = torch.FloatTensor(x_val_key)\n        label = torch.FloatTensor(y_val)\n\n        if cuda0 != None:\n            tweet = tweet.cuda()\n            label = label.cuda()\n\n        pred = ann_model(tweet)\n        pred = pred.reshape(-1)\n\n        loss = criterion_key(pred, label)\n\n    val_losses.append(loss.item())\n    val_accuracies.append( ( (pred>0.5) == (label==1) ).sum().item() / len(x_val_key) )\n    \n    if (epoch+1)%50 == 0:\n        print('Epoch {} Summary:'.format(epoch+1))\n        print(f'Train Loss: {train_losses[-1]:7.2f}  Train Accuracy: {train_accuracies[-1]*100:6.3f}%')\n        print(f'Validation Loss: {val_losses[-1]:7.2f}  Validation Accuracy: {val_accuracies[-1]*100:6.3f}%')\n        print('')\n\n    # scheduler_key.step()\n\nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_axis = [i+1 for i in range(len(train_losses))]\n\nplt.plot(x_axis, train_losses, label='training loss')\nplt.plot(x_axis, val_losses, label='validation loss')\nplt.title('Loss for each epoch')\nplt.legend();\nplt.show()\n\nplt.plot(x_axis, train_accuracies, label='training accuracy')\nplt.plot(x_axis, val_accuracies, label='validation accuracy')\nplt.title('Accuracy for each epoch')\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generating Final Predictions of the ANN Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_model.eval()\n\n# predictions for the training set\nwith torch.no_grad():\n\n    tweet = torch.FloatTensor(x_train_key)\n\n    if cuda0 != None:\n        tweet = tweet.cuda()\n\n    pred_train_key = ann_model(tweet)\n    pred_train_key = pred_train_key.reshape(-1)\n    \n\n# predictions for the cross validation set\nwith torch.no_grad():\n\n    tweet = torch.FloatTensor(x_val_key)\n\n    if cuda0 != None:\n        tweet = tweet.cuda()\n\n    pred_val_key = ann_model(tweet)\n    pred_val_key = pred_val_key.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM Model for Prediction using tweet Text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTMnetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden_size = 50\n        self.input_size = text_embedding_dimension\n        self.num_layers = 1\n        self.bidirectional = True\n        self.num_directions = 1\n        self.dropout1 = nn.Dropout(p=0.3)\n\n        if self.bidirectional:\n            self.num_directions = 2\n \n        self.lstm = nn.LSTM( self.input_size, self.hidden_size, self.num_layers, \n                             bidirectional=self.bidirectional )\n        \n        self.linear = nn.Linear(self.hidden_size*self.num_directions,1)\n\n    def forward(self, tweet):\n        \n        lstm_out, _ = self.lstm( tweet.view(len(tweet), 1, -1) )\n\n        x = self.dropout1( lstm_out.view(len(tweet),-1) )\n        \n        output = self.linear(x)\n        \n        pred = torch.sigmoid( output[-1] )\n        \n        return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_model = LSTMnetwork()\n\nif cuda0 != None:\n  lstm_model.to(cuda0)\n\ncriterion_text = nn.BCELoss()\noptimizer_text = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\nscheduler_text = torch.optim.lr_scheduler.ExponentialLR(optimizer_text, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_model_weight = 0.3\nlstm_model_weight = 1-ann_model_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start_time = time.time()\n\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nfor epoch in range(4):  \n\n    epoch_start_time = time.time()\n\n    print('Epoch : {}'.format(epoch+1))\n\n    trainLoss = 0\n    correct = 0\n\n    lstm_model.train()\n\n    for i in range(len(x_train_text)):\n\n        lstm_model.zero_grad()\n\n        tweet = torch.FloatTensor(x_train_text[i])\n        label = torch.FloatTensor( np.array([y_train[i]]) )\n\n        if cuda0 != None:\n            tweet = tweet.cuda()\n            label = label.cuda()\n\n        pred = lstm_model(tweet)\n\n        loss = criterion_text(pred, label)\n\n        lambdaParam = torch.tensor(0.001)\n        l2_reg = torch.tensor(0.)\n\n        if cuda0 != None:\n          lambdaParam = lambdaParam.cuda()\n          l2_reg = l2_reg.cuda() \n\n        for param in lstm_model.parameters():\n          if cuda0 != None:\n            l2_reg += torch.norm(param).cuda()\n          else:\n            l2_reg += torch.norm(param)\n\n        loss += lambdaParam * l2_reg\n\n        pred = pred.item()*lstm_model_weight + pred_train_key[i].item()*ann_model_weight\n        \n        if pred > 0.5:\n            pred = 1\n        else:\n            pred = 0\n\n        if pred == int( label.item() ):\n            correct += 1\n\n        trainLoss += loss.item()\n\n        optimizer_text.zero_grad()\n        loss.backward()\n        optimizer_text.step()\n\n        if (i+1)%1000 == 0:\n            print('Processed {} tweets out of {}'.format(i+1, len(x_train_text)))\n\n    train_losses.append(trainLoss/len(x_train_text))\n    train_accuracies.append( correct/len(x_train_text) )\n\n    valLoss = 0\n    correct = 0\n\n    lstm_model.eval()\n\n    with torch.no_grad():\n\n        for i in range(len(x_val_text)):\n\n            tweet = torch.FloatTensor(x_val_text[i])\n            label = torch.FloatTensor( np.array([y_val[i]]) )\n\n            if cuda0 != None:\n                tweet = tweet.cuda()\n                label = label.cuda()\n\n            pred = lstm_model( tweet )\n\n            loss = criterion_text(pred, label)\n\n            valLoss += loss.item()\n\n            pred = pred.item()*lstm_model_weight + pred_val_key[i].item()*ann_model_weight\n\n            if pred > 0.5:\n                pred = 1\n            else:\n                pred = 0\n\n            if pred == int( label.item() ):\n                correct += 1\n\n    val_losses.append(valLoss/len(x_val_text))\n    val_accuracies.append( correct/len(x_val_text) )\n\n    print('Epoch Summary:')\n    print(f'Train Loss: {train_losses[-1]:7.2f}  Train Accuracy: {train_accuracies[-1]*100:6.3f}%')\n    print(f'Validation Loss: {val_losses[-1]:7.2f}  Validation Accuracy: {val_accuracies[-1]*100:6.3f}%')\n    print(f'Duration: {time.time() - epoch_start_time:.0f} seconds')\n    print('')\n\n    scheduler_text.step()\n\nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_axis = [i+1 for i in range(len(train_losses))]\n\nplt.plot(x_axis, train_losses, label='training loss')\nplt.plot(x_axis, val_losses, label='validation loss')\nplt.title('Loss for each epoch')\nplt.legend();\nplt.show()\n\nplt.plot(x_axis, train_accuracies, label='training accuracy')\nplt.plot(x_axis, val_accuracies, label='validation accuracy')\nplt.title('Accuracy for each epoch')\nplt.legend();\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading and Preprocessing Test Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['text'] = test_df['text'].apply(lambda x: clean_text(x))\n\ntest_df['text'] = test_df['text'].apply(lambda words: expand_contractions(words))\n\ntest_df['text'] = test_df['text'].apply(lambda words: remove_apostrophes(words))\n\ntest_df['text'] = test_df['text'].apply(lambda words: remove_stop_words(words))\n\ntest_df['text'] = test_df['text'].apply(lambda words: text_embed(words))\n\ntest_df['keyword'] = test_df.apply(lambda x: keyword_embed(x['keyword'], x['text']), axis=1)\n\ntest_df.drop('location', axis=1).sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test_text = test_df['text'].values\nx_test_key = test_df['keyword'].values\n\nx_test_key = np.array( [i for i in x_test_key] ).reshape(-1, key_embedding_dimension)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating Predictions for the Test Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_model.eval()\n\nwith torch.no_grad():\n\n    tweet = torch.FloatTensor(x_test_key)\n\n    if cuda0 != None:\n        tweet = tweet.cuda()\n\n    pred_test_key = ann_model(tweet)\n    pred_test_key = pred_test_key.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_model.eval()\n\nwith torch.no_grad():\n\n    for i in range(len(x_test_text)):\n\n        tweet = torch.FloatTensor(x_test_text[i])\n\n        if cuda0 != None:\n            tweet = tweet.cuda()\n\n        pred = lstm_model( tweet )\n\n        pred = pred.item()*lstm_model_weight + pred_test_key[i].item()*ann_model_weight\n\n        if pred > 0.5:\n            pred = 1\n        else:\n            pred = 0\n\n        test_predictions.append(pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Saving the Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = np.array(test_predictions)\n\nids = test_df['id'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'id': ids, 'target': test_predictions})\n\noutput.to_csv('/kaggle/working/my_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}