{"cells":[{"metadata":{},"cell_type":"markdown","source":"#                                   NLP with disaster tweets"},{"metadata":{},"cell_type":"markdown","source":"### Twitter is an American microblogging and social networking service on which users post and interact with messages known as \"tweets\". "},{"metadata":{},"cell_type":"markdown","source":"### Here we are predicting whether a given tweet is about a real disaster or not."},{"metadata":{},"cell_type":"markdown","source":"Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data."},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{},"cell_type":"markdown","source":"We are importing libraries nltk,numpy,pandas and sklearn.\nThe Natural Language ToolKit is one of the best-known and most-used NLP libraries, useful for all sorts of tasks from t tokenization, stemming, tagging, parsing, and beyond."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading dataset"},{"metadata":{},"cell_type":"markdown","source":"There are three files train.csv, test.csv and sample_submission.csv"},{"metadata":{},"cell_type":"markdown","source":"Each sample in the train and test set has the following information:\n\n* The text of a tweet\n* A keyword from that tweet (although this may be blank!)\n* The location the tweet was sent from (may also be blank)"},{"metadata":{},"cell_type":"markdown","source":"### Columns\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"},{"metadata":{},"cell_type":"markdown","source":"The dataset contains 10,000 tweets that are hand classified."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df=pd.read_csv('../input/nlp-getting-started/test.csv')\nsample_submission_df=pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"target=train_df['target'].value_counts()\nsns.barplot(target.index,target,edgecolor=(0,0,0),linewidth=1.5)\nplt.title('Comparing disaster tweets and non disaster tweets',fontsize=15)\nplt.xticks(fontsize=20)\nplt.ylabel('Samples',fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keyword=train_df['keyword'].value_counts()[:20]\nplt.figure(figsize=(10,7))\nsns.barplot(keyword.index,keyword.values,edgecolor=(0,0,0),linewidth=2)\nplt.title('Top 20 keywords',fontsize=20)\nplt.xticks(fontsize=20,rotation=270)\nplt.yticks(fontsize=20)\nplt.xlabel('Keywords',fontsize=20,color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"location=train_df['location'].value_counts()[:20]\nplt.figure(figsize=(10,7))\nsns.barplot(location.index,location.values,edgecolor=(0,0,0),linewidth=2)\nplt.title('Top 20 Location',fontsize=20)\nplt.xticks(fontsize=20,rotation=270)\nplt.yticks(fontsize=20)\nplt.xlabel('Locations',fontsize=20,color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_df[train_df['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train_df[train_df['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets',fontsize=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train_df[train_df['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train_df[train_df['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='blue')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweets',fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Processing"},{"metadata":{},"cell_type":"markdown","source":"Text Processing is one of the most common task in many ML applications.\n"},{"metadata":{},"cell_type":"markdown","source":"The text column in our dataset contains hyperlinks, punctuation, stop words, numbers. So we have to remove all these using text processing. "},{"metadata":{},"cell_type":"markdown","source":"## Convert text to lowercase\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lower(words):\n    return words.lower()\ntrain_df['text']=train_df['text'].apply(lambda x:lower(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_numbers(words):\n    return re.sub(r'\\d+','',words)\ntrain_df['text']=train_df['text'].apply(lambda x: remove_numbers(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":" def remove_punctuation(words):\n    table=str.maketrans('','',string.punctuation)\n    return words.translate(table)\ntrain_df['text']=train_df['text'].apply(lambda x: remove_punctuation(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenization"},{"metadata":{},"cell_type":"markdown","source":"Tokenization is the first step in NLP. It is the process of breaking strings into tokens which in turn are small structures or units. Tokenization involves three steps which are breaking a complex sentence into words, understanding the importance of each word with respect to the sentence and finally produce structural description on an input sentence."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['text']=train_df['text'].apply(lambda x:word_tokenize(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing Stop words"},{"metadata":{},"cell_type":"markdown","source":"Stop words are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(words):\n    stop_words=set(stopwords.words('english'))\n    return [word for word in words if word not in stop_words]\ntrain_df['text']=train_df['text'].apply(lambda x: remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing links"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_links(words):\n    \n    return [re.sub(r'(https?://\\S+)','',word)for word in words]\ntrain_df['text']=train_df['text'].apply(lambda x:remove_links(x))\n                 ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stemming "},{"metadata":{},"cell_type":"markdown","source":"Stemming usually refers to normalizing words into its base form or root form."},{"metadata":{"trusted":true},"cell_type":"code","source":"# def stemming(words):\n#     ps=PorterStemmer()\n#     return [ps.stem(word) for word in words]\n# train_df['text']=train_df['text'].apply(lambda x: stemming(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lemmatizing"},{"metadata":{},"cell_type":"markdown","source":"In simpler terms, it is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors."},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatizing(words):\n    lemmatizer =WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\ntrain_df['text']=train_df['text'].apply(lambda x: lemmatizing(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def final_text(words):\n     return ' '.join(words)\ntrain_df['text']=train_df['text'].apply(lambda x:final_text(x))\n   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning "},{"metadata":{},"cell_type":"markdown","source":"The words need to be encoded as integers or floating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization).\n\nThe scikit-learn library offers easy-to-use tools to perform both tokenization and feature extraction of your text data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvect=TfidfVectorizer(min_df=2\n                      ,max_features = None,analyzer=\"word\",  ngram_range=(1,3) # (1,6)\n                           ).fit(train_df['text'])\nx_train_vect=vect.transform(train_df['text'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.linear_model import LogisticRegression\nmodel=LogisticRegression()\nmodel.fit(x_train_vect,train_df['target'])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=model.predict(vect.transform(test_df['text']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsample_submission_df['target']=predictions\nsample_submission_df.to_csv('submission.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}