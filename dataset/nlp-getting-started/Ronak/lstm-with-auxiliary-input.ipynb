{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Disaster Tweets Classification\n\nThis kernel intends to show data wrangling techiniques for tweets and using multiple inputs LSTM model (using Keras Funtional API). The techniques used here provides about 0.80 (0.79987 to be precise) score on leaderboard.\n\nI plan to update it as I figure better ways to preprocess ad model data. In case you are here reading this, please let me know your feedback/questions in comments.\n\n* [Data Preprocessing](Data-preprocessing)\n* [LSTM Model](LSTM-Model)"},{"metadata":{"id":"2m8M33MAVGI9","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom collections import Counter\nimport statistics\nimport pickle\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\n\nimport spacy\nimport en_core_web_sm   #spacy model\nnlp = en_core_web_sm.load(disable=['parser', 'ner'])\n\nfrom tensorflow import keras\nfrom tensorflow.keras import models, initializers, regularizers, Input\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense, Dropout, Embedding, concatenate, LSTM, Bidirectional\nimport tensorflow as tf; tf.random.set_seed(420)","execution_count":null,"outputs":[]},{"metadata":{"id":"SGVatc3alNfJ","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"OmZZkzCKlSQu","outputId":"01aef1f0-98ce-4ac4-c57e-9dd83936e5f5","trusted":true},"cell_type":"code","source":"print(df_train.shape, \"\\n\")\nprint(df_train.columns, \"\\n\")\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check some values for keyword and location\nprint(df_train['keyword'].unique()[:10])\nprint(df_train['location'].unique()[:10])","execution_count":null,"outputs":[]},{"metadata":{"id":"XCMo_fhSNIJk","outputId":"3f74e3de-e5d7-445e-b0dc-29f819d03b68","trusted":true},"cell_type":"code","source":"# Checking class balance\n(df_train['target'].value_counts()*100)/df_train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"20aaarlINSMM"},"cell_type":"markdown","source":"57% not real disaster and 43% real disaster tweet"},{"metadata":{"id":"2dBdq1gLm0QP"},"cell_type":"markdown","source":"# Data preprocessing"},{"metadata":{"id":"HVrJM3rIlYfu","trusted":true},"cell_type":"code","source":"# function to remove all links from tweets\n# Input - @bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\n# Output - @bbcmtd Wholesale Markets ablaze\ndef remove_http_links(string):\n    string = re.sub(r'http\\S+', '', str(string))\n    string = re.sub(r'http', '', string)\n    return string","execution_count":null,"outputs":[]},{"metadata":{"id":"JcDuM64itNmF","outputId":"06ab8e4c-b114-4fa0-a6a6-c37adad5fc66","trusted":true},"cell_type":"code","source":"#remove_http_links('RT @HuffPostComedy: We should build a wall that keeps Burning Man attendees from coming home http://t.co/xwVW1sft4I http://t.co/j7HUKhWmal')","execution_count":null,"outputs":[]},{"metadata":{"id":"KUne5deFnLdN","trusted":true},"cell_type":"code","source":"# function to remove metions to other accounts\n# Input - @bbcmtd Wholesale Markets ablaze\n# Output - Wholesale Markets ablaze\n\ndef remove_mentions(string):\n    return re.sub(r'@\\S+', '', str(string))","execution_count":null,"outputs":[]},{"metadata":{"id":"5AdRJA7Ot3u-","outputId":"48b41d62-b9ba-4d50-e652-94e75c430d8e","trusted":true},"cell_type":"code","source":"#remove_mentions('RT @HuffPostComedy: We should build a wall that keeps Burning Man attendees from coming home ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_html_symbols(string):\n    return string.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replace_html_symbols(\"abcd &amp; and are &gt; <\")","execution_count":null,"outputs":[]},{"metadata":{"id":"KdSGlE_itWNj","trusted":true},"cell_type":"code","source":"# Replace %20 as space char\n# Input - blew%20up\n# Output - blew up\ndef replace_with_space(string):\n    s = re.sub(r'(%20)+', ' ', str(string))\n    s = re.sub(r'(&amp)+|(&AMP)+|(&)+', ' and ', str(s))\n    return s","execution_count":null,"outputs":[]},{"metadata":{"id":"OqOBwgx2tW8q","trusted":true},"cell_type":"code","source":"# function to capture hastags in separate column\n# Input - You just got GIF bombed #AfricansInSF #BeyondGPS\n# Output - AfricansInSF, BeyondGPS\ndef capture_hashtags(string):\n    tags_list = re.findall(r'#[\\S]*',str(string))\n    if len(tags_list) > 0:\n        return ' , '.join(tags_list).replace('#','')\n    else:\n        return ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hahaha - to laugh, lol to laugh - haha\\w*, (lo*l)\ndef convert_haha_lol(string):\n    return re.sub(r'\\b(ha+h+a\\w*)|\\b(lo+l\\w*)', ' laugh ', string)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# convert ccontractions to proper word (https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions)\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#contractions.get(\"I'll\")\ndef contractions_to_expansions(string):\n    string_list = string.split(' ')\n    for i,word in enumerate(string_list):\n        if contractions.get(word, \"--\") != \"--\":\n            string_list[i] = contractions.get(word)\n    return \" \".join(string_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contractions_to_expansions(\"y'all'd've asked where's food?\") #Test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove stop words\nall_stopwords = nlp.Defaults.stop_words\n\ndef remove_stopwords(sentence):\n    tokens = sentence.split(\" \")\n    tokens_filtered= [word for word in tokens if not word in all_stopwords]\n    return (\" \").join(tokens_filtered)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove_stopwords(\"hello how are you ? how's everything\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace emoticon\nemoticon_dict = {\":)\": \"laugh\", \":-)\": \"laugh\", \";)\": \"laugh\", \";-)\": \"laugh\", \":P\": \"laugh\", \":-P\": \"laugh\", \":D\": \"laugh\", \n                 \":-D\": \"laugh\", \":(\": \"sad\", \":-(\": \"sad\", \":~(\": \"sad\", \":-|\": \"sad\", \">:-(\": \"sad\", \"8-)\": \"laugh\", \n                 \":-O\": \"sad\", \"8-O\": \"sad\", \">8-O\": \"sad\"}\ndef replace_emoticons(string):\n    for key in emoticon_dict.keys():\n        if key in string:\n            string = string.replace(key, \" \"+emoticon_dict[key]+\" \")\n    return string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replace_emoticons(\"@LauradeHolanda I have the Forrest version from '83 that's bloody awful as well :))) xxx\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lemmatization\ndef lemma(string):\n    text = []\n    for tok in nlp(string):\n        if tok.lemma_ != \"-PRON-\":\n            text.append(tok.lemma_)\n        else:\n            text.append(str(tok))\n    return \" \".join(text)","execution_count":null,"outputs":[]},{"metadata":{"id":"U1Y8tWAguj6m","trusted":true},"cell_type":"code","source":"# function to remove special characters (but not emojis)\ndef remove_special_chars(string):\n    string = re.sub(r'@','at', str(string))\n    string = re.sub(r'&','and', str(string)) #if there still are any\n    return re.sub(r'[^a-z ]', ' ', string)","execution_count":null,"outputs":[]},{"metadata":{"id":"NebYAceLtWx4","trusted":true},"cell_type":"code","source":"# function to remove extra white spaces\ndef remove_extra_blanks(string):\n    return re.sub(r'\\s+', ' ', str(string).strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def perform_all_preprocessing(string):\n    string = string.lower()\n    string = remove_http_links(string)\n    string = remove_mentions(string)\n    string = contractions_to_expansions(string)\n    string = replace_with_space(string)\n    string = replace_html_symbols(string)\n    string = replace_emoticons(string)\n    string = convert_haha_lol(string)\n    string = remove_stopwords(string)\n    string = lemma(string)\n    string = remove_special_chars(string)\n    string = remove_extra_blanks(string)\n    return string","execution_count":null,"outputs":[]},{"metadata":{"id":"sYIpNTymtWK_","trusted":true},"cell_type":"code","source":"df_train['text'].fillna('', inplace=True)\ndf_train['keyword'].fillna('', inplace=True)\ndf_train['location'].fillna('', inplace=True)\n\ndf_train['text'] = df_train['text'].apply(perform_all_preprocessing)\n\n# create separate columns with hashtags\ndf_train['hashtags'] = df_train['text'].apply(capture_hashtags)\n\nfor col in ['keyword', 'location']:\n    df_train[col] = df_train[col].apply(str.lower)\n    df_train[col] = df_train[col].apply(replace_with_space)\n    df_train[col] = df_train[col].apply(remove_special_chars) #maybe not do this\n    df_train[col] = df_train[col].apply(remove_extra_blanks)","execution_count":null,"outputs":[]},{"metadata":{"id":"37a26i48BGrs","outputId":"6bd27d3b-9c5c-4f21-e1db-4143b8d8ab8a","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Gfn88rt38o1s","outputId":"9eb40eee-9f2d-465d-9987-86e37f128707","trusted":true},"cell_type":"code","source":"df_test['text'].fillna('', inplace=True)\ndf_test['keyword'].fillna('', inplace=True)\ndf_test['location'].fillna('', inplace=True)\n\ndf_test['text'] = df_test['text'].apply(perform_all_preprocessing)\n\n# create separate columns with hashtags\ndf_test['hashtags'] = df_test['text'].apply(capture_hashtags)\n\nfor col in ['keyword', 'location']:\n    df_test[col] = df_test[col].apply(str.lower)\n    df_test[col] = df_test[col].apply(replace_with_space)\n    df_test[col] = df_test[col].apply(remove_special_chars) #maybe not do this\n    df_test[col] = df_test[col].apply(remove_extra_blanks)\n\nprint(df_test.shape, \"\\n\")\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"hrSL5119-_--","outputId":"07b32dec-4fd1-401e-cfd7-3ca2b97b973b","trusted":true},"cell_type":"code","source":"# apprently tere are some duplocates too in text with conflicting target response\ndf_train[df_train['id'].isin([1409, 1420])]['text'].values","execution_count":null,"outputs":[]},{"metadata":{"id":"nw6TMfnZjB95","outputId":"987510a1-8501-4b46-8958-ac7f9b099e40","trusted":true},"cell_type":"code","source":"print(df_train.shape)\ndf_train.drop_duplicates(['text']).shape","execution_count":null,"outputs":[]},{"metadata":{"id":"iRgGR8-Nk3y9","trusted":true},"cell_type":"code","source":"id1 = df_train['id'].values.tolist()\nid2 = df_train.drop_duplicates(subset='text')['id'].values.tolist()\nid_del = list(set(id1) - set(id2))\ntext_del = df_train[df_train['id'].isin(id_del)]['text'].unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{"id":"6CFIM-92pPbx","outputId":"1416f40c-287d-4de4-815b-0ec7f3d874e3","trusted":true},"cell_type":"code","source":"print(len(id_del), len(text_del))","execution_count":null,"outputs":[]},{"metadata":{"id":"mUT-sjFxoBhM","trusted":true},"cell_type":"code","source":"text_rep = Counter(df_train[df_train['text'].isin(text_del)]['text'].values.tolist())\ntext_rep = sorted(text_rep.items(), key=lambda x: x[1], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"_ytaJmpUqx72","trusted":true},"cell_type":"code","source":"id = []; keyword = []; location = []; tweets = []; hashtags = []; target = []\nerror_texts = []\n\nfor t in text_del:\n    df_temp = df_train[df_train['text'] == t]\n    flag = 0\n    try:\n        tgt = statistics.mode(df_temp['target'].values)\n        target.append(tgt)\n    except:\n        error_texts.append(t)\n        flag = 1\n\n    if flag != 1:\n        try:\n            kwd = statistics.mode(df_temp['keyword'].values)\n            keyword.append(kwd)\n        except:\n            keyword.append(df_temp['keyword'].values[0])\n\n        try:\n            ltn = statistics.mode(df_temp['location'].values)\n            location.append(ltn)\n        except:\n            location.append(df_temp['location'].values[0])\n\n        hashtags.append(df_temp['hashtags'].values[0])\n        id.append(df_temp['id'].values[0])\n        tweets.append(t)","execution_count":null,"outputs":[]},{"metadata":{"id":"6AzU4FXKuj2V","trusted":true},"cell_type":"code","source":"df_train_2 = pd.DataFrame.from_dict({'id':id, 'keyword':keyword, 'location':location, 'text': tweets, 'target':target, 'hashtags':hashtags})","execution_count":null,"outputs":[]},{"metadata":{"id":"z7NiaVXQ58jd","outputId":"b8ef8b1e-3a19-4146-ac19-40249b447fc5","trusted":true},"cell_type":"code","source":"#train_df = df_train_1.append(df_train_2, ignore_index=True)\ntrain_df = df_train[~df_train['text'].isin(text_del)]\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"kFpPBVak9Z46","outputId":"018595aa-a3e5-49d0-9aea-836166744e59","trusted":true},"cell_type":"code","source":"train_df['target'].value_counts()/train_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding single entry for repeated tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.concat([train_df, df_train_2], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['target'].value_counts()/train_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"id":"JRwHyc-q90Y6"},"cell_type":"markdown","source":"**Let's ignore `error_texts` for now, will handle them later**"},{"metadata":{"id":"cF_2Xa9vZI49"},"cell_type":"markdown","source":"## Feature engineering/tokenization"},{"metadata":{"id":"gRwmOQ82FcGv","trusted":true},"cell_type":"code","source":"# Range (inclusive) of n-gram sizes for tokenizing text.\nNGRAM_RANGE = (1, 2)\n\n# Limit on the number of features\nTOP_K_ngram = 5000\n\n# One of 'word', 'char'.\nTOKEN_MODE = 'word'\n\nMIN_DOCUMENT_FREQUENCY = 2\n\ndef ngram_vectorize(train_texts, train_labels, TOP_K_ngram):\n    \n    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n    kwargs = {\n            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n            'dtype': 'int32',\n            'strip_accents': 'unicode',\n            'decode_error': 'replace',\n            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n            'min_df': MIN_DOCUMENT_FREQUENCY,\n    }\n    vectorizer = TfidfVectorizer(**kwargs)\n\n    # Learn vocabulary from training texts and vectorize training texts.\n    x_train = vectorizer.fit_transform(train_texts)\n\n    # Select top 'k' of the vectorized features.\n    selector = SelectKBest(f_classif, k=min(TOP_K_ngram, x_train.shape[1]))\n    selector.fit(x_train, train_labels)\n    x_train = selector.transform(x_train).astype('float32')\n    \n    #with open('/mnt/d/Kaggle/nlp-getting-started/vectorizer_selector.pkl', 'wb') as vect_sel:\n    with open('./vectorizer_selector.pkl', 'wb') as vect_sel:\n      vect_select_pkl = {'vectorizer':vectorizer, 'selector':selector}\n      pickle.dump(vect_select_pkl, vect_sel, pickle.HIGHEST_PROTOCOL)\n\n    x_train = x_train.todense()\n    \n    return x_train","execution_count":null,"outputs":[]},{"metadata":{"id":"i7dcV2-YUHrc"},"cell_type":"markdown","source":"# LSTM Model"},{"metadata":{"id":"rOkkoVgulJnp","outputId":"addeab8d-ccd0-432c-c775-f3bc37491d21","trusted":true},"cell_type":"code","source":"#!wget http://nlp.stanford.edu/data/glove.6B.zip\n#!unzip glove*.zip\n#!ls","execution_count":null,"outputs":[]},{"metadata":{"id":"3PGtPqLoUIPn","trusted":true},"cell_type":"code","source":"# Limit on the number of features. We use the top 5K features.\nTOP_K = 5000\n\n# Limit on the length of text sequences. Sequences longer than this will be truncated.\nMAX_SEQUENCE_LENGTH = 30\n\ndef sequence_vectorize(train_texts):\n    # Create vocabulary with training texts.\n    tokenizer = text.Tokenizer(num_words=TOP_K)\n    tokenizer.fit_on_texts(train_texts)\n\n    # Vectorize training and validation texts.\n    x_train = tokenizer.texts_to_sequences(train_texts)\n\n    # Get max sequence length.\n    max_length = len(max(x_train, key=len))#; print(\"max_length: \", max_length)\n    if max_length > MAX_SEQUENCE_LENGTH:\n        max_length = MAX_SEQUENCE_LENGTH\n\n    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n\n    #with open('/mnt/d/Kaggle/nlp-getting-started/tokenizer_max_length.pkl', 'wb') as seq_tranform:\n    with open('./tokenizer_max_length.pkl', 'wb') as seq_tranform:\n        seq_tranform_pkl = {'tokenizer':tokenizer, 'max_length':max_length}\n        pickle.dump(seq_tranform_pkl, seq_tranform, pickle.HIGHEST_PROTOCOL)\n\n    return x_train, tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_embedding_matrix(word_index, embedding_dim):\n    \"\"\"Gets embedding matrix from the embedding index data.\n\n    # Arguments\n        word_index: dict, word to index map that was generated from the data.\n        embedding_dim: int, dimension of the embedding vectors.\n\n    # Returns\n        dict, word vectors for words in word_index from pre-trained embedding.\n    \"\"\"\n\n    # Read the pre-trained embedding file and get word to word vector mappings.\n    embedding_matrix_all = {}\n\n    # We are using 50d GloVe embeddings.\n    fname = '../input/glove6b50dtxt/glove.6B.50d.txt'\n    with open(fname) as f:\n        for line in f:  # Every line contains word followed by the vector value\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embedding_matrix_all[word] = coefs\n\n    # Prepare embedding matrix with just the words in our word_index dictionary\n    num_words = min(len(word_index) + 1, TOP_K)\n    embedding_matrix = np.zeros((num_words, embedding_dim))\n\n    for word, i in word_index.items():\n        if i >= TOP_K:\n            continue\n        embedding_vector = embedding_matrix_all.get(word)\n        if embedding_vector is not None:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lstm_model(embedding_dim, dropout_rate, input_shape, keyw_shape, num_classes,\n                 num_features, use_pretrained_embedding=False, is_embedding_trainable=False, embedding_matrix=None):\n\n    main_input = Input(shape=(input_shape[0]))\n    keyword_input = Input(shape=(keyw_shape[0]))\n    \n    if use_pretrained_embedding:\n        model = Embedding(input_dim=num_features, output_dim=embedding_dim, input_length=input_shape[0],\n                            weights=[embedding_matrix], trainable=is_embedding_trainable)(main_input)\n    else:\n        model = Embedding(input_dim=num_features, input_length=input_shape[0])(main_input)\n    \n    model = Bidirectional(LSTM(32, return_sequences=True))(model)\n    model = Bidirectional(LSTM(32, return_sequences=False))(model)\n    \n    model = Dense(32, activation='relu')(model)\n    keyword = Dense(32, activation='relu')(keyword_input)\n    \n    model_ = concatenate([model, keyword], axis=1)\n    model_ = Dropout(rate=dropout_rate)(model_)\n    model_ = Dropout(rate=dropout_rate)(model_)\n    model_ = Dense(32, activation='relu')(model_)\n    model_ = Dropout(rate=dropout_rate)(model_)\n    \n    model_pred = Dense(1, activation='sigmoid')(model_)\n    model_1 = keras.Model(inputs=[main_input, keyword_input], outputs=[model_pred])\n    \n    return model_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train_df[['text', 'keyword']], train_df['target'], test_size=0.3\n                                                  , stratify=train_df['target'], random_state=42)\n\nX_train_keyw = X_train.iloc[:,1]\nX_train = X_train.iloc[:,0]\nX_val_keyw = X_val.iloc[:,1]\nX_val = X_val.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, word_index = sequence_vectorize(X_train)\nX_train_keyw = ngram_vectorize(X_train_keyw, y_train, 200)\n\nwith open('./tokenizer_max_length.pkl', 'rb') as test_transformation:\n  test_transformation_dict = pickle.load(test_transformation)\n  tokenizer = test_transformation_dict['tokenizer']\n  max_length = test_transformation_dict['max_length']\n\n#tokenizer.texts_to_sequences('fire burn')\nX_val = tokenizer.texts_to_sequences(X_val)\nX_val = sequence.pad_sequences(X_val, maxlen=max_length)\n\nwith open('./vectorizer_selector.pkl', 'rb') as test_transformation:\n  test_transformation_dict = pickle.load(test_transformation)\n  _vectorizer_ = test_transformation_dict['vectorizer']\n  _selector_ = test_transformation_dict['selector']\n\nX_val_keyw = _vectorizer_.transform(X_val_keyw)\nX_val_keyw = _selector_.transform(X_val_keyw)\nX_val_keyw = X_val_keyw.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_train_keyw.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lstm_model(embedding_dim=50, dropout_rate=0.4,\n                     input_shape=X_train.shape[1:], keyw_shape=X_train_keyw.shape[1:], \n                     num_classes=2, num_features=TOP_K, use_pretrained_embedding=True, \n                     is_embedding_trainable=False, embedding_matrix=_get_embedding_matrix(word_index, 50))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True, dpi=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functional API Model\n\ndef train_fine_tuned_functional_api_model(data, learning_rate=7e-5, epochs=200, batch_size=32, dropout_rate=0.5\n                                          , embedding_dim=50):\n\n    # Get the data.\n    (x_train, x_train_key, train_labels), (x_val, x_val_key, val_labels), word_index = data\n\n    # Number of features will be the embedding input dimension. Add 1 for the\n    # reserved index 0.\n    num_features = min(len(word_index) + 1, TOP_K)\n\n    embedding_matrix = _get_embedding_matrix(word_index, embedding_dim)\n\n    # Create model instance. First time we will train rest of network while\n    # keeping embedding layer weights frozen. So, we set\n    # is_embedding_trainable as False.\n    \n    model = lstm_model(embedding_dim=embedding_dim, \n                       dropout_rate=dropout_rate, \n                       input_shape=x_train.shape[1:],\n                       keyw_shape=X_train_keyw.shape[1:],\n                       num_classes=2,\n                       num_features=num_features,\n                       use_pretrained_embedding=True,\n                       is_embedding_trainable=False,\n                       embedding_matrix=embedding_matrix)\n\n    # Compile model with learning parameters.\n    loss = BinaryCrossentropy(from_logits=True)\n    optimizer = Adam(lr=learning_rate)\n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n    # Create callback for early stopping on validation loss. If the loss does\n    # not decrease in two consecutive tries, stop training.\n    callbacks = [EarlyStopping(monitor='val_loss', patience=2)]\n\n    # Train and validate model.\n    model.fit([x_train, x_train_key],\n              train_labels,\n              epochs=epochs,\n              callbacks=callbacks,\n              validation_data=([x_val, x_val_key], val_labels),\n              verbose=2,  # Logs once per epoch.\n              batch_size=batch_size)\n\n    # Save the model.\n    model.save_weights('./fine_tuned_lstm_model_with_pre_trained_embedding.h5')\n\n    # Create another model instance. This time we will unfreeze the embedding\n    # layer and let it fine-tune to the given dataset.\n    model = lstm_model(embedding_dim=embedding_dim,\n                         dropout_rate=dropout_rate,\n                         input_shape=x_train.shape[1:],\n                         keyw_shape=X_train_keyw.shape[1:], \n                         num_classes=2,\n                         num_features=num_features,\n                         use_pretrained_embedding=True,\n                         is_embedding_trainable=True,\n                         embedding_matrix=embedding_matrix)\n\n    # Compile model with learning parameters.\n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n    # Load the weights that we had saved into this new model.\n    model.load_weights('./fine_tuned_lstm_model_with_pre_trained_embedding.h5')\n    \n    # Train and validate model.\n    history = model.fit([x_train, x_train_key],\n                        train_labels,\n                        epochs=epochs,\n                        callbacks=callbacks,\n                        validation_data=([x_val, x_val_key], val_labels),\n                        verbose=2,  # Logs once per epoch.\n                        batch_size=batch_size)\n\n    # Print results.\n    history = history.history\n    print('Validation accuracy: {acc}, loss: {loss}'.format(acc=history['val_accuracy'][-1], loss=history['val_loss'][-1]))\n\n    # Save model.\n    model.save('./disaster_tweets_lstm_fine_tuned_model')\n    return history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (X_train, X_train_keyw, y_train), (X_val, X_val_keyw, y_val), word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"history = train_fine_tuned_functional_api_model(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_graphs(history, metric):\n  plt.plot(history[metric])\n  plt.plot(history['val_'+metric], '')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplot_graphs(history, 'accuracy')\nplt.ylim(None,1)\nplt.subplot(1,2,2)\nplot_graphs(history, 'loss')\nplt.ylim(0,None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the model\nmodel = keras.models.load_model('./disaster_tweets_lstm_fine_tuned_model')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict on provided test set for Kaggle leaderboard"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_keyw = df_test['keyword']\nX_test = df_test['text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('./tokenizer_max_length.pkl', 'rb') as test_transformation:\n  test_transformation_dict = pickle.load(test_transformation)\n  tokenizer = test_transformation_dict['tokenizer']\n  max_length = test_transformation_dict['max_length']\n\nwith open('./vectorizer_selector.pkl', 'rb') as test_transformation:\n  test_transformation_dict = pickle.load(test_transformation)\n  _vectorizer_ = test_transformation_dict['vectorizer']\n  _selector_ = test_transformation_dict['selector']\n\nX_test = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(X_test, maxlen=max_length)\n\nX_test_keyw = _vectorizer_.transform(X_test_keyw)\nX_test_keyw = _selector_.transform(X_test_keyw)\n\nX_test_keyw = X_test_keyw.todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = (model.predict([X_test,X_test_keyw]) > 0.5).astype(\"int32\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#output = pd.DataFrame.from_dict({'id':df_test['id'].values.tolist(), 'target':y_test.ravel()})\n#output.to_csv('./LSTM_emb_tuned_v4.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}