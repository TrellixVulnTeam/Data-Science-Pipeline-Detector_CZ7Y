{"cells":[{"metadata":{},"cell_type":"markdown","source":" <font size=\"+3\" color=\"darkmagenta\"><b> Disaster Tweets: very simple LSTM model with and without GloVe </b></font><br> <a id=\"3\"></a>\n","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <font size=\"+3\" color=\"black\"><b> Visualising the training dataset </b></font><br> <a id=\"3\"></a>\n","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df=pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = 'Disaster Tweets', 'Non-Disaster Tweets'\nsizes = np.array(train_df.target.value_counts())/len(train_df)*100\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"=> Our training dataset is pretty balanced","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" <font size=\"+3\" color=\"black\"><b> Checking the importance of 'keyword' and 'location' </b></font><br> <a id=\"3\"></a>\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_correlation = train_df.dropna()\ndf_correlation=df_correlation.drop(columns=['id','text'])\ndf_correlation['keyword']=df_correlation['keyword']=df_correlation['keyword'].astype('category').cat.codes\ndf_correlation['location']=df_correlation['location']=df_correlation['location'].astype('category').cat.codes\ncorr=df_correlation.corr()\nsns.heatmap(corr, vmax=0.8)\ncorr_values=corr['target'].sort_values(ascending=False)\ncorr_values=abs(corr_values).sort_values(ascending=False)\nprint(\"Correlation of keyword and location with target in ascending order\")\nprint(abs(corr_values).sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"=> the correlation of keyword and location with target is extremely low, so we can safely remove them without losing information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=train_df.drop(columns=['location','keyword','id'])\ntest_df=test_df.drop(columns=['location','keyword'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <font size=\"+3\" color=\"black\"><b> Removing URLs and emojis from texts </b></font><br> <a id=\"3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\ndef remove_URL(df):\n    for i in range(df.shape[0]):\n        df.text[i]=re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',df.text[i])\n        \nremove_URL(train_df)\nremove_URL(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntrain_df['text']=train_df['text'].apply(lambda x: remove_emoji(x))\ntest_df['text']=test_df['text'].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Source of the previous cell:  https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"black\"><b> 1. Model 1: Embedding without GloVe </b></font><br> <a id=\"3\"></a>\n\n<font size=\"+2\" color=\"black\"><b> 1.1 Tokenization </b></font><br> <a id=\"3\"></a>\nWe will create a dictionnary of size 4500 using our training dataset. Each vector will have a length of 40 (using zero-padding).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE=4500\nMAXLEN=40\ntokenizer=Tokenizer(VOCAB_SIZE,oov_token='<oov>', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')  # filtering special characters\n\ntokenizer.fit_on_texts(train_df.text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_to_padded_sequences(df,tokenizer):\n    sequences=tokenizer.texts_to_sequences(df.text)                                              #text to sequence of integers\n    padded_sequences=pad_sequences(sequences,maxlen=MAXLEN, padding='post', truncating='post')  #padding\n    return padded_sequences\n\nX_train=df_to_padded_sequences(train_df,tokenizer)\nX_test=df_to_padded_sequences(test_df,tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Original tweet: ',train_df.text[0])\nprint('Tokenized tweet: ',X_train[0])\nprint('Token to tweet: ',tokenizer.sequences_to_texts(X_train)[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The more occurent a word the smaller the encoded integer, 2 ('the') being the most occurent word. A word is encoded as 1 if the word is absent of the dictionnary.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" <font size=\"+2\" color=\"black\"><b> 1.2 Splitting the dataset into training and validation </b></font><br> <a id=\"3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train=train_df.target\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\n\nprint('Training features shape: ',X_train.shape)\nprint('Validation features shape: ',X_val.shape)\n\nprint('Training labels shape: ', Y_train.shape)\nprint('Validation labels shape: ', Y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <font size=\"+2\" color=\"black\"><b> 1.3 Building and training the model </b></font><br> <a id=\"3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = keras.Sequential([\n    keras.layers.Embedding(VOCAB_SIZE, 32,input_length=MAXLEN),\n    keras.layers.SpatialDropout1D(0.2),\n    keras.layers.Bidirectional(keras.layers.LSTM(16)),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS=30\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_acc', \n    verbose=1,\n    patience=5,\n    mode='max',\n    restore_best_weights=True)\n\n\n\nmodel1.compile(loss=\"binary_crossentropy\",optimizer=keras.optimizers.RMSprop(1e-4), metrics=['acc'])\n\nhistory1 = model1.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=32, epochs=EPOCHS, callbacks = [early_stopping])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"black\"><b> 2. Model 2: Embedding with GloVe </b></font><br> <a id=\"3\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"black\"><b> 2.1 Tokenization </b></font><br> <a id=\"3\"></a>\nWe'll use this time all the words for the vocabulary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer=Tokenizer(oov_token='<oov>', filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')  # filtering special characters\n\ntokenizer.fit_on_texts(train_df.text)\n\nvocab_length = len(tokenizer.word_index) + 1\nprint(vocab_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAXLEN=40\n\ndef df_to_padded_sequences(df,tokenizer):\n    sequences=tokenizer.texts_to_sequences(df.text)                                              #text to sequence of integers\n    padded_sequences=pad_sequences(sequences,maxlen=MAXLEN, padding='post', truncating='post')  #padding\n    return padded_sequences\n\nX_train2=df_to_padded_sequences(train_df,tokenizer)\nX_test2=df_to_padded_sequences(test_df,tokenizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <font size=\"+2\" color=\"black\"><b> 2.2 Splitting the dataset into training and validation </b></font><br> <a id=\"3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train2=train_df.target\nX_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train2, Y_train2, test_size=0.2)\n\nprint('Training features shape: ',X_train2.shape)\nprint('Validation features shape: ',X_val2.shape)\n\nprint('Training labels shape: ', Y_train2.shape)\nprint('Validation labels shape: ', Y_val2.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <font size=\"+2\" color=\"black\"><b> 2.3 Building and training the model </b></font><br> <a id=\"3\"></a>\n We build an embedding_matrix with the Glove representation and will this matrix as our embedding layer (therefore we won't train it). We'll use 50D embedding representation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_dictionary = dict()\nembedding_dim = 50\nglove_file = open('../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt')\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\nglove_file.close()\n\n\n\nembedding_matrix = np.zeros((vocab_length, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Source of the previous cell: https://www.kaggle.com/mariapushkareva/nlp-disaster-tweets-with-glove-and-lstm#EDA-and-Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = keras.Sequential([\n    keras.layers.Embedding(vocab_length, 50, embeddings_initializer=Constant(embedding_matrix), input_length=MAXLEN, trainable=False),\n    keras.layers.SpatialDropout1D(0.2),\n    keras.layers.Bidirectional(keras.layers.LSTM(16)),\n    keras.layers.Dense(1, activation=\"sigmoid\")\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS=30\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_acc', \n    verbose=1,\n    patience=5,\n    mode='max',\n    restore_best_weights=True)\n\n\n\nmodel2.compile(loss=\"binary_crossentropy\",optimizer=keras.optimizers.RMSprop(1e-4), metrics=['acc'])\n\nhistory2 = model2.fit(X_train2, Y_train2, validation_data=(X_val2, Y_val2), batch_size=32, epochs=EPOCHS, callbacks = [early_stopping])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"black\"><b> Visualizing training results </b></font><br> <a id=\"3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"loss1, acc1 = model1.evaluate(X_val,Y_val)\nloss2, acc2 = model2.evaluate(X_val2,Y_val2)\nprint(\"Val_acc model1: \",acc1)\nprint(\"Val_acc model2: \",acc2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font  color=\"red\"><b> Results are very similar but they are a bit better on the first model so we will use this one for the rest of the notebook </b></font><br> <a id=\"3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history1.history['acc']\nval_acc = history1.history['val_acc']\n\nloss = history1.history['loss']\nval_loss = history1.history['val_loss']\n\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(history1.epoch, acc, label='Training Accuracy')\nplt.plot(history1.epoch, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy ')\n\nplt.subplot(1, 2, 2)\nplt.plot(history1.epoch, loss, label='Training Loss')\nplt.plot(history1.epoch, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_cm(labels, predictions, p=0.5): \n  cm = confusion_matrix(labels, predictions > p)\n  cm_sum = np.sum(cm, axis=1, keepdims=True)\n  cm_perc = cm / cm_sum.astype(float) * 100\n  annot = np.empty_like(cm).astype(str)\n  nrows, ncols = cm.shape\n  for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n  plt.figure(figsize=(5,5))\n  sns.heatmap(cm, annot=annot, fmt=\"\",cmap=\"YlGnBu\")\n  plt.title('Confusion matrix')\n  plt.ylabel('Actual label')\n  plt.xlabel('Predicted label')\n\n  print('Non-Disaster Tweet Detected (True Negatives): ', cm[0][0])\n  print('Disaster Tweet Incorrectly Detected (False Positives): ', cm[0][1])\n  print('Disaster Tweet Missed (False Negatives): ', cm[1][0])\n  print('Disaster Tweet Detected (True Positives): ', cm[1][1])\n  print('Total Disaster Tweet: ', np.sum(cm[1]))\n  print('Total Non-Disaster Tweet: ', np.sum(cm[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_prediction=model1.predict(X_val)\nplot_cm(Y_val, val_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <font size=\"+3\" color=\"black\"><b> Predicting on the test set with the first model and submitting the predictions </b></font><br> <a id=\"3\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test=model1.predict(X_test)\nprint(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test = [int(i>0.5) for i in Y_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_dataframe = pd.DataFrame({\"id\" : test_df['id'], \"target\" : Y_test})\nsubmission_dataframe.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_dataframe","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}