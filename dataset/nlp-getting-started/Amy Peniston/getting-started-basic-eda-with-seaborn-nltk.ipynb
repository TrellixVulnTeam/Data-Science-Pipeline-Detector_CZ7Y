{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Disaster Tweets: Exploratory Data Analysis\n\nGreetings! Thanks for checking out my code. \n\nIn this notebook, we will conduct basic EDA to get a better understanding of our dataset and how features are distributed. We will also utilize the NLTK package to parse common words and stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict, Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords', quiet=True)\nstopwords = stopwords.words('english')\n\nsns.set(style=\"white\", font_scale=1.2)\nplt.rcParams[\"figure.figsize\"] = [10,8]\npd.set_option.display_max_columns = 0\npd.set_option.display_max_rows = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore the Data"},{"metadata":{},"cell_type":"markdown","source":"**Data Dictionary**\n\n* id - unique identifier\n* keyword - a specific keyword from the tweet\n* location - origin of tweet\n* text - content of tweet\n* target - binary indicator denoting whether the tweet is about a disaster, our value to predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 7613 observations in the training data. The test dataset is less than half this size, totalling 3263 rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape, test.shape[0]/train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a significant number of missing values in the keyword and location columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_counts = pd.DataFrame({\"Num_Null\": train.isnull().sum()})\nnull_counts[\"Pct_Null\"] = null_counts[\"Num_Null\"] / train.count() * 100\nnull_counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Keyword\n\nWhat are some of the most commonly used keywords?"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords_vc = pd.DataFrame({\"Count\": train[\"keyword\"].value_counts()})\nsns.barplot(y=keywords_vc[0:30].index, x=keywords_vc[0:30][\"Count\"], orient='h')\nplt.title(\"Top 30 Keywords\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, there are 221 different keywords associated with tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train[\"keyword\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see when we pull out the top 30 keywords for disaster vs non-disaster tweets, word appearance alone is not sufficient to classify content. For example, \"body%20bags\" and \"armageddon\" are the top 1st and 2nd keyword for non-disaster tweets!"},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_keywords = train.loc[train[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster_keywords = train.loc[train[\"target\"] == 0][\"keyword\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_keywords[0:30].index, x=disaster_keywords[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_keywords[0:30].index, x=nondisaster_keywords[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Keywords - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Keywords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can pull out non-disaster tweets containing words that we would expect to be associated with disasters. Clearly, an accurate model will need to take context into account."},{"metadata":{"trusted":true},"cell_type":"code","source":"armageddon_tweets = train[(train[\"keyword\"].fillna(\"\").str.contains(\"armageddon\")) & (train[\"target\"] == 0)]\nprint(\"An example tweet:\\n\", armageddon_tweets.iloc[10, 3])\narmageddon_tweets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also calculate the probability of disaster given the appearance of a keyword."},{"metadata":{"trusted":true},"cell_type":"code","source":"def keyword_disaster_probabilities(x):\n    tweets_w_keyword = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x))\n    tweets_w_keyword_disaster = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x) & train[\"target\"] == 1)\n    return tweets_w_keyword_disaster / tweets_w_keyword\n\nkeywords_vc[\"Disaster_Probability\"] = keywords_vc.index.map(keyword_disaster_probabilities)\nkeywords_vc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's sort our dataframe to see which keywords are most often correlated with disaster."},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords_vc.sort_values(by=\"Disaster_Probability\", ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Likewise, we can reverse the sort order to see the least \"disastrous\" words. Some of these are surprising!"},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords_vc.sort_values(by=\"Disaster_Probability\").head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Location\n\nWhere do most tweet originate? As we can see in the histogram below, USA/United States is the most common tweet origin. Locations range from specific cities, such as Chicago, IL, to more general (and perhaps less helpful) regions, such as Earth and Everywhere."},{"metadata":{"trusted":true},"cell_type":"code","source":"locations_vc = train[\"location\"].value_counts()\nsns.barplot(y=locations_vc[0:30].index, x=locations_vc[0:30], orient='h')\nplt.title(\"Top 30 Locations\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Overall, there are 3341 unique locations."},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train[\"location\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, observe that the location histogram does vary based on disaster vs. non-disaster content. For example, Nigeria is the top 3rd location for disaster tweets yet does not appear in the top 30 locations for non-disaster tweets. Similar observations can be made for India and Mumbai."},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_locations = train.loc[train[\"target\"] == 1][\"location\"].value_counts()\nnondisaster_locations = train.loc[train[\"target\"] == 0][\"location\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_locations[0:30].index, x=disaster_locations[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_locations[0:30].index, x=nondisaster_locations[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Locations - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Locations - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text aka Tweet Content\n\n**Tweet Length**\n\nWhat is the distribution of tweet length?"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"tweet_length\"] = train[\"text\"].apply(len)\nsns.distplot(train[\"tweet_length\"])\nplt.title(\"Histogram of Tweet Length\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Density\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The shortest tweet is 7 characters long; the longest tweet is 157 characters long."},{"metadata":{"trusted":true},"cell_type":"code","source":"min(train[\"tweet_length\"]), max(train[\"tweet_length\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we look at the distribution of tweet length for disaster and non-disaster tweets separately, we see that disaster tweets tend to be lengthier."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"tweet_length\")\nplt.suptitle(\"Distribution Tweet Length\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Number of Words**\n\nSimilarly, we can count the number of words in each tweet and examine the distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_words(x):\n    return len(x.split())\n\ntrain[\"num_words\"] = train[\"text\"].apply(count_words)\nsns.distplot(train[\"num_words\"], bins=10)\nplt.title(\"Histogram of Number of Words per Tweet\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Density\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However, no strong pattern appears when we create separate histograms for disaster vs. non-disaster tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"num_words\")\nplt.suptitle(\"Distribution Number of Words\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Average Word Length**\n\nWhat about average word length?"},{"metadata":{"trusted":true},"cell_type":"code","source":"def avg_word_length(x):\n    return np.sum([len(w) for w in x.split()]) / len(x.split())\n\ntrain[\"avg_word_length\"] = train[\"text\"].apply(avg_word_length)\nsns.distplot(train[\"avg_word_length\"])\nplt.title(\"Histogram of Average Word Length\")\nplt.xlabel(\"Average Word Length\")\nplt.ylabel(\"Density\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, we see no striking difference between histograms of average word length for disaster vs. non-disaster tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"avg_word_length\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Common Stopwords**\n\nNow, let's see the most common stopwords, i.e. the words that should be ignored. This is not a particularly illuminating exercise, however it is a good refresher for using the NLTK package."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n\n    for w in train.loc[train[\"target\"] == target][\"text\"].str.split():\n        for i in w:\n            corpus.append(i)\n            \n    return corpus\n\ndef create_corpus_dict(target):\n    corpus = create_corpus(target)\n            \n    stop_dict = defaultdict(int)\n    for word in corpus:\n        if word in stopwords:\n            stop_dict[word] += 1\n    return sorted(stop_dict.items(), key=lambda x:x[1], reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_disaster_dict = create_corpus_dict(0)\ncorpus_non_disaster_dict = create_corpus_dict(1)\n\ndisaster_x, disaster_y = zip(*corpus_disaster_dict)\nnon_disaster_x, non_disaster_y = zip(*corpus_non_disaster_dict)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=list(disaster_x)[0:30], x=list(disaster_y)[0:30], orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(y=list(non_disaster_x)[0:30], x=list(non_disaster_y)[0:30], orient='h', palette=\"Blues_d\", ax=ax[1]) \nax[0].set_title(\"Top 30 Stop Words - Disaster Tweets\")\nax[0].set_xlabel(\"Stop Word Frequency\")\nax[1].set_title(\"Top 30 Stop Words - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Stop Word Frequency\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Common Words**\n\nWith that out of the way, let's examine the most common non-stopwords."},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus_disaster, corpus_non_disaster = create_corpus(1), create_corpus(0)\ncounter_disaster, counter_non_disaster = Counter(corpus_disaster), Counter(corpus_non_disaster)\nx_disaster, y_disaster, x_non_disaster, y_non_disaster = [], [], [], []\n\ncounter = 0\nfor word, count in counter_disaster.most_common()[0:100]:\n    if (word not in stopwords and counter < 15):\n        counter += 1\n        x_disaster.append(word)\n        y_disaster.append(count)\n\ncounter = 0\nfor word, count in counter_non_disaster.most_common()[0:100]:\n    if (word not in stopwords and counter < 15):\n        counter += 1\n        x_non_disaster.append(word)\n        y_non_disaster.append(count)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\nax[0].set_title(\"Top 15 Non-Stopwords - Disaster Tweets\")\nax[0].set_xlabel(\"Word Frequency\")\nax[1].set_title(\"Top 15 Non-Stopwords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Word Frequency\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Common Bigrams**\n\nFinally, let's look at the most common bigrams (word pairs)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bigrams(target):\n    corpus = train[train[\"target\"] == target][\"text\"]\n    count_vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = count_vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigrams_disaster = bigrams(1)[:15]\nbigrams_non_disaster = bigrams(0)[:15]\n\nx_disaster, y_disaster = map(list, zip(*bigrams_disaster))\nx_non_disaster, y_non_disaster = map(list, zip(*bigrams_non_disaster))\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\n\nax[0].set_title(\"Top 15 Bigrams - Disaster Tweets\")\nax[0].set_xlabel(\"Word Frequency\")\nax[1].set_title(\"Top 15 Bigrams - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Word Frequency\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target\n\nDo we have approximately equal numbers of true/false targets? More or less. Based on the histogram below, we see that 43% of tweets correspond to real disasters."},{"metadata":{"trusted":true},"cell_type":"code","source":"target_vc = train[\"target\"].value_counts(normalize=True)\nprint(\"Not Disaster: {:.2%}, Disaster: {:.2%}\".format(target_vc[0], target_vc[1]))\nsns.barplot(x=target_vc.index, y=target_vc)\nplt.title(\"Histogram of Disaster vs. Non-Disaster\")\nplt.xlabel(\"0 = Non-Disaster, 1 = Disaster\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\n\nThanks very much for reading; I hope you enjoyed this exploratory data analysis. If you did, be sure to upvote so you can find this notebook again easily in your Favorites tab.\n\nSuggestions for additional EDA? Please leave a comment below.\n\nUntil next time, happy coding :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}