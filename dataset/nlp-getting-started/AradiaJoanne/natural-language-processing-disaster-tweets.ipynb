{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing: Disaster Tweets - EDA WIP","metadata":{}},{"cell_type":"markdown","source":"# Introduction <br>\nThe aim of this notebook is to be able to classify tweets into two categories, tweets that are about actual disasters and tweets that are not about disasters. This will be done using NLP and machine learning, on a dataset of 10,000 tweets. \n\n**DISCLAIMER: SOME TWEETS MAY CONTAIN EXPLICIT LANGUAGE**\n\n**What is NLP?**<br>\n\nNLP stands for Natural Language Processing, this is defined by the Oxford Languages Dictionary as 'The application of computational techniques to the analysis and synthesis of natural language and speech'. Natural language is the way humans talk to each other through things such as text and speech.<br>\nLanguage can be incredibly difficult, it is a thing that takes each one of us years to learn and even then we often make mistakes throughout our lives. NLP as a tool is very powerful but it faces many limitations and problems; such as the ability to understand context, sarcasm, errors in the text, slang, and ambiguity. This can have an impact on how our models may decide the outcome of the tweet so we can already predict that it will make some mistakes when trying to predict the outcome. ","metadata":{}},{"cell_type":"markdown","source":"## Let's get started! \n\nThe first steps like in every project are to import our libraries and then read in our data. ","metadata":{}},{"cell_type":"code","source":"# Install Libs\n!pip install pyspellchecker\n!pip install keras\n!pip install tensorflow-cpu==2.1.0 #No GPU :'(\n!pip install pyspellchecker","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:54:18.029455Z","iopub.execute_input":"2022-06-08T12:54:18.030101Z","iopub.status.idle":"2022-06-08T12:55:18.7531Z","shell.execute_reply.started":"2022-06-08T12:54:18.029997Z","shell.execute_reply":"2022-06-08T12:55:18.752056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Libraries \nimport io \nimport re\nimport string \nimport unicodedata\nimport spacy \nimport matplotlib\nimport torch\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\n\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tqdm import tqdm\nfrom spellchecker import SpellChecker\nfrom wordcloud import WordCloud\nfrom IPython.display import display","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:18.754893Z","iopub.execute_input":"2022-06-08T12:55:18.755203Z","iopub.status.idle":"2022-06-08T12:55:25.065672Z","shell.execute_reply.started":"2022-06-08T12:55:18.755167Z","shell.execute_reply":"2022-06-08T12:55:25.064775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we need to read in our dataset and store it in a dataframe so we can easily access and manipulate it. ","metadata":{}},{"cell_type":"code","source":"# Read in Data\ntweets = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:25.06683Z","iopub.execute_input":"2022-06-08T12:55:25.067153Z","iopub.status.idle":"2022-06-08T12:55:25.137816Z","shell.execute_reply.started":"2022-06-08T12:55:25.067121Z","shell.execute_reply":"2022-06-08T12:55:25.136695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take a look at what the data looks like \ntweets.head(10)\ntest.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:25.139305Z","iopub.execute_input":"2022-06-08T12:55:25.139526Z","iopub.status.idle":"2022-06-08T12:55:25.163343Z","shell.execute_reply.started":"2022-06-08T12:55:25.139502Z","shell.execute_reply":"2022-06-08T12:55:25.162479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Rows: {tweets.shape[0]}, Columns: {tweets.shape[1]} in the training data.')\nprint(f'Rows: {test.shape[0]}, Columns: {test.shape[1]} in the test data.')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:25.164515Z","iopub.execute_input":"2022-06-08T12:55:25.16483Z","iopub.status.idle":"2022-06-08T12:55:25.171346Z","shell.execute_reply.started":"2022-06-08T12:55:25.1648Z","shell.execute_reply":"2022-06-08T12:55:25.170177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we can see that the test data is missing the target column, but our training data includes this. So the end result is to predict the target column for the test data. ","metadata":{}},{"cell_type":"code","source":"# Lets see what the target looks like \ntweets.describe()\ntweets.target.value_counts().plot(kind = 'bar')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:25.172752Z","iopub.execute_input":"2022-06-08T12:55:25.173307Z","iopub.status.idle":"2022-06-08T12:55:25.417754Z","shell.execute_reply.started":"2022-06-08T12:55:25.173274Z","shell.execute_reply":"2022-06-08T12:55:25.416683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are more tweets with no mention of disaster than tweets with a disaster. This is known as a class imbalance, machine learning models work best when the outcome classes are about equal. It isn't a huge difference so maybe we can play about with reducing this imbalance later if we don't get the results we are hoping for. We will consider a range of measures when trying to determine the performance of the models as accuracy can be flawed when there is a class imbalance. ","metadata":{}},{"cell_type":"code","source":"tweets['text'][0]","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:25.419279Z","iopub.execute_input":"2022-06-08T12:55:25.419617Z","iopub.status.idle":"2022-06-08T12:55:25.431335Z","shell.execute_reply.started":"2022-06-08T12:55:25.419575Z","shell.execute_reply":"2022-06-08T12:55:25.43055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning \n\nThe next step should be to clean the data and make it as usable as possible. I will first make a copy of the data just to be on the safe side.","metadata":{}},{"cell_type":"code","source":"# Combine both test and train data into a new df for cleaning \ndf = pd.concat([tweets, test])\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:25.433445Z","iopub.execute_input":"2022-06-08T12:55:25.433892Z","iopub.status.idle":"2022-06-08T12:55:25.450628Z","shell.execute_reply.started":"2022-06-08T12:55:25.433829Z","shell.execute_reply":"2022-06-08T12:55:25.449978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hashtags are a common feature of many tweets and are often used inline, so the symbols should be stripped along with any other special characters. ","metadata":{}},{"cell_type":"code","source":"test = \"[This is just to test] somerandom@text.totest, https://maybe.the/tweet/contains.alink<br>ðŸ˜”, I am a #bad #person corevt the word?\"","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:25.451948Z","iopub.execute_input":"2022-06-08T12:55:25.452399Z","iopub.status.idle":"2022-06-08T12:55:25.456575Z","shell.execute_reply.started":"2022-06-08T12:55:25.452366Z","shell.execute_reply":"2022-06-08T12:55:25.455749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Regex\nI really don't like how intimidating regex looks and sometimes it can be a little bit of a pain to use, but its perfecct for what we want to do in our data cleaning steps. I took the long steps to writing this by running them in this notebook on a test string to see the outcome, since writing this set of functions I have found this really handy website that helps you figure out what is going on, its also the first result if you google regex debugger. https://regex101.com/ \n\n# Data Cleaning Functions in more Detail\nIt should be pretty self explanatory what is happening in the below functions but if this is your first look at NLP let's touch on a couple of things. \n\nBeautiful Soup - it is a really helpful libarary that let's you grab data from HTML and XML files, you can do things like prettify your documents, grab specific sections, in this case I am using it to grab all of the text regardless of if it is a title or body or any other formatting and leave behind all of the tags so that we just take the usable text. Now I don't use twitter so I don't know how likely this is to occur in our dataset so better to clean it out just in case! https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n\nSpellchecker - I'm not going to manually spellcheck 10,000 tweets, no one has time for that. This is the pyspellchecker library, it will save you so much time! It uses the Levenshtein distance algorithm to determine if a word has a spelling error. Simply put the algorithm looks at how many single characters in a string need to be changed to make it into another word. If it is few and one word exists in the dictionary and the other doesn't, it will change the word to the one in the dictionary, if it is many characters difference then it will leave it alone.  This step isn't necassary, this is one of the really time consuming steps too, while it is running YOU CANNOT SAVE YOUR NOTEBOOK so yeet it into an ipynb or py file too!\n\nNo one is making you run the spellchecking, but you really should! It will eliminate duplicate words created by spelling errors, and it will allow you to get improved accuracy in your results too. \n\nUnless you are doing a degree in english, if you are new to NLP projects, you may of not heard of lemmatization and stop words. <br>\n\nLemmatization or Lemmatisation if you are from the UK like me, is a process of grouping words together that share the same 'lemma' which is their dictionary form. This is so they can be analysed as a single item. Now if you are checking what your data is looking like at various stages in this process, it may appear to make less sense from a gramatical perspective but the underlying meaning of the text remains.  This isn't an entirely necassary step and there are other approaches you can take to get similar results such as stemming, the difference is that stemming uses the 'stem' of the word which is more objective, it removes the prefixes or suffixes, where as lemmatization looks at the context of the word, so it needs a really detailed dictionary which is what the 'sp = spacy.load('en_core_web_sm');' section of that function is bringing in. \n\nLemmatization has been chosen in this case to help increase the recall performance metric. \n\nThe NLTK library is bringing us a lovely list of stop words (But you can also define them yourself if you want to be specific!), we have so many words in this data set and not all of them are needed to determine if the tweet is mentioning disaster, these are words like 'And', 'a', 'the' and such, the words that we use to add more sense and flow to what we say. For example, you could say 'I am pretty sure there was just an earthquake near Kailua-Kona, HI', removing the stopwords would give us 'pretty sure, just earthquake near Kailua-Kona, HI' which doesn't sound great but it still implies that there was an earthquake. https://www.nltk.org/book/ch02.html\n\nThis removes the aspect of pointless words that only serve to make the text flow better to us having an impact on determining that a natural disaster was mentioned in the tweet. It is unlikely that more tweets that mention disaster aslo use the word 'The' for example, but if that was the case there is a chance our model later on in the notebook may use that as a determining factor which would give us pretty useless results. \n\n","metadata":{}},{"cell_type":"code","source":"# Remove any links \ndef remove_hyperlink(data):\n    isurl = re.compile(r'https?://\\S+|www.\\.\\S+')\n    return isurl.sub(r'', data)\n\n# Remove any special characters\ndef remove_special_chars(data):\n    data = re.sub('[,\\.!:;()\"]', ' ', data)\n    data = re.sub('[^a-zA-Z\"]', ' ', data)\n    data = re.sub('\\[[^]]*\\]', ' ', data)\n    return data\n\n# Make Lower Case\ndef make_lower(data):\n    data = data.str.lower()\n    return data\n\n# Remove any HTML\ndef remove_html(data):\n    soup = BeautifulSoup(data, 'html.parser')\n    return soup.get_text()\n\n# Remove any Emojis\ndef remove_emojis(data):\n    emojis = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return emojis.sub(r'',data)\n\n# Remove double spaces\ndef remove_doublespaces(data):\n    data = re.sub('  ', ' ', data)\n    data = re.sub('  ', ' ', data)\n    return data\n\n# Correct Mispellings \nspell = SpellChecker()\ndef correct_spelling(data):\n    corrected = []\n    mispellings = spell.unknown(data.split())\n    for word in data.split():\n        if word in mispellings:\n            corrected.append(spell.correction(word))\n        else:\n            corrected.append(word)\n    return \" \".join(corrected)\n\n\nstopw = set(stopwords.words('english'))\n# Tokenize and remove stopwords \ndef filter_on_stopwords(data):\n    word_tokens = word_tokenize(data)\n    no_stopword_tokens = [token for token in word_tokens if token not in stopw]\n    data = ' '.join(no_stopword_tokens)\n    return data\n\nsp = spacy.load('en_core_web_sm')\ndef lemmatize_words(data):\n    doc = sp(data)   \n    data = ' '.join([token.lemma_ for token in doc])\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:25.45905Z","iopub.execute_input":"2022-06-08T12:55:25.459674Z","iopub.status.idle":"2022-06-08T12:55:26.397351Z","shell.execute_reply.started":"2022-06-08T12:55:25.459628Z","shell.execute_reply":"2022-06-08T12:55:26.396406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call these functions \ntest = remove_hyperlink(test)\ntest = remove_special_chars(test)\n#test = make_lower(test) #str has no attrib str, but the actual data is in a series, so the funct works for the actual data\ntest = remove_html(test)\ntest = remove_emojis(test)\ntest = remove_doublespaces(test)\ntest = correct_spelling(test)\n# Outputs the re.subbed and cleansed test string \ntest","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:26.398451Z","iopub.execute_input":"2022-06-08T12:55:26.398678Z","iopub.status.idle":"2022-06-08T12:55:26.864216Z","shell.execute_reply.started":"2022-06-08T12:55:26.398652Z","shell.execute_reply":"2022-06-08T12:55:26.863272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply the data cleaning functions to the text","metadata":{}},{"cell_type":"code","source":"# Now we know that the data cleanining functions work as expected we can apply them to our combibned data\n# This can take a really really long time, like up to hours, so be patient! Spellchecking and lemmatizing this many individual words and things that are hardly words is slow :'(\ndf['text'] = df['text'].apply(remove_hyperlink)\ndf['text'] = df['text'].apply(remove_special_chars)\ndf['text'] = make_lower(df['text'])\ndf['text'] = df['text'].apply(remove_html)\ndf['text'] = df['text'].apply(remove_emojis)\ndf['text'] = df['text'].apply(remove_doublespaces)\ndf['text'] = df['text'].apply(correct_spelling)\ndf['text'] = df['text'].apply(filter_on_stopwords)\ndf['text'] = df['text'].apply(lemmatize_words)\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T12:55:26.865693Z","iopub.execute_input":"2022-06-08T12:55:26.866266Z","iopub.status.idle":"2022-06-08T13:36:13.180163Z","shell.execute_reply.started":"2022-06-08T12:55:26.86623Z","shell.execute_reply":"2022-06-08T13:36:13.179023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration or to be fancy this is our EDA <br>\n\nNow that the data has been cleaned and the stopwords have been removed, we can do a deeper exploration of the data and see what it looks like and a little bit of an analysis on it, just to see if we have any really obvious patterns that stand out, then we can expect to see our results mirroring this somewhat. ","metadata":{}},{"cell_type":"code","source":"# Number of words in tweets about disaster\nlen_of_tweet = df[df['target'] == 1]['text'].str.split().map(lambda x: len(x))\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.hist(len_of_tweet, color = 'green')\nax1.set_title('No of Words in Disaster Tweets')\nlen_of_tweet = df[df['target'] == 0]['text'].str.split().map(lambda x: len(x))\nax2.hist(len_of_tweet, color = 'red')\nax2.set_title('No of Words in Not Disaster Tweets')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:36:13.182687Z","iopub.execute_input":"2022-06-08T13:36:13.183604Z","iopub.status.idle":"2022-06-08T13:36:13.873481Z","shell.execute_reply.started":"2022-06-08T13:36:13.183556Z","shell.execute_reply":"2022-06-08T13:36:13.872484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like tweets that mention disaster seem to have slightly less words, with number of tweets on the y and number of words on the x","metadata":{}},{"cell_type":"code","source":"# Get the frequency distribution of the most popular words\nwords = [word for word in pd.Series(' '.join(df['text']).split())]\nmost_used_words = FreqDist(words).most_common(10)\nmost_used_words = pd.Series(dict(most_used_words))\n#Plot the most commonly used words\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\nsns.barplot(x = most_used_words.index, y = most_used_words.values, ax=ax1)\n\n#Get the frequency distribution where tweets are about disaster\ndisaster_tweets = df[df['target']==1]['text']\ndisaster_words = [word.split()[0] for word in disaster_tweets]\nmost_used_disaster_words = FreqDist(disaster_words).most_common(10)\nmost_used_disaster_words = pd.Series(dict(most_used_disaster_words))\nsns.barplot(x = most_used_disaster_words.index, y = most_used_disaster_words.values, ax=ax2)\nax1.set_title('Most Common Words in All Tweets')\nax2.set_title('Most Common Words in Tweets about Disaster')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:36:13.87472Z","iopub.execute_input":"2022-06-08T13:36:13.874975Z","iopub.status.idle":"2022-06-08T13:36:14.472571Z","shell.execute_reply.started":"2022-06-08T13:36:13.874946Z","shell.execute_reply":"2022-06-08T13:36:14.471547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Note: -PRON- is replacing any pronoun, I am not sure which library is causing this, likely the spellcheck or lemmatization, people's specific pronouns shouldn't make an impact on this, so I won't worry about this for now.","metadata":{}},{"cell_type":"code","source":"# Number of unique words\nunique_words = []\nfor word in words:\n    if not word in unique_words:\n        unique_words.append(word)\n\nprint(f'Number of Uniques Words: {len(unique_words)}')","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:36:14.473761Z","iopub.execute_input":"2022-06-08T13:36:14.474041Z","iopub.status.idle":"2022-06-08T13:36:19.644907Z","shell.execute_reply.started":"2022-06-08T13:36:14.474012Z","shell.execute_reply":"2022-06-08T13:36:19.644257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fun Fact\nIt took 3143.782 seconds to clean the data set, including spellchecking and lemmatizing 15,833 unique words, thats 53-ish mins!","metadata":{}},{"cell_type":"markdown","source":"I have never tried a bag of words before or LSA, so let's try it out. \nCode from: https://www.kaggle.com/vbmokin/nlp-eda-bag-of-words-tf-idf-glove-bert","metadata":{}},{"cell_type":"code","source":"# Bag of Words\ndef _count_vectorizer(data):\n    count_vectorizer = CountVectorizer()\n    emb = count_vectorizer.fit_transform(data)\n    return emb, count_vectorizer\n\nlist_corpus = df['text'].tolist()\nlist_labels = df['target'].tolist()\n\nx_train, x_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size = 0.3, random_state = 0)\n\nx_train_counts, count_vectorizer = _count_vectorizer(x_train)\nx_test_counts = count_vectorizer.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:36:19.645827Z","iopub.execute_input":"2022-06-08T13:36:19.646315Z","iopub.status.idle":"2022-06-08T13:36:19.810409Z","shell.execute_reply.started":"2022-06-08T13:36:19.646272Z","shell.execute_reply":"2022-06-08T13:36:19.809357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Latent Semantic Analysis\n\nThis is where relationships in the context of the documents are analysed. It assumes that words that have similar meanings will appear close together in a block of text. \n","metadata":{}},{"cell_type":"code","source":"def plot_LSA(data, labels, savepath ='PCA.csv', plot = True):\n    lsa = TruncatedSVD(n_components=2)\n    lsa.fit(data)\n\n    lsa_scores = lsa.transform(data)\n\n    color_mapper = {label:idx for idx, label in enumerate(set(labels))}\n    color_column = [color_mapper[label] for label in labels]\n    colors = ['green','red']\n\n    if plot:\n        plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s = 8, alpha = .8, c = labels, cmap = matplotlib.colors.ListedColormap(colors))\n        green_patch = matplotlib.patches.Patch(color = 'green', label = 'Not Mentioning Disaster')\n        red_patch = matplotlib.patches.Patch(color = 'red', label = 'Are Mentioning Disaster')\n        plt.legend(handles = [green_patch, red_patch], prop = {'size':30})\n\nfig = plt.figure(figsize=(10,10))\nplot_LSA(x_train_counts, y_train)\nplt.show","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:36:19.813008Z","iopub.execute_input":"2022-06-08T13:36:19.813966Z","iopub.status.idle":"2022-06-08T13:36:20.622151Z","shell.execute_reply.started":"2022-06-08T13:36:19.813928Z","shell.execute_reply":"2022-06-08T13:36:20.621259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Term Frequency - Inverse Document Frequency (TF-IDF)\nTF-IDF produces a statistic that reflects how important a word is to a document. It increases proportinally based on the number of times a word appears in the document. This kind of thing is how search engines determine how relavent a search result is to your query. ","metadata":{}},{"cell_type":"code","source":"# See how TF IDF looks\ndef tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n    t = tfidf_vectorizer.fit_transform(data)\n    return t, tfidf_vectorizer\n\nx_train_tfidf, tfidf_vectorizer = tfidf(x_train)\nx_test_tfidf = tfidf_vectorizer.transform(x_test)\n\nfig = plt.figure(figsize=(10, 10))\nplot_LSA(x_train_tfidf, y_train)\nplt.show","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:36:20.623503Z","iopub.execute_input":"2022-06-08T13:36:20.624011Z","iopub.status.idle":"2022-06-08T13:36:21.774255Z","shell.execute_reply.started":"2022-06-08T13:36:20.623976Z","shell.execute_reply":"2022-06-08T13:36:21.77298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(x_train[2])","metadata":{"execution":{"iopub.status.busy":"2022-06-08T13:36:21.775659Z","iopub.execute_input":"2022-06-08T13:36:21.775975Z","iopub.status.idle":"2022-06-08T13:36:21.781123Z","shell.execute_reply.started":"2022-06-08T13:36:21.77594Z","shell.execute_reply":"2022-06-08T13:36:21.780428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}