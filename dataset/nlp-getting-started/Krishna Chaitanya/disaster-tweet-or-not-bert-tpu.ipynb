{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport string\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bert-for-tf2\n!pip install sentencepiece","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1>Data Preprocessing</h1>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ndisplay(train.head())\nprint(len(train))\ndisplay(test.head())\nprint(len(test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keywords = train[\"keyword\"].value_counts()\nplt.grid()\nsns.barplot(keywords.index, keywords)\nplt.title(\"Keywords\")\nprint(keywords)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train[\"target\"].value_counts()\nplt.grid()\nsns.barplot(x.index, x)\nplt.title(\"Real or Not\")\nprint(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset has almost balanced target variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus = []\n    for x in train[train[\"target\"] == target][\"text\"].str.split():\n        print(x)\n        for i in x:\n            corpus.append(i)\n            \n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nstop_words=stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stop_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nfrom random import shuffle\nrandom.seed(1)\n\n# import these modules \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import sent_tokenize, word_tokenize\n#cleaning up text\nimport re\ndef Preprocess_text(line):\n\n    clean_line = \"\"\n\n    line = line.replace(\"â€™\", \"\")\n    line = line.replace(\"'\", \"\")\n    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n    line = line.replace(\"\\t\", \" \")\n    line = line.replace(\"\\n\", \" \")\n    line = line.lower()\n\n    for char in line:\n        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n            clean_line += char\n        else:\n            clean_line += ' '\n\n    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n    if clean_line[0] == ' ':\n        clean_line = clean_line[1:]\n    \n    #Removing stop words and convert words to base forms\n    clean_line=LemmaSentence(clean_line)\n    return clean_line\n\ndef LemmaSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words\n    New_sentence=[]\n    updated_word_list = list(set([word for word in token_words if word not in stop_words]))\n    for word in token_words:\n        lemmatizer = WordNetLemmatizer()\n        New_sentence.append(lemmatizer.lemmatize(word))\n        New_sentence.append(\" \")\n        \n    return \"\".join(New_sentence)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking an Example text"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the text post preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"Preprocess_text(train['text'][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text']=train.text.apply(lambda x:Preprocess_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text']=test.text.apply(lambda x:Preprocess_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef remove_url(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n                               u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n                               u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n                               u\"\\U0001F1E0-\\U0001F1FF\" #flags\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"    \n                               \"]+\", flags = re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punctuation(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_url(x))\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_html(x))\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_emoji(x))\ntrain[\"text\"] = train[\"text\"].apply(lambda x: remove_punctuation(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_url(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_punctuation(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_html(x))\ntest[\"text\"] = test[\"text\"].apply(lambda x: remove_emoji(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=train[\"text\"]\ny= np.array(list(train[\"target\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(text, tokenizer):\n    \n  num_examples = len(text)\n  \n  sentence = tf.ragged.constant([encode_sentence(s) for s in np.array(text)])\n  \n\n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence.shape[0]\n  input_word_ids = tf.concat([cls, sentence], axis=-1)\n\n  input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n  type_cls = tf.zeros_like(cls)\n  type_s = tf.zeros_like(sentence)\n  input_type_ids = tf.concat(\n      [type_cls, type_s], axis=-1).to_tensor()\n\n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n  return inputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('[SEP]')\n   return tokenizer.convert_tokens_to_ids(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nmodel_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\ntrain_input = bert_encode(x, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = 30\n\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strategy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nwith strategy.scope():\n    model = build_model()\n    model.summary()\n    early_stopping=EarlyStopping(monitor='val_accuracy',mode='max',patience=5,min_delta=0.01)\n    model.fit(train_input, y, epochs = 10, verbose = 1, batch_size = 128, validation_split = 0.2,callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_input=bert_encode(test[\"text\"], tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(test_input)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission['target'] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', header=True, index=False) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}