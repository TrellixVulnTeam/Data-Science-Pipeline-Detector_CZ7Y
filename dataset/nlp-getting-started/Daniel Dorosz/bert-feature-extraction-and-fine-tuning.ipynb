{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers ekphrasis keras-tuner","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:39:39.09948Z","iopub.execute_input":"2021-05-26T12:39:39.099717Z","iopub.status.idle":"2021-05-26T12:39:44.963256Z","shell.execute_reply.started":"2021-05-26T12:39:39.09969Z","shell.execute_reply":"2021-05-26T12:39:44.962222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports & Preamble","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport urllib\nimport statistics\nimport math\nimport pprint\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import (\n    Input,\n    Dense,\n    Embedding,\n    Flatten,\n    Dropout,\n    GlobalMaxPooling1D,\n    GRU,\n    concatenate,\n)\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom transformers import (\n    DistilBertTokenizerFast,\n    TFDistilBertModel,\n    DistilBertConfig,\n)\n\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\nfrom ekphrasis.classes.tokenizer import Tokenizer\nfrom ekphrasis.dicts.emoticons import emoticons\nfrom ekphrasis.dicts.noslang.slangdict import slangdict\n\nimport kerastuner","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-26T12:39:44.966575Z","iopub.execute_input":"2021-05-26T12:39:44.966843Z","iopub.status.idle":"2021-05-26T12:39:53.052732Z","shell.execute_reply.started":"2021-05-26T12:39:44.966808Z","shell.execute_reply":"2021-05-26T12:39:53.051921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper Function for Assessing Keras Models","metadata":{}},{"cell_type":"code","source":"def print_metrics(model, x_train, y_train, x_val, y_val):\n    train_acc = dict(model.evaluate(x_train, y_train, verbose=0, return_dict=True))[\n        \"accuracy\"\n    ]\n    val_acc = dict(model.evaluate(x_val, y_val, verbose=0, return_dict=True))[\n        \"accuracy\"\n    ]\n\n    val_preds = model.predict(x_val)\n    val_preds_bool = val_preds >= 0.5\n\n    print(\"\")\n    print(f\"Training Accuracy:   {train_acc:.2%}\")\n    print(f\"Validation Accuracy: {val_acc:.2%}\")\n    print(\"\")\n    print(f\"Validation f1 score: {sklearn.metrics.f1_score(val_preds_bool, y_val):.2%}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:39:53.053945Z","iopub.execute_input":"2021-05-26T12:39:53.054277Z","iopub.status.idle":"2021-05-26T12:39:53.060867Z","shell.execute_reply.started":"2021-05-26T12:39:53.054243Z","shell.execute_reply":"2021-05-26T12:39:53.059999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instantiate Pretrained Bert Model & Tokenizer","metadata":{}},{"cell_type":"code","source":"# Using DistilBERT:\nmodel_class, tokenizer_class, pretrained_weights = (TFDistilBertModel, DistilBertTokenizerFast, 'distilbert-base-uncased')\n\npretrained_bert_tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n\ndef get_pretrained_bert_model(config=pretrained_weights):\n    if not config:\n        config = DistilBertConfig(num_labels=2)\n\n    return model_class.from_pretrained(pretrained_weights, config=config)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:39:53.062519Z","iopub.execute_input":"2021-05-26T12:39:53.063275Z","iopub.status.idle":"2021-05-26T12:40:01.225298Z","shell.execute_reply.started":"2021-05-26T12:39:53.063234Z","shell.execute_reply":"2021-05-26T12:40:01.224449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and Examine Data ","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:40:01.226543Z","iopub.execute_input":"2021-05-26T12:40:01.226886Z","iopub.status.idle":"2021-05-26T12:40:01.29245Z","shell.execute_reply.started":"2021-05-26T12:40:01.226848Z","shell.execute_reply":"2021-05-26T12:40:01.291554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.info())\n\nprint(\"\")\nprint(\"train rows:\", len(train_df.index))\nprint(\"test rows:\", len(test_df.index))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T10:51:06.986631Z","iopub.execute_input":"2021-05-26T10:51:06.987027Z","iopub.status.idle":"2021-05-26T10:51:07.010804Z","shell.execute_reply.started":"2021-05-26T10:51:06.986988Z","shell.execute_reply":"2021-05-26T10:51:07.009412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"label counts:\")\ntrain_df.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T09:26:05.986167Z","iopub.execute_input":"2021-05-26T09:26:05.986492Z","iopub.status.idle":"2021-05-26T09:26:05.995492Z","shell.execute_reply.started":"2021-05-26T09:26:05.986461Z","shell.execute_reply":"2021-05-26T09:26:05.994567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"train precentage of nulls:\")\nprint(round(train_df.isnull().sum() / train_df.count() * 100, 2))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T09:26:06.494792Z","iopub.execute_input":"2021-05-26T09:26:06.495174Z","iopub.status.idle":"2021-05-26T09:26:06.522952Z","shell.execute_reply.started":"2021-05-26T09:26:06.495142Z","shell.execute_reply":"2021-05-26T09:26:06.521959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"test precentage of nulls:\")\nprint(round(test_df.isnull().sum() / test_df.count() * 100, 2))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T09:26:08.818725Z","iopub.execute_input":"2021-05-26T09:26:08.819161Z","iopub.status.idle":"2021-05-26T09:26:08.833999Z","shell.execute_reply.started":"2021-05-26T09:26:08.819118Z","shell.execute_reply":"2021-05-26T09:26:08.83296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check that we don't have any keywords appearing in one set and not the other\ntrain_keywords = set(train_df[\"keyword\"].dropna())\ntest_keywords = set(test_df[\"keyword\"].dropna())\n\nall_keywords = train_keywords.union(test_keywords)\nunique_test_keywords = all_keywords - train_keywords\nunique_train_keywords = all_keywords - test_keywords\n\nprint(f\"unique_test_keywords: {unique_test_keywords}\")\nprint(f\"unique_train_keywords: {unique_train_keywords}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T09:26:11.363893Z","iopub.execute_input":"2021-05-26T09:26:11.364234Z","iopub.status.idle":"2021-05-26T09:26:11.37498Z","shell.execute_reply.started":"2021-05-26T09:26:11.364202Z","shell.execute_reply":"2021-05-26T09:26:11.373821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# We'll use these weights later on to make up for the slightly imbalanced dataset\nclasses = np.unique(train_df[\"target\"])\nclass_weights = sklearn.utils.class_weight.compute_class_weight(\n    \"balanced\", classes=classes, y=train_df[\"target\"]\n)\n\nclass_weights = {clazz : weight for clazz, weight in zip(classes, class_weights)}","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:40:01.296372Z","iopub.execute_input":"2021-05-26T12:40:01.296621Z","iopub.status.idle":"2021-05-26T12:40:01.306642Z","shell.execute_reply.started":"2021-05-26T12:40:01.296597Z","shell.execute_reply":"2021-05-26T12:40:01.305638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop Duplicates","metadata":{}},{"cell_type":"code","source":"# Commented out the graceful handling of duplicated because the Kaggle kernel version of statistics.mode()\n# won't handle multimodal results\n\n# Duplicates aren't consistently labeled, so we keep one example of the most frequently occuring label\n# train_df[\"duplicated\"] = train_df.duplicated(subset=\"text\")\n# duplicated_tweets = train_df.loc[lambda df: df[\"duplicated\"] == True, :]\n# aggregated_duplicates = duplicated_tweets.groupby(\"text\", as_index=False).aggregate(\n#     statistics.mode\n# )\n\n# train_df.drop_duplicates(subset=\"text\", inplace=True, keep=False)\n# train_df = train_df.append(aggregated_duplicates, ignore_index=True)\n\ntrain_df.drop_duplicates(subset=\"text\", inplace=True, keep=False)\nprint(\"train rows:\", len(train_df.index))\nprint(\"test rows:\", len(test_df.index))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:40:01.308257Z","iopub.execute_input":"2021-05-26T12:40:01.308765Z","iopub.status.idle":"2021-05-26T12:40:01.329982Z","shell.execute_reply.started":"2021-05-26T12:40:01.308728Z","shell.execute_reply":"2021-05-26T12:40:01.32902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean Tweets","metadata":{}},{"cell_type":"code","source":"class TweetPreProcessor:\n    \"\"\"\n    This class does some cleaning and normalization prior to BPE tokenization\n    \"\"\"\n\n    def __init__(self):\n\n        self.text_processor = TextPreProcessor(\n            # terms that will be normalized\n            normalize=[\n                \"url\",\n                \"email\",\n                \"phone\",\n                \"user\",\n                \"time\",\n                \"date\",\n            ],\n            # terms that will be annotated\n            annotate={\"repeated\", \"elongated\"},\n            # corpus from which the word statistics are going to be used\n            # for word segmentation\n            segmenter=\"twitter\",\n            # corpus from which the word statistics are going to be used\n            # for spell correction\n            spell_correction=True,\n            corrector=\"twitter\",\n            unpack_hashtags=False,  # perform word segmentation on hashtags\n            unpack_contractions=False,  # Unpack contractions (can't -> can not)\n            spell_correct_elong=True,  # spell correction for elongated words\n            fix_bad_unicode=True,\n            tokenizer=Tokenizer(lowercase=True).tokenize,\n            # list of dictionaries, for replacing tokens extracted from the text,\n            # with other expressions. You can pass more than one dictionaries.\n            dicts=[emoticons, slangdict],\n        )\n\n    def preprocess_tweet(self, tweet):\n        return \" \".join(self.text_processor.pre_process_doc(tweet))\n    \n    # this will return the tokenized text     \n    def __call__(self, tweet):\n        return self.text_processor.pre_process_doc(tweet)\n    \ntweet_preprocessor = TweetPreProcessor()","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:40:01.331445Z","iopub.execute_input":"2021-05-26T12:40:01.331842Z","iopub.status.idle":"2021-05-26T12:40:19.475164Z","shell.execute_reply.started":"2021-05-26T12:40:01.331793Z","shell.execute_reply":"2021-05-26T12:40:19.474337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Have a look at how the TweetProcessor is doing\nfor tweet in train_df[100:120][\"text\"]:\n    print(\"original:  \", tweet)\n    print(\"processed: \", tweet_preprocessor.preprocess_tweet(tweet))\n    print(\"\")","metadata":{"execution":{"iopub.status.busy":"2021-05-25T16:44:42.503259Z","iopub.execute_input":"2021-05-25T16:44:42.503622Z","iopub.status.idle":"2021-05-25T16:44:42.536198Z","shell.execute_reply.started":"2021-05-25T16:44:42.503582Z","shell.execute_reply":"2021-05-25T16:44:42.535437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"text\"] = train_df[\"text\"].apply(tweet_preprocessor.preprocess_tweet)\ntest_df[\"text\"] = test_df[\"text\"].apply(tweet_preprocessor.preprocess_tweet)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:40:19.476438Z","iopub.execute_input":"2021-05-26T12:40:19.476797Z","iopub.status.idle":"2021-05-26T12:40:24.209053Z","shell.execute_reply.started":"2021-05-26T12:40:19.476756Z","shell.execute_reply":"2021-05-26T12:40:24.208048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean Keywords","metadata":{}},{"cell_type":"code","source":"# Fill NA\ntrain_df[\"keyword\"].fillna(\"\", inplace=True)\ntest_df[\"keyword\"].fillna(\"\", inplace=True)\n\n# remove %20 from keywords\ntrain_df[\"keyword\"] = train_df[\"keyword\"].apply(urllib.parse.unquote)\ntest_df[\"keyword\"] = test_df[\"keyword\"].apply(urllib.parse.unquote)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:40:24.210526Z","iopub.execute_input":"2021-05-26T12:40:24.210858Z","iopub.status.idle":"2021-05-26T12:40:24.230044Z","shell.execute_reply.started":"2021-05-26T12:40:24.210822Z","shell.execute_reply":"2021-05-26T12:40:24.229088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train-Test Split","metadata":{}},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = sklearn.model_selection.train_test_split(\n    train_df[[\"text\", \"keyword\"]], train_df[\"target\"], test_size=0.3, random_state=42, stratify=train_df[\"target\"]\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:40:24.231203Z","iopub.execute_input":"2021-05-26T12:40:24.231752Z","iopub.status.idle":"2021-05-26T12:40:24.255216Z","shell.execute_reply.started":"2021-05-26T12:40:24.231715Z","shell.execute_reply":"2021-05-26T12:40:24.254419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenisation and Encode ","metadata":{}},{"cell_type":"code","source":"def tokenize_encode(tweets, max_length=None):\n    return pretrained_bert_tokenizer(\n        tweets,\n        add_special_tokens=True,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_length,\n        return_tensors=\"tf\",\n    )\n\n\n# need to be explicit about the lengths (instead of just specifying padding=True in the tokenizer)\n# otherwise train tweets end up being 71 and validation tweets end up as 70, which causes problems/warnings\nmax_length_tweet = 72\nmax_length_keyword = 8\n\ntrain_tweets_encoded = tokenize_encode(x_train[\"text\"].to_list(), max_length_tweet) \nvalidation_tweets_encoded = tokenize_encode(x_val[\"text\"].to_list(), max_length_tweet) \n\ntrain_keywords_encoded = tokenize_encode(x_train[\"keyword\"].to_list(), max_length_keyword) \nvalidation_keywords_encoded = tokenize_encode(x_val[\"keyword\"].to_list(), max_length_keyword) \n\ntrain_inputs_encoded = dict(train_tweets_encoded)\ntrain_inputs_encoded[\"keywords\"] = train_keywords_encoded[\"input_ids\"]\n\nvalidation_inputs_encoded = dict(validation_tweets_encoded)\nvalidation_inputs_encoded[\"keywords\"] = validation_keywords_encoded[\"input_ids\"]\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:40:24.256572Z","iopub.execute_input":"2021-05-26T12:40:24.256942Z","iopub.status.idle":"2021-05-26T12:40:31.519458Z","shell.execute_reply.started":"2021-05-26T12:40:24.256888Z","shell.execute_reply":"2021-05-26T12:40:31.518609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create TF Dataset","metadata":{}},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(train_tweets_encoded), y_train)\n)\n\nval_dataset = tf.data.Dataset.from_tensor_slices(\n    (dict(validation_tweets_encoded), y_val)\n)\n\ntrain_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\n    (train_inputs_encoded, y_train)\n)\n\nval_multi_input_dataset = tf.data.Dataset.from_tensor_slices(\n    (validation_inputs_encoded, y_val)\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:40:31.522472Z","iopub.execute_input":"2021-05-26T12:40:31.52272Z","iopub.status.idle":"2021-05-26T12:40:31.544478Z","shell.execute_reply.started":"2021-05-26T12:40:31.522695Z","shell.execute_reply":"2021-05-26T12:40:31.543747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline with Logistic Regression on a TF-IDF Bag of Words","metadata":{}},{"cell_type":"markdown","source":"## Create TF-IDF Vectors","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n    tokenizer=tweet_preprocessor, min_df=1, ngram_range=(1, 1), norm=\"l2\"\n)\n\ntrain_vectors = tfidf_vectorizer.fit_transform(raw_documents=x_train[\"text\"]).toarray()\nvalidation_vectors = tfidf_vectorizer.transform(x_val[\"text\"]).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T16:44:54.316931Z","iopub.execute_input":"2021-05-25T16:44:54.317216Z","iopub.status.idle":"2021-05-25T16:44:57.961321Z","shell.execute_reply.started":"2021-05-25T16:44:54.317187Z","shell.execute_reply":"2021-05-25T16:44:57.960422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run Classifier","metadata":{}},{"cell_type":"code","source":"# I obtained the value of C by experimenting with LogisticRegressionCV but I'm leaving it out for brevity\nlogisticRegressionClf = LogisticRegression(n_jobs=-1, C=2.78)\nlogisticRegressionClf.fit(train_vectors, y_train)\n\ndef print_metrics_sk(clf, x_train, y_train, x_val, y_val):\n    print(f\"Train Accuracy:         {clf.score(x_train, y_train):.2%}\")\n    print(f\"Validation Accuracy:    {clf.score(x_val, y_val):.2%}\")\n    print(\"\")\n    print(f\"f1 score:               {sklearn.metrics.f1_score(y_val, clf.predict(x_val)):.2%}\")\n\nprint_metrics_sk(logisticRegressionClf, train_vectors, y_train, validation_vectors, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T16:44:57.962727Z","iopub.execute_input":"2021-05-25T16:44:57.963152Z","iopub.status.idle":"2021-05-25T16:45:41.348271Z","shell.execute_reply.started":"2021-05-25T16:44:57.963095Z","shell.execute_reply":"2021-05-25T16:45:41.347252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction with BERT","metadata":{}},{"cell_type":"markdown","source":"## Extract Sentence Vectors and Attention Embeddings","metadata":{}},{"cell_type":"code","source":"feature_extractor = get_pretrained_bert_model()\n\n# Run a forward pass on the tokenized inputs\n# model_outputs = feature_extractor(\n#     train_tweets_encoded[\"input_ids\"], train_tweets_encoded[\"attention_mask\"]\n# )\nmodel_outputs = feature_extractor.predict(\n    train_dataset.batch(32)\n)\n# BERT's sentence representation can be retrieved from a hidden vector at index 0 in the sequence, \n# (where the special token CLS was prepended by the tokenizer)\ntrain_sentence_vectors = model_outputs.last_hidden_state[:, 0, :]\n\n# The rest of the sequence contains the embeddings \n# (modified by successive layers of self-attention) for each token\ntrain_word_vectors = model_outputs.last_hidden_state[:, 1:, :]\n\n# And the same again for the validation set\n# model_outputs = feature_extractor(\n#     validation_tweets_encoded[\"input_ids\"], validation_tweets_encoded[\"attention_mask\"]\n# )\nmodel_outputs = feature_extractor.predict(\n    val_dataset.batch(32)\n)\nvalidation_sentence_vectors = model_outputs.last_hidden_state[:, 0, :]\nvalidation_word_vectors = model_outputs.last_hidden_state[:, 1:, :]","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:40:31.546218Z","iopub.execute_input":"2021-05-26T12:40:31.546459Z","iopub.status.idle":"2021-05-26T12:41:26.918322Z","shell.execute_reply.started":"2021-05-26T12:40:31.546435Z","shell.execute_reply":"2021-05-26T12:41:26.917214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression with BERT Sentence Vectors","metadata":{}},{"cell_type":"code","source":"logisticRegressionClf = LogisticRegression(n_jobs=-1, class_weight=class_weights)\nlogisticRegressionClf.fit(train_sentence_vectors, y_train)\n\nprint_metrics_sk(\n    logisticRegressionClf,\n    train_sentence_vectors,\n    y_train,\n    validation_sentence_vectors,\n    y_val,\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T16:57:11.472817Z","iopub.execute_input":"2021-05-25T16:57:11.473181Z","iopub.status.idle":"2021-05-25T16:57:18.398028Z","shell.execute_reply.started":"2021-05-25T16:57:11.473146Z","shell.execute_reply":"2021-05-25T16:57:18.396919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RNN with BERT Attention Embeddings ","metadata":{}},{"cell_type":"code","source":"def create_gru_model() -> keras.Model:\n\n    model = keras.Sequential()\n    model.add(keras.layers.InputLayer(input_shape=train_word_vectors.shape[1:]))\n    model.add(GRU(32, return_sequences=True))\n    model.add(GlobalMaxPooling1D())\n    model.add(Dense(1, activation=\"sigmoid\"))\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\nmodel = create_gru_model()\n\nhistory = model.fit(\n    train_word_vectors,\n    y_train,\n    validation_data=(validation_word_vectors, y_val),\n    class_weight=class_weights,\n    epochs=20,\n    verbose=0,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(model, train_word_vectors, y_train, validation_word_vectors, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T16:59:34.124884Z","iopub.execute_input":"2021-05-25T16:59:34.125265Z","iopub.status.idle":"2021-05-25T16:59:52.544561Z","shell.execute_reply.started":"2021-05-25T16:59:34.125228Z","shell.execute_reply":"2021-05-25T16:59:52.543696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multi-Input Classifier with Sentence Vectors & Keywords","metadata":{}},{"cell_type":"code","source":"def create_multi_input_model() -> keras.Model:\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(1)(keyword_features)\n\n    tweet_classification_vectors = keras.Input((train_sentence_vectors.shape[1],), name=\"tweets\")\n    tweet_features = Dense(1, activation='relu')(tweet_classification_vectors)    \n\n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, tweet_classification_vectors], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\n\nmodel = create_multi_input_model()\n\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_sentence_vectors}\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_sentence_vectors}\n\nhistory = model.fit(\n    train_inputs,\n    y_train,\n    validation_data=(validation_inputs, y_val),\n    class_weight=class_weights,\n    epochs=20,\n    verbose=0,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\n\nprint_metrics(model, train_inputs, y_train, validation_inputs, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T17:00:19.273658Z","iopub.execute_input":"2021-05-25T17:00:19.274017Z","iopub.status.idle":"2021-05-25T17:00:29.935428Z","shell.execute_reply.started":"2021-05-25T17:00:19.273981Z","shell.execute_reply":"2021-05-25T17:00:29.934523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RNN with Attention Embeddings & Keywords","metadata":{}},{"cell_type":"code","source":"def create_multi_input_rnn_model() -> keras.Model:\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(1)(keyword_features)\n\n    tweet_token_embeddings = Input(train_word_vectors.shape[1:], name=\"tweets\")\n    tweet_features = GRU(32, return_sequences=True)(tweet_token_embeddings)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n    tweet_features = Dense(1, activation='relu')(tweet_features)    \n\n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, tweet_token_embeddings], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\n\nmodel = create_multi_input_rnn_model()\n\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_word_vectors}\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_word_vectors}\n\nhistory = model.fit(\n    train_inputs,\n    y_train,\n    validation_data=(validation_inputs, y_val),\n    class_weight=class_weights,\n    epochs=20,\n    verbose=0,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(model, train_inputs, y_train, validation_inputs, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T17:03:10.435393Z","iopub.execute_input":"2021-05-25T17:03:10.435776Z","iopub.status.idle":"2021-05-25T17:03:47.790488Z","shell.execute_reply.started":"2021-05-25T17:03:10.435744Z","shell.execute_reply":"2021-05-25T17:03:47.789508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Architecture Search For Best Classification Head","metadata":{}},{"cell_type":"code","source":"def create_candidate_model_with_fx(hp: kerastuner.HyperParameters) -> keras.Model:\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=feature_extractor.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(hp.Choice(\"keyword_units\", values=[1, 8, 16, 32], default=1))(keyword_features)\n\n    tweet_token_embeddings = Input(train_word_vectors.shape[1:], name=\"tweets\")\n    \n    tweet_features = GRU(hp.Choice(\"GRU_units\", values=[8, 16, 32, 64, 128], default=32), return_sequences=True)(tweet_token_embeddings)\n    tweet_features = Dropout(hp.Float(\"GRU_dropout\", min_value=0.0, max_value=0.5, step=0.1))(tweet_features)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n    \n    for i in range(hp.Int(\"num_layers\", min_value=0, max_value=3, step=1)):\n        tweet_features = Dense(hp.Choice(\"layer_\" + str(i) + \"_units\", values=[2, 8, 16, 32, 64, 128, 256]), activation=\"relu\")(tweet_features)\n        tweet_features = Dropout(hp.Float(\"layer_\" + str(i) + \"_dropout\", min_value=0.0, max_value=0.5, step=0.1))(tweet_features)\n    \n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, tweet_token_embeddings], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\ntrain_inputs = {\"keywords\" : train_keywords_encoded[\"input_ids\"], \"tweets\" : train_word_vectors}\nvalidation_inputs = {\"keywords\" : validation_keywords_encoded[\"input_ids\"], \"tweets\" : validation_word_vectors}\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:41:26.919842Z","iopub.execute_input":"2021-05-26T12:41:26.920231Z","iopub.status.idle":"2021-05-26T12:41:26.930898Z","shell.execute_reply.started":"2021-05-26T12:41:26.920187Z","shell.execute_reply":"2021-05-26T12:41:26.930017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperband Tuning\nMAX_EPOCHS = 10\nFACTOR = 3\nITERATIONS = 3\n\nprint(f\"Number of models in each bracket: {math.ceil(1 + math.log(MAX_EPOCHS, FACTOR))}\")\nprint(f\"Number of epochs over all trials: {round(ITERATIONS * (MAX_EPOCHS * (math.log(MAX_EPOCHS, FACTOR) ** 2)))}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:41:26.932227Z","iopub.execute_input":"2021-05-26T12:41:26.932816Z","iopub.status.idle":"2021-05-26T12:41:26.951715Z","shell.execute_reply.started":"2021-05-26T12:41:26.932759Z","shell.execute_reply":"2021-05-26T12:41:26.950533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner = kerastuner.Hyperband(\n    create_candidate_model_with_fx,\n    max_epochs=MAX_EPOCHS,\n    hyperband_iterations=ITERATIONS, \n    factor=FACTOR, \n    objective=\"val_accuracy\",\n    directory=\"hyperparam-search\",\n    project_name=\"architecture-hyperband\",\n)\n\ntuner.search(\n    train_inputs,\n    y_train,\n    validation_data=(validation_inputs, y_val),\n    class_weight=class_weights,\n    epochs=10,\n    verbose=1,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=3,\n            restore_best_weights=True,\n        )\n    ],\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:41:26.953082Z","iopub.execute_input":"2021-05-26T12:41:26.953434Z","iopub.status.idle":"2021-05-26T13:00:55.090434Z","shell.execute_reply.started":"2021-05-26T12:41:26.9534Z","shell.execute_reply":"2021-05-26T13:00:55.089626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuner.results_summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = tuner.get_best_models()[0]\n# best_model.summary()\nprint(\"\")\nbest_arch_hp = tuner.get_best_hyperparameters()[0]\npprint.pprint(best_arch_hp.values, indent=4)\nprint(\"\")\n\nprint_metrics(best_model, train_inputs, y_train, validation_inputs, y_val)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T13:08:53.477729Z","iopub.execute_input":"2021-05-26T13:08:53.478247Z","iopub.status.idle":"2021-05-26T13:08:58.501341Z","shell.execute_reply.started":"2021-05-26T13:08:53.478204Z","shell.execute_reply":"2021-05-26T13:08:58.499892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT Fine Tuning","metadata":{}},{"cell_type":"markdown","source":"N.B. Typically one might freeze the base model, train the added classifier for a bit, unfreeze the base model, lower the learning rate and train the whole model again. \n\nHowever Huggingface recommend that training an unfrozen model right from the beginning (with a low learning rate) works better with transformers. \n\nI tried both and there seemed to be no advantage to freeze-unfreeze. Sometimes it even reported an inferior score. However, it's hard to be certain given the large random fluctations between training runs with such a small dataset. I didn't test this with kfold validation which may have yielded more conclusive results. \n\nI have read in some papers that gradual unfreezing of the blocks in the base model can lead to better results.","metadata":{}},{"cell_type":"markdown","source":"## Fine-Tune BERT with Simple Head on Sentence Vector","metadata":{}},{"cell_type":"code","source":"# To create a baseline for the simplest possible fine-tuned BERT\ndef create_bert_simple_for_ft():\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\n\n    pretrained_bert_model = get_pretrained_bert_model()\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\n\n    prediction = Dense(1, activation=\"sigmoid\")(bert_outputs.last_hidden_state[:, 0, :])\n    return keras.Model(inputs=[input_ids, attention_mask], outputs=prediction)\n\nmodel = create_bert_simple_for_ft()\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\nmodel.fit(\n    train_dataset.batch(32),\n    validation_data=val_dataset.batch(32),\n    class_weight=class_weights,\n    epochs=20,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=5,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(\n    model, dict(train_tweets_encoded), y_train, dict(validation_tweets_encoded), y_val\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T17:04:23.671572Z","iopub.execute_input":"2021-05-25T17:04:23.671909Z","iopub.status.idle":"2021-05-25T17:07:31.736689Z","shell.execute_reply.started":"2021-05-25T17:04:23.671869Z","shell.execute_reply":"2021-05-25T17:07:31.735228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tune BERT with RNN on Attention Embeddings and Keywords","metadata":{}},{"cell_type":"code","source":"def create_bert_rnn_for_ft():\n    \n    pretrained_bert_model = get_pretrained_bert_model()\n    \n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=pretrained_bert_model.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(1)(keyword_features)\n\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\n\n    bert_token_embeddings = bert_outputs.last_hidden_state[:, 1:, :]\n    tweet_features = GRU(32, return_sequences=True)(bert_token_embeddings)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n\n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, input_ids, attention_mask], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n\nmodel = create_bert_rnn_for_ft()\n\nmodel.fit(\n    train_multi_input_dataset.batch(32),\n    validation_data=val_multi_input_dataset.batch(32),\n    epochs=20,\n    class_weight=class_weights,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\",\n            min_delta=0.001,\n            patience=3,\n            restore_best_weights=True,\n        )\n    ],\n)\n\nprint_metrics(\n    model, train_inputs_encoded, y_train, validation_inputs_encoded, y_val\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T09:43:25.431992Z","iopub.execute_input":"2021-05-26T09:43:25.432357Z","iopub.status.idle":"2021-05-26T09:46:13.289672Z","shell.execute_reply.started":"2021-05-26T09:43:25.432326Z","shell.execute_reply":"2021-05-26T09:46:13.288206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine-Tune BERT with With Best Classification Head","metadata":{}},{"cell_type":"code","source":"def create_model_candidate() -> keras.Model:\n    pretrained_bert_model = get_pretrained_bert_model()\n\n    keyword_ids = keras.Input((8,), name=\"keywords\")\n    keyword_features = Embedding(input_dim=pretrained_bert_model.config.vocab_size, output_dim=16, input_length=8, mask_zero=True)(keyword_ids)\n    keyword_features = Flatten()(keyword_features)\n    keyword_features = Dense(best_arch_hp.get(\"keyword_units\"))(keyword_features)\n\n    input_ids = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"input_ids\")\n    attention_mask = Input(shape=(max_length_tweet,), dtype=\"int32\", name=\"attention_mask\")\n    bert_outputs = pretrained_bert_model(input_ids, attention_mask)\n    bert_token_embeddings = bert_outputs.last_hidden_state[:, 1:, :]\n    tweet_features = GRU(best_arch_hp.get(\"GRU_units\"), return_sequences=True)(bert_token_embeddings)\n    tweet_features = Dropout(best_arch_hp.get(\"GRU_dropout\"))(tweet_features)\n    tweet_features = GlobalMaxPooling1D()(tweet_features)\n    \n    for i in range(best_arch_hp.get(\"num_layers\")):\n        tweet_features = Dense(best_arch_hp.get(\"layer_\" + str(i) + \"_units\"), activation=\"relu\")(tweet_features)\n        tweet_features = Dropout(best_arch_hp.get(\"layer_\" + str(i) + \"_dropout\"))(tweet_features)\n    \n    combined_features = concatenate([keyword_features, tweet_features])\n    combined_prediction = Dense(1, activation=\"sigmoid\")(combined_features)\n\n    model = keras.Model(inputs = [keyword_ids, input_ids, attention_mask], outputs=combined_prediction)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n        loss=\"binary_crossentropy\",\n        metrics=keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n    )\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:04:57.684238Z","iopub.execute_input":"2021-05-26T14:04:57.684551Z","iopub.status.idle":"2021-05-26T14:04:57.694584Z","shell.execute_reply.started":"2021-05-26T14:04:57.684522Z","shell.execute_reply":"2021-05-26T14:04:57.69359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model_candidate()\n\nhistory = model.fit(\n    train_multi_input_dataset.batch(32),\n    validation_data=val_multi_input_dataset.batch(32),\n    epochs=6,\n    class_weight=class_weights,\n    callbacks=[\n        keras.callbacks.EarlyStopping(\n            monitor=\"val_accuracy\", restore_best_weights=True\n        )\n    ],\n)\n\nbest_epoch = len(history.history[\"val_accuracy\"]) - 1\n\nprint_metrics(\n    model, train_inputs_encoded, y_train, validation_inputs_encoded, y_val\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:28:06.798573Z","iopub.execute_input":"2021-05-26T14:28:06.798898Z","iopub.status.idle":"2021-05-26T14:31:24.928451Z","shell.execute_reply.started":"2021-05-26T14:28:06.798868Z","shell.execute_reply":"2021-05-26T14:31:24.927543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission","metadata":{}},{"cell_type":"markdown","source":"## Tokenize and Encode Test Set","metadata":{}},{"cell_type":"code","source":"test_tweets_encoded = tokenize_encode(test_df[\"text\"].to_list(), max_length_tweet)\ntest_inputs_encoded = dict(test_tweets_encoded)\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_inputs_encoded)\n\ntest_keywords_encoded = tokenize_encode(test_df[\"keyword\"].to_list(), max_length_keyword)\ntest_inputs_encoded[\"keywords\"] = test_keywords_encoded[\"input_ids\"]\ntest_multi_input_dataset = tf.data.Dataset.from_tensor_slices(test_inputs_encoded)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:07:22.074732Z","iopub.execute_input":"2021-05-26T14:07:22.075115Z","iopub.status.idle":"2021-05-26T14:07:22.47863Z","shell.execute_reply.started":"2021-05-26T14:07:22.075083Z","shell.execute_reply":"2021-05-26T14:07:22.477373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Model","metadata":{}},{"cell_type":"code","source":"full_train_dataset = train_multi_input_dataset.concatenate(val_multi_input_dataset)\nmodel = create_model_candidate()\n\nmodel.fit(\n    full_train_dataset.batch(32),\n    epochs=best_epoch,\n    class_weight=class_weights,\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:36:34.572631Z","iopub.execute_input":"2021-05-26T14:36:34.57297Z","iopub.status.idle":"2021-05-26T14:36:40.946512Z","shell.execute_reply.started":"2021-05-26T14:36:34.572936Z","shell.execute_reply":"2021-05-26T14:36:40.943822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Predictions","metadata":{}},{"cell_type":"code","source":"preds = np.squeeze(model.predict(test_multi_input_dataset.batch(32)))\npreds = (preds >= 0.5).astype(int)\npd.DataFrame({\"id\": test_df.id, \"target\": preds}).to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T14:14:50.028077Z","iopub.execute_input":"2021-05-26T14:14:50.02839Z","iopub.status.idle":"2021-05-26T14:14:57.617967Z","shell.execute_reply.started":"2021-05-26T14:14:50.028364Z","shell.execute_reply":"2021-05-26T14:14:57.617143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}