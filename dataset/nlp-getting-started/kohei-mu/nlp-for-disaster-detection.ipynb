{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n1. **[Library Load](#Library-Load)**\n2. **[Data Load](#Data-Load)**\n3. **[EDA](#EDA)**\n4. **[Preprocessing](#Preprocessing)**\n5. **[Feature Extraction](#Feature-Extraction)**\n6. **[Modeling](#Modeling)**\n7. **[Submission](#Submission)**"},{"metadata":{},"cell_type":"markdown","source":"## Library Load"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport copy\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwnl = WordNetLemmatizer()\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\npd.set_option(\"display.max_colwidth\", 80)\nimport modeling_functions as mf\nimport nlp_preprocessing_functions as npf\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Load"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\nprint('Train Set Shape = {}'.format(train.shape))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\nprint('Test Set Shape = {}'.format(test.shape))\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{},"cell_type":"markdown","source":"### Handling Missing Values"},{"metadata":{},"cell_type":"markdown","source":"According to `Data Description`, there are many missing values in the keyward/location column.\n- there are so many missing values in 'location' columnn than 'keyword' column.\n- the ratios of missing values in train/test set are so close.\n- the missing values are filled with 'no_keyword' and 'no_location' respectively."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"missing_cols = ['keyword', 'location']\n\nfig, axes = plt.subplots(ncols=2, figsize=(20, 5))\n\ntrain_sums = train[missing_cols].isnull().sum()\ntest_sums = test[missing_cols].isnull().sum()\nsns.barplot(x=train_sums.index, y=train_sums.values, ax=axes[0])\nsns.barplot(x=test_sums.index, y=test_sums.values, ax=axes[1])\n\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].set_title('Train Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)\naxes[0].tick_params(axis='x', labelsize=12)\naxes[0].tick_params(axis='y', labelsize=12)\naxes[1].tick_params(axis='x', labelsize=12)\naxes[1].tick_params(axis='y', labelsize=12)\n\nplt.show()\n\ntrain_dropna, test_dropna = copy.copy(train), copy.copy(test)\nfor df in [train_dropna, test_dropna]:\n    for col in ['keyword', 'location']:\n        df[col] = df[col].fillna(f'no_{col}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Processing keyword and location"},{"metadata":{},"cell_type":"markdown","source":"- Since there are many unique values(and nulls) in the location column, it can be said that location value is not generated automatically and may be human-inputs.\n- As described above, the column is dirty and <u>shouldn't be used as a feature</u>."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Unique Values in location column(train set) : {train_dropna[\"location\"].nunique()} / {len(train_dropna[\"location\"])}')\nprint(f'Unique Values in location column(test set) : {test_dropna[\"location\"].nunique()} / {len(test_dropna[\"location\"])}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Compared to location column, there are less unique values(and nulls) in keyword column, so keyword column is more reliable and may have relationship or rules with target column.\n- As described above, the column can be <u>used as a feature by itself</u>."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Unique Values in keyword column(train set) : {train_dropna[\"keyword\"].nunique()} / {len(train_dropna[\"keyword\"])}')\nprint(f'Unique Values in keyword column(test set) : {test_dropna[\"keyword\"].nunique()} / {len(test_dropna[\"keyword\"])}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### keyword / location column"},{"metadata":{},"cell_type":"markdown","source":"- drop location column.\n- encode keyword column into labeled feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preprocessed, test_preprocessed = copy.copy(train_dropna), copy.copy(test_dropna)\ntrain_preprocessed.drop(['location'], axis=1, inplace=True)\nle = LabelEncoder()\nle.fit(train_preprocessed['keyword'])\ntrain_preprocessed['keyword'] = le.transform(train_preprocessed['keyword'])\ntrain_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preprocessed.drop(['location'], axis=1, inplace=True)\ntest_preprocessed['keyword'] = le.transform(test_preprocessed['keyword'])\ntest_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### id column"},{"metadata":{},"cell_type":"markdown","source":"- Since id column is not related with modeling, drop the column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preprocessed.drop(['id'], axis=1, inplace=True)\ntrain_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preprocessed.drop(['id'], axis=1, inplace=True)\ntest_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction"},{"metadata":{},"cell_type":"markdown","source":"### TF-IDF"},{"metadata":{},"cell_type":"markdown","source":"- load util functions for preprocessing.  \n    - [NLP Preprocessing Functions](https://www.kaggle.com/koheimuramatsu/nlp-preprocessing-functions)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences_raw_train = train_preprocessed['text']\nsentences_raw_test = test_preprocessed['text']\nsentences_preprocessed_train = []\nsentences_preprocessed_test = []\nfor sentence in sentences_raw_train:\n    lemmas = npf.tokenizer(sentence)\n    sentence_without_stop_words = npf.remove_stop_words(lemmas, st_list=['amp','ca','ha','http http','new','rt','wa'])\n    sentences_preprocessed_train.append(sentence_without_stop_words)\nsentences_preprocessed_train = [\" \".join(doc) for doc in sentences_preprocessed_train]\nfor sentence in sentences_raw_test:\n    lemmas = npf.tokenizer(sentence)\n    sentence_without_stop_words = npf.remove_stop_words(lemmas, st_list=['amp','ca','ha','http http','new','rt','wa'])\n    sentences_preprocessed_test.append(sentence_without_stop_words)\nsentences_preprocessed_test = [\" \".join(doc) for doc in sentences_preprocessed_test]\nsentences_tfidf = copy.copy(sentences_preprocessed_train)\nsentences_tfidf.extend(sentences_preprocessed_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_train, tfidf_test = npf.tfidf_features(docs_tfidf=sentences_tfidf, docs_train=sentences_preprocessed_train, docs_test=sentences_preprocessed_test, _max_features=1000)\ntfidf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preprocessed = pd.concat([train_preprocessed, tfidf_train], axis=1)\ntrain_preprocessed.drop(['text'], axis=1, inplace=True)\ntrain_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preprocessed = pd.concat([test_preprocessed, tfidf_test], axis=1)\ntest_preprocessed.drop(['text'], axis=1, inplace=True)\ntest_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Meta Features\nthis idea is from [this notebook](https://www.kaggle.com/gunesevitan/nlp-with-disaster-tweets-read-before-start-eda)"},{"metadata":{},"cell_type":"markdown","source":"- Checking sentences from the text column, I found out that disaster tweets has longer texts and the style of texts are more formal than non-disaster tweeets.\n- I think this is because disaster tweets tend to be written by news agencies and non-disaster tweets tend to be written by individual twitter users.\n  \nSo, the meta features I extracted is ...\n- word_count\n- unique_word_count\n- mean_word_count\n- punctuation_count\n\nAnd I added news related features.\n- news_word_count\n- disaster_word_count"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_feature_train = pd.DataFrame(columns=[\"word_count\", \"unique_word_count\", \"mean_word_count\", \"punctuation_count\", \"news_word_count\", \"disaster_word_count\"])\nmeta_feature_test = pd.DataFrame(columns=[\"word_count\", \"unique_word_count\", \"mean_word_count\", \"punctuation_count\", \"news_word_count\", \"disaster_word_count\"])\n\ntokenized_train =  sentences_raw_train.apply(lambda x: npf.tokenizer(x))\ntokenized_test =  sentences_raw_test.apply(lambda x: npf.tokenizer(x))\n\n#word_count\nmeta_feature_train[\"word_count\"] = tokenized_train.apply(lambda x: len(x))\nmeta_feature_test[\"word_count\"] = tokenized_test.apply(lambda x: len(x))\n\n#unique_word_count\nmeta_feature_train[\"unique_word_count\"] = tokenized_train.apply(lambda x: len(set(x)))\nmeta_feature_test[\"unique_word_count\"] = tokenized_test.apply(lambda x: len(set(x)))\n\n#mean_word_count\nmeta_feature_train[\"mean_word_count\"] = tokenized_train.apply(lambda x: np.mean([len(i) for i in x]))\nmeta_feature_test[\"mean_word_count\"] = tokenized_test.apply(lambda x: np.mean([len(i) for i in x]))\n\n#punctuation_count\nmeta_feature_train[\"punctuation_count\"] = sentences_raw_train.apply(lambda x: len([c for c in x if c in string.punctuation]))\nmeta_feature_test[\"punctuation_count\"] = sentences_raw_test.apply(lambda x: len([c for c in x if c in string.punctuation]))\n\n#news_word_count\nnews_related_word = [\"news\", \"report\",\"pm\", \"am\", \"utc\", \"breaking\", \"bbc\", \"abc\", \"fox\", \"gov\", \"government\", \"whitehouse\", \"huff\", \"journal\", \"cbc\", \"cbs\", \"official\", \"officer\", \"cnn\", \"yorker\", \"yahoo\", \"tv\", \"radio\"]\nmeta_feature_train[\"news_word_count\"] = tokenized_train.apply(lambda x: len([w for w in x if w in news_related_word]))\nmeta_feature_test[\"news_word_count\"] = tokenized_test.apply(lambda x: len([w for w in x if w in news_related_word]))\n\n#disaster_word_count\ndisaster_related_word = [\"disaster\", \"accudent\", \"kill\", \"killed\", \"killing\", \"died\", \"earthquake\", \"death\", \"bomb\", \"bombed\", \"bombing\", \"flood\", \"fire\", \"wildfire\", \"burn\", \"burning\", \"crash\", \"victims\", \"war\", \"weapons\", \"military\", \"force\", \"forces\", \"survive\", \"survived\", \"blood\"]\nmeta_feature_train[\"disaster_word_count\"] = tokenized_train.apply(lambda x: len([w for w in x if w in disaster_related_word]))\nmeta_feature_test[\"disaster_word_count\"] = tokenized_test.apply(lambda x: len([w for w in x if w in disaster_related_word]))\n\ntrain_preprocessed = pd.concat([train_preprocessed, meta_feature_train], axis=1)\ntrain_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preprocessed = pd.concat([test_preprocessed, meta_feature_test], axis=1)\ntest_preprocessed.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_preprocessed['target']\nx_train = train_preprocessed.drop(['target'], axis=1)\nx_test = test_preprocessed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperprameter Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_model_set = {}\n\nparam_list_rf_1 = {\n    \"n_estimators\" : [\"int\",[170,200]],\n    \"max_depth\" : [\"int\",[6,8]],\n    \"random_state\" : 1234\n}\nmodel_rf_1 = RandomForestClassifier()\ntarget_model_set[\"random_forest_1\"] = [param_list_rf_1, model_rf_1]\n\nparam_list_knn_1 = {\n    \"n_neighbors\" : [\"int\",[5,20]],\n    \"weights\" : \"distance\",\n    \"p\" : [\"int\",[1,2]],\n    \"algorithm\" : \"auto\"\n}\nmodel_knn_1 =  KNeighborsClassifier()\ntarget_model_set[\"knn_1\"] = [param_list_knn_1, model_knn_1]\n\nparam_list_rc_1 = {\n    \"alpha\" : [\"discrete_uniform\",[0.1,1.0,0.01]],\n    \"random_state\" : 1234\n}\nmodel_rc_1 =  RidgeClassifier()\ntarget_model_set[\"rc_1\"] = [param_list_rc_1, model_rc_1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#params_for_stacking, parameter_search_results = mf.parameter_search(target_model_set=target_model_set, kf_num=5, trial_num=10, x_train=x_train, y_train=y_train, x_test=x_test)\nparams_for_stacking, parameter_search_results = mf.parameter_search(target_model_set=target_model_set, kf_num=5, trial_num=2, x_train=x_train, y_train=y_train, x_test=x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- check out validation result for each model"},{"metadata":{"trusted":true},"cell_type":"code","source":"for model_name, results in parameter_search_results.items():\n    validation_result = results[1]\n    print(model_name + \" => \" + validation_result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stacking"},{"metadata":{},"cell_type":"markdown","source":"- select the high score model for stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"# params_for_stacking.pop(\"lc_1\")\nparams_for_stacking.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacking_result = mf.stacking_function(x_train=x_train, y_train=y_train, x_test=x_test, params_for_stacking=params_for_stacking, kf_num=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"result_for_submission = [int(round(result)) for result in stacking_result]\nsubmission['target'] = result_for_submission\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}