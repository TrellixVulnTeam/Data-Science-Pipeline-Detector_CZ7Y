{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nimport nltk\nfrom nltk.tokenize import word_tokenize,RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom keras.optimizers import Adam\nimport os\n\n\nfrom textblob import TextBlob\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test ve Train datalarında location verisinde birçok değer eksik. Hedef sütunumuza bir göz atalım."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train['target'].value_counts().index,train['target'].value_counts(),palette='rocket')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Afet ve afet dışı tweetlerden bir örneği inceleyelim."},{"metadata":{"trusted":true},"cell_type":"code","source":"disaster_tweets = train[train['target']==1]['text']\ndisaster_tweets.values[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"non_disaster_tweets = train[train['target']==0]['text']\nnon_disaster_tweets.values[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train ve Test verisi preprocessing**\n\ntrain ve test verisi içerisindeki @<kullanici_adi> ibareleri temizleyelim.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef remove_nick_name(text):\n    \n    return re.sub(r\"\\@\\S+\", \"\", text)\n\ntrain['text']=train['text'].apply(lambda x : remove_nick_name(x))\ntest['text']=test['text'].apply(lambda x : remove_nick_name(x))\ntrain[500:700]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train ve test verisi içerisindeki numaraları temizleyelim."},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_numbers(self):\n    result = ''.join(i for i in self if not i.isdigit())\n    return(result)\n\n\ntrain['text']=train['text'].apply(lambda x : remove_numbers(x))\ntest['text']=test['text'].apply(lambda x : remove_numbers(x))\ntrain\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train ve test verisi içerisindeki url temizleyelim."},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ntrain['text']=train['text'].apply(lambda x : remove_URL(x))\ntest['text']=test['text'].apply(lambda x : remove_URL(x))\ntest\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train ve test verisi içerisindeki html temizleyelim."},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ntrain['text']=train['text'].apply(lambda x : remove_html(x))\ntest['text']=test['text'].apply(lambda x : remove_html(x))\n\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train ve test verisi içerisindeki emojileri temizleyelim."},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ntrain['text']=train['text'].apply(lambda x: remove_emoji(x))\ntest['text']=test['text'].apply(lambda x: remove_emoji(x))\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"train ve test verisi içerisindeki noktalama işaretlerini temizleyelim."},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ntrain['text']=train['text'].apply(lambda x : remove_punct(x))\ntest['text']=test['text'].apply(lambda x : remove_punct(x))\ntrain.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train ve Test verimizdeki bütün harfleri, küçük harf yapalım."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.applymap(lambda s:s.lower() if type(s) == str else s)\ntest = test.applymap(lambda s:s.lower() if type(s) == str else s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"train = train.applymap(lambda s:s.lower() if type(s) == str else s)\ntest = test.applymap(lambda s:s.lower() if type(s) == str else s)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install textblob","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text olarak verilen tweetlerin, sentiment(olumluluk/ olumsuzluk) durumlarını inceleyip, polarity sutunu oluşturup buraya kaydeder."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef polarity_check_sayi(self):\n    polarity_list = []\n    for el in self['text']:\n        result = TextBlob(el).sentiment.polarity\n        polarity_list.append(result)\n\n    self['polarity'] = polarity_list\n    \n    \n\ndef polarity_check_label(self):\n    polarity_list = []\n    for el in self['text']:\n        result = TextBlob(el).sentiment.polarity\n        if result < 0:\n            check = 'Negative'\n        elif result == 0:\n            check = 'Neutral'\n        else:\n            check = 'Positive'\n            \n        polarity_list.append(check)\n\n    self['polarity'] = polarity_list\n    \n\n    \npolarity_check_label(test)\npolarity_check_label(train)\n\ntest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train ve Test datamızdaki  \"keyword\" ve \"locations\" sutünlarının her ikisinin de boş olduğu verileri silelim."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"train.dropna(subset=['keyword', 'location'], how='all', inplace=True)\ntest.dropna(subset=['keyword', 'location'], how='all', inplace=True)\n#test.dropna(inplace=True)\n\ntrain"},{"metadata":{},"cell_type":"markdown","source":"Veri setindeki locations sutununda en çok kullanılan değerleri görelim."},{"metadata":{"trusted":true},"cell_type":"code","source":"loc_dict={'United States':'USA','New York':'USA',\"London\":'UK',\"Los Angeles, CA\":'USA',\"Washington, D.C.\":'USA',\n          \"California\":'USA',\"Chicago, IL\":'USA',\"Chicago\":'USA',\"New York, NY\":'USA',\"California, USA\":'USA',\n          \"FLorida\":'USA',\"Nigeria\":'Africa',\"Kenya\":'Africa',\"Everywhere\":'Worldwide',\"San Francisco\":'USA',\n          \"Florida\":'USA',\"United Kingdom\":'UK',\"Los Angeles\":'USA',\"Toronto\":'Canada',\"San Francisco, CA\":'USA',\n          \"NYC\":'USA',\"Seattle\":'USA',\"Earth\":'Worldwide',\"Ireland\":'UK',\"London, England\":'UK',\"New York City\":'USA',\n          \"Texas\":'USA',\"London, UK\":'UK',\"Atlanta, GA\":'USA',\"Mumbai\":\"India\"}\n\ntrain['location'].replace(loc_dict,inplace=True)\n\n#Create barchart for top 10 locations using seaborn\nsns.barplot(y=train['location'].value_counts()[:10].index,x=train['location'].value_counts()[:10],\n            orient='h');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Location sütunumuzdaki boş olan verilere en çok kullanılan veriyi ekleyelim."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.replace(np.nan, 'usa', regex=True)\ntest = test.replace(np.nan, 'usa', regex=True)\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenization: Bir cümleyi kelimelerine ayırmaya yarar. Kelimeler bir liste oluşturur."},{"metadata":{"trusted":true},"cell_type":"code","source":"text = \"Benim adım Başak.\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\nprint(\"-\"*100)\nprint(\"Example Text: \",text)\nprint(\"-\"*100)\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"İngilizce deki anlamsız kelimeleri çıkartalım \"the gibi\""},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_stopwords(text):\n    \n\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.text.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.text.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yukarıda dizin olarak ayrılan kelimeleri pre processing sonrası tekrar bir araya getirdik."},{"metadata":{"trusted":true},"cell_type":"code","source":"def combine_text(text):\n    \n    '''\n    Input-text= list cleand and tokenized text\n    Output- Takes a list of text and returns combined one large chunk of text.\n    \n    '''\n    all_text = ' '.join(text)\n    return all_text\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vectorizer=CountVectorizer()\ntrain_cv=count_vectorizer.fit_transform(train[\"text\"])\ntest_cv=count_vectorizer.transform(test[\"text\"])\nprint(train_cv[0].todense())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf=TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2))\ntrain_tf=tfidf.fit_transform(train[\"text\"])\ntest_tf=tfidf.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Split the CountVector vectorized data into train and test datasets for model training and testing\nX_train_cv, X_test_cv, y_train_cv, y_test_cv =train_test_split(train_cv,train.target,test_size=0.2,random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_and_predict(model,X_train,y_train,X_test,y_test):\n    \n    '''Input- model=model to be trained\n              X_train, y_train= traing data set\n              X_test,  y_test = testing data set\n       Output- Print accuracy of model for training and test data sets   \n    '''\n    \n    # Fitting a simple Logistic Regression on Counts\n    clf = model\n    clf.fit(X_train, y_train)\n    predictions=clf.predict(X_test)\n    confusion_matrix(y_test,predictions)\n    print(classification_report(y_test,predictions))\n    print('-'*50)\n    print(\"{}\" .format(model))\n    print('-'*50)\n    print('Accuracy of classifier on training set:{}%'.format(round(clf.score(X_train, y_train)*100)))\n    print('-'*50)\n    print('Accuracy of classifier on test set:{}%' .format(round(accuracy_score(y_test,predictions)*100)))\n    print('-'*50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models=[LogisticRegression(C=1.0),SVC(),MultinomialNB(),DecisionTreeClassifier(),\n        KNeighborsClassifier(n_neighbors=5),RandomForestClassifier()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Loop through the list of models and use 'fit_and_predict()' function to trian and make predictions\nfor model in models:\n    fit_and_predict(model,X_train_cv, y_train_cv,X_test_cv,y_test_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the TFDIF vectorized data into train and test datasets for model training and testing\nX_train_tf, X_test_tf, y_train_tf, y_test_tf =train_test_split(train_tf,train.target,test_size=0.2,random_state=2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loop through the list of models and use 'fit_and_predict()' function to train and make predictions on the TFDIF vectororized data\nfor model in models:\n    fit_and_predict(model,X_train_tf, y_train_tf,X_test_tf,y_test_tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Printing model performance results.\nresults_dict={'Classifier':['Logistic regression', 'SVC', 'MultinomialNB', 'DecisionTreeClassifier',\n                            'KNeighborsClassifier','RandomForestClassifier'],\n              'F1-Score':[0.81, 0.40, .80, .75,0.65,0.76],'Accuracy':['81%', '56%', '80%','75%','69%','77%']} \nresults=pd.DataFrame(results_dict)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Printing model performance results.\nresults_dict={'Classifier':['Logistic regression', 'SVC', 'MultinomialNB', 'DecisionTreeClassifier',\n                            'KNeighborsClassifier','RandomForestClassifier'],\n              'F1-Score':[0.81, 0.40, .80, .75,0.65,0.76],'Accuracy':['81%', '56%', '80%','75%','69%','77%']} \nresults=pd.DataFrame(results_dict)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LogisticRegression(C=1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting 'LogisticRegression()' with CountVectorizer() fit dataset\nclf_logreg = MultinomialNB()\nclf_logreg.fit(X_train_cv, y_train_cv)\npred=clf_logreg.predict(X_test_cv)\nconfusion_matrix(y_test_cv,pred)\nprint(classification_report(y_test_cv,pred))\nprint('Accuracy of classifier on training set:{}%'.format(round(clf_logreg.score(X_train_cv, y_train_cv)*100)))\nprint('Accuracy of classifier on test set:{}%' .format(round(accuracy_score(y_test_cv,pred)*100)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_logreg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"sub_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsub_df[\"target\"] = model.predict(test_cv)\nsub_df.to_csv(\"submission.csv\", index=False)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsub_df[\"target\"] = clf_logreg.predict(test_cv)\nsub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}