{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Competition Description\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n"},{"metadata":{},"cell_type":"markdown","source":"### Data Provided\nDataset contains a set of tweets which are divided into train and test tweets. \nThe aim of this competetition is to predict whether the given tweet is of a Disaster or not. \n\nOur job is to create a ML model to predict whether the test set tweets belong to a disaster or not, in the form of 1 or 0.This is a classic case of a Binary Classification problem."},{"metadata":{},"cell_type":"markdown","source":"### Evaluation Metrics\nThe valid metric considered for this competition is[ F1](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) score. \n\nThe F score is defined as the weighted harmonic mean of the test’s precision and recall.\n![](https://imgur.com/nC4QwrO.png)\n\nWhere, \nTP - True Positive\nFP - False Positive\nTN - True Negative\nFN - False Negative"},{"metadata":{},"cell_type":"markdown","source":"### Contents:\n1.  Importing Necessary Libraries\n1.  Reading different csv files\n1.  Exploratory Data Analysis(EDAs)\n1.  Text Cleaning \n1.  Basic Models\n1.  Long Short Term Memory(LSTMs) "},{"metadata":{},"cell_type":"markdown","source":"> 1. ### **Importing Necessary Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Please write all the code with proper documentation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\nimport xgboost as xgb\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\n\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn import tree\nimport re\n# Tutorial about Python regular expressions: https://pymotw.com/2/re/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * >  ### **2. Reading CSV Files**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training data\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\nprint('Training data shape: ', train.shape)\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\nprint('Testing data shape: ', test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### **3. Exploratory Data Analysis(EDAs)**"},{"metadata":{},"cell_type":"markdown","source":"### Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing values in training set\nprint(\"Missing values in train data\")\ntrain.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing values in testing set\nprint(\"Missing values in test data\")\ntest.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Target Column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(train['target'].value_counts().index,train['target'].value_counts(),palette='rocket')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Text in Positive and negative class"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_positive=train[train['target']==1]['text']\ntrain_positive.values[120]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_positive.values[125]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_negative=train[train['target']==0]['text']\ntrain_negative.values[12]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_negative.values[11]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Keyword Column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20],orient='h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how often the word 'disaster' come in the dataset and whether this help us in determining whether a tweet belongs to a disaster category or [not.](https://www.kaggle.com/parulpandey/getting-started-with-nlp-a-general-intro)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train['text'].str.contains('disaster', na=False, case=False)].target.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Location Column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['location'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(y=train['location'].value_counts()[:20].index,x=train['location'].value_counts()[:20],orient='h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Replacing the ambigious locations name with Standard names**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the ambigious locations name with Standard names\ntrain['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                           \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\n\nsns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],orient='h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### **4. Text Cleaning and Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\n# Refer my Github for more similar examples\n# Below is a genric function which can be found at a lot of surces to clean text\nfrom bs4 import BeautifulSoup\n# https://www.kaggle.com/parulpandey/getting-started-with-nlp-a-general-intro\nimport re\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\nprint('Text after cleaning')\ntrain['text'][120]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### **Number of Words and Stop Words**\n\nLet's just check Number of Stopwords and Number of Words for fun"},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_stopwords = set(stopwords.words(\"english\"))\ntrain[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.subplot(1,2,1)\nsns.violinplot(x = 'target', y = 'num_stopwords', data = train)\nplt.subplot(1,2,2)\nsns.distplot(train[train['target'] == 1.0]['num_stopwords'][0:] , label = \"1\", color = 'red')\nsns.distplot(train[train['target'] == 0.0]['num_stopwords'][0:] , label = \"0\" , color = 'blue' )\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"num_words\"] = train[\"text\"].apply(lambda x: len(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.subplot(1,2,1)\nsns.violinplot(x = 'target', y = 'num_words', data = train[0:])\nplt.subplot(1,2,2)\nsns.distplot(train[train['target'] == 1.0]['num_words'][0:] , label = \"1\", color = 'red')\nsns.distplot(train[train['target'] == 0.0]['num_words'][0:] , label = \"0\" , color = 'blue' )\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\nwordcloud = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(train_positive))\nplt.figure(figsize = (12, 12), facecolor = None) \nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Disaster Tweets',fontsize=40);\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(train_negative))\nplt.figure(figsize = (12, 12), facecolor = None) \nplt.imshow(wordcloud)\nplt.axis('off')\nplt.title('Non Disaster Tweets',fontsize=40);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Text Processing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/a/47091490/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining all the above stundents \nfrom tqdm import tqdm\npreprocessed_reviews = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(train['text'].values):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = decontracted(sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    # https://gist.github.com/sebleier/554280\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\n    preprocessed_reviews.append(sentance.strip())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling Using Various Models and Vectorization Techniques"},{"metadata":{},"cell_type":"markdown","source":"### ** Using Bag of Words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=preprocessed_reviews[:]\ny=train['target'][:]\nX_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.30, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bow is bag of words\nbow = CountVectorizer()\ntrain_vectors = bow.fit_transform(train['text'])\ntest_vectors = bow.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)"},{"metadata":{},"cell_type":"markdown","source":"Unigram Features from BOW"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes\n# It is generally used as a benchmark for many NLP tasks\n\nclf = MultinomialNB()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Naive Bayes: ')\nprint(scores)\n\n# Fitting a simple Logistic Regression on BOW\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('Logistic Regression : ')\nprint(scores)\n# Fitting a simple Decision Trees on BOW\nclf = tree.DecisionTreeClassifier()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Decision Trees')\nprint(scores)\n# Fitting a simple Logistic Regression on Counts\nclf = RandomForestClassifier()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Random Forests')\nprint(scores)\n\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of XGBoost')\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bigram features from BOW"},{"metadata":{"trusted":true},"cell_type":"code","source":"# bow is bag of words\nbow = CountVectorizer(ngram_range=(1, 2))\ntrain_vectors = bow.fit_transform(train['text'])\ntest_vectors = bow.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes\n# It is generally used as a benchmark for many NLP tasks\n\nclf = MultinomialNB()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Naive Bayes: ')\nprint(scores)\n\n# Fitting a simple Logistic Regression on BOW\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('Logistic Regression : ')\nprint(scores)\n# Fitting a simple Decision Trees on BOW\nclf = tree.DecisionTreeClassifier()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Decision Trees')\nprint(scores)\n# Fitting a simple Logistic Regression on Counts\nclf = RandomForestClassifier()\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Random Forests')\nprint(scores)\n\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of XGBoost')\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Using TFIDF using bigram features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Bayes\n# It is generally used as a benchmark for many NLP tasks\n\nclf = MultinomialNB()\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Naive Bayes: ')\nprint(scores)\n\n# #Fitting a simple Logistic Regression on BOW\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('Logistic Regression : ')\nprint(scores)\n## Fitting a simple Decision Trees on BOW\nclf = tree.DecisionTreeClassifier()\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Decision Trees')\nprint(scores)\n# #Fitting a simple Logistic Regression on Counts\nclf = RandomForestClassifier()\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of Random Forests')\nprint(scores)\n\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nprint('scores of XGBoost')\nprint(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LSTMs"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n# fix random seed for reproducibility\nnp.random.seed(7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train['text']\ny=train['target'][:]\nX_train=X[:6500]\nX_test=X[6500:]\ny_train=y[:6500]\ny_test=y[6500:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(train['text'].apply(lambda x: len(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(train['text'].apply(lambda x: len(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\ntest=tokenizer.texts_to_sequences(test['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Zero Padding\nmax_review_length = 157\nX_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\nX_test = sequence.pad_sequences(X_test, maxlen=max_review_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = sequence.pad_sequences(test, maxlen=max_review_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train[45])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\ntop_words=10000\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=1, verbose=2, validation_data=(X_test, y_test))\n\n\n\nscore = model.evaluate(X_test, y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig,ax = plt.subplots(1,1)\n# ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# # list of epoch numbers\n# x = list(range(1,16))\n# vy = history.history['val_loss']\n# ty = history.history['loss']\n# plt_dynamic(x, vy, ty, ax)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\ntop_words=30000\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(128))\nmodel.add(Dense(1, activation='sigmoid'))\n\nprint(model.summary())\nepoch=15\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=epoch, verbose=2, validation_data=(X_test, y_test))\n\n\n\nscore = model.evaluate(X_test, y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epoch+1))\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\ntop_words=30000\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(128))\nmodel.add(Dense(1, activation='relu'))\n\nprint(model.summary())\nepoch=5\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=epoch, verbose=2, validation_data=(X_test, y_test))\n\nscore = model.evaluate(X_test, y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epoch+1))\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nfrom keras.layers import Bidirectional\nfrom keras.layers import Dropout\ntop_words=30000\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\n\nmodel.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.50))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# model.add(LSTM(128))\n# model.add(Dense(1, activation='relu'))\n\nprint(model.summary())\nepoch=5\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=epoch, verbose=2, validation_data=(X_test, y_test))\n\nscore = model.evaluate(X_test, y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epoch+1))\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the model\nfrom keras.layers import Bidirectional\nfrom keras.layers import Dropout\ntop_words=30000\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\n\nmodel.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.50))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# model.add(LSTM(128))\n# model.add(Dense(1, activation='relu'))\n\nprint(model.summary())\nepoch=5\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=64, epochs=epoch, verbose=2, validation_data=(X_test, y_test))\n\nscore = model.evaluate(X_test, y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epoch+1))\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf_nb = MultinomialNB()\n# scores = model_selection.cross_val_score(clf_nb, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\n# print('scores of Naive Bayes: ')\n# print(scores)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submissions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf_nb.fit(train_tfidf, train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.evaluate(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file_path = \"../input/nlp-getting-started/sample_submission.csv\"\nsample_submission = pd.read_csv(submission_file_path)\nsample_submission[\"target\"] = model.predict_classes(test)\nsample_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission_file_path = \"../input/nlp-getting-started/sample_submission.csv\"\n# test_vectors=test\n# submission(submission_file_path,model,test_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}