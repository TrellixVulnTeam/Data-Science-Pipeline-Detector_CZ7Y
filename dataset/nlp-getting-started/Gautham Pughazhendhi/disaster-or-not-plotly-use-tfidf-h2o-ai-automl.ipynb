{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Real or Not? NLP with Disaster Tweets\n\nThis kernel uses **Plotly** to visualize the distribution of tweets with respect to their characteristics, and it uses some simple natural language cleaning techniques to purify the tweets. I combined the **tf-idf** word-based and **Google's Universal Sentence Encoder's** 512-dimension features as it improved the model's accuracy than being individually used. Finally, I used **h20.ai's AutoML** module to fit the transformed data and was able to achieve a **F1 score** of **82.413%** on the test data. If you find this kernel useful please **upvote** and share your valuable feedback.\n\nMy other kernels (Please **upvote** if you like the implementation):\n\n* [House Sales Price Prediction](https://www.kaggle.com/gauthampughazh/house-sales-price-prediction-svr)\n* [Titanic Survival Prediction](https://www.kaggle.com/gauthampughazh/titanic-survival-prediction-pandas-plotly-keras)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport h2o\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom h2o.frame import H2OFrame\nfrom h2o.automl import H2OAutoML, get_leaderboard\nfrom IPython.display import FileLink\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_colwidth', 200)\n# nnlm_embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\nuse_embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsubmission_df = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"**Peeking the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking top 100 tweets with missing ***location*** feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df['location'].isnull()].head(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the tweets in class 0 with missing ***keyword*** feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[(train_df['target'] == 0) & (train_df['keyword'].isnull())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the tweets in class 1 with missing ***keyword*** feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[(train_df['target'] == 1) & (train_df['keyword'].isnull())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"No. of unique keywords in class 0: {train_df[(train_df['target'] == 0)]['keyword'].value_counts().shape[0]}\")\nprint(f\"No. of unique keywords in class 1: {train_df[(train_df['target'] == 1)]['keyword'].value_counts().shape[0]}\")\nprint(f\"Total no. of unique keywords: {train_df['keyword'].value_counts().shape[0]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**: Keywords present in class 1 are also present in class 0"},{"metadata":{},"cell_type":"markdown","source":"**Checking the distribution of tweets among the classes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.bar(x=['Non-disaster tweet', 'Disaster tweet'], y=train_df['target'].value_counts().values)\nfig.update_layout(title_text='Distribution of tweets', xaxis_title='Tweet type', yaxis_title='Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing the distribution of punctuations of the tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"URL_PATTERN = re.compile(r\"(https:\\/\\/\\S+)|(http:\\/\\/\\S+)|(www\\.\\S+)\")\nHTML_TAGS_PATTERN = re.compile(r'<.*>')\nALPHA_NUMERIC_PATTERN = re.compile(r\"\\w*[:,-]*(\\d+[:,-]*)+\\d*\\w*\")\nPUNCTUATION_PATTERN = re.compile(r'[^a-zA-Z ]')\nMENTIONS_PATTERN = re.compile(r'@[\\w]*')\nHASH_TAGS_PATTERN = re.compile(r'#\\S+')\nUNWANTED_WORDS_PATTERN = re.compile(r'&amp;|RT: \\S+:|RT \\S+:|FYI|CAD|RT |GMT|UTC|JST|\\s[b-zB-Z]\\s|ST|nsfw')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef get_punctuations(text):\n    \n    return PUNCTUATION_PATTERN.findall(text)\n\npunctuations = train_df.apply(lambda x: get_punctuations(x['text']), axis=1)\npunctuations = punctuations[punctuations.notnull()].explode().value_counts()\n\nfig = px.bar(x=punctuations.index, y=punctuations.values)\nfig.update_layout(title_text='Punctuation count', xaxis_title='Punctuations', yaxis_title='Count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing the distribution of unwanted words across the different classes of the tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def has_unwanted_words(text):\n    \n    return 'Tweets with unwanted words' if re.search(UNWANTED_WORDS_PATTERN, text)\\\n                                        else 'Tweets without unwanted words'\n\nhas_unwanted_words = train_df.apply(lambda x: has_unwanted_words(x['text']), axis=1).to_frame()\nhas_unwanted_words.columns = ['Unwanted words'] \nhas_unwanted_words['target'] = train_df['target']\n\npx.histogram(has_unwanted_words, x='Unwanted words', color='target', barmode='group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing the distribution of HTML across the different classes of the tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def has_html_tags(text):\n    \n    return 'Tweets with HTML tags' if re.search(HTML_TAGS_PATTERN, text) else 'Tweets without HTML tags'\n\nhas_tags = train_df.apply(lambda x: has_html_tags(x['text']), axis=1).to_frame()\nhas_tags.columns = ['HTML tags'] \nhas_tags['target'] = train_df['target']\n\npx.histogram(has_tags, x='HTML tags', color='target', barmode='group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing the distribution of URLs across the different classes of the tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def has_urls(text):\n    \n    return 'Tweets with URLs' if re.search(URL_PATTERN, text) else 'Tweets without URLs'\n\nhas_urls = train_df.apply(lambda x: has_urls(x['text']), axis=1).to_frame()\nhas_urls.columns = ['URLs'] \nhas_urls['target'] = train_df['target']\n\npx.histogram(has_urls, x='URLs', color='target', barmode='group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing the distribution of alphanumeric words across the different classes of the tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def has_alnums(text):\n    \n    return 'Tweets with Alphanumeric words' if re.search(ALPHA_NUMERIC_PATTERN, text) else 'Tweets without Alphanumeric words'\n\nhas_alnums = train_df.apply(lambda x: has_alnums(x['text']), axis=1).to_frame()\nhas_alnums.columns = ['Alnums'] \nhas_alnums['target'] = train_df['target']\n\npx.histogram(has_alnums, x='Alnums', color='target', barmode='group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing the distribution of mentions across the different classes of the tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def has_mentions(text):\n    \n    return 'Tweets with mentions' if re.search(MENTIONS_PATTERN, text) else 'Tweets without mentions'\n    \nhas_mentions = train_df.apply(lambda x: has_mentions(x['text']), axis=1).to_frame()\nhas_mentions.columns = ['Mentions'] \nhas_mentions['target'] = train_df['target']\n\npx.histogram(has_mentions, x='Mentions', color='target', barmode='group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualizing the distribution of hashtags across the different classes of the tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def has_hash_tags(text):\n    \n    return 'Tweets with hash tags' if re.search(HASH_TAGS_PATTERN, text) else 'Tweets without hash tags'\n    \nhas_hash_tags = train_df.apply(lambda x: has_hash_tags(x['text']), axis=1).to_frame()\nhas_hash_tags.columns = ['Hash tags'] \nhas_hash_tags['target'] = train_df['target']\n\npx.histogram(has_hash_tags, x='Hash tags', color='target', barmode='group')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Using a word cloud to view the top 200 most frequently occurred words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = train_df[['text']].apply(lambda x: \" \".join(x))['text'].lower()\nwordcloud = WordCloud(max_words=200, width=1000, height=600, background_color='white').generate(tweets)\n\nplt.figure(figsize=(15, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning the tweets"},{"metadata":{},"cell_type":"markdown","source":"**Replacing the contractions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: https://www.kaggle.com/prachichitnis/stack-tfidf-embedding-xgboost\n\ndef replace_contractions(tweet):\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)\n    \n    return tweet","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating a custom sklearn transformer to perform basic cleaning operations on the tweets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TweetTransformer(BaseEstimator, TransformerMixin):\n    \n    \n    def __init__(self, remove_stopwords=False):\n        \"\"\" decides whether to remove the stopwords or not\n        \"\"\"\n        self.remove_stopwords_ = remove_stopwords\n        \n    def replace_contractions(self, tweet):\n        \"\"\" replaces the contractions in the tweet\n        \"\"\"\n        \n        return replace_contractions(tweet)\n\n    def replace_urls(self, tweet):\n        \"\"\" replaces the URLs in the tweet\n        \"\"\"\n\n        return URL_PATTERN.sub('', tweet)\n    \n    def replace_unwanted_words(self, tweet):\n        \"\"\" replaces the unwanted words in the tweet\n        \"\"\"\n        \n        return UNWANTED_WORDS_PATTERN.sub(' ', tweet)\n\n    def replace_mentions(self, tweet):\n        \"\"\" replaces the mentions in the tweet\n        \"\"\"\n\n        return MENTIONS_PATTERN.sub('', tweet)\n\n    def replace_alpha_nums(self, tweet):\n        \"\"\" replaces the alphanumeric words in the tweet\n        \"\"\"\n\n        return ALPHA_NUMERIC_PATTERN.sub('', tweet)\n\n    def remove_emoji_characters(self, tweet):\n        \"\"\" removes the emoticons from the tweet \n        \"\"\"\n\n        return tweet.encode('ascii', 'ignore').decode('ascii')\n\n    def replace_punctuations(self, tweet):\n        \"\"\" replaces the punctuations in the tweet\n        \"\"\"\n\n        return PUNCTUATION_PATTERN.sub(' ', tweet)\n    \n    def modify_empty_and_rare_utterances(self, tweet):\n        \"\"\" handles empty tweets and other rare cases\n        \"\"\"\n        \n        if tweet.strip() == '':\n            tweet = 'no text'\n            \n        tweet = tweet.lower()\n        tweet = re.sub('lo+l', 'laughing out loud', tweet)\n        tweet = re.sub('coo+l', 'cool', tweet)\n        tweet = re.sub('(calif|cal)', 'california', tweet)\n        tweet = tweet.replace('rd', 'road')\n        tweet = tweet.replace('nyc', 'new york city')\n        tweet = tweet.replace('sismo', 'earthquake')\n        tweet = tweet.replace('detactado', 'detected')\n        tweet = re.sub(' +', ' ', tweet)\n        tweet = re.sub('^ +', '', tweet)\n        \n        return tweet\n    \n    def remove_stopwords(self, tweet):\n        \"\"\" removes the stopwords from the tweet\n        \"\"\"\n        \n        tokens = word_tokenize(tweet)\n        for i, token in enumerate(tokens):\n            if token in STOPWORDS:\n                del tokens[i]\n\n        return \" \".join(tokens)\n    \n    def transform_helper(self, tweet):\n        \"\"\" makes a call to the preprocessing functions\n        \"\"\"\n        \n        tweet = self.replace_contractions(tweet)\n        tweet = self.replace_urls(tweet)\n        tweet = self.replace_unwanted_words(tweet)\n        tweet = self.replace_mentions(tweet)\n        tweet = self.replace_alpha_nums(tweet)\n        tweet = self.remove_emoji_characters(tweet)\n        tweet = self.replace_punctuations(tweet)\n        tweet = self.modify_empty_and_rare_utterances(tweet)\n        \n        if self.remove_stopwords_:\n            tweet = self.remove_stopwords(tweet)\n        \n        return tweet\n    \n    def fit(self, X=None):\n        \n        return self\n    \n    def transform(self, X):\n        \"\"\" returns a pd.Series of transformed tweets\n        \"\"\"\n        \n        return X.map(lambda x: self.transform_helper(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating a custom scikit-learn transformer to generate Google's NNLM 128-dimension embeddings**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NNLMEmbeddingsTransformer(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X=None):\n        \n        return self\n    \n    def transform(self, X):\n        \n        return nnlm_embed(X.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating a custom scikit-learn transformer to generate Google's USE 512-dimension embeddings**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class USEEmbeddingsTransformer(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X=None):\n        \n        return self\n    \n    def transform(self, X):\n        \n        return use_embed(X.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating custom scikit-learn pipelines to transform the tweets**"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# Switching to USE embeddings as it helps to capture the context across a sentence rather than individual words\n\n# nnlm_embeddings_pipeline = Pipeline([\n#     ('tweet_transformer', TweetTransformer()),\n#     ('nnlm_embeddings_transformer', NNLMEmbeddingsTransformer())\n# ])\n\n# USE embeddings pipeline\nuse_embeddings_pipeline = Pipeline([\n    ('tweet_transformer', TweetTransformer()),\n    ('use_embeddings_transformer', USEEmbeddingsTransformer())\n])\n\n# tf-idf pipeline\ntfidf_pipeline = Pipeline([\n    # using remove_stopwords parameter with True for tf-idf\n    # as it doesn't take the context into account\n    \n    ('tweet_transformer', TweetTransformer(remove_stopwords=True)),\n    ('tfidf_vectorizer', TfidfVectorizer())\n])\n\n# Combining the USE and tf-idf pipelines\ndata_prep_pipeline = FeatureUnion([\n    ('use_embeddings', use_embeddings_pipeline),\n    ('tfidf', tfidf_pipeline)\n])\n\n# Transforming the training data\nX_train = data_prep_pipeline.fit_transform(train_df['text'])\n# Target labels\ny_train = train_df['target']\n# Transforming the test data\nX_test = data_prep_pipeline.transform(test_df['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the shape of the transformed data\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{},"cell_type":"markdown","source":"**Initilaize H2O**"},{"metadata":{"trusted":true},"cell_type":"code","source":"h2o.init()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Convert the transformed data into H2OFrames**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_h2o = H2OFrame(X_train.todense())\nx = train_h2o.columns\ny = 'target'\ntrain_h2o[y] = H2OFrame(y_train.values).asfactor()\nX_test_h2o = H2OFrame(X_test.todense())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the shape of the training data\n\ntrain_h2o.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Traning the models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating 10 models with a maximum traning time of 10 hours\n# and training them on the train data\n\naml = H2OAutoML(max_models=20, max_runtime_secs=18000, seed=1)\naml.train(x=x, y=y, training_frame=train_h2o)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Displaying the model results**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Displaying the leaderboard\n\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making Predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test data\n\npredictions = aml.predict(X_test_h2o)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Submission**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the submission file\n\nsubmission_df['target'] = predictions['predict'].as_data_frame().values\nsubmission_df.to_csv('submissions.csv', index=False)\nFileLink('submissions.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the submission data frame\n\nsubmission_df.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}