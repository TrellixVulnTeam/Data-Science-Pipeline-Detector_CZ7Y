{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-19T13:30:45.3331Z","iopub.execute_input":"2021-11-19T13:30:45.333813Z","iopub.status.idle":"2021-11-19T13:30:45.346317Z","shell.execute_reply.started":"2021-11-19T13:30:45.333762Z","shell.execute_reply":"2021-11-19T13:30:45.345447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is my first time to practice NLP skills through a Kaggle competition.GOOD luck!!!\n\n#import packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:30:53.261761Z","iopub.execute_input":"2021-11-19T13:30:53.262471Z","iopub.status.idle":"2021-11-19T13:30:54.406424Z","shell.execute_reply.started":"2021-11-19T13:30:53.262427Z","shell.execute_reply":"2021-11-19T13:30:54.40532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:30:58.594872Z","iopub.execute_input":"2021-11-19T13:30:58.595221Z","iopub.status.idle":"2021-11-19T13:30:58.68635Z","shell.execute_reply.started":"2021-11-19T13:30:58.595186Z","shell.execute_reply":"2021-11-19T13:30:58.685407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:31:02.49239Z","iopub.execute_input":"2021-11-19T13:31:02.492689Z","iopub.status.idle":"2021-11-19T13:31:02.523911Z","shell.execute_reply.started":"2021-11-19T13:31:02.492657Z","shell.execute_reply":"2021-11-19T13:31:02.523169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#quick look for the data \n\n#positive comment eg.\nprint(train_df[train_df[\"target\"] == 0][\"text\"].values[0])\n\n#negative comment eg.\nprint(train_df[train_df[\"target\"] == 1][\"text\"].values[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:31:08.645962Z","iopub.execute_input":"2021-11-19T13:31:08.64664Z","iopub.status.idle":"2021-11-19T13:31:08.657838Z","shell.execute_reply.started":"2021-11-19T13:31:08.646607Z","shell.execute_reply":"2021-11-19T13:31:08.657117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"count_vectorizer = feature_extraction.text.CountVectorizer()\n\n##  let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:31:16.183954Z","iopub.execute_input":"2021-11-19T13:31:16.184521Z","iopub.status.idle":"2021-11-19T13:31:16.197069Z","shell.execute_reply.started":"2021-11-19T13:31:16.184472Z","shell.execute_reply":"2021-11-19T13:31:16.195982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:33:46.383093Z","iopub.execute_input":"2021-11-19T13:33:46.383712Z","iopub.status.idle":"2021-11-19T13:33:46.390512Z","shell.execute_reply.started":"2021-11-19T13:33:46.383675Z","shell.execute_reply":"2021-11-19T13:33:46.389764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"The above tells us that:\n\nThere are 54 unique words (or \"tokens\") in the first five tweets.\nThe first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that DO exist in the first tweet.\nNow let's create vectors for all of our tweets.","metadata":{}},{"cell_type":"code","source":"train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:39:05.08501Z","iopub.execute_input":"2021-11-19T13:39:05.085304Z","iopub.status.idle":"2021-11-19T13:39:05.412485Z","shell.execute_reply.started":"2021-11-19T13:39:05.085272Z","shell.execute_reply":"2021-11-19T13:39:05.411617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model \n\n##岭回归\n##ridge regression\n\nclf = linear_model.RidgeClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:55:51.820226Z","iopub.execute_input":"2021-11-19T13:55:51.82055Z","iopub.status.idle":"2021-11-19T13:55:51.825183Z","shell.execute_reply.started":"2021-11-19T13:55:51.820516Z","shell.execute_reply":"2021-11-19T13:55:51.824303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's test our model and see how well it does on the training data. For this we'll use cross-validation - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.\n\nThe metric for this competition is F1, so let's use that here.","metadata":{}},{"cell_type":"code","source":"#交叉验证法\n#F1 - Score作为评价指标\nscores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv = 3, scoring = \"f1\")\nscores","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:57:47.330923Z","iopub.execute_input":"2021-11-19T13:57:47.331279Z","iopub.status.idle":"2021-11-19T13:57:47.854463Z","shell.execute_reply.started":"2021-11-19T13:57:47.331233Z","shell.execute_reply":"2021-11-19T13:57:47.853423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.fit(train_vectors, train_df[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:59:00.418452Z","iopub.execute_input":"2021-11-19T13:59:00.418972Z","iopub.status.idle":"2021-11-19T13:59:00.686735Z","shell.execute_reply.started":"2021-11-19T13:59:00.418936Z","shell.execute_reply":"2021-11-19T13:59:00.685605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:59:26.345163Z","iopub.execute_input":"2021-11-19T13:59:26.346018Z","iopub.status.idle":"2021-11-19T13:59:26.36363Z","shell.execute_reply.started":"2021-11-19T13:59:26.345972Z","shell.execute_reply":"2021-11-19T13:59:26.362669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission[\"target\"] = clf.predict(test_vectors)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:59:44.495874Z","iopub.execute_input":"2021-11-19T13:59:44.496217Z","iopub.status.idle":"2021-11-19T13:59:44.502961Z","shell.execute_reply.started":"2021-11-19T13:59:44.496184Z","shell.execute_reply":"2021-11-19T13:59:44.501812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T13:59:57.547056Z","iopub.execute_input":"2021-11-19T13:59:57.547372Z","iopub.status.idle":"2021-11-19T13:59:57.557916Z","shell.execute_reply.started":"2021-11-19T13:59:57.547333Z","shell.execute_reply":"2021-11-19T13:59:57.556825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T14:00:22.86106Z","iopub.execute_input":"2021-11-19T14:00:22.861387Z","iopub.status.idle":"2021-11-19T14:00:22.877933Z","shell.execute_reply.started":"2021-11-19T14:00:22.861353Z","shell.execute_reply":"2021-11-19T14:00:22.877073Z"},"trusted":true},"execution_count":null,"outputs":[]}]}