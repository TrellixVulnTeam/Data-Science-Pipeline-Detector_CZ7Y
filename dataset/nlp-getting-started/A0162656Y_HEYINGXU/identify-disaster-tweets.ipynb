{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install -U spacy[cuda101]\n!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport re\nimport json\nimport time\nimport copy\nimport datetime\nimport pandas as pd\nimport numpy as np\nfrom statistics import mean\nfrom collections import defaultdict\nfrom tqdm import tqdm_notebook\nfrom uuid import uuid4\n\n## Scipy\nfrom scipy.sparse import csr_matrix, hstack\n\n## sklearn\nfrom sklearn import svm\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.naive_bayes import MultinomialNB as MB\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n## Keras Modules\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model\nfrom keras.metrics import Precision, Recall\nfrom keras.layers import Dense, LSTM, GRU, Bidirectional, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.regularizers import l1, l2, l1_l2\nfrom keras import backend as K\n\nimport tensorflow as tf\n\n## Torch Modules\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\n## PyTorch Transformer\nfrom transformers import RobertaModel, RobertaTokenizer\nfrom transformers import RobertaForSequenceClassification, RobertaConfig, AdamW, get_linear_schedule_with_warmup\n\nimport spacy\n\n# spacy.prefer_gpu()\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer as wn_lemmatizer\nfrom nltk.tokenize import word_tokenize, wordpunct_tokenize\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet as wn\n\nnltk.download('stopwords')\nstop_words = stopwords.words('english')\n# !python -m spacy download en\nnlp = spacy.load('en_core_web_sm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This notebook compared machine learning models and deep learning models on their abilities to classify texts. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Check if Cuda is Available\nprint(torch.cuda.is_available())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_STATE = 1234\n\ntqdm_notebook().pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 0. Utility Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def stratified_cv(classifier, X_train, y_train, **kwargs):\n    \"\"\"\n    Calculates the stratefied cross-validation scores with preserving the percentage of samples for each class in each fold.\n    \"\"\"\n    skf = StratifiedKFold(**kwargs)\n    total_accuracy = []\n    total_precision = []\n    total_recall = []\n    total_f1 = []\n    for train_index, val_index in skf.split(X_train, y_train):\n        current_X_train = X_train[train_index]\n        current_y_train = y_train.iloc[train_index]\n        current_X_val = X_train[val_index]\n        current_y_val = y_train.iloc[val_index]\n\n        clf = copy.deepcopy(classifier)\n        clf.fit(current_X_train, current_y_train)\n\n        current_predictions = clf.predict(current_X_val)\n        total_accuracy.append(accuracy_score(current_y_val, current_predictions))\n        total_precision.append(precision_score(current_y_val, current_predictions))\n        total_recall.append(recall_score(current_y_val, current_predictions))\n        total_f1.append(f1_score(current_y_val, current_predictions))\n        \n    ave_accuracy = mean(total_accuracy)\n    ave_precision = mean(total_precision)\n    ave_recall = mean(total_recall)\n    ave_f1 = mean(total_f1)\n    \n    print(\"Average Accuracy: {:.4f}\".format(ave_accuracy))\n    print(\"Average Precision: {:.4f}\".format(ave_precision))\n    print(\"Average Recall: {:.4f}\".format(ave_recall))\n    print(\"Average F1: {:.4f}\".format(ave_f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    # remove special characters, url links and numbers from the original text.\n    \n    # Special charactes\n    text = re.sub(r\"\\x89Û_\", \"\", text)\n    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n    text = re.sub(r\"\\x89Û÷\", \"\", text)\n    text = re.sub(r\"\\x89Ûª\", \"\", text)\n    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n    text = re.sub(r\"å_\", \"\", text)\n    text = re.sub(r\"\\x89Û¢\", \"\", text)\n    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n    text = re.sub(r\"åÊ\", \"\", text)\n    text = re.sub(r\"åÈ\", \"\", text)\n    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n    text = re.sub(r\"Ì©\", \"e\", text)\n    text = re.sub(r\"å¨\", \"\", text)\n    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n    text = re.sub(r\"åÇ\", \"\", text)\n    text = re.sub(r\"å£3million\", \"3 million\", text)\n    text = re.sub(r\"åÀ\", \"\", text)\n    \n    # remove url link\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    \n    # remove html tag\n    text = re.sub(r'<.*?>', '', text)\n    \n    # remove numbers\n    text = re.sub(r'[\\d]+', ' ', text)\n    \n    return text\n\ndef preprocess(text, allowed_tags=[\"N\", \"J\", \"R\", \"V\"]):\n    # lemmatization and remove stop words \n    tag_map = defaultdict(lambda : wn.NOUN)\n    tag_map['N'] = wn.NOUN\n    tag_map['J'] = wn.ADJ\n    tag_map['R'] = wn.ADV\n    tag_map['V'] = wn.VERB\n#     tag_map['D'] = wn.DET\n    \n    text = text.lower()\n\n    tokens = [token for token in wordpunct_tokenize(text)]\n\n    lemmatizer = wn_lemmatizer()\n\n    lemmas = [lemmatizer.lemmatize(token, tag_map[tag[0]]) for token, tag in pos_tag(tokens) if tag[0] in allowed_tags]\n\n    lemmas = [lemma for lemma in lemmas if lemma not in stop_words and lemma.isalpha()]\n\n    return ' '.join(lemmas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def array_equal(array_1, array_2):\n    \"\"\"\n    custom function to compare two 1D np arrays.\n    \"\"\"\n    if array_1.shape != array_2.shape:\n        return False\n    \n    for i in range(len(array_1)):\n        if (array_1[i] is array_2[i]) or (array_1[i] == array_2[i]):\n            continue \n        return False\n    return True\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Cleaning","execution_count":null},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assert the keyword sets are equal in train and test data.\nassert array_equal(data[\"keyword\"].unique(), test[\"keyword\"].unique()), \"Keywords in train and test data are not equal\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove controversial tweets\nunique_targets = data.groupby('text').agg(unique_target=('target', pd.Series.nunique))\ncontroversial_tweets = unique_targets[unique_targets['unique_target'] > 1].index\n\ndata = data[~data['text'].isin(controversial_tweets)]\n\n# remove duplicates rows\ndata = data.drop_duplicates(subset='text', keep='first')\n\n# remove special characters, url, and html tags\ndata['text'] = data['text'].apply(clean_text) \ntest['text'] = test['text'].apply(clean_text)\n\n# convert the keywords to feature maps\ndata['value'] = 1\ndata['keyword'] = data['keyword'].fillna(\"nan\")\n\ntest['value'] = 1\ntest['keyword'] = test['keyword'].fillna(\"nan\")\n\ndata = pd.pivot_table(data, values=\"value\", index=[\"id\", \"text\", \"target\"], columns=[\"keyword\"]).fillna(0).reset_index()\ntest = pd.pivot_table(test, values=\"value\", index=[\"id\", \"text\"], columns=[\"keyword\"]).fillna(0).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split data into train and test set\nTRAIN_VAL = 0.8\ntrain = data.sample(frac=TRAIN_VAL, random_state=RANDOM_STATE)\nval = data.drop(train.index).reset_index(drop=True)\ntrain = train.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pruned = train.copy(deep=True)\ntrain_pruned['text'] = train_pruned['text'].progress_apply(preprocess)\n\nval_pruned = val.copy(deep=True)\nval_pruned['text'] = val_pruned['text'].progress_apply(preprocess)\n\ntest_pruned = test.copy(deep=True)\ntest_pruned['text'] = test_pruned['text'].progress_apply(preprocess)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Tf-idf matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = pd.concat([train_pruned.drop(['target'], axis=1), val_pruned.drop(['target'], axis=1), test_pruned], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# exploring uni-gram and bi-gram features \ncv = CountVectorizer(ngram_range=(1,2))\ntfidf_transformer = TfidfTransformer()\ncounts = cv.fit_transform(all_text['text'])\ntfidf = tfidf_transformer.fit_transform(counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augmented = hstack([tfidf, csr_matrix(all_text.iloc[:, 3:])]).tocsr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = train_pruned.shape[0]\nval_size = val_pruned.shape[0]\n\nX_train_tfidf = tfidf[:train_size,:]\nX_train_augmented = augmented[:train_size,:]\n\nX_val_tfidf = tfidf[train_size: val_size + train_size,:]\nX_val_augmented = augmented[train_size: val_size + train_size,:]\n\nX_test_tfidf = tfidf[val_size + train_size:,:]\nX_test_augmented = augmented[val_size + train_size:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_tfidf = train_pruned['target']\ny_val_tfidf = val_pruned['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Keyword feature map","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_keyword = csr_matrix(train_pruned.iloc[:, 3:]).tocsr()\nX_val_keyword = csr_matrix(val_pruned.iloc[:, 3:]).tocsr()\nX_test_keyword = csr_matrix(test_pruned.iloc[:, 2:]).tocsr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Naive Bayse","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"mb_clf = MB()\nmb_clf.fit(X_train_tfidf, y_train_tfidf)\n\npred = mb_clf.predict(X_train_tfidf)\nprint(classification_report(y_train_tfidf, pred))\n\nstratified_cv(mb_clf, X_train_tfidf, y_train_tfidf, random_state=RANDOM_STATE, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mb_clf_augmented = MB()\nmb_clf_augmented.fit(X_train_augmented, y_train_tfidf)\n\npred = mb_clf_augmented.predict(X_train_augmented)\nprint(classification_report(y_train_tfidf,pred))\n\nstratified_cv(mb_clf_augmented, X_train_augmented, y_train_tfidf, random_state=RANDOM_STATE, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mb_clf_keyword = MB()\nmb_clf_keyword.fit(X_train_keyword, y_train_tfidf)\n\npred = mb_clf_keyword.predict(X_train_keyword)\nprint(classification_report(y_train_tfidf,pred))\n\nstratified_cv(mb_clf_keyword, X_train_keyword, y_train_tfidf, random_state=RANDOM_STATE, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: try sub-sampling features to prevent overfitting. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_pruned['target'] = mb_clf_augmented.predict(X_test_augmented)\n# test_pruned[['id', 'target']].to_csv(\"submission_2.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n\nsvm_clf.fit(X_train_tfidf, y_train_tfidf)\npred = svm_clf.predict(X_train_tfidf)\nprint(classification_report(y_train_tfidf,pred))\n\nstratified_cv(svm_clf, X_train_tfidf, y_train_tfidf, random_state=RANDOM_STATE, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf_augmented = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n\nsvm_clf_augmented.fit(X_train_augmented, y_train_tfidf)\npred = svm_clf_augmented.predict(X_train_augmented)\nprint(classification_report(y_train_tfidf,pred))\n\nstratified_cv(svm_clf_augmented, X_train_augmented, y_train_tfidf, random_state=RANDOM_STATE, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf_keyword = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n\nsvm_clf_keyword.fit(X_train_keyword, y_train_tfidf)\npred = svm_clf_keyword.predict(X_train_keyword)\nprint(classification_report(y_train_tfidf,pred))\n\nstratified_cv(svm_clf_keyword, X_train_keyword, y_train_tfidf, random_state=RANDOM_STATE, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_pruned['target'] = svm_clf_augmented.predict(X_test_augmented)\n# test_pruned[['id', 'target']].to_csv(\"submission_svm.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_train_preds = svm_clf_keyword.predict(X_train_keyword)\nsvm_val_preds = svm_clf_keyword.predict(X_val_keyword)\nsvm_test_preds = svm_clf_keyword.predict(X_test_keyword)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators = 100, random_state = RANDOM_STATE)\nrf_clf.fit(X_train_tfidf, y_train_tfidf)\nrf_preds = rf_clf.predict(X_train_tfidf)\n\nprint(classification_report(y_train_tfidf, rf_preds))\n\nstratified_cv(rf_clf, X_train_tfidf, y_train_tfidf, random_state=RANDOM_STATE, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf_augmented = RandomForestClassifier(n_estimators = 100, random_state = RANDOM_STATE)\nrf_clf_augmented.fit(X_train_augmented, y_train_tfidf)\nrf_preds_augmented = rf_clf_augmented.predict(X_train_augmented)\n\nprint(classification_report(y_train_tfidf, rf_preds))\n\nstratified_cv(rf_clf_augmented, X_train_augmented, y_train_tfidf, random_state=RANDOM_STATE, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_train_preds = rf_clf_augmented.predict(X_train_augmented)\nrf_val_preds = rf_clf_augmented.predict(X_val_augmented)\nrf_test_preds = rf_clf_augmented.predict(X_test_augmented)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_clf = LogisticRegression(fit_intercept=True, \n                              random_state=RANDOM_STATE, \n                              solver = 'liblinear',\n                              penalty = 'l1',\n                              max_iter = 200)\nlr_clf.fit(X_train_tfidf, y_train_tfidf)\nlr_preds = lr_clf.predict(X_train_tfidf)\n\nprint(classification_report(y_train_tfidf, lr_preds))\n\nstratified_cv(lr_clf, X_train_tfidf, y_train_tfidf, random_state=RANDOM_STATE, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_clf = LogisticRegression(fit_intercept=True, \n                              random_state=RANDOM_STATE, \n                              solver = 'liblinear',\n                              penalty = 'l1',\n                              max_iter = 200)\nlr_clf.fit(X_train_augmented, y_train_tfidf)\nlr_preds = lr_clf.predict(X_train_augmented)\n\nprint(classification_report(y_train_tfidf, lr_preds))\n\nstratified_cv(lr_clf, X_train_augmented, y_train_tfidf, random_state=RANDOM_STATE, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. TextCNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# prevent tensorflow from using GPU to save memory for roBERTa\ntf.config.set_visible_devices([], 'GPU')\nvisible_devices = tf.config.get_visible_devices()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corpus = train_pruned['text'].apply(lambda x: x.split()).to_list()\nval_corpus = val_pruned['text'].apply(lambda x: x.split()).to_list()\ntest_corpus = test_pruned['text'].apply(lambda x: x.split()).to_list()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = train_corpus + val_corpus + test_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\n\ntrain_sequences = tokenizer.texts_to_sequences(train_corpus)\nval_sequences = tokenizer.texts_to_sequences(val_corpus)\ntest_sequences = tokenizer.texts_to_sequences(test_corpus)\n\n# the dictionary of word occurrences.\nword_index = tokenizer.word_index\n\ntrain_max_length = max([len(x) for x in train_sequences])\nval_max_length = max([len(x) for x in val_sequences])\ntest_max_length = max([len(x) for x in test_sequences])\n\nmax_length = max(train_max_length, val_max_length, test_max_length)\n\nX_train_pad = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\")\ny_train = train_pruned['target']\n\nX_val_pad = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\")\ny_val = val_pruned['target']\n\nX_test_pad = pad_sequences(test_sequences, maxlen=max_length, padding=\"post\")\n\nvocab = np.array(list(tokenizer.word_index.keys()))\nvocab_size = len(tokenizer.word_index) + 1","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_pad.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ConvNet(max_sequence_length, num_words, embedding_dim, labels_index):\n \n    embedding_layer = Embedding(num_words,\n                            embedding_dim,\n                            input_length=max_sequence_length)\n    \n    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n    print()\n    convs = []\n    filter_sizes = [2, 3, 4, 5, 6]\n    for filter_size in filter_sizes:\n        l_conv = Conv1D(filters=30, \n                        kernel_size=filter_size, \n                        activation='relu')(embedded_sequences)\n        l_conv = Dropout(0.2)(l_conv)\n        l_pool = GlobalMaxPooling1D()(l_conv)\n        convs.append(l_pool)\n    l_merge = concatenate(convs, axis=1)\n#     x = Dropout(0.2)(l_merge)  \n    x = Dense(30, activation='relu')(l_merge)\n    x = Dropout(0.2)(x)\n    preds = Dense(labels_index, activation='sigmoid', kernel_regularizer=l1_l2(0.01, 0.01))(x)\n    model = Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc', Precision(), Recall()])\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model = ConvNet(max_length, vocab_size, EMBEDDING_DIM, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hist = cnn_model.fit(X_train_pad, \n                 y_train, \n                 epochs=10, \n                 batch_size=64, \n                 validation_data=(X_val_pad, y_val), \n                 verbose=2) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_test_preds = cnn_model.predict(X_test_pad)\ncnn_test_preds_binary = list(map(lambda x: 1 if x >= 0.5 else 0, cnn_test_preds))\n\ncnn_train_preds = cnn_model.predict(X_train_pad)\ncnn_train_preds_binary = list(map(lambda x: 1 if x >= 0.5 else 0, cnn_train_preds))\n\ncnn_val_preds = cnn_model.predict(X_val_pad)\ncnn_val_preds_binary = list(map(lambda x: 1 if x >= 0.5 else 0, cnn_val_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test['target'] = cnn_test_preds_binary\n# test[['id', 'target']].to_csv(\"submission_cnn.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Pretrained Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The model is trained on a combination of 5 english datasets, totaling over 160 GB. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"GPU is available: {}\".format(torch.cuda.is_available()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base') \nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base').to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_features(data_set, labels=None, max_seq_length = 200, \n                     zero_pad = True, include_special_tokens = True): \n    \n    ## Tokenzine Input\n    input_ids = []\n    attention_masks = []\n    \n    for sent in data_set:\n        encoded_dict = tokenizer.encode_plus(\n                    sent,                      # Sentence to encode.\n                    add_special_tokens = include_special_tokens, # Add '[CLS]' and '[SEP]'\n                    max_length = max_seq_length,           # Max length according to our text data.\n                    pad_to_max_length = zero_pad, # Pad & truncate all sentences.\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',     # Return pytorch tensors.\n               )\n    \n        # Add the encoded sentence to the id list. \n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks.append(encoded_dict['attention_mask'])\n    \n    # convert the lists into tensors\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    \n    if labels is not None: \n        labels = torch.tensor(labels)\n        return input_ids, attention_masks, labels\n    else: \n        return input_ids, attention_masks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nLEARNING_RATE = 1e-05\nEPSILON = 1e-8\nMAX_EPOCHS = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the train and validation set. \ntrain_val = 0.8\ntrain = data.sample(frac=train_val, random_state=RANDOM_STATE)\nval = data.drop(train.index).reset_index(drop=True)\ntrain = train.reset_index(drop=True)\n\n# generate the input sequences.\ntrain_input_ids, train_attention_masks, train_labels = prepare_features(\n    train['text'], train['target'])\nval_input_ids, val_attention_masks, val_labels = prepare_features(\n    val['text'], val['target'])\ntest_input_ids, test_attention_masks = prepare_features(\n    test['text'])\n\n# Convert the data into torch tensor set.\ntraining_set = TensorDataset(train_input_ids, train_attention_masks, train_labels)\nvalidation_set = TensorDataset(val_input_ids, val_attention_masks, val_labels)\ntest_set = TensorDataset(test_input_ids, test_attention_masks)\n\n# Prepare the train & validation data loader.\nloading_params = {'batch_size': BATCH_SIZE,\n          'shuffle': True,\n          'drop_last': False,\n          'num_workers': 1}\n\nloading_params_no_shuffle = {'batch_size': BATCH_SIZE,\n          'shuffle': False,\n          'drop_last': False,\n          'num_workers': 1}\n\ntraining_loader = DataLoader(training_set, **loading_params)\nvalidation_loader = DataLoader(validation_set, **loading_params)\n\ntraining_loader_no_shuffle = DataLoader(training_set, **loading_params_no_shuffle)\nvalidation_loader_no_shuffle = DataLoader(validation_set, **loading_params_no_shuffle)\n\n\n# Prepare the test data loader. The data loader will not be shuffled. \ntesting_loader = DataLoader(test_set, **loading_params_no_shuffle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/datafan07/disaster-tweets-nlp-eda-bert-with-transformers\n\n# loss_function = nn.CrossEntropyLoss()\n\noptimizer = AdamW(model.parameters(),\n                  lr = LEARNING_RATE, # args.learning_rate\n                  eps = EPSILON # args.adam_epsilon\n                )\n\n# number of training steps\ntotal_steps = len(training_loader) * MAX_EPOCHS\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_time(elapsed):    \n    \"\"\"A function that takes a time in seconds and returns a string hh:mm:ss\"\"\"\n    \n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model = model.train()\n\nfor epoch in tqdm_notebook(range(MAX_EPOCHS)):\n    # start time for each epoch\n    t0 = time.time()\n    \n    total_train_loss = 0\n    \n    model.train()\n    \n    print(\"EPOCH -- {} / {}\".format(epoch, MAX_EPOCHS))\n    for step, batch in enumerate(training_loader):\n        if step % 30 == 0 and not step == 0: \n            elapsed = format_time(time.time() - t0)\n            print(' Batch {} of {}. Elapsed: {:}'.format(step, len(training_loader), elapsed))\n            \n        input_ids = batch[0].to(device).to(torch.int64)\n        input_masks = batch[1].to(device).to(torch.int64)\n        labels = batch[2].to(device).to(torch.int64)          \n                  \n        # Always clear any previously calculated gradients before performing a backward pass. PyTorch doesn't do this automatically because accumulating the gradients is 'convenient while training RNNs'. \n        model.zero_grad()\n                  \n        loss, logits = model(input_ids, \n                           token_type_ids=None,\n                           attention_mask=input_masks, \n                           labels=labels)\n                  \n        logits = logits.detach().cpu().numpy()\n        label_ids = labels.to('cpu').numpy()\n        \n        total_train_loss += loss.item()\n        loss.backward()\n                  \n        # Clip the norm of the gradients to 1.0. This is to help \n        # prevent the 'exploding gradients' problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # update parameters and move a step forward using the computed gradients          \n        optimizer.step()\n        scheduler.step()\n        \n    avg_train_loss = total_train_loss / len(training_loader)\n    training_time = format_time(time.time() - t0)\n            \n    print('')\n    print(' Average training loss: {0:.4f}'.format(avg_train_loss))\n    print(' Training epoch took: {:}'.format(training_time))\n    \n    print('Running Validation')\n                  \n    model.eval()\n        \n    val_predictions = []\n    val_labels = []\n    for batch in validation_loader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        with torch.no_grad():\n            loss, logits = model(b_input_ids, \n                       token_type_ids=None, \n                       attention_mask=b_input_mask,\n                       labels=b_labels)\n            \n        val_predictions.append(logits.detach().cpu().numpy())\n        val_labels.append(b_labels.to('cpu').numpy())\n\n    val_predictions = np.array([item for sublist in val_predictions for item in sublist])\n    val_labels = np.array([item for sublist in val_labels for item in sublist])\n    \n    val_predictions_flat = np.argmax(val_predictions, axis=1)\n    print(val_predictions_flat.shape)\n        \n    val_accuracy = accuracy_score(val_labels, val_predictions_flat)\n    val_recall = recall_score(val_labels, val_predictions_flat)\n    val_precision = precision_score(val_labels, val_predictions_flat)\n    val_f1 = f1_score(val_labels, val_predictions_flat)\n    \n    print('\\tAccuracy: {0:.4f}\\n\\tRecall: {1:.4f}\\n\\tPrecision: {2:.4f}\\n\\tF1_score: {3:.4f}\\n\\t'.\n          format(val_accuracy, val_recall, val_precision, val_f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\n\n# get prediction for training set.\nroberta_train_preds = []\nfor batch in training_loader_no_shuffle:\n    b_input_ids = batch[0].to(device)\n    b_input_mask = batch[1].to(device)\n    b_labels = batch[2].to(device)\n    \n    with torch.no_grad():\n        loss, logits = model(b_input_ids, \n                       token_type_ids=None, \n                       attention_mask=b_input_mask,\n                       labels=b_labels)\n        \n        roberta_train_preds.append(logits.detach().cpu().numpy())\nroberta_train_preds = np.array([item for sublist in roberta_train_preds for item in sublist])\n\n# get prediction for validation set.\nroberta_val_preds = []\nfor batch in validation_loader_no_shuffle:\n    b_input_ids = batch[0].to(device)\n    b_input_mask = batch[1].to(device)\n    b_labels = batch[2].to(device)\n    \n    with torch.no_grad():\n        loss, logits = model(b_input_ids, \n                       token_type_ids=None, \n                       attention_mask=b_input_mask,\n                       labels=b_labels)\n        \n        roberta_val_preds.append(logits.detach().cpu().numpy())\nroberta_val_preds = np.array([item for sublist in roberta_val_preds for item in sublist])\n\n# get prediction for training set.\nroberta_test_preds = []\nfor batch in testing_loader:\n    b_input_ids = batch[0].to(device)\n    b_input_mask = batch[1].to(device)\n    \n    with torch.no_grad():\n        logits = model(b_input_ids, \n                       token_type_ids=None, \n                       attention_mask=b_input_mask)[0]\n        \n        roberta_test_preds.append(logits.detach().cpu().numpy())\nroberta_test_preds = np.array([item for sublist in roberta_test_preds for item in sublist])\n\n# convert into dataframe\nroberta_train_preds_binary = np.argmax(roberta_train_preds, axis=1)\nroberta_val_preds_binary = np.argmax(roberta_val_preds, axis=1)\nroberta_test_preds_binary = np.argmax(roberta_test_preds, axis=1)\n\nroberta_train_preds = pd.DataFrame(roberta_train_preds, columns=[\"roberta_fake\",\"roberta_real\"])\nroberta_val_preds = pd.DataFrame(roberta_val_preds, columns=[\"roberta_fake\",\"roberta_real\"])\nroberta_test_preds = pd.DataFrame(roberta_test_preds, columns=[\"roberta_fake\",\"roberta_real\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test[['id', 'target']].to_csv(\"submission_roberta.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 6. Stacking Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stacking = train.loc[:, [\"target\"]]\nval_stacking = val.loc[:, [\"target\"]]\ntest_stacking = test.loc[:, []]\n\n\ntrain_stacking[\"svm_predicted\"] = svm_train_preds\n# train_stacking[\"rf_predicted\"] = rf_train_preds\n# train_stacking[\"cnn_predicted\"] = cnn_train_preds_binary\n# train_stacking[\"roberta_predicted\"] = roberta_train_preds_binary\ntrain_stacking = pd.concat([train_stacking, roberta_train_preds], axis=1)\nX_train_stacking = train_stacking.drop([\"target\"], axis=1)\ny_train_stacking = train_stacking[\"target\"]\n\nval_stacking[\"svm_predicted\"] = svm_val_preds\n# val_stacking[\"rf_predicted\"] = rf_val_preds\n# val_stacking[\"cnn_predicted\"] = cnn_val_preds_binary\n# val_stacking[\"roberta_predicted\"] = roberta_val_preds_binary\nval_stacking = pd.concat([val_stacking, roberta_val_preds], axis=1)\nX_val_stacking = val_stacking.drop([\"target\"], axis=1)\ny_val_stacking = val_stacking[\"target\"]\n\ntest_stacking[\"svm_predicted\"] = svm_test_preds\n# test_stacking[\"rf_predicted\"] = rf_test_preds\n# test_stacking[\"cnn_predicted\"] = cnn_test_preds_binary\n# test_stacking[\"roberta_predicted\"] = roberta_test_preds_binary\ntest_stacking = pd.concat([test_stacking, roberta_test_preds], axis=1)\nX_test_stacking = test_stacking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_stacking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_clf_stacking = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n\nsvm_clf_stacking.fit(X_train_stacking, y_train_stacking)\npred = svm_clf_stacking.predict(X_val_stacking)\nprint(f1_score(y_val_stacking,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_clf_stacking = LogisticRegression(fit_intercept=True, \n                              random_state=RANDOM_STATE, \n                              solver = 'liblinear',\n                              penalty = 'l1',\n                              max_iter = 200)\nlr_clf_stacking.fit(X_train_stacking, y_train_stacking)\nlr_preds = lr_clf_stacking.predict(X_train_stacking)\n\nprint(accuracy_score(y_train_stacking,lr_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier(n_estimators = 100, max_depth=2, random_state = RANDOM_STATE)\nrf_clf.fit(X_train_stacking, y_train_stacking)\nrf_preds = rf_clf.predict(X_train_stacking)\n\nprint(accuracy_score(y_train_stacking, rf_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"target\"] = lr_clf_stacking.predict(X_test_stacking)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[['id', 'target']].to_csv(\"submission_stacking.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}