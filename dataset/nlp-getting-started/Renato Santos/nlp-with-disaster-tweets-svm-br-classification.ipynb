{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ntrain = pd.read_csv('../input/nlp-getting-started/train.csv')\ntrain.drop('location',axis=1,inplace=True)\ntrain.drop('keyword',axis=1,inplace=True)\n#train.shape\ntrain.head()\n\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\ntest.drop('location',axis=1,inplace=True)\ntest.drop('keyword',axis=1,inplace=True)\n#test.shape\n#test.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-10T17:05:57.372091Z","iopub.execute_input":"2022-03-10T17:05:57.372392Z","iopub.status.idle":"2022-03-10T17:05:57.41754Z","shell.execute_reply.started":"2022-03-10T17:05:57.372362Z","shell.execute_reply":"2022-03-10T17:05:57.417049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re\n\nimport emoji\nimport unicodedata\nfrom nltk.corpus import wordnet\nfrom emoji.unicode_codes import UNICODE_EMOJI\nfrom textblob import TextBlob, Word\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('omw')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-03-10T17:05:57.418653Z","iopub.execute_input":"2022-03-10T17:05:57.418905Z","iopub.status.idle":"2022-03-10T17:05:57.427034Z","shell.execute_reply.started":"2022-03-10T17:05:57.418882Z","shell.execute_reply":"2022-03-10T17:05:57.426336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clearText(text):\n  try:\n    # remove URLs\n    text = re.sub('https?://[A-Za-z0-9./?&=_]+','',text)\n    # hashtags\n    text = re.sub('#[A-Za-z0-9]+','',text)\n    # mentions\n    text = re.sub('@[A-Za-z0-9._-]+','',text)\n    # to lower\n    text = text.lower()\n    # remove pontuation\n    text = re.sub(r\"[^\\w\\s]\",\"\",text)\n    #remove white spaces\n    text = \" \".join(text.strip().split())\n    text = re.sub(r\"[\\W\\s]\",\" \",text)\n    text = re.sub(\"\\n\",\"\",text)\n  except Exception as e:\n    print(\"clearText error - \", e)\n\n  return text\n\ndef pos_tagger(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    else:          \n        return None\n\ndef stemmingText(text):\n  # get adj, verb, nouns\n  textWords = word_tokenize(text)\n  pos_tagged = nltk.pos_tag(textWords)\n  \n  wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n\n  # stemming\n  stemmer = nltk.stem.SnowballStemmer('english') \n  stemSentence = \"\"\n  for word, tag in wordnet_tagged:\n    if (tag is not None):\n      stem = stemmer.stem(word)\n      stemSentence+=stem\n      stemSentence += \" \"\n  stemSentence = stemSentence.strip()\n  \n  # remove stop words\n  words = word_tokenize(stemSentence)\n  stopwords = nltk.corpus.stopwords.words('english')\n  pals = [word for word in words if not word in stopwords] \n  text = \" \".join(pals)\n  return text","metadata":{"execution":{"iopub.status.busy":"2022-03-10T17:05:57.428232Z","iopub.execute_input":"2022-03-10T17:05:57.428602Z","iopub.status.idle":"2022-03-10T17:05:57.447993Z","shell.execute_reply.started":"2022-03-10T17:05:57.428568Z","shell.execute_reply":"2022-03-10T17:05:57.446473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def runPreprocessing(text):\n  text = clearText(text)\n  text = stemmingText(str(text))\n  return text","metadata":{"execution":{"iopub.status.busy":"2022-03-10T17:05:57.450086Z","iopub.execute_input":"2022-03-10T17:05:57.450542Z","iopub.status.idle":"2022-03-10T17:05:57.471924Z","shell.execute_reply.started":"2022-03-10T17:05:57.450503Z","shell.execute_reply":"2022-03-10T17:05:57.471202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntrain_text = train['text']\ntest_text = test['text']\n\nvectorizer = TfidfVectorizer(preprocessor=runPreprocessing)\n\nx_train = vectorizer.fit_transform(train_text)\nx_test = vectorizer.transform(test_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T17:05:57.474871Z","iopub.execute_input":"2022-03-10T17:05:57.475235Z","iopub.status.idle":"2022-03-10T17:06:24.2746Z","shell.execute_reply.started":"2022-03-10T17:05:57.475201Z","shell.execute_reply":"2022-03-10T17:06:24.273006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.svm import LinearSVC\n\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Binarizer\n\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2022-03-10T17:11:52.922822Z","iopub.execute_input":"2022-03-10T17:11:52.92346Z","iopub.status.idle":"2022-03-10T17:11:52.930035Z","shell.execute_reply.started":"2022-03-10T17:11:52.923433Z","shell.execute_reply":"2022-03-10T17:11:52.929128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = Binarizer() \n\nx = x_train\ny = train['target']\n\n#balance dataset with Synthetic minority oversampling\nsmote = SMOTE()\nx_smote, y_smote = smote.fit_resample(x, y)\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=456)\n\n# Grid Search CV to find optimal hyperparameters\ngrid={\"C\":np.logspace(-2,2,5), \"penalty\":[\"l1\",\"l2\"]}\nsvm_cv = GridSearchCV(LinearSVC(), grid, cv=cv, scoring = 'f1')\n\npipeline = Pipeline([('scale',scaler), ('gridsearch', svm_cv)])\n\npipeline.fit(x_smote, y_smote)\n\ny_pred = pipeline.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T17:11:54.587531Z","iopub.execute_input":"2022-03-10T17:11:54.588792Z","iopub.status.idle":"2022-03-10T17:11:59.40965Z","shell.execute_reply.started":"2022-03-10T17:11:54.588726Z","shell.execute_reply":"2022-03-10T17:11:59.408548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv(\"../input/nlp-getting-started/test.csv\")\nsubmission.drop([\"keyword\"],axis=1,inplace=True)\nsubmission.drop([\"location\"],axis=1,inplace=True)\nsubmission.drop([\"text\"],axis=1,inplace=True)\n\ntarget_pred = pd.DataFrame(y_pred,columns=[\"target\"])\n\nsubmission = pd.concat([submission,target_pred],axis=1)\nsubmission ","metadata":{"execution":{"iopub.status.busy":"2022-03-10T17:12:01.814353Z","iopub.execute_input":"2022-03-10T17:12:01.81469Z","iopub.status.idle":"2022-03-10T17:12:01.848573Z","shell.execute_reply.started":"2022-03-10T17:12:01.814651Z","shell.execute_reply":"2022-03-10T17:12:01.847347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T17:12:04.317232Z","iopub.execute_input":"2022-03-10T17:12:04.317512Z","iopub.status.idle":"2022-03-10T17:12:04.329503Z","shell.execute_reply.started":"2022-03-10T17:12:04.317483Z","shell.execute_reply":"2022-03-10T17:12:04.328118Z"},"trusted":true},"execution_count":null,"outputs":[]}]}