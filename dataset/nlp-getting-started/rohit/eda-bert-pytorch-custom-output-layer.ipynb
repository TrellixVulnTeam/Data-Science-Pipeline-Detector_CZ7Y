{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport os\n\n\n# pytorch: helps us make model\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom torch.utils.data import TensorDataset\nimport torch.nn.functional as F\nimport torch.nn as nn\n\n# sklearn: help to genrate predicitve report\nfrom sklearn.metrics import classification_report, f1_score\nfrom sklearn.model_selection import train_test_split\n\n# huggingface: Stores pre-trained models\nfrom transformers import BertTokenizer #, BertForSequenceClassification, set_seed\nfrom transformers import BertModel, AdamW, BertConfig\nfrom transformers import pipeline, AdamW, get_linear_schedule_with_warmup\nimport json \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet= pd.read_csv('../input/nlp-getting-started/train.csv')\ntest=pd.read_csv('../input/nlp-getting-started/test.csv')\ntweet.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet.target.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.style.use('ggplot')\nsns.countplot(x=\"target\", data=tweet ,hue=\"target\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('ggplot')\nf,axes=plt.subplots(1,2,figsize=(10,5))\nf.suptitle('Characters in tweets')\n\ntweet_len=tweet[tweet['target']==1]['text'].str.len()\nax1=sns.histplot(tweet_len,ax=axes[0],color='red')\n# ax1.set(xlabel='common xlabel', ylabel='common ylabel')\nax1.set_title('disaster tweets')\n\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2=sns.histplot(tweet_len,ax=axes[1])\n# ax2.set(xlabel='common xlabel', ylabel='common ylabel')\nax2.set_title('Not disaster tweets')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_len=tweet[tweet['target']==1]['text'].str.len()\nsns.histplot(tweet_len.values,color='red')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use('ggplot')\nf,axes=plt.subplots(1,2,figsize=(14,5))\nf.suptitle('Words in a tweet')\ntweet_len=tweet[tweet.target==1]['text'].str.split(' ').map(lambda x: len(x))\nax1=sns.histplot(tweet_len,ax=axes[0],color='red')\n# ax1.set(xlabel='common xlabel', ylabel='common ylabel')\nax1.set_title('disaster tweets')\n\ntweet_len=tweet[tweet.target==0]['text'].str.split(' ').map(lambda x: len(x))\nax2=sns.histplot(tweet_len,ax=axes[1])\n# ax2.set(xlabel='common xlabel', ylabel='common ylabel')\nax2.set_title('Not disaster tweets')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text=''\nfor i in tweet['text']:\n    text+=i\n#     print(i)\n#     break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict,Counter\ncount_punctuation=defaultdict(int)\nimport string\n\nfor x,y in Counter(text).items():\n\n    if x in string.punctuation:\n#         print(x)\n#         print(type(x))\n        count_punctuation[x]=y\n#         print(x,y)\n#     print(x,y)\n#     break\nprint(count_punctuation)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(10,5)})\nax=sns.barplot(x='punctuation',y='count',\n            data=pd.DataFrame(count_punctuation.items()\n                              ,columns=['punctuation','count']).sort_values\n            (by=['count'], ascending=False))\nax.set_title('Count of Punctuations')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Common words\nword_count=Counter(text.split(' '))\nmost_common=word_count.most_common(40)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(12,8)})\nsns.barplot(x='count',y='word',data=pd.DataFrame(most_common,columns=['word','count']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nimport spacy\nsp = spacy.load('en_core_web_sm')\n\ndef cleaning_text(text,emojis=True,html_tag=True,http=True,lemmitize=True,punctuation=True):\n    \n    #remove emojis\n    if emojis is True:\n        regrex_pattern = re.compile(pattern = \"[\"                                                   \n                                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                                       \"]+\", flags = re.UNICODE)\n\n        text=regrex_pattern.sub(r'',text)\n    \n    \n    \n    #lower \n    text=text.lower()\n    \n    #remove html tag\n    if html_tag is True:\n        text=re.sub('<.*?>',\"\",text)\n        \n    #remove http link\n    if http is True:\n        text = re.sub(\"https?:\\/\\/t.co\\/[A-Za-z0-9]*\", '', text)\n    \n#     lemmitizing\n    if lemmitize is True:\n        lemmatized = [word.lemma_ for word in sp(text)]\n        text = ' '.join(lemmatized)\n    \n    #remove punctuation\n    if punctuation is True:\n        text = text.translate(str.maketrans('', '', string.punctuation))\n        \n    # removing extra space\n    text = re.sub(\"\\s+\", ' ', text)\n    \n    \n#     print(text)\n    \n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet['cleaned']=tweet['text'].apply(lambda x: cleaning_text(x,lemmitize=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# split dataset into train and val for validating model performance\nx_train, x_val, y_train, y_val = train_test_split(\n    tweet['cleaned'].values, tweet['target'].values, test_size=0.05, stratify=tweet['target'].values)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape,x_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# init tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n# get embedings\n\nencoded_data_train = tokenizer.batch_encode_plus(\n    list(x_train), \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    padding='max_length',\n    max_length=50,   # max length is taken 50 as max len of tweets is around 30\n    return_tensors='pt',\n    truncation=True,\n    return_token_type_ids=False\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    list(x_val), \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True,\n    padding='max_length', \n    max_length=50, \n    return_tensors='pt',\n    truncation=True,\n    return_token_type_ids=False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(list(y_train))#.float()\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(list(y_val))#.float()\n\n\n# making dataset\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n\n# making dataloader\ndataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=8)\ndataloader_val = DataLoader(dataset_val, sampler=SequentialSampler(dataset_val), batch_size=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#building model\nclass SentimentClassifier(nn.Module):\n    \n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.drop = nn.Dropout(p=0.1)\n        self.out1 = nn.Linear(self.bert.config.hidden_size, 300)\n        self.out2 = nn.Linear(300, 50)\n        self.out3 = nn.Linear(50, n_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        o = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n\n        output = torch.max(o.last_hidden_state, axis=1).values  #global_max_1d of tensorflow\n        output = self.out1(output)\n        output = torch.relu(output)\n        \n        output = self.drop(output)\n        \n        output = self.out2(output)\n        output = torch.relu(output)\n        \n        output = self.out3(output)\n        \n        return output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\nmodel = SentimentClassifier(n_classes=2)\nmodel = model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n# I believe the 'W' stands for 'Weight Decay fix\"\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\nfrom transformers import get_linear_schedule_with_warmup\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 3\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(dataloader_train) * epochs\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)\nloss_fn = nn.CrossEntropyLoss().to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport datetime\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n# This training code is based on the `run_glue.py` script here:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n    # Reset the total loss for this epoch.\n    total_loss = 0\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n    model.train()\n    # For each batch of training data...\n    for step, batch in enumerate(dataloader_train):\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(dataloader_train), elapsed))\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n        # Perform a forward pass (evaluate the model on this training batch).\n        # This will return the loss (rather than the model output) because we\n        # have provided the `labels`.\n        # The documentation for this `model` function is here: \n        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n        outputs = model(b_input_ids,  \n                    attention_mask=b_input_mask)\n        \n        # The call to `model` always returns a tuple, so we need to pull the \n        # loss value out of the tuple.\n        # output=torch.argmax(outputs,axis=1)\n        # print(outputs)\n        # print(b_labels)\n        loss=loss_fn(outputs, b_labels)\n\n        # print(\"loss \",loss)\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_loss += loss.item()\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n        # Update the learning rate.\n        scheduler.step()\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss / len(dataloader_train)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n    print(\"\")\n    print(\"Running Validation...\")\n    t0 = time.time()\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n    # Tracking variables \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    # Evaluate data for one epoch\n    for batch in dataloader_val:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():        \n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have\n            # not provided labels.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n            outputs = model(b_input_ids,  \n                            attention_mask=b_input_mask)\n        \n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        # logits = outputs[0]\n        logits=torch.sigmoid(outputs)\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        # Accumulate the total accuracy.\n        eval_accuracy += tmp_eval_accuracy\n        # Track the number of batches\n        nb_eval_steps += 1\n    # Report the final accuracy for this validation run.\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\nprint(\"\")\nprint(\"Training complete!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Generating output file\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['cleaned']=test['text'].apply(lambda x: cleaning_text(x,lemmitize=False))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_data_test = tokenizer.batch_encode_plus(\n    test['cleaned'].values, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    padding='max_length',\n    max_length=50, \n    return_tensors='pt',\n    truncation=True,\n    return_token_type_ids=False\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids_test = encoded_data_test['input_ids']\nattention_masks_test = encoded_data_test['attention_mask']\n\n\n# making dataset\ndataset_test = TensorDataset(input_ids_test, attention_masks_test)\n\n\n# making dataloader\ndataloader_test = DataLoader(dataset_test,sampler=SequentialSampler(dataset_test), batch_size=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Prediction on test set\nprint('Predicting labels for {:,} test sentences...'.format(len(dataloader_test)))\n# Put model in evaluation mode\nmodel.eval()\n# Tracking variables \npredictions , true_labels = [], []\n# Predict \nfor batch in dataloader_test:\n  # Add batch to GPU\n  batch = tuple(t.to(device) for t in batch)\n  \n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask = batch\n  \n  # Telling the model not to compute or store gradients, saving memory and \n  # speeding up prediction\n  with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      outputs = model(b_input_ids,attention_mask=b_input_mask)\n  # logits = outputs[0]\n  logits=torch.sigmoid(outputs)\n\n      \n\n\n  # Move logits and labels to CPU\n  logits = logits.detach().cpu().numpy()\n  # label_ids = b_labels.to('cpu').numpy()\n  \n  # Store predictions and true labels\n  predictions.append(logits)\n  # true_labels.append(label_ids)\nprint('DONE.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_flat=[]\nfor i in predictions:\n  pred_flat.append(np.argmax(i, axis=1).tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_sumbit=[]\nfor i in range(len(test)):\n  to_sumbit.append([test['id'][i],pred_flat[i][0]])\n  # break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sumbit=pd.DataFrame(to_sumbit,columns=['id','target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sumbit.to_csv(\"submission.csv\",index=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}