{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing The Required Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nimport tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.regularizers import l1\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score \nimport numpy as np\nfrom time import time\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading The Dataset.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data= pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv', encoding='latin-1')\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are only going to use the first two columns also we are going to replace spam with 1 and ham with 0 in column 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.iloc[:,:2]\ndata['message_len'] = data.v2.apply(len)\ndata['v1']=data['v1'].replace({'ham':0,'spam':1})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets Do Some EDA.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets checkout the message length whether the sms is a spam or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\ndata[data.v1==0].message_len.plot(bins=35, kind='hist', color='blue', \n                                       label='Ham messages', alpha=0.6)\ndata[data.v1==1].message_len.plot(kind='hist', color='red', \n                                       label='Spam messages', alpha=0.6)\nplt.legend()\nplt.xlabel(\"Message Length\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df=data.iloc[:,:2]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n \n    text = re.sub('[^a-zA-Z]', ' ', text)  \n\n    text = text.lower()  \n\n    text = text.split(' ')  \n    \n    text = [w for w in text if not w in set(stopwords.words('english'))] \n\n    text = ' '.join(text)    \n            \n    return text\n\n\n\ndf['v2'] = df['v2'].apply(lambda x : clean_text(x))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GloVe for Vectorization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 50 D here because the Dataset is small.If you have a larger dataset,you can use 100D or 200D.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['v2']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n\ncorpus=create_corpus(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dict={}\nwith open('/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN=10\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,50))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting The Data into train and validation set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val, y_train, y_val = train_test_split(tweet_pad,df.v1, test_size=.2, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of train sequences: ',X_train.shape)\nprint('Shape of train labels: ',y_train.shape)\nprint(\"Shape of Validation sequences: \",X_val.shape)\nprint(\"Shape of Validation  labels: \",y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Our Model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\n\nembedding_layer=Embedding(num_words,50,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding_layer)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2,return_sequences=True))\nmodel.add(tf.keras.layers.LSTM(32,return_sequences=True))\nmodel.add(tf.keras.layers.LSTM(16))\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\n\nmodel.add(Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimzer=Adam(learning_rate=1e-4)\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['acc'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting The Model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(X_train,y_train,batch_size=32,epochs=10,validation_data=(X_val,y_val),verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating a Dataframe of Loss And Accuracy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets visualize the Loss over the Epochs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_loss[['loss','val_loss']].plot(ylim=[0,1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets visualize the Accuracy over the Epochs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_loss[['acc','val_acc']].plot(ylim=[0,1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thankyou For Reading!!!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}