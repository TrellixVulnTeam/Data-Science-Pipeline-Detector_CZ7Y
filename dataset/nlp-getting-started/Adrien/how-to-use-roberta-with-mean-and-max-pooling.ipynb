{"cells":[{"metadata":{},"cell_type":"markdown","source":"# RoBERTa with mean and max pooling\n\nThis notebook shows you how to set up a classifier based on RoBERTa, with mean and max pooling.\n\n## Read the data\n\nWe load the train and test datasets."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\n\ntrain = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\").sample(frac=1.)\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize the tweets\n\nWe clean the tweets a little by removing tokens that are shorter than 2 characters or longer than 15 characters."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"collapsed":true},"cell_type":"code","source":"from transformers import *\nfrom gensim.utils import simple_preprocess\nimport numpy as np\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nX_train = np.array([tokenizer.encode(\" \".join(simple_preprocess(text, min_len=2, max_len=15)),\n                                     add_special_tokens=True, \n                                     max_length=40, \n                                     pad_to_max_length=True) for text in train[\"text\"]])\nX_test = np.array([tokenizer.encode(\" \".join(simple_preprocess(text, min_len=2, max_len=15)), \n                           add_special_tokens=True, \n                           max_length=40, \n                           pad_to_max_length=True) for text in test[\"text\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RoBERTa with mean and max pooling\n\nWe embed the words using RoBERTa, we perform mean and max pooling over the embedded sequences "},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.initializers import glorot_normal\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow import int32\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.utils import plot_model\n\n# Input layer\ninput_ids = Input((40,), dtype=int32)\n# RoBERTa layer\nlm = TFRobertaModel.from_pretrained('roberta-base')\nsequence, cls = lm(input_ids) \n# Parallel mean and max global pooling\nmean_pooling = GlobalAveragePooling1D()(sequence)\nmax_pooling = GlobalMaxPooling1D()(sequence)\npooling = concatenate([mean_pooling, max_pooling])\n# Dropout layer\ndropout = Dropout(0.5, name=\"dropout\")(pooling)\n# Classification layer\nclassification = Dense(1, activation=\"sigmoid\", kernel_initializer=glorot_normal(seed=1), bias_initializer=glorot_normal(seed=1), name=\"classification\")(pooling)\nmodel = Model(input_ids, classification)\nplot_model(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We train the model, with a linearly decaying learning rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import LearningRateScheduler\n\ndef rate(epoch):\n    return 1.5e-5/(epoch + 1)\n\nscheduler = LearningRateScheduler(rate)\nmodel.compile(optimizer=Adam(beta_1=0.9, beta_2=0.999), \n              loss=\"binary_crossentropy\", \n              metrics=[\"accuracy\"])\nlog = model.fit(X_train, train[\"target\"].values, \n                callbacks=[scheduler],\n                batch_size=12, \n                epochs=3, \n                verbose=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"proba\"] = model.predict(X_test)\ntest[\"target\"] = test[\"proba\"].apply(lambda p: int(p > 0.5))\ntest[[\"id\", \"target\"]].to_csv(\"/kaggle/working/submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}