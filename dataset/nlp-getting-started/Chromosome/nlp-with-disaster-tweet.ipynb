{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h1><b>Real or Not? NLP with Disaster Tweet</b></h1></center>\n<center>DSI206 Multimedia Representation Management</center>"},{"metadata":{},"cell_type":"markdown","source":"### รายชื่อสมาชิก\n> 1. นางสาวอโรชา หมอสินธุ์ 6209656104\n> 2. นางสาวศศิมา นิ่มมา 6209656013\n> 3. นางสาวรวงข้าว แสงทวีปทวีกิจ 6209656146"},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n#### ในปัจจุบัน twitter เป็นที่นิยมมากในการใช้ติดต่อสื่อสารที่สำคัญ เพราะมีความรวดเร็ว สามารถเฝ้าสังเกตสถานการณ์ฉุกเฉินได้แบบเรียลไทม์ จึงทำให้ผู้คนส่วนใหญ่เลือกที่จะใช้ twitter ในการการกระจายข่าวสารต่างๆ ดังนั้นจึงมีความสนใจการทำนายข้อความจาก twitter ว่าเป็นข้อความที่เกี่ยวข้องกับภัยพิบัติหรือไม่ โดยจะสร้างโมเดลแมชชีนเลิร์นนิ่งที่คาดการณ์ว่าทวีตใดเกี่ยวกับภัยพิบัติจริงและไม่จริง"},{"metadata":{},"cell_type":"markdown","source":"# Data Set\n### 1. train คือ ข้อมูลที่ใช้การการฝึก Machine Learning\n> * id - เลข id ผู้ใช้งาน twitter\n> * text - ข้อความที่ผู้ใช้งาน twitter ทวีต\n> * location - ตำแหน่งของผู้ใช้งานทวีต ณ เวลาที่ทวีต\n> * keyword - คำที่ใช้ในการทำนาย\n> * target - ผลการทำนายว่าเกี่ยวข้องกับภัยพิบัติหรือไม่\n\n### 2. test คือ ข้อมูลที่รวบรวมข้อมูลที่ใช้ในการทดสอบ Machine Learning\n> * id - เลข id ผู้ใช้งาน twitter\n> * text - ข้อความที่ผู้ใช้งาน twitter ทวีต\n> * location - ตำแหน่งของผู้ใช้งานทวีต ณ เวลาที่ทวีต\n> * keyword - คำที่ใช้ในการทำนาย\n\n### 3. sample_submission.csv คือ ไฟล์ที่เก็บข้อมูลในรูปแบบที่ผ่านการวิเคราะห์แล้ว"},{"metadata":{},"cell_type":"markdown","source":"### ทำการ import module ดังนี้\n> * ประกาศขอใช้ numpy สำหรับช่วยในการจัดการค่าต่างๆ ที่เกี่ยวการคำนวณ \n> * ประกาศขอใช้ pandas สำหรับจัดการข้อมูลและแสดงผลข้อมูลแบบกราฟิก\n> * ประกาศขอใช้ os สำหรับช่วยในการเช็คข้อมูลใน dataset ว่ามีอะไรบ้าง"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cleaning\nจากนั้นดึงไฟล์เก็บเข้าในตัวแปร แล้วทำการ cleaning data\n\n> * ขอใช้ Regular Expression ในการดำเนินการกับข้อความในรูปแบบ string โดยการ import re และ import string\n\n> * สร้างฟังก์ชัน contraction ในการเปลี่ยนแปลงตัวอักขระหรือคำบางคำที่เป็นตัวย่อให้กลายเป็นคำรูปแบบเต็ม\n\n> * ใช้ re.sub() ในการเขียนทับข้อความตรงส่วนนั้นๆ\n\n> * หลังจากสร้างฟังก์ชันแล้วจึงทำการนำมาใช้กับตัวแปร train และ test โดยการใช้ lambda เข้ามาช่วย เพื่อลดรูปในการสร้างฟังก์ชัน\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\ndef contraction(text):\n    # specific\n    text = re.sub(r\"won\\'t\", \"will not\", text)\n    # general\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    return text\n    \n\ntrain['text'] = train['text'].apply(lambda x : contraction(x))\ntest['text'] = test['text'].apply(lambda x : contraction(x))\n\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * สร้างฟังก์ชัน urls ในการช่วยในการตัด link\n\n> * ใช้ re.sub() ในการลบข้อมูลตรงส่วนนั้นออก\n\n> * หลังจากสร้างฟังก์ชันแล้วจึงทำการนำมาใช้กับตัวแปร train และ test โดยการใช้ lambda เข้ามาช่วย"},{"metadata":{"trusted":true},"cell_type":"code","source":"def urls(text):\n    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    return text\n\ntrain['text'] = train['text'].apply(lambda x : urls(x))\ntest['text'] = test['text'].apply(lambda x : urls(x))\n\ntrain['text'].head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * สร้างฟังก์ชัน spacial ในการช่วยในการตัดข้อความบางส่วนที่ไม่จำเป็นออกไป\n\n> * ใช้ re.sub() ในการทำตัวอักษรให้เป็นตัวพิมพ์เล็กทั้งหมด รวมถึงลบเครื่องหมายต่างๆที่ไม่ต้องการ รวมไปถึงการขึ้นบรรทัดใหม่ เครื่องหมายวรรคตอน คำที่มีตัวเลขคั่น\n\n> * หลังจากสร้างฟังก์ชันแล้วจึงทำการนำมาใช้กับตัวแปร train และ test โดยการใช้ lambda เข้ามาช่วย"},{"metadata":{"trusted":true},"cell_type":"code","source":"def spacial(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(\"\\\\W\",\" \",text) # remove special chars\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ntrain['text'] = train['text'].apply(lambda x : spacial(x))\ntest['text'] = test['text'].apply(lambda x : spacial(x))\n\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> * สร้างฟังก์ชัน emoji ในการลบ emoji ออกไป\n\n> * ใช้ re.compile() ในการรวบรวมรูปแบบที่เกี่ยวข้องกับ emoji โดยการแทน emoji เป็นรหัสในทุกๆหมวดหมู่ และลบโดยใช้ไลบรารี่ re\n\n> * หลังจากสร้างฟังก์ชันแล้วจึงทำการนำมาใช้กับตัวแปร train และ test โดยการใช้ lambda เข้ามาช่วย"},{"metadata":{"trusted":true},"cell_type":"code","source":"def emoji(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)\ntrain['text'] = train['text'].apply(lambda x : emoji(x))\ntest['text'] = test['text'].apply(lambda x : emoji(x))\n\ntrain['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyzing \nทางกลุ่มได้ทำการ import nltk เพื่อนำมาใช้กับ stopwords และ import CountVectorizer จาก sklearn เพื่อแปลงชุดข้อความให้กลายเป็นเลข โดยจะแยกตามตำแหน่งของคำ\n\n> * token_pattern : รูปแบบของ token ที่จะทำการวิเคราะห์\n> * ngram_range : กำหนดขอบเขตการเลือกคำที่ติดกัน n คำ และให้ค่าความน่าจะเป็นของชุดคำที่เกิดขึ้นร่วมกัน\n> * stop_words : ตัวแปรในการตัดคำที่พบบ่อยในประโยคแต่ไม่สื่อถึงอะไร โดยใช้ตัวแปร stopwords ที่เก็บ list ของคำภาษาอังกฤษที่พบบ่อย"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nstopwords = stopwords.words('english')\n\ncount_vector = CountVectorizer(token_pattern=r'\\w{1,}', ngram_range=(1, 2), stop_words = stopwords)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ขอใช้คำสั่ง train_test_split จากไลบรารี่ sklearn\n\n* Train ใช้สำหรับป้อนให้โมเดลใช้เทรน\n* Test ใช้สำหรับทดสอบหา Metrics หลังจากเทรนเสร็จว่า โมเดลจะทำงานได้ดีแค่ไหน\n\nสร้างตัวแปรอิสระ X เป็นฟีทเจอร์หรือคุณสมบัติ\n\nสร้างตัวแปรตาม y เป็นเลเบล (label) ซึ่งเป็นคำตอบที่ต้องการ \n\nสร้างตัวแปร X_train, X_test , y_train และ y_test ด้วยฟังก์ชั่น train_test_split() กำหนด test_size 10% (0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = train.text\ny = train.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ขอใช้ LogisticRegression จากไลบรารี่ sklearn\n\nขอใช้ Pipeline จากไลบรารี่ sklearn\n\nclf : ตัวแปรที่เก็บฟังก์ชัน LogisticRegression\n\npipe : ซึ่งการทำงานของมันคือทำงานเป็นลำดับเหมือนท่อ โดยเราสามารถใส่ฟังก์ชั่นเข้าไปท่อนี้ดังโค้ดข้างล่าง คือการนำเอาฟังก์ชั่น CountVectorizer ไว้จัดการ จากนั้นนำค่าไป scale แล้วก็เข้าโมเดลของเราคือ LogisticRegression นั้นเอง\n\npipe.fit() แต่ละตัวภายในไปป์ไลน์จะติดตั้งกับเอาต์พุตของก่อนหน้า\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nclf = LogisticRegression()\npipe = Pipeline([\n    ('count_vector', CountVectorizer()),  \n    ('clf', LogisticRegression()) \n])\npipe.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"จากนั้นทางกลุ่มจะมาดูว่าความถูกต้องในการแยกแยะข้อมูลแต่ละรายการในเซ็ตข้อมูลนั้น แม่นยำเพียงไร \n\nขอใช้ metrics จากไลบรารี่ sklearn เพื่อทำการทำนายผลความถูกต้องแม่นยำ \n\nทางกลุ่มจะทำการทำนาย predict กับ y_test ซึ่งจะดูว่าผลการทำนายกับผลลัพท์ที่ถูกต้องจากเลเบลว่ามีมากน้อยเพียงไร และทำการแจกแจงคะแนนเพื่อให้สามารถเข้าใจได้ง่ายขึ้น"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\npredicted = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"accuracy :\",  metrics.accuracy_score(predicted, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\n\nทำการเก็บ model ที่ได้มา predict กับ test.text ลงในไฟล์ submission ในคอลัมม์ target จากนั้นอัพโหลดเข้าไฟล์ submission.csv เพื่อทำการส่งผลการทำนาย"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsubmission[\"target\"] = pipe.predict(test.text)\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reference\n[https://medium.com/@minatonamikazerak/python-%E0%B8%A5%E0%B8%AD%E0%B8%87%E0%B9%80%E0%B8%A5%E0%B9%88%E0%B8%99-regular-expression-aeffdb07c794](http://)\n\n[https://python3.wannaphong.com/2014/10/python-regular-expressions.html](http://)\n\n[https://qastack.in.th/programming/33091376/python-what-is-exactly-sklearn-pipeline-pipeline](http://)\n\n[https://www.kaggle.com/iavinas/nlp-spacy-basics](http://)\n\n[https://www.kaggle.com/patipanrattanawin/disaster-tweet-text-classification-simplest-way](http://)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}