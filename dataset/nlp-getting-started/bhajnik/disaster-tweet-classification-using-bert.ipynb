{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n!pip install autoviml\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\npd.set_option('display.max_colwidth', -1)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"from autoviml.Auto_NLP import Auto_NLP","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"destination_folder = './'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nclass TextProcessing:\n    def __init__(self,text):\n        self.text = text\n    \n    def remove_punctuation(self,text):\n        return \"\".join([i for i in text if i not in string.punctuation])\n    def remove_url(self,text):\n        url = re.compile(r'https?://\\S+|www\\.\\S+')\n        text = url.sub(r'',text)\n        url = re.compile(r'http?://\\S+|www\\.\\S+')\n        return url.sub(r'',text)\n    def remove_emoji(self,text):\n        emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', text)\n    def remove_html(self,text):\n        html=re.compile(r'<.*?>')\n        return html.sub(r'',text)\n    def clean(self):\n#         self.text = self.text.apply(lambda x: re.sub(\"s+\",\" \", x) )\n#         text = self.text.apply(lambda x: self.remove_punctuation(x)  )\n        self.text = self.text.apply(lambda x: x.lower()  )\n#         self.text = self.text.apply(lambda x: self.remove_url(x)  )\n#         self.text = self.text.apply(lambda x: self.remove_emoji(x)  )\n#         self.text = self.text.apply(lambda x: self.remove_html(x)  )\n        return self.text\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Libraries\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\n\n# Preliminaries\n\nfrom torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n\n# Models\n\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Training\n\nimport torch.optim as optim\n\n# Evaluation\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport seaborn as sns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\").fillna('')\ntrain['text_keyword'] = train[[\"keyword\",\"text\"]].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\nttp = TextProcessing(train['text_keyword'])\ntrain['text_keyword'] = ttp.clean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.keyword.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum() /train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train[train.target==1].groupby(['keyword'])['keyword'].count().plot.bar()\ntrain[train.target==1]['keyword'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.target==0]['keyword'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(4, 4), stop_words='english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_words = get_top_n_bigram(train[train.target == 1]['text'], 20)\ndf1 = pd.DataFrame(common_words, columns = ['text' , 'count'])\ndf1.groupby('text').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams for disaster text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_words = get_top_n_bigram(train[train.target == 0]['text'], 20)\ndf1 = pd.DataFrame(common_words, columns = ['text' , 'count'])\ndf1.groupby('text').sum()['count'].sort_values(ascending=False).iplot(\n    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigramsv for non disaster text')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\").fillna('')\ntest['text_keyword'] = test[[\"keyword\",\"text\"]].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\ntetp = TextProcessing(test['text_keyword'])\ntest['text_keyword'] = tetp.clean()\n#Dummy target Variable\ntest['target'] = 0\ntest[['text_keyword','target']].to_csv('test_bert.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_bert = pd.DataFrame()\nvalid_bert = pd.DataFrame()\ndef create_dataset(X,Y,df):\n    df['text_keyword'] = X\n    df['target'] = Y    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split( train['text_keyword'],train['target'], test_size=0.33, random_state=42, stratify=train['target'])\ncreate_dataset(X_train,y_train,train_bert)\ncreate_dataset(X_valid,y_valid,valid_bert)\ntrain_bert.to_csv(\"train_bert.csv\",index=False)\nvalid_bert.to_csv(\"valid_bert.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building a benchmark model using AutoNLP**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, test_x, predictor, predicted= Auto_NLP('text_keyword', train_bert, test,'target',score_type=\"balanced_accuracy\",\n                                            top_num_features=200,modeltype=\"Classification\",verbose=2,build_model=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Model parameter\nMAX_SEQ_LEN = 128\nPAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\nUNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n\n# Fields\n\nlabel_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\ntext_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,\n                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)\ntrain_fields = [ ('text_keyword', text_field),('target', label_field)]\ntest_fields = [ ('text_keyword', text_field)]\n\n# TabularDataset\n\ntrain, valid,test = TabularDataset.splits(path='./', train='train_bert.csv', validation='valid_bert.csv',\n                                          test='test_bert.csv', format='CSV', fields=train_fields, skip_header=True)\n\n\n# Iterators\n\ntrain_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text_keyword),\n                            device=device, train=True, sort=True, sort_within_batch=True)\nvalid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text_keyword),\n                            device=device, train=True, sort=True, sort_within_batch=True)\ntest_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERT(nn.Module):\n\n    def __init__(self):\n        super(BERT, self).__init__()\n\n        options_name = \"bert-base-uncased\"\n        self.encoder = BertForSequenceClassification.from_pretrained(options_name)\n\n    def forward(self, text, label):\n        loss, text_fea = self.encoder(text, labels=label)[:2]\n\n        return loss, text_fea","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save and Load Functions\n\ndef save_checkpoint(save_path, model, valid_loss):\n\n    if save_path == None:\n        return\n    \n    state_dict = {'model_state_dict': model.state_dict(),\n                  'valid_loss': valid_loss}\n    \n    torch.save(state_dict, save_path)\n    print(f'Model saved to ==> {save_path}')\n\ndef load_checkpoint(load_path, model):\n    \n    if load_path==None:\n        return\n    \n    state_dict = torch.load(load_path, map_location=device)\n    print(f'Model loaded from <== {load_path}')\n    \n    model.load_state_dict(state_dict['model_state_dict'])\n    return state_dict['valid_loss']\n\n\ndef save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n\n    if save_path == None:\n        return\n    \n    state_dict = {'train_loss_list': train_loss_list,\n                  'valid_loss_list': valid_loss_list,\n                  'global_steps_list': global_steps_list}\n    \n    torch.save(state_dict, save_path)\n    print(f'Model saved to ==> {save_path}')\n\n\ndef load_metrics(load_path):\n\n    if load_path==None:\n        return\n    \n    state_dict = torch.load(load_path, map_location=device)\n    print(f'Model loaded from <== {load_path}')\n    \n    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Function\n\ndef train(model,\n          optimizer,\n          criterion = nn.BCELoss(),\n          train_loader = train_iter,\n          valid_loader = valid_iter,\n          num_epochs = 5,\n          eval_every = len(train_iter) // 2,\n          file_path = destination_folder,\n          best_valid_loss = float(\"Inf\")):\n    \n    # initialize running values\n    running_loss = 0.0\n    valid_running_loss = 0.0\n    global_step = 0\n    train_loss_list = []\n    valid_loss_list = []\n    global_steps_list = []\n\n    # training loop\n    model.train()\n    for epoch in range(num_epochs):\n        for (text_keyword, target), _ in train_loader:\n            target = target.type(torch.LongTensor)           \n            target = target.to(device)\n            text_keyword = text_keyword.type(torch.LongTensor)  \n            text_keyword = text_keyword.to(device)\n            output = model(text_keyword, target)\n            loss, _ = output\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # update running values\n            running_loss += loss.item()\n            global_step += 1\n\n            # evaluation step\n            if global_step % eval_every == 0:\n                model.eval()\n                with torch.no_grad():                    \n\n                    # validation loop\n                    for (text_keyword,target), _ in valid_loader:\n                        target = target.type(torch.LongTensor)           \n                        target = target.to(device)\n                        text_keyword = text_keyword.type(torch.LongTensor)  \n                        text_keyword = text_keyword.to(device)\n                        output = model(text_keyword, target)\n                        loss, _ = output\n                        \n                        valid_running_loss += loss.item()\n\n                # evaluation\n                average_train_loss = running_loss / eval_every\n                average_valid_loss = valid_running_loss / len(valid_loader)\n                train_loss_list.append(average_train_loss)\n                valid_loss_list.append(average_valid_loss)\n                global_steps_list.append(global_step)\n\n                # resetting running values\n                running_loss = 0.0                \n                valid_running_loss = 0.0\n                model.train()\n\n                # print progress\n                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n                              average_train_loss, average_valid_loss))\n                \n                # checkpoint\n                if best_valid_loss > average_valid_loss:\n                    best_valid_loss = average_valid_loss\n                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)\n                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n    \n    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n    print('Finished Training!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BERT().to(device)\noptimizer = optim.Adam(model.parameters(), lr=2e-5)\n\ntrain(model=model, optimizer=optimizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loss_list, valid_loss_list, global_steps_list = load_metrics(destination_folder + '/metrics.pt')\nplt.plot(global_steps_list, train_loss_list, label='Train')\nplt.plot(global_steps_list, valid_loss_list, label='Valid')\nplt.xlabel('Global Steps')\nplt.ylabel('Loss')\nplt.legend()\nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Selecting the best model after training step**"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = BERT().to(device)\n\nload_checkpoint(destination_folder + '/model.pt', best_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluate"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_eval = []\ny_true_eval = []\ndef evaluate(model, test_loader):  \n\n    model.eval()\n    with torch.no_grad():\n        for (text_keyword,target), _ in test_loader:\n\n                target = target.type(torch.LongTensor)           \n                target = target.to(device)\n                text_keyword = text_keyword.type(torch.LongTensor)  \n                text_keyword = text_keyword.to(device)\n                output = model(text_keyword, target)\n\n                _, output = output\n                y_eval.extend(torch.argmax(output, 1).tolist())\n                y_true_eval.extend(target.tolist())\n    \n    print('Classification Report:')\n    print(classification_report(y_true_eval, y_eval, labels=[0,1], digits=4))\n    \n    cm = confusion_matrix(y_true_eval, y_eval, labels=[0,1])\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n\n    ax.set_title('Confusion Matrix')\n\n    ax.set_xlabel('Predicted Labels')\n    ax.set_ylabel('True Labels')\n\n    ax.xaxis.set_ticklabels(['Not Disaster', 'REAL'])\n    ax.yaxis.set_ticklabels(['Not Disaster', 'REAL'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(best_model, valid_iter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_bert['y_eval'] = y_eval\n\nvalid_bert[valid_bert.target ==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = []\ndef predict(model, test_loader):    \n\n    model.eval()\n    with torch.no_grad():\n        for (text_keyword,target), _ in test_loader:\n                target = target.type(torch.LongTensor)           \n                target = target.to(device)\n                text_keyword = text_keyword.type(torch.LongTensor)  \n                text_keyword = text_keyword.to(device)\n                output = model(text_keyword, target)\n\n                _, output = output\n                y_pred.extend(torch.argmax(output, 1).tolist())\n               ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(best_model, test_iter)\ntest_final = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntest_final['target'] = y_pred\ntest_final[['id','target']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_final['autoNlpPrediction'] = predicted\n# test_final[(test_final.target==1) & (test_final.autoNlpPrediction==0)]\ntest_final['target'] = predicted\ntest_final[['id','target']].to_csv('submission1.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}