{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Load librairies and Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport string\n\nplt.style.use('seaborn')\nplt.rcParams['lines.linewidth'] = 1\n\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom sklearn.metrics import f1_score\n\nNBR_STAR=70\n\nX = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ny = X[\"target\"]\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleaning\nI've not written the function clean_text, the writter is Sahib here https://www.kaggle.com/sahib12/nlp-starter-for-beginners"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['keyword'].fillna('',inplace=True)\nX['keyword'] = X['keyword'].map(lambda x:x.replace('%20', ' '))\ntest['keyword'].fillna('',inplace=True)\ntest['keyword'] = test['keyword'].map(lambda x:x.replace('%20', ' '))\n\n# source https://www.kaggle.com/sahib12/nlp-starter-for-beginners\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n#X['text'] = X['text'].apply(lambda x: clean_text(x))\n#test['text'] = test['text'].apply(lambda x: clean_text(x))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# List of stop words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of stop words\nstop_word = list(ENGLISH_STOP_WORDS)\nstop_word.append('http')\nstop_word.append('https')\nstop_word.append('รป_')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Class balance\n<span>&#171;</span> \nA given tweet is about a real disaster (target=1) or not (target=0)\n<span>&#187;</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['target'].value_counts().plot(kind = 'barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plots the text length distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sample_length_distribution(sample_texts):\n    plt.figure(figsize=(10,10))\n    plt.hist([len(s) for s in sample_texts], 50)\n    plt.xlabel('Length of a sample')\n    plt.ylabel('Number of samples')\n    plt.title('Sample length distribution')\n    plt.show()\n\nplot_sample_length_distribution(X['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore the column keyword"},{"metadata":{"trusted":true},"cell_type":"code","source":"keyword_stats = X.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\nkeywords_disaster = keyword_stats.loc[keyword_stats['Disaster Probability']==1]\nkeywords_no_disaster  = keyword_stats.loc[keyword_stats['Disaster Probability']==0]\nkeyword_stats.sort_values('Disaster Probability', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wordcloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\nSTOPWORDS.add('http')  \nSTOPWORDS.add('https')  \nSTOPWORDS.add('CO')  \nSTOPWORDS.add('รป_')\nno_disaster_text = \" \".join(X[X[\"target\"] == 0].text.to_numpy().tolist())\nreal_disaster_text = \" \".join(X[X[\"target\"] == 1].text.to_numpy().tolist())\n\nno_disaster_cloud = WordCloud(stopwords=stop_word, background_color=\"white\").generate(no_disaster_text)\nreal_disaster_cloud = WordCloud(stopwords=stop_word, background_color=\"white\").generate(real_disaster_text)\n\ndef show_word_cloud(cloud, title):\n  plt.figure(figsize = (16, 10))\n  plt.imshow(cloud, interpolation='bilinear')\n  plt.title(title)\n  plt.axis(\"off\")\n  plt.show();\n\nshow_word_cloud(no_disaster_cloud, \"No disaster common words\")\nshow_word_cloud(real_disaster_cloud, \"Real disaster common words\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply bag-of-words to the dataset  -  ngram_range=(1, 2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = CountVectorizer(min_df=2,ngram_range=(1, 2), stop_words=stop_word)\nX_train = vect.fit_transform(X['text'])\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plots the frequency distribution of first 50 n-grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_ngrams = list(vect.get_feature_names())\nnum_ngrams = 50\n\nall_counts = X_train.sum(axis=0).tolist()[0]\nall_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n    zip(all_counts, all_ngrams), reverse=True)])\nngrams = list(all_ngrams)[:num_ngrams]\ncounts = list(all_counts)[:num_ngrams]\n\nidx = np.arange(num_ngrams)\nplt.figure(figsize=(10,10))\nplt.barh(idx, counts,  color='orange')\nplt.ylabel('N-grams')\nplt.xlabel('Frequencies')\nplt.title('Frequency distribution of n-grams')\nplt.yticks(idx, ngrams)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# First try with this bag of word and a logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First try with this bag of word and a logistic regression\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import ShuffleSplit\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=123)\nscores = cross_val_score(LogisticRegression(), X_train, y,scoring=\"f1\", cv=cv)\nprint(\"*\"*NBR_STAR+\"\\n LogisticRegression on bag of word - cross-validation f1_score: {:.5f}\\n\".format(np.mean(scores))+\"*\"*NBR_STAR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction from term frequency-inverse document frequency\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(#max_df=0.1,         # drop words that occur in more than X percent of documents\n                             min_df=10,      # only use words that appear at least X times\n                             stop_words='english', # remove stop words\n                             lowercase=True, # Convert everything to lower case \n                             use_idf=True,   # Use idf\n                             norm=u'l2',     # Normalization\n                             smooth_idf=True, # Prevents divide-by-zero errors\n                             ngram_range=(1,3)\n                            )\nX_train = vect.fit_transform(X['text'])\n# find maximum value for each of the features over dataset:\nmax_value = X_train.max(axis=0).toarray().ravel()\nsorted_by_tfidf = max_value.argsort()\n# get feature names\nfeature_names = np.array(vect.get_feature_names())\n\nprint(\"Features with lowest tfidf:\\n{}\".format(\n      feature_names[sorted_by_tfidf[:20]]))\n\nprint(\"Features with highest tfidf: \\n{}\".format(\n      feature_names[sorted_by_tfidf[-20:]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(LogisticRegression(), X_train, y,scoring=\"f1\", cv=cv)\nprint(\"*\"*NBR_STAR+\"\\n LogisticRegression with tfidf - cross-validation f1_score: {:.5f}\\n\".format(np.mean(scores))+\"*\"*NBR_STAR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" \n# Visualize the largest (most positive) and smallest (most negative)  n_top_features coefficients of the logistic regression\n<span>&#171;</span>\nI have found this visualization in this book : **\"Introduction to Machine Learning with Python\" by [Andreas Mueller](http://amueller.io) and [Sarah Guido](https://twitter.com/sarah_guido).**\n<span>&#187;</span>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib.colors import ListedColormap, colorConverter, LinearSegmentedColormap\n\ndef visualize_coefficients(coefficients, feature_names, n_top_features=25):\n    coefficients = coefficients.squeeze()\n    if coefficients.ndim > 1:\n        # this is not a row or column vector\n        raise ValueError(\"coeffients must be 1d array or column vector, got\"\n                         \" shape {}\".format(coefficients.shape))\n    coefficients = coefficients.ravel()\n\n    if len(coefficients) != len(feature_names):\n        raise ValueError(\"Number of coefficients {} doesn't match number of\"\n                         \"feature names {}.\".format(len(coefficients),\n                                                    len(feature_names)))\n    # get coefficients with large absolute values\n    coef = coefficients.ravel()\n    positive_coefficients = np.argsort(coef)[-n_top_features:]\n    negative_coefficients = np.argsort(coef)[:n_top_features]\n    interesting_coefficients = np.hstack([negative_coefficients,\n                                          positive_coefficients])\n    # plot them\n    plt.figure(figsize=(20, 7))\n    cm = ListedColormap(['#0000aa', '#ff2020'])\n    colors = [cm(1) if c < 0 else cm(0)\n              for c in coef[interesting_coefficients]]\n    plt.bar(np.arange(2 * n_top_features), coef[interesting_coefficients],\n            color=colors)\n    feature_names = np.array(feature_names)\n    plt.subplots_adjust(bottom=0.3)\n    plt.xticks(np.arange(1, 1 + 2 * n_top_features),\n               feature_names[interesting_coefficients], rotation=60,\n               ha=\"right\")\n    plt.ylabel(\"Coefficient magnitude\")\n    plt.xlabel(\"Words\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, y)\ny_predict = logreg.predict(X_train)\nprint(\"*\"*NBR_STAR+\"\\n LogisticRegression with tfidf, no cross validation f1_score: {:.5f}\\n\".format(f1_score(y, y_predict, average='weighted'))+\"*\"*NBR_STAR)\nvisualize_coefficients(logreg.coef_, feature_names, n_top_features=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build an MLP model with (1,2) n_grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras import models\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import Dropout\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts, val_texts, train_labels , val_labels = train_test_split(\n    X['text'].values, X[\"target\"].values, test_size=0.10, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vectorize train and val texts."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n                             min_df=2,      # only use words that appear at least X times\n                             #stop_words='english', # remove stop words\n                             #lowercase=True, # Convert everything to lower case \n                             use_idf=True,   # Use idf\n                             norm=u'l2',     # Normalization\n                             smooth_idf=True, # Prevents divide-by-zero errors\n                             ngram_range=(1,3),\n                             #dtype='int32',\n                             analyzer='word',\n                             strip_accents = 'unicode',\n                             decode_error = 'replace'\n                            )\nx_train = vectorizer.fit_transform(train_texts)\nx_val = vectorizer.transform(val_texts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Select top 'k' of the vectorized features. top_k = 10000"},{"metadata":{"trusted":true},"cell_type":"code","source":"selector = SelectKBest(f_classif, k=min(10000, x_train.shape[1]))\nselector.fit(x_train, train_labels)\nx_train = selector.transform(x_train)\nx_val = selector.transform(x_val)\n\nx_train = x_train.astype('float32')\nx_val = x_val.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model parameters\nlearning_rate=1e-4\nepochs=1000\nbatch_size=128\nlayers=2\nunits=64\ndropout_rate=0.2\n\nmodel = models.Sequential()\nmodel.add(Dropout(rate=dropout_rate, input_shape=x_train.shape[1:]))\n\nfor _ in range(layers-1):\n    model.add(Dense(units=units, activation='relu'))\n    model.add(Dropout(rate=dropout_rate))\n\nmodel.add(Dense(units=1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = 'binary_crossentropy'\noptimizer = tf.keras.optimizers.Adam(lr=learning_rate)\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n\n# Create callback for early stopping on validation loss. If the loss does\n# not decrease in two consecutive tries, stop training.\ncallbacks = [tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2)]\n\n# Train and validate model.\nhistory = model.fit(\n        x_train.toarray(),\n        train_labels,\n        epochs=epochs,\n        callbacks=callbacks,\n        validation_data=(x_val.toarray(), val_labels),\n        verbose=0,  # Logs once per epoch.\n        batch_size=batch_size)\n\n# Print results.\nhistory = history.history\nprint('Validation accuracy: {acc}, loss: {loss}'.format(\n        acc=history['val_acc'][-1], loss=history['val_loss'][-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### F1_SCORE"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_all = vectorizer.transform(X['text'].values)\nx_all = selector.transform(x_all)\ny_predict = model.predict_classes(x_all.toarray())\nfrom sklearn.metrics import f1_score\n\nscore = f1_score(y, y_predict, average='weighted')\nprint(\"*\"*NBR_STAR+\"\\n MLP Model f1_score: {:.5f}\\n\".format(score)+\"*\"*NBR_STAR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Improve (I hope so...) prediction with column keyword\nThe idea is simple, for instance, in train data, if the keyword is \"wreckage\", this is always a disaster, so we can force the prediction to 1 ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predict[X.loc[X['keyword'].isin(list(keywords_disaster.index) )].index]=1\ny_predict[X.loc[X['keyword'].isin(list(keywords_no_disaster.index) )].index]=0\nscore = f1_score(y, y_predict, average='weighted')\nprint(\"*\"*NBR_STAR+\"\\n MLP Model f1_score: {:.5f}\\n\".format(score)+\"*\"*NBR_STAR)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\ntest_all = vectorizer.transform(test['text'].values)\ntest_all = selector.transform(test_all)\n\ny_predict = model.predict_classes(test_all.toarray())\ny_predict[test.loc[test['keyword'].isin(list(keywords_disaster.index) )].index]=1\ny_predict[test.loc[test['keyword'].isin(list(keywords_no_disaster.index) )].index]=0\n\nsample_submission[\"target\"] = y_predict\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><b><span>&#171;</span> please don't forget to upvote, that will keep me motivated <span>&#187;</span></b></div> \n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}