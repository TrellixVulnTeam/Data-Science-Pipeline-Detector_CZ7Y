{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom time import time\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T17:16:54.310728Z","iopub.execute_input":"2022-02-08T17:16:54.311041Z","iopub.status.idle":"2022-02-08T17:16:54.321083Z","shell.execute_reply.started":"2022-02-08T17:16:54.311012Z","shell.execute_reply":"2022-02-08T17:16:54.320178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### load datasets\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:16:54.323287Z","iopub.execute_input":"2022-02-08T17:16:54.324001Z","iopub.status.idle":"2022-02-08T17:16:54.343129Z","shell.execute_reply.started":"2022-02-08T17:16:54.323956Z","shell.execute_reply":"2022-02-08T17:16:54.34247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### read data\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:16:54.344409Z","iopub.execute_input":"2022-02-08T17:16:54.344841Z","iopub.status.idle":"2022-02-08T17:16:54.381633Z","shell.execute_reply.started":"2022-02-08T17:16:54.34481Z","shell.execute_reply":"2022-02-08T17:16:54.380703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:16:54.383045Z","iopub.execute_input":"2022-02-08T17:16:54.383254Z","iopub.status.idle":"2022-02-08T17:16:54.395791Z","shell.execute_reply.started":"2022-02-08T17:16:54.38323Z","shell.execute_reply":"2022-02-08T17:16:54.394903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x='target',data=train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:16:54.398441Z","iopub.execute_input":"2022-02-08T17:16:54.399182Z","iopub.status.idle":"2022-02-08T17:16:54.535902Z","shell.execute_reply.started":"2022-02-08T17:16:54.399147Z","shell.execute_reply":"2022-02-08T17:16:54.535056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['cleaned_tweet'] = train['text'].replace(r'\\'|\\\"|\\,|\\.|\\?|\\+|\\-|\\/|\\=|\\(|\\)|\\n|\"', '', regex=True)\ntrain['cleaned_tweet'] = train['cleaned_tweet'].replace(\"  \", \" \")","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:16:54.537149Z","iopub.execute_input":"2022-02-08T17:16:54.537697Z","iopub.status.idle":"2022-02-08T17:16:54.568827Z","shell.execute_reply.started":"2022-02-08T17:16:54.537656Z","shell.execute_reply":"2022-02-08T17:16:54.568047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clean Tweets Function","metadata":{}},{"cell_type":"code","source":"def cleantext(df, words_to_remove): \n    ### dont change the original tweet\n    # remove emoticons form the tweets\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'<ed>','', regex = True)\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'\\B<U+.*>|<U+.*>\\B|<U+.*>','', regex = True)\n    \n    # convert tweets to lowercase\n    df['cleaned_tweet'] = df['cleaned_tweet'].str.lower()\n    \n    #remove user mentions\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^(@\\w+)',\"\", regex=True)\n    \n    #remove 'rt' in the beginning\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^(rt @)',\"\", regex=True)\n    \n    #remove_symbols\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'[^a-zA-Z0-9]', \" \", regex=True)\n\n    #remove punctuations \n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'[[]!\"#$%\\'()\\*+,-./:;<=>?^_`{|}] +',\"\", regex = True)\n\n    #remove_URL(x):\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'https.*$', \"\", regex = True)\n\n    #remove 'amp' in the text\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'amp',\"\", regex = True)\n    \n    #remove words of length 1 or 2 \n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'\\b[a-zA-Z]{1,2}\\b','', regex=True)\n\n    #remove extra spaces in the tweet\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^\\s+|\\s+$',\" \", regex=True)\n    return df\n    #remove stopwords and words_to_remove\n#     stop_words = set(stopwords.words('english'))\n#     mystopwords = [stop_words, \"via\", words_to_remove]\n    \n#     df['fully_cleaned_tweet'] = df['cleaned_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in mystopwords]))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:16:54.570224Z","iopub.execute_input":"2022-02-08T17:16:54.571089Z","iopub.status.idle":"2022-02-08T17:16:54.581299Z","shell.execute_reply.started":"2022-02-08T17:16:54.571044Z","shell.execute_reply":"2022-02-08T17:16:54.580539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_base = cleantext(train,'null')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:16:54.582501Z","iopub.execute_input":"2022-02-08T17:16:54.582899Z","iopub.status.idle":"2022-02-08T17:16:54.847617Z","shell.execute_reply.started":"2022-02-08T17:16:54.58287Z","shell.execute_reply":"2022-02-08T17:16:54.846728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Removing stopwords ####\nstop_words = set(stopwords.words('english'))\ntrain_base['fully_cleaned_tweet'] = train_base['cleaned_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n#train_base['fully_cleaned_text'] = train_base['fully_cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in words_remove]))\n  ","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:16:54.848773Z","iopub.execute_input":"2022-02-08T17:16:54.848997Z","iopub.status.idle":"2022-02-08T17:16:54.876923Z","shell.execute_reply.started":"2022-02-08T17:16:54.848969Z","shell.execute_reply":"2022-02-08T17:16:54.876148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_base.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:16:54.878Z","iopub.execute_input":"2022-02-08T17:16:54.878241Z","iopub.status.idle":"2022-02-08T17:16:54.890822Z","shell.execute_reply.started":"2022-02-08T17:16:54.878206Z","shell.execute_reply":"2022-02-08T17:16:54.890078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Cloud","metadata":{}},{"cell_type":"code","source":"### Disaster Tweets wordcloud ####\nneg_tweets = train_base[train_base.target == 1]\nneg_string = []\nfor t in neg_tweets.fully_cleaned_tweet:\n    neg_string.append(t)\nneg_string = pd.Series(neg_string).str.cat(sep=' ')\nwordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:16:54.891948Z","iopub.execute_input":"2022-02-08T17:16:54.892159Z","iopub.status.idle":"2022-02-08T17:16:57.83283Z","shell.execute_reply.started":"2022-02-08T17:16:54.892132Z","shell.execute_reply":"2022-02-08T17:16:57.830434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Positive tweets wordcloud ###\npost_tweets = train_base[train_base.target == 0]\npos_string = []\nfor t in post_tweets.fully_cleaned_tweet:\n    pos_string.append(t)\npos_string = pd.Series(pos_string).str.cat(sep=' ')\nwordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(pos_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:16:57.834472Z","iopub.execute_input":"2022-02-08T17:16:57.835376Z","iopub.status.idle":"2022-02-08T17:17:00.860389Z","shell.execute_reply.started":"2022-02-08T17:16:57.835321Z","shell.execute_reply":"2022-02-08T17:17:00.859422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Tokenizer and stemmer\ndef text_process(text,stem=False):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Tokenizes and removes punctuation\n    3. Stems\n    4. Returns a list of the cleaned text\n    \"\"\"\n\n    # tokenizing\n    tokenizer = TweetTokenizer(r'\\w+')\n    text_processed=tokenizer.tokenize(text)\n    \n    \n    # steming\n    if stem:\n        porter_stemmer = PorterStemmer()\n        text_processed = [porter_stemmer.stem(word) for word in text_processed]\n    \n\n    return text_processed","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:17:00.861767Z","iopub.execute_input":"2022-02-08T17:17:00.862469Z","iopub.status.idle":"2022-02-08T17:17:00.868769Z","shell.execute_reply.started":"2022-02-08T17:17:00.862423Z","shell.execute_reply":"2022-02-08T17:17:00.867914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_base['tokenized_stemmed_tweet']=train_base['fully_cleaned_tweet'].apply(lambda x: text_process(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:17:00.872176Z","iopub.execute_input":"2022-02-08T17:17:00.872501Z","iopub.status.idle":"2022-02-08T17:17:01.11223Z","shell.execute_reply.started":"2022-02-08T17:17:00.872458Z","shell.execute_reply":"2022-02-08T17:17:01.111582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#if a word has a digit, remove that word\ntrain_base['tokenized_stemmed_tweet_1'] = train_base['tokenized_stemmed_tweet'].apply(lambda x: [y for y in x if not any(c.isdigit() for c in y)])","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:17:01.113529Z","iopub.execute_input":"2022-02-08T17:17:01.114414Z","iopub.status.idle":"2022-02-08T17:17:01.193927Z","shell.execute_reply.started":"2022-02-08T17:17:01.114372Z","shell.execute_reply":"2022-02-08T17:17:01.192818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_base.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:17:01.195564Z","iopub.execute_input":"2022-02-08T17:17:01.196223Z","iopub.status.idle":"2022-02-08T17:17:01.214714Z","shell.execute_reply.started":"2022-02-08T17:17:01.196176Z","shell.execute_reply":"2022-02-08T17:17:01.213888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Run with major classifiers and comparison","metadata":{}},{"cell_type":"code","source":"#### Train test dataset\nx = train_base.fully_cleaned_tweet\ny = train_base.target\nSEED = 2000\nx_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size=.02, random_state=SEED)\nprint (\"Train set has total {0} entries with {1:.2f}% disaster tweets, {2:.2f}% positive tweets\".format(len(x_train),(len(x_train[y_train == 1]) / (len(x_train)*1.))*100,(len(x_train[y_train == -0]) / (len(x_train)*1.))*100))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:17:01.216236Z","iopub.execute_input":"2022-02-08T17:17:01.216544Z","iopub.status.idle":"2022-02-08T17:17:01.232078Z","shell.execute_reply.started":"2022-02-08T17:17:01.216501Z","shell.execute_reply":"2022-02-08T17:17:01.23103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Classifiers\nnames = [\"Logistic Regression\", \"Linear SVC\", \"LinearSVC with L1-based feature selection\",\"Multinomial NB\", \n         \"Bernoulli NB\", \"Ridge Classifier\", \"AdaBoost\", \"Perceptron\",\"Passive-Aggresive\", \"Nearest Centroid\"]\nclassifiers = [\n    LogisticRegression(),\n    LinearSVC(),\n    Pipeline([\n  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n  ('classification', LinearSVC(penalty=\"l2\"))]),\n    MultinomialNB(),\n    BernoulliNB(),\n    RidgeClassifier(),\n    AdaBoostClassifier(),\n    Perceptron(),\n    PassiveAggressiveClassifier(),\n    NearestCentroid()\n    ]\nzipped_clf = zip(names,classifiers)\n\ntvec = TfidfVectorizer()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:17:01.233221Z","iopub.execute_input":"2022-02-08T17:17:01.234013Z","iopub.status.idle":"2022-02-08T17:17:01.240402Z","shell.execute_reply.started":"2022-02-08T17:17:01.233983Z","shell.execute_reply":"2022-02-08T17:17:01.239603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Helper functions\ndef accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n    else:\n        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n    t0 = time()\n    sentiment_fit = pipeline.fit(x_train, y_train)\n    y_pred = sentiment_fit.predict(x_test)\n    train_test_time = time() - t0\n    accuracy = accuracy_score(y_test, y_pred)\n    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n    if accuracy > null_accuracy:\n        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n    elif accuracy == null_accuracy:\n        print (\"model has the same accuracy with the null accuracy\")\n    else:\n        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n    print (\"train and test time: {0:.2f}s\".format(train_test_time))\n    print (\"-\"*80)\n    return accuracy, train_test_time\ndef classifier_comparator(vectorizer=tvec, n_features=10000, stop_words=None, ngram_range=(1, 1), classifier=zipped_clf):\n    result = []\n    vectorizer.set_params(stop_words=stop_words, max_features=n_features, ngram_range=ngram_range)\n    for n,c in classifier:\n        checker_pipeline = Pipeline([\n            ('vectorizer', vectorizer),\n            ('classifier', c)\n        ])\n        print (\"Validation result for {}\".format(n))\n        print (c)\n        clf_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n        result.append((n,clf_accuracy,tt_time))\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:17:01.24189Z","iopub.execute_input":"2022-02-08T17:17:01.242413Z","iopub.status.idle":"2022-02-08T17:17:01.26092Z","shell.execute_reply.started":"2022-02-08T17:17:01.242249Z","shell.execute_reply":"2022-02-08T17:17:01.26018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##3-gram comparison with different classifiers and tfidf vectorizer\ntrigram_result = classifier_comparator(n_features=100000,ngram_range=(1,3))","metadata":{"execution":{"iopub.status.busy":"2022-02-08T17:17:01.262251Z","iopub.execute_input":"2022-02-08T17:17:01.262537Z","iopub.status.idle":"2022-02-08T17:17:10.52765Z","shell.execute_reply.started":"2022-02-08T17:17:01.262505Z","shell.execute_reply":"2022-02-08T17:17:10.52681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}