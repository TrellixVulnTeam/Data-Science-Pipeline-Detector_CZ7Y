{"cells":[{"metadata":{},"cell_type":"markdown","source":"Edit:\nFinally TPU acceleration is enabled in Kaggle Notebooks !!! The difference is as much as 20x lower training time.\n\n**Background on tech that is used:\n**\n**DNN architecture:\n**\n> RoBERTa iterates on BERT's pretraining procedure, including training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data. See the associated paper for more details.\n\n**Implementation:**\n\nTensorflow implementation of Roberta pretrained for classification, provided by:\nhttps://github.com/huggingface/transformers"},{"metadata":{},"cell_type":"markdown","source":"We import the necessary libraries. "},{"metadata":{"colab":{},"colab_type":"code","id":"YrfVCobxH9Qy","trusted":true},"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold\nimport tensorflow as tf\n\nfrom transformers import RobertaTokenizer, RobertaConfig, TFRobertaPreTrainedModel\nfrom transformers.modeling_tf_roberta import TFRobertaMainLayer\nfrom transformers.modeling_tf_utils import get_initializer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These lines are used to connect and initialize with the TPU system "},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167},"colab_type":"code","id":"uQwMz31PH52m","outputId":"b82371bb-feea-490c-cf3b-d78c6c4e410a","trusted":true},"cell_type":"code","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This loads the pretrained tokenizers from Transformer library."},{"metadata":{"colab":{},"colab_type":"code","id":"lTODfRYSI5dL","trusted":true},"cell_type":"code","source":"MODEL_NAME = 'roberta-base'\nMAX_LEN = 128\ntokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\ndf_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv', dtype={'id': np.int16, 'target': np.int8})\ndf_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv', dtype={'id': np.int16, 'target': np.int8})\ndf_sample_submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We fix erroneous labels. Check kernel link given at the end to find out why they are erroneous."},{"metadata":{"trusted":true},"cell_type":"code","source":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ndf_train.at[df_train['id'].isin(ids_with_target_error),'target'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do entire encoding and tokenizing using the below function.\nEncoding formats the text supported by Roberta vocab and removes unwanted characters. Tokenizer creates an array from these encoded tokens. This is a numpy array and can be used for training. It is called input_ids. We also pad these input ids to a fixed length so that there is no variations in lenth between indivual rows."},{"metadata":{"colab":{},"colab_type":"code","id":"xe81i15aKDI6","trusted":true},"cell_type":"code","source":"def to_tokens(input_text, tokenizer):\n    output = tokenizer.encode_plus(input_text, max_length=MAX_LEN, pad_to_max_length=True)\n    return output\n\ndef select_field(features, field):\n    return [feature[field] for feature in features]","execution_count":null,"outputs":[]},{"metadata":{"colab":{},"colab_type":"code","id":"btdxv4neKdzh","trusted":true},"cell_type":"code","source":"tokenizer_output_train = df_train['text'].apply(lambda x: to_tokens(x, tokenizer))\ntokenizer_output_test = df_test['text'].apply(lambda x: to_tokens(x, tokenizer))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create and store the input ids as well as attention masks as an numpy array. The input ids are the numbers which are understood by the Neural Network."},{"metadata":{"colab":{},"colab_type":"code","id":"axQ3yJhpU75V","trusted":true},"cell_type":"code","source":"input_ids_train = np.array(select_field(tokenizer_output_train, 'input_ids'))\nattention_masks_train = np.array(select_field(tokenizer_output_train, 'attention_mask'))\n\ninput_ids_test = np.array(select_field(tokenizer_output_test, 'input_ids'))\nattention_masks_test = np.array(select_field(tokenizer_output_test, 'attention_mask'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we load the pretrained roberta model, create the optimizer, loss and metric functions. Since TPU support was added to Kaggle, we are using this by enclosing model.compile() in the scope of distribution strategy."},{"metadata":{"trusted":true},"cell_type":"code","source":" class CustomModel(TFRobertaPreTrainedModel):\n    def __init__(self, config, *inputs, **kwargs):\n        super(CustomModel, self).__init__(config, *inputs, **kwargs)\n        self.num_labels = config.num_labels\n        self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\n        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n        self.classifier = tf.keras.layers.Dense(units=config.num_labels,\n                                                name='classifier', \n                                                kernel_initializer=get_initializer(\n                                                    config.initializer_range))\n\n    def call(self, inputs, **kwargs):\n        outputs = self.roberta(inputs, **kwargs)\n        pooled_output = outputs[1]\n        pooled_output = self.dropout_1(pooled_output, training=kwargs.get('training', False))\n        logits = self.classifier(pooled_output)\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        return outputs","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":786},"colab_type":"code","id":"wyzbDMGhPV4G","outputId":"31990084-57e3-4d75-a1a1-15c45a9d356c","trusted":true},"cell_type":"code","source":"def init_model(model_name):\n    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n    with strategy.scope():\n        config = RobertaConfig.from_pretrained(model_name, num_labels=2)\n        model = CustomModel.from_pretrained(model_name)\n        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n        metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We convert the target labels to categorical one hot encoded format as the roberta model is by default configured to return a tensor with 2 labels (which is the minimum for classification). For training with TPUs, the batch size should be optimally 128. I had used the batch size of 64 because it is divisible by 8 which is recommended by GCP page on TPUs. The gpus are usually severely constrained by Video RAM which is quite low (~15.9GBs) for P100 gpus used by kaggle thus they form a bottleneck. We don't face such problems with TPUs."},{"metadata":{},"cell_type":"markdown","source":"TPUs can only work with data if their length is a multiple of the batch size. Since there are 7613 individual rows of texts we have to remove 61 rows of training data because 7613-61 = 7552 which is perfectly divisible by the batch size of 128. In other words, 128 * 59 = 7552. The ideal batch size for TPU training is 128 which also perfectly divides 7552.\n\nFor training with GPUs this is not required. Running the training in TPU takes ~20s per epoch (initially it takes more due to loading being a bottleneck). This is in contrast with a NVIDIA P100 GPU taking 10mins per epoch.\n\nBelow I am also using Stratified KFold."},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"colab_type":"code","id":"w-P6UeXcEas_","outputId":"ab3e5228-e92c-4f4e-933f-f3a5ae94c3ed","trusted":true},"cell_type":"code","source":"BATCH_SIZE = 128\nEPOCHS = 10\nSPLITS = 5\ncallbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n                                              patience=3, verbose=0, \n                                              restore_best_weights=True)]\nmodel_output = np.zeros((df_sample_submission['target'].shape[0], 2))\nskf = StratifiedKFold(n_splits=SPLITS, shuffle=False)\nX, y = input_ids_train, df_train['target'].values.reshape(-1, 1)\nskf.get_n_splits(X, y)\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\n    X_train, attention_masks_train_stratified = X[train_index], attention_masks_train[train_index]\n    X_test, attention_masks_test_stratified =  X[test_index], attention_masks_train[test_index]\n    y_train, y_test = tf.keras.utils.to_categorical(y[train_index]), tf.keras.utils.to_categorical(y[test_index])\n    X_train = X_train[:-divmod(X_train.shape[0], BATCH_SIZE)[1]]\n    attention_masks_train_stratified = attention_masks_train_stratified[:-divmod(attention_masks_train_stratified.shape[0], \n                                                                                 BATCH_SIZE)[1]]\n    y_train = y_train[:-divmod(y_train.shape[0], BATCH_SIZE)[1]]\n    model = init_model(MODEL_NAME)\n    if i == 0:\n        print(model.summary())\n    model.fit([X_train, attention_masks_train_stratified], y_train, \n              validation_data=([X_test, attention_masks_test_stratified], y_test), \n              batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks)\n    model_output += model.predict([input_ids_test, attention_masks_test])\n    del model\n    gc.collect()\n    print('='*22 + ' Split ' + str(i+1) + ' finished ' + '='*22)\nmodel_output /= SPLITS","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output is predicted and saved to submission csv file."},{"metadata":{"colab":{},"colab_type":"code","id":"v-wzCxaAmNwy","trusted":true},"cell_type":"code","source":"df_sample_submission['target'] = np.argmax(model_output, axis=1).flatten()\ndf_sample_submission['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"colab_type":"code","id":"bd5OBnJoS0m6","outputId":"f4e8cc19-764c-4828-b957-6a58653775e8","trusted":true},"cell_type":"code","source":"df_sample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks to [this](Thanks to https://www.kaggle.com/wrrosa/keras-bert-using-tfhub-modified-train-data) notebook for finding out flaws with some of the train data.\nSpecial thanks to Huggingface 🤗 for providing such a wonderful library and Kaggle team for bringing TPUs to notebooks."}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"name":"Text Classification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}