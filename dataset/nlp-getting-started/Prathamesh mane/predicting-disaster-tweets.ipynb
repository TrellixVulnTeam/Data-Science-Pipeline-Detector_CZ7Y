{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-27T07:33:49.255775Z","iopub.execute_input":"2021-08-27T07:33:49.256128Z","iopub.status.idle":"2021-08-27T07:33:49.26563Z","shell.execute_reply.started":"2021-08-27T07:33:49.256099Z","shell.execute_reply":"2021-08-27T07:33:49.264511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tweet Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Since we are dealing with tweets in this competation, we need to do specific tweet text cleaning along with normal text pre-processing. A tweet may contains\n* URL's\n* Mentions\n* Hashtags\n* Emojis\n* Specific words etc.\n\nTo clean the tweet, we can use a python library tweet-preprocessor instead of writing the cleaning logic ourself.","metadata":{}},{"cell_type":"markdown","source":"# Reading Datasets","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"../input/nlp-getting-started/test.csv\") ","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:49.267516Z","iopub.execute_input":"2021-08-27T07:33:49.267907Z","iopub.status.idle":"2021-08-27T07:33:49.322941Z","shell.execute_reply.started":"2021-08-27T07:33:49.267859Z","shell.execute_reply":"2021-08-27T07:33:49.321892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:49.325054Z","iopub.execute_input":"2021-08-27T07:33:49.325394Z","iopub.status.idle":"2021-08-27T07:33:49.34223Z","shell.execute_reply.started":"2021-08-27T07:33:49.325361Z","shell.execute_reply":"2021-08-27T07:33:49.340802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:49.344734Z","iopub.execute_input":"2021-08-27T07:33:49.34516Z","iopub.status.idle":"2021-08-27T07:33:49.358693Z","shell.execute_reply.started":"2021-08-27T07:33:49.345115Z","shell.execute_reply":"2021-08-27T07:33:49.357556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tweet-preprocessor","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:49.360223Z","iopub.execute_input":"2021-08-27T07:33:49.360949Z","iopub.status.idle":"2021-08-27T07:33:58.548421Z","shell.execute_reply.started":"2021-08-27T07:33:49.360906Z","shell.execute_reply":"2021-08-27T07:33:58.547256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.count()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:58.550064Z","iopub.execute_input":"2021-08-27T07:33:58.55042Z","iopub.status.idle":"2021-08-27T07:33:58.56493Z","shell.execute_reply.started":"2021-08-27T07:33:58.550373Z","shell.execute_reply":"2021-08-27T07:33:58.564174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Droppping duplicates and NaN from the DataFrame.","metadata":{}},{"cell_type":"code","source":"train_df = train_df.dropna()\ntrain_df = train_df.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:58.565996Z","iopub.execute_input":"2021-08-27T07:33:58.566278Z","iopub.status.idle":"2021-08-27T07:33:58.592962Z","shell.execute_reply.started":"2021-08-27T07:33:58.566236Z","shell.execute_reply":"2021-08-27T07:33:58.591864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.count()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:58.594602Z","iopub.execute_input":"2021-08-27T07:33:58.594913Z","iopub.status.idle":"2021-08-27T07:33:58.608254Z","shell.execute_reply.started":"2021-08-27T07:33:58.594884Z","shell.execute_reply":"2021-08-27T07:33:58.606363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:58.6111Z","iopub.execute_input":"2021-08-27T07:33:58.611376Z","iopub.status.idle":"2021-08-27T07:33:58.624633Z","shell.execute_reply.started":"2021-08-27T07:33:58.611351Z","shell.execute_reply":"2021-08-27T07:33:58.623425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Tweet Processing\n\nApply tweet preprocessing first. Define a process function and use pandas to apply it on each value of 'text'","metadata":{}},{"cell_type":"code","source":"import preprocessor as p\n\ndef preprocess_tweet(row):\n    text = row['text']\n    text = p.clean(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:58.626984Z","iopub.execute_input":"2021-08-27T07:33:58.627437Z","iopub.status.idle":"2021-08-27T07:33:58.654205Z","shell.execute_reply.started":"2021-08-27T07:33:58.62737Z","shell.execute_reply":"2021-08-27T07:33:58.65299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['text'] = train_df.apply(preprocess_tweet, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:58.655925Z","iopub.execute_input":"2021-08-27T07:33:58.656314Z","iopub.status.idle":"2021-08-27T07:33:59.526287Z","shell.execute_reply.started":"2021-08-27T07:33:58.656273Z","shell.execute_reply":"2021-08-27T07:33:59.525466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tweet has been cleaned to normal text.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:59.527744Z","iopub.execute_input":"2021-08-27T07:33:59.528237Z","iopub.status.idle":"2021-08-27T07:33:59.539994Z","shell.execute_reply.started":"2021-08-27T07:33:59.528205Z","shell.execute_reply":"2021-08-27T07:33:59.539317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normal Preprocessing\n\n### Now We can apply bormal text preprocessing like\n* Lowercasing\n* Punctuation Removal\n* Replace Extra white Spaces\n* Stopwords removal","metadata":{}},{"cell_type":"code","source":"from gensim.parsing.preprocessing import remove_stopwords\n\ndef stopword_removal(row):\n    text = row['text']\n    text = remove_stopwords(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:59.541324Z","iopub.execute_input":"2021-08-27T07:33:59.541897Z","iopub.status.idle":"2021-08-27T07:33:59.981574Z","shell.execute_reply.started":"2021-08-27T07:33:59.541864Z","shell.execute_reply":"2021-08-27T07:33:59.980675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['text'] = train_df.apply(stopword_removal, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:33:59.983112Z","iopub.execute_input":"2021-08-27T07:33:59.983611Z","iopub.status.idle":"2021-08-27T07:34:00.083863Z","shell.execute_reply.started":"2021-08-27T07:33:59.983568Z","shell.execute_reply":"2021-08-27T07:34:00.082732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:34:00.085514Z","iopub.execute_input":"2021-08-27T07:34:00.085955Z","iopub.status.idle":"2021-08-27T07:34:00.10044Z","shell.execute_reply.started":"2021-08-27T07:34:00.08591Z","shell.execute_reply":"2021-08-27T07:34:00.099146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove Extra white spaces, punctuation and apply lower casing","metadata":{}},{"cell_type":"code","source":"train_df['text'] = train_df['text'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:34:00.102076Z","iopub.execute_input":"2021-08-27T07:34:00.102492Z","iopub.status.idle":"2021-08-27T07:34:00.157028Z","shell.execute_reply.started":"2021-08-27T07:34:00.102458Z","shell.execute_reply":"2021-08-27T07:34:00.156089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:34:00.158263Z","iopub.execute_input":"2021-08-27T07:34:00.158576Z","iopub.status.idle":"2021-08-27T07:34:00.173309Z","shell.execute_reply.started":"2021-08-27T07:34:00.158545Z","shell.execute_reply":"2021-08-27T07:34:00.171994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now input tweet has been pre-processed and lets find features.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntweets = train_df['text']\nvectorizer = TfidfVectorizer(stop_words='english')\n\n# Learn vocabulary from sentences. \nvectorizer.fit(tweets)\n\n# Get vocabularies.\nvectorizer.vocabulary_","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:34:00.174985Z","iopub.execute_input":"2021-08-27T07:34:00.175396Z","iopub.status.idle":"2021-08-27T07:34:00.333303Z","shell.execute_reply.started":"2021-08-27T07:34:00.175356Z","shell.execute_reply":"2021-08-27T07:34:00.33215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nstopwords = stopwords.words('english')\n\nprint(stopwords)\n\ncount_vector = CountVectorizer(token_pattern = r'\\w{1,}', ngram_range = (1, 2), stop_words = stopwords)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:34:00.334621Z","iopub.execute_input":"2021-08-27T07:34:00.334906Z","iopub.status.idle":"2021-08-27T07:34:01.032804Z","shell.execute_reply.started":"2021-08-27T07:34:00.334879Z","shell.execute_reply":"2021-08-27T07:34:01.031996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = train_df.text\ny = train_df.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:34:01.034264Z","iopub.execute_input":"2021-08-27T07:34:01.034923Z","iopub.status.idle":"2021-08-27T07:34:01.044443Z","shell.execute_reply.started":"2021-08-27T07:34:01.034878Z","shell.execute_reply":"2021-08-27T07:34:01.043122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nclf = LogisticRegression()\npipe = Pipeline([\n    ('count_vector', CountVectorizer()),\n    ('clf', LogisticRegression())\n])\npipe.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:34:01.046335Z","iopub.execute_input":"2021-08-27T07:34:01.046845Z","iopub.status.idle":"2021-08-27T07:34:01.320529Z","shell.execute_reply.started":"2021-08-27T07:34:01.046796Z","shell.execute_reply":"2021-08-27T07:34:01.319699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\npredicted = pipe.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:34:01.3217Z","iopub.execute_input":"2021-08-27T07:34:01.322141Z","iopub.status.idle":"2021-08-27T07:34:01.337099Z","shell.execute_reply.started":"2021-08-27T07:34:01.322107Z","shell.execute_reply":"2021-08-27T07:34:01.336187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"accuracy :\", metrics.accuracy_score(predicted, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:34:01.338282Z","iopub.execute_input":"2021-08-27T07:34:01.338714Z","iopub.status.idle":"2021-08-27T07:34:01.346643Z","shell.execute_reply.started":"2021-08-27T07:34:01.338682Z","shell.execute_reply":"2021-08-27T07:34:01.345656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Storing Result","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsubmission['target'] = pipe.predict(test_df.text)\nsubmission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:34:01.348056Z","iopub.execute_input":"2021-08-27T07:34:01.348397Z","iopub.status.idle":"2021-08-27T07:34:01.45805Z","shell.execute_reply.started":"2021-08-27T07:34:01.348365Z","shell.execute_reply":"2021-08-27T07:34:01.456931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Result","metadata":{}},{"cell_type":"code","source":"submission.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-27T07:34:01.459581Z","iopub.execute_input":"2021-08-27T07:34:01.459986Z","iopub.status.idle":"2021-08-27T07:34:01.473042Z","shell.execute_reply.started":"2021-08-27T07:34:01.459943Z","shell.execute_reply":"2021-08-27T07:34:01.471642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}