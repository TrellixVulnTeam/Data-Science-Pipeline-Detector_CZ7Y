{"cells":[{"metadata":{},"cell_type":"markdown","source":"Disaster Tweet Classification -Logistic Regression and Naive Bayes score of Logistic Regression - 0.7932 "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imports:\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport math\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom matplotlib import rcParams\nfrom wordcloud import WordCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None) \npd.set_option('display.max_rows', None)  \npd.set_option('display.max_colwidth', -1) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Code:\n# reading the csv file into pandas dataframes\ndf=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a new column- length \n# this gives the length of the post\ndf['length'] = np.NaN\nfor i in range(0,len(df['text'])):\n    df['length'][i]=(len(df['text'][i]))\ndf.length = df.length.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating subplots to see distribution of length of tweet\nsns.set_style(\"darkgrid\");\nf, (ax1, ax2) = plt.subplots(figsize=(12,6),nrows=1, ncols=2,tight_layout=True);\nsns.distplot(df[df['target']==1][\"length\"],bins=30,ax=ax1);\nsns.distplot(df[df['target']==0][\"length\"],bins=30,ax=ax2);\nax1.set_title('\\n Distribution of length of tweet labelled Disaster\\n');\nax2.set_title('\\nDistribution of length of tweet labelled No Disaster\\n ');\nax1.set_ylabel('Frequency');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud for words related to Disaster \ntext=\" \".join(post for post in df[df['target']==1].text)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud for words related to No Disaster \ntext=\" \".join(post for post in df[df['target']==0].text)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to No Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating basline accuracy\ndf['target'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenizing\n\nWhen we \"tokenize\" data, we take it and split it up into distinct chunks based on some pattern."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Tokenizer\nfrom nltk.tokenize import RegexpTokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate Tokenizer\ntokenizer = RegexpTokenizer(r'\\w+') \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#changing the contents of selftext to lowercase\ndf.loc[:,'text'] = df.text.apply(lambda x : str.lower(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing hyper link, latin characters and digits\ndf['text']=df['text'].str.replace('http.*.*', '',regex = True)\ndf['text']=df['text'].str.replace('û.*.*', '',regex = True)\ndf['text']=df['text'].str.replace(r'\\d+','',regex= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"Run\" Tokenizer\ndf['tokens'] = df['text'].map(tokenizer.tokenize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#displaying first 5 rows of dataframe\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing Stop Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing English stopwords\nprint(stopwords.words(\"english\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#assigning stopwords to a variable\nstop = stopwords.words(\"english\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding this stop word to list of stopwords as it appears on frequently occuring word\nitem=['amp'] #'https','co','http','û','ûò','ûó','û_'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop.extend(item)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing stopwords from tokens\ndf['tokens']=df['tokens'].apply(lambda x: [item for item in x if item not in stop])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatizing \nWhen we \"lemmatize\" data, we take words and attempt to return their *lemma*, or the base/dictionary form of a word.<br>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing lemmatizer \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n# Instantiating lemmatizer \nlemmatizer = WordNetLemmatizer()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatize_words=[]\nfor i in range (len(df['tokens'])):\n    word=''\n    for j in range(len(df['tokens'][i])):\n        lemm_word=lemmatizer.lemmatize(df['tokens'][i][j])#lemmatize\n        \n        word=word + ' '+lemm_word # joining tokens into sentence    \n    lemmatize_words.append(word) # store in list\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a new column to store the result\ndf['lemmatized']=lemmatize_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#displaying first 5 rows of dataframe\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling \n---\n This step creates two models \n\n>1.Logistic Regression Model<br>\n>2.Naive Bayes Model<br>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#imports\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining X and y for the model\nX = df['lemmatized']\ny = df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spliting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#ensuring that the value counts are quite evenly distributed\ny_train.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pipeline will consist of two stages:\n# 1.Instantiating countVectorizer\n# 2.Instantiating logistic regression model\n\npipe = Pipeline([\n    ('cvec', CountVectorizer()),  \n    ('lr', LogisticRegression()) \n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuned_params = {\n    'cvec__max_features': [2500, 3000, 3500],\n    'cvec__min_df': [2,3],\n    'cvec__max_df': [.9, .95],\n    'cvec__ngram_range': [(1,1), (1,2)]\n}\ngs = GridSearchCV(pipe, param_grid=tuned_params, cv=3) # Evaluating model on unseen data\n\nmodel_lr=gs.fit(X_train, y_train) # Fitting model\n\n# This is the average of all cv folds for a single \n#combination of the parameters specified in the tuned_params\nprint(gs.best_score_) \n\n#displaying the best values of parameters\ngs.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\ngs.score(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\ngs.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating predictions!\npredictions_lr = model_lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the confusion matrix function\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating confusion matrix\nconfusion_matrix(y_test, predictions_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#interpreting confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_test, predictions_lr).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#values with coreesponding labels\nprint(\"True Negatives: %s\" % tn)\nprint(\"False Positives: %s\" % fp)\nprint(\"False Negatives: %s\" % fn)\nprint(\"True Positives: %s\" % tp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing model\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiating model\nnb = MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiating CountVectorizer.\ncvec = CountVectorizer(max_features = 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit_transform() fits the model and transforms training data into feature vectors\nX_train_cvec = cvec.fit_transform(X_train, y_train).todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tranform test data and convert into array\nX_test_cvec = cvec.transform(X_test).todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting model\nmodel_nb=nb.fit(X_train_cvec, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating predictions\npredictions_nb = model_nb.predict(X_test_cvec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training score\nmodel_nb.score(X_train_cvec, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test score\nmodel_nb.score(X_test_cvec, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating confusion matrix\nconfusion_matrix(y_test, predictions_nb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#interpreting confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_test, predictions_nb).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#values with coreesponding labels\nprint(\"True Negatives: %s\" % tn)\nprint(\"False Positives: %s\" % fp)\nprint(\"False Negatives: %s\" % fn)\nprint(\"True Positives: %s\" % tp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud for Frequntly occuring words related to Disaster\ntext=\" \".join(post for post in df[df['target']==1].lemmatized)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud for Frequntly occuring words related to No Disaster\ntext=\" \".join(post for post in df[df['target']==0].lemmatized)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to No Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TEST DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading the test data\ntest=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a new column- length \n# this gives the length of the post\ntest['length'] = np.NaN\nfor i in range(0,len(test['text'])):\n    test['length'][i]=(len(test['text'][i]))\ntest.length = test.length.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud for Frequntly occuring words in test dataframe\ntext=\" \".join(post for post in df.text)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words in test dataframe \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate Tokenizer\ntokenizer = RegexpTokenizer(r'\\w+')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#changing the contents of selftext to lowercase\ntest.loc[:,'text'] = test.text.apply(lambda x : str.lower(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing hyper link and latin characters\ntest['text']=test['text'].str.replace('http.*.*', '',regex = True)\ntest['text']=test['text'].str.replace('û.*.*', '',regex = True)\ntest['text']=test['text'].str.replace(r'\\d+','',regex= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"Run\" Tokenizer\ntest['tokens'] = test['text'].map(tokenizer.tokenize)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#displaying first 5 rows of dataframe\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing stopwords from tokens\ntest['tokens']=test['tokens'].apply(lambda x: [item for item in x if item not in stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatize_words=[]\nfor i in range (len(test['tokens'])):\n    word=''\n    for j in range(len(test['tokens'][i])):\n        lemm_word=lemmatizer.lemmatize(test['tokens'][i][j])#lemmatize\n        \n        word=word + ' '+lemm_word # joining tokens into sentence    \n    lemmatize_words.append(word) # store in list\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a new column to store the result\ntest['lemmatized']=lemmatize_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#displaying first 5 rows of dataframe\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word cloud for Frequntly occuring words in test dataframe after lemmatizing\ntext=\" \".join(post for post in df.lemmatized)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words in test dataframe \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_kaggle = model_lr.predict(test['lemmatized'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tranform test data and convert into array\nkaggle_cvec = cvec.transform(test['lemmatized']).todense()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_kaggle_nb=model_nb.predict(kaggle_cvec)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating  .csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating an empty data frame\nsubmission_kaggle = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigning values to the data frame-submission_kaggle\nsubmission_kaggle['Id'] = test.id\nsubmission_kaggle['target'] = predictions_kaggle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Head of submission_kaggle\nsubmission_kaggle.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving data as  final_kaggle.csv\nsubmission_kaggle.loc[ :].to_csv('final_kaggle.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NAIVE BAYES PREDICTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating an empty data frame\nsubmission_kaggle_nb = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assigning values to the data frame-submission_kaggle\nsubmission_kaggle_nb['Id'] = test.id\nsubmission_kaggle_nb['target'] = predictions_kaggle_nb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Head of submission_kaggle\nsubmission_kaggle_nb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving data as  final_kaggle.csv\nsubmission_kaggle_nb.loc[ :].to_csv('final_kaggle_nb.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}