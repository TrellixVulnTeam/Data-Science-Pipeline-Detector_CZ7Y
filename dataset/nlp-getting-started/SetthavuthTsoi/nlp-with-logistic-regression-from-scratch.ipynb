{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP with Logistic Regression from Scratch \n### Case Study: Disaster Tweet Identification\nIn this notebook, we will walk through Natural Language Processing with Classification and Vector Spaces. Today, we will predict real disaster from given tweet using Logistc Regression and basic text processing. Objective of this notebook is to get familiar with Logistic Regression and how we can do natural language processing. I hope that this notebook will be useful to people who start learning natural language processing and text classification. Let's get started :)","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:00:54.172932Z","iopub.execute_input":"2022-04-17T06:00:54.173309Z","iopub.status.idle":"2022-04-17T06:00:54.197181Z","shell.execute_reply.started":"2022-04-17T06:00:54.173214Z","shell.execute_reply":"2022-04-17T06:00:54.196468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, Let's start with exploring dataset ","metadata":{}},{"cell_type":"code","source":"KAGGLE_HOME = '../input/nlp-getting-started'\n\ntweets = pd.read_csv(os.path.join(KAGGLE_HOME, 'train.csv'))\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:00:54.200065Z","iopub.execute_input":"2022-04-17T06:00:54.20043Z","iopub.status.idle":"2022-04-17T06:00:54.280782Z","shell.execute_reply.started":"2022-04-17T06:00:54.200383Z","shell.execute_reply":"2022-04-17T06:00:54.280059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning and Text Processing\nBefore we get started, we will remove unnecessary part in tweet as below:\n- **Remove stopwords**\n- **Remove punctuation**\n- **Remove url**","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords')\n\nURL_PATTERN = '((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*'\nall_stopwords = stopwords.words('english')\n\ndef process_text(text):\n    # remove stopwords\n    remove_stop = ' '.join([word for word in text.split() if word not in all_stopwords])\n    remove_url = re.sub(URL_PATTERN, '', remove_stop)\n    remove_punc = re.sub(r'[^\\w\\s]', '', remove_url)\n    \n    return remove_punc.lower()\n\ntweets['processed_text'] = tweets['text'].apply(lambda text: process_text(text))\n\nprint(f'\\nBefore text processing: \\n{tweets.text[100]}')\nprint(f'\\nAfter text processing: \\n{process_text(tweets.text[100])}')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:00:54.282528Z","iopub.execute_input":"2022-04-17T06:00:54.282831Z","iopub.status.idle":"2022-04-17T06:00:56.823414Z","shell.execute_reply.started":"2022-04-17T06:00:54.282797Z","shell.execute_reply":"2022-04-17T06:00:56.822439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After removing stopwords, punctuation and url, we will make corpus and word frequency dictionaries for each class","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n\n# create generating word frequency dictionary\ndef create_word_index(string):\n    freq_dict = defaultdict(int)\n    \n    for word in string.split():\n        if word not in freq_dict:\n            freq_dict[word] = 1\n        else:\n            freq_dict[word] += 1\n    \n    return freq_dict\n\npositive_corpus = ' '.join(text for text in tweets[tweets['target'] == 1]['processed_text'])\nnegative_corpus = ' '.join(text for text in tweets[tweets['target'] == 0]['processed_text'])\n\npos_freq_dict = create_word_index(positive_corpus)\nneg_freq_dict = create_word_index(negative_corpus)\n\n# create map function to map sum of frequency of each tweets\ndef extract_features(freq_dict, tweet):\n    freq = 0\n    \n    for word in tweet.split():\n        freq += freq_dict[word]\n        \n    return freq\n\ntweets['pos_freq'] = tweets['processed_text'].apply(lambda tweet: extract_features(pos_freq_dict, tweet))\ntweets['neg_freq'] = tweets['processed_text'].apply(lambda tweet: extract_features(neg_freq_dict, tweet))\n\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:00:56.825373Z","iopub.execute_input":"2022-04-17T06:00:56.825706Z","iopub.status.idle":"2022-04-17T06:00:56.942304Z","shell.execute_reply.started":"2022-04-17T06:00:56.825659Z","shell.execute_reply":"2022-04-17T06:00:56.94125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split data to train set and validation set\nIn this section, we will split data into train and validation set with .8 split size.\n<br/> **Let's practice with our own split function !!!**","metadata":{}},{"cell_type":"code","source":"def split_train_test(features, labels, split_size):\n    train_size = int(len(features) * split_size)\n    \n    data = list(zip(features, labels))\n    shuffle_data = random.sample(data, len(data))\n    \n    shuffle_features = [feature for feature, label in shuffle_data]\n    shuffle_labels = [label for feature, label in shuffle_data]\n    \n    x_train = np.array(shuffle_features[:train_size])\n    y_train = np.array(shuffle_labels[:train_size]).reshape((len(shuffle_labels[:train_size]), 1))\n    \n    x_test = np.array(shuffle_features[train_size:])\n    y_test = np.array(shuffle_labels[train_size:]).reshape((len(shuffle_labels[train_size:]), 1))\n    \n    return x_train, x_test, y_train, y_test\n\n# split training and testing data to 80:20\n#tweets_text = tweets.copy().pop('processed_text').values\n#labels = tweets.copy().pop('target').values\n\n#X_train, X_test, y_train, y_test = split_train_test(tweets_text, labels, 0.8)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:00:56.944329Z","iopub.execute_input":"2022-04-17T06:00:56.944582Z","iopub.status.idle":"2022-04-17T06:00:56.952781Z","shell.execute_reply.started":"2022-04-17T06:00:56.944551Z","shell.execute_reply":"2022-04-17T06:00:56.951901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementing Logistic Regression from scratch\nIn this section, we will implement **Logistic Regression** to predict whether tweet is real disaster or not. However, we will not implement logistic regession using machine learning library, for example, scikit-learn and statsmodels. As we will learn to implement logistic regression from scratch, we will implement it by using custom logistic regression function.\n**<br/> Let's get started how we can implement logistic regression !!!**","metadata":{}},{"cell_type":"markdown","source":"First, we will explore the distribution of frequency features for both class through scatter plot.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# log transformation\n# tweets['log_pos_freq'] = np.log(tweets['pos_freq'])\n# tweets['log_neg_freq'] = np.log(tweets['neg_freq'])\n\n# normalization\ntweets['norm_pos_freq'] = (tweets['pos_freq'] - tweets['pos_freq'].mean()) / tweets['pos_freq'].std()\ntweets['norm_neg_freq'] = (tweets['neg_freq'] - tweets['neg_freq'].mean()) / tweets['neg_freq'].std()\n\nfig, axes = plt.subplots(ncols = 2, figsize = (15, 5))\nsns.scatterplot(\n    x = 'pos_freq', y = 'neg_freq', hue = 'target', data = tweets, \n    ax = axes[0], alpha = 0.1\n)\n\nsns.scatterplot(\n    x = 'norm_pos_freq', y = 'norm_neg_freq', hue = 'target', data = tweets, \n    ax = axes[1], alpha = 0.1\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:00:56.95375Z","iopub.execute_input":"2022-04-17T06:00:56.954582Z","iopub.status.idle":"2022-04-17T06:00:58.605318Z","shell.execute_reply.started":"2022-04-17T06:00:56.954545Z","shell.execute_reply":"2022-04-17T06:00:58.604258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the frequency features have a large value, we have to normalize to reduce the value so our model could be trained efficiently","metadata":{}},{"cell_type":"markdown","source":"## Custom Logistic Regression\nIn this section, we will create custom Logistic Regression\n<br/> Five step of Logistic Regression\n<br/> **Step 1:** Initialize parameters\n<br/> **Step 2:** Classify/predict\n<br/> **Step 3:** Compute gradient\n<br/> **Step 4:** Update parameter\n<br/> **Step 5:** Compute loss\n<br/> After complete five steps, repeat step 2 through step 5 until we get minimize loss and maximum accuracy","metadata":{}},{"cell_type":"code","source":"# Custom logistic regression\nclass LogisticRegression:\n    def __init__(self):\n        weight = None\n        costs = []\n        accuracies = []\n        \n    def sigmoid(self, x):\n        output = 1 / (1 + np.exp(-x))\n        return output\n        \n    def compute_cost(self, y_pred, y):\n        error = (y *  np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n        return -np.mean(error)\n    \n    def compute_gradient(self, X, y, y_pred):\n        gradient = (1/len(X)) * np.dot(X.T, (y_pred - y))\n        return gradient\n    \n    def fit(self, X, y, epoch, learning_rate):\n        # initialize weight\n        self.weight = np.zeros((X.shape[1], 1))\n\n        # append cost hand accuracy istory\n        costs = []\n        accuracies= []\n        \n        for _ in tqdm(range(epoch)):\n            y_pred = self.sigmoid(np.dot(X, self.weight))\n            \n            cost = self.compute_cost(y_pred, y)\n            gradient = self.compute_gradient(X, y, y_pred)\n            \n            self.weight -= learning_rate * gradient \n            #self.bias -= learning_rate * gradient\n            \n            accuracy = self.score(X, y)\n            \n            costs.append(cost)\n            accuracies.append(accuracy)\n    \n            \n        self.costs = costs\n        self.accuracies = accuracies\n    \n    def predict(self, X):\n        y_pred = self.sigmoid(np.dot(X, self.weight))\n        return y_pred > 0.5\n    \n    def predict_prob(self, X):\n        y_prob = self.sigmoid(np.dot(X, self.weight))\n        return y_prob\n    \n    def score(self, X, y):\n        y_pred = self.predict(X)\n        return np.mean(y_pred == y)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:00:58.606659Z","iopub.execute_input":"2022-04-17T06:00:58.606973Z","iopub.status.idle":"2022-04-17T06:00:58.62431Z","shell.execute_reply.started":"2022-04-17T06:00:58.606931Z","shell.execute_reply":"2022-04-17T06:00:58.623469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Start Training Logistic Regression !!!","metadata":{}},{"cell_type":"code","source":"# Normalization\nEPOCH = 500\nLEARNING_RATE = 1\n\ntweets_text = tweets[['pos_freq', 'neg_freq']].values\n\n# compute mean and standard deviation of each column\nmean = np.mean(tweets_text, axis = 0)\nstd = np.std(tweets_text, axis = 0)\n\n# normalize feature (x - mean) / sd\ntweets_text = (tweets_text - mean) / std\nlabels = tweets.copy().pop('target').values\n\nX_train, X_test, y_train, y_test = split_train_test(tweets_text, labels, 0.8)\n\n# Add intercept term: [bias w_1 w_2]\nX_train = np.append(np.ones((X_train.shape[0], 1)), X_train, axis = 1)\nX_test = np.append(np.ones((X_test.shape[0], 1)), X_test, axis = 1)\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train, EPOCH, LEARNING_RATE)\n\nprint(f'Optimized parameters: \\n{log_reg.weight}')\nprint(f'Loss: {log_reg.costs[-1]}')\nprint(f'Test Accuracy: {log_reg.score(X_test, y_test)}')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:00:58.6256Z","iopub.execute_input":"2022-04-17T06:00:58.626053Z","iopub.status.idle":"2022-04-17T06:00:59.25917Z","shell.execute_reply.started":"2022-04-17T06:00:58.62602Z","shell.execute_reply":"2022-04-17T06:00:59.258098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols = 2, figsize = (15, 5))\n\nloss_history = log_reg.costs\naccuracy_history = log_reg.accuracies\n\naxes[0].plot(range(EPOCH), loss_history)\naxes[0].set_title('Loss')\n\naxes[1].plot(range(EPOCH), accuracy_history)\naxes[1].set_title('Accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:00:59.261103Z","iopub.execute_input":"2022-04-17T06:00:59.261708Z","iopub.status.idle":"2022-04-17T06:00:59.719963Z","shell.execute_reply.started":"2022-04-17T06:00:59.261655Z","shell.execute_reply":"2022-04-17T06:00:59.718866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Boundary\nIn this section, we will draw decision boundary to visualize the line that separate two classes","metadata":{}},{"cell_type":"code","source":"ax = sns.scatterplot(\n    x = 'norm_pos_freq', y = 'norm_neg_freq', hue = 'target',\n    data = tweets, alpha = 0.1\n)\n\nax.legend([1, 0])\nax.set_xlabel('Negative Frequency')\nax.set_ylabel('Positive Frequency')\nax.set_title('Decision Boundary')\n\n# theta_0 + theta_1 * pos_freq + theta_2 * neg_freq = 0\n# pos_freq = -(theta_0 + theta_2 * neg_freq) / theta_1\n\nx_bound = np.array([np.min(tweets_text[:, 1]), np.max(tweets_text[:, 1])])\ny_bound = -(log_reg.weight[0] + log_reg.weight[1] * x_bound) / log_reg.weight[2]\n\nsns.lineplot(\n    x = x_bound, y = y_bound, \n    color = 'orangered'\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:00:59.721498Z","iopub.execute_input":"2022-04-17T06:00:59.721837Z","iopub.status.idle":"2022-04-17T06:01:00.62554Z","shell.execute_reply.started":"2022-04-17T06:00:59.72179Z","shell.execute_reply":"2022-04-17T06:01:00.624418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tuning Learning Rate \nIn this notebook, I set learning rate to 1. Learning rate is the hyperparameter that control how much to change model parameters, so you can try different learning rate and select leaning rate that make loss level-off without a huge number of iterations.","metadata":{}},{"cell_type":"code","source":"learning_rates = {0.0001: [], 0.001: [], 0.01: [], 0.1: [], 1: []}\n\nfor lr in learning_rates.keys():\n    model = LogisticRegression()\n    model.fit(X_train, y_train, EPOCH, lr)\n    learning_rates[lr] += model.costs\n\n\nfor lr in learning_rates.keys():\n    plt.plot(range(EPOCH), learning_rates[lr])\n\nplt.title('Gradient Descent by learning rate')\nplt.xlabel('iteration')\nplt.ylabel('loss')\nplt.legend(learning_rates.keys())","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:01:00.629648Z","iopub.execute_input":"2022-04-17T06:01:00.630219Z","iopub.status.idle":"2022-04-17T06:01:03.366584Z","shell.execute_reply.started":"2022-04-17T06:01:00.630149Z","shell.execute_reply":"2022-04-17T06:01:03.365137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation and ROC Curve","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\n\ny_prob = log_reg.predict_prob(X_train)\n\nfpr, tpr, threshold = roc_curve(y_train, y_prob)\nroc_auc = roc_auc_score(y_train, y_prob)\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, label = \"AUC %.2f\" %roc_auc)\nplt.plot([0, 1], [0, 1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:01:03.36778Z","iopub.execute_input":"2022-04-17T06:01:03.36823Z","iopub.status.idle":"2022-04-17T06:01:03.67015Z","shell.execute_reply.started":"2022-04-17T06:01:03.368154Z","shell.execute_reply":"2022-04-17T06:01:03.66923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# F1 Score\n# 2 * (Precision * Recall) / (Precision + Recall)\n# Precision = tp / (tp + fp)\n\ndef f1_score(y, y_pred):\n    tp = np.sum((y == 1) & (y_pred == 1))\n    fp = np.sum((y == 0) & (y_pred == 1))\n    fn = np.sum((y == 1) & (y_pred == 0))\n    \n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    \n    f1 = 2 * (precision * recall) / (precision + recall)\n    \n    return f1\n\ny_pred = log_reg.predict(X_train)\ny_prob = log_reg.predict_prob(X_train)\nval_prob = log_reg.predict_prob(X_test)\n\n# print(pd.crosstab(np.array(y_train).reshape(-1), np.array(y_pred).reshape(-1)))\nprint(f'Training F1 Score: {f1_score(y_train, y_prob >= 0.4)}')\nprint(f'Validation F1 Score: {f1_score(y_test, val_prob >= 0.4)}')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:58:03.511374Z","iopub.execute_input":"2022-04-17T06:58:03.511693Z","iopub.status.idle":"2022-04-17T06:58:03.531736Z","shell.execute_reply.started":"2022-04-17T06:58:03.511659Z","shell.execute_reply":"2022-04-17T06:58:03.530646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission\nIn this section, we will process unseen data with predefined function.\n<br/> **Now we're done and ready to make a submission !!!**\n<br/> My submission show 73.58% accuracy on unseen data. Our custom model is not so good but not bad!!! \n<br/> I hope all beginners like me will enjoy learning my notebook. Thanks :)","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv(os.path.join(KAGGLE_HOME, 'test.csv'))\ntest_data['processed_tweets'] = test_data['text'].apply(lambda tweet: process_text(tweet))\ntest_data['pos_freq'] = test_data['processed_tweets'].apply(lambda tweet: extract_features(pos_freq_dict, tweet))\ntest_data['neg_freq'] = test_data['processed_tweets'].apply(lambda tweet: extract_features(neg_freq_dict, tweet))\n\nfeatures = test_data[['pos_freq', 'neg_freq']].values\n\n# normalize data\nfeatures_mean = np.mean(features, axis = 0)\nfeatures_std = np.std(features, axis = 0)\nfeatures = (features - features_mean) / features_std\nfeatures = np.append(np.ones((features.shape[0], 1)), features, axis = 1)\n\nsubmission_prediction = log_reg.predict_prob(features)\nsubmission_prediction = [1 if p >= 0.35 else 0 for p in submission_prediction]\ntest_data['target'] = submission_prediction\n\n#test_data[['id', 'target']].to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:50:30.170571Z","iopub.execute_input":"2022-04-17T06:50:30.171005Z","iopub.status.idle":"2022-04-17T06:50:30.453482Z","shell.execute_reply.started":"2022-04-17T06:50:30.170944Z","shell.execute_reply":"2022-04-17T06:50:30.452263Z"},"trusted":true},"execution_count":null,"outputs":[]}]}