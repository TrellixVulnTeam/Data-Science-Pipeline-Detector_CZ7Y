{"cells":[{"metadata":{},"cell_type":"markdown","source":"## TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python .\n"},{"metadata":{},"cell_type":"markdown","source":"### Creating TF-IDF Model from Scratch"},{"metadata":{},"cell_type":"markdown","source":"In this notebook I will explain how to implement tf-idf technique in python from scratch , this technique is used to find meaning of sentences consisting of words and cancels out the incapabilities of Bag of Words technique which is good for text classification or for helping a machine read words in numbers."},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents:\n- Terminology .\n- Term Frequency(TF) .\n- Document Frequency .\n- Inverse Document Frequency .\n- Implementation in Python ."},{"metadata":{},"cell_type":"markdown","source":"## 1 - Terminology :\n- t — term (word)\n- d — document (set of words)\n- N — count of corpus\n- corpus — the total document set"},{"metadata":{},"cell_type":"markdown","source":"## 2 -Term Frequency (TF):\nSuppose we have a set of English text documents and wish to rank which document is most relevant to the query , “Data Science is awesome !” A simple way to start out is by eliminating documents that do not contain all three words “Data”,”is”, “Science”, and “awesome”, but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document; the number of times a term occurs in a document is called its term frequency."},{"metadata":{},"cell_type":"markdown","source":"### The weight of a term that occurs in a document is simply proportional to the term frequency.\n# Formula :\n## tf(t,d) = count of t in d / number of words in d"},{"metadata":{},"cell_type":"markdown","source":"## 3 -Document Frequency :\n\nThis measures the importance of document in whole set of corpus, this is very similar to TF. The only difference is that TF is frequency counter for a term t in document d, where as DF is the count of occurrences of term t in the document set N. In other words, DF is the number of documents in which the word is present. We consider one occurrence if the term consists in the document at least once, we do not need to know the number of times the term is present.\n\n## df(t) = occurrence of t in documents"},{"metadata":{},"cell_type":"markdown","source":"## 4 -Inverse Document Frequency(IDF):\n\nWhile computing TF, all terms are considered equally important. However it is known that certain terms, such as “is”, “of”, and “that”, may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing IDF, an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\nIDF is the inverse of the document frequency which measures the informativeness of term t. When we calculate IDF, it will be very low for the most occurring words such as stop words (because stop words such as “is” is present in almost all of the documents, and N/df will give a very low value to that word). This finally gives what we want, a relative weightage.\n\n\n## idf(t) = N/df\n\n\nNow there are few other problems with the IDF , in case of a large corpus,say **100,000,000** , the **IDF** value explodes , to avoid the effect we take the log of idf .\nDuring the query time, when a word which is not in vocab occurs, the df will be **0**. As we cannot divide by 0, we smoothen the value by adding 1 to the denominator.\nthat’s the final formula:\n\n\n# Formula :\n\n## idf(t) = log(N/(df + 1))\n\n\n**tf-idf** now is a the right measure to evaluate how important a word is to a document in a collection or corpus.here are many different variations of TF-IDF but for now let us concentrate on the this basic version.\n# Formula :\n\n## tf-idf(t, d) = tf(t, d) * log(N/(df + 1))"},{"metadata":{},"cell_type":"markdown","source":"## 5 -Implementing TF-IDF in Python From Scratch :\n\nTo make ** TF-IDF** from scratch in python,let’s imagine those two sentences from diffrent document :\n\n- first_sentence : “Data Science is the sexiest job of the 21st century”.\n- second_sentence : “machine learning is the key for data science”.\n\nFirst step we have to create the TF function to calculate total word frequency for all documents. Here are the codes below:\nfirst as usual we should import the necessary libraries :\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport sklearn as sk\nimport math ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so let’s load our sentences and combine them together in a single set :"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"first_sentence = \"Data Science is the sexiest job of the 21st century\"\nsecond_sentence = \"machine learning is the key for data science\"\n#split so each word have their own string\nfirst_sentence = first_sentence.split(\" \")\nsecond_sentence = second_sentence.split(\" \")#join them to remove common duplicate words\ntotal= set(first_sentence).union(set(second_sentence))\nprint(total)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets add a way to count the words using a dictionary key-value pairing for both sentences :"},{"metadata":{"trusted":true},"cell_type":"code","source":"wordDictA = dict.fromkeys(total, 0) \nwordDictB = dict.fromkeys(total, 0)\nfor word in first_sentence:\n    wordDictA[word]+=1\n    \nfor word in second_sentence:\n    wordDictB[word]+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we put them in a dataframe and then view the result:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame([wordDictA, wordDictB])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No let’s writing the TF Function :"},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeTF(wordDict, doc):\n    tfDict = {}\n    corpusCount = len(doc)\n    for word, count in wordDict.items():\n        tfDict[word] = count/float(corpusCount)\n    return(tfDict)\n#running our sentences through the tf function:\ntfFirst = computeTF(wordDictA, first_sentence)\ntfSecond = computeTF(wordDictB, second_sentence)\n#Converting to dataframe for visualization\ntf = pd.DataFrame([tfFirst, tfSecond])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![hi](https://miro.medium.com/max/704/1*3WwlcabDKuIljpk2ec1r-w.png)"},{"metadata":{},"cell_type":"markdown","source":"That’s all for TF formula , just i wanna talk about stop words that we should eliminate them because they are the most commonly occurring words which don’t give any additional value to the document vector .\nin-fact removing these will increase computation and space efficiency.\n\nnltk library has a method to download the stopwords, so instead of explicitly mentioning all the stopwords ourselves we can just use the nltk library and iterate over all the words and remove the stop words. There are many efficient ways to do this, but ill just give a simple method.\nthose a sample of a stopwords in english language :\n"},{"metadata":{},"cell_type":"markdown","source":"![StopWords](https://miro.medium.com/max/850/1*R1NmayfziRv8QKKUw4dt_w.png)"},{"metadata":{},"cell_type":"markdown","source":"and this is a simple code to download stop words and removing them ."},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nset(stopwords.words('english'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_sentence = []\nfor word in wordDictA:\n    if str(word) not in set(stopwords.words('english')):\n        filtered_sentence.append(word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_sentence","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And now that we finished the TF section, we move onto the IDF part:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeIDF(docList):\n    idfDict = {}\n    N = len(docList)\n    \n    idfDict = dict.fromkeys(docList[0].keys(), 0)\n    for word, val in idfDict.items():\n        idfDict[word] = math.log10(N / (float(val) + 1))\n        \n    return(idfDict)\n#inputing our sentences in the log file\nidfs = computeIDF([wordDictA, wordDictB])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and now we implement the idf formula , let’s finish with calculating the TFI-DF"},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeTFIDF(tfBow, idfs):\n    tfidf = {}\n    for word, val in tfBow.items():\n        tfidf[word] = val*idfs[word]\n    return(tfidf)\n#running our two sentences through the IDF:\nidfFirst = computeTFIDF(tfFirst, idfs)\nidfSecond = computeTFIDF(tfSecond, idfs)\n#putting it in a dataframe\nidf= pd.DataFrame([idfFirst, idfSecond])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That was a lot of work. But it is handy to know, if you are asked to code TF-IDF from scratch in the future. However, this can be done a lot simpler thanks to sklearn library. Let’s look at the example from them below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#first step is to import the library\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#for the sentence, make sure all words are lowercase or you will run #into error. for simplicity, I just made the same sentence all #lowercase\nfirstV= \"Data Science is the sexiest job of the 21st century\"\nsecondV= \"machine learning is the key for data science\"\n#calling the TfidfVectorizer\nvectorize= TfidfVectorizer()\n#fitting the model and passing our sentences right away:\nresponse= vectorize.fit_transform([firstV, secondV])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"and that’s the expected output :"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(response)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary :\n\nIn this kernel we are going to explain how to use python and a natural language processing (NLP) technique known as Term Frequency — Inverse Document Frequency (tf-idf) to summarize documents.\nWe’ll areusing sklearn along with nltk to accomplish this task.\nRemember that you can find the fully working code in my github repository [here](https://github.com/Yassine-Hamdaoui/Tf-Idf).\nThanks for reading and I will be glad to discuss any questions or corrections you may have :) Find me on [LinkedIn](https://www.linkedin.com/in/yassine-hamdaoui/) if you want to discuss Machine Learning or anything else.\n\n# I hope you find this kernel useful and enjoyable.\n# Your comments and feedback are most welcome.\n# Upvote if you liked it and found useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}