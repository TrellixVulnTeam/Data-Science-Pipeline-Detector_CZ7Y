{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-04T10:49:03.412582Z","iopub.execute_input":"2022-01-04T10:49:03.413163Z","iopub.status.idle":"2022-01-04T10:49:09.587135Z","shell.execute_reply.started":"2022-01-04T10:49:03.413041Z","shell.execute_reply":"2022-01-04T10:49:09.585704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:49:09.590202Z","iopub.execute_input":"2022-01-04T10:49:09.591076Z","iopub.status.idle":"2022-01-04T10:49:14.858759Z","shell.execute_reply.started":"2022-01-04T10:49:09.591018Z","shell.execute_reply":"2022-01-04T10:49:14.858063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel, AutoTokenizer,TFAutoModel\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:49:14.86014Z","iopub.execute_input":"2022-01-04T10:49:14.861181Z","iopub.status.idle":"2022-01-04T10:49:17.036366Z","shell.execute_reply.started":"2022-01-04T10:49:14.861133Z","shell.execute_reply":"2022-01-04T10:49:17.035324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_roBerta ='joeddav/xlm-roberta-large-xnli'\nmodel_Bert = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_Bert)\nmodel = TFBertModel.from_pretrained(model_Bert)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:49:17.037773Z","iopub.execute_input":"2022-01-04T10:49:17.038761Z","iopub.status.idle":"2022-01-04T10:49:55.872432Z","shell.execute_reply.started":"2022-01-04T10:49:17.038718Z","shell.execute_reply":"2022-01-04T10:49:55.871632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:49:55.874873Z","iopub.execute_input":"2022-01-04T10:49:55.875443Z","iopub.status.idle":"2022-01-04T10:49:55.927396Z","shell.execute_reply.started":"2022-01-04T10:49:55.875403Z","shell.execute_reply":"2022-01-04T10:49:55.9263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[10:]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:49:55.928817Z","iopub.execute_input":"2022-01-04T10:49:55.929613Z","iopub.status.idle":"2022-01-04T10:49:55.958796Z","shell.execute_reply.started":"2022-01-04T10:49:55.929571Z","shell.execute_reply":"2022-01-04T10:49:55.958027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = pd.DataFrame(train, columns = {'id','text'})\n#y_train = pd.DataFrame(train, columns = {'target'})\ny_train = train[['target']]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:49:55.960107Z","iopub.execute_input":"2022-01-04T10:49:55.961153Z","iopub.status.idle":"2022-01-04T10:49:55.971577Z","shell.execute_reply.started":"2022-01-04T10:49:55.961101Z","shell.execute_reply":"2022-01-04T10:49:55.970366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\ndef remove_noise(text):\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('ûò', '', text)\n    return text\nx_train.text = x_train.text.apply(lambda x: remove_noise(x))\nx_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:49:55.973372Z","iopub.execute_input":"2022-01-04T10:49:55.973768Z","iopub.status.idle":"2022-01-04T10:49:56.292496Z","shell.execute_reply.started":"2022-01-04T10:49:55.973712Z","shell.execute_reply":"2022-01-04T10:49:56.290758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEQ_LEN = 100 #len(x_train['text'])  # 236#max(train.astype('str').applymap(lambda x: len(x)).max())\n\ndef bert_encode(df, tokenizer):    \n    batch_tweets = df['text'].tolist()\n    \n    tokens = tokenizer(batch_tweets, max_length = SEQ_LEN,\n                   truncation=True, padding='max_length',\n                   add_special_tokens=True, return_attention_mask=True,\n                   return_token_type_ids=True, #only for BERT\n                   return_tensors='tf')\n    #tokens['input_ids'] = tf.reshape(tokens['input_ids'], [7613, 10, 10])\n    #tokens['attention_mask'] = tf.reshape(tokens['attention_mask'], [7613, 10, 10])\n    #tokens['token_type_ids'] = tf.reshape(tokens['token_type_ids'], [7613, 10, 10])\n    inputs = {\n          'input_ids': tokens['input_ids'], \n          'attention_mask': tokens['attention_mask'],\n           'token_type_ids': tokens['token_type_ids']  \n    } #  only for BERT\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:49:56.29439Z","iopub.execute_input":"2022-01-04T10:49:56.2951Z","iopub.status.idle":"2022-01-04T10:49:56.307048Z","shell.execute_reply.started":"2022-01-04T10:49:56.295047Z","shell.execute_reply":"2022-01-04T10:49:56.305061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_input = bert_encode(x_train, tokenizer)\nx_train_input # = tf.reshape(x_train_input, [7613, 10, 10])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:49:56.310075Z","iopub.execute_input":"2022-01-04T10:49:56.310538Z","iopub.status.idle":"2022-01-04T10:50:00.618199Z","shell.execute_reply.started":"2022-01-04T10:49:56.310504Z","shell.execute_reply":"2022-01-04T10:50:00.617211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import regularizers\n\ndef build_model():   # hp\n    #FBertModel\n    encoder = TFBertModel.from_pretrained(model_Bert)\n    input_ids = tf.keras.Input(shape=([SEQ_LEN, ]), dtype=tf.int32, name = \"input_ids\")\n    attention_mask = tf.keras.Input(shape=([SEQ_LEN, ]), dtype=tf.int32, name = \"attention_mask\")\n    token_type_ids = tf.keras.Input(shape=([SEQ_LEN, ]), \n                                    dtype=tf.int32,  name = \"token_type_ids\") # only for BERT  \n        \n    embedding = encoder([input_ids, attention_mask, token_type_ids])[0] # [1] #  only for BERT\n    print(embedding)\n    inputs=[input_ids, attention_mask, token_type_ids] #   only for Bert\n    hp_units1 = 128 # hp.Int('Inits1', min_value = 32, max_value = 512, step = 32)\n    hp_units2 = 32 #hp.Int('Inits2', min_value = 32, max_value = 512, step = 32), kernel_regularizer=regularizers.l2(l2=1e-4)\n    x = tf.keras.layers.Conv1D(32, 7, activation=tf.nn.relu)(embedding)\n    x = tf.keras.layers.MaxPool1D(5)(x)\n    x = tf.keras.layers.Conv1D(32, 5, activation=tf.nn.relu)(x)\n    x = tf.keras.layers.GlobalMaxPool1D()(x) \n    #x = tf.keras.layers.Dense(units = hp_units2, activation=tf.nn.relu)(x)  #embedding[:,0,:]\n    output = tf.keras.layers.Dense(2, activation='sigmoid')(x)\n    model = tf.keras.Model(inputs=inputs, outputs=output)\n    hp_learning_rate = 1e-6 # hp.Choise('learning_rate', values = [1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]) hp_learning_ratesparse_ sparse_categorical\n    model.compile(tf.keras.optimizers.Adam(learning_rate = hp_learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])   \n    return model ","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:14:30.54074Z","iopub.execute_input":"2022-01-04T11:14:30.541222Z","iopub.status.idle":"2022-01-04T11:14:30.555741Z","shell.execute_reply.started":"2022-01-04T11:14:30.541171Z","shell.execute_reply":"2022-01-04T11:14:30.554585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope(): # defines the compute distribution policy for building the model. or in other words: makes sure that the model is created on the TPU/GPU/CPU, depending on to what the Accelerator is set in the Notebook Settings\n    model = build_model() # our model is being built\n    model.summary()       # let's look at some of its properties","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:14:30.557733Z","iopub.execute_input":"2022-01-04T11:14:30.558157Z","iopub.status.idle":"2022-01-04T11:14:52.119634Z","shell.execute_reply.started":"2022-01-04T11:14:30.55811Z","shell.execute_reply":"2022-01-04T11:14:52.11841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key in x_train_input.keys():\n    x_train_input[key] = x_train_input[key][:,:SEQ_LEN]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:14:52.121353Z","iopub.execute_input":"2022-01-04T11:14:52.121734Z","iopub.status.idle":"2022-01-04T11:14:52.130245Z","shell.execute_reply.started":"2022-01-04T11:14:52.121682Z","shell.execute_reply":"2022-01-04T11:14:52.129113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train_input, y_train, epochs = 20, batch_size=128, \n                    validation_split = 0.2) ","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:14:52.133378Z","iopub.execute_input":"2022-01-04T11:14:52.133756Z","iopub.status.idle":"2022-01-04T11:18:56.297213Z","shell.execute_reply.started":"2022-01-04T11:14:52.133709Z","shell.execute_reply":"2022-01-04T11:18:56.296205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:18:56.29873Z","iopub.execute_input":"2022-01-04T11:18:56.299019Z","iopub.status.idle":"2022-01-04T11:18:56.549552Z","shell.execute_reply.started":"2022-01-04T11:18:56.298978Z","shell.execute_reply":"2022-01-04T11:18:56.548497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.clf() #Очистить рисунок\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:18:56.551518Z","iopub.execute_input":"2022-01-04T11:18:56.551881Z","iopub.status.idle":"2022-01-04T11:18:56.814586Z","shell.execute_reply.started":"2022-01-04T11:18:56.551836Z","shell.execute_reply":"2022-01-04T11:18:56.813486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/nlp-getting-started/test.csv')\nx_test = pd.DataFrame(test, columns = {'id','text'})\nx_test.text = x_test.text.apply(lambda x: remove_noise(x))\nx_test.head()\nx_test_input = bert_encode(x_test, tokenizer)\nx_test_input","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:18:56.816051Z","iopub.execute_input":"2022-01-04T11:18:56.816317Z","iopub.status.idle":"2022-01-04T11:18:58.668775Z","shell.execute_reply.started":"2022-01-04T11:18:56.816281Z","shell.execute_reply":"2022-01-04T11:18:58.668042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key in x_test_input.keys():\n    x_test_input[key] = x_test_input[key][:,:SEQ_LEN]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:18:58.670036Z","iopub.execute_input":"2022-01-04T11:18:58.670608Z","iopub.status.idle":"2022-01-04T11:18:58.678179Z","shell.execute_reply.started":"2022-01-04T11:18:58.670572Z","shell.execute_reply":"2022-01-04T11:18:58.677316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = [np.argmax(i) for i in model.predict(x_test_input)]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:18:58.679697Z","iopub.execute_input":"2022-01-04T11:18:58.680008Z","iopub.status.idle":"2022-01-04T11:19:08.334883Z","shell.execute_reply.started":"2022-01-04T11:18:58.679973Z","shell.execute_reply":"2022-01-04T11:19:08.333755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test.id.copy().to_frame()\nsubmission['target'] = predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:19:08.337452Z","iopub.execute_input":"2022-01-04T11:19:08.337735Z","iopub.status.idle":"2022-01-04T11:19:08.348996Z","shell.execute_reply.started":"2022-01-04T11:19:08.3377Z","shell.execute_reply":"2022-01-04T11:19:08.346851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"./submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:19:08.35087Z","iopub.execute_input":"2022-01-04T11:19:08.351695Z","iopub.status.idle":"2022-01-04T11:19:08.369421Z","shell.execute_reply.started":"2022-01-04T11:19:08.351637Z","shell.execute_reply":"2022-01-04T11:19:08.36819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[submission.target == 1]","metadata":{"execution":{"iopub.status.busy":"2022-01-04T11:19:08.371355Z","iopub.execute_input":"2022-01-04T11:19:08.372165Z","iopub.status.idle":"2022-01-04T11:19:08.388176Z","shell.execute_reply.started":"2022-01-04T11:19:08.372117Z","shell.execute_reply":"2022-01-04T11:19:08.386933Z"},"trusted":true},"execution_count":null,"outputs":[]}]}