{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **<span style=\"color:#023e8a\"><center> ðŸ”¥Guided LDA. Semi-supervised TM.ðŸ”¥</center></span>**\n## **<center><span style=\"color:#FEF1FE;background-color:#023e8a;border-radius: 5px;padding: 5px\">If you find this notebook useful or interesting, please, support with an upvote :)</span></center>**\n\n## **<span style=\"color:#023e8a;font-size:1000%\"><center>NLP</center></span><span style=\"color:#023e8a;font-size:200%\"><center>Topic Modeling. Semi-supervised LDA.</center></span>**\n>**<span style=\"color:#023e8a;\">Hello everyone!</span>**  \n>**<span style=\"color:#023e8a;\">I hope that this notebook will be interesting and useful for you. Guided LDA gives more opportunities to work with topic comparing with original LDA.</span>**","metadata":{}},{"cell_type":"markdown","source":"# **<a id=\"Content\" style=\"color:#023e8a;\">Table of Content</a>**\n* [**<span style=\"color:#023e8a;\">1. Downloading data</span>**](#Downloading)  \n* [**<span style=\"color:#023e8a;\">2. Data prep and stemming</span>**](#Data)  \n* [**<span style=\"color:#023e8a;\">3. Modeling</span>**](#Modeling)   ","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import PorterStemmer\nfrom nltk import word_tokenize\nimport numpy as np\nfrom gensim.models.ldamulticore import LdaMulticore\nimport gensim\nfrom nltk.corpus import stopwords\nstops = stopwords.words(\"english\")\nimport re","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:51:41.260923Z","iopub.execute_input":"2022-02-05T10:51:41.26135Z","iopub.status.idle":"2022-02-05T10:51:41.267932Z","shell.execute_reply.started":"2022-02-05T10:51:41.261314Z","shell.execute_reply":"2022-02-05T10:51:41.267419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span id=\"Downloading\" style=\"color:#023e8a;\">1. Downloading data</span>**","metadata":{}},{"cell_type":"markdown","source":"[**<span style=\"color:#FEF1FE;background-color:#023e8a;border-radius: 5px;padding: 2px\">Go to Table of Content</span>**](#Content)","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:22:40.67488Z","iopub.execute_input":"2022-02-05T10:22:40.675468Z","iopub.status.idle":"2022-02-05T10:22:40.727857Z","shell.execute_reply.started":"2022-02-05T10:22:40.675416Z","shell.execute_reply":"2022-02-05T10:22:40.726817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:22:45.444926Z","iopub.execute_input":"2022-02-05T10:22:45.445257Z","iopub.status.idle":"2022-02-05T10:22:45.465683Z","shell.execute_reply.started":"2022-02-05T10:22:45.445222Z","shell.execute_reply":"2022-02-05T10:22:45.465129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **<span id=\"Data\" style=\"color:#023e8a;\">2. Data prep and stemming</span>**","metadata":{}},{"cell_type":"markdown","source":"[**<span style=\"color:#FEF1FE;background-color:#023e8a;border-radius: 5px;padding: 2px\">Go to Table of Content</span>**](#Content)","metadata":{}},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">For more efficient work of `LDA` we need to lemmatize text. `Lemmatization` is necessary to bring words to their initial form. That is helpful to consider words \"student\" and, for instance, \"students\" as the same word. However, `stemming` (that is the procedure consisting in separating the root of the word only) is a is an appropriate tool for English too and in terms of the speed it is much more beneficial than `lemmatization`.</span>**\n\n**<span style=\"color:#023e8a;\">Learn more</span>**: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html","metadata":{}},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">Cleaning def. Thanks to:</span>** https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove","metadata":{}},{"cell_type":"code","source":"def text_cleaning(texts):\n    texts_cleaning = []\n    for txt in tqdm(texts):\n        url = re.compile(r'https?://\\S+|www\\.\\S+')\n        html = re.compile(r'<.*?>')\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        txt = emoji_pattern.sub(r'', txt)\n        txt = html.sub(r'',txt)\n        txt = url.sub(r'',txt)\n        txt = re.sub('[^A-Za-z\\s]', '', txt)\n        \n        texts_cleaning.append(txt.lower())\n    return texts_cleaning\ntext = text_cleaning(train.text.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:36:13.189799Z","iopub.execute_input":"2022-02-05T10:36:13.190572Z","iopub.status.idle":"2022-02-05T10:36:13.392766Z","shell.execute_reply.started":"2022-02-05T10:36:13.190533Z","shell.execute_reply":"2022-02-05T10:36:13.39193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\ntext = [t.split() for t in text]\nstemmed_text = []\nps = PorterStemmer()\nfor sentence in tqdm(text):\n    sent = []\n    for word in sentence:\n        sent.append(ps.stem(word))\n    stemmed_text.append(sent)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:37:28.531993Z","iopub.execute_input":"2022-02-05T10:37:28.532262Z","iopub.status.idle":"2022-02-05T10:37:31.499228Z","shell.execute_reply.started":"2022-02-05T10:37:28.532233Z","shell.execute_reply":"2022-02-05T10:37:31.498603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">Comparing original and stemmed texts.</span>**","metadata":{}},{"cell_type":"code","source":"print(*stemmed_text[5][:20])\nprint(*text[5][:20])","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:50:16.439045Z","iopub.execute_input":"2022-02-05T10:50:16.439344Z","iopub.status.idle":"2022-02-05T10:50:16.449189Z","shell.execute_reply.started":"2022-02-05T10:50:16.439316Z","shell.execute_reply":"2022-02-05T10:50:16.448268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">After that, we need to bring the words to a numerical expression. For this you can use:</span>**\n* `Countvectorizer`\n* `Tf-idf`\n* `Embeddings`  \n\n`Countvectorizer` **<span style=\"color:#023e8a;\">gives matrix num_words X texts where each number is a number of count in all texts.</span>**\n\n`TF-IDF` **<span style=\"color:#023e8a;\">is an abbreviation standing for frequencyâ€“inverse document frequency,which is a numerical statistics that are aimed to reflect how important a word is for a document in a collection or corpus. </span>**\n\n**<span style=\"color:#023e8a;\">Learn more</span>**: https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n\n**<span style=\"color:#023e8a;\">`Embeddings` allows to represent words like numerical vector.</span>**:\n\n\n\n**<span style=\"color:#023e8a;\">`Gensim`  allows to get bow by method `doc2bow`. This method converts document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples.  </span>**\n\n**<span style=\"color:#023e8a;\">TF-IDF doesnt use for LDA because authors recommend to use bow:</span>**  \n\n>In fact, Blei (who developed LDA), points out in the introduction of the paper of 2003 (entitled \"Latent Dirichlet Allocation\") that LDA >addresses the shortcomings of the TF-IDF model and leaves this approach behind. LSA is compeltely algebraic and generally (but not >necessarily) uses a TF-IDF matrix, while LDA is a probabilistic model that tries to estimate probability distributions for topics in >documents and words in topics. The weighting of TF-IDF is not necessary for this.  \n\n**<span style=\"color:#023e8a;\">Learn more</span>**: https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\n\n\n**<span style=\"color:#023e8a;\">So, `count-vectorizer` of `bow` are appropriate methods to use for LDA. </span>**","metadata":{}},{"cell_type":"code","source":"dictionary = gensim.corpora.Dictionary(stemmed_text)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:51:45.558458Z","iopub.execute_input":"2022-02-05T10:51:45.558859Z","iopub.status.idle":"2022-02-05T10:51:45.826337Z","shell.execute_reply.started":"2022-02-05T10:51:45.558825Z","shell.execute_reply":"2022-02-05T10:51:45.825713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">Filter dictionary by stopwords and most common words (more than in 70% of texts) and not frequently used words (<20 counts).</span>**","metadata":{}},{"cell_type":"code","source":"stopword_ids = map(dictionary.token2id.get, stops)\ndictionary.filter_tokens(bad_ids=stopword_ids)\ndictionary.filter_extremes(no_below=20, no_above=0.7, keep_n=None)\ndictionary.compactify() # remove gaps in id sequence\nbow = [dictionary.doc2bow(line) for line in tqdm(stemmed_text)]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:52:03.661849Z","iopub.execute_input":"2022-02-05T10:52:03.662298Z","iopub.status.idle":"2022-02-05T10:52:03.836904Z","shell.execute_reply.started":"2022-02-05T10:52:03.662268Z","shell.execute_reply":"2022-02-05T10:52:03.836079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**<span style=\"color:#023e8a;\">`Seeded (or Guided) LDA` is a method that allows to add apriori information about the distribution of words in topics. Thus, we can get a desired topic with the given dictionary and do not depend only on the black box results.</span>**\n\n\n**<span style=\"color:#023e8a;\">Learn more</span>** https://nlp.stanford.edu/pubs/llda-emnlp09.pdf","metadata":{}},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">Here we create one topic, dedicated disasters. The second topic will include the another part of tweets.</span>**","metadata":{}},{"cell_type":"code","source":"disasters = ['disaster', 'bloodbath', 'collapse', 'crash', 'meltdown', 'doomsday', 'convulsion', 'accident', 'casualty', 'fatality', \n            'blast', 'catastrophe', 'traffic','hybrid', 'engine', 'license', \n            'tsunami', 'volcano','tornado','avalanche','earthquake','blizzard','drought','bushfire','tremor','magma','twister',\n            'windstorm','cyclone','flood','fire','hailstorm','lava','lightning','hail','hurricane','seismic','erosion','whirlpool','whirlwind',\n            'cloud','thunderstorm','barometer','gale','blackout','gust','force','volt','snowstorm','rainstorm','storm','nimbus','violent storm',\n            'sandstorm','fatal','cumulonimbus','death','lost','destruction','money','tension','cataclysm','damage','uproot','underground',\n            'destroy','arsonist','wind scale','arson','rescue','permafrost','fault','shelter', 'bomb', 'suicide', 'tragedy', 'weapon']\n\ndisasters = [ps.stem(word) for word in disasters]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:07:39.85702Z","iopub.execute_input":"2022-02-05T11:07:39.857346Z","iopub.status.idle":"2022-02-05T11:07:39.868981Z","shell.execute_reply.started":"2022-02-05T11:07:39.857317Z","shell.execute_reply":"2022-02-05T11:07:39.867978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">`Create_eta` function gives eta matrix with apriori words in topics. Here we create the dict of words for our first topic - `disaster`. After this we create ones matrix and fill the huge number for words from the created dict in topic 1. </span>**","metadata":{}},{"cell_type":"code","source":"seed_topics = {}\nfor word in disasters:\n    seed_topics[word] = 0","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:07:41.532482Z","iopub.execute_input":"2022-02-05T11:07:41.533028Z","iopub.status.idle":"2022-02-05T11:07:41.537523Z","shell.execute_reply.started":"2022-02-05T11:07:41.532985Z","shell.execute_reply":"2022-02-05T11:07:41.536492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_eta(priors, etadict, ntopics):\n    eta = np.full(shape=(ntopics, len(etadict)), fill_value=1) # create a (ntopics, nterms) matrix and fill with 1\n    for word, topic in priors.items(): # for each word in the list of priors\n        keyindex = [index for index,term in etadict.items() if term==word] # look up the word in the dictionary\n        if (len(keyindex)>0): # if it's in the dictionary\n            eta[topic,keyindex[0]] = 1e7  # put a large number in there\n    eta = np.divide(eta, eta.sum(axis=0)) # normalize so that the probabilities sum to 1 over all topics\n    return eta","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:07:43.823341Z","iopub.execute_input":"2022-02-05T11:07:43.823848Z","iopub.status.idle":"2022-02-05T11:07:43.83055Z","shell.execute_reply.started":"2022-02-05T11:07:43.823803Z","shell.execute_reply":"2022-02-05T11:07:43.829629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eta = create_eta(seed_topics, dictionary, 2)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:07:48.536216Z","iopub.execute_input":"2022-02-05T11:07:48.53653Z","iopub.status.idle":"2022-02-05T11:07:48.61287Z","shell.execute_reply.started":"2022-02-05T11:07:48.536496Z","shell.execute_reply":"2022-02-05T11:07:48.612101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">Number of topics = 2:</span>**\n* `disasters`\n* `common topic`","metadata":{}},{"cell_type":"markdown","source":"# **<span id=\"Modeling\" style=\"color:#023e8a;\">3. Modeling</span>**","metadata":{}},{"cell_type":"markdown","source":"[**<span style=\"color:#FEF1FE;background-color:#023e8a;border-radius: 5px;padding: 2px\">Go to Table of Content</span>**](#Content)","metadata":{}},{"cell_type":"code","source":"lda_model = LdaMulticore(corpus=bow, #bag of words\n                         id2word=dictionary, #our common dict, need for print words in topics, not numbers from bow\n                         num_topics=2,\n                         eta=eta, #our eta matrix\n                         chunksize=2000,\n                         passes=10,\n                         random_state=42,\n                         alpha='symmetric', #param of LDA distribution. If you dont know use symmetric\n                         per_word_topics=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:07:53.540117Z","iopub.execute_input":"2022-02-05T11:07:53.540912Z","iopub.status.idle":"2022-02-05T11:08:16.082192Z","shell.execute_reply.started":"2022-02-05T11:07:53.540867Z","shell.execute_reply":"2022-02-05T11:08:16.08117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">You may change the number of topics and check `Coherence` for model selection. Moreover, you may set initially more words in topics for better results.</span>**","metadata":{}},{"cell_type":"code","source":"for num, params in lda_model.print_topics():\n    print(f'{num}: {params}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-02-05T11:08:38.505632Z","iopub.execute_input":"2022-02-05T11:08:38.505994Z","iopub.status.idle":"2022-02-05T11:08:38.513113Z","shell.execute_reply.started":"2022-02-05T11:08:38.50594Z","shell.execute_reply":"2022-02-05T11:08:38.512259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<span style=\"color:#023e8a;\">You also may enhance results, comparing `Coherence` metric of different number of topics LDA. </span>**\n\n**<span style=\"color:#023e8a;\">These results may try to make better main model results, or you may try to separate disaster tweets from another part only using Guided LDA. Create a-priori word distribution in topics helps to get appropriate results. </span>**","metadata":{}},{"cell_type":"markdown","source":"## **<span style=\"color:#FEF1FE;background-color:#023e8a;border-radius: 5px;padding: 5px\">Thank you for reading! Please, upvote this notebook if you learned smth new :)</span>**","metadata":{}}]}