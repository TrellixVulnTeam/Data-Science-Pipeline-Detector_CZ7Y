{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìö Natural Language Processing with Disaster Tweets üí¨\n \n>- üéØ Goal: Predict which Tweets are about real disasters and which ones are not","metadata":{}},{"cell_type":"code","source":"import re\nimport string\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport transformers\nfrom transformers import BertTokenizer\nfrom transformers import TFAutoModel","metadata":{"execution":{"iopub.status.busy":"2021-06-13T07:56:50.567652Z","iopub.execute_input":"2021-06-13T07:56:50.567994Z","iopub.status.idle":"2021-06-13T07:56:58.0958Z","shell.execute_reply.started":"2021-06-13T07:56:50.56796Z","shell.execute_reply":"2021-06-13T07:56:58.095024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T07:56:58.09747Z","iopub.execute_input":"2021-06-13T07:56:58.097806Z","iopub.status.idle":"2021-06-13T07:56:58.10303Z","shell.execute_reply.started":"2021-06-13T07:56:58.097778Z","shell.execute_reply":"2021-06-13T07:56:58.102136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìã Loading the Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')\nsample_sub = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-13T07:56:58.104107Z","iopub.execute_input":"2021-06-13T07:56:58.104421Z","iopub.status.idle":"2021-06-13T07:56:58.172063Z","shell.execute_reply.started":"2021-06-13T07:56:58.104385Z","shell.execute_reply":"2021-06-13T07:56:58.171387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T07:56:58.17315Z","iopub.execute_input":"2021-06-13T07:56:58.173475Z","iopub.status.idle":"2021-06-13T07:56:58.192985Z","shell.execute_reply.started":"2021-06-13T07:56:58.173441Z","shell.execute_reply":"2021-06-13T07:56:58.1919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T07:56:58.196184Z","iopub.execute_input":"2021-06-13T07:56:58.19646Z","iopub.status.idle":"2021-06-13T07:56:58.206445Z","shell.execute_reply.started":"2021-06-13T07:56:58.196427Z","shell.execute_reply":"2021-06-13T07:56:58.205405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üî® Preprocessing","metadata":{}},{"cell_type":"code","source":"#Use regex to clean the data\ndef remove_url(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndef decontraction(text):\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)\n    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    return text \n\ndef seperate_alphanumeric(text):\n    words = text\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\n\ndef cont_rep_char(text):\n    tchr = text.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] \n\ndef unique_char(rep, text):\n    substitute = re.sub(r'(\\w)\\1+', rep, text)\n    return substitute\n\ntrain['text'] = train['text'].apply(lambda x : remove_url(x))\ntrain['text'] = train['text'].apply(lambda x : remove_punct(x))\ntrain['text'] = train['text'].apply(lambda x : remove_emoji(x))\ntrain['text'] = train['text'].apply(lambda x : decontraction(x))\ntrain['text'] = train['text'].apply(lambda x : seperate_alphanumeric(x))\ntrain['text'] = train['text'].apply(lambda x : unique_char(cont_rep_char,x))\n\ntest['text'] = test['text'].apply(lambda x : remove_url(x))\ntest['text'] = test['text'].apply(lambda x : remove_punct(x))\ntest['text'] = test['text'].apply(lambda x : remove_emoji(x))\ntest['text'] = test['text'].apply(lambda x : decontraction(x))\ntest['text'] = test['text'].apply(lambda x : seperate_alphanumeric(x))\ntest['text'] = test['text'].apply(lambda x : unique_char(cont_rep_char,x))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T07:56:58.208865Z","iopub.execute_input":"2021-06-13T07:56:58.209292Z","iopub.status.idle":"2021-06-13T07:56:59.018547Z","shell.execute_reply.started":"2021-06-13T07:56:58.209256Z","shell.execute_reply":"2021-06-13T07:56:59.017669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ‚úÇÔ∏è Tokenization","metadata":{}},{"cell_type":"code","source":"seq_len = 256\nbatch_size = 16\nnum_samples = len(train)\nmodel_name = 'cardiffnlp/twitter-roberta-base-sentiment'","metadata":{"execution":{"iopub.status.busy":"2021-06-13T07:56:59.02029Z","iopub.execute_input":"2021-06-13T07:56:59.020669Z","iopub.status.idle":"2021-06-13T07:56:59.025248Z","shell.execute_reply.started":"2021-06-13T07:56:59.020631Z","shell.execute_reply":"2021-06-13T07:56:59.024441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\ntrain_tokens = tokenizer(train['text'].tolist(), max_length=seq_len, \n                         truncation=True, padding='max_length', \n                         add_special_tokens=True, return_tensors='np')\n\ny_train = train['target'].values\nlabels = np.zeros((num_samples, y_train.max() + 1))\nlabels[np.arange(num_samples), y_train] = 1\n\ndataset = tf.data.Dataset.from_tensor_slices((train_tokens['input_ids'], train_tokens['attention_mask'], labels))\n\ndef map_func(input_ids, masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': masks\n    }, labels\n\ndataset = dataset.map(map_func)\ndataset = dataset.shuffle(10000).batch(batch_size=batch_size, drop_remainder=True)\n\nsplit = 0.7\nsize = int((train_tokens['input_ids'].shape[0] // batch_size) * split)\n\ntrain_ds = dataset.take(size)\nval_ds = dataset.skip(size)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T07:56:59.027276Z","iopub.execute_input":"2021-06-13T07:56:59.027682Z","iopub.status.idle":"2021-06-13T07:57:12.399257Z","shell.execute_reply.started":"2021-06-13T07:56:59.027642Z","shell.execute_reply":"2021-06-13T07:57:12.398317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ü§ñ Model Building","metadata":{}},{"cell_type":"code","source":"model = TFAutoModel.from_pretrained(model_name)\n\n# Two inputs\ninput_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\nmask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n\n# Transformer\n# embeddings = model.bert(input_ids, attention_mask=mask)[1]\nembeddings = model(input_ids, attention_mask=mask)[0]\nembeddings = embeddings[:, 0, :]\n# Classifier head\nx = tf.keras.layers.Dense(512, activation='relu')(embeddings)\n# x = tf.keras.layers.Dropout(0.1)(x)\ny = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\n\nbert_model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n\n# freeze bert layers\n# bert_model.layers[2].trainable = False\n\noptimizer = tf.keras.optimizers.Adam(lr=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy()\nacc = tf.keras.metrics.BinaryAccuracy()\n\nbert_model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n\nhistory = bert_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=2,\n    batch_size=batch_size\n)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-13T08:08:50.674818Z","iopub.execute_input":"2021-06-13T08:08:50.675147Z","iopub.status.idle":"2021-06-13T08:24:17.613769Z","shell.execute_reply.started":"2021-06-13T08:08:50.675117Z","shell.execute_reply":"2021-06-13T08:24:17.612996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üìä Model Evaluation","metadata":{}},{"cell_type":"code","source":"def plot_learning_evolution(r):\n    plt.figure(figsize=(12, 8))\n    \n    plt.subplot(2, 2, 1)\n    plt.plot(r.history['loss'], label='Loss')\n    plt.plot(r.history['val_loss'], label='val_Loss')\n    plt.title('Loss evolution during trainig')\n    plt.legend()\n\n    plt.subplot(2, 2, 2)\n    plt.plot(r.history['binary_accuracy'], label='binary_accuracy')\n    plt.plot(r.history['val_binary_accuracy'], label='val_binary_accuracy')\n    plt.title('Accuracy score evolution during trainig')\n    plt.legend();","metadata":{"execution":{"iopub.status.busy":"2021-06-13T08:08:45.564074Z","iopub.status.idle":"2021-06-13T08:08:45.564453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_learning_evolution(history)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T08:08:45.565548Z","iopub.status.idle":"2021-06-13T08:08:45.566124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.evaluate(val_ds)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T08:08:45.56712Z","iopub.status.idle":"2021-06-13T08:08:45.567649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_data(text):\n    tokens = tokenizer(text, max_length=512, truncation=True, \n                       padding='max_length', \n                       add_special_tokens=True, \n                       return_tensors='tf')\n    return {'input_ids': tokens['input_ids'], \n            'attention_mask': tokens['attention_mask']}\n\ntest['target'] = None\n\nfor i, row in test.iterrows():\n    tokens = prep_data(row['text'])\n    probs = bert_model.predict(tokens)\n    pred = np.argmax(probs)\n    test.at[i, 'target'] = pred\n    \ntest['target'] = test['target'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T08:08:45.568759Z","iopub.status.idle":"2021-06-13T08:08:45.569562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T08:08:45.570707Z","iopub.status.idle":"2021-06-13T08:08:45.57137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Making submission","metadata":{"trusted":true}},{"cell_type":"code","source":"sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(), 'target':test['target']})\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T08:08:45.572593Z","iopub.status.idle":"2021-06-13T08:08:45.573307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-13T08:08:45.57455Z","iopub.status.idle":"2021-06-13T08:08:45.575234Z"},"trusted":true},"execution_count":null,"outputs":[]}]}