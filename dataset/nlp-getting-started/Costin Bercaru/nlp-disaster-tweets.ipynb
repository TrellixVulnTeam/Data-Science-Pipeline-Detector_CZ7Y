{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook tries to classify tweets as either being about a **real** disaster or not. It is intended only for learning and practicing NLP tehniques.","metadata":{}},{"cell_type":"markdown","source":"<a id='top'></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"><p style=\"font-size : 25px\">NLP Disaster Tweets:</p></h3>\n    \n0. [Imports](#0)\n    \n1. [Data Understanding and Feature Selection](#1)      \n    \n2. [Text Cleaning + Glove Embeddings](#2)\n    \n3. [Neural Networks](#3)\n    - 3.1 [Bidirectional LSTM](#3.1)\n    - 3.2 [1D CNN + Bidirectional LSTM](#3.2)\n    - 3.3 [Bidirectional LSTM + FC Network on numerical features](#3.3)\n    - 3.4 [Bert](#3.4)  \n    \n4. [Submission](#4)\n\n5. [Conclusions](#5)","metadata":{}},{"cell_type":"markdown","source":"# <font size=\"+2\" color=\"black\"><b>0. Imports </b></font><br><a id=\"0\"></a>","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords    \nfrom nltk.tokenize import word_tokenize\nfrom textblob import TextBlob\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, MaxPooling1D, Conv1D, Concatenate, Bidirectional, GlobalMaxPool1D, ActivityRegularization, BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport re\nimport string\n\n%matplotlib inline\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nnr_examples = 75","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-01T19:04:41.353668Z","iopub.execute_input":"2021-10-01T19:04:41.354275Z","iopub.status.idle":"2021-10-01T19:04:41.39829Z","shell.execute_reply.started":"2021-10-01T19:04:41.354232Z","shell.execute_reply":"2021-10-01T19:04:41.397326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font size=\"+2\" color=\"black\"><b>1. Data Understanding and Feature Selection </b></font><br><a id=\"1\"></a>\n\n**Let's take our first look at the data.**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ntrain_df.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-10-01T19:04:41.659239Z","iopub.execute_input":"2021-10-01T19:04:41.659607Z","iopub.status.idle":"2021-10-01T19:04:41.753027Z","shell.execute_reply.started":"2021-10-01T19:04:41.659576Z","shell.execute_reply":"2021-10-01T19:04:41.752161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**There seem to be a lot of NaN in keyword and location. Let's take a closer look at these columns in order to see if we can use them as features or not.**","metadata":{}},{"cell_type":"markdown","source":"# Does location matter?","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20, 5))\n\nax.plot(train_df['location'].value_counts().head(nr_examples).index, train_df['location'].value_counts().head(nr_examples).values)\nax.set(xlabel = 'locations', ylabel = '# of appearances')\n\nplt.xticks(rotation = 'vertical')\nplt.tight_layout()\nplt.show()\n\ntrain_df['haslocation'] = np.where(train_df['location'].isnull(), 0, 1)\n\"{:.2f}% of the entries have a location\".format((len(train_df[(train_df['haslocation'] == 1)]) / len(train_df) * 100))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:42.262654Z","iopub.execute_input":"2021-10-01T19:04:42.263254Z","iopub.status.idle":"2021-10-01T19:04:43.248736Z","shell.execute_reply.started":"2021-10-01T19:04:42.2632Z","shell.execute_reply":"2021-10-01T19:04:43.247361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**66% is pretty low and we don't have a good way to replace the missing data. Let's see whether the presence of the location could be a useful feature instead of the location itself.**","metadata":{}},{"cell_type":"code","source":"\"{:.2f}% of the entries which represent actual disasters have a location\".format((len(train_df[(train_df['haslocation'] == 1) & (train_df['target'] == 1)]) / len(train_df[(train_df['target'] == 1)]) * 100)) ","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:43.251614Z","iopub.execute_input":"2021-10-01T19:04:43.252093Z","iopub.status.idle":"2021-10-01T19:04:43.264904Z","shell.execute_reply.started":"2021-10-01T19:04:43.252046Z","shell.execute_reply":"2021-10-01T19:04:43.263907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"{:.2f}% of the entries which do not represent disasters have a location\".format((len(train_df[(train_df['haslocation'] == 1) & (train_df['target'] == 0)]) / len(train_df[(train_df['target'] == 0)]) * 100)) ","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:43.267163Z","iopub.execute_input":"2021-10-01T19:04:43.267506Z","iopub.status.idle":"2021-10-01T19:04:43.279985Z","shell.execute_reply.started":"2021-10-01T19:04:43.267472Z","shell.execute_reply":"2021-10-01T19:04:43.278635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The presence/absence of the location does not seem to be correlated with the target. Let's drop location and haslocation.**","metadata":{}},{"cell_type":"code","source":"train_df.drop([\"location\", \"haslocation\"], axis = 1, inplace = True)\ntest_df.drop([\"location\"], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:43.281573Z","iopub.execute_input":"2021-10-01T19:04:43.281914Z","iopub.status.idle":"2021-10-01T19:04:43.295199Z","shell.execute_reply.started":"2021-10-01T19:04:43.28187Z","shell.execute_reply":"2021-10-01T19:04:43.294283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# But what about the keyword?","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20, 5))\n\nax.bar(np.arange(nr_examples), train_df.where(train_df['target'] == 1)['keyword'].value_counts().head(nr_examples), 0.35, color = \"C0\")\nax.set_ylabel('# appeareances')\nax.set_title('Keyword')\nax.set_xticks(np.arange(nr_examples))\nax.set_xticklabels(train_df.where(train_df['target'] == 1)['keyword'].value_counts().head(nr_examples).index, rotation = 'vertical')\n\nplt.tight_layout()\nplt.show()\n\ntrain_df['haskeyword'] = np.where(train_df['keyword'].isnull(), 0, 1)\n\"{:.2f}% of the entries have a keyword\".format((len(train_df[(train_df['haskeyword'] == 1)]) / len(train_df) * 100)) ","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:43.297829Z","iopub.execute_input":"2021-10-01T19:04:43.29833Z","iopub.status.idle":"2021-10-01T19:04:44.410267Z","shell.execute_reply.started":"2021-10-01T19:04:43.29829Z","shell.execute_reply":"2021-10-01T19:04:44.409459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now that's a percentage we can work with. Let's see what kind of keywords do the real disaster tweets have compared to the non-disater ones.**","metadata":{}},{"cell_type":"code","source":"fig, (ax, bx) = plt.subplots(nrows = 2, ncols = 1, figsize = (20, 10))\n\nax.bar(np.arange(nr_examples), train_df.where(train_df['target'] == 1)['keyword'].value_counts().head(nr_examples), 0.35, color = \"C1\")\nax.set_ylabel('# appeareances')\nax.set_title('Disaster')\nax.set_xticks(np.arange(nr_examples))\nax.set_xticklabels(train_df.where(train_df['target'] == 1)['keyword'].value_counts().head(nr_examples).index, rotation='vertical')\n\nbx.bar(np.arange(nr_examples), train_df.where(train_df['target'] == 0)['keyword'].value_counts().head(nr_examples), 0.35, color = \"C0\")\nbx.set_ylabel('# appeareances')\nbx.set_title('Non-disaster')\nbx.set_xticks(np.arange(nr_examples))\nbx.set_xticklabels(train_df.where(train_df['target'] == 0)['keyword'].value_counts().head(nr_examples).index, rotation='vertical')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:44.412492Z","iopub.execute_input":"2021-10-01T19:04:44.412925Z","iopub.status.idle":"2021-10-01T19:04:46.519415Z","shell.execute_reply.started":"2021-10-01T19:04:44.412893Z","shell.execute_reply":"2021-10-01T19:04:46.517981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In both cases the keyword seems to be a negative word, with misspellings or not. Not very helpful. Let us convince ourselves of the lack of importance of the keyword by comparing the sentiment of the keywords for dissaster vs non-dissaster tweets. If we're doing this, we might aswell analyze the sentiment for the text too.**","metadata":{}},{"cell_type":"code","source":"train_df['keyword_sentiment'] = train_df['keyword'].apply(lambda x: (TextBlob(x).sentiment[0] + 1) / 2 if type(x) == str else None)\ntrain_df['tweet_sentiment'] = train_df['text'].apply(lambda x: (TextBlob(x).sentiment[0] + 1) / 2)\ntest_df['tweet_sentiment'] = test_df['text'].apply(lambda x: (TextBlob(x).sentiment[0] + 1) / 2)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:46.521502Z","iopub.execute_input":"2021-10-01T19:04:46.521977Z","iopub.status.idle":"2021-10-01T19:04:50.597651Z","shell.execute_reply.started":"2021-10-01T19:04:46.521926Z","shell.execute_reply":"2021-10-01T19:04:50.596773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster = train_df.where(train_df['target'] == 1)['keyword_sentiment'].dropna().values\nnot_disaster = train_df.where(train_df['target'] == 0)['keyword_sentiment'].dropna().values\n\nfig, ax = plt.subplots(figsize = (20, 5))\nax.set_title('Disaster keyword sentiment vs Non-disaster keyword sentiment')\nax.boxplot([disaster, not_disaster])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:50.599424Z","iopub.execute_input":"2021-10-01T19:04:50.600177Z","iopub.status.idle":"2021-10-01T19:04:50.829064Z","shell.execute_reply.started":"2021-10-01T19:04:50.600124Z","shell.execute_reply":"2021-10-01T19:04:50.827952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disaster = train_df.where(train_df['target'] == 1)['tweet_sentiment'].dropna().values\nnot_disaster = train_df.where(train_df['target'] == 0)['tweet_sentiment'].dropna().values\n\nfig, ax = plt.subplots(figsize = (20, 5))\nax.set_title('Disaster overall sentiment vs Non-disaster overall sentiment')\nax.boxplot([disaster, not_disaster])\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:50.830914Z","iopub.execute_input":"2021-10-01T19:04:50.831649Z","iopub.status.idle":"2021-10-01T19:04:51.048137Z","shell.execute_reply.started":"2021-10-01T19:04:50.831593Z","shell.execute_reply":"2021-10-01T19:04:51.047289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The keyword sentiment does not appear to matter that much. The overall tweet sentiment however, seems to be of somewhat importance. Let's keep that one and drop the keyword sentiment. Due to the unfortunately small size of the dataset thought, we will add the keyword to the text.**","metadata":{}},{"cell_type":"code","source":"train_df['text'] = train_df['keyword'].astype(str) + ' ' + train_df['text'].astype(str)\ntest_df['text'] = test_df['keyword'].astype(str) + ' ' + test_df['text'].astype(str)\n\ntrain_df.drop([\"keyword\", \"haskeyword\", \"keyword_sentiment\"], axis = 1, inplace = True)\ntest_df.drop([\"keyword\"], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:51.049464Z","iopub.execute_input":"2021-10-01T19:04:51.049944Z","iopub.status.idle":"2021-10-01T19:04:51.075156Z","shell.execute_reply.started":"2021-10-01T19:04:51.049911Z","shell.execute_reply":"2021-10-01T19:04:51.074022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# So the location and the keyword did not help us very much, but the tweet sentiment seems to. Let's try and find more valuable features to add. We will normalize everything in order for the future neural net to learn better.","metadata":{}},{"cell_type":"markdown","source":"**Number of characters.**","metadata":{}},{"cell_type":"code","source":"train_df['nr_of_char'] = train_df['text'].str.len()\ntrain_df['nr_of_char'] = train_df['nr_of_char'] / train_df['nr_of_char'].max()\n\ndisaster = train_df.where(train_df['target'] == 1)['nr_of_char'].dropna().values\nnot_disaster = train_df.where(train_df['target'] == 0)['nr_of_char'].dropna().values\n\nfig, ax = plt.subplots(figsize = (20, 5))\nax.set_title('Disaster # of chars vs Non-disaster # of chars')\nax.boxplot([disaster, not_disaster])\n\nplt.tight_layout()\nplt.show()\n\ntest_df['nr_of_char'] = test_df['text'].str.len()\ntest_df['nr_of_char'] = test_df['nr_of_char'] / test_df['nr_of_char'].max()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:51.076698Z","iopub.execute_input":"2021-10-01T19:04:51.077291Z","iopub.status.idle":"2021-10-01T19:04:51.575734Z","shell.execute_reply.started":"2021-10-01T19:04:51.077245Z","shell.execute_reply":"2021-10-01T19:04:51.574614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of words.**","metadata":{}},{"cell_type":"code","source":"train_df['nr_of_words'] = train_df['text'].str.split().str.len()\ntrain_df['nr_of_words'] = train_df['nr_of_words'] / train_df['nr_of_words'].max()\n\ndisaster = train_df.where(train_df['target'] == 1)['nr_of_words'].dropna().values\nnot_disaster = train_df.where(train_df['target'] == 0)['nr_of_words'].dropna().values\n\nfig, ax = plt.subplots(figsize = (20, 5))\nax.set_title('Disaster # of words vs Non-disaster # of words')\nax.boxplot([disaster, not_disaster])\n\nplt.tight_layout()\nplt.show()\n\ntest_df['nr_of_words'] = test_df['text'].str.split().str.len()\ntest_df['nr_of_words'] = test_df['nr_of_words'] / test_df['nr_of_words'].max()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:51.577241Z","iopub.execute_input":"2021-10-01T19:04:51.577794Z","iopub.status.idle":"2021-10-01T19:04:51.832537Z","shell.execute_reply.started":"2021-10-01T19:04:51.577758Z","shell.execute_reply":"2021-10-01T19:04:51.831627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of unique words.**","metadata":{}},{"cell_type":"code","source":"train_df['nr_of_unique_words'] = train_df['text'].apply(lambda x: len(set(x.split())))\ntrain_df['nr_of_unique_words'] = train_df['nr_of_unique_words'] / train_df['nr_of_unique_words'].max()\n\ndisaster = train_df.where(train_df['target'] == 1)['nr_of_unique_words'].dropna().values\nnot_disaster = train_df.where(train_df['target'] == 0)['nr_of_unique_words'].dropna().values\n\nfig, ax = plt.subplots(figsize = (20, 5))\nax.set_title('Disaster # of unique words vs Non-disaster # of unique words')\nax.boxplot([disaster, not_disaster])\n\nplt.tight_layout()\nplt.show()\n\ntest_df['nr_of_unique_words'] = test_df['text'].apply(lambda x: len(set(x.split())))\ntest_df['nr_of_unique_words'] = test_df['nr_of_unique_words'] / test_df['nr_of_unique_words'].max()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:51.833866Z","iopub.execute_input":"2021-10-01T19:04:51.834383Z","iopub.status.idle":"2021-10-01T19:04:52.094421Z","shell.execute_reply.started":"2021-10-01T19:04:51.834347Z","shell.execute_reply":"2021-10-01T19:04:52.093574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of punctuation marks.**","metadata":{}},{"cell_type":"code","source":"train_df['nr_of_punctuation'] = train_df['text'].str.split(r\"\\?|,|\\.|\\!|\\\"|'\").str.len()\ntrain_df['nr_of_punctuation'] = train_df['nr_of_punctuation'] / train_df['nr_of_punctuation'].max()\n\ndisaster = train_df.where(train_df['target'] == 1)['nr_of_punctuation'].dropna().values\nnot_disaster = train_df.where(train_df['target'] == 0)['nr_of_punctuation'].dropna().values\n\nfig, ax = plt.subplots(figsize = (20, 5))\nax.set_title('Disaster # of punctuation signs vs Non-disaster # of punctuation signs')\nax.boxplot([disaster, not_disaster])\n\nplt.tight_layout()\nplt.show()\n\ntest_df['nr_of_punctuation'] = test_df['text'].str.split(r\"\\?|,|\\.|\\!|\\\"|'\").str.len()\ntest_df['nr_of_punctuation'] = test_df['nr_of_punctuation'] / test_df['nr_of_punctuation'].max()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:52.09578Z","iopub.execute_input":"2021-10-01T19:04:52.096271Z","iopub.status.idle":"2021-10-01T19:04:52.368312Z","shell.execute_reply.started":"2021-10-01T19:04:52.096238Z","shell.execute_reply":"2021-10-01T19:04:52.367437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of stopwords.**","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\ntrain_df['nr_of_stopwords'] = train_df['text'].str.split().apply(lambda x: len(set(x) & stop_words))\ntrain_df['nr_of_stopwords'] = train_df['nr_of_stopwords'] / train_df['nr_of_stopwords'].max()\n\ndisaster = train_df.where(train_df['target'] == 1)['nr_of_stopwords'].dropna().values\nnot_disaster = train_df.where(train_df['target'] == 0)['nr_of_stopwords'].dropna().values\n\nfig, ax = plt.subplots(figsize = (20, 5))\nax.set_title('Disaster # of stopwords vs Non-disaster # of stopwords')\nax.boxplot([disaster, not_disaster])\n\nplt.tight_layout()\nplt.show()\n\ntest_df['nr_of_stopwords'] = test_df['text'].str.split().apply(lambda x: len(set(x) & stop_words))\ntest_df['nr_of_stopwords'] = test_df['nr_of_stopwords'] / test_df['nr_of_stopwords'].max()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:52.369675Z","iopub.execute_input":"2021-10-01T19:04:52.370202Z","iopub.status.idle":"2021-10-01T19:04:52.674602Z","shell.execute_reply.started":"2021-10-01T19:04:52.370166Z","shell.execute_reply":"2021-10-01T19:04:52.673652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.corr().iplot(kind='heatmap',colorscale=\"Reds\",title=\"Feature Correlation Matrix\")","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:52.675937Z","iopub.execute_input":"2021-10-01T19:04:52.676453Z","iopub.status.idle":"2021-10-01T19:04:53.522246Z","shell.execute_reply.started":"2021-10-01T19:04:52.676419Z","shell.execute_reply":"2021-10-01T19:04:53.521218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It seems like none of our features are highly correlated with the target. Nevertheless, we will try to use them in a network as an experiment.**","metadata":{}},{"cell_type":"markdown","source":"# <font size=\"+2\" color=\"black\"><b>2. Text Cleaning + Glove Embeddings </b></font><br><a id=\"2\"></a>\n\n**First, we'll download the Glove embeddings and see how much of the text they already cover.**","metadata":{}},{"cell_type":"code","source":"glove_embeddings = np.load('/kaggle/input/embfile/emb/glove.840B.300d.pkl', allow_pickle=True)\n\n# credit to \ndef build_vocab(X):\n    \n    tweets = X.apply(lambda s: s.split()).values      \n    vocab = {}\n    \n    for tweet in tweets:\n        for word in tweet:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1                \n    return vocab\n\ndef check_embeddings_coverage(X, embeddings):\n    \n    vocab = build_vocab(X)    \n    \n    covered = {}\n    oov = {}    \n    n_covered = 0\n    n_oov = 0\n    \n    for word in vocab:\n        try:\n            covered[word] = embeddings[word]\n            n_covered += vocab[word]\n        except:\n            n_oov += vocab[word]\n            \n    vocab_coverage = len(covered) / len(vocab)\n    text_coverage = (n_covered / (n_covered + n_oov))\n    \n    return vocab_coverage, text_coverage\n\ntrain_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_df['text'], glove_embeddings)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\ntest_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_df['text'], glove_embeddings)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:04:53.523719Z","iopub.execute_input":"2021-10-01T19:04:53.524039Z","iopub.status.idle":"2021-10-01T19:05:30.456872Z","shell.execute_reply.started":"2021-10-01T19:04:53.524006Z","shell.execute_reply":"2021-10-01T19:05:30.455553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It seems pretty bad. We can make those percentages better by cleaning the text.**","metadata":{}},{"cell_type":"code","source":"def clean(tweet): \n            \n    # Special characters\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n           \n    # Urls\n    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n        \n    # Words with punctuations and special characters\n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n    for p in punctuations:\n        tweet = tweet.replace(p, '')\n        \n    # ... and ..\n    tweet = tweet.replace('...', ' ... ')\n    if '...' not in tweet:\n        tweet = tweet.replace('..', ' ... ')\n        \n    #Spaces\n    tweet = tweet.replace('  ', ' ')\n    tweet = tweet.replace('   ', ' ')\n        \n    tweet = tweet.lower()\n    \n    tweet = \" \".join(tweet.split())\n    \n    return tweet\n\ntrain_df['text_cleaned'] = train_df['text'].apply(lambda s : clean(s))\ntest_df['text_cleaned'] = test_df['text'].apply(lambda s : clean(s))\n\ntrain_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_df['text_cleaned'], glove_embeddings)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set'.format(train_glove_vocab_coverage, train_glove_text_coverage))\ntest_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_df['text_cleaned'], glove_embeddings)\nprint('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set'.format(test_glove_vocab_coverage, test_glove_text_coverage))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:05:30.45864Z","iopub.execute_input":"2021-10-01T19:05:30.458947Z","iopub.status.idle":"2021-10-01T19:05:32.624982Z","shell.execute_reply.started":"2021-10-01T19:05:30.458917Z","shell.execute_reply":"2021-10-01T19:05:32.624089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**That's more like it. Now let's prepare our train and test corpus.**","metadata":{}},{"cell_type":"code","source":"def create_corpus(df):\n    \n    corpus = []\n    \n    for tweet in tqdm(df['text_cleaned']):\n        words = [word.lower() for word in word_tokenize(tweet) if((word.isalpha() == 1) and word not in stop_words)]\n        corpus.append(words)\n        \n    return corpus\n\ntrain_corpus = create_corpus(train_df)\ntest_corpus = create_corpus(test_df)\n\ntokenizer_obj_train = Tokenizer()\ntokenizer_obj_train.fit_on_texts(train_corpus)\nseq_train=tokenizer_obj_train.texts_to_sequences(train_corpus)\n\ntokenizer_obj_test = Tokenizer()\ntokenizer_obj_test.fit_on_texts(test_corpus)\nseq_test=tokenizer_obj_test.texts_to_sequences(test_corpus)\n\nMAX_WORDS = 0\nfor i in seq_train:\n    MAX_WORDS = max(MAX_WORDS, len(i))\n\nfor i in seq_test:\n    MAX_WORDS = max(MAX_WORDS, len(i))\n    \ntrain_pad = pad_sequences(seq_train, maxlen = MAX_WORDS, truncating = 'post', padding = 'post')\ntest_pad = pad_sequences(seq_test, maxlen = MAX_WORDS, truncating = 'post', padding = 'post')\n\ntokenizer_obj_train.word_index.update(tokenizer_obj_test.word_index)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:05:32.62615Z","iopub.execute_input":"2021-10-01T19:05:32.626426Z","iopub.status.idle":"2021-10-01T19:05:35.649074Z","shell.execute_reply.started":"2021-10-01T19:05:32.626398Z","shell.execute_reply":"2021-10-01T19:05:35.647827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_index = tokenizer_obj_train.word_index\nprint('Number of unique words:',len(word_index))","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:05:35.650797Z","iopub.execute_input":"2021-10-01T19:05:35.651244Z","iopub.status.idle":"2021-10-01T19:05:35.657016Z","shell.execute_reply.started":"2021-10-01T19:05:35.651202Z","shell.execute_reply":"2021-10-01T19:05:35.656092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words, 300))\n\ncounter = 0\n\nfor word, i in tqdm(word_index.items()):\n    emb_vec = glove_embeddings.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec\n    else:\n        counter += 1\n\ndel glove_embeddings\ncounter","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:05:35.658414Z","iopub.execute_input":"2021-10-01T19:05:35.658751Z","iopub.status.idle":"2021-10-01T19:05:36.916327Z","shell.execute_reply.started":"2021-10-01T19:05:35.658719Z","shell.execute_reply":"2021-10-01T19:05:36.915323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We are missing about 25% of the words in the corpus, which is kind of high, but getting to a better percentage would require a large amount of manual work on the dataset and that is not our purpose.**","metadata":{}},{"cell_type":"markdown","source":"# <font size=\"+2\" color=\"black\"><b>3. Neural Networks </b></font><br><a id=\"3\"></a>\n\n**We will try 3 types of models: Bidirectional LSTM, 1D CNN + Bidirectional LSTM and Bidirectional LSTM + FC Network on numerical features.**","metadata":{}},{"cell_type":"markdown","source":"**First, let's split the original training set in training and validation.**","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(train_pad, train_df['target'].values, test_size = 0.25, random_state = 42)\n\nprint(\"Shape of train\", x_train.shape)\nprint(\"Shape of Validation\", x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:05:36.917631Z","iopub.execute_input":"2021-10-01T19:05:36.918116Z","iopub.status.idle":"2021-10-01T19:05:36.926203Z","shell.execute_reply.started":"2021-10-01T19:05:36.918054Z","shell.execute_reply":"2021-10-01T19:05:36.92545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"+1\" color=\"black\"><b>3.1 Bidirectional LSTM </b></font><br><a id=\"3.1\"></a>\n\n**Now, for our first model. We will try a simple bidirectional LSTM with a lot of regularization since the dataset is so small.**","metadata":{}},{"cell_type":"code","source":"inp = Input(shape = (MAX_WORDS, ))\nx = Embedding(num_words, 300, weights = [embedding_matrix])(inp)\nx = Bidirectional(LSTM(MAX_WORDS, dropout = 0.2, recurrent_dropout = 0.2, return_sequences = True))(x)\nx = ActivityRegularization(l2 = 0.1)(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.2)(x)\nx = BatchNormalization()(x)\nx = Dense(4, activation = \"relu\")(x)\nx = ActivityRegularization(l2 = 0.1)(x)\nx = Dropout(0.2)(x)\nx = Dense(1, activation = \"sigmoid\")(x)\n\nmodel_LSTM = Model(inputs = inp, outputs = x)\nmodel_LSTM.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nprint(model_LSTM.summary())","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:05:36.92767Z","iopub.execute_input":"2021-10-01T19:05:36.927991Z","iopub.status.idle":"2021-10-01T19:05:37.844796Z","shell.execute_reply.started":"2021-10-01T19:05:36.927958Z","shell.execute_reply":"2021-10-01T19:05:37.843335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\n    'model_LSTM.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\n\nmodel_LSTM.fit(x_train, y_train, batch_size = 32, epochs = 15, validation_data = (x_test, y_test), callbacks = [reduce_lr, checkpoint])","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:05:37.84662Z","iopub.execute_input":"2021-10-01T19:05:37.846985Z","iopub.status.idle":"2021-10-01T19:10:18.572083Z","shell.execute_reply.started":"2021-10-01T19:05:37.846951Z","shell.execute_reply":"2021-10-01T19:10:18.571223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We'll finish by checking the performance on the validation set.**","metadata":{}},{"cell_type":"code","source":"model_LSTM.load_weights('model_LSTM.h5')\npred_LSTM = model_LSTM.predict(x_test)\n\nacc_LSTM = accuracy_score(y_test, np.where(pred_LSTM > 0.5, 1, 0))\nf1_LSTM = f1_score(y_test, np.where(pred_LSTM > 0.5, 1, 0))\n\nprint(acc_LSTM, f1_LSTM)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:10:18.576378Z","iopub.execute_input":"2021-10-01T19:10:18.576949Z","iopub.status.idle":"2021-10-01T19:10:19.617138Z","shell.execute_reply.started":"2021-10-01T19:10:18.576912Z","shell.execute_reply":"2021-10-01T19:10:19.615682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"+1\" color=\"black\"><b>3.2 1D CNN + Bidirectional LSTM </b></font><br><a id=\"3.2\"></a>\n\n**For our second model, we will add a 1D convolutional layer with a kernel size of 3 hoping that it will help the model understand meaning from tri-grams.**","metadata":{}},{"cell_type":"code","source":"inp = Input(shape = (MAX_WORDS, ))\nx = Embedding(num_words, 300, weights = [embedding_matrix])(inp)\nx = Conv1D(MAX_WORDS, 3)(x)\nx = Activation('relu')(x)\nx = MaxPooling1D(pool_size=2, strides=2)(x)\nx = Bidirectional(LSTM(MAX_WORDS, dropout = 0.2, recurrent_dropout = 0.2, return_sequences = True))(x)\nx = ActivityRegularization(l2 = 0.1)(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.2)(x)\nx = BatchNormalization()(x)\nx = Dense(4, activation = \"relu\")(x)\nx = ActivityRegularization(l2 = 0.1)(x)\nx = Dropout(0.2)(x)\nx = Dense(1, activation = \"sigmoid\")(x)\n\nmodel_CNN_LSTM = Model(inputs = inp, outputs = x)\nmodel_CNN_LSTM.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nprint(model_CNN_LSTM.summary())","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:10:19.618923Z","iopub.execute_input":"2021-10-01T19:10:19.619401Z","iopub.status.idle":"2021-10-01T19:10:20.133923Z","shell.execute_reply.started":"2021-10-01T19:10:19.619359Z","shell.execute_reply":"2021-10-01T19:10:20.132453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\n    'model_CNN_LSTM.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\n\nmodel_CNN_LSTM.fit(x_train, y_train, batch_size = 64, epochs = 10, validation_data = (x_test, y_test), callbacks = [reduce_lr, checkpoint])","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:10:20.136297Z","iopub.execute_input":"2021-10-01T19:10:20.13667Z","iopub.status.idle":"2021-10-01T19:11:33.278513Z","shell.execute_reply.started":"2021-10-01T19:10:20.136638Z","shell.execute_reply":"2021-10-01T19:11:33.277271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As with the first model, we'll finish by checking the performance on the validation set.**","metadata":{}},{"cell_type":"code","source":"model_CNN_LSTM.load_weights('model_CNN_LSTM.h5')\npred_CNN_LSTM = model_CNN_LSTM.predict(x_test)\n\nacc_CNN_LSTM = accuracy_score(y_test, np.where(pred_CNN_LSTM > 0.5, 1, 0))\nf1_CNN_LSTM = f1_score(y_test, np.where(pred_CNN_LSTM > 0.5, 1, 0))\n\nprint(acc_CNN_LSTM, f1_CNN_LSTM)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:11:33.280378Z","iopub.execute_input":"2021-10-01T19:11:33.280835Z","iopub.status.idle":"2021-10-01T19:11:34.136535Z","shell.execute_reply.started":"2021-10-01T19:11:33.280789Z","shell.execute_reply":"2021-10-01T19:11:34.13577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=\"+1\" color=\"black\"><b>3.3 Bidirectional LSTM + FC Network on numerical features </b></font><br><a id=\"3.3\"></a>\n\n**Here, we will try using the numerical features that we collected in the first chapter. In order to do that, we need to fetch them.**","metadata":{}},{"cell_type":"code","source":"numerical_features = train_df[['tweet_sentiment', 'nr_of_char', 'nr_of_words', 'nr_of_unique_words', 'nr_of_punctuation', 'nr_of_stopwords']].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:11:34.137842Z","iopub.execute_input":"2021-10-01T19:11:34.138378Z","iopub.status.idle":"2021-10-01T19:11:34.145387Z","shell.execute_reply.started":"2021-10-01T19:11:34.138341Z","shell.execute_reply":"2021-10-01T19:11:34.144485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp_inp = Input(shape = (MAX_WORDS, ))\nx = Embedding(num_words, 300, weights = [embedding_matrix])(nlp_inp)\nx = Bidirectional(LSTM(MAX_WORDS,dropout = 0.2, recurrent_dropout = 0.2, return_sequences = True))(x)\nx = ActivityRegularization(l2 = 0.1)(x)\nx = GlobalMaxPool1D()(x)\nx = Dropout(0.2)(x)\nx = BatchNormalization()(x)\nx = Dense(4, activation = \"relu\")(x)\nx = ActivityRegularization(l2 = 0.1)(x)\n\nnum_features_inp = Input(shape = (6, ), name = 'num_features_inp')\ny = Dense(4, activation = \"relu\")(num_features_inp)\nx = ActivityRegularization(l2 = 0.1)(x)\n\nz = Concatenate()([x, y])\nz = Dropout(0.2)(z)\nz = Dense(1, activation = \"sigmoid\")(z)\n\nmodel_LSTM_FC = Model(inputs = [nlp_inp, num_features_inp], outputs = z)\nmodel_LSTM_FC.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nprint(model_LSTM_FC.summary())","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:11:34.146698Z","iopub.execute_input":"2021-10-01T19:11:34.147218Z","iopub.status.idle":"2021-10-01T19:11:34.679778Z","shell.execute_reply.started":"2021-10-01T19:11:34.147184Z","shell.execute_reply":"2021-10-01T19:11:34.67882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Also, we will have to re-create the testing and validation data since we also need the numerical features for this model.**","metadata":{}},{"cell_type":"code","source":"x_train_text, x_test_text, x_train_num, x_test_num, y_train, y_test = train_test_split(train_pad, numerical_features, train_df['target'].values, test_size=0.25, random_state = 42)\n\nprint(\"Shape of train\", x_train_text.shape)\nprint(\"Shape of Validation\", x_test_text.shape)\nprint(\"Shape of train\", x_train_num.shape)\nprint(\"Shape of Validation\", x_test_num.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:11:34.68207Z","iopub.execute_input":"2021-10-01T19:11:34.68266Z","iopub.status.idle":"2021-10-01T19:11:34.693964Z","shell.execute_reply.started":"2021-10-01T19:11:34.682622Z","shell.execute_reply":"2021-10-01T19:11:34.692725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(\n    'model_LSTM_FC.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\n\nmodel_LSTM_FC.fit([x_train_text, x_train_num], y_train, batch_size=32, epochs=15, validation_data=([x_test_text, x_test_num], y_test), callbacks = [reduce_lr, checkpoint])","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:11:34.695016Z","iopub.execute_input":"2021-10-01T19:11:34.695353Z","iopub.status.idle":"2021-10-01T19:16:14.205871Z","shell.execute_reply.started":"2021-10-01T19:11:34.695323Z","shell.execute_reply":"2021-10-01T19:16:14.20466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As with the first two models, we'll finish by checking the performance on the validation set.**","metadata":{}},{"cell_type":"code","source":"model_LSTM_FC.load_weights('model_LSTM_FC.h5')\npred_LSTM_FC = model_LSTM_FC.predict([x_test_text, x_test_num])\n\nacc_LSTM_FC = accuracy_score(y_test, np.where(pred_LSTM_FC > 0.5, 1, 0))\nf1_LSTM_FC = f1_score(y_test, np.where(pred_LSTM_FC > 0.5, 1, 0))\n\nprint(acc_LSTM_FC, f1_LSTM_FC)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:16:14.209511Z","iopub.execute_input":"2021-10-01T19:16:14.209901Z","iopub.status.idle":"2021-10-01T19:16:15.244133Z","shell.execute_reply.started":"2021-10-01T19:16:14.209858Z","shell.execute_reply":"2021-10-01T19:16:15.241923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now that we have the accuracies and f1-scores for all of our first 3 models, let's compare them.**","metadata":{}},{"cell_type":"code","source":"print(\"Accuracy and F1-Score of the LSTM: \", acc_LSTM, f1_LSTM)\nprint(\"Accuracy and F1-Score of the CNN_LSTM: \", acc_CNN_LSTM, f1_CNN_LSTM)\nprint(\"Accuracy and F1-Score of the LSTM_FC: \", acc_LSTM_FC, f1_LSTM_FC)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:16:15.245797Z","iopub.execute_input":"2021-10-01T19:16:15.246134Z","iopub.status.idle":"2021-10-01T19:16:15.253629Z","shell.execute_reply.started":"2021-10-01T19:16:15.246083Z","shell.execute_reply":"2021-10-01T19:16:15.252311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Unfortunately, the performance is not really what we would of wanted and it doesn't differ that much between the 3 models. I expect this to happen because of the small dataset and the fact that there are lots of tweets with very few words, of which 25% we don't even have in the embeddings. In order to get good performance with the 3 models we tried until now, we would need to clean the data much better. As I previously mentioned, this would be a tedious task and not the target of this notebook. Therefore, let's try Google's Bert and see if it has any improvements over our approaches.**","metadata":{}},{"cell_type":"markdown","source":"<font size=\"+1\" color=\"black\"><b>3.4 Bert </b></font><br><a id=\"3.4\"></a>\n\n**We are going to use the SimpleTransformers library in order to try and train Bert for our problem. In order to use it, we need to install it and then put the data in a specific format.**","metadata":{}},{"cell_type":"code","source":"!pip install transformers\n!pip install simpletransformers\n!pip uninstall -y pyarrow\n!pip install 'pyarrow>=1.0.0, <5.0.0'\n!pip uninstall -y tqdm\n!pip install 'tqdm==v4.43.0'\n!pip uninstall -y tokenizers\n!pip install 'tokenizers>=0.10.1,<0.11'","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:16:15.25513Z","iopub.execute_input":"2021-10-01T19:16:15.255473Z","iopub.status.idle":"2021-10-01T19:17:51.736307Z","shell.execute_reply.started":"2021-10-01T19:16:15.255441Z","shell.execute_reply":"2021-10-01T19:17:51.734694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = train_df[[\"text_cleaned\", \"target\"]]\ntrain_df_V2 = columns.copy()","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:17:51.738326Z","iopub.execute_input":"2021-10-01T19:17:51.738724Z","iopub.status.idle":"2021-10-01T19:17:51.750315Z","shell.execute_reply.started":"2021-10-01T19:17:51.738681Z","shell.execute_reply":"2021-10-01T19:17:51.749242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rename = {\"text_cleaned\": \"text\", \"target\": \"labels\"}\ntrain_df_V2.rename(columns = rename, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:17:51.752257Z","iopub.execute_input":"2021-10-01T19:17:51.752669Z","iopub.status.idle":"2021-10-01T19:17:51.763804Z","shell.execute_reply.started":"2021-10-01T19:17:51.752631Z","shell.execute_reply":"2021-10-01T19:17:51.76195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x_y = train_df_V2.sample(frac = 0.75, random_state = 42)\ntest_x_y = pd.concat([train_df_V2, train_x_y]).drop_duplicates(keep=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:17:51.76587Z","iopub.execute_input":"2021-10-01T19:17:51.766356Z","iopub.status.idle":"2021-10-01T19:17:52.423693Z","shell.execute_reply.started":"2021-10-01T19:17:51.766244Z","shell.execute_reply":"2021-10-01T19:17:52.422211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from simpletransformers.classification import ClassificationModel, ClassificationArgs\n\n\nmodel_args = ClassificationArgs()\nmodel_args.use_early_stopping = True\nmodel_args.early_stopping_delta = 0.01\nmodel_args.early_stopping_metric = \"mcc\"\nmodel_args.early_stopping_metric_minimize = False\nmodel_args.early_stopping_patience = 5\nmodel_args.evaluate_during_training_steps = 1000\nmodel_args.reprocess_input_data = True\nmodel_args.overwrite_output_dir = True\nmodel_args.no_save = True\n\nmodel_bert = ClassificationModel('bert', 'bert-base-uncased', args=model_args, use_cuda=False)\nmodel_bert.train_model(train_x_y)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T19:17:52.425352Z","iopub.execute_input":"2021-10-01T19:17:52.425979Z","iopub.status.idle":"2021-10-01T20:29:37.21447Z","shell.execute_reply.started":"2021-10-01T19:17:52.425928Z","shell.execute_reply":"2021-10-01T20:29:37.212973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As with the first three models, we'll finish by checking the performance on the validation set. Admittedly, it is not the same validation set as before, but it should be just as useful to show the model's performance.**","metadata":{}},{"cell_type":"code","source":"pred_bert, out_bert = model_bert.predict(test_x_y['text'].tolist())\n\nacc_bert = accuracy_score(test_x_y['labels'].to_numpy(), pred_bert)\nf1_bert = f1_score(test_x_y['labels'].to_numpy(), pred_bert)\n\nprint(acc_bert, f1_bert)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T20:37:32.267829Z","iopub.execute_input":"2021-10-01T20:37:32.268283Z","iopub.status.idle":"2021-10-01T20:43:18.817187Z","shell.execute_reply.started":"2021-10-01T20:37:32.26824Z","shell.execute_reply":"2021-10-01T20:43:18.815683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now that seems a bit better.**","metadata":{}},{"cell_type":"markdown","source":"# <font size=\"+2\" color=\"black\"><b>4. Submission </b></font><br><a id=\"4\"></a>\n\n**We will train the Bert model on the whole training dataset and use it to make the final submission.**","metadata":{}},{"cell_type":"code","source":"model_bert = ClassificationModel(\"bert\", \"bert-base-uncased\", args=model_args, use_cuda=False)\nmodel_bert.train_model(train_df_V2)\n\nfinal_pred_bert, final_out_bert = model_bert.predict(test_df['text_cleaned'].tolist())\n\nsubmit = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\nsubmit['target'] = final_pred_bert\nsubmit.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-01T20:43:18.820546Z","iopub.execute_input":"2021-10-01T20:43:18.821056Z","iopub.status.idle":"2021-10-01T22:28:57.080628Z","shell.execute_reply.started":"2021-10-01T20:43:18.821005Z","shell.execute_reply":"2021-10-01T22:28:57.078982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font size=\"+2\" color=\"black\"><b>5. Conclusions </b></font><br><a id=\"5\"></a>\n\n**In conclusion, as expected, Bert does do a better job at the task at hand than our previous simpler models. I'm pretty sure that with enough data cleaning, the simpler models would have done just fine. Furthermore, with more data cleaning and some fine tuning, the Bert model could have achieved a much higher score, but as previously stated this is not the goal of this notebook. The goal was to practice investigating detasets, visalisations and model creation. In my opinion, the goal has been achieved.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}