{"cells":[{"metadata":{},"cell_type":"markdown","source":"I have used the [ULMFiT](https://arxiv.org/abs/1801.06146) method on the task. It comprises the following training steps:\n\n- Take a language model (LM) pre-trained on a large general domain corpus, in this case an AWD-LSTM trained on WikiText-103\n- Fine-tune the LM on the task text (train and test) using the various fastai training tricks including traingular slanted learning rates\n- Further fine-tune on the classification task using gradual unfreezing of layers\n\nI've managed to get up to 0.818 F1, but this varies significantly between runs."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"## libraries\nimport numpy as np\nimport pandas as pd\nfrom fastai.text import *\nfrom pathlib import Path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create directory and path for models\nif not os.path.isdir('../model'):\n    os.makedirs('../model')\n    \npath_model = Path(\"../model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## read in datasets\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsub = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create databunch with both train and test text and label for language model\nbs = 48\ndata_lm = (TextList.from_df(pd.concat([train[['text']], test[['text']]], ignore_index=True, axis=0), path_model)\n           .split_by_rand_pct(0.1)\n           .label_for_lm()\n           .databunch(bs=bs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check tokenisation looks ok on training set\ndata_lm.show_batch()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create lm learner with pre-trained model\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## run lr finder\nlearn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## plot lr finder\nlearn.recorder.plot(skip_end=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## train for one epoch frozen\nlearn.fit_one_cycle(1, 1.3e-2, moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## unfreeze and train for four further cycles unfrozen\nlearn.unfreeze()\nlearn.fit_one_cycle(4, 1e-3, moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## save model and encoder\nlearn.save('fine_tuned')\nlearn.save_encoder('fine_tuned_enc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## training set with text and target\ndf = train[['text', 'target']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## test set with text\ndf_test = test[['text']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create databunch for classification task, \n## including randomly selected validation set, and test set\nbs = 16\ndata_clas = (TextList.from_df(df, path_model, vocab=data_lm.vocab)\n             #.split_none()\n             .split_by_rand_pct(0.1)\n             .label_from_df('target')\n             .add_test(TextList.from_df(df_test, path_model, vocab=data_lm.vocab))\n             .databunch(bs=bs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## check test set looks ok\ndata_clas.show_batch(ds_type=DatasetType.Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create classification learning, including f1 score in metrics, and add encoder\nlearn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, FBeta(beta=1)])\nlearn.load_encoder('fine_tuned_enc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## run lr finder\nlearn.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## plot lr finder\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## train for 1 cycle frozen\nlearn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## unfreeze the last 2 layers and train for 1 cycle\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## unfreeze the last 3 layers and train for 1 cycle\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## unfreeze all and train for 2 cycles\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## get test set predictions and ids\npreds, _ = learn.get_preds(ds_type=DatasetType.Test,  ordered=True)\npreds = preds.argmax(dim=-1)\n\nid = test['id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_submission = pd.DataFrame({'id': id, 'target': preds})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}