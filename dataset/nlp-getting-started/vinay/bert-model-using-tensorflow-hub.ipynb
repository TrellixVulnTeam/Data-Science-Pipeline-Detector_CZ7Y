{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\ntrain = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\ntrain.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n!pip install sentencepiece","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_hub as hub\nimport tokenization\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(train['text'])\ny = np.array(train['target'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(texts,tokenizer, max_len = 512):\n  all_tokens = []\n  all_masks = []\n  all_segments = []\n\n  for text in texts:\n    text = tokenizer.tokenize(text)\n\n    text = text[:max_len-2]\n    input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n\n    tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n    pad_len = max_len - len(input_sequence)\n\n    tokens += [0]*pad_len\n    pad_mask = [1]*len(input_sequence)+[0]*pad_len\n\n    segment_id = [0]*max_len\n\n    all_tokens.append(tokens)\n    all_masks.append(pad_mask)\n    all_segments.append(segment_id)\n\n  return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(bert_layer,max_len=512):\n\n  input_word_ids = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32,name=\"input_word_ids\")\n  input_mask = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32,name=\"input_mask\")\n  input_segment_ids = tf.keras.layers.Input(shape=(max_len,),dtype=tf.int32,name=\"input_segment_ids\")\n\n  _,sequence_output = bert_layer([input_word_ids,input_mask,input_segment_ids])\n  clf_output = sequence_output[:,0,:]\n  model_X = tf.keras.layers.Dense(100,activation='relu')(clf_output)\n  model_X = tf.keras.layers.BatchNormalization()(model_X)\n  model_X = tf.keras.layers.Dropout(0.5)(model_X)\n  model_X = tf.keras.layers.Dense(100,activation='relu')(model_X)\n  model_X = tf.keras.layers.BatchNormalization()(model_X)\n  model_X = tf.keras.layers.Dropout(0.5)(model_X)\n  model_X = tf.keras.layers.Dense(100,activation='relu')(model_X)\n  model_X = tf.keras.layers.BatchNormalization()(model_X)\n  model_X = tf.keras.layers.Dropout(0.5)(model_X)\n  out = tf.keras.layers.Dense(1,activation='sigmoid')(model_X)\n\n  model = tf.keras.models.Model(inputs=[input_word_ids,input_mask,input_segment_ids],outputs=out)\n\n  return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodule_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\ntrain_input = bert_encode(X_train,tokenizer,max_len=264)\nval_input = bert_encode(X_test,tokenizer,max_len=264)\ntest_input = bert_encode(test.text.values,tokenizer,max_len=264)\ntrain_labels = y_train\nval_labels = y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model(bert_layer,max_len=264)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_input[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\nes = EarlyStopping(monitor='val_loss',patience=3,verbose=1,restore_best_weights=True,min_delta=0.01)\n\nmodel.compile(optimizer=Adam(lr=1e-5),loss='binary_crossentropy',metrics=['accuracy'])\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_data=(val_input,val_labels),\n    epochs=50,\n    callbacks=[checkpoint,es],\n    batch_size=12\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(train_history.history['loss'], label='train')\nplt.plot(train_history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(test_input)\n\nans = pd.DataFrame({'id':np.array(test['id']),'target':np.array(y_pred.round().astype(int)).reshape(-1)})\nans.to_csv('submission.csv',index=False)\nans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}