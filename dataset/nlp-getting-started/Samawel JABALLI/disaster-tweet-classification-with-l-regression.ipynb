{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_data  =pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['text'][0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# text preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef  clean_text(df, text_field, new_text_field_name):\n    df[new_text_field_name] = df[text_field].str.lower()\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n    # remove numbers\n    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n    \n    return df\ndata_clean = clean_text(train_data, 'text', 'text_clean')\ndata_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk.corpus\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndata_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndata_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# text tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk \nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ndata_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\ndata_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stemming words with NLTK"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\ndef word_stemmer(text):\n    stem_text = [PorterStemmer().stem(i) for i in text]\n    return stem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\ndata_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# text lemmatisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\ndef word_lemmatizer(text):\n    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n    return lem_text\ndata_clean['text_clean_tokens'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))\ndata_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_clean['text_clean'],data_clean['target'],random_state = 0)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\npipeline_sgd = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf',  TfidfTransformer()),\n    ('lr', LogisticRegression()),\n])\nmodel = pipeline_sgd.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ny_predict = model.predict(X_test)\nprint(classification_report(y_test, y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion Matrix Visualisation\nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(model, X_test, y_test) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_test_clean = test_data.copy()\nsubmission_test_clean = clean_text(submission_test_clean, \"text\",\"text_clean\")\nsubmission_test_clean['text_clean'] = submission_test_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\nsubmission_test_clean.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_test_pred = model.predict(submission_test_clean['text_clean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_col = test_data['id']\nsubmission_df_kaggle = pd.DataFrame({\"id\": id_col,\"target\": submission_test_pred})\nsubmission_df_kaggle.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df_kaggle.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}