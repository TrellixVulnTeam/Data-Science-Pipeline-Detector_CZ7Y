{"cells":[{"metadata":{"id":"cRJITwe5MkU7","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re","execution_count":null,"outputs":[]},{"metadata":{"id":"II5ygoSGNAxD","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"9kT--rkqNGyx","outputId":"5455c2f4-fca8-4d7d-af5b-266e84cb8d80","trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"id":"i-yCXPc1NHYJ","trusted":true},"cell_type":"code","source":"train = train.drop(['keyword', 'location'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"id":"JKhBT1ijNUiM","outputId":"7c8d720e-4ae3-4de9-e60e-e513a6dd3c29","trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Cleaning"},{"metadata":{"id":"uuLuCzttUpIR","outputId":"6450a99f-5bff-4d9c-a2e5-1ba74bae5ca3","trusted":true},"cell_type":"code","source":"import re #remove punctuations\nimport nltk\nnltk.download('all')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords #and, in, the, a ... etc\nfrom nltk.stem import WordNetLemmatizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning Train text"},{"metadata":{"id":"8YYq5qLPU7_b","trusted":true},"cell_type":"code","source":"corpus = []\nfor i in range(0, 7613):\n    review_train = re.sub(r\"[^a-zA-Z]\", ' ', train['text'][i]) #replace anything that is not a letter like \"\" , ... with space\n    review_train = re.sub(r'http\\S+', r'', review_train)\n    review_train = re.sub(r'#([^\\s]+)', r'\\1', review_train)  \n    review_train = re.sub('[\\s]+', ' ', review_train)\n\n    review_train = review_train.lower()  #all letters to lower-case\n    review_train = review_train.split() #splitting review in diferent words\n    \n\n    lemmatizer = WordNetLemmatizer()\n    stopwords1 = stopwords.words('english')\n    \n    review_train = [lemmatizer.lemmatize(word) for word in review_train if not word in stopwords1]\n\n    review_train = ' '.join(review_train) #separating words with space and joining\n    corpus.append(review_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"Z1vo4m7ShXZ3","outputId":"413d6ec3-2bee-4466-8503-09464a8f490e","trusted":true},"cell_type":"code","source":"#corpus","execution_count":null,"outputs":[]},{"metadata":{"id":"emL_4rKOXl3B","trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning Test text"},{"metadata":{"id":"iGaHHqIuXyP5","trusted":true},"cell_type":"code","source":"corpus_test = []\nfor i in range(0, 3263):\n    review = re.sub(r\"[^a-zA-Z]\", ' ', test['text'][i]) #replace anything that is not a letter like \"\" , ... with space\n    review = re.sub(r'http\\S+', r'', review)\n    review = re.sub(r'#([^\\s]+)', r'\\1', review)\n    review = re.sub('[\\s]+', ' ', review)\n\n    review = review.lower()  #all letters to lower-case\n    review = review.split() #splitting review in diferent words\n    \n    lemmatizer_2 = WordNetLemmatizer()\n    stopwords_2 = stopwords.words('english')\n    \n    review = [lemmatizer_2.lemmatize(word) for word in review if not word in stopwords_2]\n\n    review = ' '.join(review) #separating words with space and joining\n    corpus_test.append(review)","execution_count":null,"outputs":[]},{"metadata":{"id":"QMeCRlMYi-RO","trusted":true},"cell_type":"code","source":"#corpus_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TfidfVectorizer"},{"metadata":{"id":"Db8Tz9c5Q0VH","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":null,"outputs":[]},{"metadata":{"id":"ci6ywJC-g_2_","trusted":true},"cell_type":"code","source":"vect = TfidfVectorizer(min_df=2 ,max_features = None,analyzer=\"word\",  ngram_range=(1,3))","execution_count":null,"outputs":[]},{"metadata":{"id":"Gkc8Nm0lhL4G","trusted":true},"cell_type":"code","source":"X_vect = vect.fit_transform(corpus)","execution_count":null,"outputs":[]},{"metadata":{"id":"aRxO4xCMhYFp","trusted":true},"cell_type":"code","source":"y_vect = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_vect.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_vect.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"pi5Yem5whghn","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X_vect, y_vect, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"Q-GFF653hzbe","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nmodel_1 = LogisticRegression(penalty = 'l2', C=3, max_iter = 550)","execution_count":null,"outputs":[]},{"metadata":{"id":"4MEf_f9QiJan","outputId":"3f528847-4b88-4b06-a8b8-cda7b9562162","trusted":true},"cell_type":"code","source":"model_1.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"PZxHyMbziSds","outputId":"1dec2c9d-059e-4887-bc8f-8c97f477ef5c","trusted":true},"cell_type":"code","source":"model_1.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"5idb1Rj67Xmj","trusted":true},"cell_type":"code","source":"X_vect_test = vect.transform(corpus_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting"},{"metadata":{"id":"H2WPOcDk7XbZ","trusted":true},"cell_type":"code","source":"predict_n = model_1.predict(X_vect_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"id":"LrcD8WaQajUj","trusted":true},"cell_type":"code","source":"test_id = test['id']","execution_count":null,"outputs":[]},{"metadata":{"id":"UAn4VEB07P3O","trusted":true},"cell_type":"code","source":"submission_2 = pd.DataFrame({'id':test_id, 'target':predict_n})","execution_count":null,"outputs":[]},{"metadata":{"id":"E_7PF-W07Pzd","outputId":"47915376-02f1-4d1e-9a9f-3e8481456d6a","trusted":true},"cell_type":"code","source":"submission_2","execution_count":null,"outputs":[]},{"metadata":{"id":"H9CRAa-v8PuP","trusted":true},"cell_type":"code","source":"submission_2.to_csv('./NLP_start_9.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}