{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading the dataset ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Descriptive statistics using info() and describe()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"info() gives us the details of datatypes of each feature and null value count.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Target feature ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(train['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train['target'].value_counts()/train.shape[0])*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As shown above,we saw the count of target feature value-0,1 and visualised using seaborn countplot,Finally checked the percentage of count.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Missing Values ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you see,there are missing values in keyword and location features.we now fill those missing values with 'no_keyword' and 'no_location'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].fillna('no_keyword',inplace=True)\ntrain['location'].fillna('no_location',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values left.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Visualising keyword and location features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['location'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['location'].value_counts()[:20].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].value_counts()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].value_counts()[:20].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Important terms and definitions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Stop words:\n                A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.We can check list of stopwords as shown below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#List of stopwords in english\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stemming:\n                    Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# lemmatization:\n                        Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word. \n                        Difference between stemming and lemmatization is that lemmatization gives proper meaningful dictionary words.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Bag of words:\n                            The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n  \n Let’s take an example to understand this concept in depth.\n\n“It was the best of times”\n“It was the worst of times”\n“It was the age of wisdom”\n“It was the age of foolishness”\n\nWe treat each sentence as a separate document and we make a list of all words from all the four documents excluding the punctuation. We get,\n\n‘It’, ‘was’, ‘the’, ‘best’, ‘of’, ‘times’, ‘worst’, ‘age’, ‘wisdom’, ‘foolishness’\n\nThe next step is the create vectors. Vectors convert text that can be used by the machine learning algorithm.\n\nWe take the first document — “It was the best of times” and we check the frequency of words from the 10 unique words.\n“it” = 1\n“was” = 1\n“the” = 1\n“best” = 1\n“of” = 1\n“times” = 1\n“worst” = 0\n“age” = 0\n“wisdom” = 0\n“foolishness” = 0\n\nRest of the documents will be:\n“It was the best of times” = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n“It was the worst of times” = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n“It was the age of wisdom” = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n“It was the age of foolishness” = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Tf-IDF(Term Frequency and Inverse Document Frequency):\n                 TF-IDF weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n1.     Term Frequency (TF): is a scoring of the frequency of the word in the current document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. The term frequency is often divided by the document length to normalize.\n                \n                TF=Number of times term t appers in documnet/total no of terms in document.\n\n2.Inverse Document Frequency (IDF): is a scoring of how rare the word is across documents. IDF is a measure of how rare a term is. Rarer the term, more is the IDF score.                \n                    \n                IDF=log(total no of documents/no of documents with term t in it)\n                \n                \n                Final score=TF*IDF.\n                  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Implementation:\n                        Here i use stemming and bag of words model.Importing nessesary libraries for text preprocessing.\n      1.re-used for regular expressions.\n      2.nltk-natural language tool kit-one of the best library for nlp.\n      3.topwords.\n      4.PorterStemmer(for stemming).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.stem.porter import PorterStemmer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = []\nfor i in range(0,train.shape[0]):\n  review = re.sub('[^a-zA-Z]', ' ',train['text'][i])\n  review = review.lower()\n  review = review.split()\n  ps = PorterStemmer()\n  all_stopwords = stopwords.words('english')\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  review = ' '.join(review)\n  corpus.append(review)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Code Explanation:\n                1.First created a list called corpus to store all the sentences.\n                2.looping through all the sentences in text feature and perform following steps.\n                3.Replace non alphabets with space.\n                4.Convert everything into lowercase.\n                5.Select all the words apart from stop words and apply stemming.\n                6.Finally join all the reviews and append to corpus.\n     The corpus is as follows           ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating bag of words model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1000)\nX = cv.fit_transform(corpus).toarray()\nX[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Spliting data into training and testing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny=train.iloc[:,-1].values\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Naive Bayes model ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score\nprediction=classifier.predict(X_test)\ncm = confusion_matrix(Y_test,prediction)\nprint(cm)\naccuracy_score(Y_test,prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Please upvote if you like,any suggestions and mistakes put it in comments,Thank you.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}