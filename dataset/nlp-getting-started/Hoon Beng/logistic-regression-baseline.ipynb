{"cells":[{"metadata":{"_uuid":"9dacb7a2a5a9e582d7e597be4ea479cedd79489a","_cell_guid":"d49b7eaa-0bf7-48e0-9fe6-51ba03036ba1"},"cell_type":"markdown","source":"# Disasters, real or fake?\n# Logistic regression baseline\nStarted on 16 Jan 2020"},{"metadata":{"_uuid":"8f9dbcf8df12e40c0daee0f48d91fd07cd084cae","_cell_guid":"06e8c4a5-feea-41ea-8831-13ab7089dcc5"},"cell_type":"markdown","source":"##### Comments:\n* In this kernel, I use logistic regression as the binary classifier.\n* I shall start with the twitter text only. My purpose is to create a baseline model. Further on, I will explore adding other features, and using other models to see how much improvement can be made to the classification task.\n* To process the text data, I will simply use Count Vectorizer."},{"metadata":{"_uuid":"5c5f4cc8865644748e11336736bbe584adebe7b1","_cell_guid":"8f6a95ee-cc95-4c9f-a8f7-72ae58ec13d6","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f65d03ddbfd127307d3e415003346eb898b4d6b","_cell_guid":"80d61838-9025-4cba-bb0e-58175586b21b"},"cell_type":"markdown","source":"# Read \"train.csv\" and \"test.csv into pandas"},{"metadata":{"_uuid":"4e35cd5fcae1581dbd6bc51f14728e27fe63fe70","_cell_guid":"094fff47-db10-447c-965e-08056f718bde","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('../input/nlp-getting-started/test.csv')\nsubmission = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89d1c9a4f9598427e8a20d66fa9e56796ad720f6","_cell_guid":"09986b08-eda6-4438-9cbe-52a61d8d57fa"},"cell_type":"markdown","source":"# Examine the train data"},{"metadata":{"_uuid":"2ff9ed83ba328872d446add97695285dc49f4165","_cell_guid":"981dff09-3acf-4014-b38a-22afc02a6654","trusted":true},"cell_type":"code","source":"# check the class distribution for the target label in train_df?\ntrain_df['target'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2bd8ddc6445aad024d327d7b78004f80cfd83f6","_cell_guid":"391dfeb1-5549-4f85-ac4c-4ed66e9cce6b"},"cell_type":"markdown","source":"* The class distribution looks quite balanced, with about 40% 'disaster' tweets."},{"metadata":{"_uuid":"cae81f6b1d9bb475fc486d5fbb81981025cc3672","_cell_guid":"f5abe72a-13a7-41f6-ae8e-1c34dca97110"},"cell_type":"markdown","source":"# Define X and y from train data for use in tokenization by Vectorizers"},{"metadata":{"_uuid":"061d59552c6ef83bea8ecf9ffbf203286aeab6f8","_cell_guid":"49547fd7-9633-4f6a-bd46-84d8966f1e8b","trusted":true},"cell_type":"code","source":"X = train_df['text']\ny = train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"903c319c5b83198edf0af59d49818bd9071ec8dc","_cell_guid":"4a6e6a69-91b7-43a9-8e91-3c8fd20d2790"},"cell_type":"markdown","source":"# Split train data into a training and a validation set"},{"metadata":{"_uuid":"32e74067044fb018d0a71ed97c3326d578b1c0a6","_cell_guid":"438ceb39-2687-41b5-b7cb-8784a1aab35d","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=123)\nprint(X_train.shape, y_train.shape, X_val.shape, y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40e308981530947ddb7b68ac0075b36c7decde43","_cell_guid":"7010f557-ccce-4d8a-a289-d081d2c3c0c9","trusted":true},"cell_type":"code","source":"# examine the class distribution in y_train and y_test\nprint(y_train.value_counts(),'\\n', y_val.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc4c2aeef221f5a97e1bd5ea1154052172175351","_cell_guid":"b2d898ae-79bc-48b8-8544-fb077f876c67"},"cell_type":"markdown","source":"# Vectorize the data"},{"metadata":{"_uuid":"7b7adf15d16408eb99689884906d0d687c2f8407","_cell_guid":"9be5a4f2-0e85-4f9a-ac00-b8e9916116cb","trusted":true},"cell_type":"code","source":"# import and instantiate CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = CountVectorizer(lowercase=True, stop_words='english', token_pattern=r'(?u)\\b\\w+\\b|\\,|\\.|\\;|\\:')\nvect","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b755b1d4db58eeb5b0ab668d1aaf4a651d3de441","_cell_guid":"283c1d48-c267-431b-834e-37c8d9222b3c","trusted":true},"cell_type":"code","source":"# learn the vocabulary in the training data, then use it to create a document-term matrix\nX_train_dtm = vect.fit_transform(X_train)\n# examine the document-term matrix created from X_train\nX_train_dtm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"408301ccb78e3f4056a6d2ebbd239594d1a59da0","_cell_guid":"54050711-560a-47cf-b4f9-2fbaf59bc2e4","trusted":true},"cell_type":"code","source":"# transform the test data using the earlier fitted vocabulary, into a document-term matrix\nX_val_dtm = vect.transform(X_val)\n# examine the document-term matrix from X_test\nX_val_dtm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a989e534fe575ebeb3d5952e221c5b445631738a","_cell_guid":"04ab10ee-4c43-41a2-87a8-ea3c3400a546"},"cell_type":"markdown","source":"# Build and evaluate the disaster tweet classification model using Logistic Regression"},{"metadata":{"_uuid":"e7e5707b19c35c8371a0cff7351bc8aadc33acd1","_cell_guid":"e30be87f-e0a6-4fd7-bff2-b3c37737cbfe","trusted":true},"cell_type":"code","source":"# import and instantiate the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state=8)\nlogreg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6cd7e92d4c27463effddedc8ce5fabb1358323d","_cell_guid":"dc8c51b0-1187-471a-a4ee-957371d43104","trusted":true},"cell_type":"code","source":"# tune hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid_values = {'C':[0.01, 0.1, 1.0, 3.0, 5.0]}\ngrid_logreg = GridSearchCV(logreg, param_grid=grid_values, scoring='neg_log_loss', cv=5)\ngrid_logreg.fit(X_train_dtm, y_train)\ngrid_logreg.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"723423cc5420eac2c5771723ab31b2f4dad610c0","_cell_guid":"9fa49e9a-7a48-4aa7-b78a-bcd334b9332a","trusted":true},"cell_type":"code","source":"# set with recommended parameter\nlogreg = LogisticRegression(C=1.0, random_state=8)\n# train the model using X_train_dtm & y_train\nlogreg.fit(X_train_dtm, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98f1dfbb75b68b65d49fa9f8fa69ebf64554e894","_cell_guid":"4a298037-97bc-49eb-8cf7-bf52d4894f11","trusted":true},"cell_type":"code","source":"# make class predictions for X_test_dtm\ny_pred_val = logreg.predict(X_val_dtm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5000bb48f01b5632c5b3d4387154e32b6be4410a","_cell_guid":"9c459dae-783d-49e0-9b0c-dffd35419dbc","trusted":true},"cell_type":"code","source":"# compute the accuracy of the predictions\nfrom sklearn import metrics\nmetrics.accuracy_score(y_val, y_pred_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"847ef102dcb57e88f06003603a4ec7bd7cdde87f","_cell_guid":"816ea321-cdfd-41d0-b2f8-b89fcc513c43","trusted":true},"cell_type":"code","source":"# compute the accuracy of predictions with the training data\ny_pred_train = logreg.predict(X_train_dtm)\nmetrics.accuracy_score(y_train, y_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"302632e4ecd6790e1cbb92baab53f98517028bcc","_cell_guid":"c6f4a678-7072-44c4-a415-de7ba1d55d52","trusted":true},"cell_type":"code","source":"# look at the confusion matrix for y_test\nmetrics.confusion_matrix(y_val, y_pred_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6a1543f17f4031e31b250aef5d98ddb6b1f1d0d","_cell_guid":"8a238463-87eb-44c5-afcd-794accd04d71","trusted":true},"cell_type":"code","source":"# compute the predicted probabilities for X_test_dtm\ny_pred_prob = logreg.predict_proba(X_val_dtm)\ny_pred_prob[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad9cfb160671a6fbcf92b8ca6e800440948cf232","_cell_guid":"91e879ac-744b-4d10-b798-c99535e5c23b","trusted":true},"cell_type":"code","source":"# compute the log loss number\nmetrics.log_loss(y_val, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef4f2b854c0a2c346a6615ac8de40e5e4bcbe868","_cell_guid":"962928a6-503e-4757-8518-b7b23f74d09a"},"cell_type":"markdown","source":"# Train the Logistic Regression model with the entire dataset from \"train.csv\""},{"metadata":{"_uuid":"d7e09c9fe6524a9f964d50d8048c5226a4a67475","_cell_guid":"5018c932-1fc4-401b-8993-2f1a48f2415e","trusted":true},"cell_type":"code","source":"# Learn the vocabulary in the entire training data, and create the document-term matrix\nX_dtm = vect.fit_transform(X)\n# Examine the document-term matrix created from X_train\nX_dtm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab43f652017097166901c83c679485b444c6c192","_cell_guid":"69cacde4-b731-40a8-b754-e3ecd4decff7","trusted":true},"cell_type":"code","source":"# Train the Logistic Regression model using X_dtm & y\nlogreg.fit(X_dtm, y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65f85a9921ce7d0664762fddb1b592a8bb005cae","_cell_guid":"48c91a38-87d7-4b72-a418-829730c9ee37","trusted":true},"cell_type":"code","source":"# Compute the accuracy of training data predictions\ny_pred_train = logreg.predict(X_dtm)\nmetrics.accuracy_score(y, y_pred_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"968c394fe3ba44506a2a65606317b8d1983d24ee","_cell_guid":"8bc6812d-6592-412e-80a5-ae339488a0bf"},"cell_type":"markdown","source":"# Make predictions on the test data and compute the probabilities for submission"},{"metadata":{"_uuid":"2e45db51b1b8d7ea951db649770e33cc725608eb","_cell_guid":"4b22b19b-fbf5-429b-b980-fc954000dd1c","trusted":true},"cell_type":"code","source":"test = test_df['text']\n# transform the test data using the earlier fitted vocabulary, into a document-term matrix\ntest_dtm = vect.transform(test)\n# examine the document-term matrix from X_test\ntest_dtm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3290ddf6fe948d9b4a0ed26419ef94e35f295e84","_cell_guid":"19cc4c8b-ac52-4ed8-b7b3-9d0d1d96099d","trusted":true},"cell_type":"code","source":"# make author (class) predictions for test_dtm\nLR_y_pred = logreg.predict(test_dtm)\nprint(LR_y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abaefe7c47cac96b429315d480900e8e9e800f98","_cell_guid":"0c051ec9-b17b-42b6-8449-6b692431d6c5","trusted":true},"cell_type":"code","source":"# calculate predicted probabilities for test_dtm\nLR_y_pred_prob = logreg.predict_proba(test_dtm)\nLR_y_pred_prob[:10]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee6d942b861235107e0d94174a8d21c2bad0e9ea","_cell_guid":"5d0970eb-bc44-4fad-91b7-e2a23c2f986d"},"cell_type":"markdown","source":"# Create submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['target'] = LR_y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"86d7d2fa2b5671183438fe0769cfb7594eb4efa5","_cell_guid":"bef9209e-c843-4ca6-be4f-35820ffa258d","trusted":true},"cell_type":"code","source":"# Generate submission file in csv format\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbbce014f73f54e9bf3e88e096ad66b89b88fcf4","_cell_guid":"7a012382-ad72-4c76-b53c-1f756544e23e"},"cell_type":"markdown","source":"### Thank you for reading this.\n### Please upvote if you find it useful. Cheers!"}],"metadata":{"language_info":{"pygments_lexer":"ipython3","version":"3.6.4","mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}