{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"654b7cc6-5ea1-f37b-58fb-8906498a939b"},"source":"## In this kernel we use xgboost to predict house prices.  \n\nThere are 5 parts to this kernel:\n\n1. Import the libraries and data \n1. Prepare the data \n1. Train the xgboost model \n1. Understand the model \n1. Create predictions"},{"cell_type":"markdown","metadata":{"_cell_guid":"83472482-64a5-2434-2ca1-44c29e734970"},"source":"## 1a. Import the libraries we are going to use\n\nWe start by importing the various libraries we are going to use."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"339bfa19-774a-3cdf-9c46-3557dd11b380"},"outputs":[],"source":"import numpy as np # mathematical library including linear algebra\nimport pandas as pd #data processing and CSV file input / output\n\nimport xgboost as xgb # this is the extreme gradient boosting library\nimport matplotlib.pyplot as plt\n\nfrom sklearn import model_selection, preprocessing \nfrom sklearn.metrics import mean_squared_error\n\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"_cell_guid":"9a210082-43aa-ce02-7975-e425beda1cdb"},"source":"## 1b. Next we import the data\n\nNow we read in the training and test data. We are using Pandas \"read_csv\" function for this."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c1117352-92ee-9ac6-8596-039d8e07d9b1"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\ndf_test = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"11d2c3fd-0fa0-7860-ee91-c5a4c1bcaca7"},"source":"##  2. Data preparation\n\n- Create a vector containing the id's for our predictions\n - Create a vector of the target variables in the training set\n - Create joint train and test set to make data wrangling quicker and consistent on train and test\n - Removing the id (could it be a useful source of leakage?)\n - Feature engineering\n     - Convert the date into a number (of days since some point)\n     - Deal with categorical variables\n     - Deal with missing values"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53bf73b2-c1fd-a50d-2c5c-6f850833dc14"},"outputs":[],"source":"\n# Create a vector containing the id's for our predictions\nid_test = df_test.id\n\n#Create a vector of the target variables in the training set\n# Transform target variable so that loss function is correct (ie we use RMSE on transormed to get RMLSE)\n# ylog1p_train_whole will be log(1+y), as suggested by https://github.com/dmlc/xgboost/issues/446#issuecomment-135555130\nylog1p_train_all = np.log1p(df_train['price_doc'].values)\ndf_train = df_train.drop([\"price_doc\"], axis=1)\n\n# Create joint train and test set to make data wrangling quicker and consistent on train and test\ndf_train[\"trainOrTest\"] = \"train\"\ndf_test[\"trainOrTest\"] = \"test\"\nnum_train = len(df_train)\n\ndf_all = pd.concat([df_train, df_test])\ndel df_train\ndel df_test\n\n# Removing the id (could it be a useful source of leakage?)\ndf_all = df_all.drop(\"id\", axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"17505a6a-d4bb-1f8b-7d14-b007147fef84"},"source":"### Feature engineering"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29597ad7-a1f0-8a18-ea89-13b4a678923f"},"outputs":[],"source":"# Convert the date into a number (of days since some point)\nfromDate = min(df_all['timestamp'])\ndf_all['timedelta'] = (df_all['timestamp'] - fromDate).dt.days.astype(int)\nprint(df_all[['timestamp', 'timedelta']].head())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a8d8409-23dd-8c82-19c9-d6c2c1635021"},"outputs":[],"source":"# Add month-year count - i.e. how many sales in the month \nmonth_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ndf_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n# Add week-year count\nweek_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\nweek_year_cnt_map = week_year.value_counts().to_dict()\ndf_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n# Add month and day-of-week\ndf_all['month'] = df_all.timestamp.dt.month\ndf_all['dow'] = df_all.timestamp.dt.dayofweek\n\n# Other feature engineering\ndf_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\ndf_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n\n# Remove timestamp column (may overfit the model in train)\ndf_all.drop(['timestamp'], axis=1, inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"57361ebc-5650-5293-a752-ab32c9b3d096"},"source":"### Encoding categorical features\nWe will take a naive approach and assign a numeric value to each categorical feature in our training and test sets. \nSklearn's preprocessing unit has a tool called LabelEncoder() which can do just that for us. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c4ac82c-961f-66f2-4f2d-90755bb5bc3b"},"outputs":[],"source":"for c in df_all.columns:\n    if df_all[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_all[c].values)) \n        df_all[c] = lbl.transform(list(df_all[c].values))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8d97fbcc-bcef-18d9-720d-6035d740cd8d"},"outputs":[],"source":"# Alternative using the rather nice .select_dtypes\n# df_numeric = df_all.select_dtypes(exclude=['object'])\n# df_obj = df_all.select_dtypes(include=['object']).copy()\n\n# for c in df_obj:\n#    df_obj[c] = pd.factorize(df_obj[c])[0]\n\n# df_all = pd.concat([df_numeric, df_obj], axis=1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"03534b22-32c5-b1fc-9a56-040dd26df214"},"source":"### Addressing problems with NaN in the data\n\nAs we saw from our EDA there were quite a lot of NaN in the data. Our model won't know what to do with these so we need to replace them with something sensible.\n\nThere are quite a few options we can use - the mean, median, most_frequent, or a numeric value like 0. Playing with these will give different results, for now I have it set to use the mean.\n\nFor the random forest we used the median.  Here we just replace na with -1.  Which is better?  Why?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"193ae207-b47f-0f2e-7581-13f3e030f161"},"outputs":[],"source":"# Fill missing values with  -1.  \ndf_all.fillna(-1, inplace = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db960d3d-7922-6276-2671-48112ab046a6"},"outputs":[],"source":"# Convert to numpy values\nX_all = df_all.values\nprint(X_all.shape)\n\n# Create a validation set, with last 20% of data\nnum_val = int(num_train * 0.2)\n\n#X_train_all  = X_all[:num_train]\nX_train      = X_all[:num_train-num_val]\nX_val        = X_all[num_train-num_val:num_train]\nylog1p_train = ylog1p_train_all[:-num_val]\nylog1p_val   = ylog1p_train_all[-num_val:]\n\nX_test = X_all[num_train:]\n\ndf_columns = df_all.columns\n\ndel df_all\n\n#print('X_train_all shape is', X_train_all.shape)\nprint('X_train shape is',     X_train.shape)\nprint('y_train shape is',     ylog1p_train.shape)\nprint('X_val shape is',       X_val.shape)\nprint('y_val shape is',       ylog1p_val.shape)\nprint('X_test shape is',      X_test.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b7fd6182-3795-e5a9-8564-571f0c495c1f"},"source":"Format the train and test sets we modified above for use in xgboost \n(Dmatrix is the format required by the xgboost library)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c6afb75-b6b2-06d3-5fb0-a82a8bac218d"},"outputs":[],"source":"##dtrain_all = xgb.DMatrix(X_train_all, ylog1p_train_all, feature_names = df_columns)\n#dtrain     = xgb.DMatrix(X_train,     ylog1p_train,     feature_names = df_columns)\n#dval       = xgb.DMatrix(X_val,       ylog1p_val,       feature_names = df_columns)\ndtest      = xgb.DMatrix(X_test,                        feature_names = df_columns)"},{"cell_type":"markdown","metadata":{"_cell_guid":"39eaa756-9f62-2cd3-62d1-5ca1eddae7ec"},"source":"## 3. Train the xgboost model"},{"cell_type":"markdown","metadata":{"_cell_guid":"c5c28385-716c-a5df-f585-76ad98c7d9f7"},"source":"####  Parameters for xgboost are as follows:\n\n### Booster parameters \nThese parameters are used to optimise the algorithm in terms of both accuracy and performance.\n\n**eta / learning_rate:  This is similar to the learning rate (alpha) in gradient descent. \nMakes the model more robust by shrinking the weights on each step. Typical final values range from 0.01-0.2.  \n\n**max_depth:  It sets the maximum depth of a tree and is used to control over-fitting as higher depth allows the model to learn relations very specific to a particular sample. We tune it using cross-validation. Typical values range from 3-10\n\n**subsample:  It denotes the fraction of obeservations to be randomly samples for each tree. Lower values make the algorithm conservative and prevent overfitting but too small and we may get under-fitting. Typical values range from 0.5-1 \n\n**colsample_bytree:  It denotes the fraction of columns to be randomly samples for each tree. Typical values range from 0.5-1  \n\n### Learning Task Parameters\nThese parameters are used to define the optimisation metric to be calculated at each step.\n\n**'eval_metric': 'rmse'** sets our evaluation metric to root mean squared error\n    This  evaluation metric used to score submissions in this competition is the log root mean squared error, however this option is not available to us within xgboost so this is the closest match.\n\n### General parameters\n**booster** - left at default by not setting it, which means we are using a tree-based model. It can also be set to use linear models.\n\n**silent: 1** - this defaults to 0 and is a binary switch. When set to 0 running messages will be printed which may help to understand the model. It can be set to 1 to suppress running messages."},{"cell_type":"markdown","metadata":{"_cell_guid":"e02b10e8-dc7f-9fc4-076b-ad7f3f7e3dbb"},"source":"### 3a. Instantiate the booster with the parameters we have chosen"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a6a8f0d-d2eb-a208-0fc4-84bac5d489c9"},"outputs":[],"source":"# Choose values for the key parameters - keep the number of estimators low for now - not more than 200\n\nmodel = xgb.XGBRegressor(    objective = 'reg:linear'\n                           , n_estimators =  \n                           , max_depth = 5\n                           # , min_child_weight = min_child_weight\n                           , subsample = 1.0\n                           , colsample_bytree = \n                           , learning_rate = \n                           , silent = 1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f92599d4-9ac9-1331-b9a9-15db3b288b75"},"source":"## 3b. Train the booster on our data\n\nWe do this in two stages.  First using partitioning the training data into training and validation data to find how many boosting rounds to carry out and then using all of the training data for the given number of rounds."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4276b88a-99b4-0205-c324-53567f4f7c4f"},"outputs":[],"source":"eval_set  = [( X_train, ylog1p_train), ( X_val, ylog1p_val)]\n\nmodel.fit(X = X_train, \n          y = ylog1p_train,\n          eval_set = eval_set, \n          eval_metric = \"rmse\", \n          early_stopping_rounds = 30,\n          verbose = True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"543adea8-147a-6360-d4ac-4cbec53baf20"},"outputs":[],"source":"num_boost_round = model.best_iteration \nnum_boost_round\n\n# Is num_boost_rounds one less than the n_estimators you chose above?  If it is \n# what does this tell you?  What should you do about it?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fdc4dcc5-a066-fbd1-75f5-607d9c74dc95"},"outputs":[],"source":"# Fill eta below with whatever you used for learning_rate above.\n# Likewise for colsample_bytree\n\n# Different syntax used here than above, due to issues with xgboost package (we can't get \n# variable importance the other way)\n\nxgb_params = {\n    'eta': ,\n    'max_depth': 5,\n    'subsample': 1.0,\n    'colsample_bytree': ,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 0\n}\n\ndtrain_all = xgb.DMatrix(np.vstack((X_train, X_val)), ylog1p_train_all, feature_names = df_columns)\nmodel = xgb.train(xgb_params, dtrain_all, num_boost_round = num_boost_round)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5e7bf3b0-b216-2886-613a-9731cfba80e0"},"source":"## 4a. Variable importance"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"39eaf20e-1c8a-bba7-96c5-45881b9dd2d4"},"outputs":[],"source":"# Create a dataframe of the variable importances\ndict_varImp = model.get_score(importance_type = 'weight')\ndf_ = pd.DataFrame(dict_varImp, index = ['varImp']).transpose().reset_index()\ndf_.columns = ['feature', 'fscore']"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb6096a0-c14c-a20f-5735-37e13b9373ae"},"outputs":[],"source":"# Plot the relative importance of the top 10 features\ndf_['fscore'] = df_['fscore'] / df_['fscore'].max()\ndf_.sort_values('fscore', ascending = False, inplace = True)\ndf_ = df_[0:10]\ndf_.sort_values('fscore', ascending = True, inplace = True)\ndf_.plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\nplt.title('xgboost feature importance', fontsize = 24)\nplt.xlabel('')\nplt.ylabel('')\nplt.xticks([], [])\nplt.yticks(fontsize=20)\nplt.show()\n#plt.gcf().savefig('feature_importance_xgb.png')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ab40ccd-00b2-3267-fce6-094ff881bed0"},"outputs":[],"source":"### 4b. Partial dependence plots"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5881790-d207-70e1-fe3f-ef8ab244aa3c"},"outputs":[],"source":"# from https://xiaoxiaowang87.github.io/monotonicity_constraint/\ndef partial_dependency(bst, X, y, feature_ids = [], f_id = -1):\n\n    \"\"\"\n    Calculate the dependency (or partial dependency) of a response variable on a predictor (or multiple predictors)\n    1. Sample a grid of values of a predictor.\n    2. For each value, replace every row of that predictor with this value, calculate the average prediction.\n    \"\"\"\n\n    X_temp = X.copy()\n\n    grid = np.linspace(np.percentile(X_temp[:, f_id], 0.1),\n                       np.percentile(X_temp[:, f_id], 99.5),\n                       50)\n    y_pred = np.zeros(len(grid))\n\n    if len(feature_ids) == 0 or f_id == -1:\n        print ('Input error!')\n        return\n    else:\n        for i, val in enumerate(grid):\n\n            X_temp[:, f_id] = val\n            data = xgb.DMatrix(X_temp, feature_names = df_columns)\n\n            y_pred[i] = np.average(bst.predict(data))\n\n    return grid, y_pred"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"536ce9f6-5f61-1667-951e-297cbb8e1e59"},"outputs":[],"source":"lst_f = ['full_sq', 'timedelta', 'floor']\nfor f in lst_f:\n    f_id = df_columns.tolist().index(f)\n\n\n    feature_ids = range(X_train.shape[1])\n\n    grid, y_pred = partial_dependency(model,\n                                      X_train,\n                                      ylog1p_train,\n                                      feature_ids = feature_ids,\n                                      f_id = f_id\n                                      )\n\n    fig, ax = plt.subplots()\n    fig.set_size_inches(8, 8)\n    plt.subplots_adjust(left = 0.17, right = 0.94, bottom = 0.15, top = 0.9)\n\n    ax.plot(grid, y_pred, '-', color = 'red', linewidth = 2.5, label='fit')\n    ax.plot(X_train[:, f_id], ylog1p_train, 'o', color = 'grey', alpha = 0.01)\n\n    ax.set_xlim(min(grid), max(grid))\n    ax.set_ylim(0.95 * min(y_pred), 1.05 * max(y_pred))\n\n    ax.set_xlabel(f, fontsize = 24)\n    ax.set_ylabel('Partial Dependence', fontsize = 24)\n\n    plt.xticks(fontsize=20)\n    plt.yticks(fontsize=20)\n\n    plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e667f7e6-5466-5d5e-1e16-776055615392"},"outputs":[],"source":"## 5. Create the predictions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0558d536-d788-2bf3-a887-713d4ad7e0f5"},"outputs":[],"source":"# Create the predictions\nylog_pred = model.predict(dtest)\ny_pred = np.exp(ylog_pred) - 1"},{"cell_type":"markdown","metadata":{"_cell_guid":"661e004e-bb56-5e10-5181-7540b62b8c97"},"source":"### Output the data to CSV for submission"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79888128-ca76-42c2-5e1d-533ab6530c91"},"outputs":[],"source":"output = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\noutput.to_csv('xgb_1.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}