{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"5d0b3f2d-a46b-11b4-615c-414821ebfaf2"},"source":"## WLS Baseline and Small XGBoost with Macro Adjustments"},{"cell_type":"markdown","metadata":{"_cell_guid":"cea20abc-5668-18db-08ac-2e10b8a1b8b4"},"source":"This is the first single-kernel implementation of my \"model macro and micro separately\" approach.  The macro model is the very simple one from [my previous kernel][1].  The micro model is roughly what was in my \"[Using WLS to Create Features for XGBoost][2]\" kernel except that it omits all macro variables.  Overall, it's a 3-step approach:\n\n1. Fit baseline WLS and use coefficients to create composite features\n2. Add additional features and fit XGBoost.\n3. Fit macro model to monthly medians and use its predictions to adjust XGBoost predictions. \n\n  [1]: https://www.kaggle.com/aharless/simple-macro-model-for-monthly-house-prices\n  [2]: https://www.kaggle.com/aharless/using-wls-to-create-features-for-xgboost"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ca63164-cb8d-5417-9499-bab3791eccfb"},"outputs":[],"source":"# Parameters\nuse_pipe = True  \nweight_base = \"2011-08-19\"\nxgb_lr = .01 #  Learning rate.  I use .007, but needs to be larger to run on Kaggle."},{"cell_type":"markdown","metadata":{"_cell_guid":"b1c07b6d-4791-29ff-c96c-13cc06a7be73"},"source":"### Read and munge the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4da8ff5c-0f2d-1116-74db-7e3db1e9b9a0"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('../input/train.csv')\nmacro = pd.read_csv('../input/macro.csv')\ntest = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19068c3e-1c53-4504-bd5c-1f94f7b6029c"},"outputs":[],"source":"dfa = pd.concat([train, test])  # \"dfa\" stands for \"data frame all\"\n# Eliminate spaces and special characters in area names\ndfa.loc[:,\"sub_area\"] = dfa.sub_area.str.replace(\" \",\"\").str.replace(\"\\'\",\"\").str.replace(\"-\",\"\")\ndfa = dfa.merge(macro, \n                on='timestamp', suffixes=['','_macro'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9e4ea376-5244-8d9c-fe19-7b511a961c86"},"outputs":[],"source":"dfa[\"fullzero\"] = (dfa.full_sq==0)\ndfa[\"fulltiny\"] = (dfa.full_sq<4)\ndfa[\"fullhuge\"] = (dfa.full_sq>2000)\ndfa[\"lnfull\"] = dfa.full_sq\n\ndfa[\"nolife\"] = dfa.life_sq.isnull()\ndfa.life_sq = dfa.life_sq.fillna(dfa.life_sq.median())\ndfa[\"lifezero\"] = (dfa.life_sq==0)\ndfa[\"lifetiny\"] = (dfa.life_sq<4)\ndfa[\"lifehuge\"] = (dfa.life_sq>2000)\ndfa[\"lnlife\"] = dfa.life_sq \n\ndfa[\"nofloor\"] = dfa.floor.isnull()\ndfa.floor = dfa.floor.fillna(dfa.floor.median())\ndfa[\"floor1\"] = (dfa.floor==1)\ndfa[\"floor0\"] = (dfa.floor==0)\ndfa[\"floorhuge\"] = (dfa.floor>50)\ndfa[\"lnfloor\"] = dfa.floor\n\ndfa[\"nomax\"] = dfa.max_floor.isnull()\ndfa.max_floor = dfa.max_floor.fillna(dfa.max_floor.median())\ndfa[\"max1\"] = (dfa.max_floor==1)\ndfa[\"max0\"] = (dfa.max_floor==0)\ndfa[\"maxhuge\"] = (dfa.max_floor>80)\ndfa[\"lnmax\"] = dfa.max_floor\n\ndfa[\"norooms\"] = dfa.num_room.isnull()\ndfa.num_room = dfa.num_room.fillna(dfa.num_room.median())\ndfa[\"zerorooms\"] = (dfa.num_room==0)\ndfa[\"lnrooms\"] = dfa.num_room\n\ndfa[\"nokitch\"] = dfa.kitch_sq.isnull()\ndfa.kitch_sq = dfa.kitch_sq.fillna(dfa.kitch_sq.median())\ndfa[\"kitch1\"] = (dfa.kitch_sq==1)\ndfa[\"kitch0\"] = (dfa.kitch_sq==0)\ndfa[\"kitchhuge\"] = (dfa.kitch_sq>400)\ndfa[\"lnkitch\"] = dfa.kitch_sq"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1956b91-be30-3167-e9e6-da16eb69e640"},"outputs":[],"source":"dfa[\"material0\"] = dfa.material.isnull()\ndfa[\"material1\"] = (dfa.material==1)\ndfa[\"material2\"] = (dfa.material==2)\ndfa[\"material3\"] = (dfa.material==3)\ndfa[\"material4\"] = (dfa.material==4)\ndfa[\"material5\"] = (dfa.material==5)\ndfa[\"material6\"] = (dfa.material==6)\n\n# \"state\" isn't explained but it looks like an ordinal number, so for now keep numeric\ndfa.loc[dfa.state>5,\"state\"] = np.NaN  # Value 33 seems to be invalid; others all 1-4\ndfa.state = dfa.state.fillna(dfa.state.median())\n\n# product_type gonna be ugly because there are missing values in the test set but not training\n# Check for the same problem with other variables\ndfa[\"owner_occ\"] = (dfa.product_type=='OwnerOccupier')\ndfa.owner_occ.fillna(dfa.owner_occ.mean())\n\ndfa = pd.get_dummies(dfa, columns=['sub_area'], drop_first=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b299145e-1131-bf66-e7d9-2f7d0bfa396e"},"outputs":[],"source":"# Build year is ugly\n# Can be missing\n# Can be zero\n# Can be one\n# Can be some ridiculous pre-Medieval number\n# Can be some invalid huge number like 20052009\n# Can be some other invalid huge number like 4965\n# Can be a reasonable number but later than purchase year\n# Can be equal to purchase year\n# Can be a reasonable nubmer before purchase year\n\ndfa.loc[dfa.build_year>2030,\"build_year\"] = np.NaN\ndfa[\"nobuild\"] = dfa.build_year.isnull()\ndfa[\"sincebuild\"] = pd.to_datetime(dfa.timestamp).dt.year - dfa.build_year\ndfa.sincebuild.fillna(dfa.sincebuild.median(),inplace=True)\ndfa[\"futurebuild\"] = (dfa.sincebuild < 0)\ndfa[\"newhouse\"] = (dfa.sincebuild==0)\ndfa[\"tooold\"] = (dfa.sincebuild>1000)\ndfa[\"build0\"] = (dfa.build_year==0)\ndfa[\"build1\"] = (dfa.build_year==1)\ndfa[\"untilbuild\"] = -dfa.sincebuild.apply(np.min, args=[0]) # How many years until planned build\ndfa[\"lnsince\"] = dfa.sincebuild.mul(dfa.sincebuild>0).add(1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fd7d25a-295a-278d-d652-4f0950e1adfc"},"outputs":[],"source":"# Interaction terms\ndfa[\"fullzero_Xowner\"] = dfa.fullzero.astype(\"float64\") * dfa.owner_occ\ndfa[\"fulltiny_Xowner\"] = dfa.fulltiny.astype(\"float64\") * dfa.owner_occ\ndfa[\"fullhuge_Xowner\"] = dfa.fullhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnfull_Xowner\"] = dfa.lnfull * dfa.owner_occ\ndfa[\"nofloor_Xowner\"] = dfa.nofloor.astype(\"float64\") * dfa.owner_occ\ndfa[\"floor0_Xowner\"] = dfa.floor0.astype(\"float64\") * dfa.owner_occ\ndfa[\"floor1_Xowner\"] = dfa.floor1.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnfloor_Xowner\"] = dfa.lnfloor * dfa.owner_occ\ndfa[\"max1_Xowner\"] = dfa.max1.astype(\"float64\") * dfa.owner_occ\ndfa[\"max0_Xowner\"] = dfa.max0.astype(\"float64\") * dfa.owner_occ\ndfa[\"maxhuge_Xowner\"] = dfa.maxhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnmax_Xowner\"] = dfa.lnmax * dfa.owner_occ\ndfa[\"kitch1_Xowner\"] = dfa.kitch1.astype(\"float64\") * dfa.owner_occ\ndfa[\"kitch0_Xowner\"] = dfa.kitch0.astype(\"float64\") * dfa.owner_occ\ndfa[\"kitchhuge_Xowner\"] = dfa.kitchhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnkitch_Xowner\"] = dfa.lnkitch * dfa.owner_occ\ndfa[\"nobuild_Xowner\"] = dfa.nobuild.astype(\"float64\") * dfa.owner_occ\ndfa[\"newhouse_Xowner\"] = dfa.newhouse.astype(\"float64\") * dfa.owner_occ\ndfa[\"tooold_Xowner\"] = dfa.tooold.astype(\"float64\") * dfa.owner_occ\ndfa[\"build0_Xowner\"] = dfa.build0.astype(\"float64\") * dfa.owner_occ\ndfa[\"build1_Xowner\"] = dfa.build1.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnsince_Xowner\"] = dfa.lnsince * dfa.owner_occ\ndfa[\"state_Xowner\"] = dfa.state * dfa.owner_occ"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f50f1a10-8276-1ede-7183-9516fef680ad"},"outputs":[],"source":"dfa[\"lnruboil\"] = dfa.oil_urals * dfa.usdrub"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f47ccd19-3222-7068-0810-a879e85df749"},"outputs":[],"source":"# Sets of features that go together\n\n# Features derived from full_sq\nfullvars = [\"fullzero\", \"fulltiny\",\n           # For now I'm going to drop the one \"fullhuge\" case. Later use dummy, maybe.\n           #\"fullhuge\",\n           \"lnfull\" ]\n\n# Features derived from floor\nfloorvars = [\"nofloor\", \"floor1\", \"floor0\",\n             # floorhuge isn't very important, and it's causing problems, so drop it\n             #\"floorhuge\", \n             \"lnfloor\"]\n\n# Features derived from max_floor\nmaxvars = [\"max1\", \"max0\", \"maxhuge\", \"lnmax\"]\n\n# Features derived from kitch_sq\nkitchvars = [\"kitch1\", \"kitch0\", \"kitchhuge\", \"lnkitch\"]\n\n# Features derived from bulid_year\nbuildvars = [\"nobuild\", \"futurebuild\", \"newhouse\", \"tooold\", \n             \"build0\", \"build1\", \"untilbuild\", \"lnsince\"]\n\n# Features (dummy set) derived from material\nmatervars = [\"material1\", \"material2\",  # material3 is rare, so lumped in with missing \n             \"material4\", \"material5\", \"material6\"]\n\n# Features derived from interaction of floor and product_type\nfloorXvars = [\"nofloor_Xowner\", \"floor1_Xowner\", \"lnfloor_Xowner\"]\n\n# Features derived from interaction of kitch_sq and product_type\nkitchXvars = [\"kitch1_Xowner\", \"kitch0_Xowner\", \"lnkitch_Xowner\"]\n\n# Features (dummy set) derived from sub_area\nsubarvars = [\n       'sub_area_Akademicheskoe',\n       'sub_area_Altufevskoe', 'sub_area_Arbat',\n       'sub_area_Babushkinskoe', 'sub_area_Basmannoe', 'sub_area_Begovoe',\n       'sub_area_Beskudnikovskoe', 'sub_area_Bibirevo',\n       'sub_area_BirjulevoVostochnoe', 'sub_area_BirjulevoZapadnoe',\n       'sub_area_Bogorodskoe', 'sub_area_Brateevo', 'sub_area_Butyrskoe',\n       'sub_area_Caricyno', 'sub_area_Cheremushki',\n       'sub_area_ChertanovoCentralnoe', 'sub_area_ChertanovoJuzhnoe',\n       'sub_area_ChertanovoSevernoe', 'sub_area_Danilovskoe',\n       'sub_area_Dmitrovskoe', 'sub_area_Donskoe', 'sub_area_Dorogomilovo',\n       'sub_area_FilevskijPark', 'sub_area_FiliDavydkovo',\n       'sub_area_Gagarinskoe', 'sub_area_Goljanovo',\n       'sub_area_Golovinskoe', 'sub_area_Hamovniki',\n       'sub_area_HoroshevoMnevniki', 'sub_area_Horoshevskoe',\n       'sub_area_Hovrino', 'sub_area_Ivanovskoe', 'sub_area_Izmajlovo',\n       'sub_area_Jakimanka', 'sub_area_Jaroslavskoe', 'sub_area_Jasenevo',\n       'sub_area_JuzhnoeButovo', 'sub_area_JuzhnoeMedvedkovo',\n       'sub_area_JuzhnoeTushino', 'sub_area_Juzhnoportovoe',\n       'sub_area_Kapotnja', 'sub_area_Konkovo', 'sub_area_Koptevo',\n       'sub_area_KosinoUhtomskoe', 'sub_area_Kotlovka',\n       'sub_area_Krasnoselskoe', 'sub_area_Krjukovo',\n       'sub_area_Krylatskoe', 'sub_area_Kuncevo', \n       'sub_area_Kuzminki', 'sub_area_Lefortovo', 'sub_area_Levoberezhnoe',\n       'sub_area_Lianozovo', 'sub_area_Ljublino', 'sub_area_Lomonosovskoe',\n       'sub_area_Losinoostrovskoe', 'sub_area_Marfino',\n       'sub_area_MarinaRoshha', 'sub_area_Marino', 'sub_area_Matushkino',\n       'sub_area_Meshhanskoe', 'sub_area_Metrogorodok', 'sub_area_Mitino',\n       'sub_area_MoskvorecheSaburovo',\n       'sub_area_Mozhajskoe', 'sub_area_NagatinoSadovniki',\n       'sub_area_NagatinskijZaton', 'sub_area_Nagornoe',\n       'sub_area_Nekrasovka', 'sub_area_Nizhegorodskoe',\n       'sub_area_NovoPeredelkino', 'sub_area_Novogireevo',\n       'sub_area_Novokosino', 'sub_area_Obruchevskoe',\n       'sub_area_OchakovoMatveevskoe', 'sub_area_OrehovoBorisovoJuzhnoe',\n       'sub_area_OrehovoBorisovoSevernoe', 'sub_area_Ostankinskoe',\n       'sub_area_Otradnoe', 'sub_area_Pechatniki', 'sub_area_Perovo',\n       'sub_area_PokrovskoeStreshnevo', 'sub_area_PoselenieDesjonovskoe',\n       'sub_area_PoselenieFilimonkovskoe', \n       'sub_area_PoselenieKrasnopahorskoe',\n       'sub_area_PoselenieMoskovskij', 'sub_area_PoselenieMosrentgen',\n       'sub_area_PoselenieNovofedorovskoe',\n       'sub_area_PoseleniePervomajskoe', 'sub_area_PoselenieRjazanovskoe',\n       'sub_area_PoselenieRogovskoe', \n       'sub_area_PoselenieShherbinka', 'sub_area_PoselenieSosenskoe',\n       'sub_area_PoselenieVnukovskoe',  \n       'sub_area_PoselenieVoskresenskoe', 'sub_area_Preobrazhenskoe',\n       'sub_area_Presnenskoe', 'sub_area_ProspektVernadskogo',\n       'sub_area_Ramenki', 'sub_area_Rjazanskij', 'sub_area_Rostokino',\n       'sub_area_Savelki', 'sub_area_Savelovskoe', 'sub_area_Severnoe',\n       'sub_area_SevernoeButovo', 'sub_area_SevernoeIzmajlovo',\n       'sub_area_SevernoeMedvedkovo', 'sub_area_SevernoeTushino',\n       'sub_area_Shhukino', 'sub_area_Silino', 'sub_area_Sokol',\n       'sub_area_SokolinajaGora', 'sub_area_Sokolniki',\n       'sub_area_Solncevo', 'sub_area_StaroeKrjukovo', 'sub_area_Strogino',\n       'sub_area_Sviblovo', 'sub_area_Taganskoe', 'sub_area_Tekstilshhiki',\n       'sub_area_TeplyjStan', 'sub_area_Timirjazevskoe',\n       'sub_area_Troickijokrug', 'sub_area_TroparevoNikulino',\n       'sub_area_Tverskoe', 'sub_area_Veshnjaki', \n       'sub_area_Vojkovskoe', \n       'sub_area_VostochnoeDegunino', 'sub_area_VostochnoeIzmajlovo',\n       'sub_area_VyhinoZhulebino', 'sub_area_Zamoskvoreche',\n       'sub_area_ZapadnoeDegunino', 'sub_area_Zjablikovo', 'sub_area_Zjuzino'\n       ]\n\n\n# Lump together small sub_areas\n\ndfa = dfa.assign( sub_area_SmallSW =\n   dfa.sub_area_PoselenieMihajlovoJarcevskoe + \n   dfa.sub_area_PoselenieKievskij +\n   dfa.sub_area_PoselenieKlenovskoe +\n   dfa.sub_area_PoselenieVoronovskoe +\n   dfa.sub_area_PoselenieShhapovskoe )\n\ndfa = dfa.assign( sub_area_SmallNW =\n   dfa.sub_area_Molzhaninovskoe +\n   dfa.sub_area_Kurkino )\n\ndfa = dfa.assign( sub_area_SmallW =\n   dfa.sub_area_PoselenieMarushkinskoe +\n   dfa.sub_area_Vnukovo +\n   dfa.sub_area_PoselenieKokoshkino )\n\ndfa = dfa.assign( sub_area_SmallN =\n   dfa.sub_area_Vostochnoe +\n   dfa.sub_area_Alekseevskoe )\n\nsubarvars += [\"sub_area_SmallSW\", \"sub_area_SmallNW\", \"sub_area_SmallW\", \"sub_area_SmallN\"]\n                 \n\n\n# For now eliminate case with ridiculous value of full_sq\ndfa = dfa[~dfa.fullhuge]\n\n    \n# Independent features\n\nindievars = [\"owner_occ\", \"state\", \"state_Xowner\"]\n\n\n# Complete list of features to use for fit\n\nallvars = fullvars + floorvars + maxvars + kitchvars + buildvars + matervars\nallvars += floorXvars + kitchXvars + subarvars + indievars"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7db350c-4d15-33ab-9272-ff19529ed26c"},"outputs":[],"source":"# The normalized target variable:  log real sale price\ntraining = dfa[dfa.price_doc.notnull()]\ntraining.lnrp = training.price_doc.div(training.cpi)\ny = training.lnrp\n\n# Features to use in heteroskedasticity model if I go back to that\nmillion1 = (training.price_doc==1e6)\nmillion2 = (training.price_doc==2e6)\nmillion3 = (training.price_doc==3e6)\n\n# Create X matrix for fitting\nkeep = allvars + ['timestamp']  # Need to keep timestamp to calculate weights\nX = training[keep] "},{"cell_type":"markdown","metadata":{"_cell_guid":"752d6eee-be1e-3b9f-fee4-d6a9dce59f0c"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a5e7af5-0534-1a53-016b-f672ca213c54"},"outputs":[],"source":"def get_weights(df):\n    # Weight cases linearly on time\n    # with later cases (more like test data) weighted more heavily\n    basedate = pd.to_datetime(weight_base).toordinal() # Basedate gets a weight of zero\n    wtd = pd.to_datetime(df.timestamp).apply(lambda x: x.toordinal()) - basedate\n    wts = np.array(wtd)/1e3 # The denominator here shouldn't matter, just gives nice numbers.\n    return wts"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67c4983a-963e-f2c7-411e-fb94303b473b"},"outputs":[],"source":"wts = get_weights(X)\nX = X.drop(\"timestamp\", axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d04f1f9f-5e32-1bad-7ade-75b8e2dc95d1"},"outputs":[],"source":"from sklearn.preprocessing import Imputer, StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n# Make a pipeline that transforms X\npipe = make_pipeline(Imputer(), StandardScaler())\npipe.fit(X)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d88258e-aca5-968a-c458-ee4e8d5cb3a9"},"outputs":[],"source":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression(fit_intercept=True)\nlr.fit(pipe.transform(X), y, sample_weight=wts)\nlr.score( pipe.transform(X), y ) # Unweighted R^2, just to see what it looks like"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a8fd8317-70ab-24ae-f3bb-a8d7b6031ddf"},"outputs":[],"source":"# Function to create an indicator array that selects positions\n#   corresponding to a set of features from the regression\n\ndef get_selector( df, varnames ):\n    selector = np.zeros( df.shape[1] )\n    selector[[df.columns.get_loc(x) for x in varnames]] = 1\n    return( selector )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a74c7302-9aa4-cf13-133d-22fc6e99918d"},"outputs":[],"source":"def append_composite( df, varnames, name, X, Xuse, estimator ):\n    selector = get_selector(X, varnames)\n    v = pd.Series( np.matmul( Xuse, selector*estimator.coef_ ), \n                   name=name, index=df.index )\n    return( pd.concat( [df, v], axis=1 ) )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fce0fc5c-d0db-d72d-1192-14ba642cde9a"},"outputs":[],"source":"Xuse = pipe.transform(X)\n\nvars = {\"fullv\":fullvars,     \"floorv\":floorvars,   \"maxv\":maxvars, \n        \"kitchv\":kitchvars,   \"buildv\":buildvars,   \"materv\":matervars, \n        \"floorxv\":floorXvars, \"kitchxv\":kitchXvars, \"subarv\":subarvars}\nfor v in vars:\n    training = append_composite( training, vars[v], v, X, Xuse, lr )\n\nshortvarlist = list(vars.keys())\nshortvarlist += indievars\n\nXshort = training[shortvarlist]\n\npipe1 = make_pipeline(Imputer(), StandardScaler())\npipe1.fit(Xshort)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea1f344c-3083-4951-6938-0ced25a2a314"},"outputs":[],"source":"# Fit again to make sure result is same\nlr1 = LinearRegression(fit_intercept=True)\nlr1.fit(pipe1.transform(Xshort), y, sample_weight=wts)\nlr1.score( pipe1.transform(Xshort), y )"},{"cell_type":"markdown","metadata":{"_cell_guid":"f934c72b-f17c-62b5-e3d8-c6df2080f63a"},"source":"### Add more features and run XGBoost"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ea100464-8f9f-078d-0adc-cec7bd1ba356"},"outputs":[],"source":"# Set up test data\n\ntesting = dfa[dfa.price_doc.isnull()]\n\ndf_test_full = pd.DataFrame(columns=X.columns)\nfor column in df_test_full.columns:\n        df_test_full[column] = testing[column]        \n\nXuse = pipe.transform(df_test_full)\nfor v in vars:\n    df_test_full = append_composite( df_test_full, vars[v], v, X, Xuse, lr )\n\ndf_test = pd.DataFrame(columns=Xshort.columns)\nfor column in df_test.columns:\n        df_test[column] = df_test_full[column]  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d050e8a-a75d-9523-5064-5892c34d7b47"},"outputs":[],"source":"def append_series( X_train, X_test, train_input, test_input, sername ):\n    vtrain = pd.Series( train_input[sername], name=sername, index=X_train.index )\n    X_train_out = pd.concat( [X_train, vtrain], axis=1 )\n    vtest = pd.Series( test_input[sername], name=sername, index=X_test.index )\n    X_test_out = pd.concat( [X_test, vtest], axis=1 )\n    return( X_train_out, X_test_out )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8bf105a9-19f7-bc65-c7e2-1dedab44bf16"},"outputs":[],"source":"# Arbitrary down-weighting of ridiculous prices\nwts *= (1 - .2*million1 + .1*million2 + .05*million3)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ca4406d1-dc12-ffc0-87a6-45db764bc0a7"},"outputs":[],"source":"vars_to_add = [\n    \"kindergarten_km\", \n    \"railroad_km\", \n    \"swim_pool_km\", \n    \"public_transport_station_km\",\n    \"big_road1_km\",\n    \"big_road2_km\",\n    \"church_synagogue_km\",\n    \"ttk_km\",\n    \"metro_min_walk\",\n    \"kremlin_km\",\n    \"mosque_km\",\n]\nXdata_train = Xshort\nXdata_test = df_test\nprint( Xdata_train.shape )\nprint( Xdata_test.shape )\nfor v in vars_to_add:\n    Xdata_train, Xdata_test = append_series( Xdata_train, Xdata_test, training, testing, v )\nprint( Xdata_train.shape )\nprint( Xdata_test.shape )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7546343d-3b1c-a625-f401-fe5bd24b9a44"},"outputs":[],"source":"y.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18c976c2-cdaa-acba-520a-054777830b75"},"outputs":[],"source":"import xgboost as xgb\nimport joblib\nxgb_params = {\n    'eta': xgb_lr,\n    'max_depth': 6,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'nthread': 4,\n    'silent': 1,\n}\n\n\ndtrain = xgb.DMatrix(Xdata_train, y, weight=wts)\ndtest = xgb.DMatrix(Xdata_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d2e40a94-72fe-5751-3dd9-97ac7dad66c4"},"outputs":[],"source":"%matplotlib inline\ncv_output = xgb.cv(xgb_params, dtrain, num_boost_round=2000, early_stopping_rounds=20,\n    verbose_eval=50, show_stdv=False)\n# cv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\ncv_output[\"test-rmse-mean\"][len(cv_output)-1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f267208c-23e3-c91a-c01d-c3cccdba0da7"},"outputs":[],"source":"num_boost_rounds = len(cv_output)\nprint( num_boost_rounds )\nmodel = xgb.train(xgb_params, dtrain, num_boost_round= num_boost_rounds)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d240ec20-3c53-ddf1-15d5-487d778f17a3"},"outputs":[],"source":"fig, ax = plt.subplots(1, 1, figsize=(8, 13))\nxgb.plot_importance(model, height=0.5, ax=ax)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a5eea5e0-2e25-657f-6b0e-1dc3cf82fd60"},"outputs":[],"source":"y_predict = model.predict(dtest)\npredictions = y_predict*testing.cpi\n\n# And put this in a dataframe\npredxgb_df = pd.DataFrame()\npredxgb_df['id'] = testing['id']\npredxgb_df['price_doc'] = predictions\npredxgb_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5c97475-4fb5-42e1-d525-010a1b0d552a"},"outputs":[],"source":"predxgb_df.to_csv('xgb_predicitons.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2fee6ad3-5f67-af74-8fe3-c85d43cf34d5"},"outputs":[],"source":"test[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\ntest[\"year\"]  = test[\"timestamp\"].dt.year\ntest[\"month\"] = test[\"timestamp\"].dt.month\ntest[\"yearmonth\"] = 100*test.year + test.month\ntest_ids = test[[\"yearmonth\",\"id\"]]\ntest_data = test_ids.merge(predxgb_df,on=\"id\")\n\ntest_prices = test_data[[\"yearmonth\",\"price_doc\"]]\ntest_p = test_prices.groupby(\"yearmonth\").median()\ntest_p.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"72c3794f-e1e6-bb60-9154-6295a7409ace"},"source":"### Run the macro model and make predictions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6396ed13-874a-4372-2c5f-c281727f46e5"},"outputs":[],"source":"import statsmodels.api as sm"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfd0f480-60e5-e5d4-fe41-72fa61ea36c3"},"outputs":[],"source":"macro[\"timestamp\"] = pd.to_datetime(macro[\"timestamp\"])\nmacro[\"year\"]  = macro[\"timestamp\"].dt.year\nmacro[\"month\"] = macro[\"timestamp\"].dt.month\nmacro[\"yearmonth\"] = 100*macro.year + macro.month\nmacmeds = macro.groupby(\"yearmonth\").median()\n\ntrain[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\ntrain[\"year\"]  = train[\"timestamp\"].dt.year\ntrain[\"month\"] = train[\"timestamp\"].dt.month\ntrain[\"yearmonth\"] = 100*train.year + train.month\nprices = train[[\"yearmonth\",\"price_doc\"]]\np = prices.groupby(\"yearmonth\").median()\n\ndf = macmeds.join(p)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"81f92404-d09b-6730-33a5-c13e7ed624d1"},"outputs":[],"source":"#  Adapted from code at http://adorio-research.org/wordpress/?p=7595\n#  Original post was dated May 31st, 2010\n#    but was unreachable last time I tried\n\nimport numpy.matlib as ml\n \ndef almonZmatrix(X, maxlag, maxdeg):\n    \"\"\"\n    Creates the Z matrix corresponding to vector X.\n    \"\"\"\n    n = len(X)\n    Z = ml.zeros((len(X)-maxlag, maxdeg+1))\n    for t in range(maxlag,  n):\n       #Solve for Z[t][0].\n       Z[t-maxlag,0] = sum([X[t-lag] for lag in range(maxlag+1)])\n       for j in range(1, maxdeg+1):\n             s = 0.0\n             for i in range(1, maxlag+1):       \n                s += (i)**j * X[t-i]\n             Z[t-maxlag,j] = s\n    return Z"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cb14347d-4b86-dbd1-f8ae-53cd5fca6705"},"outputs":[],"source":"y_macro = df.price_doc.div(df.cpi).loc[201108:201506]\nnobs = 47  # August 2011 through June 2015, months with price_doc data\ntblags = 5    # Number of lags used on PDL for Trade Balance\nmrlags = 5    # Number of lags used on PDL for Mortgage Rate\nztb = almonZmatrix(df.balance_trade.loc[201103:201506].as_matrix(), tblags, 1)\nzmr = almonZmatrix(df.mortgage_rate.loc[201103:201506].as_matrix(), mrlags, 1)\ncolumns = ['tb0', 'tb1', 'mr0', 'mr1']\nz = pd.DataFrame( np.concatenate( (ztb, zmr), axis=1), y_macro.index.values, columns )\nX_macro = sm.add_constant( z )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0180c753-f0de-1a60-267d-b52b99095dc6"},"outputs":[],"source":"macro_fit = sm.OLS(y_macro, X_macro).fit()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"92cce03a-dc23-0985-6dc3-6d262bd29178"},"outputs":[],"source":"test_cpi = df.cpi.loc[201507:201605]\ntest_index = test_cpi.index\nztb_test = almonZmatrix(df.balance_trade.loc[201502:201605].as_matrix(), tblags, 1)\nzmr_test = almonZmatrix(df.mortgage_rate.loc[201502:201605].as_matrix(), mrlags, 1)\nz_test = pd.DataFrame( np.concatenate( (ztb_test, zmr_test), axis=1), test_index, columns )\nX_macro_test = sm.add_constant( z_test )\npred_lnrp = macro_fit.predict( X_macro_test )\npred_p = pred_lnrp* test_cpi"},{"cell_type":"markdown","metadata":{"_cell_guid":"ebf80cd3-d0ff-8de5-cedc-d0b992be2f43"},"source":"### Use macro model predictions to adjust XGBoost micro predictions"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78af5200-7f60-8ab8-36c4-7bd10779542b"},"outputs":[],"source":"adjust = pd.DataFrame( pred_p/test_p.price_doc, columns=[\"adjustment\"] )\nadjust"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec5c7268-2b20-eeb7-a02f-b065de4acc22"},"outputs":[],"source":"combo = test_data.join(adjust, on='yearmonth')\ncombo['adjusted'] = combo.price_doc * combo.adjustment\nadjxgb_df = pd.DataFrame()\nadjxgb_df['id'] = combo.id\nadjxgb_df['price_doc'] = combo.adjusted\nadjxgb_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7fa532f7-e0b0-cf88-e641-a196c380e7fa"},"outputs":[],"source":"adjxgb_df.to_csv('adjusted_xgb_predicitons.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4db38da5-a601-f605-3108-c276b37c6622"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}