{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"761f7b37-9d95-9c91-7fe4-c9e893f36b7b"},"source":"#### (The Next Checkpoint in My Modeling Progress)"},{"cell_type":"markdown","metadata":{"_cell_guid":"72c9f61a-b791-3a45-fd70-f33b1d746d5f"},"source":"To be honest, I've been disappointed with this script's public leaderboard performance, but there's still a lot of modeling to do.  (This gets 34 to 35-ish scores, which is at least better than plain WLS.  Can get close to 33 by tweaking, but I don't figure on using the public leaderboard as a cross-validation set, so I've been choosing my final tweaks based on CV scores within the training data set.  For now I do a lot better on the public LB by finding optimal combinations of other people's variations on the kitchen sink naive XGB model.)  I still think the technique in this script is promising, even if I'm in the early stages of applying it.\n\nThe idea is to use weighted least squares (still with arbitrary linear time weights, for now) to generate linear combinations of related variables, and use those linear combinations as features for more sophisticated fitting procedures (in this case, just XGBoost, but I hope to try other things).  \n\nFor example, I have 8 different variables -- some of them continuous and some of them dummies -- to represent different aspects of \"build year.\"  (It can be before, after, or in the same year as the sale, which are different types of cases; it can be missing; it can be clearly invalid; it can be in a range that might or might not be valid; and so on.)  I see these different variables as being, as it were, part of the same organism, and I don't want some officious regularization procedure telling me that its eyes and legs can stay in the model but its arms and mouth have to go.  So I fit them all with least squares, and I take the part of the least squares model that refers to \"build year\" variables and separate it from the rest of the model to use as a single feature.\n\nThis method also serves to make continuous versions of variables like sub_area.  Call it \"raion quality.\"  I just let the least squares fit tell me which raions are better than others and by how much.  (Another common technique to make the variable continuous would be to use mean prices for each raion, which is equivalent to a least squares fit with no other variables.)  I may later want to visit other aspects of sub_area. (There are plenty of variables in the data set that refer to raions rather than individual houses, so getting rid of the dummies might later allow those to become meaningful features.  From the point of view of a tree model, my \"raion quality\" variable is a particular, but particularly meaningful, way of ordering the raions, but other ways may also be useful.)  For a first cut, I keep it simple and use a single continuous variable to represent raion.\n\nMy baseline WLS model has one macro variable -- mortgage growth -- which I chose as a lazy first cut proxy for the macro data.  The baseline also includes what I considered the most obvious micro variables, for which one ought to adjust rather than, say, merely using the mean within each raion for raion quality.  (For example, if one raion has larger houses, you want to adjust for that and not attribute to raion quality what is really the effect of house size.)  After carving up the baseline model and feeding its pieces to XGBoost, I add other variables (mostly micro but a few macro) to see if they seem to help.  For now these other variables involve little or no feature engineering, so there's still a lot to be done."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"812c6811-0609-73cb-0daf-65c05ca04e89"},"outputs":[],"source":"# Parameters\nuse_pipe = True  # Standardize. (Shouldn't matter for OLS, but there are compuatation issues.)\nweight_base = \"2011-08-19\"  # Linear weights start from zero"},{"cell_type":"markdown","metadata":{"_cell_guid":"92dc4db7-1187-7c80-cb68-d89fd3571555"},"source":"### Read and munge the data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4355ce58-b754-8a93-95d7-1e6609618c62"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('../input/train.csv')\nmacro = pd.read_csv('../input/macro.csv')\ntest = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bffa22a7-3df9-1508-08bf-76571cceb23c"},"outputs":[],"source":"dfa = pd.concat([train, test])  # \"dfa\" stands for \"data frame all\"\n# Eliminate spaces and special characters in area names\ndfa.loc[:,\"sub_area\"] = dfa.sub_area.str.replace(\" \",\"\").str.replace(\"\\'\",\"\").str.replace(\"-\",\"\")\ndfa = dfa.merge(macro, \n                on='timestamp', suffixes=['','_macro'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c7841d49-dfbb-a50b-8d6a-2369d25fd18b"},"outputs":[],"source":"dfa[\"fullzero\"] = (dfa.full_sq==0)\ndfa[\"fulltiny\"] = (dfa.full_sq<4)\ndfa[\"fullhuge\"] = (dfa.full_sq>2000)\ndfa[\"lnfull\"] = np.log(dfa.full_sq+1)\n\ndfa[\"nolife\"] = dfa.life_sq.isnull()\ndfa.life_sq = dfa.life_sq.fillna(dfa.life_sq.median())\ndfa[\"lifezero\"] = (dfa.life_sq==0)\ndfa[\"lifetiny\"] = (dfa.life_sq<4)\ndfa[\"lifehuge\"] = (dfa.life_sq>2000)\ndfa[\"lnlife\"] = np.log( dfa.life_sq + 1 )\n\ndfa[\"nofloor\"] = dfa.floor.isnull()\ndfa.floor = dfa.floor.fillna(dfa.floor.median())\ndfa[\"floor1\"] = (dfa.floor==1)\ndfa[\"floor0\"] = (dfa.floor==0)\ndfa[\"floorhuge\"] = (dfa.floor>50)\ndfa[\"lnfloor\"] = np.log(dfa.floor+1)\n\ndfa[\"nomax\"] = dfa.max_floor.isnull()\ndfa.max_floor = dfa.max_floor.fillna(dfa.max_floor.median())\ndfa[\"max1\"] = (dfa.max_floor==1)\ndfa[\"max0\"] = (dfa.max_floor==0)\ndfa[\"maxhuge\"] = (dfa.max_floor>80)\ndfa[\"lnmax\"] = np.log(dfa.max_floor+1)\n\ndfa[\"norooms\"] = dfa.num_room.isnull()\ndfa.num_room = dfa.num_room.fillna(dfa.num_room.median())\ndfa[\"zerorooms\"] = (dfa.num_room==0)\ndfa[\"lnrooms\"] = np.log( dfa.num_room + 1 )\n\ndfa[\"nokitch\"] = dfa.kitch_sq.isnull()\ndfa.kitch_sq = dfa.kitch_sq.fillna(dfa.kitch_sq.median())\ndfa[\"kitch1\"] = (dfa.kitch_sq==1)\ndfa[\"kitch0\"] = (dfa.kitch_sq==0)\ndfa[\"kitchhuge\"] = (dfa.kitch_sq>400)\ndfa[\"lnkitch\"] = np.log(dfa.kitch_sq+1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5fab2741-44a8-2491-7702-16f7b1f743ad"},"outputs":[],"source":"dfa[\"material0\"] = dfa.material.isnull()\ndfa[\"material1\"] = (dfa.material==1)\ndfa[\"material2\"] = (dfa.material==2)\ndfa[\"material3\"] = (dfa.material==3)\ndfa[\"material4\"] = (dfa.material==4)\ndfa[\"material5\"] = (dfa.material==5)\ndfa[\"material6\"] = (dfa.material==6)\n\n# \"state\" isn't explained but it looks like an ordinal number, so for now keep numeric\ndfa.loc[dfa.state>5,\"state\"] = np.NaN  # Value 33 seems to be invalid; others all 1-4\ndfa.state = dfa.state.fillna(dfa.state.median())\n\n# product_type gonna be ugly because there are missing values in the test set but not training\n# Check for the same problem with other variables\ndfa[\"owner_occ\"] = (dfa.product_type=='OwnerOccupier')\ndfa.owner_occ.fillna(dfa.owner_occ.mean())\n\ndfa = pd.get_dummies(dfa, columns=['sub_area'], drop_first=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6214cf5d-585a-3896-b6a6-7fe824b1f5b2"},"outputs":[],"source":"# Build year is ugly\n# Can be missing\n# Can be zero\n# Can be one\n# Can be some ridiculous pre-Medieval number\n# Can be some invalid huge number like 20052009\n# Can be some other invalid huge number like 4965\n# Can be a reasonable number but later than purchase year\n# Can be equal to purchase year\n# Can be a reasonable nubmer before purchase year\n\ndfa.loc[dfa.build_year>2030,\"build_year\"] = np.NaN\ndfa[\"nobuild\"] = dfa.build_year.isnull()\ndfa[\"sincebuild\"] = pd.to_datetime(dfa.timestamp).dt.year - dfa.build_year\ndfa.sincebuild.fillna(dfa.sincebuild.median(),inplace=True)\ndfa[\"futurebuild\"] = (dfa.sincebuild < 0)\ndfa[\"newhouse\"] = (dfa.sincebuild==0)\ndfa[\"tooold\"] = (dfa.sincebuild>1000)\ndfa[\"build0\"] = (dfa.build_year==0)\ndfa[\"build1\"] = (dfa.build_year==1)\ndfa[\"untilbuild\"] = -dfa.sincebuild.apply(np.min, args=[0]) # How many years until planned build\ndfa[\"lnsince\"] = dfa.sincebuild.mul(dfa.sincebuild>0).add(1).apply(np.log)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4054ea1b-6366-e7a3-bb13-ef677e79c754"},"outputs":[],"source":"# Interaction terms (many not used, ultimately, but I haven't whittled it down yet).\ndfa[\"fullzero_Xowner\"] = dfa.fullzero.astype(\"float64\") * dfa.owner_occ\ndfa[\"fulltiny_Xowner\"] = dfa.fulltiny.astype(\"float64\") * dfa.owner_occ\ndfa[\"fullhuge_Xowner\"] = dfa.fullhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnfull_Xowner\"] = dfa.lnfull * dfa.owner_occ\ndfa[\"nofloor_Xowner\"] = dfa.nofloor.astype(\"float64\") * dfa.owner_occ\ndfa[\"floor0_Xowner\"] = dfa.floor0.astype(\"float64\") * dfa.owner_occ\ndfa[\"floor1_Xowner\"] = dfa.floor1.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnfloor_Xowner\"] = dfa.lnfloor * dfa.owner_occ\ndfa[\"max1_Xowner\"] = dfa.max1.astype(\"float64\") * dfa.owner_occ\ndfa[\"max0_Xowner\"] = dfa.max0.astype(\"float64\") * dfa.owner_occ\ndfa[\"maxhuge_Xowner\"] = dfa.maxhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnmax_Xowner\"] = dfa.lnmax * dfa.owner_occ\ndfa[\"kitch1_Xowner\"] = dfa.kitch1.astype(\"float64\") * dfa.owner_occ\ndfa[\"kitch0_Xowner\"] = dfa.kitch0.astype(\"float64\") * dfa.owner_occ\ndfa[\"kitchhuge_Xowner\"] = dfa.kitchhuge.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnkitch_Xowner\"] = dfa.lnkitch * dfa.owner_occ\ndfa[\"nobuild_Xowner\"] = dfa.nobuild.astype(\"float64\") * dfa.owner_occ\ndfa[\"newhouse_Xowner\"] = dfa.newhouse.astype(\"float64\") * dfa.owner_occ\ndfa[\"tooold_Xowner\"] = dfa.tooold.astype(\"float64\") * dfa.owner_occ\ndfa[\"build0_Xowner\"] = dfa.build0.astype(\"float64\") * dfa.owner_occ\ndfa[\"build1_Xowner\"] = dfa.build1.astype(\"float64\") * dfa.owner_occ\ndfa[\"lnsince_Xowner\"] = dfa.lnsince * dfa.owner_occ\ndfa[\"state_Xowner\"] = dfa.state * dfa.owner_occ"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"283b20a9-ce76-971c-0544-587b62f92ecb"},"outputs":[],"source":"# Just a tiny bit of feature engineering:  (log) price of oil in rubles\ndfa[\"lnruboil\"] = np.log( dfa.oil_urals * dfa.usdrub )"},{"cell_type":"markdown","metadata":{"_cell_guid":"b0f3c8a1-c26f-a62b-8901-24f01005d321"},"source":"### Select features to fit"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3b4364d-0baa-c34c-586f-3657db84e15d"},"outputs":[],"source":"# Sets of features that go together\n\n# Features derived from full_sq\nfullvars = [\"fullzero\", \"fulltiny\",\n           # For now I'm going to drop the one \"fullhuge\" case. Later use dummy, maybe.\n           #\"fullhuge\",\n           \"lnfull\" ]\n\n# Features derived from floor\nfloorvars = [\"nofloor\", \"floor1\", \"floor0\",\n             # floorhuge isn't very important, and it's causing problems, so drop it\n             #\"floorhuge\", \n             \"lnfloor\"]\n\n# Features derived from max_floor\nmaxvars = [\"max1\", \"max0\", \"maxhuge\", \"lnmax\"]\n\n# Features derived from kitch_sq\nkitchvars = [\"kitch1\", \"kitch0\", \"kitchhuge\", \"lnkitch\"]\n\n# Features derived from bulid_year\nbuildvars = [\"nobuild\", \"futurebuild\", \"newhouse\", \"tooold\", \n             \"build0\", \"build1\", \"untilbuild\", \"lnsince\"]\n\n# Features (dummy set) derived from material\nmatervars = [\"material1\", \"material2\",  # material3 is rare, so lumped in with missing \n             \"material4\", \"material5\", \"material6\"]\n\n# Features derived from interaction of floor and product_type\nfloorXvars = [\"nofloor_Xowner\", \"floor1_Xowner\", \"lnfloor_Xowner\"]\n\n# Features derived from interaction of kitch_sq and product_type\nkitchXvars = [\"kitch1_Xowner\", \"kitch0_Xowner\", \"lnkitch_Xowner\"]\n\n# Features (dummy set) derived from sub_area\nsubarvars = [\n       'sub_area_Akademicheskoe',\n        # Aggregate with neighboring districts\n        #'sub_area_Alekseevskoe', \n       'sub_area_Altufevskoe', 'sub_area_Arbat',\n       'sub_area_Babushkinskoe', 'sub_area_Basmannoe', 'sub_area_Begovoe',\n       'sub_area_Beskudnikovskoe', 'sub_area_Bibirevo',\n       'sub_area_BirjulevoVostochnoe', 'sub_area_BirjulevoZapadnoe',\n       'sub_area_Bogorodskoe', 'sub_area_Brateevo', 'sub_area_Butyrskoe',\n       'sub_area_Caricyno', 'sub_area_Cheremushki',\n       'sub_area_ChertanovoCentralnoe', 'sub_area_ChertanovoJuzhnoe',\n       'sub_area_ChertanovoSevernoe', 'sub_area_Danilovskoe',\n       'sub_area_Dmitrovskoe', 'sub_area_Donskoe', 'sub_area_Dorogomilovo',\n       'sub_area_FilevskijPark', 'sub_area_FiliDavydkovo',\n       'sub_area_Gagarinskoe', 'sub_area_Goljanovo',\n       'sub_area_Golovinskoe', 'sub_area_Hamovniki',\n       'sub_area_HoroshevoMnevniki', 'sub_area_Horoshevskoe',\n       'sub_area_Hovrino', 'sub_area_Ivanovskoe', 'sub_area_Izmajlovo',\n       'sub_area_Jakimanka', 'sub_area_Jaroslavskoe', 'sub_area_Jasenevo',\n       'sub_area_JuzhnoeButovo', 'sub_area_JuzhnoeMedvedkovo',\n       'sub_area_JuzhnoeTushino', 'sub_area_Juzhnoportovoe',\n       'sub_area_Kapotnja', 'sub_area_Konkovo', 'sub_area_Koptevo',\n       'sub_area_KosinoUhtomskoe', 'sub_area_Kotlovka',\n       'sub_area_Krasnoselskoe', 'sub_area_Krjukovo',\n       'sub_area_Krylatskoe', 'sub_area_Kuncevo', \n        # Aggregate with small neighbor\n        #'sub_area_Kurkino',\n       'sub_area_Kuzminki', 'sub_area_Lefortovo', 'sub_area_Levoberezhnoe',\n       'sub_area_Lianozovo', 'sub_area_Ljublino', 'sub_area_Lomonosovskoe',\n       'sub_area_Losinoostrovskoe', 'sub_area_Marfino',\n       'sub_area_MarinaRoshha', 'sub_area_Marino', 'sub_area_Matushkino',\n       'sub_area_Meshhanskoe', 'sub_area_Metrogorodok', 'sub_area_Mitino',\n        # Aggregate with neighboring districts\n        #'sub_area_Molzhaninovskoe', \n       'sub_area_MoskvorecheSaburovo',\n       'sub_area_Mozhajskoe', 'sub_area_NagatinoSadovniki',\n       'sub_area_NagatinskijZaton', 'sub_area_Nagornoe',\n       'sub_area_Nekrasovka', 'sub_area_Nizhegorodskoe',\n       'sub_area_NovoPeredelkino', 'sub_area_Novogireevo',\n       'sub_area_Novokosino', 'sub_area_Obruchevskoe',\n       'sub_area_OchakovoMatveevskoe', 'sub_area_OrehovoBorisovoJuzhnoe',\n       'sub_area_OrehovoBorisovoSevernoe', 'sub_area_Ostankinskoe',\n       'sub_area_Otradnoe', 'sub_area_Pechatniki', 'sub_area_Perovo',\n       'sub_area_PokrovskoeStreshnevo', 'sub_area_PoselenieDesjonovskoe',\n       'sub_area_PoselenieFilimonkovskoe', \n        # This one is almost empty.  Will lump in with another category.\n        #'sub_area_PoselenieKievskij',\n        # This one is almost empty.  Will lump in with another category.\n        #'sub_area_PoselenieKlenovskoe', \n        # Aggregate with neighboring districts\n        #'sub_area_PoselenieKokoshkino',\n       'sub_area_PoselenieKrasnopahorskoe',\n        # Aggregate with neighboring districts\n        #'sub_area_PoselenieMarushkinskoe',\n        # This one is almost empty.  Will lump in with another category.\n        #'sub_area_PoselenieMihajlovoJarcevskoe',\n       'sub_area_PoselenieMoskovskij', 'sub_area_PoselenieMosrentgen',\n       'sub_area_PoselenieNovofedorovskoe',\n       'sub_area_PoseleniePervomajskoe', 'sub_area_PoselenieRjazanovskoe',\n       'sub_area_PoselenieRogovskoe', \n        # This one is almost empty.  Will lump in with another category.\n        #'sub_area_PoselenieShhapovskoe',\n       'sub_area_PoselenieShherbinka', 'sub_area_PoselenieSosenskoe',\n       'sub_area_PoselenieVnukovskoe',  \n        # Aggregate with neighboring districts\n        #'sub_area_PoselenieVoronovskoe',\n       'sub_area_PoselenieVoskresenskoe', 'sub_area_Preobrazhenskoe',\n       'sub_area_Presnenskoe', 'sub_area_ProspektVernadskogo',\n       'sub_area_Ramenki', 'sub_area_Rjazanskij', 'sub_area_Rostokino',\n       'sub_area_Savelki', 'sub_area_Savelovskoe', 'sub_area_Severnoe',\n       'sub_area_SevernoeButovo', 'sub_area_SevernoeIzmajlovo',\n       'sub_area_SevernoeMedvedkovo', 'sub_area_SevernoeTushino',\n       'sub_area_Shhukino', 'sub_area_Silino', 'sub_area_Sokol',\n       'sub_area_SokolinajaGora', 'sub_area_Sokolniki',\n       'sub_area_Solncevo', 'sub_area_StaroeKrjukovo', 'sub_area_Strogino',\n       'sub_area_Sviblovo', 'sub_area_Taganskoe', 'sub_area_Tekstilshhiki',\n       'sub_area_TeplyjStan', 'sub_area_Timirjazevskoe',\n       'sub_area_Troickijokrug', 'sub_area_TroparevoNikulino',\n       'sub_area_Tverskoe', 'sub_area_Veshnjaki', \n        # Aggregate with neighboring districts\n        #'sub_area_Vnukovo',\n       'sub_area_Vojkovskoe', \n        # Aggregate with neighboring districts\n        #'sub_area_Vostochnoe',\n       'sub_area_VostochnoeDegunino', 'sub_area_VostochnoeIzmajlovo',\n       'sub_area_VyhinoZhulebino', 'sub_area_Zamoskvoreche',\n       'sub_area_ZapadnoeDegunino', 'sub_area_Zjablikovo', 'sub_area_Zjuzino'\n       ]\n\n\n# Lump together small sub_areas\n\ndfa = dfa.assign( sub_area_SmallSW =\n   dfa.sub_area_PoselenieMihajlovoJarcevskoe + \n   dfa.sub_area_PoselenieKievskij +\n   dfa.sub_area_PoselenieKlenovskoe +\n   dfa.sub_area_PoselenieVoronovskoe +\n   dfa.sub_area_PoselenieShhapovskoe )\n\ndfa = dfa.assign( sub_area_SmallNW =\n   dfa.sub_area_Molzhaninovskoe +\n   dfa.sub_area_Kurkino )\n\ndfa = dfa.assign( sub_area_SmallW =\n   dfa.sub_area_PoselenieMarushkinskoe +\n   dfa.sub_area_Vnukovo +\n   dfa.sub_area_PoselenieKokoshkino )\n\ndfa = dfa.assign( sub_area_SmallN =\n   dfa.sub_area_Vostochnoe +\n   dfa.sub_area_Alekseevskoe )\n\nsubarvars += [\"sub_area_SmallSW\", \"sub_area_SmallNW\", \"sub_area_SmallW\", \"sub_area_SmallN\"]\n                 \n\n\n# For now eliminate case with ridiculous value of full_sq\ndfa = dfa[~dfa.fullhuge]\n\n    \n# Independent features\n\nindievars = [\"owner_occ\", \"state\", \"state_Xowner\",\n             # Dropping due to \"visiual regularizaiton\" and unclear relationship to fullv\n             #\"lnfull_Xowner\",\n             #\"lnruboil\",\n             \"mortgage_growth\" ]\n          \n\n# Complete list of features to use for fit\n\nallvars = fullvars + floorvars + maxvars + kitchvars + buildvars + matervars\nallvars += floorXvars + kitchXvars + subarvars + indievars\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"9008bcc9-a576-bed6-5716-9e70600bdc30"},"source":"### Set up target variable and fitting data set"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c844a6e-b2ce-a7d7-5942-0d55677bbed1"},"outputs":[],"source":"# The normalized target variable:  log real sale price\ntraining = dfa[dfa.price_doc.notnull()]\ntraining.lnrp = training.price_doc.div(training.cpi).apply(np.log)\ny = training.lnrp\n\n# Features to use in heteroskedasticity model if I go back to that\nmillion1 = (training.price_doc==1e6)\nmillion2 = (training.price_doc==2e6)\nmillion3 = (training.price_doc==3e6)\n\n# Create X matrix for fitting\nkeep = allvars + ['timestamp']  # Need to keep timestamp to calculate weights\nX = training[keep] "},{"cell_type":"markdown","metadata":{"_cell_guid":"25cb3856-3f53-8aef-077a-2e83a88e6f52"},"source":"### Set up (time-weights, imputation, scaling, etc.) for initial fit"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24ab694d-fd4f-5fb1-8086-3ca9ad4b6f77"},"outputs":[],"source":"def get_weights(df):\n    # Weight cases linearly on time\n    # with later cases (more like test data) weighted more heavily\n    basedate = pd.to_datetime(weight_base).toordinal() # Basedate gets a weight of zero\n    wtd = pd.to_datetime(df.timestamp).apply(lambda x: x.toordinal()) - basedate\n    wts = np.array(wtd)/1e3 # The denominator here shouldn't matter, just gives nice numbers.\n    return wts"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4bfcdb2b-2f00-1404-199b-5fe28363af01"},"outputs":[],"source":"wts = get_weights(X)\nX = X.drop(\"timestamp\", axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b451839-66de-bebc-6988-7a9fbb61f531"},"outputs":[],"source":"if use_pipe:\n    from sklearn.preprocessing import Imputer, StandardScaler\n    from sklearn.pipeline import make_pipeline\n\n    # Make a pipeline that transforms X\n    pipe = make_pipeline(Imputer(), StandardScaler())\n    pipe.fit(X)\n    pipe.transform(X)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7dd98ec4-62a4-6f72-f1e7-1ddb75c6b2cd"},"source":"## Fit WLS"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"96afff92-2c55-bcc6-5f40-f84877391d26"},"outputs":[],"source":"\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression(fit_intercept=True)\nif use_pipe:\n    lr.fit(pipe.transform(X), y, sample_weight=wts)\nelse:\n    lr.fit(X, y, sample_weight=wts)\n\n# At home I have version 0.18, in which LinearRegression knows its sum of squared residuals\n# WTF, Scikit-learn developers, why did you deprecate this??\n# I needed it to check that my code is working\n\n# lr.residues_  # Show SSR to check it will be same as verison with composite features"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"67e77712-d843-74b0-bcaf-61e45256438f"},"outputs":[],"source":"# Hey, Scikit-learn developers, I want my residues_ back!\n# Dang ML types think you can just use statistical methods as a tool\n#   and ignore the actual statistics.  That bad science IMO, even bad data science.\nimport sklearn\nsklearn.__version__"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ecff74d-2865-08f5-4bb3-05309ff58401"},"outputs":[],"source":"# Function to create an indicator array that selects positions\n#   corresponding to a set of variables from the regression\n\ndef get_selector( df, varnames ):\n    selector = np.zeros( df.shape[1] )\n    selector[[df.columns.get_loc(x) for x in varnames]] = 1\n    return( selector )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7169406f-95b2-2fc0-fc14-2e0088696639"},"outputs":[],"source":"# Function to calculate a composite feature and append it to the data frame\n\ndef append_composite( df, varnames, name, X, Xuse, estimator ):\n    selector = get_selector(X, varnames)\n    v = pd.Series( np.matmul( Xuse, selector*estimator.coef_ ), \n                   name=name, index=df.index )\n    return( pd.concat( [df, v], axis=1 ) )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7958d9e8-16b7-7bc7-613d-7c4fdde74c07"},"outputs":[],"source":"# Generate composite features for groups of input variables using WLS coefficeints\n\nif use_pipe:\n    Xuse = pipe.transform(X)\nelse:\n    Xuse = X\n\nvars = {\"fullv\":fullvars,     \"floorv\":floorvars,   \"maxv\":maxvars, \n        \"kitchv\":kitchvars,   \"buildv\":buildvars,   \"materv\":matervars, \n        \"floorxv\":floorXvars, \"kitchxv\":kitchXvars, \"subarv\":subarvars}\nfor v in vars:\n    training = append_composite( training, vars[v], v, X, Xuse, lr )\n\nshortvarlist = list(vars.keys())\nshortvarlist += indievars\n\nXshort = training[shortvarlist]\n\nif use_pipe:\n    pipe1 = make_pipeline(Imputer(), StandardScaler())\n    pipe1.fit(Xshort)\n    pipe1.transform(Xshort)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1ffbe0d-87a8-afac-e867-603770eb986e"},"outputs":[],"source":"# Fit again to make sure result is same\nlr1 = LinearRegression(fit_intercept=True)\nif use_pipe:\n    lr1.fit(pipe1.transform(Xshort), y, sample_weight=wts)\nelse:\n    lr1.fit(Xshort, y, sample_weight=wts)\n\n# Sorry, can't do that in version 0.19,\n#   apparently unless you do the extra step of predicting on the training data\n# Never mind, I've already debugged this.\n\n# lr1.residues_"},{"cell_type":"markdown","metadata":{"_cell_guid":"b75681e6-12da-aef4-24d8-ad6254389e41"},"source":"### Predict on the test set (just for the hell of it)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"664e695f-8648-2194-dc79-10e183c9bc86"},"outputs":[],"source":"testing = dfa[dfa.price_doc.isnull()]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6ad5a47-66b7-c9b3-f279-b2a54f4dde39"},"outputs":[],"source":"df_test_full = pd.DataFrame(columns=X.columns)\nfor column in df_test_full.columns:\n        df_test_full[column] = testing[column]        \nif use_pipe:\n    Xuse = pipe.transform(df_test_full)\nelse:\n    Xuse = df_test_full\n\nfor v in vars:\n    df_test_full = append_composite( df_test_full, vars[v], v, X, Xuse, lr )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"457bcd1f-2846-3d40-56be-fc26f449c5f2"},"outputs":[],"source":"df_test = pd.DataFrame(columns=Xshort.columns)\nfor column in df_test.columns:\n        df_test[column] = df_test_full[column]        "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"309461b9-daeb-b7b5-7f9a-8173e5543e20"},"outputs":[],"source":"# Make the predictions\nif use_pipe:\n    pred = lr1.predict( pipe1.transform(df_test) )\nelse:\n    pred = lr1.predict(df_test)\npredictions = np.exp(pred)*testing.cpi\n\n# And put this in a dataframe\npredictions_df = pd.DataFrame()\npredictions_df['id'] = testing['id']\npredictions_df['price_doc'] = predictions\npredictions_df.head()\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b14f7c3c-787f-2bb3-3efa-fd55948d5d6b"},"outputs":[],"source":"predictions_df.to_csv('wls_predictions.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"c361d7de-8126-ebca-b9e8-904538388db2"},"source":"### Check on in-sample WLS fit to see if it looks right"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0c22607-5e95-6e5f-764b-66ee7ba17822"},"outputs":[],"source":"# Check for ridiculous coefficients, likely indicating collinearity\nco = lr.coef_\nra = range(len(co))\nmask = np.abs(co)>1e4\nX.columns[mask].values\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6edbb0b-83aa-296f-52a6-007ed58f95aa"},"outputs":[],"source":"from statsmodels.regression.linear_model import WLS\nxdat = Xshort.copy().astype(np.float64)\nxdat[\"constant\"] = 1\nydat = y.copy().astype(np.float64)\nresult = WLS(ydat, xdat, weights=wts).fit()\nresult.summary()"},{"cell_type":"markdown","metadata":{"_cell_guid":"b58df994-e502-619e-0616-f70b35eb6336"},"source":"The composite features all have coefficients of 1.0, as they should, by construction.  The t-statistics aren't very meaningful, since I've made linear combinations that I knew would fit well.  But I still take some comfort in the fact that they are all very large: you can say my features are overfit or contrived, but you can't say they don't matter."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6d85ec00-e3f5-7598-2642-94db7bc386e8"},"outputs":[],"source":"# Note that, if the model is run without the pipe transform, the coefficients below\n#  should be the same as those above.  Sometimes they have been, sometimes not.\n#  If they're not the same, probably numerical instability due to collinearity.\n#  In any case, if the pipe transform is used they are different, because\n#  they apply to standardized variables, not raw data.\npd.DataFrame(Xshort.columns, lr1.coef_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e7c2e46c-9055-eb64-c482-f13b11d67099"},"source":"## Set up for XGBoost"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f16d2bc2-f4b9-9cb5-5d1a-0fec93c0da93"},"outputs":[],"source":"# Function to add another features to the training set for XGBoost\n\ndef append_series( X_train, X_test, train_input, test_input, sername ):\n    vtrain = pd.Series( train_input[sername], name=sername, index=X_train.index )\n    X_train_out = pd.concat( [X_train, vtrain], axis=1 )\n    vtest = pd.Series( test_input[sername], name=sername, index=X_test.index )\n    X_test_out = pd.concat( [X_test, vtest], axis=1 )\n    return( X_train_out, X_test_out )"},{"cell_type":"markdown","metadata":{"_cell_guid":"3ae866b6-fbb2-5edb-9c32-17d08fee5edd"},"source":"For XGBoost I arbitrarily downweight points that have ridiculous prices.  The downweighting is not as aggressive as Raddar's, because I'm already weighting by time, which takes care of part of the problem.  Also, I'm trying to be conservative, since my earlier results didn't show much promise for heteroskedasticity weighting.  These weights are not aggressive enough to make my CV scores indicative of likely test set scores, but I'm not convinced that that is a necessary condition for good cross-validation.  (That said, my cross-validation does suck for a number of reasons, not the least of which is that I engineered my features by fitting on the same data set.  I have no good excuse, but I keep telling myself it's still early in the game, and I will do things better in the future.)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e79060a-3259-f00c-772b-5431fdf54e6f"},"outputs":[],"source":"wts *= (1 - .2*million1 + .1*million2 + .05*million3)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7920e237-838b-e7d0-5323-8ee018fe7958"},"source":"### Add  new features for the XGBoost regression\n\nThe commented-out things are ones that I tried and decided against, usually because they reduced the CV scores.  Granted, the CV scores are biased against new features because I already overfit the old features using the same data.  I will have to reconsider a lot of things, but here we go.  This is more of a demo than a serious analysis at this point."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e11355b-0b56-8ca0-c8ea-12f3b5b9b6a4"},"outputs":[],"source":"vars_to_add = [\n    \"kindergarten_km\", \n    \"railroad_km\", \n    \"swim_pool_km\", \n#    \"fitness_km\",\n#    \"workplaces_km\",\n#    \"radiation_km\",\n    \"public_transport_station_km\",\n    \"big_road1_km\",\n    \"big_road2_km\",\n#    \"university_km\",\n#    \"big_church_km\",\n#    \"park_km\",\n#    \"power_transmission_line_km\",\n#    \"green_zone_km\",\n#    \"public_healthcare_km\",\n#    \"additional_education_km\",\n#    \"catering_km\",\n    \"church_synagogue_km\",\n#    \"school_km\",\n#    \"theater_km\",\n#    \"water_km\",\n#    \"stadium_km\",\n#    \"nuclear_reactor_km\",\n#    \"lnlife\",\n#    \"lnrooms\",\n#    \"hospice_morgue_km\",\n    \"ttk_km\",\n#    \"metro_min_avto\",\n#    \"metro_km_avto\",\n    \"metro_min_walk\",\n#    \"metro_km_walk\",\n#    \"cemetery_km\",\n#    \"incineration_km\",\n#    \"railroad_station_walk_min\",\n#    \"railroad_station_walk_km\",\n#    \"ID_railroad_station_walk\",\n#    \"railroad_station_avto_km\",\n#    \"railroad_station_avto_min\",\n#    \"ID_railroad_station_avto\",\n#    \"public_transport_station_min_walk\",\n#    \"water_1line\", # PROBLEM WITH DATA TYPE\n#    \"mkad_km\",\n#    \"sadovoe_km\"\n#    \"bulvar_ring_km\"\n    \"kremlin_km\",\n#    \"ID_big_road1\",\n#    \"big_road1_1line\", # PROBLEM WITH DATA TYPE\n#    \"ID_big_road2\",\n#    \"railroad_1line\", # PROBLEM WITH DATA TYPE\n#    \"zd_vokzaly_avto_km\",\n#    \"ID_railroad_terminal\",\n#    \"bus_terminal_avto_km\",\n#    \"ID_bus_terminal\",\n#    \"oil_chemistry_km\",\n#    \"thermal_power_plant_km\",\n#    \"ts_km\".\n#    \"big_market_km\",\n#    \"market_shop_km\",\n#    \"ice_rink_km\",\n#    \"basketball_km\",\n#    \"detention_facility_km\",\n#    \"shopping_centers_km\",\n#    \"office_km\",\n#    \"preschool_km\",\n    \"mosque_km\",\n#    \"museum_km\",\n#    \"exhibition_km\",\n#    \"cafe_count_500_price_high\",\n#    \"cafe_count_1000_price_high\",\n#    \"cafe_count_1000_price_4000\",\n#    \"cafe_count_500_price_4000\",\n#    \"cafe_avg_price_1500\",\n#    \"cafe_avg_price_3000\",\n#    \"raion_popul\",\n#    \"oil_urals\",\n#    \"gdp_annual\",\n#    \"mortgage_value\",\n    \"rent_price_3room_eco\",\n#    \"gdp_quart_growth\",\n    \"mortgage_rate\",\n    \"lnruboil\"\n]\nXdata_train = Xshort\nXdata_test = df_test\nprint( Xdata_train.shape )\nprint( Xdata_test.shape )\nfor v in vars_to_add:\n    Xdata_train, Xdata_test = append_series( Xdata_train, Xdata_test, training, testing, v )\nprint( Xdata_train.shape )\nprint( Xdata_test.shape )"},{"cell_type":"markdown","metadata":{"_cell_guid":"b42786be-6903-c6d0-ce44-d9ba77b3e427"},"source":"### Set parameters and cross-validate\n\nIn this case, \"cross-validate\" definitely means choosing the number of boost rounds but also vaguely means selecting a feature set and some of the other parameters, though I did all that by hand and not systematically."},{"cell_type":"markdown","metadata":{"_cell_guid":"3a9b091a-4dc9-76c4-4654-1b050e1a45a7"},"source":"Now that I've finally managed to get XGBoost to run multiple threads on my Mac at home, I can set a low learning rate, which does seem to give better results, but this version has to run on Kaggle in 1200 seconds, so I'm putting `eta` back up. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38e4cf59-278f-414f-3103-df800c9f16ac"},"outputs":[],"source":"import xgboost as xgb\nxgb_params = {\n    'eta': 0.05,   # Try .01 or .005, but for now...\n    'max_depth': 6,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(Xdata_train, y, weight=wts)\ndtest = xgb.DMatrix(Xdata_test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19e34abf-e603-170c-2383-0e75d07a947a"},"outputs":[],"source":"cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=2000, early_stopping_rounds=20,\n    verbose_eval=50, show_stdv=False)\ncv_output[\"test-rmse-mean\"][len(cv_output)-1]"},{"cell_type":"markdown","metadata":{"_cell_guid":"255a65ad-46f4-374c-d90a-8fbb6ad3e8b6"},"source":"## Train XGBoost model and plot feature importance"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"182329ad-a168-13ec-330d-0fb049ad8cdb"},"outputs":[],"source":"num_boost_rounds = len(cv_output)\nprint( num_boost_rounds )\nmodel = xgb.train(xgb_params, dtrain, num_boost_round= num_boost_rounds)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3209306e-8e60-1ea0-e6cf-9e081b058e89"},"outputs":[],"source":"%matplotlib inline\nfig, ax = plt.subplots(1, 1, figsize=(8, 13))\nxgb.plot_importance(model, height=0.5, ax=ax)"},{"cell_type":"markdown","metadata":{"_cell_guid":"474fd104-f0b0-ec2a-dfb6-70a7358ef8ba"},"source":"## Predict on the Test Set"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"baed4101-f16d-5bc3-444c-9e3d5ea3932e"},"outputs":[],"source":"y_predict = model.predict(dtest)\npredictions = np.exp(y_predict)*testing.cpi\n\n# And put this in a dataframe\npredxgb_df = pd.DataFrame()\npredxgb_df['id'] = testing['id']\npredxgb_df['price_doc'] = predictions\npredxgb_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"96f432ab-67b5-675f-0835-43da19e7bf5b"},"outputs":[],"source":"predxgb_df.to_csv('xgb_predicitons.csv', index=False)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d73cbfb8-3c4d-090e-3617-e0ffba1680bf"},"source":"Sometimes I put the results through the [\"small improvements\" recoding][1], but that also needs some work.\n\n  [1]: https://www.kaggle.com/aharless/probabilistic-version-of-small-improvements"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df5a95cd-cb25-d780-219c-38a12ee26d3a"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}