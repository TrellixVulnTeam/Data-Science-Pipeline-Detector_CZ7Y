{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b401ada4-911c-2ef0-0f87-6bb71a5326cb"},"source":"Introduction\n------------\n\nThis notebook introduces a way to use partial Principal Component Analysis to address the collinearity issue in the data. I first classify the variables into small groups which similar features, for example, church_count, build_count, cafe_price and etc. Then apply a PCA for each group of variables and use a stop criteria of 0.95 of total variance to pick the number of principal component. Finally, I assemble the data and draw the correlation plots to compare before and after transformation of the original data.\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a20c78ae-30d3-9c65-8b15-d33c206ac1bf"},"outputs":[],"source":"## Load packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import normalize\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nfrom ggplot import *\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.set_option('display.max_columns', 500)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d70274cd-0b27-9fd7-0ea3-3a77144c4683"},"outputs":[],"source":"## Load data into Python\n\ntrain = pd.read_csv('../input/train.csv', parse_dates=['timestamp'])\ntest = pd.read_csv('../input/test.csv', parse_dates=['timestamp'])\nmacro = pd.read_csv('../input/macro.csv', parse_dates=['timestamp'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8017ca14-f6d4-e84b-3e1d-a9f28b5c76ab"},"outputs":[],"source":"print(train.columns)\nprint(train.shape)\nprint(test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e81fc6f2-600f-4998-e939-69a37c3890e4"},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b4a76a9-39b2-1553-9c86-0ea6d4a62d18"},"outputs":[],"source":"## Describe the output field\nprint(train['price_doc'].describe())\nsns.distplot(train['price_doc'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"5b990626-1a4a-cf21-8c20-01c18202525e"},"source":"The dependent variable is skewed (as expected for dollars). The easiest would be to take a log tranformation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90dceefb-7fcd-a7db-add7-55d50fd3f34f"},"outputs":[],"source":"train['LogAmt']=np.log(train.price_doc+1.0)\nprint(train['LogAmt'].describe())\nsns.distplot(train['LogAmt'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"537fa12d-471b-31d9-05f6-09845190c5d7"},"source":"** Note the two small bars between 13 and 14, and 14 and 15. We will need to dig into that and see that happens there **."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d29ef4dd-b3a8-5a1b-b427-c01bde91cca8"},"outputs":[],"source":"## Merge data into one dataset to prepare compare between train and test\ntrain_1 = train.copy()\ntrain_1['Source']='Train'\ntest_1 = test.copy()\ntest_1['Source']='Test'\nalldata = pd.concat([train_1, test_1],ignore_index=True)\n\nmacro.columns = ['mac__'+c if c!='timestamp' else 'timestamp' for c in macro.columns ]\nalldata=alldata.merge(macro,on='timestamp',how='left')\nprint(alldata.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1a53f033-4e78-e8dc-d7b8-8b500853f0ef"},"source":"Most fields are numeric, and a few of them are object."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9804020-9916-0b5f-d2dc-f4d6442b40d5"},"outputs":[],"source":"## Numerical and Categorical data types\nalldata_dtype=alldata.dtypes\ndisplay_nvar = len(alldata.columns)\nalldata_dtype_dict = alldata_dtype.to_dict()\nalldata.dtypes.value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"54996cc7-f435-217c-5f3e-e7962d117913"},"source":"Step 2: Transform Variables and Missing Data\n---------------------------\n\n**Variable Description**\n\nI wrote this function with intension to compare train/test data and check if some variable is illy behaved. It is modified a little to fit this dataset to compared between normal/fraud subset.\n\nIt can be applied to both numeric and object data types:\n\n  1. When the data type is object, it will output the value count of each categories\n  2. When the data type is numeric, it will output the quantiles\n  3. It also seeks any missing values in the dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"102d09d5-3354-7530-97e6-ff4819e18542"},"outputs":[],"source":"def var_desc(dt,alldata):\n    print('--------------------------------------------')\n    for c in alldata.columns:\n        if alldata[c].dtype==dt:\n            t1 = alldata[alldata.Source=='Train'][c]\n            t2 = alldata[alldata.Source=='Test'][c]\n            if dt==\"object\":\n                f1 = t1[pd.isnull(t1)==False].value_counts()\n                f2 = t2[pd.isnull(t2)==False].value_counts()\n            else:\n                f1 = t1[pd.isnull(t1)==False].describe()\n                f2 = t2[pd.isnull(t2)==False].describe()\n            m1 = t1.isnull().value_counts()\n            m2 = t2.isnull().value_counts()\n            f = pd.concat([f1, f2], axis=1)\n            m = pd.concat([m1, m2], axis=1)\n            f.columns=['Train','Test']\n            m.columns=['Train','Test']\n            print(dt+' - '+c)\n            print('UniqValue - ',len(t1.value_counts()),len(t2.value_counts()))\n            print(f.sort_values(by='Train',ascending=False))\n            print()\n\n            m_print=m[m.index==True]\n            if len(m_print)>0:\n                print('missing - '+c)\n                print(m_print)\n            else:\n                print('NO Missing values - '+c)\n            if dt!=\"object\":\n                if len(t1.value_counts())<=10:\n                    c1 = t1.value_counts()\n                    c2 = t2.value_counts()\n                    c = pd.concat([c1, c2], axis=1)\n                    f.columns=['Train','Test']\n                    print(c)\n            print('--------------------------------------------')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"97303857-1948-ecdd-5d39-cafae5653064"},"outputs":[],"source":"## Uncomment to run variable description\n## var_desc('object',alldata)"},{"cell_type":"markdown","metadata":{"_cell_guid":"44e8f26a-a64f-bd3c-6ae4-7c833dfd3c45"},"source":"There are only a few variables that are object type and most of them are \"yes\" and \"no\". It is reasonable to convert them into 0 and 1 numerical variables. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a9e5ac6-1d5b-f5f9-8246-82507f8e1355"},"outputs":[],"source":"## convert obj to num\nfor c in alldata.columns:\n    if alldata[c].dtype=='object' and c not in ['sub_area','timestamp','Source']:\n        if len(alldata[c].value_counts())==2:\n            alldata['num_'+c]=[0 if x in ['no','OwnerOccupier'] else 1 for x in alldata[c]]\n        if len(alldata[c].value_counts())==5:\n            alldata['num_'+c]=0\n            alldata['num_'+c].loc[alldata[c]=='poor']=0\n            alldata['num_'+c].loc[alldata[c]=='satisfactory']=1\n            alldata['num_'+c].loc[alldata[c]=='good']=2\n            alldata['num_'+c].loc[alldata[c]=='excellent']=3\n            alldata['num_'+c].loc[alldata[c]=='no data']=1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08286258-0e69-7513-2692-b37fcbe3acf3"},"outputs":[],"source":"## missing values\nmissing_col = [[c,sum(alldata[alldata.Source=='Train'][c].isnull()==True),sum(alldata[alldata.Source=='Test'][c].isnull()==True)] for c in alldata.columns]\nmissing_col = pd.DataFrame(missing_col,columns=['Var','missingTrain','missingTest'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"01bd9b72-70eb-1011-293a-7c3268a432bb"},"source":"** Missing Values **\n\nBelow is an analysis of missing values. There are a couple variables with very rate of missing. So we want to keep them in the resv_col list so that we don't PCA them and make the entire set of the principal components as missing. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40cb8f1d-7aa6-3223-d291-e45c5c6147aa"},"outputs":[],"source":"missingdf=missing_col[missing_col.missingTrain+missing_col.missingTest>0]\nmissingdf=missingdf.sort('missingTrain')\nf, ax = plt.subplots(figsize=(6, 15))\nsns.barplot(y=missingdf.Var,x=missingdf.missingTrain)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9bbb37c3-c204-4b75-2e93-88918c4b733f"},"source":"## Step 3: Principal Component Analysis##\n\nFirst, we group variables into small categories, \n\nThen apply PCA on each of the categories and show correlation plots"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"201b95c5-37fd-48b4-c754-7516bc76ba5c"},"outputs":[],"source":"excl_col=['id','timestamp','sub_area'] + [c for c in alldata.columns if alldata[c].dtype=='object']\nresv_col=['price_doc','LogAmt','Source','cafe_sum_500_max_price_avg','cafe_sum_500_min_price_avg','cafe_avg_price_500','hospital_beds_raion']\ndef sel_grp(keys):\n    lst_all = list()\n    for k in keys:\n        lst = [c for c in alldata.columns if c.find(k)!=-1 and c not in excl_col and c not in resv_col]\n        lst = list(set(lst))\n        lst_all += lst\n    return(lst_all)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"beae798b-ac11-378d-06b0-a4766f285e75"},"outputs":[],"source":"col_grp = dict({})\ncol_grp['people']=sel_grp(['_all','male'])\ncol_grp['id'] = sel_grp(['ID_'])\ncol_grp['church']=sel_grp(['church'])\ncol_grp['build']=sel_grp(['build_count_'])\ncol_grp['cafe']=sel_grp(['cafe_count'])\ncol_grp['cafeprice']=sel_grp(['cafe_sum','cafe_avg'])\ncol_grp['km']=sel_grp(['_km','metro_min','_avto_min','_walk_min','_min_walk'])\ncol_grp['mosque']=sel_grp(['mosque_count'])\ncol_grp['market']=sel_grp(['market_count'])\ncol_grp['office']=sel_grp(['office_count'])\ncol_grp['leisure']=sel_grp(['leisure_count'])\ncol_grp['sport']=sel_grp(['sport_count'])\ncol_grp['green']=sel_grp(['green_part'])\ncol_grp['prom']=sel_grp(['prom_part'])\ncol_grp['trc']=sel_grp(['trc_count'])\ncol_grp['sqm']=sel_grp(['_sqm_'])\ncol_grp['raion']=sel_grp(['_raion'])\ncol_grp['macro']=sel_grp(['mac__'])\ncol_grp.keys()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"928912f4-e4d2-82e4-21e8-0d6ff802515c"},"outputs":[],"source":"col_tmp = list()\nfor d in col_grp:\n    col_tmp+=(col_grp[d])\ncol_grp['other']=[c for c in alldata.columns if c not in col_tmp and c not in excl_col and c not in resv_col]\ncol_grp['other']  ## these 'other' variables are not to be PCA"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c468a87-5b71-6a7d-f2a1-e2ff7bb359a7"},"outputs":[],"source":"## remove variables in macro data with too many missing data\nmacro_missing_2 = pd.DataFrame([[c,sum(alldata[c].isnull())] for c in col_grp['macro']],columns=['Var','Missing'])\nmacro_missing_3=macro_missing_2[macro_missing_2.Missing>5000]\nprint(macro_missing_3)\nexcl_col+=list(macro_missing_3.Var)\nprint(excl_col)\n\ncol_grp['macro']=sel_grp(['mac__'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e00e4ff2-22d9-383e-5ad5-494b98abd26d"},"outputs":[],"source":"loopkeys=list(col_grp.keys())\nprint(loopkeys)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4d4c6a2-3f40-3b64-94a6-d1a0f786b8c2"},"outputs":[],"source":"def partial_pca(var,data,col_grp):\n    from sklearn.decomposition import PCA\n    import bisect\n    pca = PCA()\n    df = data[col_grp[var]].dropna()\n    print([len(data[col_grp[var]]), len(df)])\n    df = (df-df.mean())/df.std(ddof=0)\n    pca.fit(df)\n    varexp = pca.explained_variance_ratio_.cumsum()\n    cutoff = bisect.bisect(varexp, 0.95)\n    #print(cutoff)\n    #print(pca.explained_variance_ratio_.cumsum())\n    newcol=pd.DataFrame(pca.fit_transform(X=df)[:,0:(cutoff+1)],columns=['PCA_'+var+'_'+str(i) for i in range(cutoff+1)],index=df.index)\n    #print(newcol)\n    col_grp['PCA_'+var]=list(newcol.columns)\n    return(newcol,col_grp,pca)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f0bf70c-0870-8dfc-0e65-54b1a50ffd42"},"outputs":[],"source":"for c in loopkeys:\n    if c!='other':\n        print(c)\n        newcol,col_grp,pca = partial_pca(c,alldata,col_grp)\n        alldata=alldata.join(newcol)\n        print(alldata.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4f435b05-172a-b794-2438-9ccde2397aad"},"source":"Correlation\n------\nCorrelation is useful to find peers of input field so we are aware when building models, either to transform them (principal component) or remove one of the two.\nThe first plot below the overall correlation matrix and there are blocks of variables that are highly correlated\nThe second plot shows the highly correlated variables with response."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f31523b-c130-5ed5-0bb7-83d099d3991f"},"outputs":[],"source":"wpca=list()\nwopca=list()\nfor c in col_grp.keys():\n    if c.find('PCA_')!=-1:\n        wpca+=col_grp[c]\n    else:\n        wopca+=col_grp[c]\n        \nwpca+=col_grp['other']\nwpca+=resv_col\nwopca+=col_grp['other']\nwopca+=resv_col\n\nwpca=list(set(wpca))\nwopca=list(set(wopca))\n\nwpca.sort()\nwopca.sort()"},{"cell_type":"markdown","metadata":{"_cell_guid":"82b8c8a0-d30f-b007-b55f-782724b45af5"},"source":"Below is a comparison of w/o PCA and w/ PCA correlation, after PCA transformation, it looks much better in terms of high correlated variables. It is also interesting to notice that some of the intra-group principal components have high correlations. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"337aa9a2-310b-22a1-0581-3411a62dda74"},"outputs":[],"source":"## Correlation without PCA\ncorrmat = alldata[wopca].corr()\nf, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(corrmat, vmax=.8, square=True,xticklabels=False,yticklabels=False,cbar=False,annot=False);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6676ec60-8fbe-703d-49f3-318e655552a9"},"outputs":[],"source":"## Correlation with PCA\ncorrmat = alldata[wpca].corr()\nf, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(corrmat, vmax=.8, square=True,xticklabels=True,yticklabels=True,cbar=False,annot=False);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aefdcb6b-5faa-4eb9-578b-31f08cebaf43"},"outputs":[],"source":"## Top 20 correlated variables\ncorrmat = alldata[wpca].corr()\nk = 20 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'price_doc')['price_doc'].index\ncm = alldata[cols].corr()\nf, ax = plt.subplots(figsize=(10, 10))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=False, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0601f403-22c9-89fd-ae35-5db829f75b4d"},"source":"XGB on the PCA transformed Data\n---------\n\nNow PCA transformed data shows much better correlation. Next we will test a simple XGB model and see its performance. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e29a369b-f9f6-234c-30f8-403c2cd64f46"},"outputs":[],"source":"## these are the variables going into the model.\nalldata[wpca].columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6083bc1-74ef-7a8d-7ea5-b2e82d3ec5d4"},"outputs":[],"source":"## Add a few more features suggested by other discussions\n##\n# Add month-year\nmonth_year = (alldata.timestamp.dt.month + alldata.timestamp.dt.year * 100)\nmonth_year_cnt_map = month_year.value_counts().to_dict()\nalldata['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n# Add week-year count\nweek_year = (alldata.timestamp.dt.weekofyear + alldata.timestamp.dt.year * 100)\nweek_year_cnt_map = week_year.value_counts().to_dict()\nalldata['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n# Add month and day-of-week\nalldata['month'] = alldata.timestamp.dt.month\nalldata['dow'] = alldata.timestamp.dt.dayofweek\n\n# Other feature engineering\nalldata['rel_floor'] = alldata['floor'] / alldata['max_floor'].astype(float)\nalldata['rel_kitch_sq'] = alldata['kitch_sq'] / alldata['full_sq'].astype(float)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6684bca1-a926-6419-3b7d-04ca8be39c79"},"outputs":[],"source":"wpca +=['month_year_cnt','week_year_cnt','dow','month','rel_floor','rel_kitch_sq']\nwopca+=['month_year_cnt','week_year_cnt','dow','month','rel_floor','rel_kitch_sq']\nallfeature=list(set(wpca+wopca))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55da1688-c38f-eebd-6ffc-48cfb85c61ed"},"outputs":[],"source":"## let's try a 5-fold CV\nfrom sklearn.model_selection import KFold\nkf = KFold(5,shuffle =True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df1bcbf2-d6de-77b3-cf36-19d52cd8e867"},"outputs":[],"source":"import xgboost as xgb\nxgb_params = {\n    'eta': 0.05,\n    'max_depth': 12,\n    'subsample': 1,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1,\n    'min_child_weight': 200\n}\n\ndef cv_xgb(val_train_X,val_train_Y,val_val_X,val_val_Y):\n    dtrain = xgb.DMatrix(val_train_X, val_train_Y, feature_names=val_train_X.columns)\n    dval = xgb.DMatrix(val_val_X, val_val_Y, feature_names=val_val_X.columns)\n\n    # Uncomment to tune XGB `num_boost_rounds`\n    partial_model = xgb.train(xgb_params, dtrain, num_boost_round=1000, evals=[(dval, 'val')],\n                           early_stopping_rounds=50, verbose_eval=20)\n\n    num_boost_round = partial_model.best_iteration\n    return(num_boost_round,partial_model.best_score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55a7851d-da17-964f-ed34-b3bbfab188d9"},"outputs":[],"source":"train_col = [c for c in alldata[wpca].columns if c not in ['price_doc','Source']]\nalldata_1 = alldata[alldata.Source=='Train'][train_col]\n\nfor val_train, val_val in kf.split(alldata_1):\n    val_train_X = alldata_1.ix[val_train].drop('LogAmt',axis=1)\n    val_train_Y = alldata_1.ix[val_train].LogAmt\n    val_val_X = alldata_1.ix[val_val].drop('LogAmt',axis=1)\n    val_val_Y = alldata_1.ix[val_val].LogAmt\n    print(\"%s %s %s %s\" % (val_train_X.shape, val_train_Y.shape, val_train.shape, val_val.shape))\n    print(cv_xgb(val_train_X,val_train_Y,val_val_X,val_val_Y))\n    break  ## this takes long to run, I am breaking it to demostrate; comment the line if you want full CV"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cff28a2c-e4bf-ea82-d730-e112e480ee1c"},"outputs":[],"source":"## Run it on the full model \nnum_boost_round = 200\nall_train_X = alldata_1.drop('LogAmt',axis=1)\nall_train_Y = alldata_1.LogAmt\nall_test_X = alldata[alldata.Source=='Test'][train_col].drop('LogAmt',axis=1)\ndtrain_all = xgb.DMatrix(all_train_X, all_train_Y, feature_names=all_train_X.columns)\ndtest      = xgb.DMatrix(all_test_X, feature_names=all_test_X.columns)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain_all, num_boost_round=num_boost_round)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a7a1cd7-ec68-039f-fc26-309809858313"},"outputs":[],"source":"## important features\nfig, ax = plt.subplots(1, 1, figsize=(8, 16))\nxgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a974af27-6881-19cc-d14e-0f1f9648c795"},"outputs":[],"source":"## Make a predicition\nylog_pred = model.predict(dtest)\ny_pred = np.exp(ylog_pred) - 1\nid_test = alldata[alldata.Source=='Test'].id\ndf_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\ndf_sub.to_csv('sub_pca.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}