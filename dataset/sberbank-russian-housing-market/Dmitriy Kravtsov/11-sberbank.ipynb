{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt  \nimport matplotlib\npd.set_option('display.max_rows', 1000)\nfrom sklearn.preprocessing import StandardScaler\nfrom zipfile import ZipFile \nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ntrain = pd.read_csv(ZipFile(\"/kaggle/input/sberbank-russian-housing-market/train.csv.zip\").open('train.csv'), parse_dates=['timestamp'])\ntest = pd.read_csv(ZipFile(\"/kaggle/input/sberbank-russian-housing-market/test.csv.zip\").open('test.csv'), parse_dates=['timestamp'])\nmacro = pd.read_csv(ZipFile(\"/kaggle/input/sberbank-russian-housing-market/macro.csv.zip\").open('macro.csv'), parse_dates=['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## checking for normal distribution of saleprice\nimport seaborn as sns\nx = 'price_doc'\nfig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)    \nfig.suptitle(x, fontsize=20)\n\n### distribution    \nax[0].title.set_text('   distribution')    \nvariable = train[x].fillna(train[x].mean())    \nbreaks = np.quantile(variable, q=np.linspace(0, 1, 11))\n\n# variable = variable[ (variable > breaks[breaks[0]]) & (variable < breaks[breaks[1]]) ]     \nsns.distplot(variable, hist=True, kde=True, kde_kws={\"shade\": True}, ax=ax[0])    \ndes = train[x].describe()    \nax[0].axvline(des[\"25%\"], ls='--')    \nax[0].axvline(des[\"mean\"], ls='--')    \nax[0].axvline(des[\"75%\"], ls='--')    \nax[0].grid(True)    \ndes = round(des, 2).apply(lambda x: str(x))    \nbox = '\\n'.join((\"min: \"+des[\"min\"], \"25%: \"+des[\"25%\"], \"mean: \"+des[\"mean\"], \"75%: \"+des[\"75%\"], \"max: \"+des[\"max\"]))    \nax[0].text(0.95, 0.95, box, transform=ax[0].transAxes, fontsize=10, va='top', ha=\"right\", bbox=dict(boxstyle='round', facecolor='white', alpha=1))\n### boxplot     \nax[1].title.set_text('outliers (log scale)')    \ntmp_df = pd.DataFrame(train[x])    \ntmp_df[x] = np.log(tmp_df[x])    \ntmp_df.boxplot(column=x, ax=ax[1])    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat((train.loc[:,'timestamp':'market_count_5000'], test.loc[:,'timestamp':'market_count_5000']))\n\nmacro_cols = [\"timestamp\",\"balance_trade\",\"balance_trade_growth\",\"usdrub\",\"average_provision_of_build_contract\",\"micex_rgbi_tr\",\"micex_cbi_tr\",\"deposits_rate\",\"mortgage_value\",\"mortgage_rate\",\"income_per_cap\",\"museum_visitis_per_100_cap\",\"apartment_build\"]\ndf = df.merge(macro[macro_cols], on='timestamp', how='left')\n\n\n\ndf['Sale_year'] = df['timestamp'].dt.year\ndf['Sale_month'] = df['timestamp'].dt.month\ndf = df.drop(['timestamp'], axis=1)\ndf['max_floor'] = df['max_floor'].fillna(0)\n\n# replace NaNs with \"0\"\n    \nall_columns = list(set(df.columns))\nfor col in all_columns:\n    df[col].fillna('N', inplace = True)\n\n\n\n\ndf['floor'] = df['floor'].astype(str)\ndf['Sale_year'] = df['Sale_year'].astype(str)\ndf['material'] = df['material'].astype(str)\ndf['product_type'] = df['product_type'].astype(str)\ndf['culture_objects_top_25'] = df['culture_objects_top_25'].astype(str)\ndf['thermal_power_plant_raion'] = df['thermal_power_plant_raion'].astype(str)\ndf['incineration_raion'] = df['incineration_raion'].astype(str)\ndf['oil_chemistry_raion'] = df['oil_chemistry_raion'].astype(str)\ndf['radiation_raion'] = df['radiation_raion'].astype(str)\ndf['railroad_terminal_raion'] = df['railroad_terminal_raion'].astype(str)\ndf['big_market_raion'] = df['big_market_raion'].astype(str)\ndf['nuclear_reactor_raion'] = df['nuclear_reactor_raion'].astype(str)\ndf['detention_facility_raion'] = df['detention_facility_raion'].astype(str)\ndf['healthcare_centers_raion'] = df['healthcare_centers_raion'].astype(str)\ndf['water_1line'] = df['water_1line'].astype(str)\ndf['big_road1_1line'] = df['big_road1_1line'].astype(str)\ndf['railroad_1line'] = df['railroad_1line'].astype(str)\ndf['ID_railroad_terminal'] = df['ID_railroad_terminal'].astype(str)\ndf['ecology'] = df['ecology'].astype(str)\n\n# df['full_sq_log'] = np.log1p(df['full_sq'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler((-1,1))\nobject_columns = df.select_dtypes(include=[np.object])\nnum_columns = df.select_dtypes(exclude=[np.object])\n\n   \n# Apply label encoder for category columns\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nfor col in object_columns:\n    df[col] = le.fit_transform(df[col].astype(str))\n    \n# for col in all_columns:\n#     df[col] = scaler.fit_transform(df[[col]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_memory_usage(df):\n    \"\"\" The function will reduce memory of dataframe\n    Note: Apply this function after removing missing value\"\"\"\n    intial_memory = df.memory_usage().sum()/1024**2\n    print('Intial memory usage:',intial_memory,'MB')\n    for col in df.columns:\n        mn = df[col].min()\n        mx = df[col].max()\n        if df[col].dtype != object:            \n            if df[col].dtype == int:\n                if mn >=0:\n                    if mx < np.iinfo(np.uint8).max:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < np.iinfo(np.uint16).max:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < np.iinfo(np.uint32).max:\n                        df[col] = df[col].astype(np.uint32)\n                    elif mx < np.iinfo(np.uint64).max:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n            if df[col].dtype == float:\n                df[col] =df[col].astype(np.float32)\n    \n    red_memory = df.memory_usage().sum()/1024**2\n    print('Memory usage after complition: ',red_memory,'MB')\n    \nreduce_memory_usage(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value'] = df.isnull().sum()\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating matrices for feature selection:\nX_train = df[:train.shape[0]]\nX_test_fin = df[train.shape[0]:]\ny = train.price_doc\nX_train['Y'] = y\ndf = X_train\ndf['Y'] = df['Y']/df['usdrub']\ndf.head() ## DF for Model training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nX = df.drop('Y', axis=1)\ny = df.Y\n\n\n\nparams = {\n        'objective':'reg:linear',\n        'n_estimators': 5000,\n        'booster':'gbtree',\n        'max_depth':5,\n        'eval_metric':'mae',\n        'learning_rate':0.05, \n        'min_child_weight':0,\n        'subsample':0.8,\n        'colsample_bytree':0.79,\n        'reg_alpha':0,\n        'seed':45,\n        'gamma':0,\n        'nthread':-1\n}\n\n\nx_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=10)\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix(X_test_fin)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nclf = xgb.train(params, d_train, 5000, watchlist, early_stopping_rounds=50, feval=xgb_r2_score, maximize=True, verbose_eval=10)\n\np_test = clf.predict(d_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['ID'] = test['id']\nsub['price_doc'] =  52 * p_test\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(clf, max_num_features=50, height=0.8, ax=ax)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}