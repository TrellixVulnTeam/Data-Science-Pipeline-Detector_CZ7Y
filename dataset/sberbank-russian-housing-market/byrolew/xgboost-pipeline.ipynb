{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2ae4d9e6-a98f-c6a4-134d-22745ba18b5f"},"source":"Wzorowałam się na https://www.kaggle.com/bguberfain/naive-xgb-lb-0-317. Niestety moje próby pogorszyły wynik zamiast polepszyć, ale bardzo trudno było sprawdzać poprawność wyniku, gdyż w walidacji krzyżowej wyniki wychodziły dużo gorsze, a przede wszystkim zupełnie inne niż później na zbiorze testowym. Ma to zapewne związek z charakterem danych (szereg czasowy) i faktem, że dane testowe pochodzą z późniejszego okresu niż dane treningowe. Warto byłoby budować zbiór walidacyjnie w analogiczny sposób. W stosunku do powyższego notebooka zdecydowałam się zaimplementować cały proces przetwarzania i modelowania danych jako pipeline, który jest dużo czytelniejszym i łatwiejszym w późniejszym utrzymaniu sposobem.  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c06e864f-5639-8cf8-57f3-cb8977b53eab"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom scipy import sparse as sp\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import Imputer, OneHotEncoder\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import make_scorer"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"60da4661-4ebc-f730-3d35-fc286cf3f303"},"outputs":[],"source":"df_train = pd.read_csv('../input/train.csv')\ndf_macro = pd.read_csv('../input/macro.csv')\ndf_test = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f892f3b0-84eb-a793-711c-047f40723767"},"outputs":[],"source":"# Join with macro variables\ndf_all = pd.concat([df_train, df_test])\ndf = pd.merge_ordered(df_all, df_macro, on='timestamp', how='left')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38247205-1c64-4318-2082-0df5dfcbc604"},"outputs":[],"source":"# Change what we can to floats\nfor col in df.columns:\n    if df[col].dtype == int:\n        df[col] = df[col].astype(float).copy()\n    elif df[col].dtype != float:\n        df.loc[df[col].str.contains('^no$', na=False), col] = 0.0\n        df.loc[df[col].str.contains('^yes$', na=False), col] = 1.0\n        try:\n            df[col] = df[col].astype(float).copy()\n        except ValueError:\n            pass"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d61cab71-7379-93fc-fee5-26809d8a94a2"},"outputs":[],"source":"# Change again to train and final test\ndf_all = df[np.isfinite(df['price_doc'])]\ndf_final_test = df[~ np.isfinite(df['price_doc'])]\n\nx_final = df_final_test.drop(['price_doc', 'id'], axis=1)\ny_final = df_final_test['price_doc']\nid_test = df_final_test['id']\n\ny_train = df_all['price_doc']\nx_train = df_all.drop(['price_doc', 'id'], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f074b1b-ecf9-a5d9-fd50-15f55745aa12"},"outputs":[],"source":"# Selector coumns by name or type\nclass PandasSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, dtype=None, columns=None, inverse=False,\n                 return_vector=True):\n        self.dtype = dtype\n        self.columns = columns\n        self.inverse = inverse\n        self.return_vector = return_vector\n\n    def check_condition(self, x, col):\n        cond = (self.dtype is not None and x[col].dtype == self.dtype) or \\\n               (self.columns is not None and col in self.columns)\n        return self.inverse ^ cond\n\n    def fit(self, x, y=None):\n        return self\n\n    def _check_if_all_columns_present(self, x):\n        if not self.inverse and self.columns is not None:\n            missing_columns = set(self.columns) - set(x.columns)\n            if len(missing_columns) > 0:\n                missing_columns_ = ','.join(col for col in missing_columns)\n                raise KeyError('Keys are missing in the record: %s' %\n                               missing_columns_)\n\n    def transform(self, x):\n        # check if x is a pandas DataFrame\n        if not isinstance(x, pd.DataFrame):\n            raise KeyError('Input is not a pandas DataFrame')\n\n        selected_cols = []\n        for col in x.columns:\n            if self.check_condition(x, col):\n                selected_cols.append(col)\n\n        # if the column was selected and inversed = False make sure the column\n        # is in the DataFrame\n        self._check_if_all_columns_present(x)\n\n        # if only 1 column is returned return a vector instead of a dataframe\n        if len(selected_cols) == 1 and self.return_vector:\n            return np.array(x[selected_cols[0]])\n        else:\n            return np.array(x[selected_cols])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"52e68435-9ea1-1a65-8b3e-eac2ffad3167"},"outputs":[],"source":"# Converter fron string to int (for one hot encoder)\nclass StringConverter(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        self.map = {} # column : string : int\n    \n    def fit(self, X, *args):\n        for col in range(X.shape[1]):\n            self.map[col] = {}\n            idx = 1\n            for row in range(X.shape[0]):                \n                s = X[row, col]\n                if s not in self.map[col]:\n                    self.map[col][s] = idx\n                    idx += 1\n        return self\n\n    def transform(self, X):\n        X_int = np.zeros(shape=X.shape)\n        for col in range(X.shape[1]):\n            X_int[:, col] = np.array([self.map[col].get(s, 0) for s in X[:, col]])\n\n        return X_int"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7a4ef65-2237-3498-0f8c-31717930821f"},"outputs":[],"source":"# Adds a column in sparse matrix (because of bug in xgboost)\nclass AddDummy(BaseEstimator, TransformerMixin):\n    def fit(self, X, *args):\n        return self\n    \n    def transform(self, X):\n        return sp.hstack([X, sp.csr_matrix(np.ones((X.shape[0], 1)))])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fcda22cc-0301-9c7f-6c85-2fbd5d1c9b38"},"outputs":[],"source":"# Adds features based on date\nclass DatesFeaturer(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        self.month_year_map = {}\n        self.week_year_map = {}\n    \n    def fit(self, df, *args):\n        month_year = pd.to_datetime(df.timestamp).dt.month + pd.to_datetime(df.timestamp).dt.year * 100\n        self.month_year_map = month_year.value_counts().to_dict()\n        week_year = pd.to_datetime(df.timestamp).dt.weekofyear + pd.to_datetime(df.timestamp).dt.year * 100\n        self.week_year_map = week_year.value_counts().to_dict()\n        return self\n    \n    def transform(self, df):\n        month_year = pd.to_datetime(df.timestamp).dt.month + pd.to_datetime(df.timestamp).dt.year * 100\n        week_year = pd.to_datetime(df.timestamp).dt.weekofyear + pd.to_datetime(df.timestamp).dt.year * 100\n        \n        new_df = pd.DataFrame({\n            'month_year_count': month_year.map(self.month_year_map),\n            'week_year_count': week_year.map(self.week_year_map),\n            'month': pd.to_datetime(df.timestamp).dt.month,\n            'dow': pd.to_datetime(df.timestamp).dt.dayofweek\n        })\n        \n        return np.array(new_df)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5df06cf3-80a6-0b47-bfff-231a2afd5bb7"},"outputs":[],"source":"# Adds relative features\nclass RelativeFeaturer(BaseEstimator, TransformerMixin):\n    \n    def fit(self, df, *args):\n        return self\n    \n    def transform(self, df):   \n        new_df = pd.DataFrame({\n            'rel_floor': df['floor'] / np.maximum(1.0, df['max_floor'].astype(float)),\n            'rel_kitch_sq': df['kitch_sq'] / np.minimum(1.0, df['full_sq'].astype(float)),\n        })\n        return np.array(new_df)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ff36947-223e-25e3-5c8e-686b836e3513"},"outputs":[],"source":"# Converts estimator to transform in order to ensemble many estimators\nclass EstimatorToTransform(BaseEstimator, TransformerMixin):    \n    def __init__(self, estimator):\n        self.estimator = estimator\n    \n    def fit(self, X, *args):\n        self.estimator.fit(X, *args)\n        return self\n\n    def transform(self, X):\n        pred = self.estimator.predict(X)\n        return pred.reshape(-1, 1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30ac1ab2-defd-b434-7fa9-cf563b0e7cf9"},"outputs":[],"source":"def rmsle_score(pred, true):\n    return (np.sum((np.log(1 + pred) - np.log(1 + true))**2) / len(pred))**0.5"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a0b048c-7b2e-064e-db05-39507d62b350"},"outputs":[],"source":"# Defining pipelines\n\nfloat_pipeline = make_pipeline(\n    PandasSelector(dtype=float),\n)\n\neco_pipeline = make_pipeline(\n    PandasSelector(columns=['ecology'], return_vector=False),\n    StringConverter(),\n    OneHotEncoder(),\n)\n\nprod_pipeline = make_pipeline(\n    PandasSelector(columns=['product_type'], return_vector=False),\n    StringConverter(),\n    OneHotEncoder(),\n)\n\nsub_pipeline = make_pipeline(\n    PandasSelector(columns=['sub_area'], return_vector=False),\n    StringConverter(),\n    OneHotEncoder(),\n)\n\ndates_pipeline = make_pipeline(\n    DatesFeaturer(),\n)\n\nrel_pipeline = make_pipeline(\n    RelativeFeaturer(),\n)\n\npipeline_ensemble = make_pipeline(\n    make_union(\n        EstimatorToTransform(\n            xgb.XGBRegressor(\n                n_estimators=500,\n                learning_rate=0.05,\n                max_depth=5,\n                subsample=0.7,\n                colsample_bytree=0.7,\n                objective='reg:linear',\n            ),\n        ),\n        EstimatorToTransform(\n            xgb.XGBRegressor(\n                n_estimators=100,\n                learning_rate=0.05,\n                max_depth=3,\n                subsample=1,\n                colsample_bytree=1,\n                objective='reg:linear',\n            ),\n        ),\n    ),\n)\n\nfinal_pipeline = make_pipeline(\n    make_union(\n        rel_pipeline,\n        dates_pipeline,\n        float_pipeline,\n        eco_pipeline,\n        prod_pipeline,\n        sub_pipeline,\n    ),\n    AddDummy(),\n    Imputer(),\n    pipeline_ensemble,\n    LinearRegression(),\n\n)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d62ccd60-5fb9-ab2b-fa65-846464d8f2b6"},"outputs":[],"source":"# Calculate cross-validate score\n\ncv_score = cross_val_score(\n        final_pipeline,\n        x_train,\n        y_train,\n        scoring=make_scorer(rmsle_score),\n        cv=3,\n)\n\nnp.mean(cv_score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86fcb679-ce46-6e87-c1f7-d5009b3c3c97"},"outputs":[],"source":"%%time\n# Fitting model\nmodel = final_pipeline.fit(x_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21ac3629-3285-ce1f-9a51-0b9d0413a152"},"outputs":[],"source":"pred_train = model.predict(x_train)\nprint(rmsle_score(pred_train, y_train))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8191081-1c98-764c-c776-d64dab2f1a86"},"outputs":[],"source":"# Predicting\nfinal_pred = model.predict(x_final)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"34b2b0ae-2c2d-b313-a823-f4ab4c1d56bf"},"outputs":[],"source":"# Creating final submission file\ndf_sub = pd.DataFrame({'id': id_test.astype(int), 'price_doc': final_pred.astype(int)})\ndf_sub.to_csv('sub.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}