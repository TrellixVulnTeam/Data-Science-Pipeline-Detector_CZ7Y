{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f6bf0c9c-ff7c-7d82-79bc-26a0d98d4263"},"source":"The author is still studying in Beijing.\n\nIn the code below, I used cov matrix to determine the most significant contribution factors to 'price_doc', then use them to train a 3-layer fully connected neural network.\n\nTrained by Adam optimizer and a batch size of 100 samples, running for 4000 iterations.\n\nThe test loss for the network is 0.6 using root mean squared log error (which is the standard loss function in evaluating models here). "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"682a3318-9adc-1b93-be37-740278cd9120"},"outputs":[],"source":"from pandas import *\nf = read_csv(r'../input/train.csv')\n#read the the first few lines of the data\nprint(f.head())\n#check if there are any duplicates\nf.duplicated().value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc49b18b-8fc6-141b-82b4-e8226fd07bdb"},"outputs":[],"source":"#check how many rows contain NaN\nprint('Rows that contains NaN')\nprint(f.isnull().any(axis=1).value_counts())\n#check how many columns contain NaN\nprint('Columns that contains Nan')\nprint(f.isnull().any(axis=0).value_counts())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0723a441-58bf-1f55-bd42-82bfb54e20c6"},"outputs":[],"source":"f['timestamp'] = f['timestamp'].map(to_datetime)#convert to datetime objects\nf['timestamp'].describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8f4814c8-6a92-47ad-fcfc-e8269c6fba6b"},"outputs":[],"source":"#get a rough view of relevence between timestamp and price_doc\nfrom matplotlib.pyplot import *\npcm_bytime = f[['timestamp', 'price_doc']].groupby('timestamp').mean()\npcm_bytime.plot()\nshow()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1bf3caa0-5f78-aef4-bcfa-10d245c84e4a"},"outputs":[],"source":"#calculate the covariance matrix\npc_cov = f.cov()['price_doc']\npc_cov = pc_cov.copy()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a6d73c57-a1c1-3151-5af5-e063eb06c618"},"outputs":[],"source":"#sorting the covariance matrix, and take a look\npc_cov.sort_values(ascending=False, inplace=True)\ndel pc_cov['price_doc']\npc_cov.head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95bcb9a6-83c2-90b1-e9e4-1c858e63775d"},"outputs":[],"source":"#we choose first 8 of the covariance matrix \nindex_sel = ['price_doc'] + pc_cov[0:8].index.values.tolist()\nfnew = f[index_sel]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7471b90-3b26-d96c-9fd1-dae0b5bd5c4d"},"outputs":[],"source":"fnew.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2221d1af-5387-8b82-90bd-3f36cc07e80d"},"outputs":[],"source":"fnew['price_doc'].describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0debcf03-e558-494a-8742-bf28114e62cb"},"outputs":[],"source":"#writing the transformed data to csv file\nfnew.to_csv(r'./data.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2ecfddd9-f9e1-8435-04b3-d7aa820c60c0"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\nsess = tf.Session()\ndata = pd.read_csv(r'./data.csv')\ny_vals = data['price_doc']\ndel data['price_doc']\ndel data[data.columns[0]] #discard first column\n\nprint(data.head())\nprint(y_vals.head())\n\nx_vals = data.as_matrix() #convert to numpy ndarray\ny_vals = y_vals.values #conver to numpy ndarray\n\nseed = 3\ntf.set_random_seed(3)\nnp.random.seed(seed=seed)\nbatch_size = 100\n\n#split into train and test sets\ntrain_indices = np.random.choice(len(x_vals), round(len(x_vals) * 0.9), replace=False)\ntest_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))\nx_vals_train = x_vals[train_indices]\ny_vals_train = y_vals[train_indices]\nx_vals_test = x_vals[test_indices]\ny_vals_test = y_vals[test_indices]\n\ndef normalize_cols(mat):\n    col_max = mat.max(axis=0)\n    col_min = mat.min(axis=0)\n    return (mat - col_min) / (col_max - col_min)\n\nx_vals_train = normalize_cols(x_vals_train)\nx_vals_test = normalize_cols(x_vals_test)\n\ndef init_weight_uniform(shape, low, high):\n    return tf.Variable(tf.random_uniform(shape=shape, minval=low, maxval=high))\n\ndef init_weight_normal(shape, st_dev):\n    return tf.Variable(tf.random_normal(shape=shape, stddev=st_dev))\n\ndef init_bias(shape, st_dev, mean=0.0):\n    return tf.Variable(tf.random_normal(shape=shape, stddev=st_dev))\n\nx_data = tf.placeholder(shape=[None, 8], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n\ndef fully_connected_layer(input_layer, weights, bias):\n    layer = tf.add(tf.matmul(input_layer, weights), bias)\n    return tf.nn.relu(layer)\n\n#building network graph\n#weight_1 = init_weight_uniform(shape=[8, 40], low=-1/np.sqrt(8), high=1/np.sqrt(8))\nweight_1 = init_weight_normal(shape=[8, 40], st_dev=10.0)\nbias_1 = init_bias(shape=[40], st_dev=10.0)\nlayer_1 = fully_connected_layer(x_data, weight_1, bias_1)\n\n#weight_2 = init_weight_uniform(shape=[40, 15], low=-1/np.sqrt(40), high=1/np.sqrt(40))\nweight_2 = init_weight_normal(shape=[40, 15], st_dev=10.0)\nbias_2 = init_bias(shape=[15], st_dev=10.0)\nlayer_2 = fully_connected_layer(layer_1, weight_2, bias_2)\n\n#weight_3 = init_weight_uniform(shape=[15, 1], low=-1/np.sqrt(20), high=1/np.sqrt(20))\nweight_3 = init_weight_normal(shape=[15, 1], st_dev=10.0)\nbias_3 = init_bias(shape=[15], st_dev=10.0)\nfinal_output = fully_connected_layer(layer_2, weight_3, bias_3)\n\n#loss function, root mean squared log error\nloss = tf.sqrt(tf.reduce_mean(tf.square(tf.log(final_output + 1) - tf.log(y_target + 1))))\nmy_opt = tf.train.AdamOptimizer(0.05)\ntrain_step = my_opt.minimize(loss)\n\n#start training\nsaver = tf.train.Saver()\ninit = tf.initialize_all_variables()\nsess.run(init)\n\ntrain_loss_vec = []\ntest_loss_vec = []\nfor i in range(4000):\n    rand_index = np.random.choice(len(x_vals_train), size=batch_size)\n    rand_x = x_vals_train[rand_index]\n    rand_y = np.transpose([y_vals_train[rand_index]])\n    sess.run(train_step, feed_dict={x_data:rand_x, y_target:rand_y})\n    tmp_loss = sess.run(loss, feed_dict={x_data:rand_x, y_target:rand_y})\n    train_loss_vec.append(tmp_loss)\n    tmp_loss = sess.run(loss, feed_dict={x_data:x_vals_test, y_target:np.transpose([y_vals_test])})\n    test_loss_vec.append(tmp_loss)\n    if i%100 == 0:\n        '''\n        #save model\n        if i%400 == 0:\n            saver.save(sess, r'../output/model', global_step=i)\n        '''\n        print('Generation: ' + str(i) + '. Loss = ' + str(tmp_loss))\n\n\n#plotting model\nfrom matplotlib import pyplot as plt\nplt.plot(train_loss_vec, 'k--', label='Train Loss')\nplt.plot(test_loss_vec, 'r--', label='Test Loss')\nplt.title('Loss per Generation')\nplt.xlabel('Generation')\nplt.ylabel('Loss')\nplt.legend(loc='upper right')\nplt.show()"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}