{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"a976b8f9-414d-9802-7573-18b4bb6010b0"},"source":"A naive model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5986d91a-19da-7400-8838-86f451c6c9d3"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns # visualization\nimport xgboost as xgb # GBDT\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor # randomForest, GradientBoosting\n\nfrom sklearn import preprocessing # categorical values\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import Imputer\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor # VIF\n\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import normalize\nfrom scipy import stats # PCA\n\nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53368997-c612-6c2d-b76e-97d906bc4bfb"},"outputs":[],"source":"df_train = pd.read_csv(\"train.csv\", parse_dates=['timestamp'])\ndf_test = pd.read_csv(\"test.csv\", parse_dates=['timestamp'])\ndf_macro = pd.read_csv(\"macro.csv\", parse_dates=['timestamp'])\n\n# truncate the extreme values in price_doc\nulimit = np.percentile(df_train.price_doc.values, 99)\nllimit = np.percentile(df_train.price_doc.values, 1)\ndf_train['price_doc'].ix[df_train['price_doc']>ulimit] = ulimit\ndf_train['price_doc'].ix[df_train['price_doc']<llimit] = llimit"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eea596dc-44bb-5776-28ec-bb2819d51613"},"outputs":[],"source":"y_train = df_train['price_doc'].values\nid_test = df_test['id']\n\ndf_train.drop(['id', 'price_doc'], axis=1, inplace=True)\ndf_test.drop(['id'], axis=1, inplace=True)\n\n# build df_all = (df_train+df_test).join(df_macro)\nnum_train = len(df_train)\ndf_all = pd.concat([df_train, df_test])\ndf_all = pd.merge_ordered(df_all, df_macro, on='timestamp', how='left')\nprint(df_all.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8fbd4eb-f7fb-2b17-b4c7-9ca07eb1ae6a"},"outputs":[],"source":"# add month-year count\nmonth_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ndf_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n# add week-year count\nweek_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\nweek_year_cnt_map = week_year.value_counts().to_dict()\ndf_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n# add year\ndf_all['year'] = df_all.timestamp.dt.year\n\n# add month of year\ndf_all['month_of_year'] = df_all.timestamp.dt.month\n\n# add week of year\ndf_all['week_of_year'] = df_all.timestamp.dt.weekofyear\n\n# add day-of-week\ndf_all['day_of_week'] = df_all.timestamp.dt.dayofweek\n\n# age of building\ndf_all['build_year'][df_all['build_year']==20052009] = 2005\ndf_all['build_year'][df_all['build_year']==0] = df_all['build_year'][0]\ndf_all['build_year'][df_all['build_year']==1] = df_all['build_year'][0]\ndf_all['build_year'][df_all['build_year']==3] = df_all['build_year'][0]\ndf_all['build_year'][df_all['build_year']==71] = df_all['build_year'][0]\ndf_all['build_year'][df_all['build_year']==4965] = df_all['build_year'][0]\ndf_all['build_year'][df_all['build_year']==20] = 2000\ndf_all['build_year'][df_all['build_year']==215] = 2015\ndf_all['age_of_building'] = df_all['year'] - df_all['build_year']\n\n# remove timestamp column (may overfit the model in train)\ndf_all.drop(['timestamp'], axis=1, inplace=True)\nprint(df_all.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c611d7aa-9b73-50e8-d1c8-99f764e88aaa"},"outputs":[],"source":"# floor/max_floor ratio\ndf_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\n\n# num of floor from top \ndf_all[\"floor_from_top\"] = df_all[\"max_floor\"] - df_all[\"floor\"]\n\n# kitchen-full ratio\ndf_all['kitchen_full'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\ndf_all['kitchen_full'].ix[df_all['kitchen_full']<0] = 0\ndf_all['kitchen_full'].ix[df_all['kitchen_full']>1] = 1\n\n# kitchen-living ratio\ndf_all['kitchen_living'] = df_all['kitch_sq'] / df_all['life_sq'].astype(float)\ndf_all['kitchen_living'].ix[df_all['kitchen_living']<0] = 0\ndf_all['kitchen_living'].ix[df_all['kitchen_living']>1] = 1\n\n# add living ratio\ndf_all['living_ratio'] = df_all['life_sq']/df_all['full_sq']\ndf_all['living_ratio'].ix[df_all['living_ratio']<0] = 0\ndf_all['living_ratio'].ix[df_all['living_ratio']>1] = 1\n\n# add non-living area\ndf_all['non_living'] = df_all['full_sq'] - df_all['life_sq']\n\n# add living area per room\ndf_all['living_room'] = df_all['life_sq'] / df_all['num_room']\n\n# apartment condition\ndf_all['state'][df_all['state']==33] = 3\n\n# add preschool ratio\ndf_all[\"ratio_preschool\"] = df_all[\"children_preschool\"] / df_all[\"preschool_quota\"].astype(\"float\")\n\n# add school ratio\ndf_all[\"ratio_school\"] = df_all[\"children_school\"] / df_all[\"school_quota\"].astype(\"float\")\n\nprint(df_all.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"064077ee-368a-ee62-9fcc-676b234a961c"},"outputs":[],"source":"# numerical and categorical data types\ndf_all_dtype=df_all.dtypes\ndisplay_nvar = len(df_all.columns)\ndf_all_dtype_dict = df_all_dtype.to_dict()\ndf_all.dtypes.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e6ba4a3-b0d4-37f4-e8fe-248bcac6d43b"},"outputs":[],"source":"# deal with categorical values\nfor f in df_all.columns:\n    if df_all[f].dtype=='object' and f not in ['sub_area']:\n        print(f) # there should be 18 categorical variables\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_all[f].values.astype('str')) + list(df_all[f].values.astype('str')))\n        df_all[f] = lbl.transform(list(df_all[f].values.astype('str')))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e31c74a-a00a-ee1c-93c6-0afc827706b7"},"outputs":[],"source":"# convert to numpy values\ndf_all.drop(['sub_area'], axis=1, inplace=True)\nX_all = df_all.values\nprint(X_all.shape)\n\nX_train = X_all[:num_train]\nX_test = X_all[num_train:]\n\ndf_columns = df_all.columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd63846c-5e27-6e88-3d3b-4a47abc4a82d"},"outputs":[],"source":"# use VIF\n\nclass ReduceVIF(BaseEstimator, TransformerMixin):\n    def __init__(self, thresh=5.0, impute=True, impute_strategy='median'):\n        # From looking at documentation, values between 5 and 10 are \"okay\".\n        # Above 10 is too high and so should be removed.\n        self.thresh = thresh\n        \n        # The statsmodel function will fail with NaN values, as such we have to impute them.\n        # By default we impute using the median value.\n        # This imputation could be taken out and added as part of an sklearn Pipeline.\n        if impute:\n            self.imputer = Imputer(strategy=impute_strategy)\n\n    def fit(self, X, y=None):\n        print('ReduceVIF fit')\n        if hasattr(self, 'imputer'):\n            self.imputer.fit(X)\n        return self\n\n    def transform(self, X, y=None):\n        print('ReduceVIF transform')\n        columns = X.columns.tolist()\n        if hasattr(self, 'imputer'):\n            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n        return ReduceVIF.calculate_vif(X, self.thresh)\n\n    @staticmethod\n    def calculate_vif(X, thresh=5.0):\n        # Taken from https://stats.stackexchange.com/a/253620/53565 and modified\n        dropped=True\n        while dropped:\n            # Loop repeatedly until we find that all columns within our dataset\n            # have a VIF value we're happy with.\n            variables = X.columns\n            dropped=False\n            vif = []\n            new_vif = 0\n            for var in X.columns:\n                new_vif = variance_inflation_factor(X[variables].values, X.columns.get_loc(var))\n                vif.append(new_vif)\n                if np.isinf(new_vif):\n                    break\n            max_vif = max(vif)\n            if max_vif > thresh:\n                maxloc = vif.index(max_vif)\n                print('Dropping %s with vif= %f' % (X.columns[maxloc],max_vif))\n                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n                dropped=True\n        return X\n    \n#transformer = ReduceVIF()\n#X = transformer.fit_transform(df_train[df_train.columns[-10:]], y_train)\n#X.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11f40dfe-8a13-0f39-2b4b-d1ca56900ca7"},"outputs":[],"source":"xgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(X_train, y_train, feature_names=df_columns)\ndtest = xgb.DMatrix(X_test, feature_names=df_columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a970d3f7-09fd-0288-2144-4cb0c06855e0"},"outputs":[],"source":"cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=200, early_stopping_rounds=20,\n    verbose_eval=50, show_stdv=False)\ncv_output[['train-rmse-mean', 'test-rmse-mean']].plot()\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6dcbe161-7a4f-5a33-9ad9-d88aab2b61fb"},"outputs":[],"source":"num_boost_rounds = len(cv_output)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fd446c7-d660-048b-e7da-8b404c51a589"},"outputs":[],"source":"model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3a05c4df-719e-5e29-a287-726dafcdfc12"},"outputs":[],"source":"y_pred = model.predict(dtest)\n\ndf_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n\ndf_sub.to_csv('sub.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ededca79-9c29-aa96-5d27-c3141a54731f"},"outputs":[],"source":"X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, random_state=1848)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2bdcb744-464f-c9f9-268c-624979a7a208"},"outputs":[],"source":"# sklearn_boost = GradientBoostingRegressor(random_state=1849)\n# sklearn_boost.fit(X_train, y_train)\n# print('Training Error: {:.3f}'.format(1 - sklearn_boost.score(X_train, y_train)))\n# print('Validation Error: {:.3f}'.format(1 - sklearn_boost.score(X_validation, y_validation)))\n# %timeit sklearn_boost.fit(X_train, y_train.values.ravel())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c0b5b25c-739b-0541-6043-2ff31ff567c4"},"outputs":[],"source":"# random_forest = RandomForestRegressor(random_state=1852)\n# random_forest.fit(X_train, y_train)\n# print('Training Error: {:.3f}'.format(1 - random_forest.score(X_train, y_train)))\n# print('Validation Error: {:.3f}'.format(1 - random_forest.score(X_validation, y_validation)))\n# %timeit random_forest.fit(X_train, y_train.values.ravel())"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}