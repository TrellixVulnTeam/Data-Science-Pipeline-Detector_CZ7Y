{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"ab1bd908-33dc-6f42-ea86-43d2ec2726a6"},"source":"from Hao"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a8c44092-3791-5953-541c-e2156fdd9c39"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n%matplotlib inline\nfrom sklearn import model_selection, preprocessing\nimport xgboost as xgb\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6e54d007-266f-4971-937f-decf5de2b833"},"outputs":[],"source":"total = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nmacro = pd.read_csv('../input/macro.csv')\n\ndf_total = pd.merge(total, macro, on='timestamp', how='left')\ndf_total.drop('id', axis = 1, inplace = True)\ndf_total['price_doc'] = np.log(df_total['price_doc'])\nYtotal = df_total['price_doc']\n\ndf_test = pd.merge(test, macro, on='timestamp', how='left')\ndf_test.drop('id', axis = 1, inplace = True)\ndf_all = pd.concat([df_total,df_test], keys = ['total','test'])\n\nprint ('total: ', df_total.shape)\nprint ('test: ', df_test.shape)\nprint ('macro: ', macro.shape)\nprint ('all: ', df_all.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d89f29fc-3479-5119-14c3-40688749d75a"},"outputs":[],"source":"def missingPattern(df):\n    numGroup = list(df._get_numeric_data().columns)\n    catGroup = list(set(df.columns) - set(numGroup))\n    print ('Total categorical/numerical variables are %s/%s' % (len(catGroup), len(numGroup)))\n    \n    #missing data\n    n = df.shape[0]\n    count = df.isnull().sum()\n    percent = 1.0 * count / n\n    dtype = df.dtypes\n    # correlation\n    missing_data = pd.concat([count, percent,dtype], axis=1, keys=['Count', 'Percent', 'Type'])\n    missing_data.sort_values('Count', ascending = False, inplace = True)\n    missing_data = missing_data[missing_data['Count'] > 0]\n    print ('Total missing columns is %s' % len(missing_data))\n\n    return numGroup, catGroup, missing_data\n\nnumGroup, catGroup, missing_data = missingPattern(df_all)\n# missing_data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"be39d4e6-8ba9-8e58-1b29-e06cecf0724a"},"outputs":[],"source":"import operator\ndef getCorr(df, numGroup, eps, *verbose):\n    corr = df[numGroup].corr()\n#     plt.figure(figsize=(8, 6))\n#     plt.pcolor(corr, cmap=plt.cm.Blues)\n#     plt.show()\n    corr.sort_values([\"price_doc\"], ascending = False, inplace = True)\n    highCorrList = list(corr.price_doc[abs(corr.price_doc)>eps].index)\n    if verbose:\n        print (\"Find most important features relative to target\")\n        print (corr.price_doc[abs(corr.price_doc)>eps])\n    return corr, highCorrList\ncorr, highCorrList = getCorr(df_all.ix['total',:], numGroup, 0.4, True)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8e74d0b4-7138-3b9a-7ed4-35ed528dc4c6"},"outputs":[],"source":"# for numerical variable, draw scatter plot(x vs y) and histogram plot(total vs test)    \ndef scatterplotNum(df, varNum, ax):\n    plt.scatter(df[varNum], df['price_doc'])\n    plt.xlabel(varNum)\n    plt.ylabel('Price_doc')\n\ndef hishplotNum(df, varNum, ax):\n    plt.hist(df.ix['total',varNum], bins = 50, alpha = 0.4)\n    plt.hist(df.ix['test',varNum], bins = 50, color = 'r', alpha = 0.4)\n    plt.xlabel(varNum)\n    plt.ylabel('Frequency')\n    plt.legend(('total','test'))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57d9e3be-25f5-3a61-eb4d-b11794cb248a"},"outputs":[],"source":"high_missing_data = missing_data[missing_data['Percent'] > 0.5]\n# print high_missing_data.index\nXYcorr = corr['price_doc'].to_dict()\n\nfor i in XYcorr:\n    if i != 'price_doc' and XYcorr[i] > -1 and i in high_missing_data.index:\n        fig = plt.figure(i)\n        ax1 = fig.add_subplot(1,1,1)\n        scatterplotNum(df_all.ix['total'], i, ax1)\n        plt.title('correlation is %.4f' %(XYcorr[i]))\n\n#         ax2 = fig.add_subplot(1,2,2)\n#         hishplotNum(pd.concat([total,test],keys = ['total','test']), i, ax2)\n#         plt.title('correlation is %.4f' %(XYcorr[i]))\n        plt.gcf().set_size_inches(6, 4)\n        plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b7a2b7ae-c79f-5f59-17cf-e2f8470cfc3c"},"outputs":[],"source":"# remove the heavy missing features\nfor i in high_missing_data.index:\n    df_all.drop(i, axis = 1, inplace = True)\n\nprint ('all: ', df_all.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5c11f91a-8cab-3f60-5f43-4815c8fc3467"},"outputs":[],"source":"# total missing\n# macro missing\nbasic_missing = list((set(missing_data.index) - set(high_missing_data.index)) & set(total.columns))\nmacro_missing = list((set(missing_data.index) - set(high_missing_data.index)) & set(macro.columns))\nprint ('missing in basic: ', len(basic_missing))\nprint ('missing in macro: ', len(macro_missing))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"045bdd17-e47a-4a24-6671-78c7fad5fa4f"},"outputs":[],"source":"### for macro info, look at the missing value info(mean, std) groupby yr, year_month\ndf_all['timestamp'] = pd.to_datetime(df_all['timestamp'])\ndf_all['year'] = df_all.timestamp.dt.year\ndf_all['year_month'] = df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d3bd204-1f83-baa7-6f6a-a109828c251c"},"outputs":[],"source":"# life_sq and full_sq are highly related to price_doc\n# life_sq <= full_sq and full_sq has no missing value\n\n# life_sq or full_sq <= 5\ndf_all['life_sq'][df_all['life_sq']<=5] = df_all['full_sq'][df_all['life_sq']<=5]\ndf_all['full_sq'][df_all['full_sq']<=5] = df_all['life_sq'][df_all['full_sq']<=5]\n\n\n# # life_sq or full_sq > 200 \ndf_all['life_sq'].ix['total'][1084] = 28.1\ndf_all['life_sq'].ix['total'][4385] = 42.6\ndf_all['life_sq'].ix['total'][9237] = 30.1\ndf_all['life_sq'].ix['total'][9256] = 45.8\ndf_all['life_sq'].ix['total'][9646] = 80.2\ndf_all['life_sq'].ix['total'][13546] = 74.78\ndf_all['life_sq'].ix['total'][13629] = 25.9\ndf_all['life_sq'].ix['total'][21080] = 34.9\ndf_all['life_sq'].ix['total'][26342] = 43.5\n\ndf_all['life_sq'].ix['test'][601] = 74.2\ndf_all['life_sq'].ix['test'][1896] = 36.1\ndf_all['life_sq'].ix['test'][2031] = 23.7\ndf_all['life_sq'].ix['test'][2791] = 86.9\ndf_all['life_sq'].ix['test'][5187] = 28.3\n\ndf_all['full_sq'].ix['total'][1478] = 35.3\ndf_all['full_sq'].ix['total'][1610] = 39.4\ndf_all['full_sq'].ix['total'][2425] = 41.2\ndf_all['full_sq'].ix['total'][2780] = 72.9\ndf_all['full_sq'].ix['total'][3527] = 53.3\ndf_all['full_sq'].ix['total'][5944] = 63.4\ndf_all['full_sq'].ix['total'][7207] = 46.1\n\n\n# life_sq > full_sq\ndf_all['life_sq'][df_all.life_sq > df_all.full_sq] = df_all['full_sq'][df_all.life_sq > df_all.full_sq]\n\n# kitch_sq > full_sq\n\ndf_all['kitch_sq'][df_all.kitch_sq > df_all.full_sq] = \\\n            df_all['full_sq'][df_all.kitch_sq > df_all.full_sq] - df_all['life_sq'][df_all.kitch_sq > df_all.full_sq]\n\n\n# else\n# floor > max_floor\ndf_all['max_floor'][df_all.floor > df_all.max_floor] = \\\n        df_all['floor'][df_all.floor > df_all.max_floor] + df_all['max_floor'][df_all.floor > df_all.max_floor]\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"628ed064-4ac8-c116-c1aa-20e4647d010e"},"outputs":[],"source":"# fill the missing value in train and test\ndef basicmissingFill(df):\n    # num variables\n    # pre-processing\n    n = df.shape[0]\n    \n    \n    df_all['life_sq'][df_all.life_sq.isnull()] = df_all['full_sq'][df_all.life_sq.isnull()]\n\n\n    df['state'] = df['state'].replace({33:3})\n    df['build_year'][df['build_year'] == 20052009] = 2005\n    df['build_year'][df['build_year'] == 4965] = float('nan')\n    df['build_year'][df['build_year'] == 0] = float('nan')\n    df['build_year'][df['build_year'] == 1] = float('nan')\n    df['build_year'][df['build_year'] == 3] = float('nan')\n    df['build_year'][df['build_year'] == 71] = float('nan')\n    df['build_year'][df['build_year'] == 20] = 2000\n    df['build_year'][df['build_year'] == 215] = 2015\n    df['build_year'].ix['total'][13117] = 1970\n\n\n    \n    # zero-filling count feature \n    zero_fil = ['build_count_brick','build_count_block','build_count_mix','build_count_before_1920',\\\n               'build_count_1921-1945','build_count_1946-1970','build_count_1971-1995','build_count_after_1995',\\\n               'build_count_monolith','build_count_slag','build_count_wood','build_count_panel','build_count_frame',\\\n               'build_count_foam','preschool_quota']\n    for i in zero_fil:\n        df[i] = df[i].fillna(0)\n    \n    # mode-filling: count feature and ID\n    mode_fil = ['state','ID_railroad_station_walk','build_year','material','num_room']\n    for i in mode_fil:\n        df[i] = df[i].fillna(df[i].mode()[0]) \n\n    # mean-filling\n    mean_fil = ['cafe_avg_price_500','cafe_avg_price_1000','cafe_avg_price_1500','cafe_avg_price_2000',\\\n               'cafe_avg_price_3000','cafe_avg_price_5000','cafe_sum_500_max_price_avg','cafe_sum_500_min_price_avg',\\\n               'cafe_sum_1000_max_price_avg','cafe_sum_1000_min_price_avg','cafe_sum_1500_max_price_avg',\\\n               'cafe_sum_1500_min_price_avg','cafe_sum_2000_max_price_avg','cafe_sum_2000_min_price_avg',\\\n               'cafe_sum_3000_max_price_avg','cafe_sum_3000_min_price_avg','cafe_sum_5000_max_price_avg',\\\n               'cafe_sum_5000_min_price_avg','railroad_station_walk_min','railroad_station_walk_km',\\\n               'school_quota','raion_build_count_with_material_info','prom_part_5000',\\\n               'raion_build_count_with_builddate_info','green_part_2000','metro_km_walk','metro_min_walk',\\\n               'hospital_beds_raion']\n    for i in mean_fil:\n        grouped = df[['year',i]].groupby('year')\n        df[i] = grouped.transform(lambda x: x.fillna(x.mean()))\n        \n    # exception: 'kitch_sq','floor','max_floor'\n    df['kitch_sq'][df.kitch_sq.isnull()] = df['full_sq'][df.kitch_sq.isnull()] - df['life_sq'][df.kitch_sq.isnull()]\n    df['floor'] = df['floor'].fillna(df['floor'].mean())\n    df['max_floor'][df.max_floor.isnull()] = df['floor'][df.max_floor.isnull()]\n    \n    #================\n    # Cat. variables\n    df['product_type'] = df['product_type'].fillna(df['product_type'].mode()[0])\n    \n    return df\n\ndf_all = basicmissingFill(df_all)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95f5ebfd-7d80-7c66-701f-1fd34435a9d5"},"outputs":[],"source":"print ('basic_missing filling finished: ', df_all[basic_missing].isnull().sum().sum() == 7662)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"30e2c947-cccc-bf0c-52c2-95534a1650da"},"outputs":[],"source":"# for Cat features in macro_missing\nmacro_missing_obj = []\nfor i in macro_missing:\n    if df_all[i].dtype == object:\n        grouped = df_all[['year',i]].groupby(['year',i])\n        # print grouped.agg(len)\n        macro_missing_obj.append(i)\n        # print missing_data.ix[i]\n        # print '\\n'\n# consider to drop macro_missing_obj\nfor i in macro_missing_obj:\n    df_all.drop(i, axis = 1, inplace = True)\n    macro_missing.remove(i)\n\nprint ('macro missing features count: ', len(macro_missing))\nprint ('df_all shape: ', df_all.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b34708b-7c94-92ec-4038-a978196fbc15"},"outputs":[],"source":"# for num features in macro_missing\n# filling strategy: for each feature->if 2015 is not null: fillna the mean(2015) else: fillna the mean(2014)\ndef macromissingFill(df):\n    for i in macro_missing:\n        fill2014 = np.nanmean(df[i][df['year']==2014])\n        fill2015 = np.nanmean(df[i][df['year']==2015])\n        # income_per_cap: the only macro_missing feature which is not agg by year\n        if ~np.isnan(fill2015):\n            df[i] = df[i].fillna(fill2015)\n        else:\n            df[i] = df[i].fillna(fill2014)\n\n    return df\n\ndf_all = macromissingFill(df_all)\nprint ('macro_missing filling finished: ', df_all[macro_missing].isnull().sum().sum() == 0)\n\n    "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d61119e5-29f9-c1f6-df25-67a08b3eba84"},"outputs":[],"source":"# running mean price vs timestamp\n# len(df_all.timestamp.unique()) -> total 1435 timestamp\n\ndef running_mean(df, x, y, agg_list, k, *condition): \n    if condition:\n        if condition[0] == 'Xless':\n            grouped = df[[x,y]][df[x] <= condition[1]].ix['total'].groupby(x)\n        elif condition[0] == 'Xlarger':\n            grouped = df[[x,y]][df[x] >= condition[1]].ix['total'].groupby(x)\n        elif condition[0] == 'Yless':\n            grouped = df[[x,y]][df[y] <= condition[1]].ix['total'].groupby(x)\n        elif condition[0] == 'Ylarger':\n            grouped = df[[x,y]][df[y] >= condition[1]].ix['total'].groupby(x)  \n        else: \n            assert 'Wrong conditions!'\n    else:\n        grouped = df[[x, y]].ix['total'].groupby(x)\n    agg_y = grouped.agg({y: agg_list})[y]\n\n    m, n = agg_y.shape\n    px = list(agg_y.index)[:m+1-k]\n    py = []\n    for i in range(n):\n        temp = []\n        for j in range(m+1-k):\n            temp.append(np.mean(agg_y.iloc[j:j+k,i]))\n        py.append(temp)\n\n    # plot\n    colors = ['r','g','b','y']\n    plt.figure()\n    for i in range(len(agg_list)):\n        if k == 1:\n            plt.scatter(px, py[i], color = colors[i])\n        else:\n            plt.plot(px, py[i], color = colors[i])\n    \n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.legend(agg_list)\n    plt.title('running mean: ' + str(k))\n    plt.gcf().set_size_inches(10, 6)\n    plt.show()        \n    return px, py\n\nx = 'timestamp'\ny = 'price_doc'\nagg_list = ['mean','median']\nk = 60\npx, py = running_mean(df_all, x, y, agg_list, k)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d09b9291-699c-1a8c-76cb-e90b8ac2d8a4"},"outputs":[],"source":"# year and housing sq\ndf_all['used_yr'] = df_all['year'] - df_all['build_year']\ndf_all['used_yr'][df_all['used_yr'] < 0] = 0\ndf_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\ndf_all['rel_life_sq'] = df_all['life_sq'] / df_all['full_sq'].astype(float)\ndf_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n# fillna\ndf_all['rel_floor'] = df_all['rel_floor'].fillna(df_all['rel_floor'].mean())\ndf_all['rel_life_sq'] = df_all['rel_life_sq'].fillna(df_all['rel_life_sq'].mean())\ndf_all['rel_kitch_sq'] = df_all['rel_kitch_sq'].fillna(df_all['rel_kitch_sq'].mean())\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"06b2b5d2-9682-183a-05bf-191024703741"},"outputs":[],"source":"numGroup = list(df_all._get_numeric_data().columns)\ncorr, highCorrList = getCorr(df_all.ix['total',:], numGroup, 0.15)\n\ncntGroup = [i for i in numGroup if re.match(r'\\w+_count+',i)]\nraionGroup = [i for i in numGroup if re.match(r'\\w+_raion+',i)]\nkmGroup = [i for i in numGroup if re.match(r'\\w+_km+',i)]\nminGroup = ['metro_min_avto', 'metro_min_walk', 'public_transport_station_min_walk', 'railroad_station_avto_min',\\\n            'railroad_station_walk_min']\nprint ('cntGroup:', len(cntGroup))\nprint ('raionGroup: ', len(raionGroup))\nprint ('kmGroup: ', len(kmGroup))\nprint ('minGroup: ', len(minGroup))\n\n# for i in kmGroup+minGroup+cntGroup:\n#     if sum(df_all[i] < 0) > 0:\n#         print i\n#     else:\n#         print 'positive'\n# all positive values in kmGroup, minGroup, cntGroup "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"96b50f68-bc97-0408-5af2-35573d3ce705"},"outputs":[],"source":"# km and minutes to neighborhood feats transformation\n# house full_sq and distance to neighborhood combination\n\ndef min_km_trans(df, group):\n    group = list(group)\n    newFeats = []\n    for i in group:\n        df['fsq_'+i+'_inv1'] = df['full_sq'] / (df[i] + 0.1)\n        df['fsq_'+i+'_inv5'] = df['full_sq'] / (df[i] + 0.5)\n        df['fsq_'+i+'_inv10'] = df['full_sq'] / (df[i] + 1.0)\n        df['fsq_'+i+'_invlg1'] = df['full_sq'] / (np.log1p(df[i]) + 0.1)\n        df['fsq_'+i+'_invlg5'] = df['full_sq'] / (np.log1p(df[i]) + 0.5)\n        df['fsq_'+i+'_invlg10'] = df['full_sq'] / (np.log1p(df[i]) + 1.0)\n        # df['full_sq_'+i] = df['full_sq'] / (np.log1p(df[i]) + 0.1)\n        newFeats += ['fsq_'+i+'_inv1','fsq_'+i+'_inv5','fsq_'+i+'_inv10','fsq_'+i+'_invlg1',\\\n                     'fsq_'+i+'_invlg5','fsq_'+i+'_invlg10']\n    group.extend(newFeats)\n    return df, group\n\ndf_all, kmFeats = min_km_trans(df_all, kmGroup)\ndf_all, minFeats = min_km_trans(df_all, minGroup)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"457074e8-a388-7468-2f41-110bab410b4c"},"outputs":[],"source":"cntFeats = list(set(cntGroup) & set(highCorrList))\nextFeats = ['full_sq','life_sq','kitch_sq','floor','max_floor','num_room','build_year',\\\n            'used_yr','rel_life_sq','rel_kitch_sq','rel_floor',\\\n           'ppi','cpi','price_doc']\nprint (len(highCorrList),len(cntFeats),len(kmFeats)/7,len(minFeats)/7,len(extFeats))\n# print highCorrList\nnumFeats = cntFeats + kmFeats + minFeats + extFeats"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61c8c070-43e6-31ac-2156-c09b2a2eaaf5"},"outputs":[],"source":"testId = list(test['id'])\ndrop_list = ['timestamp','year','year_month','price_doc']\nfor i in drop_list:\n    df_all.drop(i, axis = 1, inplace = True)\n\nnumGroup,catGroup,_ = missingPattern(df_all)\n\n# self-define numGroup\n# numGroup = numFeats\n\ndf_total_num = df_all.ix['total',numGroup]\ndf_test_num = df_all.ix['test',numGroup]\ndf_total_cat = df_all.ix['total',catGroup]\ndf_test_cat = df_all.ix['test',catGroup]\nprint ('Current training numerical variables count is %d '  %(df_total_num.shape[1]))\nprint ('Current training categorical variables count is %d '  %(df_total_cat.shape[1]))\nprint ('Current test numerical variables count is %d '  %(df_test_num.shape[1]))\nprint ('Current test categorical variables count is %d '  %(df_test_cat.shape[1]))\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d35aa35-b9bc-98d4-c91c-543fc47476ed"},"outputs":[],"source":"# one-hot encoding for categorical variables\ndf_concat_cat = pd.concat([df_total_cat,df_test_cat],keys = ['total','test'])\ndf_total_cat = pd.get_dummies(df_concat_cat).ix['total',:]\ndf_test_cat = pd.get_dummies(df_concat_cat).ix['test',:]\nprint ('After one-hot encoding, total training cat variables are %d' %(df_total_cat.shape[1]))\nprint ('After one-hot encoding, total test cat variables are %d' %(df_test_cat.shape[1]))\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f690a8df-c73e-8150-2eca-67e3a689145a"},"outputs":[],"source":"#Xtotal = pd.concat([df_total_num,df_total_cat], axis = 1)\n#Xtest = pd.concat([df_test_num,df_test_cat], axis = 1)\nXtotal = df_total_num\nXtest = df_test_num\ndtrain = xgb.DMatrix(Xtotal, Ytotal)\ndtest = xgb.DMatrix(Xtest)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"443fc813-9907-b334-745a-05b658786daf"},"source":"### Modeling"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89747693-764c-cd5a-c64d-ef5b62bde3c0"},"outputs":[],"source":"xgb_params = {\n    'eta': 0.05,\n    'max_depth': 3,\n    'subsample': 0.8,\n    'colsample_bytree': 0.7,\n    'lambda': 0.2,\n    'alpha': 0.2,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1,\n    'scale_pos_weight': 1,\n}\ncv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=50,\n    verbose_eval=50, show_stdv=False)\ncv_output[['train-rmse-mean', 'test-rmse-mean']].plot()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3d9bacab-2a91-70b1-c9b7-416ace5224df"},"outputs":[],"source":"num_boost_rounds = len(cv_output)\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c81699ed-0437-6f2f-0554-4d8ad4af8135"},"outputs":[],"source":"fig, ax = plt.subplots(1, 1, figsize=(8, 12))\nxgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9837d559-a897-0c38-6a31-705b4324c675"},"source":"### Prediction on test"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09205e39-8545-9a7a-0095-eeb03aa4fc74"},"outputs":[],"source":"Ypred = model.predict(dtest)\nYpred = np.expm1(Ypred)\noutput = pd.DataFrame({\"id\": testId, \"price_doc\": Ypred})\noutput.head()\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c1d8487e-cefa-bb1b-77ae-bab241b2f851"},"outputs":[],"source":"output.to_csv('xgb_submission.csv',index=False)\n\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}