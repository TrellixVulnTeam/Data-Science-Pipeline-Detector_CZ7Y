{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7adbe232-6352-3ec1-4955-2b74f8a183bc"},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8bb3e4b7-cc38-f5ef-a767-8018c52e6297"},"outputs":[],"source":"df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\ndf_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\ndf_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"198b4ae5-6974-8012-f84e-b2067ee34a82"},"outputs":[],"source":"df_train.sample(3)\n\nsample = df_train.sample(frac=0.5)\n\nprice_per_sq = sample.price_doc / sample['full_sq']\nprice_per_sq = price_per_sq[ np.isinf(price_per_sq) == False ].mean()\n# create the price_by_sq parameters by using the train data, cross validation will do the same with inner train and inner validation\ndf_train['price_by_sq'] =df_train['full_sq'] * price_per_sq\ndf_test['price_by_sq'] = df_test['full_sq'] * price_per_sq\ny_train = df_train['price_doc'].values\nid_test = df_test['id']\n\ndf_train.drop(['id', 'price_doc'], axis=1, inplace=True)\ndf_test.drop(['id'], axis=1, inplace=True)\n\n# Build df_all = (df_train+df_test).join(df_macro)\nnum_train = len(df_train)\ndf_all = pd.concat([df_train, df_test])\ndf_all = df_all.join(df_macro, on='timestamp', rsuffix='_macro')\nprint(df_all.shape)\nmultiplier = 0.960\n\n#clean data\nbad_index = df_all[df_all.life_sq > df_all.full_sq].index\ndf_all.ix[bad_index, \"life_sq\"] = np.NaN\nequal_index = [601,1896,2791]\nbad_index = df_all[df_all.life_sq < 5].index\ndf_all.ix[bad_index, \"life_sq\"] = np.NaN\nbad_index = df_all[df_all.full_sq < 5].index\ndf_all.ix[bad_index, \"full_sq\"] = np.NaN\nkitch_is_build_year = [13117]\ndf_all.ix[kitch_is_build_year, \"build_year\"] = df_all.ix[kitch_is_build_year, \"kitch_sq\"]\nbad_index = df_all[df_all.kitch_sq >= df_all.life_sq].index\ndf_all.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index =df_all[(df_all.kitch_sq == 0).values + (df_all.kitch_sq == 1).values].index\ndf_all.ix[bad_index, \"kitch_sq\"] = np.NaN\nbad_index = df_all[(df_all.full_sq > 210) & (df_all.life_sq / df_all.full_sq < 0.3)].index\ndf_all.ix[bad_index, \"full_sq\"] = np.NaN\nbad_index = df_all[df_all.life_sq > 300].index\ndf_all.ix[bad_index, [\"life_sq\", \"full_sq\"]] = np.NaN\ndf_all.product_type.value_counts(normalize= True)\nbad_index = df_all[df_all.build_year < 1500].index\ndf_all.ix[bad_index, \"build_year\"] = np.NaN\nbad_index = df_all[df_all.num_room == 0].index \ndf_all.ix[bad_index, \"num_room\"] = np.NaN\nbad_index = [10076, 11621, 17764, 19390, 24007, 26713, 29172,3174, 7313]\ndf_all.ix[bad_index, \"num_room\"] = np.NaN\n#bad_index = [3174, 7313]\nbad_index =df_all[(df_all.floor == 0).values * (df_all.max_floor == 0).values].index\ndf_all.ix[bad_index, [\"max_floor\", \"floor\"]] = np.NaN\nbad_index = df_all[df_all.floor == 0].index\ndf_all.ix[bad_index, \"floor\"] = np.NaN\nbad_index = df_all[df_all.max_floor == 0].index\ndf_all.ix[bad_index, \"max_floor\"] = np.NaN\nbad_index = df_all[df_all.floor > df_all.max_floor].index\ndf_all.ix[bad_index, \"max_floor\"] = np.NaN\ndf_all.floor.describe(percentiles= [0.9999])\nbad_index = [23584]\ndf_all.ix[bad_index, \"floor\"] = np.NaN\ndf_all.material.value_counts()\ndf_all.state.value_counts()\nbad_index = df_all[df_all.state == 33].index\ndf_all.ix[bad_index, \"state\"] = np.NaN\n# Add month-year\nmonth_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\nmonth_year_cnt_map = month_year.value_counts().to_dict()\ndf_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n\n# Add week-year count\nweek_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\nweek_year_cnt_map = week_year.value_counts().to_dict()\ndf_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n\n# Add month and day-of-week\ndf_all['month'] = df_all.timestamp.dt.month\ndf_all['dow'] = df_all.timestamp.dt.dayofweek\n\n# Other feature engineering\ndf_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\ndf_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n\n# Add apartment id (as suggested in https://www.kaggle.com/c/sberbank-russian-housing-market/discussion/33269)\n# and replace it with its count and count by month\ndf_all['apartment_name'] = pd.factorize(df_all.sub_area + df_all['metro_km_avto'].astype(str))[0]\ndf_all['apartment_name_month_year'] = pd.factorize(df_all['apartment_name'].astype(str) + month_year.astype(str))[0]\n\ndf_all['apartment_name_cnt'] = df_all['apartment_name'].map(df_all['apartment_name'].value_counts())\ndf_all['apartment_name_month_year_cnt'] = df_all['apartment_name_month_year'].map(df_all['apartment_name_month_year'].value_counts())\n\n# Remove timestamp column (may overfit the model in train)\ndf_all.drop(['timestamp', 'timestamp_macro'], axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5587747f-e71a-4125-3c91-b80ebec81872"},"outputs":[],"source":"factorize = lambda t: pd.factorize(t[1])[0]\n\ndf_obj = df_all.select_dtypes(include=['object'])\n\nX_all = np.c_[\n    df_all.select_dtypes(exclude=['object']).values,\n    np.array(list(map(factorize, df_obj.iteritems()))).T\n]\nprint(X_all.shape)\n\nX_train = X_all[:num_train]\nX_test = X_all[num_train:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19ded44b-38e7-e3e1-bc6a-e2cadf80131a"},"outputs":[],"source":"# Deal with categorical values\ndf_numeric = df_all.select_dtypes(exclude=['object'])\ndf_obj = df_all.select_dtypes(include=['object']).copy()\n\nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n\ndf_values = pd.concat([df_numeric, df_obj], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"550c306b-a6a5-9c89-fe73-16796a7d9c22"},"outputs":[],"source":"# Convert to numpy values\nX_all = df_values.values\nprint(X_all.shape)\n\nX_train = X_all[:num_train]\nX_test = X_all[num_train:]\n\ndf_columns = df_values.columns"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"37cfa80a-1c83-58e9-c069-3708520bbf00"},"outputs":[],"source":"xgb_params = {\n    'eta': 0.05,\n    'max_depth': 5,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(X_train, y_train, feature_names=df_columns)\ndtest = xgb.DMatrix(X_test, feature_names=df_columns)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3be5ad13-630c-f47e-c97f-0d8cd0a22097"},"outputs":[],"source":"# Uncomment to tune XGB `num_boost_rounds`\n\n#cv_result = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n#    verbose_eval=10, show_stdv=False)\n#cv_result[['train-rmse-mean', 'test-rmse-mean']].plot()\n#num_boost_rounds = len(cv_result)\n#print(\"num_boost_rounds:\", num_boost_rounds)\n\nnum_boost_round = 489"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"56ec63a2-ebfb-b2ef-4bc1-a8f4a3286852"},"outputs":[],"source":"model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76c7e814-ffdd-afa8-19aa-585df067d48b"},"outputs":[],"source":"y_pred = model.predict(dtest)\ndf_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\ndf_sub.to_csv('sub_modified.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}