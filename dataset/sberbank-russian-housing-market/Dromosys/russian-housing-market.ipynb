{"cells":[{"metadata":{},"cell_type":"markdown","source":"https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa426702b622484ac8c75673221db29d5aca880d"},"cell_type":"code","source":"!git clone https://github.com/LenzDu/Kaggle-Competition-Sberbank Utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c01a0c24522cd19fb5def755f076a999ee6a089"},"cell_type":"code","source":"import sys\n # Add directory holding utility functions to path to allow importing utility funcitons\n#sys.path.insert(0, '/kaggle/working/protein-atlas-fastai')\nsys.path.append('/kaggle/working/Utils')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab62aa720e25703dc5d4a6845ba89d22e027ed93"},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed May 17 16:36:14 2017\n@author: vrtjso\n\"\"\"\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, date\nfrom operator import le, eq\nfrom Utils import sample_vals, FeatureCombination\nimport gc\nfrom sklearn import model_selection, preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\n\n####Data Cleaning####\nprint('Data Cleaning...')\n\n#Data importing\ntrainDf = pd.read_csv('../input/train.csv').set_index('id')\ntestDf = pd.read_csv('../input/test.csv').set_index('id')\nfix = pd.read_excel('../input/BAD_ADDRESS_FIX.xlsx').set_index('id')\ntestDf['isTrain'] = 0\ntrainDf['isTrain'] = 1\nallDf = pd.concat([trainDf,testDf])\nallDf.update(fix, filter_func = lambda x:np.array([True]*x.shape[0])) #update fix data\nmacro = pd.read_csv('../input/macro.csv')\n\n#Join division and macro\n\n\n### Change price by rate ###\nallDf['timestamp'] = pd.to_datetime(allDf['timestamp'])\n\nallDf['apartment_name'] = allDf.sub_area + allDf['metro_km_avto'].astype(str)\neco_map = {'excellent':4, 'good':3, 'satisfactory':2, 'poor':1, 'no data':0}\nallDf['ecology'] = allDf['ecology'].map(eco_map)\n#encode subarea in order\n# price_by_area = allDf['price_doc'].groupby(allDf.sub_area).mean().sort_values()\n# area_dict = {}\n# for i in range(0,price_by_area.shape[0]):\n#    area_dict[price_by_area.index[i]] = i\n# allDf['sub_area'] = allDf['sub_area'].map(area_dict)\nfor c in allDf.columns:\n    if allDf[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(allDf[c].values))\n        allDf[c] = lbl.transform(list(allDf[c].values))\n\n\n###Dealing with Outlier###\nallDf.loc[allDf.full_sq>2000,'full_sq'] = np.nan\nallDf.loc[allDf.full_sq<3,'full_sq'] = np.nan\nallDf.loc[allDf.life_sq>500,'life_sq'] = np.nan\nallDf.loc[allDf.life_sq<3,'life_sq'] = np.nan\n# allDf['lifesq_to_fullsq'] = 0 # 0 for normal, 1 for close,2 for outlier\nallDf.loc[allDf.life_sq>0.8*allDf.full_sq,'life_sq'] = np.nan\n# allDf.ix[allDf.life_sq>allDf.full_sq,['life_sq','lifesq_to_fullsq']] = np.nan, 2\nallDf.loc[allDf.kitch_sq>=allDf.life_sq,'kitch_sq'] = np.nan\nallDf.loc[allDf.kitch_sq>500,'kitch_sq'] = np.nan\nallDf.loc[allDf.kitch_sq<2,'kitch_sq'] = np.nan\nallDf.loc[allDf.state>30,'state'] = np.nan\nallDf.loc[allDf.build_year<1800,'build_year'] = np.nan\nallDf.loc[allDf.build_year==20052009,'build_year'] = 2005\nallDf.loc[allDf.build_year==4965,'build_year'] = np.nan\nallDf.loc[allDf.build_year>2021,'build_year'] = np.nan\nallDf.loc[allDf.num_room>15,'num_room'] = np.nan\nallDf.loc[allDf.num_room==0,'num_room'] = np.nan\nallDf.loc[allDf.floor==0,'floor'] = np.nan\nallDf.loc[allDf.max_floor==0,'max_floor'] = np.nan\nallDf.loc[allDf.floor>allDf.max_floor,'max_floor'] = np.nan\n#allDf.ix[allDf.full_sq>300,'full_sq'] = np.nan\n#allDf.ix[allDf.life_sq>250,'life_sq'] = np.nan\n\n# brings error down a lot by removing extreme price per sqm\nbad_index = allDf[allDf.price_doc/allDf.full_sq > 600000].index\nbad_index = bad_index.append(allDf[allDf.price_doc/allDf.full_sq < 10000].index)\nallDf.drop(bad_index,0,inplace=True)\n\n####Feature Engineering####\nprint('Feature Engineering...')\ngc.collect()\n\n# allDf['month'] = np.array(month)\nallDf['year'] = allDf.timestamp.dt.year  #may be no use because test data is out of range\nallDf['weekday'] = allDf.timestamp.dt.weekday\n\n#allDf['week_of_year'] = np.array(week_of_year)\n##allDf['year_month'] = np.array(year_month)\n\n#w_map = {2011:0.8, 2012:0.8, 2013:0.9, 2014:1, 2015:1, 2016:0}\n#allDf['w'] = [w_map[i] for i in year]\n\n# Assign weight\nallDf['w'] = 1\nallDf.loc[allDf.price_doc==1000000,'w'] *= 0.5\nallDf.loc[allDf.year==2015,'w'] *= 1.5\n\n#Floor\nallDf['floor_by_max_floor'] = allDf.floor / allDf.max_floor\n#allDf['floor_to_top'] = allDf.max_floor - allDf.floor\n\n#Room\nallDf['avg_room_size'] = (allDf.life_sq - allDf.kitch_sq) / allDf.num_room\nallDf['life_sq_prop'] = allDf.life_sq / allDf.full_sq\nallDf['kitch_sq_prop'] = allDf.kitch_sq / allDf.full_sq\n\n#Calculate age of building\nallDf['build_age'] = allDf.year - allDf.build_year\nallDf = allDf.drop('build_year', 1)\n\n#Population\nallDf['popu_den'] = allDf.raion_popul / allDf.area_m\nallDf['gender_rate'] = allDf.male_f / allDf.female_f\nallDf['working_rate'] = allDf.work_all / allDf.full_all\n\n#Education\nallDf.loc[allDf.preschool_quota==0,'preschool_quota'] = np.nan\nallDf['preschool_ratio'] =  allDf.children_preschool / allDf.preschool_quota\nallDf['school_ratio'] = allDf.children_school / allDf.school_quota\n\n## Group statistics\nallDf['square_full_sq'] = (allDf.full_sq - allDf.full_sq.mean()) ** 2\nallDf['square_build_age'] = (allDf.build_age - allDf.build_age.mean()) ** 2\nallDf['nan_count'] = allDf[['full_sq','build_age','life_sq','floor','max_floor','num_room']].isnull().sum(axis=1)\nallDf['full*maxfloor'] = allDf.max_floor * allDf.full_sq\nallDf['full*floor'] = allDf.floor * allDf.full_sq\n\nallDf['full/age'] = allDf.full_sq / (allDf.build_age + 0.5)\nallDf['age*state'] = allDf.build_age * allDf.state\n\n# new trial\nallDf['main_road_diff'] = allDf['big_road2_km'] - allDf['big_road1_km']\nallDf['rate_metro_km'] = allDf['metro_km_walk'] / allDf['ID_metro'].map(allDf.metro_km_walk.groupby(allDf.ID_metro).mean().to_dict())\nallDf['rate_road1_km'] = allDf['big_road1_km'] / allDf['ID_big_road1'].map(allDf.big_road1_km.groupby(allDf.ID_big_road1).mean().to_dict())\n# best on LB with weekday\n\nallDf['rate_road2_km'] = allDf['big_road2_km'] / allDf['ID_big_road2'].map(allDf.big_road2_km.groupby(allDf.ID_big_road2).mean().to_dict())\nallDf['rate_railroad_km'] = allDf['railroad_station_walk_km'] / allDf['ID_railroad_station_walk'].map(allDf.railroad_station_walk_km.groupby(allDf.ID_railroad_station_walk).mean().to_dict())\n# increase CV from 2.35 to 2.33 but lower LB a little bit (with month)\n\nallDf.drop(['year','timestamp'], 1, inplace = True)\n\n#Separate train and test again\ntrainDf = allDf[allDf.isTrain==1].drop(['isTrain'],1)\ntestDf = allDf[allDf.isTrain==0].drop(['isTrain','price_doc', 'w'],1)\n\noutputFile = 'train_featured.csv'\ntrainDf.to_csv(outputFile,index=False)\noutputFile = 'test_featured.csv'\ntestDf.to_csv(outputFile,index=False)\n\n# Xgboost handles nan itself\n'''\n### Dealing with NA ###\n#num_room, filled by linear regression of full_sq\nif filename == 'train_encoded.csv': #na in num_room only appear in training set\n    LR = LinearRegression()\n    X = allDf.full_sq[~(np.isnan(allDf.num_room) | np.isnan(allDf.full_sq))].values.reshape(-1, 1)\n    y = np.array(allDf.num_room[~(np.isnan(allDf.num_room) | np.isnan(allDf.full_sq))])\n    LR.fit(X,y)\n    newX = allDf.full_sq[np.isnan(allDf.num_room)].values.reshape(-1, 1)\n    newX[np.isnan(newX)] = newX[~np.isnan(newX)].mean() #Special cases (na in full_sq) in test data\n    yfit = LR.predict(newX)\n    allDf.ix[np.isnan(allDf.num_room),'num_room'] = yfit\n#max_floor, twice as the floor\nallDf.ix[np.isnan(allDf.max_floor),'max_floor'] = allDf.ix[np.isnan(allDf.max_floor),'floor'] * 2\n'''","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# author: vrtjso\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import ShuffleSplit, cross_val_score\nfrom sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94b0204d2d356ae8ae0097eb080569794752bfa8"},"cell_type":"code","source":"from sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca1a3c93e9e35cf3b910652debc4482c2b0e847d"},"cell_type":"code","source":"n_folds = 5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0806e366c1000cbe552bb40bf73540da69934942"},"cell_type":"code","source":"# 封装一下lightgbm让其可以在stacking里面被调用\nclass LGBregressor(object):\n    def __init__(self,params):\n        self.params = params\n\n    def fit(self, X, y, w):\n        y /= 10000000\n        # self.scaler = StandardScaler().fit(y)\n        # y = self.scaler.transform(y)\n        split = int(X.shape[0] * 0.8)\n        indices = np.random.permutation(X.shape[0])\n        train_id, test_id = indices[:split], indices[split:]\n        x_train, y_train, w_train, x_valid, y_valid,  w_valid = X[train_id], y[train_id], w[train_id], X[test_id], y[test_id], w[test_id],\n        d_train = lgb.Dataset(x_train, y_train, weight=w_train)\n        d_valid = lgb.Dataset(x_valid, y_valid, weight=w_valid)\n        partial_bst = lgb.train(self.params, d_train, 10000, valid_sets=d_valid, early_stopping_rounds=50, verbose_eval=500)\n        num_round = partial_bst.best_iteration\n        d_all = lgb.Dataset(X, label = y, weight=w)\n        self.bst = lgb.train(self.params, d_all, num_round)\n\n    def predict(self, X):\n        return self.bst.predict(X) * 10000000\n        # return self.scaler.inverse_transform(self.bst.predict(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b92660cf59431fc7e339d80c43a2a9d79ce04da5"},"cell_type":"code","source":"# 封装一下xgboost让其可以在stacking里面被调用\nclass XGBregressor(object):\n    def __init__(self, params):\n        self.params = params\n\n    def fit(self, X, y, w=None):\n        #if (w==None):\n        #    w = np.ones(X.shape[0])\n        split = int(X.shape[0] * 0.8)\n        indices = np.random.permutation(X.shape[0])\n        train_id, test_id = indices[:split], indices[split:]\n        x_train, y_train, w_train, x_valid, y_valid,  w_valid = X[train_id], y[train_id], w[train_id], X[test_id], y[test_id], w[test_id],\n        d_train = xgb.DMatrix(x_train, label=y_train, weight=w_train)\n        d_valid = xgb.DMatrix(x_valid, label=y_valid, weight=w_valid)\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        partial_bst = xgb.train(self.params, d_train, 10000, early_stopping_rounds=50, evals = watchlist, verbose_eval=500)\n        num_round = partial_bst.best_iteration\n        d_all = xgb.DMatrix(X, label = y, weight=w)\n        self.bst = xgb.train(self.params, d_all, num_round)\n\n    def predict(self, X):\n        test = xgb.DMatrix(X)\n        return self.bst.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"819cf484f7028c907b39b8392b7c33fd3e305a36"},"cell_type":"code","source":"# This object modified from Wille on https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/\nclass Ensemble(object):\n    def __init__(self, stacker, base_models):\n        self.stacker = stacker\n        self.base_models = base_models\n\n    def fit_predict(self, trainDf, testDf):\n        X = trainDf.drop(['price_doc', 'w'], 1).values\n        y = trainDf['price_doc'].values\n        w = trainDf['w'].values\n        T = testDf.values\n\n        X_fillna = trainDf.drop(['price_doc', 'w'], 1).fillna(-999).values\n        T_fillna = testDf.fillna(-999).values\n\n        kfold = KFold(n_splits=n_folds, shuffle=True)\n        S_train = np.zeros((X.shape[0], len(self.base_models)))\n        S_test = np.zeros((T.shape[0], len(self.base_models)))\n        \n        for i, clf in enumerate(self.base_models):\n            print('Training base model ' + str(i+1) + '...')\n            \n            S_test_i = np.zeros((T.shape[0], n_folds))\n            j=0\n            for train_idx, test_idx in kfold.split(X):\n                print('Training round...' + str(j+1) + '...')\n            #for j, (train_idx, test_idx) in enumerate(folds):\n                if clf not in [xgb1,lgb1]: # sklearn models cannot handle missing values.\n                    X = X_fillna\n                    T = T_fillna\n                X_train = X[train_idx]\n                y_train = y[train_idx]\n                w_train = w[train_idx]\n                X_holdout = X[test_idx]\n                # w_holdout = w[test_idx]\n                # y_holdout = y[test_idx]\n                clf.fit(X_train, y_train, w_train)\n                y_pred = clf.predict(X_holdout)\n                S_train[test_idx, i] = y_pred\n                S_test_i[:, j] = clf.predict(T)\n                j=j+1\n            S_test[:, i] = S_test_i.mean(1)\n        \n        #self.S_train, self.S_test, self.y = S_train, S_test, y  # for diagnosis purpose\n        self.corr = pd.concat([pd.DataFrame(S_train),trainDf['price_doc']],1).corr() # correlation of predictions by different models.\n        # cv_stack = ShuffleSplit(n_splits=6, test_size=0.2)\n        # score_stacking = cross_val_score(self.stacker, S_train, y, cv=cv_stack, n_jobs=1, scoring='neg_mean_squared_error')\n        # print(np.sqrt(-score_stacking.mean())) # CV result of stacking\n        print(S_train.shape)\n        print(y.shape)\n        \n        self.stacker.fit(S_train, y, w)\n        y_pred = self.stacker.predict(S_test)\n        return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba5ff2418987ba6ca5de14470030a8f12aa229ef"},"cell_type":"code","source":"trainDf = pd.read_csv('/kaggle/working/train_featured.csv')#.sample(frac=0.01)\ntestDf = pd.read_csv('/kaggle/working/test_featured.csv')#.sample(frac=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params1 = {'eta':0.05, 'max_depth':5, 'subsample':0.8, 'colsample_bytree':0.8, 'min_child_weight':1,\n          'gamma':0, 'silent':1, 'objective':'reg:linear', 'eval_metric':'rmse'}\nxgb1 = XGBregressor(params1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params2 = {'booster':'gblinear', 'alpha':0,# for gblinear, delete this line if change back to gbtree\n           'eta':0.1, 'max_depth':2, 'subsample':1, 'colsample_bytree':1, 'min_child_weight':1,\n          'gamma':0, 'silent':1, 'objective':'reg:linear', 'eval_metric':'rmse'}\nxgb2 = XGBregressor(params2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X = trainDf.drop(['price_doc', 'w'], 1).values\n#y = trainDf['price_doc'].values\n#w = trainDf['w'].values\n#X.shape\n#xgb2.fit(X, y, w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RF = RandomForestRegressor(n_estimators=500, max_features=0.2)\nETR = ExtraTreesRegressor(n_estimators=500, max_features=0.3, max_depth=None)\nAda = AdaBoostRegressor(DecisionTreeRegressor(max_depth=15),n_estimators=200)\nGBR = GradientBoostingRegressor(n_estimators=200,max_depth=5,max_features=0.5)\nLR =LinearRegression()\n\nparams_lgb = {'objective':'regression','metric':'rmse',\n          'learning_rate':0.05,'max_depth':-1,'sub_feature':0.7,'sub_row':1,\n          'num_leaves':15,'min_data':30,'max_bin':20,\n          'bagging_fraction':0.9,'bagging_freq':40,'verbosity':0}\nlgb1 = LGBregressor(params_lgb)\n\nE = Ensemble(xgb2, [xgb1,lgb1,RF,ETR,Ada,GBR])   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = E.fit_predict(trainDf, testDf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.read_csv('../input/test.csv')\noutput = output[['id']]\noutput['price_doc'] = prediction\noutput.to_csv(r'Submission_Stack.csv',index=False)\n\n# corr = pd.concat([pd.DataFrame(S_train),trainDf['price_doc']],1).corr() # extract correlation\n# 1: 2434 2: 2421","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cbfd4cc7a77523921a3713cd9f5ae8202e9b1d5"},"cell_type":"code","source":"!rm -rf /kaggle/working/Utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01e689fee825dac0ff8dafed2899aa6301c5350f"},"cell_type":"code","source":"#LB 0.38848 > 0.31448 > 0.31412","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}