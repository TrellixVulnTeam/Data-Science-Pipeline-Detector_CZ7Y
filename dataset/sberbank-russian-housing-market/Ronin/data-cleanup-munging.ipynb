{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"f5175694-1341-c418-79e3-d37f238c6619"},"source":"Data Cleanup--our Favorite Topic (NOT!).  Lots of missing data.  Early on we just ignored, but if you use a tool other than XGBoost or Vowpal Wabbit, those things need to be fit in.  Here's what I've been up to since I can 'beat the standandard'\n\n\nFirst lets put things in a big happy data frame:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b4894c7-ac05-5c86-2e3f-5c736e8e71ae"},"outputs":[],"source":"\nimport numpy as np\nimport pandas as pd\ntrain=pd.read_csv(\"../input/train.csv\")\ntest=pd.read_csv(\"../input/test.csv\")\nn=train.shape[0]\ntest.price_doc=np.nan\nids=test['id']\ntarget=train.price_doc\ntrain=train.append(test)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"ada253ab-f909-1e4c-02e7-66aaa253364d"},"source":"Fill in the floor values--rather than do it 'generally' I thought a better guess would be to take statistics from the local area.\n\nYes I probably should have wrote a function, but I just copied and pasted as a whittled down the missing data.  \n\nNotice the exception--that subarea didn't  have enough data, so I switched to median over 'mode'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3c83b31-0a5f-cc2b-f90e-069a58b53bdf"},"outputs":[],"source":"\n\na=train.sub_area.unique()\nfor i in a:\n    if train[(train['floor'].isnull()) & (train['sub_area']==i)].shape[0]>0:\n        train.loc[(train['floor'].isnull()) & (train['sub_area']==i),'floor']=train.loc[(train['floor'].notnull()) & (train['sub_area']==i),'floor'].mode().values[0]\nprint('floors done',train['product_type'].value_counts())\n#fill in nans for max_floor\nfor i in a:\n    if train[(train['max_floor'].isnull()) & (train['sub_area']==i)].shape[0]>0:\n        if i=='Poselenie Shhapovskoe':\n            train.loc[(train['max_floor'].isnull()) & (train['sub_area']==i),'max_floor']=train.loc[(train['max_floor'].notnull()) & (train['sub_area']==i),'max_floor'].median()\n        else:\n            train.loc[(train['max_floor'].isnull()) & (train['sub_area']==i),'max_floor']=train.loc[(train['max_floor'].notnull()) & (train['sub_area']==i),'max_floor'].mode().values[0]\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"d13d82bd-10da-740d-7119-5b7a6f890e48"},"source":"Do the same thing for materials, build year and 'state'  Again some areas were short on Data, So i put them in by hand"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76ba20c5-8680-d199-d319-2c9486fdc2bf"},"outputs":[],"source":"#materials\n\nfor i in a:\n    if train[(train['material'].isnull()) & (train['sub_area']==i)].shape[0]>0:\n        train.loc[(train['material'].isnull()) & (train['sub_area']==i),'material']=train.loc[(train['material'].notnull()) & (train['sub_area']==i),'material'].mode().values[0]\n\n#build year\nfor i in a:\n    if train[(train['build_year'].isnull()) & (train['sub_area']==i)].shape[0]>0:\n        if i=='Poselenie Voronovskoe':\n            train.loc[(train['build_year'].isnull()) & (train['sub_area']==i),'build_year']=2014\n        elif i=='Poselenie Shhapovskoe':\n            train.loc[(train['build_year'].isnull()) & (train['sub_area']==i),'build_year']=2011\n        else:\n            train.loc[(train['build_year'].isnull()) & (train['sub_area']==i),'build_year']=train.loc[(train['build_year'].notnull()) & (train['sub_area']==i),'build_year'].mode().values[0]\n\n#state\nfor i in a:\n    if train[(train['state'].isnull()) & (train['sub_area']==i)].shape[0]>0:\n        if (i=='Poselenie Klenovskoe' or i=='Poselenie Kievskij'):\n            train.loc[(train['state'].isnull()) & (train['sub_area']==i),'state']=2\n        else:\n            train.loc[(train['state'].isnull()) & (train['sub_area']==i),'state']=train.loc[(train['state'].notnull()) & (train['sub_area']==i),'state'].mode().values[0]\n\n#and now the rail stations\ncols=['ID_railroad_station_walk','ID_railroad_station_avto','green_part_2000','metro_km_walk','metro_min_walk']\nfor j in cols:\n    for i in a:\n        if train[(train[j].isnull()) & (train['sub_area']==i)].shape[0]>0:\n            train.loc[(train[j].isnull()) & (train['sub_area']==i),j]=train.loc[(train[j].notnull()) & (train['sub_area']==i),j].mode().values[0]\ncols=['railroad_station_walk_km','railroad_station_walk_min']\nfor j in cols:\n    for i in a:\n        if train[(train[j].isnull()) & (train['sub_area']==i)].shape[0]>0:\n            train.loc[(train[j].isnull()) & (train['sub_area']==i),j]=train.loc[(train[j].notnull()) & (train['sub_area']==i),j].median()\n\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"c900653b-44ef-0581-883d-c34579ff7fdf"},"source":"Now for the square footage I tried to do a linear fit as there is a fairly strong relationship"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"edd36fa7-f11f-bffa-6d8c-dd73900130eb"},"outputs":[],"source":"#doing things a bit differnt for rooms, base it off sq ft.\nx=train.loc[(train.full_sq.notnull())&(train.num_room.notnull()),'full_sq']\ny=train.loc[(train.full_sq.notnull())&(train.num_room.notnull()),'num_room']\nrooms=np.polyfit(x,y,1)\ntrain.loc[train['num_room'].isnull(),'num_room']=train.loc[train['num_room'].isnull(),'full_sq']*rooms[0]+rooms[1]\n\n#kitchen space will be same\nx=train.loc[(train.kitch_sq.notnull())&(train.kitch_sq.notnull()),'full_sq']\ny=train.loc[(train.kitch_sq.notnull())&(train.kitch_sq.notnull()),'kitch_sq']\nkitch=np.polyfit(x,y,1)\ntrain.loc[train['kitch_sq'].isnull(),'kitch_sq']=train.loc[train['kitch_sq'].isnull(),'full_sq']*kitch[0]+kitch[1]\n\n#and fix up the life-sq\nx=train.loc[(train.full_sq.notnull())&(train.life_sq.notnull()),'full_sq']\ny=train.loc[(train.full_sq.notnull())&(train.life_sq.notnull()),'life_sq']\nlife=np.polyfit(x,y,1)\ntrain.loc[train['life_sq'].isnull(),'life_sq']=train.loc[train['life_sq'].isnull(),'full_sq']*life[0]+life[1]\n#fix some of the off values\ntrain.loc[(train['life_sq']>train['full_sq'])&(train['life_sq']>1000),'life_sq']=train.loc[(train['life_sq']>train['full_sq'])&(train['life_sq']>1000),'life_sq']/1000\ntrain.loc[(train['life_sq']>train['full_sq'])&(train['life_sq']>106),'life_sq']=train.loc[(train['life_sq']>train['full_sq'])&(train['life_sq']>106),'life_sq']/100\nprint('fits done',train['product_type'].value_counts())\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"1ce17efe-9cd0-ccf5-b2fb-31e5c7619459"},"source":"Now for city-region data I just do the median values--I tried to do some clustering, but didn't have the best of luck and was unsatisfied with the results.\n\nFor Hospitals / schools I tried to do a fit of the children to school for known values.\n\nAlso the test data was missing some product types.  Based on the data population, I called them 'Investments'"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"37d08cf8-c4f1-7b7a-e0af-c31ee68b1844"},"outputs":[],"source":"\n#city /region data\ncols=['build_count_1921-1945','build_count_1946-1970','build_count_1971-1995','build_count_after_1995','build_count_before_1920',\n'build_count_block','build_count_brick','build_count_foam','build_count_frame','build_count_mix','build_count_monolith',\n'build_count_panel','build_count_slag','build_count_wood','cafe_avg_price_1000','cafe_avg_price_1500','cafe_avg_price_2000',\n'cafe_avg_price_3000','cafe_avg_price_500','cafe_avg_price_5000','cafe_sum_1000_max_price_avg','cafe_sum_1000_min_price_avg',\n'cafe_sum_1500_max_price_avg','cafe_sum_1500_min_price_avg','cafe_sum_2000_max_price_avg','cafe_sum_2000_min_price_avg',\n'cafe_sum_3000_max_price_avg','cafe_sum_3000_min_price_avg','cafe_sum_5000_max_price_avg','cafe_sum_5000_min_price_avg',\n'cafe_sum_500_max_price_avg','cafe_sum_500_min_price_avg','raion_build_count_with_builddate_info','raion_build_count_with_material_info','prom_part_5000']\nfor j in cols:\n    train.loc[train[j].isnull(),j]=train.loc[train[j].notnull(),j].median()\n            \nprint('city data done',train['product_type'].value_counts())\n\n#hospital and preschoo\ni='raion_popul'\nj='hospital_beds_raion'\nx=train.loc[(train[i].notnull())&(train[j].notnull()),i]\ny=train.loc[(train[i].notnull())&(train[j].notnull()),j]\nfit=np.polyfit(x,y,1)\ntrain.loc[train[j].isnull(),j]=train.loc[train[j].isnull(),i]*fit[0]+fit[1]\n\ni='children_preschool'\nj='preschool_quota'\nx=train.loc[(train[i].notnull())&(train[j].notnull()),i]\ny=train.loc[(train[i].notnull())&(train[j].notnull()),j]\nfit=np.polyfit(x,y,1)\ntrain.loc[train[j].isnull(),j]=train.loc[train[j].isnull(),i]*fit[0]+fit[1]\n\ni='children_school'\nj='school_quota'\nx=train.loc[(train[i].notnull())&(train[j].notnull()),i]\ny=train.loc[(train[i].notnull())&(train[j].notnull()),j]\nfit=np.polyfit(x,y,1)\ntrain.loc[train[j].isnull(),j]=train.loc[train[j].isnull(),i]*fit[0]+fit[1]\n\ntrain.loc[train['product_type'].isnull(),'product_type']='Investment'\n\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"21468454-1638-a837-19ec-573b6c26e0d3"},"source":"Now its time to turn everything into numbers.  I hand-coded the ecology--this was one of my first columns I went after. Then I one-hot encode the sub-area and materials."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b2c4547-db11-8f3d-715d-7f5268b3ad65"},"outputs":[],"source":"binary=[]\nfor i in train:\n    if train[i].dtypes=='object':\n        #print(train[i].value_counts())\n        if train[i].value_counts().shape[0]==2:\n            binary.append(i)\nfor i in binary:\n    train[i]=pd.factorize(train[i])[0]\n#change the echology to a 1-4 and ohe for NANs\ntrain.loc[train['ecology']=='no data','ecology_dat']=0\ntrain.loc[train['ecology']!='no data','ecology_dat']=1\ntrain.loc[train['ecology']=='no data','ecology']=2\ntrain.loc[train['ecology']=='poor','ecology']=1\ntrain.loc[train['ecology']=='satisfactory','ecology']=2\ntrain.loc[train['ecology']=='good','ecology']=3\ntrain.loc[train['ecology']=='excellent','ecology']=4\ntrain.ecology=pd.to_numeric(train.ecology)\n#sub_area to ohe\ntrain=pd.concat([train,pd.get_dummies(train.sub_area)],axis=1)\ntrain=pd.concat([train,pd.get_dummies(train.material,prefix='material')],axis=1)\ntrain.drop(['sub_area','material'],inplace=True,axis=1)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"c65d2b4b-07a0-92ec-bd47-2d1aa6080f25"},"source":"Do some final cleanup and split into test/train."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f4e96e0-8dff-b726-d1d2-33cfdb1a5354"},"outputs":[],"source":"train.drop(['price_doc','id'],inplace=True,axis=1)\n\nfor i in train:\n    out=train[i].isnull().sum()\n    if out>0:\n        print(i)\n    \ntest=train.iloc[n:,]\ntrain=train.iloc[0:n,]\nprint(test.shape,train.shape,n)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7d5b8dac-08a7-3064-46a5-cb243b4e443d"},"source":"I know many people prefer function programming, but this was I was able to find out when there were fallouts (areas with not enough data).  Also when you get in the grove you start cut-n-paste your code and substituting in new variables.\n\nBest of luck--"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}