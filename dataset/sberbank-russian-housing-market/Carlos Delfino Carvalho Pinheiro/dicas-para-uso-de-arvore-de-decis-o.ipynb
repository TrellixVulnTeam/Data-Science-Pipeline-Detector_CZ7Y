{"cells":[{"metadata":{},"cell_type":"markdown","source":"As dicas abaixo foram incialmente obtidas em: https://scikit-learn.org/stable/modules/tree.html#\n\nMeu objetivo com estas anotações é me aprofundar na otimização, pretendo referênciar outros estudos, e aqui mesmo anotar alguns algorítmos de teste."},{"metadata":{},"cell_type":"markdown","source":"## Dicas\n\n1. As *Árvores de decisão* tendem a superestimar(overfit) os dados com um grande número de recursos(features). É importante obter a proporção adequada de amostras para o número de recursos, pois é muito provável que uma árvore com poucas amostras em um espaço de alta dimensão fique muito limitado.\n   Considere uma redução dimencional (PCA, ICA, ou seleção de *Features*) antecipadamente para dar a sua arvore de decisão a chance de encontrar *features* que sejam discriminativo.\n2. Entender a estrutura da arvore de decisão ajudará a obter mais informações sobre como a *árvore de decisão* faz suas decisões, o que é importante para entender os recursos importantes nos dados.\n3. Visualizar sua *árvore* em quanto está treinando, usando funções de exportação. Use `max_depth=3` como uma profundidade inicial da arvore para ter uma ideia de com a arvore está se adaptando aos seus dados, e então aumente a profundidade.\n\n* Visualise your tree as you are training by using the export function. Use max_depth=3 as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth.\n\n* Remember that the number of samples required to populate the tree doubles for each additional level the tree grows to. Use max_depth to control the size of the tree to prevent overfitting.\n* Use min_samples_split or min_samples_leaf to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try min_samples_leaf=5 as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While min_samples_split can create arbitrarily small leaves, min_samples_leaf guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems. For classification with few classes, min_samples_leaf=1 is often the best choice.\n* Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (sample_weight) for each class to the same value. Also note that weight-based pre-pruning criteria, such as min_weight_fraction_leaf, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like min_samples_leaf.\n* If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as min_weight_fraction_leaf, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.\n* All decision trees use np.float32 arrays internally. If training data is not in this format, a copy of the dataset will be made.\n* If the input matrix X is very sparse, it is recommended to convert to sparse csc_matrix before calling fit and sparse csr_matrix before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Code you have previously used to load data\nimport sys\nimport os\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Say, all airline-safety files...\nimport zipfile\ndataset_name = \"sberbank-russian-housing-market\"\nworking_train_file = \"./train.csv\"\n\nimport shutil\n#shutil.rmtree('/kaggle/working')\n\n#shutil.rmtree('/kaggle/working/pdf')\n\n#os.remove('/kaggle/working/Model*')\n#os.remove('/kaggle/working/train.csv')\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"../input/\"+dataset_name+\"/train.csv.zip\",\"r\") as z:\n    z.extractall(\".\")\n\nshutil.rmtree('./__MACOSX')\n    \nprint(\"unziped train.csv.zip\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Path of the file to read\ntrain_data = pd.read_csv(working_train_file,index_col='id')\n# Clean na data\ntrain_data.dropna(inplace=True)\n\ntrain_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create X, seguindo a dica 1, escolhendo alguns *features* para evitar *overfit*\n\"\"\"\nprice_doc: sale price (this is the target variable)\nid: transaction id\ntimestamp: date of transaction\nfull_sq: total area in square meters, including loggias, balconies and other non-residential areas\nlife_sq: living area in square meters, excluding loggias, balconies and other non-residential areas\nfloor: for apartments, floor of the building\nmax_floor: number of floors in the building\nmaterial: wall material\nbuild_year: year built\nnum_room: number of living rooms\nkitch_sq: kitchen area\nstate: apartment condition\nproduct_type: owner-occupier purchase or investment\nsub_area: name of the district\n\"\"\"\n# features = ['num_room', 'max_floor', 'full_sq', 'life_sq', 'floor', 'material', 'build_year', 'kitch_sq']\n# features = ['num_room', 'max_floor', 'kitch_sq', 'full_sq', 'life_sq', 'floor', 'material']\n#\n# Maxdepth: 05, Random State: 01, Validation MAE: 2,563,367, \n#    Features: ['num_room', 'max_floor', 'kitch_sq', 'full_sq', 'life_sq', 'floor', 'material', 'sub_area', 'product_type']\n#\n# Maxdepth: 05, Random State: 01, Validation MAE: 2,527,532, \n#    Features: ['num_room', 'max_floor', 'kitch_sq', 'full_sq', 'life_sq', 'floor', 'material']\n\nfeatures = ['num_room', 'max_floor', 'kitch_sq', \n            'full_sq', 'life_sq', 'floor', \n            'material']\ntargets = ['price_doc']\n\n# Create target object and call it y\ny = train_data[targets]\nX = train_data[features]\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Add these lines to turn off the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import LabelEncoder\n\nif 'sub_area' in X:\n    sub_area_encoder = LabelEncoder()\n    sub_area_encoder.fit(X['sub_area'].astype(str))\n    X['sub_area'] = sub_area_encoder.transform(X['sub_area'].astype(str))\nif 'product_type' in X:\n    product_type_encoder = LabelEncoder()\n    product_type_encoder.fit(X['product_type'].astype(str))\n    X['product_type'] = product_type_encoder.transform(X['product_type'].astype(str))\nX.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def testDecisionTree(train_X, train_y, val_X, val_y, max_depth=3,random_state=1):\n    train_model = DecisionTreeRegressor(max_depth=max_depth,random_state=random_state)\n    # Fit Model\n    train_model.fit(train_X, train_y)\n\n    # Make validation predictions and calculate mean absolute error\n    val_predictions = train_model.predict(val_X)\n    val_mae = mean_absolute_error(val_predictions, val_y)\n    #print(\"Maxdepth: {depth:2d}, Random State: {random:2d}, Validation MAE: {mae:,.0f}\"\n    #      .format(depth=max_depth, random=random_state, mae=val_mae))\n    return train_model, val_predictions, val_mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz \nfrom sklearn.datasets import load_iris\nfrom sklearn import tree \ndef showDecisionTree(model,features,targets, file_name='DecisionTree'):\n    dot_data = tree.export_graphviz(model, out_file=None, \n                                        feature_names=features,  \n                                        class_names=targets,  \n                                        filled=True, rounded=True,  \n                                        special_characters=True)  \n    graph = graphviz.Source(dot_data)  \n    #graph \n    #dot_data = tree.export_graphviz(model, out_file=None) \n    #graph = graphviz.Source(dot_data)  \n    graph.render(\"pdf/\"+file_name)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\nmin_random_state = 0\nmax_random_state = 5\nmin_depth = 3\nmax_depth = len(features)* 2\n\n#shutil.rmtree('/kaggle/working/pdf')\n\n# Conforme a dica 3, usando inicialmente uma arvore raza para ajustar conforme as decisões.\n# Iniciado com max_depth=3\n# Specify Model\nbest_train_model = None\nbest_train_depth = 3\nbest_train_state = 1\nbest_train_mae = sys.float_info.max\nfor depth in range(min_depth,max_depth+1):\n    for state in range(min_random_state, max_random_state+1):\n        train_model, val_predictions, val_mae = testDecisionTree(train_X, train_y, val_X, val_y, max_depth=depth, random_state=state)\n        if best_train_mae > val_mae:\n            best_train_mae = val_mae\n            best_train_depth = depth\n            best_train_state = state\n            best_train_model = train_model\n        print(\"Maxdepth: {depth:2d}, Random State: {random:2d}, Validation MAE: {mae:,.0f}, Best MAE: {bmae:,.0f}\"\n            .format(depth=depth, random=state, mae=val_mae, bmae=best_train_mae))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"showDecisionTree(train_model,features,targets,file_name=\"ModelDecisionTree_{depth:02d}_{random:02d}_{mae:0f}\"\n              .format(depth=best_train_depth, random=best_train_state, mae=best_train_mae))\nprint(\"Maxdepth: {depth:02d}, Random State: {random:02d}, Validation MAE: {mae:,.0f}, Features: {f}\"\n              .format(depth=best_train_depth, random=best_train_state, mae=best_train_mae, f=features))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}