{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nnp.set_printoptions(suppress=True)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\npd.set_option('display.max_columns', None)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport zipfile\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith zipfile.ZipFile(\"/kaggle/input/sberbank-russian-housing-market/train.csv.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"/train\")\n\nwith zipfile.ZipFile(\"/kaggle/input/sberbank-russian-housing-market/test.csv.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"/test\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/train/train.csv')\ndf_test = pd.read_csv('/test/test.csv')\ntrain = df_train\ntest = df_test\nnp.random.seed(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# Missing Values\nfeatures_with_na = [feature for feature in train.columns if train[feature].isnull().sum()>1]\n\nfor feature in features_with_na:\n    print(f\"The amount of NA in\", feature, np.round(train[feature].isnull().mean(),3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# The relationship between the missing values and the target variable\nfor feature in features_with_na:\n    data = train.copy()\n\n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n\n    # let's calculate the mean price_doc where the information is missing or present\n\n    data.groupby(feature)['price_doc'].mean().plot.bar()\n    plt.ticklabel_format(style='plain', axis='y')\n    plt.title(feature)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of numerical variables\nnumerical_features = [feature for feature in train.columns if train[feature].dtype!='O']\n\n# Number of num features\nprint('Number of num features:', len(numerical_features)) # shape of data (30471, 292)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of categorical variables\ncategorical_features = [feature for feature in train.columns if train[feature].dtype == 'O']\n\n# Number of num features\nprint('Number of num features:', len(categorical_features)) # shape of data (30471, 292)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values in Categorical variables\nfor feature in categorical_features:\n    print('The feature is {} and number of categories are {}'.format(feature,len(train[feature].unique())))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The effect of the Missing data on the target variable\nfor feature in categorical_features:\n    data=train.copy()\n    data.groupby(feature)['price_doc'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('price_doc')\n    plt.title(feature)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Replace missing value with the label \"Missing\"\ndef replace_missing_value(train,features_with_na, value):\n    data = train.copy()\n    data[features_with_na]=data[features_with_na].fillna(value)\n    return data\n\ntrain=replace_missing_value(train,categorical_features, 'Missing')\ntest=replace_missing_value(test,categorical_features, 'Missing')\n\nprint(train[categorical_features].isnull().sum())\nprint(test[categorical_features].isnull().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features.remove('price_doc')\n# Replace NA in numerical features with median\ndef replace_missing_median(data,features):\n    for feature in features:\n    ## We will replace by using median since there are outliers\n        median_value=data[feature].median()\n        data[feature].fillna(median_value,inplace=True)\n    return data\ntrain = replace_missing_median(train,numerical_features)\ntest = replace_missing_median(test,numerical_features)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dealing with timestamp to get the year sold\ndef from_timestamp(train):\n    train['timestamp_parsed'] = pd.to_datetime(train['timestamp'], format='%Y-%m-%d') # Format 2011-08-20 : %Y-%m-%d\n    train['sold_year'] = train['timestamp_parsed'].dt.year\n    train['sold_month'] = train['timestamp_parsed'].dt.month\n    train['sold_day'] = train['timestamp_parsed'].dt.day\n    return train\n\ntrain = from_timestamp(train)\ntest = from_timestamp(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(['timestamp','timestamp_parsed'], axis=1)\ntest = test.drop(['timestamp','timestamp_parsed'], axis=1)\n\nsns.histplot(train['sold_year'], kde=False, bins=5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_features.remove('timestamp')\ndef feature_scaling(train, categorical_features):\n    for feature in categorical_features:\n        lb = LabelEncoder()\n        train[feature] = lb.fit_transform(train[feature])\n    return train\n\ntrain = feature_scaling(train, categorical_features)\ntest = feature_scaling(test, categorical_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the correlation between features\nnumerical_features.remove('id')\n\ncorrmat = train[numerical_features].corr()\nfig, ax = plt.subplots()\nsns.heatmap(corrmat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr\n\ncorr_features = correlation(train[numerical_features], 0.8)\nprint('Number of correlated features:', len(set(corr_features)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop(corr_features,axis=1)\ntest = test.drop(corr_features,axis=1)\n\nX_train = train.drop(['price_doc'], axis = 1)\ny_train = train['price_doc']\nX_test = test\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test= sc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Engineering","metadata":{}},{"cell_type":"code","source":"sns.histplot(y_train, bins=50, kde=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(np.log10(y_train), bins=50, kde=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.log10(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LGBM Model\n\nIn the first try I did the LGBM with the parameters below, after the submission, I had an rmse of 0.48122 I tried to improve the model using the Random Grid Search, and I got an RMSE of 0.40039","metadata":{}},{"cell_type":"code","source":"# The first set of params\nparams = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"num_leaves\": 64,\n    \"learning_rate\": 0.01,\n    'max_depth': -1,\n    'colsample_bytree': 0.9,\n    'num_leaves': 150,\n    \"bagging_seed\": 42,\n    \"verbosity\": 1,\n    \"seed\": 42,\n}\n\nlgtrain = lgb.Dataset(X_train, label=y_train)\nmodel = lgb.train(params, lgtrain, 5000)\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\nprint(\"LightGBM Training Completed...\")\n# The Submission File => RMSE = 0.48122","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The second set of params\nparams = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    'learning_rate': 0.3777518392924809,\n    'sub_feature': 0.5424987750103974,\n    'max_depth': 94,\n    'colsample_bytree': 0.9,\n    'num_leaves': 194,\n    \"bagging_seed\": 42,\n    'min_data': 31,\n    \"verbosity\": 1,\n    \"seed\": 42,\n    'boosting_type': 'dart',\n}\n\nlgtrain = lgb.Dataset(X_train, label=y_train)\nmodel = lgb.train(params, lgtrain, 5000)\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\nprint(\"LightGBM Training Completed...\")\n# The Submission File => RMSE = 0.40039","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntransformed_y_pred = 10 ** y_pred\n# Submitting the file\nmy_submission = pd.DataFrame({'id': df_test.id, 'price_doc': transformed_y_pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}