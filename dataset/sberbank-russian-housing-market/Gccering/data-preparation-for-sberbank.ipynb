{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b401ada4-911c-2ef0-0f87-6bb71a5326cb"},"source":"Introduction\n------------\n\nThis notebook aims to introduce a standard way of\n\n  1. loading the data into python notebook\n  2. Visual and identify issues\n  3. Pre-process the data e.g., variable transformation, normalization and etc."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a20c78ae-30d3-9c65-8b15-d33c206ac1bf"},"outputs":[],"source":"## Load packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import normalize\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nfrom ggplot import *"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d70274cd-0b27-9fd7-0ea3-3a77144c4683"},"outputs":[],"source":"## Load data into Python\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')"},{"cell_type":"markdown","metadata":{"_cell_guid":"da91ea54-d3bd-f2b0-9d30-cefc75c61a62"},"source":"Step 1: Take a peak\n-------------------\n\nNow take a peak at what's in the data. This including how many rows and columns and what are they."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8017ca14-f6d4-e84b-3e1d-a9f28b5c76ab"},"outputs":[],"source":"print(train.columns)\nprint(train.shape)\nprint(test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e81fc6f2-600f-4998-e939-69a37c3890e4"},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b4a76a9-39b2-1553-9c86-0ea6d4a62d18"},"outputs":[],"source":"## Describe the output field\nprint(train['price_doc'].describe())\nsns.distplot(train['price_doc'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"5b990626-1a4a-cf21-8c20-01c18202525e"},"source":"The dependent variable is skewed (as expected for dollars). The easiest would be to take a log tranformation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90dceefb-7fcd-a7db-add7-55d50fd3f34f"},"outputs":[],"source":"train['LogAmt']=np.log(train.price_doc+0.5)\nprint(train['LogAmt'].describe())\nsns.distplot(train['LogAmt'])"},{"cell_type":"markdown","metadata":{"_cell_guid":"537fa12d-471b-31d9-05f6-09845190c5d7"},"source":"** Note the two small bars between 13 and 14, and 14 and 15. We will need to dig into that and see that happens there **."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d29ef4dd-b3a8-5a1b-b427-c01bde91cca8"},"outputs":[],"source":"## Merge data into one dataset to prepare compare between train and test\ntrain_1 = train.copy()\ntrain_1['Source']='Train'\ntest_1 = test.copy()\ntest_1['Source']='Test'\nalldata = pd.concat([train_1, test_1])\nprint(alldata.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1a53f033-4e78-e8dc-d7b8-8b500853f0ef"},"source":"Most fields are numeric, and a few of them are object. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9804020-9916-0b5f-d2dc-f4d6442b40d5"},"outputs":[],"source":"## Numerical and Categorical data types\nalldata_dtype=alldata.dtypes\ndisplay_nvar = len(alldata.columns)\nalldata_dtype_dict = alldata_dtype.to_dict()\nalldata.dtypes.value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"54996cc7-f435-217c-5f3e-e7962d117913"},"source":"Step 2: Data VisualizationÂ¶\n---------------------------\n\n**Variable Description**\n\nI wrote this function with intension to compare train/test data and check if some variable is illy behaved. It is modified a little to fit this dataset to compared between normal/fraud subset.\n\nIt can be applied to both numeric and object data types:\n\n  1. When the data type is object, it will output the value count of each categories\n  2. When the data type is numeric, it will output the quantiles\n  3. It also seeks any missing values in the dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"102d09d5-3354-7530-97e6-ff4819e18542"},"outputs":[],"source":"def var_desc(dt):\n    print('--------------------------------------------')\n    for c in alldata.columns:\n        if alldata[c].dtype==dt:\n            t1 = alldata[alldata.Source=='Train'][c]\n            t2 = alldata[alldata.Source=='Test'][c]\n            if dt==\"object\":\n                f1 = t1[pd.isnull(t1)==False].value_counts()\n                f2 = t2[pd.isnull(t2)==False].value_counts()\n            else:\n                f1 = t1[pd.isnull(t1)==False].describe()\n                f2 = t2[pd.isnull(t2)==False].describe()\n            m1 = t1.isnull().value_counts()\n            m2 = t2.isnull().value_counts()\n            f = pd.concat([f1, f2], axis=1)\n            m = pd.concat([m1, m2], axis=1)\n            f.columns=['Train','Test']\n            m.columns=['Train','Test']\n            print(dt+' - '+c)\n            print('UniqValue - ',len(t1.value_counts()),len(t2.value_counts()))\n            print(f.sort_values(by='Train',ascending=False))\n            print()\n\n            m_print=m[m.index==True]\n            if len(m_print)>0:\n                print('missing - '+c)\n                print(m_print)\n            else:\n                print('NO Missing values - '+c)\n            if dt!=\"object\":\n                if len(t1.value_counts())<=10:\n                    c1 = t1.value_counts()\n                    c2 = t2.value_counts()\n                    c = pd.concat([c1, c2], axis=1)\n                    f.columns=['Train','Test']\n                    print(c)\n            print('--------------------------------------------')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9fda7818-ae1e-2d19-ceb9-c03003977099"},"outputs":[],"source":"var_desc('int64')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"21e6d16f-12d5-3874-7ca2-5db416fc798c"},"outputs":[],"source":"var_desc('float64')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f88a1e2-59ad-b55d-3bde-1902b6744e44"},"outputs":[],"source":"var_desc('object')"},{"cell_type":"markdown","metadata":{"_cell_guid":"4f435b05-172a-b794-2438-9ccde2397aad"},"source":"Correlation\n-----------\n\nCorrelation is useful to find peers of input field so we are aware when building models, either to transform them (principal component) or remove one of the two.\n\nThe first plot below the overall correlation matrix and there are blocks of variables that are highly correlated\n\nThe second plot shows the highly correlated variables with response. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6676ec60-8fbe-703d-49f3-318e655552a9"},"outputs":[],"source":"## Correlation\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(corrmat, vmax=.8, square=True,xticklabels=False,yticklabels=False,cbar=False);"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"aefdcb6b-5faa-4eb9-578b-31f08cebaf43"},"outputs":[],"source":"# Top 20 correlated variables\ncorrmat = train.corr()\nk = 20 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'price_doc')['price_doc'].index\ncm = np.corrcoef(train[cols].values.T)\nf, ax = plt.subplots(figsize=(12, 12))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=False, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bf0aa3e3-52a2-6c31-196d-dacfe754016a"},"source":"Detailed Look\n-------------\n\nLet's take a detailed look of the variables with response variable. "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7c75ddd-f016-dbf5-5714-32a5a355b4a7"},"outputs":[],"source":"dt = 'object'\nsel_col = train.columns[train.dtypes==dt]\nsel_col = [t for t in sel_col if t not in ['Id','timestamp','LogAmt']] \n\nfor sc in sel_col:\n    data_1  = pd.concat([train[sc], train.LogAmt], axis=1)\n    data_2  = pd.melt(data_1,id_vars='LogAmt')\n    data_2  = data_2[pd.isnull(data_2.value)==False]\n    p = ggplot(data_2, aes(x='value',y='LogAmt')) + geom_boxplot(alpha=0.5,) + theme(plot_margin = dict(right = 1, top=0.5), axis_title_x=sc)\n    print(p)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5fb4ac3a-cee6-a612-7737-4e5130c12b38"},"outputs":[],"source":"dt = 'int64'\nsel_col = train.columns[train.dtypes==dt]\nsel_col = [t for t in sel_col if t not in ['Id','timestamp','LogAmt']] \n\nfor sc in sel_col:\n    data_1  = pd.concat([train[sc], train.LogAmt], axis=1)\n    data_2  = pd.melt(data_1,id_vars='LogAmt')\n    data_2  = data_2[pd.isnull(data_2.value)==False]\n    p = ggplot(data_2, aes(x='value',y='LogAmt')) + geom_point(alpha=0.5) + theme(plot_margin = dict(right = 1, top=0.5), axis_title_x=sc)\n    print(p)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}