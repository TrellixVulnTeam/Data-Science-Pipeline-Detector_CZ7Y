{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!conda install '/kaggle/input/pydicom-conda-helper/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:31:37.796589Z","iopub.execute_input":"2021-07-22T07:31:37.797119Z","iopub.status.idle":"2021-07-22T07:33:03.652547Z","shell.execute_reply.started":"2021-07-22T07:31:37.797032Z","shell.execute_reply":"2021-07-22T07:33:03.651252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys; \n\npackage_paths = [\n    '../input/timm-pytorch-image-models/pytorch-image-models-master',\n]\n\nfor pth in package_paths:\n    sys.path.append(pth)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-22T07:33:03.654778Z","iopub.execute_input":"2021-07-22T07:33:03.655252Z","iopub.status.idle":"2021-07-22T07:33:03.665635Z","shell.execute_reply.started":"2021-07-22T07:33:03.655214Z","shell.execute_reply":"2021-07-22T07:33:03.664547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport os\nimport time\nimport random\n\nimport numpy as np  # linear algebra!\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport PIL\n\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\nfrom tqdm.contrib.concurrent import process_map\n\nimport torch\nimport torchvision\nfrom torch.utils.data.dataset import Dataset\nimport torch.cuda.amp as amp\n\nimport timm\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nSIZE = (384, 384)\n\nMODEL_DIR = \"/kaggle/input/classification-training-script1\"\nDATA_DIR = RESIZE_DIR = \"/kaggle/working/\"\nFOLDS = 5\nNUM_CLASSES = 4\nBATCHSIZE = 64\nSEED = 420\nMODEL_NAME = \"tf_efficientnetv2_s\"","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:33:03.669594Z","iopub.execute_input":"2021-07-22T07:33:03.66994Z","iopub.status.idle":"2021-07-22T07:33:09.747619Z","shell.execute_reply.started":"2021-07-22T07:33:03.669908Z","shell.execute_reply":"2021-07-22T07:33:09.746295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\nprint(len(sub_df))\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:33:09.749837Z","iopub.execute_input":"2021-07-22T07:33:09.750317Z","iopub.status.idle":"2021-07-22T07:33:09.794989Z","shell.execute_reply.started":"2021-07-22T07:33:09.750283Z","shell.execute_reply":"2021-07-22T07:33:09.79388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_df = sub_df.loc[sub_df.id.str.contains('_study')]\nlen(study_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:33:09.796682Z","iopub.execute_input":"2021-07-22T07:33:09.797155Z","iopub.status.idle":"2021-07-22T07:33:09.812111Z","shell.execute_reply.started":"2021-07-22T07:33:09.797109Z","shell.execute_reply":"2021-07-22T07:33:09.810178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_df = sub_df.loc[sub_df.id.str.contains('_image')]\nlen(image_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:33:09.81406Z","iopub.execute_input":"2021-07-22T07:33:09.814652Z","iopub.status.idle":"2021-07-22T07:33:09.829996Z","shell.execute_reply.started":"2021-07-22T07:33:09.814604Z","shell.execute_reply":"2021-07-22T07:33:09.828737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data\n\ndef resize_xray(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:33:09.831958Z","iopub.execute_input":"2021-07-22T07:33:09.832447Z","iopub.status.idle":"2021-07-22T07:33:09.842219Z","shell.execute_reply.started":"2021-07-22T07:33:09.832402Z","shell.execute_reply":"2021-07-22T07:33:09.840747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nTEST_PATH = f'/kaggle/working/test_{SIZE[0]}x{SIZE[1]}'\ndef prepare_test_images():\n    image_id = []\n    dim0 = []\n    dim1 = []\n\n    os.makedirs(TEST_PATH, exist_ok=True)\n\n    for dirname, _, filenames in tqdm(os.walk(f'../input/siim-covid19-detection/test')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            xray = read_xray(os.path.join(dirname, file))\n            im = resize_xray(xray, size=SIZE[0])  \n            im.save(os.path.join(TEST_PATH, file.replace('dcm', 'jpg')))\n\n            image_id.append(file.replace('.dcm', ''))\n            dim0.append(xray.shape[0])\n            dim1.append(xray.shape[1])\n            \n    return image_id, dim0, dim1","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:33:09.846991Z","iopub.execute_input":"2021-07-22T07:33:09.848055Z","iopub.status.idle":"2021-07-22T07:33:09.857461Z","shell.execute_reply.started":"2021-07-22T07:33:09.848009Z","shell.execute_reply":"2021-07-22T07:33:09.85633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_ids, dim0, dim1 = prepare_test_images()\nprint(f'Number of test images: {len(os.listdir(TEST_PATH))}')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:33:09.860994Z","iopub.execute_input":"2021-07-22T07:33:09.861662Z","iopub.status.idle":"2021-07-22T07:41:57.624297Z","shell.execute_reply.started":"2021-07-22T07:33:09.861615Z","shell.execute_reply":"2021-07-22T07:41:57.623103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imgs_study_mapping = pd.DataFrame.from_dict({'image_id': image_ids, 'dim0': dim0, 'dim1': dim1})\n\n# Associate image-level id with study-level ids.\n# Note that a study-level might have more than one image-level ids.\nfor study_dir in os.listdir('../input/siim-covid19-detection/test'):\n    for series in os.listdir(f'../input/siim-covid19-detection/test/{study_dir}'):\n        for image in os.listdir(f'../input/siim-covid19-detection/test/{study_dir}/{series}/'):\n            image_id = image[:-4]\n            test_imgs_study_mapping.loc[test_imgs_study_mapping['image_id'] == image_id, 'study_id'] = study_dir\n        \ntest_imgs_study_mapping.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:41:57.625994Z","iopub.execute_input":"2021-07-22T07:41:57.626662Z","iopub.status.idle":"2021-07-22T07:42:00.724551Z","shell.execute_reply.started":"2021-07-22T07:41:57.626617Z","shell.execute_reply":"2021-07-22T07:42:00.723423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir \"/kaggle/working/test_{SIZE[0]}x{SIZE[1]}\"\n# !tar -xzf \"/kaggle/input/train-{SIZE[0]}x{SIZE[1]}/test_{SIZE[0]}x{SIZE[1]}.tar.gz\" -C \"/kaggle/working/test_{SIZE[0]}x{SIZE[1]}\" .","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:00.726244Z","iopub.execute_input":"2021-07-22T07:42:00.726676Z","iopub.status.idle":"2021-07-22T07:42:00.734006Z","shell.execute_reply.started":"2021-07-22T07:42:00.726631Z","shell.execute_reply":"2021-07-22T07:42:00.732735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make results reproducible\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:00.735768Z","iopub.execute_input":"2021-07-22T07:42:00.736549Z","iopub.status.idle":"2021-07-22T07:42:00.751137Z","shell.execute_reply.started":"2021-07-22T07:42:00.736468Z","shell.execute_reply":"2021-07-22T07:42:00.749967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pattern = f\"{MODEL_NAME}\"\nmodelnames = glob.glob(os.path.join(MODEL_DIR, f\"{MODEL_NAME}*\", \"*.pth\"))\nprint(f\"{len(modelnames)}: Models Found\")","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:00.753127Z","iopub.execute_input":"2021-07-22T07:42:00.753822Z","iopub.status.idle":"2021-07-22T07:42:00.787762Z","shell.execute_reply.started":"2021-07-22T07:42:00.753758Z","shell.execute_reply":"2021-07-22T07:42:00.786692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_imgs_study_mapping = pd.read_csv(\"/kaggle/input/test-image-to-study-mapping/test_study_id_to_image_mapping.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:00.791277Z","iopub.execute_input":"2021-07-22T07:42:00.791603Z","iopub.status.idle":"2021-07-22T07:42:00.798883Z","shell.execute_reply.started":"2021-07-22T07:42:00.791572Z","shell.execute_reply":"2021-07-22T07:42:00.797621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST_DATA_PATH = os.path.join(RESIZE_DIR, f\"test_{SIZE[0]}x{SIZE[1]}\")\nprint(\"Test Data Path {}\".format(TEST_PATH))\ndef get_img_path(row):\n    study_id = row[\"study_id\"]\n    img_id = row[\"image_id\"]\n    paths = glob.glob(os.path.join(TEST_PATH, \"{}*.jpg\".format(img_id)))\n    for path in paths:\n        if img_id in path:\n            return path\n    return None","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:00.80094Z","iopub.execute_input":"2021-07-22T07:42:00.801441Z","iopub.status.idle":"2021-07-22T07:42:00.811494Z","shell.execute_reply.started":"2021-07-22T07:42:00.801396Z","shell.execute_reply":"2021-07-22T07:42:00.810287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imgs_study_mapping[\"path\"] = test_imgs_study_mapping.apply(get_img_path, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:00.813472Z","iopub.execute_input":"2021-07-22T07:42:00.814228Z","iopub.status.idle":"2021-07-22T07:42:03.643165Z","shell.execute_reply.started":"2021-07-22T07:42:00.814182Z","shell.execute_reply":"2021-07-22T07:42:03.642075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imgs_study_mapping[test_imgs_study_mapping[\"path\"].isna()]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:03.644654Z","iopub.execute_input":"2021-07-22T07:42:03.645116Z","iopub.status.idle":"2021-07-22T07:42:03.666411Z","shell.execute_reply.started":"2021-07-22T07:42:03.645073Z","shell.execute_reply":"2021-07-22T07:42:03.664759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(modelname):\n    summary = torch.load(modelname)\n    print(f\"Loaded model {modelname}\")\n    print(f\"Epoch {summary['epoch']}\")\n    print(f\"Map@2 {summary['map_at_2']}\")\n    print(f\"AUC@2 {summary['auc']}\")\n    model = timm.create_model(model_name=MODEL_NAME, pretrained=False, in_chans=3)\n    model.classifier = torch.nn.Linear(\n        in_features=model.classifier.in_features, out_features=NUM_CLASSES\n    )\n    model.load_state_dict(summary[\"state_dict\"], strict=True)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:03.668308Z","iopub.execute_input":"2021-07-22T07:42:03.669237Z","iopub.status.idle":"2021-07-22T07:42:03.683302Z","shell.execute_reply.started":"2021-07-22T07:42:03.66919Z","shell.execute_reply":"2021-07-22T07:42:03.68208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class XRayDatasetFromDF(Dataset):\n    def __init__(self, df, train=True, augment=True, normalize=False, size=(384, 384)):\n        self.df = df\n        self.name_to_label_map = {\n            \"Negative\": 0,\n            \"Typical\": 1,\n            \"Indeterminate\": 2,\n            \"Atypical\": 3,\n        }\n        self.study_ids = df.index.sort_values()\n        self.path_suffix = (\n            os.path.join(DATA_DIR, \"train\") if train else os.path.join(DATA_DIR, \"test\")\n        )\n        self._train = train\n        self._augment = augment\n        self._normalize = normalize\n        self._size = size\n        self._transform_list = [\n            # A.Resize(size[0], size[1], p=1)\n        ]\n\n        if self._augment:\n            self._transform_list.extend(\n                [\n                    A.VerticalFlip(p=0.5),\n                    A.HorizontalFlip(p=0.5),\n                    A.ShiftScaleRotate(\n                        scale_limit=0.20,\n                        rotate_limit=10,\n                        shift_limit=0.1,\n                        p=0.5,\n                        border_mode=cv2.BORDER_CONSTANT,\n                        value=0,\n                    ),\n                    A.RandomBrightnessContrast(p=0.5),\n                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                    ToTensorV2(),\n                ]\n            )\n        elif self._normalize and not self._augment:  # test mode\n            self._transform_list.extend(\n                [\n                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                    ToTensorV2(),\n                ]\n            )\n        self._transforms = A.Compose(self._transform_list)\n\n    def __len__(self):\n        return len(self.study_ids)\n\n    def assign_label(self, row):\n        for k in self.name_to_label_map:\n            if row[k]:\n                return self.name_to_label_map[k]\n\n    def __getitem__(self, idx):\n        study_id = self.study_ids[idx]\n        study_imgs = self.df.loc[study_id]\n        path = None\n        label = None\n\n        path = study_imgs[\"path\"]\n        label = study_imgs[\"int_label\"] if self._train else -1\n\n        # ideally, we'd clean up the df,\n        # but may be we use it to produce predictions as well.\n        dicom_arr = (\n            cv2.imread(path)\n            if path.endswith(\".jpg\")\n            else dicom2array(path, size=self._size)\n        )\n        img = cv2.cvtColor(dicom_arr, cv2.COLOR_BGR2RGB)\n        img = self._transforms(image=img)[\"image\"]\n\n        return img, label","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:03.684826Z","iopub.execute_input":"2021-07-22T07:42:03.685533Z","iopub.status.idle":"2021-07-22T07:42:03.704789Z","shell.execute_reply.started":"2021-07-22T07:42:03.685486Z","shell.execute_reply":"2021-07-22T07:42:03.703386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = XRayDatasetFromDF(df=test_imgs_study_mapping, train=False, augment=True, normalize=False, size=SIZE)\ntest_dl = torch.utils.data.DataLoader(\n        dataset=test_ds,\n        batch_size=BATCHSIZE * 2,\n        pin_memory=True,\n        num_workers=8,\n        drop_last=False,\n        shuffle=False,\n        prefetch_factor=8,\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:03.706653Z","iopub.execute_input":"2021-07-22T07:42:03.70725Z","iopub.status.idle":"2021-07-22T07:42:03.722483Z","shell.execute_reply.started":"2021-07-22T07:42:03.707203Z","shell.execute_reply":"2021-07-22T07:42:03.721402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, slide_dl, tta_times=10):\n    sample_size = len(slide_dl.dataset)\n    print(\"Predicting on {} Images {} times\".format(sample_size, tta_times))\n    probs = np.zeros((sample_size, NUM_CLASSES))\n    loss_sum = 0\n\n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\").to(dev)\n\n    for i in range(tta_times):\n\n        offset = 0\n\n        for i, grid in enumerate(tqdm(slide_dl)):\n            with torch.no_grad():\n                img, _ = grid\n                curr_batch_size = img.shape[0]\n\n                pred = model(img.to(dev))\n                # remove the redundant dimension added by\n                # pytorch's collate_fn\n\n                prob = pred.sigmoid()\n                probs[offset : offset + curr_batch_size, :] += prob.cpu().numpy()\n                offset += curr_batch_size\n\n\n    return probs / tta_times","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:03.725959Z","iopub.execute_input":"2021-07-22T07:42:03.726414Z","iopub.status.idle":"2021-07-22T07:42:03.737429Z","shell.execute_reply.started":"2021-07-22T07:42:03.726383Z","shell.execute_reply":"2021-07-22T07:42:03.736209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(dev)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:03.739189Z","iopub.execute_input":"2021-07-22T07:42:03.739758Z","iopub.status.idle":"2021-07-22T07:42:03.81257Z","shell.execute_reply.started":"2021-07-22T07:42:03.739713Z","shell.execute_reply":"2021-07-22T07:42:03.811201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor modelname in modelnames:\n    model = load_model(modelname)\n    model = model.to(dev)\n    model = model.eval()\n    probs = predict(model, test_dl)\n    # probs = torch.from_numpy(probs)\n    # probs = probs.softmax(dim=1)\n\n    for k in test_ds.name_to_label_map:\n        col_idx = test_ds.name_to_label_map[k]\n        test_imgs_study_mapping[k] = probs[:, col_idx]\n    test_imgs_study_mapping[\"modelname\"] = modelname\n    predictions.append(test_imgs_study_mapping.copy())","metadata":{"execution":{"iopub.status.busy":"2021-07-22T07:42:03.818961Z","iopub.execute_input":"2021-07-22T07:42:03.819665Z","iopub.status.idle":"2021-07-22T08:11:15.257104Z","shell.execute_reply.started":"2021-07-22T07:42:03.819617Z","shell.execute_reply":"2021-07-22T08:11:15.255125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df = pd.concat(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:15.260384Z","iopub.execute_input":"2021-07-22T08:11:15.260833Z","iopub.status.idle":"2021-07-22T08:11:15.285717Z","shell.execute_reply.started":"2021-07-22T08:11:15.260781Z","shell.execute_reply":"2021-07-22T08:11:15.284541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_predictions_df = predictions_df.groupby(\"study_id\").agg({\n    \"Negative\":\"mean\",\n    \"Typical\":\"mean\",\n    \"Indeterminate\":\"mean\",\n    \"Atypical\":\"mean\"\n}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:15.287439Z","iopub.execute_input":"2021-07-22T08:11:15.287924Z","iopub.status.idle":"2021-07-22T08:11:15.319591Z","shell.execute_reply.started":"2021-07-22T08:11:15.28787Z","shell.execute_reply":"2021-07-22T08:11:15.318405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_predictions_df[\"id\"] = mean_predictions_df[\"study_id\"] + \"_study\"","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:15.321407Z","iopub.execute_input":"2021-07-22T08:11:15.321877Z","iopub.status.idle":"2021-07-22T08:11:15.383615Z","shell.execute_reply.started":"2021-07-22T08:11:15.321819Z","shell.execute_reply":"2021-07-22T08:11:15.382475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_predictions_df","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:15.385335Z","iopub.execute_input":"2021-07-22T08:11:15.386108Z","iopub.status.idle":"2021-07-22T08:11:15.414199Z","shell.execute_reply.started":"2021-07-22T08:11:15.386061Z","shell.execute_reply":"2021-07-22T08:11:15.413132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OPBB = \"0 0 1 1\"\ndef generate_cls_prediction_strings(row):\n    predictions = []\n\n    for k in test_ds.name_to_label_map:\n        \n        p_k = row[k]\n        predictions.append(k.lower())\n        predictions.append(str(p_k))\n        predictions.append(OPBB)\n    return \" \".join(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:15.416009Z","iopub.execute_input":"2021-07-22T08:11:15.416767Z","iopub.status.idle":"2021-07-22T08:11:15.42425Z","shell.execute_reply.started":"2021-07-22T08:11:15.41672Z","shell.execute_reply":"2021-07-22T08:11:15.423146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_predictions_df[\"PredictionString\"] = mean_predictions_df.apply(generate_cls_prediction_strings, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:15.426148Z","iopub.execute_input":"2021-07-22T08:11:15.426661Z","iopub.status.idle":"2021-07-22T08:11:15.491914Z","shell.execute_reply.started":"2021-07-22T08:11:15.426603Z","shell.execute_reply":"2021-07-22T08:11:15.490817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_submission_df = mean_predictions_df[[\"id\", \"PredictionString\"]]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:15.493524Z","iopub.execute_input":"2021-07-22T08:11:15.494003Z","iopub.status.idle":"2021-07-22T08:11:15.502648Z","shell.execute_reply.started":"2021-07-22T08:11:15.493938Z","shell.execute_reply":"2021-07-22T08:11:15.501223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n\nimport gc\nimport glob\nfrom tqdm import tqdm\nfrom shutil import copyfile","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:15.504767Z","iopub.execute_input":"2021-07-22T08:11:15.505397Z","iopub.status.idle":"2021-07-22T08:11:20.941705Z","shell.execute_reply.started":"2021-07-22T08:11:15.505349Z","shell.execute_reply":"2021-07-22T08:11:20.940438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n    # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:20.94349Z","iopub.execute_input":"2021-07-22T08:11:20.944192Z","iopub.status.idle":"2021-07-22T08:11:22.764736Z","shell.execute_reply.started":"2021-07-22T08:11:20.944145Z","shell.execute_reply":"2021-07-22T08:11:22.762897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\nprint(len(sub_df))\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:22.768201Z","iopub.execute_input":"2021-07-22T08:11:22.768517Z","iopub.status.idle":"2021-07-22T08:11:22.797753Z","shell.execute_reply.started":"2021-07-22T08:11:22.768486Z","shell.execute_reply":"2021-07-22T08:11:22.796358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_df = sub_df.loc[sub_df.id.str.contains('_study')]\nlen(study_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:22.799746Z","iopub.execute_input":"2021-07-22T08:11:22.800237Z","iopub.status.idle":"2021-07-22T08:11:22.813758Z","shell.execute_reply.started":"2021-07-22T08:11:22.80019Z","shell.execute_reply":"2021-07-22T08:11:22.812232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_df = sub_df.loc[sub_df.id.str.contains('_image')]\nlen(image_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:22.816112Z","iopub.execute_input":"2021-07-22T08:11:22.816904Z","iopub.status.idle":"2021-07-22T08:11:22.82846Z","shell.execute_reply.started":"2021-07-22T08:11:22.816851Z","shell.execute_reply":"2021-07-22T08:11:22.826793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data\n\ndef resize_xray(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:22.83082Z","iopub.execute_input":"2021-07-22T08:11:22.831416Z","iopub.status.idle":"2021-07-22T08:11:22.84423Z","shell.execute_reply.started":"2021-07-22T08:11:22.831368Z","shell.execute_reply":"2021-07-22T08:11:22.842809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_PATH = f'/kaggle/tmp/test/'\nIMG_SIZE = 512\n\ndef prepare_test_images():\n    image_id = []\n    dim0 = []\n    dim1 = []\n\n    os.makedirs(TEST_PATH, exist_ok=True)\n\n    for dirname, _, filenames in tqdm(os.walk(f'../input/siim-covid19-detection/test')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            xray = read_xray(os.path.join(dirname, file))\n            im = resize_xray(xray, size=IMG_SIZE)  \n            im.save(os.path.join(TEST_PATH, file.replace('dcm', 'png')))\n\n            image_id.append(file.replace('.dcm', ''))\n            dim0.append(xray.shape[0])\n            dim1.append(xray.shape[1])\n            \n    return image_id, dim0, dim1","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:22.846035Z","iopub.execute_input":"2021-07-22T08:11:22.846562Z","iopub.status.idle":"2021-07-22T08:11:22.860096Z","shell.execute_reply.started":"2021-07-22T08:11:22.846516Z","shell.execute_reply":"2021-07-22T08:11:22.858771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_ids, dim0, dim1 = prepare_test_images()\nprint(f'Number of test images: {len(os.listdir(TEST_PATH))}')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:11:22.862155Z","iopub.execute_input":"2021-07-22T08:11:22.862665Z","iopub.status.idle":"2021-07-22T08:21:24.591072Z","shell.execute_reply.started":"2021-07-22T08:11:22.862612Z","shell.execute_reply":"2021-07-22T08:21:24.586705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_df = pd.DataFrame.from_dict({'image_id': image_ids, 'dim0': dim0, 'dim1': dim1})\n\n# Associate image-level id with study-level ids.\n# Note that a study-level might have more than one image-level ids.\nfor study_dir in os.listdir('../input/siim-covid19-detection/test'):\n    for series in os.listdir(f'../input/siim-covid19-detection/test/{study_dir}'):\n        for image in os.listdir(f'../input/siim-covid19-detection/test/{study_dir}/{series}/'):\n            image_id = image[:-4]\n            meta_df.loc[meta_df['image_id'] == image_id, 'study_id'] = study_dir\n        \nmeta_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:21:24.592863Z","iopub.execute_input":"2021-07-22T08:21:24.593359Z","iopub.status.idle":"2021-07-22T08:21:27.417353Z","shell.execute_reply.started":"2021-07-22T08:21:24.593313Z","shell.execute_reply":"2021-07-22T08:21:27.416051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cp /kaggle/input/yolomodels-obj/yolov5L512_1607.pt /kaggle/working\n%cp /kaggle/input/yolomodels-obj/yolov5m6_512.pt /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:21:27.418994Z","iopub.execute_input":"2021-07-22T08:21:27.419454Z","iopub.status.idle":"2021-07-22T08:21:33.223583Z","shell.execute_reply.started":"2021-07-22T08:21:27.419409Z","shell.execute_reply":"2021-07-22T08:21:33.222165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/input/siimcovidyolov5l/yolov5/detect.py --weights /kaggle/working/yolov5L512_1607.pt /kaggle/working/yolov5m6_512.pt \\\n                                      --source {TEST_PATH} \\\n                                      --img 512 \\\n                                      --conf 0.22 \\\n                                      --iou-thres 0.5 \\\n                                      --max-det 10 \\\n                                      --save-txt \\\n                                      --save-conf \\\n                                      --augment","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:21:33.227743Z","iopub.execute_input":"2021-07-22T08:21:33.228094Z","iopub.status.idle":"2021-07-22T08:24:31.166607Z","shell.execute_reply.started":"2021-07-22T08:21:33.228059Z","shell.execute_reply":"2021-07-22T08:24:31.165306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PRED_PATH = 'runs/detect/exp/labels'\nprediction_files = os.listdir(PRED_PATH)\nprint(f'Number of opacity predicted by YOLOv5: {len(prediction_files)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:24:31.170752Z","iopub.execute_input":"2021-07-22T08:24:31.171123Z","iopub.status.idle":"2021-07-22T08:24:31.181945Z","shell.execute_reply.started":"2021-07-22T08:24:31.171087Z","shell.execute_reply":"2021-07-22T08:24:31.180127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correct_bbox_format(bboxes):\n    correct_bboxes = []\n    for b in bboxes:\n        xc, yc = int(np.round(b[0]*IMG_SIZE)), int(np.round(b[1]*IMG_SIZE))\n        w, h = int(np.round(b[2]*IMG_SIZE)), int(np.round(b[3]*IMG_SIZE))\n\n        xmin = xc - int(np.round(w/2))\n        ymin = yc - int(np.round(h/2))\n        xmax = xc + int(np.round(w/2))\n        ymax = yc + int(np.round(h/2))\n        \n        correct_bboxes.append([xmin, ymin, xmax, ymax])\n        \n    return correct_bboxes\n\ndef scale_bboxes_to_original(row, bboxes):\n    # Get scaling factor\n    scale_x = IMG_SIZE/row.dim1\n    scale_y = IMG_SIZE/row.dim0\n    \n    scaled_bboxes = []\n    for bbox in bboxes:\n        xmin, ymin, xmax, ymax = bbox\n        \n        xmin = int(np.round(xmin/scale_x))\n        ymin = int(np.round(ymin/scale_y))\n        xmax = int(np.round(xmax/scale_x))\n        ymax = int(np.round(ymax/scale_y))\n        \n        scaled_bboxes.append([xmin, ymin, xmax, ymax])\n        \n    return scaled_bboxes\n\n# Read the txt file generated by YOLOv5 during inference and extract \n# confidence and bounding box coordinates.\ndef get_conf_bboxes(file_path):\n    confidence = []\n    bboxes = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            preds = line.strip('\\n').split(' ')\n            preds = list(map(float, preds))\n            confidence.append(preds[-1])\n            bboxes.append(preds[1:-1])\n    return confidence, bboxes","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:24:31.18411Z","iopub.execute_input":"2021-07-22T08:24:31.184616Z","iopub.status.idle":"2021-07-22T08:24:31.200883Z","shell.execute_reply.started":"2021-07-22T08:24:31.18457Z","shell.execute_reply":"2021-07-22T08:24:31.199195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_pred_strings = []\nctr = 0\nfor i in tqdm(range(len(image_df))):\n    row = meta_df.loc[i]\n    id_name = row.image_id\n    \n    if f'{id_name}.txt' in prediction_files:\n        # opacity label\n        confidence, bboxes = get_conf_bboxes(f'{PRED_PATH}/{id_name}.txt')\n        bboxes = correct_bbox_format(bboxes)\n        ori_bboxes = scale_bboxes_to_original(row, bboxes)\n        \n        pred_string = ''\n        for j, conf in enumerate(confidence):\n            pred_string += f'opacity {conf} ' + ' '.join(map(str, ori_bboxes[j])) + ' '\n        \n        row = mean_predictions_df.loc[mean_predictions_df['study_id'] == row.study_id]\n        neg = row.Negative.item()\n        typ = row.Typical.item()\n        ind = row.Indeterminate.item()\n        atp = row.Atypical.item()\n        output_class = np.argmax(np.array([neg,typ,ind,atp]))\n        if output_class == 0 and neg > 0.7:\n            ctr+=1\n            image_pred_strings.append(\"none 1 0 0 1 1\")\n        else:\n            image_pred_strings.append(pred_string[:-1])\n    else:\n        image_pred_strings.append(\"none 1 0 0 1 1\")\nprint('Number of images that were detected as opacity but are forced to none on the basis of classification output are :' + str(ctr))","metadata":{"execution":{"iopub.status.busy":"2021-07-22T09:25:24.014807Z","iopub.execute_input":"2021-07-22T09:25:24.015289Z","iopub.status.idle":"2021-07-22T09:25:25.404289Z","shell.execute_reply.started":"2021-07-22T09:25:24.015255Z","shell.execute_reply":"2021-07-22T09:25:25.402781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_df['PredictionString'] = image_pred_strings\nimage_df = meta_df[['study_id','image_id', 'PredictionString']]\n# image_df.insert(0, 'id', image_df.apply(lambda row: row.image_id+'_image', axis=1))\n# image_df = image_df.drop('image_id', axis=1)\nimage_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:24:31.834683Z","iopub.execute_input":"2021-07-22T08:24:31.835035Z","iopub.status.idle":"2021-07-22T08:24:31.855681Z","shell.execute_reply.started":"2021-07-22T08:24:31.835002Z","shell.execute_reply":"2021-07-22T08:24:31.854428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_df.insert(0, 'id', image_df.apply(lambda row: row.image_id+'_image', axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:24:31.857463Z","iopub.execute_input":"2021-07-22T08:24:31.857917Z","iopub.status.idle":"2021-07-22T08:24:31.902856Z","shell.execute_reply.started":"2021-07-22T08:24:31.857869Z","shell.execute_reply":"2021-07-22T08:24:31.901649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_df_new = image_df[['id','PredictionString']]","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:24:31.90461Z","iopub.execute_input":"2021-07-22T08:24:31.905074Z","iopub.status.idle":"2021-07-22T08:24:31.912427Z","shell.execute_reply.started":"2021-07-22T08:24:31.905027Z","shell.execute_reply":"2021-07-22T08:24:31.910848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_submission_df = cls_submission_df.append(image_df_new).reset_index(drop=True)\ncls_submission_df.to_csv('/kaggle/working/submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T08:24:31.914479Z","iopub.execute_input":"2021-07-22T08:24:31.915036Z","iopub.status.idle":"2021-07-22T08:24:31.945199Z","shell.execute_reply.started":"2021-07-22T08:24:31.914989Z","shell.execute_reply":"2021-07-22T08:24:31.944069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%rm -rf runs\n%rm -rf test_384x384\n%rm yolov5L512_1607.pt\n%rm yolov5m6_512.pt","metadata":{},"execution_count":null,"outputs":[]}]}