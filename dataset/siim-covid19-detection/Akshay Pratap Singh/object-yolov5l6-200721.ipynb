{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Reference to load DICOM Images: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\nand https://www.kaggle.com/tanlikesmath/siim-covid-19-detection-a-simple-eda","metadata":{"gradient":{"editing":false}}},{"cell_type":"code","source":"!conda install -c conda-forge gdcm -y >> /dev/null\n!pip install timm\n!pip install wandb","metadata":{"gradient":{"editing":false},"execution":{"iopub.status.busy":"2021-07-29T13:01:18.048204Z","iopub.execute_input":"2021-07-29T13:01:18.049048Z","iopub.status.idle":"2021-07-29T13:02:55.675265Z","shell.execute_reply.started":"2021-07-29T13:01:18.048918Z","shell.execute_reply":"2021-07-29T13:02:55.673906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wandb login 528ac788f9f5a417bf1f94f3f1423607eb82d84c","metadata":{"execution":{"iopub.status.busy":"2021-07-29T13:03:45.95281Z","iopub.execute_input":"2021-07-29T13:03:45.953255Z","iopub.status.idle":"2021-07-29T13:03:48.975874Z","shell.execute_reply.started":"2021-07-29T13:03:45.953217Z","shell.execute_reply":"2021-07-29T13:03:48.974801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification Training Script\nThis script is written to be modified and facilitate different types of experiments.","metadata":{"gradient":{"editing":false}}},{"cell_type":"code","source":"import glob\nimport os\nimport time\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport PIL\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom tqdm.contrib.concurrent import process_map\n\nimport torch\nimport torchvision\nfrom torch.utils.data.dataset import Dataset\n\nimport timm\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nDATA_DIR = \"/kaggle/input/siim-covid19-detection\"\nRESIZE_DIR = \"/kaggle/input/resized/\"\nSIZE = (512, 512)\nFOLDS = 5\nEPOCHS = 10\nNUM_CLASSES = 4\nBATCHSIZE = 24","metadata":{"gradient":{"editing":false},"execution":{"iopub.status.busy":"2021-07-29T12:16:43.907802Z","iopub.execute_input":"2021-07-29T12:16:43.90826Z","iopub.status.idle":"2021-07-29T12:16:43.918102Z","shell.execute_reply.started":"2021-07-29T12:16:43.908203Z","shell.execute_reply":"2021-07-29T12:16:43.916924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:39:42.152221Z","iopub.execute_input":"2021-07-29T12:39:42.152629Z","iopub.status.idle":"2021-07-29T12:40:12.226794Z","shell.execute_reply.started":"2021-07-29T12:39:42.152586Z","shell.execute_reply":"2021-07-29T12:40:12.226019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean column names in CSV's\nWe clean up a few column names in the `train_image_level.csv` and the `train_study_level.csv`, to merge these two csvs into one.\nWe also rename the columns to simplified names to use later on.","metadata":{"gradient":{"editing":false}}},{"cell_type":"code","source":"train_images_df = pd.read_csv(os.path.join(DATA_DIR, 'train_image_level.csv'))\ntrain_study_df = pd.read_csv(os.path.join(DATA_DIR, 'train_study_level.csv'))\ntrain_images_df['StudyInstanceUID'] = train_images_df['StudyInstanceUID'] + \"_study\"\n\ntrain_study_df.columns = train_study_df.columns.map(lambda x: x.split(' ')[0])\n\ntrain_study_df.rename(columns={\"id\":\"study_id\"}, inplace=True)\ntrain_images_df.rename(columns={\"StudyInstanceUID\":\"study_id\"}, inplace=True)","metadata":{"gradient":{"editing":false},"execution":{"iopub.status.busy":"2021-07-29T11:29:41.228704Z","iopub.execute_input":"2021-07-29T11:29:41.229089Z","iopub.status.idle":"2021-07-29T11:29:41.310043Z","shell.execute_reply.started":"2021-07-29T11:29:41.229054Z","shell.execute_reply":"2021-07-29T11:29:41.30913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Label Map Creation\nAt prediction time, we will need these labels in string form. These maps can also be modified to train different types of models, like Binary classification.","metadata":{"gradient":{"editing":false}}},{"cell_type":"code","source":"if NUM_CLASSES == 4:\n    NAME_TO_LABEL_MAP = {\n                \"Negative\":0,\n                \"Typical\":1,\n                \"Indeterminate\":2,\n                \"Atypical\":3\n    }\n\n\ndef get_str_label(row):\n    for k in NAME_TO_LABEL_MAP:\n            if row[k]:\n                return k\n    return None\ndef get_int_label(row):\n    for k in NAME_TO_LABEL_MAP:\n            if row[k]:\n                return NAME_TO_LABEL_MAP[k]\n    return None","metadata":{"gradient":{},"execution":{"iopub.status.busy":"2021-07-29T11:29:50.884341Z","iopub.execute_input":"2021-07-29T11:29:50.884713Z","iopub.status.idle":"2021-07-29T11:29:50.891716Z","shell.execute_reply.started":"2021-07-29T11:29:50.88468Z","shell.execute_reply":"2021-07-29T11:29:50.890552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Group K Fold\nWe create a `fold` column to be used while we run cross validation.\nWe choose the `study_id` column created above to split into the number of groups defined by the `FOLDS` variable.","metadata":{}},{"cell_type":"code","source":"train_study_df[\"int_label\"] = train_study_df.apply(get_int_label, axis=1)\ntrain_study_df[\"str_label\"] = train_study_df.apply(get_str_label, axis=1)\n\ngkf  = GroupKFold(n_splits = FOLDS)\ntrain_study_df['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(train_study_df, groups = train_study_df[\"study_id\"].tolist())):\n    train_study_df.loc[val_idx, 'fold'] = fold","metadata":{"gradient":{},"execution":{"iopub.status.busy":"2021-07-29T11:29:51.835481Z","iopub.execute_input":"2021-07-29T11:29:51.835843Z","iopub.status.idle":"2021-07-29T11:29:52.080773Z","shell.execute_reply.started":"2021-07-29T11:29:51.835812Z","shell.execute_reply":"2021-07-29T11:29:52.079918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting Group Counts\nWe see from the plots below that each group is fairly balanced. The `Typical` category is the most common followed by `Negative`.","metadata":{}},{"cell_type":"code","source":"sns.catplot(x = \"str_label\", col=\"fold\", data=train_study_df, kind=\"count\")","metadata":{"gradient":{},"execution":{"iopub.status.busy":"2021-07-29T11:29:52.749371Z","iopub.execute_input":"2021-07-29T11:29:52.749834Z","iopub.status.idle":"2021-07-29T11:29:53.99735Z","shell.execute_reply.started":"2021-07-29T11:29:52.749776Z","shell.execute_reply":"2021-07-29T11:29:53.996269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clean Study level data\n1. As per the recommendations made [here](https://www.kaggle.com/c/siim-covid19-detection/discussion/246597) for studies with more than one image it appears that there only one which has bounding boxes. \n\n2. As per the recommendation made [here](https://www.kaggle.com/c/siim-covid19-detection/discussion/240250#1351079), the label for only the one with bounding boxes is retained since the other images were not looked at by the annotators.\n\n3. For studies that have more than one image but no bounding boxes associated with them, it is unclear as to which image was looked at therefore all images are retained in those studies.","metadata":{}},{"cell_type":"code","source":"train_samples_df = pd.merge(train_images_df, train_study_df, on=\"study_id\", how=\"inner\").reset_index(drop=True)\n\nbox_and_images_counts_df = train_samples_df.groupby(\"study_id\")[[\"id\", \"boxes\"]].count().sort_values(ascending=False, by=\"id\").reset_index()\nbox_and_images_counts_df.rename(columns={\n    \"id\":\"id_count\", \"boxes\":\"boxes_count\"\n}, inplace=True)","metadata":{"gradient":{},"execution":{"iopub.status.busy":"2021-07-29T11:29:53.99924Z","iopub.execute_input":"2021-07-29T11:29:53.999687Z","iopub.status.idle":"2021-07-29T11:29:54.033606Z","shell.execute_reply.started":"2021-07-29T11:29:53.999646Z","shell.execute_reply":"2021-07-29T11:29:54.032658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=box_and_images_counts_df, x=\"id_count\")","metadata":{"gradient":{},"execution":{"iopub.status.busy":"2021-07-29T11:29:54.035466Z","iopub.execute_input":"2021-07-29T11:29:54.035883Z","iopub.status.idle":"2021-07-29T11:29:54.207862Z","shell.execute_reply.started":"2021-07-29T11:29:54.035837Z","shell.execute_reply":"2021-07-29T11:29:54.206824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_samples_df = pd.merge(train_samples_df, box_and_images_counts_df, how=\"inner\", on=\"study_id\")","metadata":{"gradient":{},"execution":{"iopub.status.busy":"2021-07-29T11:29:54.209639Z","iopub.execute_input":"2021-07-29T11:29:54.210011Z","iopub.status.idle":"2021-07-29T11:29:54.224544Z","shell.execute_reply.started":"2021-07-29T11:29:54.209976Z","shell.execute_reply":"2021-07-29T11:29:54.223533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_samples_df.sort_values([\"id_count\", \"boxes_count\"], ascending=False, inplace=True)\ntrain_samples_df.head(5)\n","metadata":{"gradient":{},"execution":{"iopub.status.busy":"2021-07-29T11:29:54.435457Z","iopub.execute_input":"2021-07-29T11:29:54.435817Z","iopub.status.idle":"2021-07-29T11:29:54.458548Z","shell.execute_reply.started":"2021-07-29T11:29:54.43579Z","shell.execute_reply":"2021-07-29T11:29:54.457663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(FOLDS):\n    os.makedirs(f'/kaggle/tmp/covid/images/train{fold}', exist_ok=True)\n    os.makedirs(f'/kaggle/tmp/covid/images/valid{fold}', exist_ok=True)\n\n    os.makedirs(f'/kaggle/tmp/covid/labels/train{fold}', exist_ok=True)\n    os.makedirs(f'/kaggle/tmp/covid/labels/valid{fold}', exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:29:55.155394Z","iopub.execute_input":"2021-07-29T11:29:55.155754Z","iopub.status.idle":"2021-07-29T11:29:55.163981Z","shell.execute_reply.started":"2021-07-29T11:29:55.155723Z","shell.execute_reply":"2021-07-29T11:29:55.16311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data\n\ndef resize_xray(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail(size, resample)\n    else:\n        im = im.resize(size, resample)\n    \n    return im","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:31:40.033188Z","iopub.execute_input":"2021-07-29T11:31:40.033556Z","iopub.status.idle":"2021-07-29T11:31:40.041176Z","shell.execute_reply.started":"2021-07-29T11:31:40.033502Z","shell.execute_reply":"2021-07-29T11:31:40.040241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_id = []\ndim0 = []\ndim1 = []\nsave_dir = f'/kaggle/working/train/'\n\nos.makedirs(save_dir, exist_ok=True)\n\nfor dirname, _, filenames in tqdm(os.walk(f'/kaggle/input/siim-covid19-detection/train')):\n    for file in filenames:\n        # set keep_ratio=True to have original aspect ratio\n        xray = read_xray(os.path.join(dirname, file))\n        im = resize_xray(xray, SIZE)\n        im = np.array(im)\n        cv2.imwrite(os.path.join(save_dir, file.replace('dcm', 'jpg')),im)\n\n        image_id.append(file.replace('.dcm', ''))\n        dim0.append(xray.shape[0])\n        dim1.append(xray.shape[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-29T11:32:39.035063Z","iopub.execute_input":"2021-07-29T11:32:39.035474Z","iopub.status.idle":"2021-07-29T12:16:42.241132Z","shell.execute_reply.started":"2021-07-29T11:32:39.035441Z","shell.execute_reply":"2021-07-29T12:16:42.240139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, img in enumerate(image_id):\n    image_id[i] = img + '_image'\nmeta = pd.DataFrame(list(zip(image_id, dim0, dim1)),columns =['id', 'dim0', 'dim1'])","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:23:16.256086Z","iopub.execute_input":"2021-07-29T12:23:16.25644Z","iopub.status.idle":"2021-07-29T12:23:16.272613Z","shell.execute_reply.started":"2021-07-29T12:23:16.256406Z","shell.execute_reply":"2021-07-29T12:23:16.271714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_samples_df = pd.merge(train_samples_df, meta, how=\"inner\", on=\"id\")","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:23:16.738646Z","iopub.execute_input":"2021-07-29T12:23:16.738982Z","iopub.status.idle":"2021-07-29T12:23:16.75463Z","shell.execute_reply.started":"2021-07-29T12:23:16.738951Z","shell.execute_reply":"2021-07-29T12:23:16.753614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from shutil import copyfile\nignoreList = []\nwith open('/kaggle/input/ignorelist/ignore.txt','r') as f:\n    ignoreList = f.readlines()\n    \nfor fold in range(FOLDS):\n# Move the images to relevant split folder.\n    for i in tqdm(range(len(train_samples_df))):\n        row = train_samples_df.loc[i]\n        if row.id in ignoreList:\n            continue\n        if row.fold != fold:\n            copyfile(f'/kaggle/working/train/{row.id.split(\"_\")[0]}.jpg', f'/kaggle/tmp/covid/images/train{fold}/{row.id.split(\"_\")[0]}.jpg')\n        else:\n            copyfile(f'/kaggle/working/train/{row.id.split(\"_\")[0]}.jpg', f'/kaggle/tmp/covid/images/valid{fold}/{row.id.split(\"_\")[0]}.jpg')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:23:19.630605Z","iopub.execute_input":"2021-07-29T12:23:19.630936Z","iopub.status.idle":"2021-07-29T12:23:36.699847Z","shell.execute_reply.started":"2021-07-29T12:23:19.630905Z","shell.execute_reply":"2021-07-29T12:23:36.698739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5  \n%cd yolov5\n\n%pip install -qr requirements.txt  \n\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:23:36.701532Z","iopub.execute_input":"2021-07-29T12:23:36.701875Z","iopub.status.idle":"2021-07-29T12:23:50.650434Z","shell.execute_reply.started":"2021-07-29T12:23:36.701837Z","shell.execute_reply":"2021-07-29T12:23:50.64924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yaml\nfor fold in range(FOLDS): \n    data_yaml = dict(\n        train = f'/kaggle/tmp/covid/images/train{fold}',\n        val = f'/kaggle/tmp/covid/images/valid{fold}',\n        nc = 1,\n        names = ['opacity']\n    )\n    # Note that I am creating the file in the yolov5/data/ directory.\n    with open(f'/kaggle/working/yolov5/data/data{fold}.yaml', 'w') as outfile:\n        yaml.dump(data_yaml, outfile, default_flow_style=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:23:50.652942Z","iopub.execute_input":"2021-07-29T12:23:50.653631Z","iopub.status.idle":"2021-07-29T12:23:50.66485Z","shell.execute_reply.started":"2021-07-29T12:23:50.653575Z","shell.execute_reply":"2021-07-29T12:23:50.663781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_bbox(row):\n    bboxes = []\n    bbox = []\n    for i, l in enumerate(row.label.split(' ')):\n        if (i % 6 == 0) | (i % 6 == 1):\n            continue\n        bbox.append(float(l))\n        if i % 6 == 5:\n            bboxes.append(bbox)\n            bbox = []  \n            \n    return bboxes\n\n# Scale the bounding boxes according to the size of the resized image. \ndef scale_bbox(row, bboxes):\n    # Get scaling factor\n    scale_x = SIZE[1]/row.dim1\n    scale_y = SIZE[0]/row.dim0\n    \n    scaled_bboxes = []\n    for bbox in bboxes:\n        x = int(np.round(bbox[0]*scale_x, 4))\n        y = int(np.round(bbox[1]*scale_y, 4))\n        x1 = int(np.round(bbox[2]*scale_x, 4))\n        y1 = int(np.round(bbox[3]*scale_y, 4))\n\n        scaled_bboxes.append([x, y, x1, y1]) # xmin, ymin, xmax, ymax\n        \n    return scaled_bboxes\n\n# Convert the bounding boxes in YOLO format.\ndef get_yolo_format_bbox(img_w, img_h, bboxes):\n    yolo_boxes = []\n    for bbox in bboxes:\n        w = bbox[2] - bbox[0] # xmax - xmin\n        h = bbox[3] - bbox[1] # ymax - ymin\n        xc = bbox[0] + int(np.round(w/2)) # xmin + width/2\n        yc = bbox[1] + int(np.round(h/2)) # ymin + height/2\n        \n        yolo_boxes.append([xc/img_w, yc/img_h, w/img_w, h/img_h]) # x_center y_center width height\n    \n    return yolo_boxes","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:23:50.666583Z","iopub.execute_input":"2021-07-29T12:23:50.667045Z","iopub.status.idle":"2021-07-29T12:23:50.67917Z","shell.execute_reply.started":"2021-07-29T12:23:50.667004Z","shell.execute_reply":"2021-07-29T12:23:50.678182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!rm /kaggle/tmp/covid/labels/valid0/*","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:46:43.6067Z","iopub.execute_input":"2021-07-29T12:46:43.607066Z","iopub.status.idle":"2021-07-29T12:46:44.342597Z","shell.execute_reply.started":"2021-07-29T12:46:43.607028Z","shell.execute_reply":"2021-07-29T12:46:44.341487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(FOLDS):\n    for i in tqdm(range(len(train_samples_df))):\n        row = train_samples_df.loc[i]\n        # Get image id\n        if row.id in ignoreList:\n            continue\n        img_id = row.id.split(\"_\")[0]\n        # Check if bboxes exist\n        bbx_count = row.boxes_count\n\n        if row.fold != fold:\n            file_name = f'/kaggle/tmp/covid/labels/train{fold}/{img_id}.txt'\n        else:\n            file_name = f'/kaggle/tmp/covid/labels/valid{fold}/{img_id}.txt'\n\n\n        if bbx_count>0:\n            # Get bboxes\n            bboxes = get_bbox(row)\n            # Scale bounding boxes\n            scale_bboxes = scale_bbox(row, bboxes)\n            # Format for YOLOv5\n            yolo_bboxes = get_yolo_format_bbox(SIZE[0],SIZE[1], scale_bboxes)\n\n            with open(file_name, 'w') as f:\n                for bbox in yolo_bboxes:\n                    bbox = [0]+bbox\n                    bbox = [str(i) for i in bbox]\n                    bbox = ' '.join(bbox)\n                    f.write(bbox)\n                    f.write('\\n')\n        else:\n            with open(file_name, 'w') as f:\n                f.write(\"\")\n            ","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:46:45.611095Z","iopub.execute_input":"2021-07-29T12:46:45.611445Z","iopub.status.idle":"2021-07-29T12:47:02.612952Z","shell.execute_reply.started":"2021-07-29T12:46:45.611408Z","shell.execute_reply":"2021-07-29T12:47:02.611683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold = 0\n# data_file = 'data'+str(fold)+'.yaml'\n# !python train.py --img {SIZE[0]} \\\n#                  --batch {BATCHSIZE} \\\n#                  --epochs {EPOCHS} \\\n#                  --data {data_file} \\\n#                  --weights yolov5l6.pt \\\n#                  --evolve \\\n#                  --cache","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:48:59.287862Z","iopub.execute_input":"2021-07-29T12:48:59.288211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold = 1\ndata_file = 'data'+str(fold)+'.yaml'\n!python train.py --img {SIZE[0]} \\\n                 --batch {BATCHSIZE} \\\n                 --epochs {EPOCHS} \\\n                 --data {data_file} \\\n                 --weights yolov5l6.pt \\\n                 --evolve \\\n                 --cache","metadata":{"execution":{"iopub.status.busy":"2021-07-29T12:48:52.063707Z","iopub.execute_input":"2021-07-29T12:48:52.064068Z","iopub.status.idle":"2021-07-29T12:48:52.798162Z","shell.execute_reply.started":"2021-07-29T12:48:52.064029Z","shell.execute_reply":"2021-07-29T12:48:52.797178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold = 2\n# data_file = 'data'+str(fold)+'.yaml'\n# !python train.py --img {SIZE[0]} \\\n#                  --batch {BATCHSIZE} \\\n#                  --epochs {EPOCHS} \\\n#                  --data {data_file} \\\n#                  --weights yolov5l6.pt \\\n#                  --evolve \\\n#                  --cache","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold = 3\n# data_file = 'data'+str(fold)+'.yaml'\n# !python train.py --img {SIZE[0]} \\\n#                  --batch {BATCHSIZE} \\\n#                  --epochs {EPOCHS} \\\n#                  --data {data_file} \\\n#                  --weights yolov5l6.pt \\\n#                  --evolve \\\n#                  --cache","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fold = 4\n# data_file = 'data'+str(fold)+'.yaml'\n# !python train.py --img {SIZE[0]} \\\n#                  --batch {BATCHSIZE} \\\n#                  --epochs {EPOCHS} \\\n#                  --data {data_file} \\\n#                  --weights yolov5l6.pt \\\n#                  --evolve \\\n#                  --cache","metadata":{},"execution_count":null,"outputs":[]}]}