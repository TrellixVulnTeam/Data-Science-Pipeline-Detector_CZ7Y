{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!conda install '/kaggle/input/pydicom-conda-helper/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:51:07.26971Z","iopub.execute_input":"2021-07-28T07:51:07.269973Z","iopub.status.idle":"2021-07-28T07:52:15.898929Z","shell.execute_reply.started":"2021-07-28T07:51:07.269916Z","shell.execute_reply":"2021-07-28T07:52:15.897854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys; \n\npackage_paths = [\n    '../input/timm-pytorch-image-models/pytorch-image-models-master',\n]\n\nfor pth in package_paths:\n    sys.path.append(pth)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-28T07:52:15.900745Z","iopub.execute_input":"2021-07-28T07:52:15.9011Z","iopub.status.idle":"2021-07-28T07:52:15.909597Z","shell.execute_reply.started":"2021-07-28T07:52:15.90106Z","shell.execute_reply":"2021-07-28T07:52:15.908726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport os\nimport time\nimport random\n\nimport numpy as np  # linear algebra!\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport PIL\n\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\nfrom tqdm.contrib.concurrent import process_map\nfrom multiprocessing import Pool, cpu_count\n\nimport torch\nimport torchvision\nfrom torch.utils.data.dataset import Dataset\nimport torch.cuda.amp as amp\n\nimport timm\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nSIZE = (384, 384)\n\nMODEL_DIR = \"/kaggle/input/classification-training-script\"\nDATA_DIR = RESIZE_DIR = \"/kaggle/working/\"\nFOLDS = 5\nNUM_CLASSES = 4\nBATCHSIZE = 64\nSEED = 420\nMODEL_NAME = \"tf_efficientnetv2_s\"","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:56:41.194044Z","iopub.execute_input":"2021-07-28T07:56:41.194398Z","iopub.status.idle":"2021-07-28T07:56:41.202739Z","shell.execute_reply.started":"2021-07-28T07:56:41.194366Z","shell.execute_reply":"2021-07-28T07:56:41.20175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\nprint(len(sub_df))\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:52:21.048674Z","iopub.execute_input":"2021-07-28T07:52:21.049027Z","iopub.status.idle":"2021-07-28T07:52:21.086974Z","shell.execute_reply.started":"2021-07-28T07:52:21.048992Z","shell.execute_reply":"2021-07-28T07:52:21.085864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_df = sub_df.loc[sub_df.id.str.contains('_study')]\nlen(study_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:52:21.088594Z","iopub.execute_input":"2021-07-28T07:52:21.088999Z","iopub.status.idle":"2021-07-28T07:52:21.1027Z","shell.execute_reply.started":"2021-07-28T07:52:21.08896Z","shell.execute_reply":"2021-07-28T07:52:21.101891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_df = sub_df.loc[sub_df.id.str.contains('_image')]\nlen(image_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:52:21.103649Z","iopub.execute_input":"2021-07-28T07:52:21.103891Z","iopub.status.idle":"2021-07-28T07:52:21.116Z","shell.execute_reply.started":"2021-07-28T07:52:21.103857Z","shell.execute_reply":"2021-07-28T07:52:21.115194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data\n\ndef resize_xray(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:52:21.117363Z","iopub.execute_input":"2021-07-28T07:52:21.117713Z","iopub.status.idle":"2021-07-28T07:52:21.127217Z","shell.execute_reply.started":"2021-07-28T07:52:21.117675Z","shell.execute_reply":"2021-07-28T07:52:21.12604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_PATH = f'/kaggle/working/test_{SIZE[0]}x{SIZE[1]}'\n\nos.makedirs(TEST_PATH, exist_ok=True)\nfilenames = glob.glob(f'/kaggle/input/siim-covid19-detection/test/*/*/*.dcm')\n\ndef persist_image(path):\n    xray = read_xray(path)\n    im = resize_xray(xray, size=SIZE[0])\n    fname = os.path.basename(os.path.splitext(path)[-2])\n    jpg_fname = os.path.join(TEST_PATH, \"{}.jpg\".format(fname))\n    im.save(jpg_fname)\n    return [fname,xray.shape[0],xray.shape[1]]\nwith Pool(cpu_count()) as pool:\n    img_metadata = pool.map(persist_image,filenames)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:56:44.701916Z","iopub.execute_input":"2021-07-28T07:56:44.702265Z","iopub.status.idle":"2021-07-28T08:01:25.849108Z","shell.execute_reply.started":"2021-07-28T07:56:44.702234Z","shell.execute_reply":"2021-07-28T08:01:25.848046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of test images: {len(os.listdir(TEST_PATH))}')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:25.851607Z","iopub.execute_input":"2021-07-28T08:01:25.851949Z","iopub.status.idle":"2021-07-28T08:01:25.860789Z","shell.execute_reply.started":"2021-07-28T08:01:25.851908Z","shell.execute_reply":"2021-07-28T08:01:25.859562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imgs_study_mapping = pd.DataFrame(img_metadata,columns = ['image_id','dim0','dim1'])\n\n# Associate image-level id with study-level ids.\n# Note that a study-level might have more than one image-level ids.\nfor study_dir in os.listdir('../input/siim-covid19-detection/test'):\n    for series in os.listdir(f'../input/siim-covid19-detection/test/{study_dir}'):\n        for image in os.listdir(f'../input/siim-covid19-detection/test/{study_dir}/{series}/'):\n            image_id = image[:-4]\n            test_imgs_study_mapping.loc[test_imgs_study_mapping['image_id'] == image_id, 'study_id'] = study_dir\n        \ntest_imgs_study_mapping.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:25.862884Z","iopub.execute_input":"2021-07-28T08:01:25.863355Z","iopub.status.idle":"2021-07-28T08:01:27.817641Z","shell.execute_reply.started":"2021-07-28T08:01:25.863321Z","shell.execute_reply":"2021-07-28T08:01:27.816877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir \"/kaggle/working/test_{SIZE[0]}x{SIZE[1]}\"\n# !tar -xzf \"/kaggle/input/train-{SIZE[0]}x{SIZE[1]}/test_{SIZE[0]}x{SIZE[1]}.tar.gz\" -C \"/kaggle/working/test_{SIZE[0]}x{SIZE[1]}\" .","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:27.819126Z","iopub.execute_input":"2021-07-28T08:01:27.819424Z","iopub.status.idle":"2021-07-28T08:01:27.822393Z","shell.execute_reply.started":"2021-07-28T08:01:27.819394Z","shell.execute_reply":"2021-07-28T08:01:27.821595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make results reproducible\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:27.823618Z","iopub.execute_input":"2021-07-28T08:01:27.824149Z","iopub.status.idle":"2021-07-28T08:01:27.857182Z","shell.execute_reply.started":"2021-07-28T08:01:27.824098Z","shell.execute_reply":"2021-07-28T08:01:27.856514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_pattern = f\"{MODEL_NAME}\"\nmodelnames = glob.glob(os.path.join(MODEL_DIR, f\"{MODEL_NAME}*\", \"*.pth\"))\nprint(f\"{len(modelnames)}: Models Found\")","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:27.85822Z","iopub.execute_input":"2021-07-28T08:01:27.858541Z","iopub.status.idle":"2021-07-28T08:01:27.900684Z","shell.execute_reply.started":"2021-07-28T08:01:27.858507Z","shell.execute_reply":"2021-07-28T08:01:27.899991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_imgs_study_mapping = pd.read_csv(\"/kaggle/input/test-image-to-study-mapping/test_study_id_to_image_mapping.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:27.901792Z","iopub.execute_input":"2021-07-28T08:01:27.90215Z","iopub.status.idle":"2021-07-28T08:01:27.905663Z","shell.execute_reply.started":"2021-07-28T08:01:27.902103Z","shell.execute_reply":"2021-07-28T08:01:27.904855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TEST_DATA_PATH = os.path.join(RESIZE_DIR, f\"test_{SIZE[0]}x{SIZE[1]}\")\nprint(\"Test Data Path {}\".format(TEST_PATH))\ndef get_img_path(row):\n    study_id = row[\"study_id\"]\n    img_id = row[\"image_id\"]\n    paths = glob.glob(os.path.join(TEST_PATH, \"{}*.jpg\".format(img_id)))\n    for path in paths:\n        if img_id in path:\n            return path\n    return None","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:27.908578Z","iopub.execute_input":"2021-07-28T08:01:27.909235Z","iopub.status.idle":"2021-07-28T08:01:27.919607Z","shell.execute_reply.started":"2021-07-28T08:01:27.909197Z","shell.execute_reply":"2021-07-28T08:01:27.918531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imgs_study_mapping[\"path\"] = test_imgs_study_mapping.apply(get_img_path, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:27.921675Z","iopub.execute_input":"2021-07-28T08:01:27.922019Z","iopub.status.idle":"2021-07-28T08:01:30.635844Z","shell.execute_reply.started":"2021-07-28T08:01:27.921987Z","shell.execute_reply":"2021-07-28T08:01:30.634899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imgs_study_mapping[test_imgs_study_mapping[\"path\"].isna()]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:30.637165Z","iopub.execute_input":"2021-07-28T08:01:30.637739Z","iopub.status.idle":"2021-07-28T08:01:30.658397Z","shell.execute_reply.started":"2021-07-28T08:01:30.637699Z","shell.execute_reply":"2021-07-28T08:01:30.657107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(modelname):\n    summary = torch.load(modelname)\n    print(f\"Loaded model {modelname}\")\n    print(f\"Epoch {summary['epoch']}\")\n    print(f\"Map@2 {summary['map_at_2']}\")\n    print(f\"AUC@2 {summary['auc']}\")\n    model = timm.create_model(model_name=MODEL_NAME, pretrained=False, in_chans=3)\n    model.classifier = torch.nn.Linear(\n        in_features=model.classifier.in_features, out_features=NUM_CLASSES\n    )\n    model.load_state_dict(summary[\"state_dict\"], strict=True)\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:30.662084Z","iopub.execute_input":"2021-07-28T08:01:30.664163Z","iopub.status.idle":"2021-07-28T08:01:30.672167Z","shell.execute_reply.started":"2021-07-28T08:01:30.664111Z","shell.execute_reply":"2021-07-28T08:01:30.671457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class XRayDatasetFromDF(Dataset):\n    def __init__(self, df, train=True, augment=True, normalize=False, size=(384, 384)):\n        self.df = df\n        self.name_to_label_map = {\n            \"Negative\": 0,\n            \"Typical\": 1,\n            \"Indeterminate\": 2,\n            \"Atypical\": 3,\n        }\n        self.study_ids = df.index.sort_values()\n        self.path_suffix = (\n            os.path.join(DATA_DIR, \"train\") if train else os.path.join(DATA_DIR, \"test\")\n        )\n        self._train = train\n        self._augment = augment\n        self._normalize = normalize\n        self._size = size\n        self._transform_list = [\n            # A.Resize(size[0], size[1], p=1)\n        ]\n\n        if self._augment:\n            self._transform_list.extend(\n                [\n                    A.VerticalFlip(p=0.5),\n                    A.HorizontalFlip(p=0.5),\n                    A.ShiftScaleRotate(\n                        scale_limit=0.20,\n                        rotate_limit=10,\n                        shift_limit=0.1,\n                        p=0.5,\n                        border_mode=cv2.BORDER_CONSTANT,\n                        value=0,\n                    ),\n                    A.RandomBrightnessContrast(p=0.5),\n                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                    ToTensorV2(),\n                ]\n            )\n        elif self._normalize and not self._augment:  # test mode\n            self._transform_list.extend(\n                [\n                    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n                    ToTensorV2(),\n                ]\n            )\n        self._transforms = A.Compose(self._transform_list)\n\n    def __len__(self):\n        return len(self.study_ids)\n\n    def assign_label(self, row):\n        for k in self.name_to_label_map:\n            if row[k]:\n                return self.name_to_label_map[k]\n\n    def __getitem__(self, idx):\n        study_id = self.study_ids[idx]\n        study_imgs = self.df.loc[study_id]\n        path = None\n        label = None\n\n        path = study_imgs[\"path\"]\n        label = study_imgs[\"int_label\"] if self._train else -1\n\n        # ideally, we'd clean up the df,\n        # but may be we use it to produce predictions as well.\n        dicom_arr = (\n            cv2.imread(path)\n            if path.endswith(\".jpg\")\n            else dicom2array(path, size=self._size)\n        )\n        img = cv2.cvtColor(dicom_arr, cv2.COLOR_BGR2RGB)\n        img = self._transforms(image=img)[\"image\"]\n\n        return img, label","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:30.676316Z","iopub.execute_input":"2021-07-28T08:01:30.678511Z","iopub.status.idle":"2021-07-28T08:01:30.698688Z","shell.execute_reply.started":"2021-07-28T08:01:30.678474Z","shell.execute_reply":"2021-07-28T08:01:30.697751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = XRayDatasetFromDF(df=test_imgs_study_mapping, train=False, augment=True, normalize=False, size=SIZE)\ntest_dl = torch.utils.data.DataLoader(\n        dataset=test_ds,\n        batch_size=BATCHSIZE * 2,\n        pin_memory=True,\n        num_workers=8,\n        drop_last=False,\n        shuffle=False,\n        prefetch_factor=8,\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:30.702723Z","iopub.execute_input":"2021-07-28T08:01:30.704872Z","iopub.status.idle":"2021-07-28T08:01:30.713728Z","shell.execute_reply.started":"2021-07-28T08:01:30.704831Z","shell.execute_reply":"2021-07-28T08:01:30.712804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, slide_dl, tta_times=10):\n    sample_size = len(slide_dl.dataset)\n    print(\"Predicting on {} Images {} times\".format(sample_size, tta_times))\n    probs = np.zeros((sample_size, NUM_CLASSES))\n    loss_sum = 0\n\n    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\").to(dev)\n\n    for i in range(tta_times):\n\n        offset = 0\n\n        for i, grid in enumerate(tqdm(slide_dl)):\n            with torch.no_grad():\n                img, _ = grid\n                curr_batch_size = img.shape[0]\n\n                pred = model(img.to(dev))\n                # remove the redundant dimension added by\n                # pytorch's collate_fn\n\n                prob = pred.softmax(dim=1)\n                probs[offset : offset + curr_batch_size, :] += prob.cpu().numpy()\n                offset += curr_batch_size\n\n\n    return probs / tta_times","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:30.718704Z","iopub.execute_input":"2021-07-28T08:01:30.721202Z","iopub.status.idle":"2021-07-28T08:01:30.731118Z","shell.execute_reply.started":"2021-07-28T08:01:30.721164Z","shell.execute_reply":"2021-07-28T08:01:30.730273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(dev)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:30.735215Z","iopub.execute_input":"2021-07-28T08:01:30.73734Z","iopub.status.idle":"2021-07-28T08:01:30.858538Z","shell.execute_reply.started":"2021-07-28T08:01:30.7373Z","shell.execute_reply":"2021-07-28T08:01:30.85687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor modelname in modelnames:\n    model = load_model(modelname)\n    model = model.to(dev)\n    model = model.eval()\n    probs = predict(model, test_dl)\n    # probs = torch.from_numpy(probs)\n    # probs = probs.softmax(dim=1)\n\n    for k in test_ds.name_to_label_map:\n        col_idx = test_ds.name_to_label_map[k]\n        test_imgs_study_mapping[k] = probs[:, col_idx]\n    test_imgs_study_mapping[\"modelname\"] = modelname\n    predictions.append(test_imgs_study_mapping.copy())","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:01:30.859914Z","iopub.execute_input":"2021-07-28T08:01:30.860281Z","iopub.status.idle":"2021-07-28T08:28:14.166666Z","shell.execute_reply.started":"2021-07-28T08:01:30.860241Z","shell.execute_reply":"2021-07-28T08:28:14.165124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df = pd.concat(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:14.169641Z","iopub.execute_input":"2021-07-28T08:28:14.170072Z","iopub.status.idle":"2021-07-28T08:28:14.199235Z","shell.execute_reply.started":"2021-07-28T08:28:14.170022Z","shell.execute_reply":"2021-07-28T08:28:14.198387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_predictions_df = predictions_df.groupby(\"study_id\").agg({\n    \"Negative\":\"mean\",\n    \"Typical\":\"mean\",\n    \"Indeterminate\":\"mean\",\n    \"Atypical\":\"mean\"\n}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:14.200645Z","iopub.execute_input":"2021-07-28T08:28:14.201045Z","iopub.status.idle":"2021-07-28T08:28:14.234173Z","shell.execute_reply.started":"2021-07-28T08:28:14.201Z","shell.execute_reply":"2021-07-28T08:28:14.233333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_predictions_df[\"id\"] = mean_predictions_df[\"study_id\"] + \"_study\"","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:14.235505Z","iopub.execute_input":"2021-07-28T08:28:14.235886Z","iopub.status.idle":"2021-07-28T08:28:14.33023Z","shell.execute_reply.started":"2021-07-28T08:28:14.235848Z","shell.execute_reply":"2021-07-28T08:28:14.329426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_predictions_df","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:14.331663Z","iopub.execute_input":"2021-07-28T08:28:14.332274Z","iopub.status.idle":"2021-07-28T08:28:14.35696Z","shell.execute_reply.started":"2021-07-28T08:28:14.332235Z","shell.execute_reply":"2021-07-28T08:28:14.356091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OPBB = \"0 0 1 1\"\ndef generate_cls_prediction_strings(row):\n    predictions = []\n\n    for k in test_ds.name_to_label_map:\n        \n        p_k = row[k]\n        predictions.append(k.lower())\n        predictions.append(str(p_k))\n        predictions.append(OPBB)\n    return \" \".join(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:14.358374Z","iopub.execute_input":"2021-07-28T08:28:14.35876Z","iopub.status.idle":"2021-07-28T08:28:14.365883Z","shell.execute_reply.started":"2021-07-28T08:28:14.358722Z","shell.execute_reply":"2021-07-28T08:28:14.364244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_predictions_df[\"PredictionString\"] = mean_predictions_df.apply(generate_cls_prediction_strings, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:14.369611Z","iopub.execute_input":"2021-07-28T08:28:14.369952Z","iopub.status.idle":"2021-07-28T08:28:14.420023Z","shell.execute_reply.started":"2021-07-28T08:28:14.369924Z","shell.execute_reply":"2021-07-28T08:28:14.419277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_submission_df = mean_predictions_df[[\"id\", \"PredictionString\"]]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:14.424506Z","iopub.execute_input":"2021-07-28T08:28:14.424756Z","iopub.status.idle":"2021-07-28T08:28:14.430098Z","shell.execute_reply.started":"2021-07-28T08:28:14.424731Z","shell.execute_reply":"2021-07-28T08:28:14.429069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n\nimport gc\nimport glob\nfrom tqdm import tqdm\nfrom shutil import copyfile","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:14.432229Z","iopub.execute_input":"2021-07-28T08:28:14.432662Z","iopub.status.idle":"2021-07-28T08:28:18.645166Z","shell.execute_reply.started":"2021-07-28T08:28:14.432624Z","shell.execute_reply":"2021-07-28T08:28:18.643464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n    # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:18.646346Z","iopub.execute_input":"2021-07-28T08:28:18.646701Z","iopub.status.idle":"2021-07-28T08:28:20.132326Z","shell.execute_reply.started":"2021-07-28T08:28:18.646663Z","shell.execute_reply":"2021-07-28T08:28:20.13126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\nprint(len(sub_df))\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:20.133857Z","iopub.execute_input":"2021-07-28T08:28:20.134304Z","iopub.status.idle":"2021-07-28T08:28:20.163438Z","shell.execute_reply.started":"2021-07-28T08:28:20.134259Z","shell.execute_reply":"2021-07-28T08:28:20.162488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_df = sub_df.loc[sub_df.id.str.contains('_study')]\nlen(study_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:20.164842Z","iopub.execute_input":"2021-07-28T08:28:20.16523Z","iopub.status.idle":"2021-07-28T08:28:20.174669Z","shell.execute_reply.started":"2021-07-28T08:28:20.165193Z","shell.execute_reply":"2021-07-28T08:28:20.173692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_df = sub_df.loc[sub_df.id.str.contains('_image')]\nlen(image_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:20.176303Z","iopub.execute_input":"2021-07-28T08:28:20.176809Z","iopub.status.idle":"2021-07-28T08:28:20.187286Z","shell.execute_reply.started":"2021-07-28T08:28:20.176772Z","shell.execute_reply":"2021-07-28T08:28:20.186228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data\n\ndef resize_xray(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:20.188784Z","iopub.execute_input":"2021-07-28T08:28:20.18924Z","iopub.status.idle":"2021-07-28T08:28:20.196895Z","shell.execute_reply.started":"2021-07-28T08:28:20.189205Z","shell.execute_reply":"2021-07-28T08:28:20.195909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_PATH = f'/kaggle/tmp/test/'\nIMG_SIZE = 512\n\nos.makedirs(TEST_PATH, exist_ok=True)\nfilenames = glob.glob(f'/kaggle/input/siim-covid19-detection/test/*/*/*.dcm')\n\ndef persist_image(path):\n    xray = read_xray(path)\n    im = resize_xray(xray, size=IMG_SIZE)  \n    fname = os.path.basename(os.path.splitext(path)[-2])\n    jpg_fname = os.path.join(TEST_PATH, \"{}.jpg\".format(fname))\n    im.save(jpg_fname)\n    return [fname,xray.shape[0],xray.shape[1]]\nwith Pool(cpu_count()) as pool:\n    img_metadata = pool.map(persist_image,filenames)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:28:20.198392Z","iopub.execute_input":"2021-07-28T08:28:20.198822Z","iopub.status.idle":"2021-07-28T08:34:12.924778Z","shell.execute_reply.started":"2021-07-28T08:28:20.198784Z","shell.execute_reply":"2021-07-28T08:34:12.923735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of test images: {len(os.listdir(TEST_PATH))}')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:35:03.46976Z","iopub.execute_input":"2021-07-28T08:35:03.470215Z","iopub.status.idle":"2021-07-28T08:35:03.481955Z","shell.execute_reply.started":"2021-07-28T08:35:03.470174Z","shell.execute_reply":"2021-07-28T08:35:03.480966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_df = pd.DataFrame(img_metadata,columns = ['image_id','dim0','dim1'])\n\n# Associate image-level id with study-level ids.\n# Note that a study-level might have more than one image-level ids.\nfor study_dir in os.listdir('../input/siim-covid19-detection/test'):\n    for series in os.listdir(f'../input/siim-covid19-detection/test/{study_dir}'):\n        for image in os.listdir(f'../input/siim-covid19-detection/test/{study_dir}/{series}/'):\n            image_id = image[:-4]\n            meta_df.loc[meta_df['image_id'] == image_id, 'study_id'] = study_dir\n        \nmeta_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:34:57.281671Z","iopub.execute_input":"2021-07-28T08:34:57.282068Z","iopub.status.idle":"2021-07-28T08:34:59.386956Z","shell.execute_reply.started":"2021-07-28T08:34:57.282032Z","shell.execute_reply":"2021-07-28T08:34:59.38617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cp -r /kaggle/input/yolomodelsm6allfolds /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:35:16.140733Z","iopub.execute_input":"2021-07-28T08:35:16.141067Z","iopub.status.idle":"2021-07-28T08:35:21.454536Z","shell.execute_reply.started":"2021-07-28T08:35:16.141036Z","shell.execute_reply":"2021-07-28T08:35:21.453487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/input/siimcovidyolov5l/yolov5/detect.py --weights /kaggle/working/yolomodelsm6allfolds/yolov5m6fold0.pt /kaggle/working/yolomodelsm6allfolds/yolov5m6fold1.pt /kaggle/working/yolomodelsm6allfolds/yolov5m6fold2.pt /kaggle/working/yolomodelsm6allfolds/yolov5m6fold3.pt /kaggle/working/yolomodelsm6allfolds/yolov5m6fold4.pt \\\n                                      --source {TEST_PATH} \\\n                                      --img 512 \\\n                                      --conf 0.25 \\\n                                      --iou-thres 0.5 \\\n                                      --max-det 10 \\\n                                      --save-txt \\\n                                      --save-conf \\\n                                      --augment","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:35:28.456306Z","iopub.execute_input":"2021-07-28T08:35:28.459262Z","iopub.status.idle":"2021-07-28T08:47:03.601267Z","shell.execute_reply.started":"2021-07-28T08:35:28.4592Z","shell.execute_reply":"2021-07-28T08:47:03.600262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PRED_PATH = 'runs/detect/exp/labels'\nprediction_files = os.listdir(PRED_PATH)\nprint(f'Number of opacity predicted by YOLOv5: {len(prediction_files)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:47:03.604853Z","iopub.execute_input":"2021-07-28T08:47:03.605124Z","iopub.status.idle":"2021-07-28T08:47:03.613033Z","shell.execute_reply.started":"2021-07-28T08:47:03.605093Z","shell.execute_reply":"2021-07-28T08:47:03.612072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correct_bbox_format(bboxes):\n    correct_bboxes = []\n    for b in bboxes:\n        xc, yc = int(np.round(b[0]*IMG_SIZE)), int(np.round(b[1]*IMG_SIZE))\n        w, h = int(np.round(b[2]*IMG_SIZE)), int(np.round(b[3]*IMG_SIZE))\n\n        xmin = xc - int(np.round(w/2))\n        ymin = yc - int(np.round(h/2))\n        xmax = xc + int(np.round(w/2))\n        ymax = yc + int(np.round(h/2))\n        \n        correct_bboxes.append([xmin, ymin, xmax, ymax])\n        \n    return correct_bboxes\n\ndef scale_bboxes_to_original(row, bboxes):\n    # Get scaling factor\n    scale_x = IMG_SIZE/row.dim1\n    scale_y = IMG_SIZE/row.dim0\n    \n    scaled_bboxes = []\n    for bbox in bboxes:\n        xmin, ymin, xmax, ymax = bbox\n        \n        xmin = int(np.round(xmin/scale_x))\n        ymin = int(np.round(ymin/scale_y))\n        xmax = int(np.round(xmax/scale_x))\n        ymax = int(np.round(ymax/scale_y))\n        \n        scaled_bboxes.append([xmin, ymin, xmax, ymax])\n        \n    return scaled_bboxes\n\n# Read the txt file generated by YOLOv5 during inference and extract \n# confidence and bounding box coordinates.\ndef get_conf_bboxes(file_path):\n    confidence = []\n    bboxes = []\n    with open(file_path, 'r') as file:\n        for line in file:\n            preds = line.strip('\\n').split(' ')\n            preds = list(map(float, preds))\n            confidence.append(preds[-1])\n            bboxes.append(preds[1:-1])\n    return confidence, bboxes","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:47:03.615064Z","iopub.execute_input":"2021-07-28T08:47:03.615763Z","iopub.status.idle":"2021-07-28T08:47:03.62831Z","shell.execute_reply.started":"2021-07-28T08:47:03.615724Z","shell.execute_reply":"2021-07-28T08:47:03.627529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_pred_strings = []\nctr = 0\nfor i in tqdm(range(len(image_df))):\n    row = meta_df.loc[i]\n    id_name = row.image_id\n    \n    if f'{id_name}.txt' in prediction_files:\n        # opacity label\n        confidence, bboxes = get_conf_bboxes(f'{PRED_PATH}/{id_name}.txt')\n        bboxes = correct_bbox_format(bboxes)\n        ori_bboxes = scale_bboxes_to_original(row, bboxes)\n        \n        pred_string = ''\n        for j, conf in enumerate(confidence):\n            pred_string += f'opacity {conf} ' + ' '.join(map(str, ori_bboxes[j])) + ' '\n        \n        row = mean_predictions_df.loc[mean_predictions_df['study_id'] == row.study_id]\n        neg = row.Negative.item()\n        typ = row.Typical.item()\n        ind = row.Indeterminate.item()\n        atp = row.Atypical.item()\n        output_class = np.argmax(np.array([neg,typ,ind,atp]))\n        if output_class == 0 and neg > 0.7:\n            ctr+=1\n            image_pred_strings.append(\"none 1 0 0 1 1\")\n        else:\n            image_pred_strings.append(pred_string[:-1])\n    else:\n        image_pred_strings.append(\"none 1 0 0 1 1\")\nprint('Number of images that were detected as opacity but are forced to none on the basis of classification output are :' + str(ctr))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:47:03.629906Z","iopub.execute_input":"2021-07-28T08:47:03.630302Z","iopub.status.idle":"2021-07-28T08:47:05.392548Z","shell.execute_reply.started":"2021-07-28T08:47:03.630267Z","shell.execute_reply":"2021-07-28T08:47:05.391607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_df['PredictionString'] = image_pred_strings\nimage_df = meta_df[['study_id','image_id', 'PredictionString']]\n# image_df.insert(0, 'id', image_df.apply(lambda row: row.image_id+'_image', axis=1))\n# image_df = image_df.drop('image_id', axis=1)\nimage_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:47:05.393858Z","iopub.execute_input":"2021-07-28T08:47:05.394238Z","iopub.status.idle":"2021-07-28T08:47:05.411148Z","shell.execute_reply.started":"2021-07-28T08:47:05.394196Z","shell.execute_reply":"2021-07-28T08:47:05.410051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_df.insert(0, 'id', image_df.apply(lambda row: row.image_id+'_image', axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:47:05.412711Z","iopub.execute_input":"2021-07-28T08:47:05.413168Z","iopub.status.idle":"2021-07-28T08:47:05.440711Z","shell.execute_reply.started":"2021-07-28T08:47:05.413119Z","shell.execute_reply":"2021-07-28T08:47:05.439906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_df_new = image_df[['id','PredictionString']]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:47:05.441921Z","iopub.execute_input":"2021-07-28T08:47:05.44233Z","iopub.status.idle":"2021-07-28T08:47:05.453255Z","shell.execute_reply.started":"2021-07-28T08:47:05.442295Z","shell.execute_reply":"2021-07-28T08:47:05.452453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cls_submission_df = cls_submission_df.append(image_df_new).reset_index(drop=True)\ncls_submission_df.to_csv('/kaggle/working/submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:47:05.45671Z","iopub.execute_input":"2021-07-28T08:47:05.456983Z","iopub.status.idle":"2021-07-28T08:47:05.481942Z","shell.execute_reply.started":"2021-07-28T08:47:05.456959Z","shell.execute_reply":"2021-07-28T08:47:05.481189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%rm -rf runs\n%rm -rf test_384x384\n%rm -rf /kaggle/working/yolomodelsm6allfolds","metadata":{"execution":{"iopub.status.busy":"2021-07-28T08:47:05.483241Z","iopub.execute_input":"2021-07-28T08:47:05.483586Z","iopub.status.idle":"2021-07-28T08:47:07.615328Z","shell.execute_reply.started":"2021-07-28T08:47:05.483552Z","shell.execute_reply":"2021-07-28T08:47:07.614297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## MMDetection compatible torch installation\n# !pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torch-1.7.0+cu110-cp37-cp37m-linux_x86_64.whl' --no-deps\n# !pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torchvision-0.8.1+cu110-cp37-cp37m-linux_x86_64.whl' --no-deps\n# !pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torchaudio-0.7.0-cp37-cp37m-linux_x86_64.whl' --no-deps\n\n# ## Compatible Cuda Toolkit installation\n# !mkdir -p /kaggle/tmp && cp /kaggle/input/pytorch-170-cuda-toolkit-110221/cudatoolkit-11.0.221-h6bb024c_0 /kaggle/tmp/cudatoolkit-11.0.221-h6bb024c_0.tar.bz2 && conda install /kaggle/tmp/cudatoolkit-11.0.221-h6bb024c_0.tar.bz2 -y --offline\n\n# ## MMDetection Offline Installation\n# !pip install '/kaggle/input/mmdetectionv2140/addict-2.4.0-py3-none-any.whl' --no-deps\n# !pip install '/kaggle/input/mmdetectionv2140/yapf-0.31.0-py2.py3-none-any.whl' --no-deps\n# !pip install '/kaggle/input/mmdetectionv2140/terminal-0.4.0-py3-none-any.whl' --no-deps\n# !pip install '/kaggle/input/mmdetectionv2140/terminaltables-3.1.0-py3-none-any.whl' --no-deps\n# !pip install '/kaggle/input/mmdetectionv2140/mmcv_full-1_3_8-cu110-torch1_7_0/mmcv_full-1.3.8-cp37-cp37m-manylinux1_x86_64.whl' --no-deps\n# !pip install '/kaggle/input/mmdetectionv2140/pycocotools-2.0.2/pycocotools-2.0.2' --no-deps\n# !pip install '/kaggle/input/mmdetectionv2140/mmpycocotools-12.0.3/mmpycocotools-12.0.3' --no-deps\n\n# !cp -r /kaggle/input/swintrans1024repo/covid-detection /kaggle/working/\n# # !mv /kaggle/working/mmdetection-2.14.0 /kaggle/working/mmdetection\n# %cd /kaggle/working/covid-detection\n# !pip install -v -e . --no-deps\n# %cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.382307Z","iopub.status.idle":"2021-07-28T07:55:52.382908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import sys\n# sys.path.append('/opt/conda/lib/python3.7/site-packages/mmdet.egg-link')\n# sys.path.append('/kaggle/working/mmdetection')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.384021Z","iopub.status.idle":"2021-07-28T07:55:52.384633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub_df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\n\n# # Form image dataframe\n# sub_df['level'] = sub_df.id.map(lambda idx: idx[-5:])\n# image_df = sub_df[sub_df.level=='image'].rename({'id':'image_id'}, axis=1)\n\n# dcm_path = glob.glob('/kaggle/input/siim-covid19-detection/test/**/*dcm', recursive=True)\n# test_meta = pd.DataFrame({'dcm_path':dcm_path})\n# test_meta['image_id'] = test_meta.dcm_path.map(lambda x: x.split('/')[-1].replace('.dcm', '')+'_image')\n# test_meta['study_id'] = test_meta.dcm_path.map(lambda x: x.split('/')[-3].replace('.dcm', '')+'_study')\n\n# image_df = image_df.merge(test_meta, on='image_id', how='left')\n# image_df","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.385783Z","iopub.status.idle":"2021-07-28T07:55:52.386427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMAGE_DIMS = (1024, 1024)\n\n# image_dir = f'/kaggle/tmp/test/image/'\n# os.makedirs(image_dir, exist_ok=True)\n\n# def read_xray(path, voi_lut = True, fix_monochrome = True):\n#     # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n#     dicom = pydicom.read_file(path)\n    \n#     # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n#     # \"human-friendly\" view\n#     if voi_lut:\n#         data = apply_voi_lut(dicom.pixel_array, dicom)\n#     else:\n#         data = dicom.pixel_array\n               \n#     # depending on this value, X-ray may look inverted - fix that:\n#     if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n#         data = np.amax(data) - data\n        \n#     data = data - np.min(data)\n#     data = data / np.max(data)\n#     data = (data * 255).astype(np.uint8)\n#     return data\n\n# def resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n#     # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n#     im = Image.fromarray(array)\n    \n#     if keep_ratio:\n#         im.thumbnail((size, size), resample)\n#     else:\n#         im = im.resize((size, size), resample)\n#     return im\n\n# image_df['dim0'] = -1\n# image_df['dim1'] = -1\n# img_ext = '.jpg'\n# for index, row in tqdm(image_df[['image_id', 'dcm_path', 'dim0', 'dim1']].iterrows(), total=image_df.shape[0]):\n#     # set keep_ratio=True to have original aspect ratio\n#     xray = read_xray(row['dcm_path'])\n#     im = resize(xray, size=IMAGE_DIMS[0])  \n#     im.save(os.path.join(image_dir, row['image_id']+img_ext))\n#     image_df.loc[image_df.image_id==row.image_id, 'dim0'] = xray.shape[0]\n#     image_df.loc[image_df.image_id==row.image_id, 'dim1'] = xray.shape[1]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.387519Z","iopub.status.idle":"2021-07-28T07:55:52.38812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd /kaggle/working/covid-detection","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.389169Z","iopub.status.idle":"2021-07-28T07:55:52.389798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from mmdet.apis import init_detector, inference_detector\n# import torch\n# import mmcv\n# import os\n# from tqdm import tqdm\n\n# config_file = 'configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_3x_coco.py'\n# checkpoint_file = '/kaggle/input/swinmodels1024/epoch_11.pth'\n\n# model = init_detector(config_file, checkpoint_file, device='cuda:0')\n\n# classes = ['opacity']\n# imgNoBB = []\n# pred_dir = 'preds'\n# os.makedirs(pred_dir,exist_ok=True)\n# for file in tqdm(os.listdir(image_dir)):\n#     # # test a single image and show the results\n#     img = os.path.join(image_dir,file) # or img = mmcv.imread(img), which will only load it once\n#     try:\n#         result = inference_detector(model, img)\n#     except RuntimeError:\n#         imgNoBB.append(file.split('.')[0])\n#         pass\n#     # f.write(classes[int(npcid[0][i][0])] + \" \" + str(npscore[0][i][0]) + \" \" + str(xmin) + \" \" + str(ymin) + \" \" + str(\n#     #     xmax) + \" \" + str(ymax) + \"\\n\")\n#     for i,cls in enumerate(classes):\n#         for arr in result[0][i]:\n#             if arr[-1] > 0.6:\n#                 with open(os.path.join(pred_dir,file.replace(img_ext,'.txt')),'a') as f:\n#     #                 if arr[-1] > 0.5:\n#                     f.write(str(arr[-1]) + ' ' + str(arr[0]) + ' ' + str(arr[1]) + ' ' + str(arr[2]) + ' ' + str(arr[3]) + \"\\n\")\n# # print('Images with no opacities : '+str(len(imgNoBB)))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.390911Z","iopub.status.idle":"2021-07-28T07:55:52.391557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction_files = os.listdir(pred_dir)\n# print('No of predictions by Swin Transformer are : '+ str(len(prediction_files)))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.392622Z","iopub.status.idle":"2021-07-28T07:55:52.39327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMG_SIZE = 1024\n# def scale_bboxes_to_original(row, bboxes):\n#     # Get scaling factor\n#     scale_x = IMG_SIZE/row.dim1\n#     scale_y = IMG_SIZE/row.dim0\n    \n#     scaled_bboxes = []\n#     for bbox in bboxes:\n#         xmin, ymin, xmax, ymax = bbox\n        \n#         xmin = int(np.round(xmin/scale_x))\n#         ymin = int(np.round(ymin/scale_y))\n#         xmax = int(np.round(xmax/scale_x))\n#         ymax = int(np.round(ymax/scale_y))\n        \n#         scaled_bboxes.append([xmin, ymin, xmax, ymax])\n        \n#     return scaled_bboxes\n\n# def get_conf_bboxes(file_path):\n#     confidence = []\n#     bboxes = []\n#     with open(file_path, 'r') as file:\n#         for line in file:\n#             preds = line.strip('\\n').split(' ')\n#             preds = list(map(float, preds))\n#             confidence.append(preds[0])\n#             bboxes.append(preds[1:])\n#     return confidence, bboxes","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.394323Z","iopub.status.idle":"2021-07-28T07:55:52.395016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_pred_strings = []\n# ctr = 0\n# for i in tqdm(range(len(image_df))):\n#     row = image_df.loc[i]\n#     id_name = row.image_id\n# #     print(prediction_files[0])\n# #     print(id_name)\n# #     break\n#     if f'{id_name}.txt' in prediction_files:\n#         # opacity label\n#         confidence, bboxes = get_conf_bboxes(os.path.join(pred_dir,f'{id_name}.txt'))\n#         ori_bboxes = scale_bboxes_to_original(row, bboxes)\n        \n#         pred_string = ''\n#         for j, conf in enumerate(confidence):\n#             pred_string += f'opacity {conf} ' + ' '.join(map(str, ori_bboxes[j])) + ' '\n        \n#         row = mean_predictions_df.loc[mean_predictions_df['study_id'] == row.study_id.split(\"_\")[0]]\n#         neg = row.Negative.item()\n#         typ = row.Typical.item()\n#         ind = row.Indeterminate.item()\n#         atp = row.Atypical.item()\n#         output_class = np.argmax(np.array([neg,typ,ind,atp]))\n#         if output_class == 0 and neg > 0.7:\n#             ctr+=1\n#             image_pred_strings.append(\"none 1 0 0 1 1\")\n#         else:\n#             image_pred_strings.append(pred_string[:-1])\n#     else:\n#         image_pred_strings.append(\"none 1 0 0 1 1\")\n# print('Number of images that were detected as opacity but are forced to none on the basis of classification output are :' + str(ctr))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.396407Z","iopub.status.idle":"2021-07-28T07:55:52.39704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_df['PredictionString'] = image_pred_strings\n# image_df = image_df[['study_id','image_id', 'PredictionString']]\n# # image_df.insert(0, 'id', image_df.apply(lambda row: row.image_id+'_image', axis=1))\n# # image_df = image_df.drop('image_id', axis=1)\n# image_df.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.398125Z","iopub.status.idle":"2021-07-28T07:55:52.398749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_df.insert(0, 'id', image_df.apply(lambda row: row.image_id, axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.39991Z","iopub.status.idle":"2021-07-28T07:55:52.400646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_df_new = image_df[['id','PredictionString']]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.401737Z","iopub.status.idle":"2021-07-28T07:55:52.402372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.403569Z","iopub.status.idle":"2021-07-28T07:55:52.404203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cls_submission_df = cls_submission_df.append(image_df_new).reset_index(drop=True)\n# cls_submission_df.to_csv('/kaggle/working/submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.4053Z","iopub.status.idle":"2021-07-28T07:55:52.406003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %rm -rf covid-detection\n# %rm -rf test_384x384","metadata":{"execution":{"iopub.status.busy":"2021-07-28T07:55:52.407209Z","iopub.status.idle":"2021-07-28T07:55:52.407879Z"},"trusted":true},"execution_count":null,"outputs":[]}]}