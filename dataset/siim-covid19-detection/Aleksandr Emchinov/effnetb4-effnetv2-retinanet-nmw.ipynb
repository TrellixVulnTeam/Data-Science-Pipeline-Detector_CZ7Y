{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!conda install '/kaggle/input/pydicom-conda-helper/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-30T18:36:08.507744Z","iopub.execute_input":"2021-07-30T18:36:08.508107Z","iopub.status.idle":"2021-07-30T18:37:19.747075Z","shell.execute_reply.started":"2021-07-30T18:36:08.508031Z","shell.execute_reply":"2021-07-30T18:37:19.746086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Installation Detectron2**","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/detectron2/omegaconf-2.0.6-py3-none-any.whl\n\n!pip install /kaggle/input/detectron2/iopath-0.1.8-py3-none-any.whl\n\n!pip install /kaggle/input/detectron2/fvcore-0.1.3.post20210317/fvcore-0.1.3.post20210317/\n\n!pip install /kaggle/input/detectron2/pycocotools-2.0.2/dist/pycocotools-2.0.2.tar\n\n!pip install /kaggle/input/detectron2/detectron2-0.4cu110-cp37-cp37m-linux_x86_64.whl\n\n!pip install /kaggle/input/ensembling-boxes-lala/ensemble_boxes-1.0.6-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:38:15.815664Z","iopub.execute_input":"2021-07-30T18:38:15.816003Z","iopub.status.idle":"2021-07-30T18:40:36.323337Z","shell.execute_reply.started":"2021-07-30T18:38:15.81597Z","shell.execute_reply":"2021-07-30T18:40:36.322338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Import libs**","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master/')\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport pandas as pd\nimport numpy as np\nimport time\nimport cv2\nimport PIL.Image\nimport random\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport albumentations as A\nfrom albumentations import *\nfrom tqdm import tqdm\nfrom pylab import rcParams\nimport timm\nfrom albumentations.pytorch import ToTensorV2\nimport torch.nn.functional as F\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nfrom math import ceil\nfrom typing import Any, Dict, List\nimport detectron2\nfrom numpy import ndarray\nfrom detectron2 import model_zoo\nfrom detectron2.config import get_cfg\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.structures import BoxMode\nfrom detectron2.utils.logger import setup_logger\nfrom detectron2.utils.visualizer import ColorMode, Visualizer\nfrom tqdm import tqdm\nimport pickle\n\nimport numpy as np\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nfrom ensemble_boxes import *\nfrom collections import Counter\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\npd.set_option('max_columns', 50)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:40:52.304523Z","iopub.execute_input":"2021-07-30T18:40:52.304886Z","iopub.status.idle":"2021-07-30T18:40:52.491263Z","shell.execute_reply.started":"2021-07-30T18:40:52.304855Z","shell.execute_reply":"2021-07-30T18:40:52.490381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Convert .dcm to .png helper functions**","metadata":{}},{"cell_type":"markdown","source":"### Helper Functions","metadata":{}},{"cell_type":"code","source":"def read_xray(path, voi_lut = True, fix_monochrome = True):\n    dicom = pydicom.read_file(path)\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n\ndef resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    im = Image.fromarray(array)\n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    return im","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:40:53.895593Z","iopub.execute_input":"2021-07-30T18:40:53.895963Z","iopub.status.idle":"2021-07-30T18:40:53.903813Z","shell.execute_reply.started":"2021-07-30T18:40:53.895931Z","shell.execute_reply":"2021-07-30T18:40:53.90273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Image Generation**","metadata":{}},{"cell_type":"markdown","source":"# Create images 1024x1024 .png for Object Detection Task and collect meta-info about .dicoms","metadata":{}},{"cell_type":"code","source":"split = 'test'\nsave_dir = f'/kaggle/tmp/{split}/'\n\nos.makedirs(save_dir, exist_ok=True)\n\nsave_dir = f'/kaggle/tmp/{split}/image/'\nos.makedirs(save_dir, exist_ok=True)\n\ndicoms = []\nimage_study_dict = dict()\n\nfor dirname, _, filenames in tqdm(os.walk(f'../input/siim-covid19-detection/{split}')):\n    for file in filenames:\n        # set keep_ratio=True to have original aspect ratio\n        xray = read_xray(os.path.join(dirname, file))\n        dim0 = xray.shape[0]\n        dim1 = xray.shape[1]\n        im = resize(xray, size=1024)  \n        im.save(os.path.join(save_dir, file.replace('.dcm', '.jpg')))\n        \n        dicoms.append(file)\n        study = dirname.split('/')[-2] + '_study'\n        a = file + '_image'\n        a = a.replace('.dcm', '')\n        image_study_dict[a] = {'dim0': dim0, 'dim1':dim1, 'study': study, 'id':file.replace('.dcm', '')}","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:40:55.318909Z","iopub.execute_input":"2021-07-30T18:40:55.319221Z","iopub.status.idle":"2021-07-30T18:55:59.482025Z","shell.execute_reply.started":"2021-07-30T18:40:55.319192Z","shell.execute_reply":"2021-07-30T18:55:59.480516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create meta-data","metadata":{}},{"cell_type":"code","source":"meta = pd.DataFrame.from_dict(image_study_dict, orient='index')\nmeta.reset_index(inplace=True)\nmeta.rename(columns={'index':'image_id'}, inplace=True)\nprint(\"meta.shape = \", meta.shape)\nmeta.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:55:59.483714Z","iopub.execute_input":"2021-07-30T18:55:59.484093Z","iopub.status.idle":"2021-07-30T18:55:59.533918Z","shell.execute_reply.started":"2021-07-30T18:55:59.484052Z","shell.execute_reply":"2021-07-30T18:55:59.532799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create **IMAGE <-> STUDY** mapper dataframe","metadata":{}},{"cell_type":"code","source":"study_image_info = meta[['id', 'study']].copy()\nstudy_image_info.rename(columns={'id': 'image_id', 'study':'study_id'}, inplace=True)\nprint(\"study_image_info.shape = \", study_image_info.shape)\nstudy_image_info.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:55:59.536245Z","iopub.execute_input":"2021-07-30T18:55:59.536643Z","iopub.status.idle":"2021-07-30T18:55:59.553375Z","shell.execute_reply.started":"2021-07-30T18:55:59.536584Z","shell.execute_reply":"2021-07-30T18:55:59.55235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create meta_df with original sizes of .dicoms","metadata":{}},{"cell_type":"code","source":"meta_df = meta[['id', 'dim0', 'dim1']]\nprint(\"meta_df.shape = \", meta_df.shape)\nmeta_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:55:59.5551Z","iopub.execute_input":"2021-07-30T18:55:59.555438Z","iopub.status.idle":"2021-07-30T18:55:59.56772Z","shell.execute_reply.started":"2021-07-30T18:55:59.555403Z","shell.execute_reply":"2021-07-30T18:55:59.56655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create images 512x512 .png for Image Classification Task","metadata":{}},{"cell_type":"code","source":"split = 'test_512'\nsave_dir_512 = f'/kaggle/tmp/{split}/'\n\nos.makedirs(save_dir_512, exist_ok=True)\n\nsave_dir_512 = f'/kaggle/tmp/{split}/image/'\nos.makedirs(save_dir_512, exist_ok=True)\n\ndicoms = []\nimage_study_dict = dict()\n\nfor dirname, _, filenames in tqdm(os.walk(f'../input/siim-covid19-detection/test')):\n    for file in filenames:\n        # set keep_ratio=True to have original aspect ratio\n        xray = read_xray(os.path.join(dirname, file))\n        dim0 = xray.shape[0]\n        dim1 = xray.shape[1]\n        im = resize(xray, size=512)  \n        im.save(os.path.join(save_dir_512, file.replace('.dcm', '.png')))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T18:55:59.569361Z","iopub.execute_input":"2021-07-30T18:55:59.569864Z","iopub.status.idle":"2021-07-30T19:07:00.698786Z","shell.execute_reply.started":"2021-07-30T18:55:59.569829Z","shell.execute_reply":"2021-07-30T19:07:00.697546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create images 384x384 .png for Image Classification Task","metadata":{}},{"cell_type":"code","source":"split = 'test_384'\nsave_dir_384 = f'/kaggle/tmp/{split}/'\n\nos.makedirs(save_dir_384, exist_ok=True)\n\nsave_dir_384 = f'/kaggle/tmp/{split}/image/'\nos.makedirs(save_dir_384, exist_ok=True)\n\ndicoms = []\nimage_study_dict = dict()\n\nfor dirname, _, filenames in tqdm(os.walk(f'../input/siim-covid19-detection/test')):\n    for file in filenames:\n        # set keep_ratio=True to have original aspect ratio\n        xray = read_xray(os.path.join(dirname, file))\n        dim0 = xray.shape[0]\n        dim1 = xray.shape[1]\n        im = resize(xray, size=384)  \n        im.save(os.path.join(save_dir_384, file.replace('.dcm', '.png')))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:07:00.700015Z","iopub.execute_input":"2021-07-30T19:07:00.700347Z","iopub.status.idle":"2021-07-30T19:17:01.100891Z","shell.execute_reply.started":"2021-07-30T19:07:00.700311Z","shell.execute_reply":"2021-07-30T19:17:01.099994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Study Level - Image Classification Task**","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:17:01.102157Z","iopub.execute_input":"2021-07-30T19:17:01.102497Z","iopub.status.idle":"2021-07-30T19:17:01.278743Z","shell.execute_reply.started":"2021-07-30T19:17:01.102462Z","shell.execute_reply":"2021-07-30T19:17:01.277906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models - EfficientNetV2 and EfficientNet-B4 custom","metadata":{}},{"cell_type":"code","source":"class CovidGeneralModel(nn.Module):\n    def __init__(self, model_name, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=4, in_chans=3)\n\n    def forward(self, x):\n        output = self.model(x)\n        return output\n    \nclass CovidEffnetModel(nn.Module):\n    def __init__(self, model_name, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=4, in_chans=3)\n        n_features = self.model.classifier.in_features\n        self.model.global_pool = nn.Identity()\n        self.model.classifier = nn.Identity()\n        self.dropout_layer = nn.Dropout(0.5)\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(n_features, 4)\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.model(x)\n        # features = self.dropout_layer(features)\n        pooled_features = self.pooling(features).view(bs, -1)\n        pooled_features = self.dropout_layer(pooled_features)\n        output = self.fc(pooled_features)\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:17:01.282666Z","iopub.execute_input":"2021-07-30T19:17:01.282948Z","iopub.status.idle":"2021-07-30T19:17:01.294488Z","shell.execute_reply.started":"2021-07-30T19:17:01.282923Z","shell.execute_reply":"2021-07-30T19:17:01.293679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Torch Dataset","metadata":{}},{"cell_type":"code","source":"class CovidDataset(Dataset):\n    \n    def __init__(self, df, transform=None):\n        \n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        row = self.df.loc[index]\n        image_id = row.image_id\n        img = cv2.imread(row.file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform is not None:\n            res = self.transform(image=img)\n            img = res['image']\n                \n        img = img.type('torch.FloatTensor')\n        return torch.tensor(img).float(), image_id","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:17:01.296238Z","iopub.execute_input":"2021-07-30T19:17:01.296898Z","iopub.status.idle":"2021-07-30T19:17:01.307854Z","shell.execute_reply.started":"2021-07-30T19:17:01.296861Z","shell.execute_reply":"2021-07-30T19:17:01.307035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation transforms for EffNetV2 and EffNet-B4","metadata":{}},{"cell_type":"code","source":"valid_transform_effnetV2 = A.Compose([\n    A.Resize(384, 384, p=1.0),\n    A.Normalize(\n        mean=(0.485, 0.456, 0.406),\n        std=(0.229, 0.224, 0.225)\n    ),\n    ToTensorV2()\n], p=1.0)\n\nvalid_transform_effnetB4 = A.Compose([\n    A.Resize(512, 512, p=1.0),\n    A.Normalize(\n        mean=(0.485, 0.456, 0.406),\n        std=(0.229, 0.224, 0.225)\n    ),\n    ToTensorV2()\n], p=1.0)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:17:01.309139Z","iopub.execute_input":"2021-07-30T19:17:01.309681Z","iopub.status.idle":"2021-07-30T19:17:01.317807Z","shell.execute_reply.started":"2021-07-30T19:17:01.309645Z","shell.execute_reply":"2021-07-30T19:17:01.317043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TTA Inference Function","metadata":{}},{"cell_type":"code","source":"def tta_inference_func(model, test_loader, device, img_size):\n    result = {}\n    model.eval()\n    bar = tqdm(test_loader)\n    LOGITS = []\n    PREDS = []\n    IMAGES = []\n    \n    with torch.no_grad():\n        for batch_idx, (images, image_id) in enumerate(bar):\n            x = images.to(device)\n            x = torch.stack([x,x.flip(-1)],0) # hflip\n            x = x.view(-1, 3, img_size, img_size)\n            logits = model(x)\n            logits = logits.view(1, 2, -1).mean(1)\n            PREDS += [logits.sigmoid().detach().cpu()]\n            LOGITS.append(logits.cpu())\n            IMAGES += image_id\n        PREDS = torch.cat(PREDS).cpu().numpy()\n    result['image_id'] = IMAGES\n    result['negative'] = PREDS[:, 0]\n    result['typical'] = PREDS[:, 1]\n    result['indeterminate'] = PREDS[:, 2]\n    result['atypical'] = PREDS[:, 3]\n    df = pd.DataFrame(result)\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:17:01.318948Z","iopub.execute_input":"2021-07-30T19:17:01.319319Z","iopub.status.idle":"2021-07-30T19:17:01.330729Z","shell.execute_reply.started":"2021-07-30T19:17:01.319286Z","shell.execute_reply":"2021-07-30T19:17:01.32986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch Data Loaders for EfficientNetV2 and EfficientNet-B4 models","metadata":{}},{"cell_type":"code","source":"df_test_384 = study_image_info.copy()\ndf_test_384['image_name'] = df_test_384['image_id'].apply(lambda x: x +'.png')\ndf_test_384['file_path'] = df_test_384.image_name.apply(lambda x: os.path.join(save_dir_384, f'{x}'))\n\n\ndf_test_512 = study_image_info.copy()\ndf_test_512['image_name'] = df_test_512['image_id'].apply(lambda x: x +'.png')\ndf_test_512['file_path'] = df_test_512.image_name.apply(lambda x: os.path.join(save_dir_512, f'{x}'))\n\n\ndataset_effnetV2 = CovidDataset(df_test_384, transform=valid_transform_effnetV2)\n\ntest_loader_effnetV2 = torch.utils.data.DataLoader(\n    dataset_effnetV2, batch_size=1, shuffle=False, num_workers=24, pin_memory=True\n)\n\ndataset_effnetB4 = CovidDataset(df_test_512, transform=valid_transform_effnetB4)\n\ntest_loader_effnetB4 = torch.utils.data.DataLoader(\n    dataset_effnetB4, batch_size=1, shuffle=False, num_workers=24, pin_memory=True\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:17:01.332016Z","iopub.execute_input":"2021-07-30T19:17:01.332366Z","iopub.status.idle":"2021-07-30T19:17:01.357855Z","shell.execute_reply.started":"2021-07-30T19:17:01.332331Z","shell.execute_reply":"2021-07-30T19:17:01.357029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EfficientNetV2 model's list","metadata":{}},{"cell_type":"code","source":"modelsEffNetV2 = []\nMODELS_EffNetV2_PATHS = [\n    '../input/weights-vit-base-r50-s16-384-from-last-week/vit_base_r50_s16_384_fold0_best_AP.pth',\n    '../input/weights-vit-base-r50-s16-384-from-last-week/vit_base_r50_s16_384_fold1_best_AP.pth',\n    '../input/weights-vit-base-r50-s16-384-from-last-week/vit_base_r50_s16_384_fold2_best_AP.pth',\n    '../input/weights-vit-base-r50-s16-384-from-last-week/vit_base_r50_s16_384_fold3_best_AP.pth',\n    '../input/weights-vit-base-r50-s16-384-from-last-week/vit_base_r50_s16_384_fold4_best_AP.pth',\n]\nfor path in MODELS_EffNetV2_PATHS:\n    state_dict = torch.load(path, map_location=torch.device('cpu'))\n    model = CovidGeneralModel('vit_base_r50_s16_384', pretrained=False)\n    model.load_state_dict(state_dict)\n    model.eval()\n    model.to(device)\n    modelsEffNetV2.append(model)\ndel state_dict","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:17:01.359172Z","iopub.execute_input":"2021-07-30T19:17:01.359512Z","iopub.status.idle":"2021-07-30T19:17:41.072602Z","shell.execute_reply.started":"2021-07-30T19:17:01.359476Z","shell.execute_reply":"2021-07-30T19:17:41.071784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EfficientNet-B4 model's list","metadata":{}},{"cell_type":"code","source":"modelsEffNetB4 = []\nMODELS_EffNetB4_PATHS = [\n    '../input/efficientnet-b4-weights-other-skf/tf_efficientnet_b4_ns_fold0_best_AP.pth',\n    '../input/efficientnet-b4-weights-other-skf/tf_efficientnet_b4_ns_fold1_best_AP.pth',\n    '../input/efficientnet-b4-weights-other-skf/tf_efficientnet_b4_ns_fold2_best_AP.pth',\n    '../input/efficientnet-b4-weights-other-skf/tf_efficientnet_b4_ns_fold3_best_AP.pth',\n    '../input/efficientnet-b4-weights-other-skf/tf_efficientnet_b4_ns_fold4_best_AP.pth',\n]\nfor path in MODELS_EffNetB4_PATHS:\n    state_dict = torch.load(path, map_location=torch.device('cpu'))\n    model = CovidEffnetModel('tf_efficientnet_b4_ns', pretrained=False)\n    model.load_state_dict(state_dict)\n    model.eval()\n    model.to(device)\n    modelsEffNetB4.append(model)\ndel state_dict","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:17:41.073997Z","iopub.execute_input":"2021-07-30T19:17:41.074335Z","iopub.status.idle":"2021-07-30T19:17:49.12516Z","shell.execute_reply.started":"2021-07-30T19:17:41.074301Z","shell.execute_reply":"2021-07-30T19:17:49.124313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EfficientNetV2 Predictions","metadata":{}},{"cell_type":"code","source":"df_list_effnetV2 = list()\nfor model in modelsEffNetV2:\n    df = tta_inference_func(model, test_loader_effnetV2, device, 384)\n    df_list_effnetV2.append(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:17:49.12652Z","iopub.execute_input":"2021-07-30T19:17:49.12687Z","iopub.status.idle":"2021-07-30T19:26:14.565318Z","shell.execute_reply.started":"2021-07-30T19:17:49.126835Z","shell.execute_reply":"2021-07-30T19:26:14.564214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EfficientNet-B4 Predictions","metadata":{}},{"cell_type":"code","source":"df_list_effnetB4 = list()\nfor model in modelsEffNetB4:\n    df = tta_inference_func(model, test_loader_effnetB4, device, 512)\n    df_list_effnetB4.append(df)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:26:14.567077Z","iopub.execute_input":"2021-07-30T19:26:14.567573Z","iopub.status.idle":"2021-07-30T19:32:26.507195Z","shell.execute_reply.started":"2021-07-30T19:26:14.567532Z","shell.execute_reply":"2021-07-30T19:32:26.505857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Combining the predictions of two architectures","metadata":{}},{"cell_type":"code","source":"df_list = df_list_effnetV2 + df_list_effnetB4","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:32:26.508763Z","iopub.execute_input":"2021-07-30T19:32:26.509108Z","iopub.status.idle":"2021-07-30T19:32:26.514048Z","shell.execute_reply.started":"2021-07-30T19:32:26.50907Z","shell.execute_reply":"2021-07-30T19:32:26.512847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating a dataframe containing the probabilities of the absence of opacity for detection","metadata":{}},{"cell_type":"code","source":"df_none = pd.concat(df_list).groupby('image_id').mean()\ndf_none.reset_index(inplace=True)\ndf_none.rename(columns={'negative':'none'}, inplace=True)\ndf_none = df_none[['image_id', 'none']]\ndf_none['none'] = df_none['none'].astype('str')\ndf_none['none'] = 'none' + ' ' + df_none['none'] + ' ' + '0 0 1 1'\nprint(\"df_none.shape = \", df_none.shape)\ndf_none['image_id'] += '_image'\ndf_none.rename(columns={'image_id':'id'}, inplace=True)\ndf_none.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:32:58.331329Z","iopub.execute_input":"2021-07-30T19:32:58.331676Z","iopub.status.idle":"2021-07-30T19:32:58.362291Z","shell.execute_reply.started":"2021-07-30T19:32:58.331644Z","shell.execute_reply":"2021-07-30T19:32:58.361432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create result of classification task","metadata":{}},{"cell_type":"markdown","source":"### Helper Func","metadata":{}},{"cell_type":"code","source":"def prepare_data(df):\n    df['negative'] = df['negative'].astype(str)\n    df['typical'] = df['typical'].astype(str)\n    df['indeterminate'] = df['indeterminate'].astype(str)\n    df['atypical'] = df['atypical'].astype(str)\n    df.loc[:, 'negative'] = 'negative ' + df['negative'] + ' 0 0 1 1 '\n    df.loc[:, 'typical'] = 'typical ' + df['typical'] + ' 0 0 1 1 '\n    df.loc[:, 'indeterminate'] = 'indeterminate ' + df['indeterminate'] + ' 0 0 1 1 '\n    df.loc[:, 'atypical'] = 'atypical ' + df['atypical'] + ' 0 0 1 1'\n    df['PredictionString'] = df['negative'] + df['typical'] + df['indeterminate'] + df['atypical']\n    df = df[['study_id', 'PredictionString']]\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:33:05.450538Z","iopub.execute_input":"2021-07-30T19:33:05.45091Z","iopub.status.idle":"2021-07-30T19:33:05.45726Z","shell.execute_reply.started":"2021-07-30T19:33:05.450879Z","shell.execute_reply":"2021-07-30T19:33:05.456423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result = pd.concat(df_list).groupby('image_id').mean()\ndf_result = df_result.reset_index()\nmeta = df_test_512[['image_id', 'study_id']]\ndf_result = meta.merge(df_result, on='image_id')\ndf_result['negative'] = df_result.groupby(['study_id'])['negative'].transform(lambda x: np.max(x))\ndf_result['typical'] = df_result.groupby(['study_id'])['typical'].transform(lambda x: np.max(x))\ndf_result['indeterminate'] = df_result.groupby(['study_id'])['indeterminate'].transform(lambda x: np.max(x))\ndf_result['atypical'] = df_result.groupby(['study_id'])['atypical'].transform(lambda x: np.max(x))\ndf_result = df_result[['study_id', 'negative', 'typical', 'indeterminate', 'atypical']]\ndf_result = df_result.drop_duplicates()\nprint(\"df_result.shape = \", df_result.shape)\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:33:06.628007Z","iopub.execute_input":"2021-07-30T19:33:06.628335Z","iopub.status.idle":"2021-07-30T19:33:08.022964Z","shell.execute_reply.started":"2021-07-30T19:33:06.628305Z","shell.execute_reply":"2021-07-30T19:33:08.021952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result = prepare_data(df_result)\ndf_result = df_result.reset_index(drop=True)\ndf_result.rename(columns={'study_id':'id'}, inplace=True)\nprint(\"df_result.shape = \", df_result.shape)\ndf_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:33:10.096884Z","iopub.execute_input":"2021-07-30T19:33:10.09722Z","iopub.status.idle":"2021-07-30T19:33:10.125171Z","shell.execute_reply.started":"2021-07-30T19:33:10.097191Z","shell.execute_reply":"2021-07-30T19:33:10.123698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare results to submit","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nprint(\"sample_submission.shape = \", sample_submission.shape)\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:33:13.101792Z","iopub.execute_input":"2021-07-30T19:33:13.102149Z","iopub.status.idle":"2021-07-30T19:33:13.132661Z","shell.execute_reply.started":"2021-07-30T19:33:13.102119Z","shell.execute_reply":"2021-07-30T19:33:13.131499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample_submit = sample_submission.set_index('id')\ndf_result = df_result.set_index('id')\ndf_submit = df_sample_submit.copy()\ndf_submit.loc[df_result.index, 'PredictionString'] = df_result.PredictionString.values\ndf_submit = df_submit.reset_index(drop=False)\nprint(\"df_submit.shape = \", df_submit.shape)\ndf_submit.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:33:15.481009Z","iopub.execute_input":"2021-07-30T19:33:15.481332Z","iopub.status.idle":"2021-07-30T19:33:15.498166Z","shell.execute_reply.started":"2021-07-30T19:33:15.481302Z","shell.execute_reply":"2021-07-30T19:33:15.497234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Object Detection Task**","metadata":{}},{"cell_type":"markdown","source":"### Helper functions","metadata":{}},{"cell_type":"code","source":"def get_COVID19_data_dicts_test(\n        root_imgdir: str,\n        test_meta: pd.DataFrame,\n        use_cache: bool = False,\n        debug: bool = False,\n):\n    debug_str = f\"_debug{int(debug)}\"\n    cache_path = Path(\".\") / f\"dataset_dicts_cache_test.pkl\"\n    if not use_cache or not cache_path.exists():\n        print(\"Creating data...\")\n        if debug:\n            test_meta = test_meta.iloc[:100]  # For debug....\n        image_id = test_meta.iloc[0, 0]\n        image_path = root_imgdir + f'{image_id}.jpg'\n        image = cv2.imread(image_path)\n        resized_height, resized_width, ch = image.shape\n\n        dataset_dicts = []\n        for index, test_meta_row in tqdm(test_meta.iterrows(), total=len(test_meta)):\n            record = {}\n\n            image_id, height, width = test_meta_row.values\n            filename = root_imgdir + f'{image_id}.jpg'\n            record[\"file_name\"] = filename\n            record[\"image_id\"] = image_id\n            record[\"height\"] = resized_height\n            record[\"width\"] = resized_width\n            dataset_dicts.append(record)\n        with open(cache_path, mode=\"wb\") as f:\n            pickle.dump(dataset_dicts, f)\n\n    with open(cache_path, mode=\"rb\") as f:\n        dataset_dicts = pickle.load(f)\n    return dataset_dicts\n\n\ndef format_pred(labels: ndarray, boxes: ndarray, scores: ndarray) -> str:\n    pred_strings = []\n    for label, score, bbox in zip(labels, scores, boxes):\n        if label == 0:\n            label = 'opacity'\n        elif label == 1:\n            label = 'none'\n        xmin, ymin, xmax, ymax = bbox.astype(np.int64)\n        \n        if label ==  'none':\n            xmin, ymin, xmax, ymax = np.array([0, 0, 1, 1])\n        \n        pred_strings.append(f\"{label} {score} {xmin} {ymin} {xmax} {ymax}\")\n    return \" \".join(pred_strings)\n\n\ndef get_pred(image_id: str, labels: ndarray, boxes: ndarray, scores: ndarray) -> list:\n    preds = []\n    for label, score, bbox in zip(labels, scores, boxes):\n        xmin, ymin, xmax, ymax = bbox.astype(np.float64)\n        if xmin is None:\n            print(\"ALARM\")\n        res_dict = {\n            'image_id': image_id,\n            'x_min': xmin,\n            'y_min': ymin,\n            'x_max': xmax,\n            'y_max': ymax,\n            'score': score,\n            'label': label\n        }\n        preds.append(res_dict)\n    return preds\n\n\n\ndef predict_batch(predictor: DefaultPredictor, im_list: List[ndarray]) -> List:\n    with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n        inputs_list = []\n        for original_image in im_list:\n            if predictor.input_format == \"RGB\":\n                original_image = original_image[:, :, ::-1]\n            height, width = original_image.shape[:2]\n            image = original_image\n            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n            inputs = {\"image\": image, \"height\": height, \"width\": width}\n            inputs_list.append(inputs)\n        predictions = predictor.model(inputs_list)\n        return predictions\n\n    \ndef get_model_predictions(dataset_dicts, predictor, meta_df, fold_num):\n    results_list = []\n    index = 0\n    batch_size = 4\n\n    for i in tqdm(range(ceil(len(dataset_dicts) / batch_size))):\n        inds = list(range(batch_size * i, min(batch_size * (i + 1), len(dataset_dicts))))\n        dataset_dicts_batch = [dataset_dicts[i] for i in inds]\n        im_list = [cv2.imread(d[\"file_name\"]) for d in dataset_dicts_batch]\n        outputs_list = predict_batch(predictor, im_list)\n\n        for im, outputs, d in zip(im_list, outputs_list, dataset_dicts_batch):\n            resized_height, resized_width, ch = im.shape\n\n            image_id, dim0, dim1 = meta_df.iloc[index].values\n\n            instances = outputs[\"instances\"]\n            if len(instances) == 0:\n                result = [\n                    {\n                        'image_id': image_id,\n                        'x_min': 0,\n                        'y_min': 0,\n                        'x_max': 1,\n                        'y_max': 1,\n                        'score': 1.0,\n                        'label': 1\n                    }\n                ]\n            else:\n                # Find some bbox...\n                # print(f\"index={index}, find {len(instances)} bbox.\")\n                fields: Dict[str, Any] = instances.get_fields()\n                pred_classes = fields[\"pred_classes\"]  # (n_boxes,)\n                pred_scores = fields[\"scores\"]\n                # shape (n_boxes, 4). (xmin, ymin, xmax, ymax)\n                pred_boxes = fields[\"pred_boxes\"].tensor\n\n                h_ratio = dim0 / resized_height\n                w_ratio = dim1 / resized_width\n                pred_boxes[:, [0, 2]] *= w_ratio\n                pred_boxes[:, [1, 3]] *= h_ratio\n\n                pred_classes_array = pred_classes.cpu().numpy()\n                pred_boxes_array = pred_boxes.cpu().numpy()\n                pred_scores_array = pred_scores.cpu().numpy()\n\n                result = get_pred(image_id, pred_classes_array, pred_boxes_array, pred_scores_array)\n            results_list.append(result)\n            index += 1\n    final_list = [item for sublist in results_list for item in sublist]\n    result_df = pd.DataFrame(final_list)\n    result_df['weight'] = fold_num\n    return result_df","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:33:35.709468Z","iopub.execute_input":"2021-07-30T19:33:35.709854Z","iopub.status.idle":"2021-07-30T19:33:35.727142Z","shell.execute_reply.started":"2021-07-30T19:33:35.709824Z","shell.execute_reply":"2021-07-30T19:33:35.726172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"thing_classes = [\n    \"opacity\"\n]\ncategory_name_to_id = {class_name: index for index, class_name in enumerate(thing_classes)}\n\ndebug=False\nroot_imgdir = \"/kaggle/tmp/test/image/\"\noutdir = \"results/detection\"","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:36:26.425332Z","iopub.execute_input":"2021-07-30T19:36:26.425685Z","iopub.status.idle":"2021-07-30T19:36:26.431454Z","shell.execute_reply.started":"2021-07-30T19:36:26.425654Z","shell.execute_reply":"2021-07-30T19:36:26.430463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Object Detection Model","metadata":{}},{"cell_type":"code","source":"cfg = get_cfg()\noriginal_output_dir = cfg.OUTPUT_DIR\ncfg.OUTPUT_DIR = str(outdir)\nprint(f\"cfg.OUTPUT_DIR {original_output_dir} -> {cfg.OUTPUT_DIR}\")\n\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)\ncfg.MODEL.WEIGHTS = str(\"../input/weights-retinanet-4000-jpg/F0_RetinaNet_4000_jpg.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0\npredictor_0 = DefaultPredictor(cfg)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:35:46.847311Z","iopub.execute_input":"2021-07-30T19:35:46.847679Z","iopub.status.idle":"2021-07-30T19:35:54.350361Z","shell.execute_reply.started":"2021-07-30T19:35:46.847643Z","shell.execute_reply":"2021-07-30T19:35:54.349393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = get_cfg()\noriginal_output_dir = cfg.OUTPUT_DIR\ncfg.OUTPUT_DIR = str(outdir)\nprint(f\"cfg.OUTPUT_DIR {original_output_dir} -> {cfg.OUTPUT_DIR}\")\n\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)\ncfg.MODEL.WEIGHTS = str(\"../input/weights-retinanet-4000-jpg/F1_RetinaNet_4000_jpg_2_train_iter.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0\npredictor_1 = DefaultPredictor(cfg)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = get_cfg()\noriginal_output_dir = cfg.OUTPUT_DIR\ncfg.OUTPUT_DIR = str(outdir)\nprint(f\"cfg.OUTPUT_DIR {original_output_dir} -> {cfg.OUTPUT_DIR}\")\n\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)\ncfg.MODEL.WEIGHTS = str(\"../input/weights-retinanet-4000-jpg/F2_RetinaNet_4000_jpg.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0\npredictor_2 = DefaultPredictor(cfg)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = get_cfg()\noriginal_output_dir = cfg.OUTPUT_DIR\ncfg.OUTPUT_DIR = str(outdir)\nprint(f\"cfg.OUTPUT_DIR {original_output_dir} -> {cfg.OUTPUT_DIR}\")\n\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)\ncfg.MODEL.WEIGHTS = str(\"../input/weights-retinanet-4000-jpg/F3_RetinaNet_4000_jpg.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0\npredictor_3 = DefaultPredictor(cfg)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = get_cfg()\noriginal_output_dir = cfg.OUTPUT_DIR\ncfg.OUTPUT_DIR = str(outdir)\nprint(f\"cfg.OUTPUT_DIR {original_output_dir} -> {cfg.OUTPUT_DIR}\")\n\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_101_FPN_3x.yaml\"))\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = len(thing_classes)\ncfg.MODEL.WEIGHTS = str(\"../input/weights-retinanet-4000-jpg/F4_RetinaNet_4000_jpg.pth\")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.0\npredictor_4 = DefaultPredictor(cfg)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Register COCO dicts","metadata":{}},{"cell_type":"code","source":"DatasetCatalog.register(\n    \"COVID19_data_test\", lambda: get_COVID19_data_dicts_test(\n        root_imgdir=root_imgdir,\n        test_meta=meta_df,\n        use_cache=False,\n        debug=debug\n    )\n)\nMetadataCatalog.get(\"COVID19_data_test\").set(thing_classes=thing_classes)\nmetadata = MetadataCatalog.get(\"COVID19_data_test\")\ndataset_dicts = get_COVID19_data_dicts_test(\n                                                root_imgdir=root_imgdir,\n                                                test_meta=meta_df,\n                                                use_cache=False,\n                                                debug=debug\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:36:46.277296Z","iopub.execute_input":"2021-07-30T19:36:46.277642Z","iopub.status.idle":"2021-07-30T19:36:46.392216Z","shell.execute_reply.started":"2021-07-30T19:36:46.277592Z","shell.execute_reply":"2021-07-30T19:36:46.391355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Object Detection Predictions","metadata":{}},{"cell_type":"code","source":"results_dfs = []\nfor fold_id, predictor in enumerate([predictor_0, predictor_1, predictor_2, predictor_3, predictor_4]):\n    results_dfs.append(get_model_predictions(dataset_dicts, predictor, meta_df, fold_id))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:37:02.032932Z","iopub.execute_input":"2021-07-30T19:37:02.03325Z","iopub.status.idle":"2021-07-30T19:39:56.999084Z","shell.execute_reply.started":"2021-07-30T19:37:02.033222Z","shell.execute_reply":"2021-07-30T19:39:56.998133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df = pd.concat(results_dfs)\nprint(\"result_df.shape = \", result_df.shape)\nresult_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Apply non-maximum weighted","metadata":{}},{"cell_type":"code","source":"iou_thr = 0.55\nskip_box_thr = 0.0000\nsigma = 0.1\nviz_images = []\nwbf_result_dicts = []\n\nfor i, img_id in tqdm(enumerate(result_df.image_id.unique())):\n    path = f'/home/hdd/storage/siim_covid_detection/resized_images_from_kaggle/1024x1024/test/{img_id}.jpg'\n    _, dim0, dim1 = meta_df[meta_df.id == img_id].iloc[0].values\n    img_array  = cv2.imread(path)\n\n    boxes_viz = list()\n    labels_viz = list()\n    scores_viz = list()\n    for weight in [0, 1, 2, 3, 4]:\n        tmp_df = result_df[result_df.weight==weight]\n        img_annotations = tmp_df[tmp_df.image_id==img_id]\n\n        boxes_viz_tmp = (img_annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy() \\\n        / (dim1, dim0, dim1, dim0)).tolist()\n        labels_viz_tmp = img_annotations['label'].to_numpy().tolist()\n        scores_viz_tmp = img_annotations['score'].to_numpy().tolist()\n        \n        boxes_viz.append(boxes_viz_tmp)\n        labels_viz.append(labels_viz_tmp)\n        scores_viz.append(scores_viz_tmp)\n    \n        \n    count_dict = Counter(img_annotations['label'].tolist())\n    \n#     boxes, scores, box_labels= nms(boxes_viz, scores_viz, labels_viz, weights=None,\n#                                                      iou_thr=iou_thr)\n\n    boxes, scores, box_labels= non_maximum_weighted(boxes_viz, scores_viz, labels_viz, weights=None,\n                                                         iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n\n#     boxes, scores, box_labels= weighted_boxes_fusion(boxes_viz, scores_viz, labels_viz, weights=None,\n#                                                      iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    \n    boxes = boxes*(dim1, dim0, dim1, dim0)\n    boxes = boxes.tolist()\n    box_labels = box_labels.astype(int).tolist()\n    box_scores = scores.tolist()\n    \n    for i in range(len(box_labels)):\n        x_min = boxes[i][0]\n        y_min = boxes[i][1]\n        x_max = boxes[i][2]\n        y_max = boxes[i][3]\n        tmp_dict = {\n            'image_id': img_id,\n            'x_min': x_min,\n            'y_min': y_min,\n            'x_max': x_max,\n            'y_max': y_max,\n            'score': box_scores[i],\n            'label': box_labels[i]\n        }\n        wbf_result_dicts.append(tmp_dict)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wbf_result = pd.DataFrame(wbf_result_dicts)\nprint(\"wbf_result.shape = \", wbf_result.shape)\nwbf_result.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_list = list()\nfor im_id in wbf_result.image_id.unique():\n    annotations = wbf_result[wbf_result.image_id == im_id]\n    boxes = annotations[['x_min', 'y_min', 'x_max', 'y_max']].to_numpy()\n    scores = annotations['score'].to_numpy()\n    labels = annotations[['label']].to_numpy()\n    result = {\n                \"image_id\": im_id,\n                \"PredictionString\": format_pred(\n                    labels, boxes, scores\n                ),\n            }\n    results_list.append(result)\nsubmission_det = pd.DataFrame(results_list, columns=['image_id', 'PredictionString'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_det.rename(columns={'image_id':'id'}, inplace=True)\nsubmission_det['id'] += '_image'\nsubmission_det = submission_det.merge(df_none, on='id')\nsubmission_det['PredictionString'] = submission_det['PredictionString'] + ' ' + submission_det['none']\nsubmission_det = submission_det[['id', 'PredictionString']]\nprint(\"submission_det.shape = \", submission_det.shape)\nsubmission_det.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare detection results","metadata":{}},{"cell_type":"code","source":"sample_submission = df_submit.copy()\n\ndf_sample_submit = sample_submission.set_index('id')\ndf_result = submission_det.set_index('id')\n\ndf_submit_result = df_sample_submit.copy()\ndf_submit_result.loc[df_result.index, 'PredictionString'] = df_result.PredictionString.values\n# ---------------------------------------------------------------------------------\n\ndf_submit_result = df_submit_result.reset_index(drop=False)\nprint(\"df_submit_result.shape= \", df_submit_result.shape)\ndf_submit_result.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:43:02.091479Z","iopub.execute_input":"2021-07-30T19:43:02.091958Z","iopub.status.idle":"2021-07-30T19:43:02.109476Z","shell.execute_reply.started":"2021-07-30T19:43:02.09192Z","shell.execute_reply":"2021-07-30T19:43:02.108402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit_result.tail()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:43:11.429127Z","iopub.execute_input":"2021-07-30T19:43:11.429474Z","iopub.status.idle":"2021-07-30T19:43:11.438347Z","shell.execute_reply.started":"2021-07-30T19:43:11.429444Z","shell.execute_reply":"2021-07-30T19:43:11.437478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submit_result[['id', 'PredictionString']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T19:44:58.583158Z","iopub.execute_input":"2021-07-30T19:44:58.583546Z","iopub.status.idle":"2021-07-30T19:44:59.458354Z","shell.execute_reply.started":"2021-07-30T19:44:58.583512Z","shell.execute_reply":"2021-07-30T19:44:59.457515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}