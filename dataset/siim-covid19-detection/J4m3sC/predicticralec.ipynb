{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cuaderno para las pruebas del MRN PredicticRale.\n\n\"IDENTIFICACIÓN DE ANOMALÍAS COVID-19 EN RADIOGRAFÍAS DE TÓRAX MEDIANTE REDES NEURONALES Y EL SISTEMA RALE”\n\n# Marco General de Método Propuesto:\nla Siguiente Figura Muestra el método propuesto, que siguió la construcción del modelo de Red ConvRalec, la misma procesa las imágenes y las etiqueta antes de ingresar al modelo de entrenamiento y validación, y del mismo modo para el proceso de pruebas. \n\n![Marco General de Metodo Propuesto](http://drive.google.com/uc?export=view&id=1Lqeo0jAMkF-Z3RTlh2-2bkwXoIaSLK_7)\n","metadata":{}},{"cell_type":"markdown","source":"# Librerias: \npandas, os, matplotlib, pydicom, numpy, keras, math, time, tqdm, torch, glob, opencv2, pathlib, keras y tensorflow.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport pydicom as pydi\nimport numpy as np\n\nimport math\nimport time\nimport tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport matplotlib.patches as patches\n\nimport glob\nimport random\nimport sys\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport cv2\nimport pydicom as pydi\nimport pathlib\n\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.models import load_model\n\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\nprint('Segmento Terminado')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-29T20:06:45.313395Z","iopub.execute_input":"2022-05-29T20:06:45.313754Z","iopub.status.idle":"2022-05-29T20:06:54.262979Z","shell.execute_reply.started":"2022-05-29T20:06:45.313639Z","shell.execute_reply":"2022-05-29T20:06:54.262156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clases y funciones UTILS YOLO\nFunciones de la Red convolucional YOLO para la categorización de cuadrantes de Radiografías de Tórax, PosteroAnterior, preparación de imagen y Transfer, una vez localizado los cuadrantes.\n\n![Clasificacion de Cuadrantes en Caja Torasica](http://drive.google.com/uc?export=view&id=14c9Hva4zIHM144fE_95f7Ppn5kAcXMZ4)\n\n","metadata":{}},{"cell_type":"code","source":"def to_cpu(tensor):\n    return tensor.detach().cpu()\n\n\ndef load_classes(path):\n    fp = open(path, \"r\")\n    names = fp.read().split(\"\\n\")[:-1]\n    return names\n\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n\n\ndef rescale_boxes(boxes, current_dim, original_shape):\n    orig_h, orig_w = original_shape\n    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n    unpad_h = current_dim - pad_y\n    unpad_w = current_dim - pad_x\n    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n    return boxes\n\n\ndef xywh2xyxy(x):\n    y = x.new(x.shape)\n    y[..., 0] = x[..., 0] - x[..., 2] / 2\n    y[..., 1] = x[..., 1] - x[..., 3] / 2\n    y[..., 2] = x[..., 0] + x[..., 2] / 2\n    y[..., 3] = x[..., 1] + x[..., 3] / 2\n    return y\n\n\ndef ap_per_class(tp, conf, pred_cls, target_cls):\n    i = np.argsort(-conf)\n    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n    unique_classes = np.unique(target_cls)\n    ap, p, r = [], [], []\n    for c in tqdm.tqdm(unique_classes, desc=\"Computing AP\"):\n        i = pred_cls == c\n        n_gt = (target_cls == c).sum()  \n        n_p = i.sum()\n\n        if n_p == 0 and n_gt == 0:\n            continue\n        elif n_p == 0 or n_gt == 0:\n            ap.append(0)\n            r.append(0)\n            p.append(0)\n        else:\n            fpc = (1 - tp[i]).cumsum()\n            tpc = (tp[i]).cumsum()\n\n            recall_curve = tpc / (n_gt + 1e-16)\n            r.append(recall_curve[-1])\n\n            precision_curve = tpc / (tpc + fpc)\n            p.append(precision_curve[-1])\n\n            ap.append(compute_ap(recall_curve, precision_curve))\n\n    p, r, ap = np.array(p), np.array(r), np.array(ap)\n    f1 = 2 * p * r / (p + r + 1e-16)\n\n    return p, r, ap, f1, unique_classes.astype(\"int32\")\n\n\ndef compute_ap(recall, precision):\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([0.0], precision, [0.0]))\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef get_batch_statistics(outputs, targets, iou_threshold):\n    batch_metrics = []\n    for sample_i in range(len(outputs)):\n\n        if outputs[sample_i] is None:\n            continue\n\n        output = outputs[sample_i]\n        pred_boxes = output[:, :4]\n        pred_scores = output[:, 4]\n        pred_labels = output[:, -1]\n\n        true_positives = np.zeros(pred_boxes.shape[0])\n\n        annotations = targets[targets[:, 0] == sample_i][:, 1:]\n        target_labels = annotations[:, 0] if len(annotations) else []\n        if len(annotations):\n            detected_boxes = []\n            target_boxes = annotations[:, 1:]\n\n            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n\n                if len(detected_boxes) == len(annotations):\n                    break\n                if pred_label not in target_labels:\n                    continue\n\n                iou, box_index = bbox_iou(pred_box.unsqueeze(0), target_boxes).max(0)\n                if iou >= iou_threshold and box_index not in detected_boxes:\n                    true_positives[pred_i] = 1\n                    detected_boxes += [box_index]\n        batch_metrics.append([true_positives, pred_scores, pred_labels])\n    return batch_metrics\n\n\ndef bbox_wh_iou(wh1, wh2):\n    wh2 = wh2.t()\n    w1, h1 = wh1[0], wh1[1]\n    w2, h2 = wh2[0], wh2[1]\n    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n    return inter_area / union_area\n\n\ndef bbox_iou(box1, box2, x1y1x2y2=True):\n    if not x1y1x2y2:\n        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n    else:\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n\n    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n        inter_rect_y2 - inter_rect_y1 + 1, min=0\n    )\n    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n\n    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n\n    return iou\n\n\ndef non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4):\n    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n    output = [None for _ in range(len(prediction))]\n    for image_i, image_pred in enumerate(prediction):\n        image_pred = image_pred[image_pred[:, 4] >= conf_thres]\n        if not image_pred.size(0):\n            continue\n        score = image_pred[:, 4] * image_pred[:, 5:].max(1)[0]\n        image_pred = image_pred[(-score).argsort()]\n        class_confs, class_preds = image_pred[:, 5:].max(1, keepdim=True)\n        detections = torch.cat((image_pred[:, :5], class_confs.float(), class_preds.float()), 1)\n        keep_boxes = []\n        while detections.size(0):\n            large_overlap = bbox_iou(detections[0, :4].unsqueeze(0), detections[:, :4]) > nms_thres\n            label_match = detections[0, -1] == detections[:, -1]\n            invalid = large_overlap & label_match\n            weights = detections[invalid, 4:5]\n            detections[0, :4] = (weights * detections[invalid, :4]).sum(0) / weights.sum()\n            keep_boxes += [detections[0]]\n            detections = detections[~invalid]\n        if keep_boxes:\n            output[image_i] = torch.stack(keep_boxes)\n\n    return output\n\n\ndef build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres):\n\n    BoolTensor = torch.cuda.BoolTensor if pred_boxes.is_cuda else torch.BoolTensor\n    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n\n    nB = pred_boxes.size(0)\n    nA = pred_boxes.size(1)\n    nC = pred_cls.size(-1)\n    nG = pred_boxes.size(2)\n\n    obj_mask = BoolTensor(nB, nA, nG, nG).fill_(0)\n    noobj_mask = BoolTensor(nB, nA, nG, nG).fill_(1)\n    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n\n    target_boxes = target[:, 2:6] * nG\n    gxy = target_boxes[:, :2]\n    gwh = target_boxes[:, 2:]\n    ious = torch.stack([bbox_wh_iou(anchor, gwh) for anchor in anchors])\n    best_ious, best_n = ious.max(0)\n    b, target_labels = target[:, :2].long().t()\n    gx, gy = gxy.t()\n    gw, gh = gwh.t()\n    gi, gj = gxy.long().t()\n    obj_mask[b, best_n, gj, gi] = 1\n    noobj_mask[b, best_n, gj, gi] = 0\n\n    for i, anchor_ious in enumerate(ious.t()):\n        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n\n    tx[b, best_n, gj, gi] = gx - gx.floor()\n    ty[b, best_n, gj, gi] = gy - gy.floor()\n    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n    tcls[b, best_n, gj, gi, target_labels] = 1\n    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=False)\n\n    tconf = obj_mask.float()\n    return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf\n\ndef horisontal_flip(images, targets):\n    images = torch.flip(images, [-1])\n    targets[:, 2] = 1 - targets[:, 2]\n    return images, targets\n\n#---------------------------------------------------\ndef pad_to_square(img, pad_value):\n    c, h, w = img.shape\n    dim_diff = np.abs(h - w)\n    pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n    pad = (0, 0, pad1, pad2) if h <= w else (pad1, pad2, 0, 0)\n    img = F.pad(img, pad, \"constant\", value=pad_value)\n    return img, pad\n\n\ndef resize(image, size):\n    image = F.interpolate(image.unsqueeze(0), size=size, mode=\"nearest\").squeeze(0)\n    return image\n\n\ndef random_resize(images, min_size=288, max_size=448):\n    new_size = random.sample(list(range(min_size, max_size + 1, 32)), 1)[0]\n    images = F.interpolate(images, size=new_size, mode=\"nearest\")\n    return images\n\n\nclass ImageFolder(Dataset):\n    def __init__(self, folder_path, img_size=416):\n        self.files = sorted(glob.glob(\"%s/*.*\" % folder_path))\n        self.img_size = img_size\n\n    def __getitem__(self, index):\n        img_path = self.files[index % len(self.files)]\n        img = transforms.ToTensor()(Image.open(img_path))\n        img, _ = pad_to_square(img, 0)\n        img = resize(img, self.img_size)\n\n        return img_path, img\n\n    def __len__(self):\n        return len(self.files)\n\n\nclass ListDataset(Dataset):\n    def __init__(self, list_path, img_size=416, augment=True, multiscale=True, normalized_labels=True):\n        with open(list_path, \"r\") as file:\n            self.img_files = file.readlines()\n\n        self.label_files = [\n            path.replace(\"images\", \"labels\").replace(\".png\", \".txt\").replace(\".jpg\", \".txt\")\n            for path in self.img_files\n        ]\n        self.img_size = img_size\n        self.max_objects = 100\n        self.augment = augment\n        self.multiscale = multiscale\n        self.normalized_labels = normalized_labels\n        self.min_size = self.img_size - 3 * 32\n        self.max_size = self.img_size + 3 * 32\n        self.batch_count = 0\n\n    def __getitem__(self, index):\n        img_path = self.img_files[index % len(self.img_files)].rstrip()\n        img = transforms.ToTensor()(Image.open(img_path).convert('RGB'))\n        if len(img.shape) != 3:\n            img = img.unsqueeze(0)\n            img = img.expand((3, img.shape[1:]))\n\n        _, h, w = img.shape\n        h_factor, w_factor = (h, w) if self.normalized_labels else (1, 1)\n        img, pad = pad_to_square(img, 0)\n        _, padded_h, padded_w = img.shape\n\n        label_path = self.label_files[index % len(self.img_files)].rstrip()\n\n        targets = None\n        if os.path.exists(label_path):\n            boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n            # Extract coordinates for unpadded + unscaled image\n            x1 = w_factor * (boxes[:, 1] - boxes[:, 3] / 2)\n            y1 = h_factor * (boxes[:, 2] - boxes[:, 4] / 2)\n            x2 = w_factor * (boxes[:, 1] + boxes[:, 3] / 2)\n            y2 = h_factor * (boxes[:, 2] + boxes[:, 4] / 2)\n            # Adjust for added padding\n            x1 += pad[0]\n            y1 += pad[2]\n            x2 += pad[1]\n            y2 += pad[3]\n            # Returns (x, y, w, h)\n            boxes[:, 1] = ((x1 + x2) / 2) / padded_w\n            boxes[:, 2] = ((y1 + y2) / 2) / padded_h\n            boxes[:, 3] *= w_factor / padded_w\n            boxes[:, 4] *= h_factor / padded_h\n\n            targets = torch.zeros((len(boxes), 6))\n            targets[:, 1:] = boxes\n\n        if self.augment:\n            if np.random.random() < 0.5:\n                img, targets = horisontal_flip(img, targets)\n\n        return img_path, img, targets\n\n    def collate_fn(self, batch):\n        paths, imgs, targets = list(zip(*batch))\n        targets = [boxes for boxes in targets if boxes is not None]\n        for i, boxes in enumerate(targets):\n            boxes[:, 0] = i\n        targets = torch.cat(targets, 0)\n        if self.multiscale and self.batch_count % 10 == 0:\n            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))\n        imgs = torch.stack([resize(img, self.img_size) for img in imgs])\n        self.batch_count += 1\n        return paths, imgs, targets\n\n    def __len__(self):\n        return len(self.img_files)\n\n# import MODELOS\ndef parse_model_config(path):\n    file = open(path, 'r')\n    lines = file.read().split('\\n')\n    lines = [x for x in lines if x and not x.startswith('#')]\n    lines = [x.rstrip().lstrip() for x in lines]\n    module_defs = []\n    for line in lines:\n        if line.startswith('['): \n            module_defs.append({})\n            module_defs[-1]['type'] = line[1:-1].rstrip()\n            if module_defs[-1]['type'] == 'convolutional':\n                module_defs[-1]['batch_normalize'] = 0\n        else:\n            key, value = line.split(\"=\")\n            value = value.strip()\n            module_defs[-1][key.rstrip()] = value.strip()\n\n    return module_defs\n\ndef parse_data_config(path):\n    options = dict()\n    options['gpus'] = '0,1,2,3'\n    options['num_workers'] = '10'\n    with open(path, 'r') as fp:\n        lines = fp.readlines()\n    for line in lines:\n        line = line.strip()\n        if line == '' or line.startswith('#'):\n            continue\n        key, value = line.split('=')\n        options[key.strip()] = value.strip()\n    return options\n\ndef create_modules(module_defs):\n    hyperparams = module_defs.pop(0)\n    output_filters = [int(hyperparams[\"channels\"])]\n    module_list = nn.ModuleList()\n    for module_i, module_def in enumerate(module_defs):\n        modules = nn.Sequential()\n\n        if module_def[\"type\"] == \"convolutional\":\n            bn = int(module_def[\"batch_normalize\"])\n            filters = int(module_def[\"filters\"])\n            kernel_size = int(module_def[\"size\"])\n            pad = (kernel_size - 1) // 2\n            modules.add_module(\n                f\"conv_{module_i}\",\n                nn.Conv2d(\n                    in_channels=output_filters[-1],\n                    out_channels=filters,\n                    kernel_size=kernel_size,\n                    stride=int(module_def[\"stride\"]),\n                    padding=pad,\n                    bias=not bn,\n                ),\n            )\n            if bn:\n                modules.add_module(f\"batch_norm_{module_i}\", nn.BatchNorm2d(filters, momentum=0.9, eps=1e-5))\n            if module_def[\"activation\"] == \"leaky\":\n                modules.add_module(f\"leaky_{module_i}\", nn.LeakyReLU(0.1))\n\n        elif module_def[\"type\"] == \"maxpool\":\n            kernel_size = int(module_def[\"size\"])\n            stride = int(module_def[\"stride\"])\n            if kernel_size == 2 and stride == 1:\n                modules.add_module(f\"_debug_padding_{module_i}\", nn.ZeroPad2d((0, 1, 0, 1)))\n            maxpool = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=int((kernel_size - 1) // 2))\n            modules.add_module(f\"maxpool_{module_i}\", maxpool)\n\n        elif module_def[\"type\"] == \"upsample\":\n            upsample = Upsample(scale_factor=int(module_def[\"stride\"]), mode=\"nearest\")\n            modules.add_module(f\"upsample_{module_i}\", upsample)\n\n        elif module_def[\"type\"] == \"route\":\n            layers = [int(x) for x in module_def[\"layers\"].split(\",\")]\n            filters = sum([output_filters[1:][i] for i in layers])\n            modules.add_module(f\"route_{module_i}\", EmptyLayer())\n\n        elif module_def[\"type\"] == \"shortcut\":\n            filters = output_filters[1:][int(module_def[\"from\"])]\n            modules.add_module(f\"shortcut_{module_i}\", EmptyLayer())\n\n        elif module_def[\"type\"] == \"yolo\":\n            anchor_idxs = [int(x) for x in module_def[\"mask\"].split(\",\")]\n            # Extract anchors\n            anchors = [int(x) for x in module_def[\"anchors\"].split(\",\")]\n            anchors = [(anchors[i], anchors[i + 1]) for i in range(0, len(anchors), 2)]\n            anchors = [anchors[i] for i in anchor_idxs]\n            num_classes = int(module_def[\"classes\"])\n            img_size = int(hyperparams[\"height\"])\n            # Define detection layer\n            yolo_layer = YOLOLayer(anchors, num_classes, img_size)\n            modules.add_module(f\"yolo_{module_i}\", yolo_layer)\n        # Register module list and number of output filters\n        module_list.append(modules)\n        output_filters.append(filters)\n\n    return hyperparams, module_list\n\n\nclass Upsample(nn.Module):\n    def __init__(self, scale_factor, mode=\"nearest\"):\n        super(Upsample, self).__init__()\n        self.scale_factor = scale_factor\n        self.mode = mode\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=self.scale_factor, mode=self.mode)\n        return x\n\n\nclass EmptyLayer(nn.Module):\n    def __init__(self):\n        super(EmptyLayer, self).__init__()\n\n\nclass YOLOLayer(nn.Module):\n    def __init__(self, anchors, num_classes, img_dim=416):\n        super(YOLOLayer, self).__init__()\n        self.anchors = anchors\n        self.num_anchors = len(anchors)\n        self.num_classes = num_classes\n        self.ignore_thres = 0.5\n        self.mse_loss = nn.MSELoss()\n        self.bce_loss = nn.BCELoss()\n        self.obj_scale = 1\n        self.noobj_scale = 100\n        self.metrics = {}\n        self.img_dim = img_dim\n        self.grid_size = 0 \n\n    def compute_grid_offsets(self, grid_size, cuda=True):\n        self.grid_size = grid_size\n        g = self.grid_size\n        FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n        self.stride = self.img_dim / self.grid_size\n        self.grid_x = torch.arange(g).repeat(g, 1).view([1, 1, g, g]).type(FloatTensor)\n        self.grid_y = torch.arange(g).repeat(g, 1).t().view([1, 1, g, g]).type(FloatTensor)\n        self.scaled_anchors = FloatTensor([(a_w / self.stride, a_h / self.stride) for a_w, a_h in self.anchors])\n        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n\n    def forward(self, x, targets=None, img_dim=None):\n        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n        ByteTensor = torch.cuda.ByteTensor if x.is_cuda else torch.ByteTensor\n\n        self.img_dim = img_dim\n        num_samples = x.size(0)\n        grid_size = x.size(2)\n\n        prediction = (\n            x.view(num_samples, self.num_anchors, self.num_classes + 5, grid_size, grid_size)\n            .permute(0, 1, 3, 4, 2)\n            .contiguous()\n        )\n\n        x = torch.sigmoid(prediction[..., 0])  \n        y = torch.sigmoid(prediction[..., 1])  \n        w = prediction[..., 2]  \n        h = prediction[..., 3]  \n        pred_conf = torch.sigmoid(prediction[..., 4])  \n        pred_cls = torch.sigmoid(prediction[..., 5:])  \n\n        if grid_size != self.grid_size:\n            self.compute_grid_offsets(grid_size, cuda=x.is_cuda)\n\n        pred_boxes = FloatTensor(prediction[..., :4].shape)\n        pred_boxes[..., 0] = x.data + self.grid_x\n        pred_boxes[..., 1] = y.data + self.grid_y\n        pred_boxes[..., 2] = torch.exp(w.data) * self.anchor_w\n        pred_boxes[..., 3] = torch.exp(h.data) * self.anchor_h\n\n        output = torch.cat(\n            (\n                pred_boxes.view(num_samples, -1, 4) * self.stride,\n                pred_conf.view(num_samples, -1, 1),\n                pred_cls.view(num_samples, -1, self.num_classes),\n            ),\n            -1,\n        )\n\n        if targets is None:\n            return output, 0\n        else:\n            iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = build_targets(\n                pred_boxes=pred_boxes,\n                pred_cls=pred_cls,\n                target=targets,\n                anchors=self.scaled_anchors,\n                ignore_thres=self.ignore_thres,\n            )\n\n            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n            loss_cls = self.bce_loss(pred_cls[obj_mask], tcls[obj_mask])\n            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n\n            cls_acc = 100 * class_mask[obj_mask].mean()\n            conf_obj = pred_conf[obj_mask].mean()\n            conf_noobj = pred_conf[noobj_mask].mean()\n            conf50 = (pred_conf > 0.5).float()\n            iou50 = (iou_scores > 0.5).float()\n            iou75 = (iou_scores > 0.75).float()\n            detected_mask = conf50 * class_mask * tconf\n            precision = torch.sum(iou50 * detected_mask) / (conf50.sum() + 1e-16)\n            recall50 = torch.sum(iou50 * detected_mask) / (obj_mask.sum() + 1e-16)\n            recall75 = torch.sum(iou75 * detected_mask) / (obj_mask.sum() + 1e-16)\n\n            self.metrics = {\n                \"loss\": to_cpu(total_loss).item(),\n                \"x\": to_cpu(loss_x).item(),\n                \"y\": to_cpu(loss_y).item(),\n                \"w\": to_cpu(loss_w).item(),\n                \"h\": to_cpu(loss_h).item(),\n                \"conf\": to_cpu(loss_conf).item(),\n                \"cls\": to_cpu(loss_cls).item(),\n                \"cls_acc\": to_cpu(cls_acc).item(),\n                \"recall50\": to_cpu(recall50).item(),\n                \"recall75\": to_cpu(recall75).item(),\n                \"precision\": to_cpu(precision).item(),\n                \"conf_obj\": to_cpu(conf_obj).item(),\n                \"conf_noobj\": to_cpu(conf_noobj).item(),\n                \"grid_size\": grid_size,\n            }\n\n            return output, total_loss\n\n\nclass Darknet(nn.Module):\n    def __init__(self, config_path, img_size=416):\n        super(Darknet, self).__init__()\n        self.module_defs = parse_model_config(config_path)\n        self.hyperparams, self.module_list = create_modules(self.module_defs)\n        self.yolo_layers = [layer[0] for layer in self.module_list if hasattr(layer[0], \"metrics\")]\n        self.img_size = img_size\n        self.seen = 0\n        self.header_info = np.array([0, 0, 0, self.seen, 0], dtype=np.int32)\n\n    def forward(self, x, targets=None):\n        img_dim = x.shape[2]\n        loss = 0\n        layer_outputs, yolo_outputs = [], []\n        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n            if module_def[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\n                x = module(x)\n            elif module_def[\"type\"] == \"route\":\n                x = torch.cat([layer_outputs[int(layer_i)] for layer_i in module_def[\"layers\"].split(\",\")], 1)\n            elif module_def[\"type\"] == \"shortcut\":\n                layer_i = int(module_def[\"from\"])\n                x = layer_outputs[-1] + layer_outputs[layer_i]\n            elif module_def[\"type\"] == \"yolo\":\n                x, layer_loss = module[0](x, targets, img_dim)\n                loss += layer_loss\n                yolo_outputs.append(x)\n            layer_outputs.append(x)\n        yolo_outputs = to_cpu(torch.cat(yolo_outputs, 1))\n        return yolo_outputs if targets is None else (loss, yolo_outputs)\n\n    def load_darknet_weights(self, weights_path):\n        with open(weights_path, \"rb\") as f:\n            header = np.fromfile(f, dtype=np.int32, count=5)  \n            self.seen = header[3]  \n            weights = np.fromfile(f, dtype=np.float32)\n\n        cutoff = None\n        if \"darknet53.conv.74\" in weights_path:\n            cutoff = 75\n\n        ptr = 0\n        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n            if i == cutoff:\n                break\n            if module_def[\"type\"] == \"convolutional\":\n                conv_layer = module[0]\n                if module_def[\"batch_normalize\"]:\n                    bn_layer = module[1]\n                    num_b = bn_layer.bias.numel()\n                    \n                    bn_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.bias)\n                    bn_layer.bias.data.copy_(bn_b)\n                    ptr += num_b\n                    bn_w = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.weight)\n                    bn_layer.weight.data.copy_(bn_w)\n                    ptr += num_b\n                    bn_rm = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_mean)\n                    bn_layer.running_mean.data.copy_(bn_rm)\n                    ptr += num_b\n                    bn_rv = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(bn_layer.running_var)\n                    bn_layer.running_var.data.copy_(bn_rv)\n                    ptr += num_b\n                else:\n                    num_b = conv_layer.bias.numel()\n                    conv_b = torch.from_numpy(weights[ptr : ptr + num_b]).view_as(conv_layer.bias)\n                    conv_layer.bias.data.copy_(conv_b)\n                    ptr += num_b\n                num_w = conv_layer.weight.numel()\n                conv_w = torch.from_numpy(weights[ptr : ptr + num_w]).view_as(conv_layer.weight)\n                conv_layer.weight.data.copy_(conv_w)\n                ptr += num_w\n\n    def save_darknet_weights(self, path, cutoff=-1):\n        fp = open(path, \"wb\")\n        self.header_info[3] = self.seen\n        self.header_info.tofile(fp)\n\n        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n            if module_def[\"type\"] == \"convolutional\":\n                conv_layer = module[0]\n                if module_def[\"batch_normalize\"]:\n                    bn_layer = module[1]\n                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n                else:\n                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n                conv_layer.weight.data.cpu().numpy().tofile(fp)\n\n        fp.close()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:07:00.814355Z","iopub.execute_input":"2022-05-29T20:07:00.814913Z","iopub.status.idle":"2022-05-29T20:07:00.986869Z","shell.execute_reply.started":"2022-05-29T20:07:00.814875Z","shell.execute_reply":"2022-05-29T20:07:00.986184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Class Cuadrante \"CuadranteRalec\"\nClase Cuadrante de verificación de pertenencia y complementación en función de algún cuadrante encontrado, completando la cantidad de cuadrantes no encontrados por la Red Convolucional YOLO.\n\n\n![Funcion de pertenencia del cuadrante](http://drive.google.com/uc?export=view&id=1Cys0_Vr9F8zOhYoPa0nrpAZfMrtwFkCj)","metadata":{}},{"cell_type":"code","source":"class cuadrante:\n    lista=[]\n    cuadrante=[]\n    p=[]\n    h_cu=0\n    w_cu=0\n    def __init__(self,csi,csd,cii,cid,h_c,w_c):\n        self.h_cu=h_c\n        self.w_cu=w_c\n        self.cuadrante=np.zeros(4,dtype=int)\n        self.lista = np.zeros(16,dtype=int)\n\n        if len(csi)==4 and self.correcto(csi[3],csi[1])==1:\n            self.cuadrante[0]=1\n            self.lista[0] = csi[2]  # y1\n            self.lista[1] = csi[3]  # y2\n            self.lista[2] = csi[0]  # x1\n            self.lista[3] = csi[1]  # x2\n        if len(csd)==4 and self.correcto(csd[3],csd[1])==2:\n            self.cuadrante[1]=1\n            self.lista[4] = csd[2]  # y1\n            self.lista[5] = csd[3]  # y2\n            self.lista[6] = csd[0]  # x1\n            self.lista[7] = csd[1]  # x2\n        if len(cii)==4 and self.correcto(cii[3],cii[1])==3:\n            self.cuadrante[2]=1\n            self.lista[8] = cii[2]  # y1\n            self.lista[9] = cii[3]  # y2\n            self.lista[10] = cii[0]  # x1\n            self.lista[11] = cii[1]  # x2\n        if len(cid)==4 and self.correcto(cid[3],cid[1])==4:\n            self.cuadrante[3]=1\n            self.lista[12] = cid[2]  # y1\n            self.lista[13] = cid[3]  # y2\n            self.lista[14] = cid[0]  # x1\n            self.lista[15] = cid[1]  # x2\n\n    def correcto(self,h,w):\n        sector=0\n        #cuadrante 1\n        if ((h-self.h_cu/4)<self.h_cu/2) and ((w-self.w_cu/4)<self.w_cu/2):\n            sector=1\n        #cuadrante 2\n        if ((h-self.h_cu/4)<self.h_cu/2) and ((w-self.w_cu/4)>self.w_cu/2):\n            sector=2\n        #cuadrante 3\n        if((h-self.h_cu/4)>self.h_cu/2) and ((w-self.w_cu/4)<self.w_cu/2):\n            sector=3\n        # cuadrante 4\n        if ((h - self.h_cu / 4) > self.h_cu / 2) and ((w - self.w_cu / 4) < self.w_cu / 2):\n            sector=4\n\n        return sector\n\n    def completo(self):\n        return (np.prod(self.cuadrante),self.cuadrante)\n\n    def cuadranteCenter(self,index):\n        y=0\n        x=0\n        if index==0:\n            y = int((self.h_cu + (2 * self.lista[1])) / 4)\n            x = int((self.w_cu + (2 * self.lista[3])) / 4)\n        if index==1:\n            y = int((self.h_cu + (2 * self.lista[5])) / 4)\n            x = int((self.w_cu + (2 * self.lista[6])) / 4)\n        if index==2:\n            y = int((self.h_cu + (2 * self.lista[8])) / 4)\n            x = int((self.w_cu + (2 * self.lista[11])) / 4)\n        if index==3:\n            y = int((self.h_cu + (2 * self.lista[12])) / 4)\n            x = int((self.w_cu + (2 * self.lista[14])) / 4)\n        temp = [y, x]\n        self.p = temp\n\n    def cuadranteCompletar(self,x,y):\n        # en funcion de Cuadrante 0\n        if x==0 and y==1:\n            self.lista[4] = self.lista[0]\n            self.lista[5] = self.lista[1]\n            self.lista[6] = self.p[1] - 100\n            self.lista[7] = int((self.w_cu + (2 * (self.lista[6] + self.lista[3] + self.lista[2]))) / 4)\n\n        if x==0 and y==2:\n            self.lista[8] = self.p[0] - 200\n            self.lista[9] = int((self.h_cu / 2) + self.lista[8])\n            c = 0\n            if (self.w_cu / 2) - self.lista[2] > 300:\n                c = -200\n            self.lista[10] = self.lista[2] + c\n            self.lista[11] = self.p[1] + 200\n\n        if x==0 and y==3:\n            self.lista[12] = self.p[0] - 100\n            self.lista[13] = int((self.h_cu / 2) + self.lista[12])\n            self.lista[14] = self.p[1] - 100\n            self.lista[15] = int((self.w_cu / 2) + self.lista[14])\n        # en funcion de Cuadrante 1 csd\n        if x==1 and y==0:\n            self.lista[0] = self.lista[4]\n            self.lista[1] = self.lista[5]\n            self.lista[3] = self.p[1] + 100\n            self.lista[2] = int((2 * (self.lista[7] - self.lista[6] - self.lista[3]) + self.w_cu) / 4)\n\n        if x==1 and y==2:\n            self.lista[8] = self.p[0] - 100\n            self.lista[9] = int((self.h_cu / 2) + self.lista[8]) - 200\n            c = 0\n            if (self.w_cu / 2) - self.lista[2] > 300:\n                c = -200\n            self.lista[10] = self.lista[2] + c\n            # self.lista[10] =int((2*(self.lista[7]+self.lista[6]+self.lista[3])+self.w)/4)\n            self.lista[11] = self.p[1] + 100\n\n        if x==1 and y==3:\n            self.lista[12] = self.p[0] - 200\n            self.lista[13] = int((self.h_cu / 2) + self.lista[12]) - 200\n            self.lista[14] = self.p[1] - 200\n            c = 0\n            if (self.w_cu / 2) - self.lista[14] > 300:\n                c = -200\n            self.lista[15] = int((self.w_cu / 2) + self.lista[14]) + c\n\n        # en funcion de Cuadrante 2 cii\n        if x==2 and y==0:\n            self.lista[1] = self.p[0] - 100\n            c = 0\n            t = int((2 * (self.lista[9] - self.lista[8] - self.lista[1]) + self.h_cu) / 4)\n            if t > 500:\n                c = -300\n            self.lista[0] = t + c\n            c = 0\n            if ((self.w_cu / 2) - self.lista[10] > 300):\n                c = -100\n            self.lista[2] = self.lista[10] - c\n            self.lista[3] = self.lista[11]\n\n        if x==2 and y==1:\n            self.lista[4] = self.lista[0]\n            self.lista[5] = self.p[0] + 100\n            self.lista[6] = self.p[1] - 100\n            self.lista[7] = int((self.w_cu / 2) + self.lista[6]) - 100\n\n        if x==2 and y==3:\n            self.lista[12] = self.lista[8]\n            self.lista[13] = self.lista[9]\n            self.lista[14] = self.p[1] - 200\n            self.lista[15] = int((self.w_cu / 2) + self.lista[14])\n\n        # en funcion de Cuadrante 3 cid\n        if x==3 and y==0:\n            self.lista[1] = self.p[0] + 100\n            self.lista[3] = self.p[1] + 100\n            t = int((2 * (self.lista[13] - self.lista[12] - self.lista[1]) + self.h_cu) / 4)\n            c = 0\n            if t - 200 > 0:\n                c = -200\n            self.lista[0] = t + c\n            t = int((2 * (self.lista[15] - self.lista[14] - self.lista[3]) + self.w_cu) / 4)\n            c = 0\n            if t - 300 > 0:\n                c + -300\n            self.lista[2] = t + c\n\n        if x==3 and y==1:\n            self.lista[5] = self.lista[1] - 100\n            self.lista[6] = self.lista[14]\n            self.lista[7] = self.lista[15]\n            # self.lista[4]=int((2*(self.lista[13]-self.lista[12]-self.lista[5])+self.h)/4)\n            self.lista[4] = self.lista[0]\n\n        if x==3 and y==2:\n            self.lista[8] = self.lista[12]\n            self.lista[9] = self.lista[13]\n            self.lista[11] = self.p[1] + 100\n            # self.lista[10] =int((2*(self.lista[15]-self.lista[14]-self.lista[11])+self.w)/4)\n            c = 0\n            if self.lista[2] - 200 > 0:\n                c = -200\n            self.lista[10] = self.lista[2] + c\n\n    def getLista(self):\n        completo=np.prod(self.cuadrante)\n        if completo == 0:\n            for l in range(0, len(self.cuadrante)):\n                if self.cuadrante[l] == 1 and completo == 0:\n                    self.cuadranteCenter(l)\n                    for index in range(0, len(self.cuadrante)):\n                        if l != index and self.cuadrante[index] == 0:\n                            self.cuadranteCompletar(l, index)\n                            self.cuadrante[index] = 1\n                completo = np.prod(self.cuadrante)\n\n        return self.lista","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:07:30.435605Z","iopub.execute_input":"2022-05-29T20:07:30.436166Z","iopub.status.idle":"2022-05-29T20:07:30.49067Z","shell.execute_reply.started":"2022-05-29T20:07:30.43613Z","shell.execute_reply":"2022-05-29T20:07:30.489813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Funciones y Procedimientos\nFunciones y procedimientos para la deteccion del cuadrante, en su etapa de identificacion:\n\n**DcmToJpg:** realiza la lectura y verifica si la Radiografia de Torax PosteroAnterio es soportada por la libreria pydicom o si cuenta con la cantidad de bit's necesaria para poder ser tratada. el archivo **.dcm**  es guardado en el directorio **./torax/nombreDicom.jpg**.\n\n**detectorCuadrante:** detecta la pertenencia de un cuadrante de acuerdo a las siguientes etiquetas:\n*     Jcsi: Cuadrante Superior Izquierdo\n*     Jcsd: Cuadrante Superior Derecho\n*     Jcii: Cuadrante Inferior Izquierdo\n*     Jcid: Cuadrante Inferior Derecho\n\nlos cuadrantes detectados se guardan en **./torax/cuadrante/nombreDicom/nombreDicom_jc__.jpg**","metadata":{}},{"cell_type":"code","source":"def convertiRgb(img):\n    b=img[:, :, 0].copy()\n    g=img[:, :, 1].copy()\n    rc=img[:, :, 2].copy()\n\n    img[:, :, 0]=rc\n    img[:, :, 1] =g\n    img[:, :, 2] =b\n\n    return img\n\ndef convertirBgr(img):\n    rc = img[:, :, 0].copy()\n    g = img[:, :, 1].copy()\n    b = img[:, :, 2].copy()\n    \n    img[:, :, 0] = b\n    img[:, :, 1] = g\n    img[:, :, 2] = rc\n    \n    return img\n\ndef desvorde(lista_d):\n    for i in range(0,len(lista_d)):\n        if(lista_d[i]<0):\n            lista_d[i]=10\n    return lista_d\n\ndef DcmToJpg(pathDcm):\n    ds=pydi.dcmread(pathDcm)\n    file=''\n    temp_=''\n    name=''\n    try:\n        d=np.array(ds.pixel_array)\n        name=pathDcm[64:]\n        file=name.replace('.dcm','')\n        name=name.replace('.dcm','.jpg')\n        temp_='./torax/cuadrante/'+file\n        if not os.path.exists(temp_):\n            os.mkdir(temp_)\n        plt.imsave('./torax/'+name,d)\n    except:\n        print('Error Archivo DCM no Soportado')\n    \n    return(temp_,name,file)\n\ndef detectorCuadrante(ImgFile,ruta,name):\n    jcsi_=[]\n    jcsd_=[]\n    jcii_=[]\n    jcid_=[]\n    lista_=[]\n    cont=0\n\n    imgRadio=cv2.imread(ImgFile)\n    h_c=imgRadio.shape[0]\n    w_c=imgRadio.shape[1]\n    RGBimg=convertiRgb(imgRadio)\n    imgTensor=transforms.ToTensor()(imgRadio)\n    imgTensor,_=pad_to_square(imgTensor,0)\n    imgTensor=resize(imgTensor,416)\n    imgTensor=imgTensor.unsqueeze(0)\n    imgTensor=Variable(imgTensor.type(Tensor))\n\n    detections=modelo(imgTensor)\n    detections=non_max_suppression(detections,confThres,0.4)\n    \n    for detection in detections:\n        if detection is not None:\n            detection=rescale_boxes(detection,416, RGBimg.shape[:2])\n            for (x1,y1,x2,y2,conf,cls_conf,cls_pred)in detection:\n                temp=[int(x1),int(x2),int(y1),int(y2)]\n                if int(cls_pred)==0:\n                    jcsi_=temp\n                if int(cls_pred)==1:\n                    jcsd_=temp\n                if int(cls_pred)==2:\n                    jcii_=temp\n                if int(cls_pred)==3:\n                    jcid_=temp\n\n        jcsi_=desvorde(jcsi_)\n        jcsd_=desvorde(jcsd_)\n        jcii_=desvorde(jcii_)\n        jcid_=desvorde(jcid_)\n\n        r_lista=cuadrante(jcsi_,jcsd_,jcii_,jcid_,h_c,w_c)\n        lista_=r_lista.getLista()\n        lista_=desvorde(lista_)\n\n        obj1=imgRadio[lista_[0]:lista_[1],lista_[2]:lista_[3]]\n        obj2=imgRadio[lista_[4]:lista_[5],lista_[6]:lista_[7]]\n        obj3=imgRadio[lista_[8]:lista_[9],lista_[10]:lista_[11]]\n        obj4=imgRadio[lista_[12]:lista_[13],lista_[14]:lista_[15]]\n        \n        suma=np.sum(lista_)\n        if suma>0:\n            plt.imsave(ruta+'/'+name+'jcsi.jpg',obj1)\n            plt.imsave(ruta+'/'+name+'jcsd.jpg',obj2)\n            plt.imsave(ruta+'/'+name+'jcii.jpg',obj3)\n            plt.imsave(ruta+'/'+name+'jcid.jpg',obj4)\n        else:\n            print(cont,'Error cuadrante no detectado')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:07:40.374537Z","iopub.execute_input":"2022-05-29T20:07:40.375269Z","iopub.status.idle":"2022-05-29T20:07:40.398883Z","shell.execute_reply.started":"2022-05-29T20:07:40.37523Z","shell.execute_reply":"2022-05-29T20:07:40.397985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lectura de Modelo YOLO y lectura de file test\n* **weightPah:** pesos que permite reconocer los cuadrantes.\n* **modelDef:** modelo YOLO. \n* **classPath:** numero de clases para la deteccion de los cuadrantes.\n* **confThers:** porcentaje de validacion para la deteccion de un cuadrante\n\nlectura de la carpeta test para la prediccion de Covid-19 del set de Datos **siim-codiv19-detection**, ruta **../input/siim-covid19-detection/test/**","metadata":{}},{"cell_type":"code","source":"weightPath='../input/dataaug/yolov3_ckpt_299.pth'\nmodelDef='../input/datamodelo/yolov3-custom.cfg'\nclassPath='../input/datamodelo/classes.names'\nconfThres=0.90\n\npathTest='../input/siim-covid19-detection/test/'\ntemp=r'**/*.dcm'\nl=list(pathlib.Path(pathTest).glob(temp))\n\npath='./torax/'\nif not os.path.exists(path):\n    os.mkdir(path)\n    os.mkdir(path+'cuadrante')\n\nprint('Segmento Terminado')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:07:48.068391Z","iopub.execute_input":"2022-05-29T20:07:48.068982Z","iopub.status.idle":"2022-05-29T20:07:59.14273Z","shell.execute_reply.started":"2022-05-29T20:07:48.068934Z","shell.execute_reply":"2022-05-29T20:07:59.141823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Construccion Red YOLO\nmediante torch selecionamos el dispocitivo de calculo y definimos el tensor de calculo","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodelo=Darknet(modelDef,img_size=512).to(device)\nmodelo.load_state_dict(torch.load(weightPath))\nmodelo.eval()\n\nclasses=load_classes(classPath)\nTensor=torch.FloatTensor\n\nprint('Segmento Terminado')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:08:03.784004Z","iopub.execute_input":"2022-05-29T20:08:03.784776Z","iopub.status.idle":"2022-05-29T20:08:07.099696Z","shell.execute_reply.started":"2022-05-29T20:08:03.784736Z","shell.execute_reply":"2022-05-29T20:08:07.09884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Proceso de Imagen DCM\n**DcmToJpg:** convierte el archivo **.DCM** a **.JPG**., la funcion debuleve los datos ruta, archivo, name de la cual se obtiene la ruta del file.jpg, el file.jpg, y el nombre de la carpeta.\n\n**detectorCuadrante:** detectamos el cuadrante por medio de la funcion, mandando como parametros imagen de origen, ruta de los cuadrantes, y el nombre. **detectorCuadrante('./torax/'+archivo,ruta,name)**\n","metadata":{}},{"cell_type":"code","source":"#ruta,archivo,name=DcmToJpg('../input/siim-covid19-detection/train/00086460a852/9e8302230c91/65761e66de9f.dcm')\n#ruta,archivo,name=DcmToJpg('../input/siim-covid19-detection/train/0159c2d40b52/e68dcd68c9b7/e498491b1526.dcm')\n#ruta,archivo,name=DcmToJpg('../input/siim-covid19-detection/test/05ce6249b5c0/4f28dfe3ff88/e226747a2d76.dcm')\n#ruta,archivo,name=DcmToJpg('../input/siim-covid19-detection/train/fab641b3ce7c/82fb75bee506/c2d72bbff9fc.dcm')\nruta,archivo,name=DcmToJpg('../input/siim-covid19-detection/test/c85581436d7c/230faa37bfd9/9f6cc74b0062.dcm')\n\nprint(ruta,archivo, name)\n\nprint('Segmento Terminado')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:08:11.809395Z","iopub.execute_input":"2022-05-29T20:08:11.809683Z","iopub.status.idle":"2022-05-29T20:08:12.571238Z","shell.execute_reply.started":"2022-05-29T20:08:11.809651Z","shell.execute_reply":"2022-05-29T20:08:12.570493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detectorCuadrante('./torax/'+archivo,ruta,name)\nprint('Segmento Terminado')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:08:17.283693Z","iopub.execute_input":"2022-05-29T20:08:17.284007Z","iopub.status.idle":"2022-05-29T20:08:18.687699Z","shell.execute_reply.started":"2022-05-29T20:08:17.283975Z","shell.execute_reply":"2022-05-29T20:08:18.686822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Proceso de Deteccion de Neumonía Atípica \"Covid-19\"\n\ncargamos los pesos y el modelo en **ConvRalec**, [**modeloRalec.h5**, **pesosRalec.h5**] \n\ncreamos el arreglo **rale=[['jcsi',0],['jcsd',0],['jcii',0],['jcid',0]]** para la clasificacion y propiedad del cuadrante.\n\nredimencionamos las imagenes **altura=224** y **longitud=224**\n\nclasificamos los cuadrantes y mostramos el resultado: sumamos los valores rale, la sumatoria al dar 0 da como resultado el **negativo al Covid-19**\n\n","metadata":{}},{"cell_type":"code","source":"h_r,w_r=224,224\nmodelo_r='../input/modeloralec/modeloRalec.h5'\npesos_r='../input/modeloralec/pesosRalec.h5'\nConvRalec=load_model(modelo_r)\nConvRalec.load_weights(pesos_r)\n\npathC='./torax/cuadrante/'+name+'/'\nrale=[['jcsi',0],['jcsd',0],['jcii',0],['jcid',0]]\n\nprint('Segmento Terminado')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:08:28.703495Z","iopub.execute_input":"2022-05-29T20:08:28.704251Z","iopub.status.idle":"2022-05-29T20:08:30.672379Z","shell.execute_reply.started":"2022-05-29T20:08:28.704213Z","shell.execute_reply":"2022-05-29T20:08:30.671559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Recorremos los cuadrantes y predecimos cada uno de ellos con el modelo convRalec","metadata":{}},{"cell_type":"code","source":"for cuadrantes in os.listdir(pathC):\n    temp_c=load_img(pathC+cuadrantes,target_size=(w_r,h_r))\n    temp_c=img_to_array(temp_c)\n    temp_c=np.expand_dims(temp_c,axis=0)\n    tmp_c=ConvRalec.predict(temp_c)\n    resulT=tmp_c[0]\n    rx=np.argmax(resulT)\n    if rx==0 or rx==1:\n        if rx==1:\n            rale[3][1]=1\n    if rx==2 or rx==3:\n        if rx==3:\n            rale[2][1]=1\n    if rx==4 or rx==5:\n        if rx==5:\n            rale[1][1]=1\n    if rx==6 or rx==7:\n        if rx==7:\n            rale[0][1]=1\n\ndiag=0\nfor ra in rale:\n    diag+=ra[1]\ntitulos=['Negatigo Covid-19','Negativo Covid-19','Negativo Covid-19','Negativo Covid-19']\nif diag>0:\n    print('=============================================')\n    print('Resultados de Diagnostico')\n    print('Paciente con Diagnostico Atipico')\n    print('=============================================')\n    print('Detalle Cuadrantes')\n    i=0\n    for ra in rale:\n        if ra[1]==1:\n            print('Cuadrante ',ra[0],'Atipico')\n            titulos[i]='Atipico'\n        else:\n            print('Cuadrante ',ra[0],'Negativo')\n        i+=1\nelse:\n    print('Paciente con Diagnostico de Covid-19 Negativo')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:08:34.308727Z","iopub.execute_input":"2022-05-29T20:08:34.30941Z","iopub.status.idle":"2022-05-29T20:08:35.173272Z","shell.execute_reply.started":"2022-05-29T20:08:34.309369Z","shell.execute_reply":"2022-05-29T20:08:35.172345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cuadrantes y Resultados\nMostramos la **Radiografia de Torax PosteroAnterior** con sus respectivos cuadrantes con el detalle de su clasificacion. ","metadata":{}},{"cell_type":"code","source":"imgRadio=cv2.imread('./torax/'+archivo)\nimgRadio=convertirBgr(imgRadio)\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.suptitle('Resultados de predicción ConvRalec',fontsize=20,ha='center')\nplt.subplot2grid((2,3),(0,0),rowspan=2)\n#plt.subplot(2,3,1)\nplt.imshow(imgRadio)\nplt.title('Radiografia de Torax AP')\n\nobj1 = cv2.imread(ruta+'/'+name+'jcsi.jpg')\nobj1=convertirBgr(obj1)\nplt.subplot2grid((2, 3), (0, 1))\nplt.imshow(obj1)\nplt.title(titulos[0]+' JCSI')\n\nobj2 = cv2.imread(ruta+'/'+name+'jcsd.jpg')\nobj2=convertirBgr(obj2)\nplt.subplot2grid((2, 3), (0, 2))\nplt.imshow(obj2)\nplt.title(titulos[1]+' JCSD')\n\nobj3 = cv2.imread(ruta+'/'+name+'jcii.jpg')\nobj3=convertirBgr(obj3)\nplt.subplot2grid((2,3),(1,1))\nplt.imshow(obj3)\nplt.title(titulos[2]+' JCII')\n\nobj4 = cv2.imread(ruta+'/'+name+'jcid.jpg')\nobj4=convertirBgr(obj4)\nplt.subplot2grid((2,3),(1,2))\nplt.imshow(obj4)\nplt.title(titulos[3]+' JCID')\n\nplt.savefig('./torax/'+name+'dia.jpg',)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:08:41.693683Z","iopub.execute_input":"2022-05-29T20:08:41.694643Z","iopub.status.idle":"2022-05-29T20:08:44.884838Z","shell.execute_reply.started":"2022-05-29T20:08:41.694589Z","shell.execute_reply":"2022-05-29T20:08:44.884011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracción de Características.\nLectura de los conjuntos de datos para la extracción de las características en la cuarta capa convolucionada y de maxpooling.  ","metadata":{}},{"cell_type":"code","source":"h_e,w_e=224,224\nbatch_size=16\npath_Train='../input/dataconvralec/torax/train/'\npath_Valid='../input/dataconvralec/torax/valid/'\npath_Test='../input/dataconvralec/torax/test/'\n\nDataGenTrain=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\nimaTrain=DataGenTrain.flow_from_directory(path_Train,target_size=(h_e,w_e),batch_size=batch_size,class_mode='categorical')\n\nDataGenValid=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\nimaValid=DataGenValid.flow_from_directory(path_Valid,target_size=(h_e,w_e),batch_size=batch_size,class_mode='categorical')\n\nDataGenTest=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\nimaTest=DataGenTest.flow_from_directory(path_Test,target_size=(h_e,w_e),batch_size=batch_size,class_mode='categorical')\n\nprint('Segmento Terminado')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:08:54.883871Z","iopub.execute_input":"2022-05-29T20:08:54.884159Z","iopub.status.idle":"2022-05-29T20:08:57.318655Z","shell.execute_reply.started":"2022-05-29T20:08:54.88413Z","shell.execute_reply":"2022-05-29T20:08:57.317788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lectura de imagen, como parámetro la ruta del cuadrante detectado y su nombre \"los cuadrantes deben de ser creados para obtener las características\" ","metadata":{}},{"cell_type":"code","source":"#imagen=tf.keras.preprocessing.image.load_img('./torax/cuadrante/c2d72bbff9fc/c2d72bbff9fcjcsi.jpg',target_size=(h_e,w_e))\n\nimagen=tf.keras.preprocessing.image.load_img(ruta+'/'+name+'jcsi.jpg',target_size=(h_e,w_e))\nimagen=tf.keras.preprocessing.image.img_to_array(imagen)\nimg=np.expand_dims(imagen,axis=0)\nimg.shape\nprint('Segmento Terminado')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:09:05.655559Z","iopub.execute_input":"2022-05-29T20:09:05.655883Z","iopub.status.idle":"2022-05-29T20:09:05.675997Z","shell.execute_reply.started":"2022-05-29T20:09:05.655836Z","shell.execute_reply":"2022-05-29T20:09:05.67508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtención de las gráficas de cada característica en el lote de imágenes espaciales en la cuarta capa, proceso de ingreso al flatening, para la predicción y detección de Neumonía Atípica.","metadata":{}},{"cell_type":"code","source":"from collections import OrderedDict\ncapas=OrderedDict()\ni=0\nfor capa in ConvRalec.layers:\n    capas[capa.name]=capa\n\ndef get_activacion(modelo,capa,imagen):\n    f=tf.keras.backend.function([modelo.layers[0].input],[capa.output])\n    activacion=f((imagen))\n    return activacion\n\ncapa_nombre='conv_4'\n#capa_nombre='relu_4'\n#capa_nombre='max_4'\ncapa=capas[capa_nombre]\nactivador=get_activacion(ConvRalec,capa,img)\nactivo=activador[0]\nimg_activo=activo[0]\n\nfig=plt.figure(figsize=(20,20))\nn=8\n\nfor i in range(8):\n    for j in range(8):\n        idx=n*i+j\n        ax=fig.add_subplot(n,n,idx+1)\n        ax.imshow(img_activo[:,:,idx])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:09:09.842668Z","iopub.execute_input":"2022-05-29T20:09:09.843249Z","iopub.status.idle":"2022-05-29T20:09:16.295807Z","shell.execute_reply.started":"2022-05-29T20:09:09.843211Z","shell.execute_reply":"2022-05-29T20:09:16.294906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Matriz de Confusión.\n\nFunción de Matriz de confusión, recorre los datos de entrenamiento (DE), validación (DV) y test (DT), los mismos son clasificados en el arreglo **\"resulT=['jcidn','jcidp','jciin','jciip','jcsdn','jcsdp','jcsin','jcsip']\"**, para contabilizar las predicciones positivas y negativas.\n\n![array de resultado de prediccion](http://drive.google.com/uc?export=view&id=1IdL-ITFptn6Iummq2_LNNqtZu-68nyUV)","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.image import load_img, img_to_array\nfrom keras.models import load_model\n\ndef confucion(store):\n    k=-1\n    lTest=[]\n    lPred=[]\n    path=path_Test\n    c=['jcidn','jcidp','jciin','jciip','jcsdn','jcsdp','jcsin','jcsip']\n    #c=['n','p']\n    for clase in os.listdir(store):\n        for i in range(0,len(c)):\n            if clase==c[i]:\n                k=i\n                break\n        print(clase, k)\n        for file in os.listdir(store+clase):\n            #print(path+clase+'/'+file)\n            lTest.append(k)\n            temp = load_img(store+clase+'/'+file, target_size=(w_e, h_e))\n            temp = img_to_array(temp)\n            temp = np.expand_dims(temp, axis=0)\n            tmp = ConvRalec.predict(temp)\n            resulT = tmp[0]\n            #print(path+clase+'/'+file,resulT, clase, k)\n            r = np.argmax(resulT)\n            lPred.append(r)\n    \n    return lTest,lPred\n\nprint('Segmento Terminado')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:09:22.365263Z","iopub.execute_input":"2022-05-29T20:09:22.365516Z","iopub.status.idle":"2022-05-29T20:09:22.375737Z","shell.execute_reply.started":"2022-05-29T20:09:22.365489Z","shell.execute_reply":"2022-05-29T20:09:22.375067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conteo de valores Predictivos Positivos y Predictivos Negativos, obtención de matrices","metadata":{}},{"cell_type":"code","source":"print('========================================') \nreal,pred=confucion(path_Train)\nmat_train=confusion_matrix(real,pred)\nprint('Matris Entrenamiento')\nprint(mat_train)\n\nprint('========================================') \nreal,pred=confucion(path_Valid)\nmat_valid=confusion_matrix(real,pred)\nprint('Matris Validacion')\nprint(mat_valid)\n\nprint('========================================') \nreal,pred=confucion(path_Test)\nmat_test=confusion_matrix(real,pred)\nprint('Matris Testing')\nprint(mat_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:09:27.76255Z","iopub.execute_input":"2022-05-29T20:09:27.763223Z","iopub.status.idle":"2022-05-29T20:25:03.259709Z","shell.execute_reply.started":"2022-05-29T20:09:27.763178Z","shell.execute_reply":"2022-05-29T20:25:03.258032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Grafica de las Matrices (DE), (DV), (DT) ","metadata":{}},{"cell_type":"code","source":"print('Graficar Matrices')\nplot_confusion_matrix(conf_mat=mat_train)\nplot_confusion_matrix(conf_mat=mat_valid)\nplot_confusion_matrix(conf_mat=mat_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:25:09.707393Z","iopub.execute_input":"2022-05-29T20:25:09.707705Z","iopub.status.idle":"2022-05-29T20:25:11.032013Z","shell.execute_reply.started":"2022-05-29T20:25:09.707673Z","shell.execute_reply":"2022-05-29T20:25:11.031158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Arquitectura ConvRalec.\n\n* 4 capas convolucionales “Conv2D”, cada capa tiene un kernel de (3x3), un filtro de 2n, donde n = {5, 6, 7, 8} respectivamente.\n* La primera capa de convoluci´on cuenta con un input shape de (224, 224, 3) RGB \"forma de los cuadrantes en las imagenes de Radiografia de Torax PosteroAnterior”.\n* 4 capas de activaci´on “LeakyReLU”.\n* 4 capas de agrupaci´on “MaxPooling2D”, cada una con un pool size de (2x2).\n* 1 capa de aplanamiento “Flatten”, aplanando los datos para las capas densas.\n* 3 capas densas “Dense”, la primera de 256 de tama˜no y activaci´on “relu”, la segunda de 64 de tamano y activacion “relu” y la ultima de tamano 8 con una activacion de “softmax”, la cual indicara el estado de cada cuadrante y sera la capa de salida.\n* 1 capa borrado “Dropout” de salida al 50 %, con el fin de evitar un solo camino para la clasificaci´on de los cuadrantes.\n\n![Marco General de Metodo Propuesto](http://drive.google.com/uc?export=view&id=1lxjAkoH5ogQItXsUXflvOROPJYjP3ivX)","metadata":{}},{"cell_type":"code","source":"ConvRalec.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:25:16.688423Z","iopub.execute_input":"2022-05-29T20:25:16.688698Z","iopub.status.idle":"2022-05-29T20:25:16.702526Z","shell.execute_reply.started":"2022-05-29T20:25:16.68867Z","shell.execute_reply":"2022-05-29T20:25:16.701656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluación de los Casos de Prueba.\nlectura de archivo resultCov.csv con 50 muestras de la carpeta Test ","metadata":{}},{"cell_type":"code","source":"resultCov='../input/modeloralec/resultCov.csv'\ndf_result=pd.read_csv(resultCov,sep=',')\nfila,columna=df_result.shape\nlista_casos_prueba=[]\nfor l in range(0,fila):\n    csv=df_result.loc[l,'Covid-19']\n    lista_casos_prueba.append(csv)\n\n#convertimos DICOM a JPG\nlistaR=[]\nfor i in lista_casos_prueba:\n    temp=DcmToJpg(str(i))\n    if(temp[0]!=''):\n        k=[temp[0],temp[1],temp[2],i]\n        listaR.append(k)\n\n#Detectamos Cuadrantes\nfor i in listaR:\n    detectorCuadrante('./torax/'+i[1],i[0],i[2])\n\nprint('Segmento Terminado')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:25:22.02748Z","iopub.execute_input":"2022-05-29T20:25:22.028254Z","iopub.status.idle":"2022-05-29T20:26:54.022663Z","shell.execute_reply.started":"2022-05-29T20:25:22.028201Z","shell.execute_reply":"2022-05-29T20:26:54.021898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conteo de pruebas positivas y negativas \nObtenemos los resultados paralas pruebas satisfactorias y no satisfactorias y su correspondiente porcentaje para las pruebas posteriores.","metadata":{}},{"cell_type":"code","source":"\nsatisfactorio=0\nno_satisfactorio=0\nfor i in listaR:\n    pathC='./torax/cuadrante/'+i[2]+'/'\n    rale=[['jcsi',0],['jcsd',0],['jcii',0],['jcid',0]]\n    \n    for cuadrantes in os.listdir(pathC):\n        temp=load_img(pathC+cuadrantes,target_size=(224,224))\n        temp=img_to_array(temp)\n        temp=np.expand_dims(temp,axis=0)\n        tmp=ConvRalec.predict(temp)\n        resulT=tmp[0]\n        r=np.argmax(resulT)\n        if r==0 or r==1:\n            if r==1:\n                rale[3][1]=1\n        if r==2 or r==3:\n            if r==3:\n                rale[2][1]=1\n        if r==4 or r==5:\n            if r==5:\n                rale[1][1]=1\n        if r==6 or r==7:\n            if r==7:\n                rale[0][1]=1\n\n    diag=0\n    for r in rale:\n        diag+=r[1]\n    \n    if diag>0:\n        satisfactorio=satisfactorio+1\n    else:\n        no_satisfactorio=no_satisfactorio+1\n        \n        \nprint('==========================================================')\nprint('Detalle\\t\\t\\t\\t','Resultado')\nprint('----------------------------------------------------------')\nprint('Pruebas Totales\\t\\t\\t','n =',len(lista_casos_prueba))\nprint('Pruebas Satisfactorias\\t\\t','p =',satisfactorio)\nprint('Pruebas No Satisfactorias\\t','q =',no_satisfactorio)\nprint('Porcentaje de Exitos\\t\\t','p_0 =',(satisfactorio/(len(lista_casos_prueba)))*100,'%')\nprint('Porcentaje de Fracasos\\t\\t','q_0 =',(no_satisfactorio/(len(lista_casos_prueba)))*100,'%')\nprint('----------------------------------------------------------')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:32:46.660031Z","iopub.execute_input":"2022-05-29T20:32:46.660948Z","iopub.status.idle":"2022-05-29T20:33:03.524235Z","shell.execute_reply.started":"2022-05-29T20:32:46.660903Z","shell.execute_reply":"2022-05-29T20:33:03.523361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lectura de los Archivos Train_image_level.csv y Train_study_level.csv\nArchivos que contienen la información correspondiente a las muestras para los casos de Entrenamiento (DE), Validación (DV), Test (DT).","metadata":{}},{"cell_type":"code","source":"train_image_csv='../input/siim-covid19-detection/train_image_level.csv'\ntrain_study_csv='../input/siim-covid19-detection/train_study_level.csv'\ndf_train_image_csv=pd.read_csv(train_image_csv,sep=',')\ndf_train_study_csv=pd.read_csv(train_study_csv,sep=',')\nprint('Informacion archivo train_image_level.csv')\nprint(df_train_image_csv)\nprint('##########################################\\n')\nprint('Informacion archivo train_study_level.csv')\nprint(df_train_study_csv)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T20:33:10.171697Z","iopub.execute_input":"2022-05-29T20:33:10.172007Z","iopub.status.idle":"2022-05-29T20:33:10.259312Z","shell.execute_reply.started":"2022-05-29T20:33:10.171973Z","shell.execute_reply":"2022-05-29T20:33:10.258447Z"},"trusted":true},"execution_count":null,"outputs":[]}]}