{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport os\nimport shutil\nimport sys\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\nimport pydicom\nimport cv2\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom skimage import exposure\nfrom ast import literal_eval\nfrom matplotlib.patches import Rectangle","metadata":{"_uuid":"ce28f29df7b9f64725998f8133be50068c53cb69","execution":{"iopub.status.busy":"2021-06-11T05:13:17.261544Z","iopub.execute_input":"2021-06-11T05:13:17.261861Z","iopub.status.idle":"2021-06-11T05:13:18.034011Z","shell.execute_reply.started":"2021-06-11T05:13:17.261792Z","shell.execute_reply":"2021-06-11T05:13:18.033183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_stat = 123\nnp.random.seed(random_stat)","metadata":{"_uuid":"c4b33affdc049734daa58852c4ed06e37c344063","execution":{"iopub.status.busy":"2021-06-11T05:13:22.005986Z","iopub.execute_input":"2021-06-11T05:13:22.006317Z","iopub.status.idle":"2021-06-11T05:13:22.010324Z","shell.execute_reply.started":"2021-06-11T05:13:22.00626Z","shell.execute_reply":"2021-06-11T05:13:22.009466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/pjreddie/darknet.git\n!apt-get install -y python-opencv\n\n%cd darknet\n\n!make","metadata":{"_uuid":"558964006de8b4e454d76c91a5d0a605e397a93d","execution":{"iopub.status.busy":"2021-06-11T05:13:24.975251Z","iopub.execute_input":"2021-06-11T05:13:24.975545Z","iopub.status.idle":"2021-06-11T05:13:31.181395Z","shell.execute_reply.started":"2021-06-11T05:13:24.975492Z","shell.execute_reply":"2021-06-11T05:13:31.180653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(os.getcwd())","metadata":{"execution":{"iopub.status.busy":"2021-06-11T05:14:37.981167Z","iopub.execute_input":"2021-06-11T05:14:37.981462Z","iopub.status.idle":"2021-06-11T05:14:37.987813Z","shell.execute_reply.started":"2021-06-11T05:14:37.98141Z","shell.execute_reply":"2021-06-11T05:14:37.986966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = \"../../input/siim-covid19-detection\"\n\ntrain_dcm_dir = os.path.join(DATA_DIR, \"train\")\ntest_dcm_dir = os.path.join(DATA_DIR, \"test\")\n\nIMAGE_DIR = '../../input/covid19jpg/CovidJPG'\n\nlabel_dir = os.path.join(os.getcwd(), \"labels\")  # .txt\nmetadata_dir = os.path.join(os.getcwd(), \"metadata\") # .txt\n\n# YOLOv3 config file directory\ncfg_dir = os.path.join(os.getcwd(), \"cfg\")\n# YOLOv3 training checkpoints will be saved here\nbackup_dir = os.path.join(os.getcwd(), \"backup\")\n\nfor directory in [label_dir, metadata_dir, cfg_dir, backup_dir]:\n    if os.path.isdir(directory):\n        continue\n    os.mkdir(directory)","metadata":{"_uuid":"55f5d67ac085afcc445db23227ba60748eb269fa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. Generate images and labels for training YOLOv3\n* YOLOv3 needs .txt file for each image, which contains ground truth object in the image that looks like:\n```\n<object-class_1> <x_1> <y_1> <width_1> <height_1>\n<object-class_2> <x_2> <y_2> <width_2> <height_2>\n```\n* <object-class\\>: Since RSNA task is binary classification basically, <object-class\\> is 0.\n* <x\\>, <y\\>: Those are float values of bbox center coordinate, divided by image width and height respectively.\n* <w\\>, <h\\>: Those are width and height of bbox, divided by image width and height respectively.\n\n* So it is different from the format of label data provided by kaggle. We should change it.","metadata":{"_uuid":"9b26d929bb119deca558ac983a5ce4e95fe9b7cc"}},{"cell_type":"code","source":"df_image = pd.read_csv(os.path.join(DATA_DIR, 'train_image_level.csv'))\ndf_study = pd.read_csv(os.path.join(DATA_DIR, 'train_study_level.csv'))\ndf_study['id'] = df_study['id'].str.replace('_study',\"\")\ndf_study.rename({'id': 'StudyInstanceUID'},axis=1, inplace=True)\ndf_train = df_image.merge(df_study, on='StudyInstanceUID')\ndf_train.loc[df_train['Negative for Pneumonia']==1, 'study_label'] = 'negative'\ndf_train.loc[df_train['Typical Appearance']==1, 'study_label'] = 'typical'\ndf_train.loc[df_train['Indeterminate Appearance']==1, 'study_label'] = 'indeterminate'\ndf_train.loc[df_train['Atypical Appearance']==1, 'study_label'] = 'atypical'\ndf_train.drop(['Negative for Pneumonia','Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance'], axis=1, inplace=True)\ndf_train['id'] = df_train['id'].str.replace('_image', '.jpg')\ndf_train['image_label'] = df_train['label'].str.split().apply(lambda x : x[0])\ndf_size = pd.read_csv('../../input/covid19jpg/CovidJPG/size.csv')\ndf_train = df_train.merge(df_size, on='id')\ndf_train.head(3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = os.path.join(IMAGE_DIR,'train')\ntest_dir = os.path.join(IMAGE_DIR, 'test')\n\ndef preprocess_image(img):\n    equ_img = exposure.equalize_adapthist(img/255, clip_limit=0.05, kernel_size=24)\n    return equ_img\n\ndf_opa = df_train[df_train['image_label']=='opacity'].reset_index()\nfig, axs = plt.subplots(5, 2, figsize=(10,20))\nfig.subplots_adjust(hspace=.2, wspace=.2)\nn=5\nfor i in range(n):\n    img = cv2.imread(os.path.join(train_dir, df_opa['id'][i]))\n    img_proc = preprocess_image(img)\n    axs[i, 0].imshow(img)\n    axs[i, 1].imshow(img_proc)\n    axs[i, 0].axis('off')\n    axs[i, 1].axis('off')\n    boxes = literal_eval(df_opa['boxes'][i])\n    for box in boxes:\n        axs[i, 0].add_patch(Rectangle((box['x']*(512/df_opa['dim1'][i]), box['y']*(512/df_opa['dim0'][i])), box['width']*(512/df_opa['dim1'][i]), box['height']*(512/df_opa['dim0'][i]), fill=0, color='y', linewidth=3))\n        axs[i, 0].set_title(df_opa['study_label'][i])\n        axs[i, 1].add_patch(Rectangle((box['x']*(512/df_opa['dim1'][i]), box['y']*(512/df_opa['dim0'][i])), box['width']*(512/df_opa['dim1'][i]), box['height']*(512/df_opa['dim0'][i]), fill=0, color='r', linewidth=3))\n        axs[i, 1].set_title('After CLAHE')\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def isNaN(string):\n    return string != string\n\ndef save_label(df, img_dir):\n    img_size = 512\n    \n    for index, row in df.iterrows():\n        if isNaN(row['boxes']):\n            continue\n        im_path = os.path.join(img_dir, row['id'])\n        label_path = os.path.join(label_dir, \"{}.txt\".format(row['id'].split('.')[0]))\n        if row['image_label'] == \"opacity\":\n            image_label = 1\n        else:\n            image_label = 0\n        f = open(label_path, \"a\")\n        boxes=row['boxes']\n        boxes = boxes.strip('[{')\n        boxes = boxes.strip('}]')\n        box_split = boxes.split('},')\n        my_boxes=[]\n        for box in box_split:\n            box = box.strip(' {')\n            box = box.strip('}')\n            box_elem = box.split(',')\n            my_box=[]\n            for elem in box_elem:\n                my_box.append(elem.split(': ')[1])\n                \n            top_left_x = float(my_box[0]) * 512/row['dim1']\n            top_left_y = float(my_box[1]) * 512/row['dim0']\n            w = float(my_box[2]) * 512/row['dim1']\n            h = float(my_box[3]) * 512/row['dim0']\n\n            rx = top_left_x/img_size\n            ry = top_left_y/img_size\n            rw = w/img_size\n            rh = h/img_size\n            rcx = rx+rw/2\n            rcy = ry+rh/2\n\n            line = \"{} {} {} {} {}\\n\".format(image_label, rcx, rcy, rw, rh)\n            f.write(line)            \n        f.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_label(df_train, train_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ../../input/covid19jpg/CovidJPG","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ex_patient_id = '00326161e51e'\nex_img_path = os.path.join(train_dir, \"{}.jpg\".format(ex_patient_id))\nex_label_path = os.path.join(label_dir, \"{}.txt\".format(ex_patient_id))\n\nplt.imshow(cv2.imread(ex_img_path))\n\nimg_size = 512\nwith open(ex_label_path, \"r\") as f:\n    for line in f:\n        print(line)\n        class_id, rcx, rcy, rw, rh = list(map(float, line.strip().split()))\n        x = (rcx-rw/2)*img_size\n        y = (rcy-rh/2)*img_size\n        w = rw*img_size\n        h = rh*img_size\n        plt.plot([x, x, x+w, x+w, x], [y, y+h, y+h, y, y])","metadata":{"_uuid":"8ebf845a4f3b51555991292d10ddc272b226bf7c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def write_train_list(metadata_dir, img_dir, name, series):\n    list_fp = os.path.join(metadata_dir, name)\n    with open(list_fp, \"w\") as f:\n        for index, patient in series.iterrows():\n            line = \"{}\\n\".format(os.path.join(img_dir, \"{}.jpg\".format(patient['id'].split('.')[0])))\n            f.write(line)","metadata":{"_uuid":"d62f98b287e73a66c234c4c9f9e41503fc0d4bca","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"series = pd.read_csv(os.path.join(IMAGE_DIR, 'size.csv'))\ntr_series = series.loc[series['split'] == 'train']\nval_series = series.loc[series['split'] == 'test']\n\n# train image path list\nwrite_train_list(metadata_dir, train_dir, \"tr_list.txt\", tr_series)\n# validation image path list\nwrite_train_list(metadata_dir, test_dir, \"val_list.txt\", val_series)","metadata":{"_uuid":"4c3982b42711fe4a20600b90ed3d123384da7096","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ex_patient_id = 'a29c5a68b07b'\nex_img_path = os.path.join(test_dir, \"{}.jpg\".format(ex_patient_id))\n\nplt.imshow(cv2.imread(ex_img_path))","metadata":{"_uuid":"f057a0fc9938308f69a5d402cf51ba340af592c0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_extention_file_path = os.path.join(cfg_dir, 'rsna.data')\nwith open(data_extention_file_path, 'w') as f:\n    contents = \"\"\"classes= 1\ntrain  = {}\nvalid  = {}\nnames  = {}\nbackup = {}\n    \"\"\".format(os.path.join(metadata_dir, \"tr_list.txt\"),\n               os.path.join(metadata_dir, \"val_list.txt\"),\n               os.path.join(cfg_dir, 'rsna.names'),\n               backup_dir)\n    f.write(contents)","metadata":{"_uuid":"f78dcf8c107c67e49f8bba8f483ab6a5b409ec0b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat cfg/rsna.data","metadata":{"_uuid":"684b3e9dde58f56a746952d2934661712e417161","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget -q https://pjreddie.com/media/files/darknet53.conv.74","metadata":{"_uuid":"ec2d5fbdfe6f3eebd553495d10f9adcdae29f16f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget --no-check-certificate -q \"https://docs.google.com/uc?export=download&id=18ptTK4Vbeokqpux8Onr0OmwUP9ipmcYO\" -O cfg/rsna_yolov3.cfg_train","metadata":{"_uuid":"f64517332ee89f69af1b013ef6376b19d639f0a6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../../input/covid19jpg/CovidJPG","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Training YOLOv3","metadata":{"_uuid":"f03592ee228f9ae16edac192071955d01233a147"}},{"cell_type":"markdown","source":"### 4.0. Command for training with Pre-trained CNN Weights (darknet53.conv.74)\n* I didn't run following command on kaggle kernel becuase of the long output.\n* If you crash with  'CUDA Error: out of memory', Solve it by Editing 'batch' and 'subdivisions' in 'cfg/rsna_yolov3.cfg_train'\n* If 'batch' and 'subdivisions' are 64 and 64 respectively, for every iteration only one image will be loaded on GPU memory. So it will use less GPU memory.","metadata":{"_uuid":"69f52f31dea66f3da1952b537c63dc5376d64492"}},{"cell_type":"code","source":"!./darknet detector train cfg/rsna.data cfg/rsna_yolov3.cfg_train darknet53.conv.74 -i 0 | tee train_log.txt","metadata":{"_uuid":"0be53a581ab12776c1c276f72acbf08b2db16d77","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat ../../input/covid19jpg/CovidJPG/train/8951eac18958.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2. My Plot of Training Loss\nIt's a loss graph up to about 2000 iteration. Since it tooks too long on kaggle kernel, I brought it. When learning, don't be surprised of big loss values at the beginning. Stay calm and It'll go down. Please See the following loss graph.","metadata":{"_uuid":"3cfd26f1c657bd30952f04b4ebbe6ca6ff7dfea8"}},{"cell_type":"code","source":"!wget --no-check-certificate -q \"https://docs.google.com/uc?export=download&id=1OhnlV3s7r6xsEme6DKkNYjcYjsl-C_Av\" -O train_log.txt","metadata":{"_uuid":"417c9157d98d5a552f519a3935670fd4bd437a4b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iters = []\nlosses = []\ntotal_losses = []\nwith open(\"train_log.txt\", 'r') as f:\n    for i,line in enumerate(f):\n        if \"images\" in line:\n            iters.append(int(line.strip().split()[0].split(\":\")[0]))\n            losses.append(float(line.strip().split()[2]))        \n            total_losses.append(float(line.strip().split()[1].split(',')[0]))\n\nplt.figure(figsize=(20, 5))\nplt.subplot(1,2,1)\nsns.lineplot(iters, total_losses, label=\"totla loss\")\nsns.lineplot(iters, losses, label=\"avg loss\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\n\nplt.subplot(1,2,2)\nsns.lineplot(iters, total_losses, label=\"totla loss\")\nsns.lineplot(iters, losses, label=\"avg loss\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.ylim([0, 4.05])","metadata":{"_uuid":"f761373c3c46f5c7875d78cc3023becc5924f6a4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. How to use trainined YOLOv3 for test images (command line)","metadata":{"_uuid":"ef7409909b9403f43b2c92760220b23a38d42029"}},{"cell_type":"markdown","source":"### 5.0. Copy sample test image","metadata":{"_uuid":"c97da5c5f52e0257a3b1a2568e6260a9560700a0"}},{"cell_type":"code","source":"ex_patient_id = annots[annots.Target == 1].patientId.values[2]\nshutil.copy(ex_img_path, \"test.jpg\")\nprint(ex_patient_id)","metadata":{"_uuid":"688344d91c7f7fb87418486b9b6b5f91a8b26e9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.1. Load trained model (at 15300 iteration)\nSince i uploaded the weights file (large big file) on my google drive, the command is very very long ...\n* It's a weight file at 15300 iteration, which I made submission file with. If you use this weight, you'll get a score of 0.141LB.\n  * Up to 15300 iteration, It takes about 8 hours.\n    * In .cfg file, I set 'batch' and 'subdivisions' as 64 and 8 respectively.\n    * Up to 1000 iteration from 0, it takes about 1h with **one** Tesla P100 GPU.      **(1000 iter/h)**\n    * Up to 15300 iteration from 1000, it takes about 7h with **four** Tesla P100 GPU. **(2043 iter/h)**","metadata":{"_uuid":"245c607d7753b947a91fe4d7ce8daf983afdfd0e"}},{"cell_type":"code","source":"!wget --load-cookies /tmp/cookies.txt -q \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1FDzMN-kGVYCvBeDKwemAazldSVkAEFyd' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1FDzMN-kGVYCvBeDKwemAazldSVkAEFyd\" -O backup/rsna_yolov3_15300.weights && rm -rf /tmp/cookies.txt","metadata":{"_uuid":"91d60e964d4cbd5397f6094fd811d115977abc3e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -alsth backup","metadata":{"_uuid":"d745c3d07102f9f1f27bf57a0c009311f9e0fd3b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2. cfg file for test (not for training)","metadata":{"_uuid":"41b4ca297869af6808b758824960e9dad12d00c0"}},{"cell_type":"code","source":"!wget --no-check-certificate -q \"https://docs.google.com/uc?export=download&id=10Yk6ZMAKGz5LeBbikciALy82aK3lX-57\" -O cfg/rsna_yolov3.cfg_test","metadata":{"_uuid":"17708ffdc9b834060ccb2b8cb4aefaa3c00b96b6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cd darknet && ./darknet detector test ../cfg/rsna.data ../cfg/rsna_yolov3.cfg_test ../backup/rsna_yolov3_15300.weights ../test.jpg -thresh 0.005","metadata":{"_uuid":"de9f7f2f30feeeaa3417ebd338c402e57a9b1f38","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ![](predictions.jpg)\nplt.imshow(cv2.imread(\"./darknet/predictions.jpg\"))","metadata":{"_uuid":"0dec1cc6f5bd1f8c854697128091ec7bd69d6e3e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Generate Submission Files with YOLOv3 Python Wrapper","metadata":{"_uuid":"064029258018a3aa0ee22c59d74e6f321b767bf8"}},{"cell_type":"markdown","source":"### 6.0. Download darknet python wrapper (darknet.py)\n* Basically, you can use darknet/python/darknet.py files. However it'll show error.\n* So, I edited the darknet.py. There are two main modifications.\n  * Change print statement to print function for python3\n  * Edit dynamic library('libdarknet.so') file path\n* I leaved '# ===' marks where i edited in darknet.py. For example,\n```\n# ==============================================================================\n#lib = CDLL(\"/home/pjreddie/documents/darknet/libdarknet.so\", RTLD_GLOBAL)\ndarknet_lib_path = os.path.join(os.getcwd(), \"darknet\", \"libdarknet.so\")\nlib = CDLL(darknet_lib_path, RTLD_GLOBAL)\n# ==============================================================================\n```","metadata":{"_uuid":"a4848803d5f41f1082de0b0a1e348a26f5d38d58"}},{"cell_type":"code","source":"!wget --no-check-certificate -q \"https://docs.google.com/uc?export=download&id=1-KTV7K9G1bl3SmnLnzmpkDyNt6tDmH7j\" -O darknet.py","metadata":{"_uuid":"6cef00c59783cf8ff052b3c8b79903a9dfa24fc5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.1. Load darknet python wrapper module","metadata":{"_uuid":"91eafd163f9e78557c7acb1be7176845cc15d4c1"}},{"cell_type":"code","source":"from darknet import *","metadata":{"_uuid":"238a70858c290db54f72700fe57215bd7934f6d8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2. Generate submission files\n* When making submission files, be aware of label format which is different in yolo.","metadata":{"_uuid":"1f6193268a5bd5b27d3ba855d6d36715a63d4ee0"}},{"cell_type":"code","source":"threshold = 0.2","metadata":{"_uuid":"877c1dc79f4a4cb0dc3c322a05282d3d902ed615","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_file_path = \"submission.csv\"\ncfg_path = os.path.join(cfg_dir, \"rsna_yolov3.cfg_test\")\nweight_path = os.path.join(backup_dir, \"rsna_yolov3_15300.weights\")\n\ntest_img_list_path = os.path.join(metadata_dir, \"te_list.txt\")","metadata":{"_uuid":"b776bab5a37d180d2e5185555b47cafb1a964a71","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpu_index = 0\nnet = load_net(cfg_path.encode(),\n               weight_path.encode(), \n               gpu_index)\nmeta = load_meta(data_extention_file_path.encode())","metadata":{"_uuid":"d82a944502827338091976c4912f5eb6af6e886e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_dict = {\"patientId\": [], \"PredictionString\": []}\n\nwith open(test_img_list_path, \"r\") as test_img_list_f:\n    # tqdm run up to 1000(The # of test set)\n    for line in tqdm(test_img_list_f):\n        patient_id = line.strip().split('/')[-1].strip().split('.')[0]\n\n        infer_result = detect(net, meta, line.strip().encode(), thresh=threshold)\n\n        submit_line = \"\"\n        for e in infer_result:\n            confi = e[1]\n            w = e[2][2]\n            h = e[2][3]\n            x = e[2][0]-w/2\n            y = e[2][1]-h/2\n            submit_line += \"{} {} {} {} {} \".format(confi, x, y, w, h)\n\n        submit_dict[\"patientId\"].append(patient_id)\n        submit_dict[\"PredictionString\"].append(submit_line)\n\npd.DataFrame(submit_dict).to_csv(submit_file_path, index=False)","metadata":{"_uuid":"c11fa319a577beff0a0d53f107489ee79771bc22","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls -lsht\n!rm -rf darknet images labels metadata backup cfg\n!rm -rf train_log.txt darknet53.conv.74 darknet.py darknet_gpu\n!rm -rf test.jpg\n!rm -rf __pycache__ .ipynb_checkpoints","metadata":{"_uuid":"7ffbb3e3205f76f5b8f216266539469c65d56cc1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -alsht","metadata":{"_uuid":"74cd76e2b853568a7f2962f0c220a64fe87e67c1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Future works & Etc\n\n### Future works (Things to try)\n* Image augmentation\n* More training\n* Utilizing the not labeled images because we got rid of not labeled images above\n\n### ETC\n* For a private matter, i can not proceed RSNA task after 09/27. If you have any ideas, questions and problems with this kernel after 09/27, Please leave those things anyway~! Collaborator '@John Byun' will reply to your comments.","metadata":{"_uuid":"2fbf050ee377907eb544c04382d07e1449ef577a","trusted":true}}]}