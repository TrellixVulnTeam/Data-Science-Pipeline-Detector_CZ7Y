{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"For study -  \n512 color image public eff7  \n384 gray image size -> 1.25 size up (37.9)   with softmax(주의해야 함, logit이 sigmoid에 최적화)\n480 gray image size -> random sizeup (1,1.1) (37.6 map)  (주의해야 함, logit이 sigmoid에 최적화)\n(two 380, 480 ensemble -> 38.5)  \n\nTwo class  \n512 color image size ->public eff7 (0.87 auc)  \n\nObject deteciton  \n640 gray image size -> yolov5 M -> (0.523)\n\nTwo class detection\n384 gray image size -> 1.25 size up   with softmax(주의해야 함, logit이 sigmoid에 최적화)  \n480 gray image size -> random sizeup (1,1.1)\nTwo class auc 89.2  \n\nObject detection에 bayesian 확률을 활용해서 더 현실적으로 만들어야 함  \n384, 480, 440, 540, 640 random(1,1.33)에서 cv 최적화(39.3)","metadata":{}},{"cell_type":"code","source":"!conda install '/kaggle/input/pydicom-conda-helper/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y\n!pip install ../input/timm-pytorch-image-models/pytorch-image-models-master\n\n!pip install '/kaggle/input/kerasapplications' --no-deps\n!pip install '/kaggle/input/efficientnet-keras-source-code' --no-deps\n#!pip install '/kaggle/input/effdet-latestvinbigdata-wbf-fused/ensemble_boxes-1.0.4-py3-none-any.whl' --no-deps","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-08T05:23:01.659025Z","iopub.execute_input":"2021-08-08T05:23:01.659404Z","iopub.status.idle":"2021-08-08T05:25:28.111Z","shell.execute_reply.started":"2021-08-08T05:23:01.659373Z","shell.execute_reply":"2021-08-08T05:25:28.109922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torch-1.7.0+cu110-cp37-cp37m-linux_x86_64.whl' --no-deps\n!pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torchvision-0.8.1+cu110-cp37-cp37m-linux_x86_64.whl' --no-deps\n!pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torchaudio-0.7.0-cp37-cp37m-linux_x86_64.whl' --no-deps\n\n## Compatible Cuda Toolkit installation\n!mkdir -p /kaggle/tmp && cp /kaggle/input/pytorch-170-cuda-toolkit-110221/cudatoolkit-11.0.221-h6bb024c_0 /kaggle/tmp/cudatoolkit-11.0.221-h6bb024c_0.tar.bz2 && conda install /kaggle/tmp/cudatoolkit-11.0.221-h6bb024c_0.tar.bz2 -y --offline\n\n## MMDetection Offline Installation\n!pip install '/kaggle/input/mmdetectionv2140/addict-2.4.0-py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/yapf-0.31.0-py2.py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/terminal-0.4.0-py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/terminaltables-3.1.0-py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/mmcv_full-1_3_8-cu110-torch1_7_0/mmcv_full-1.3.8-cp37-cp37m-manylinux1_x86_64.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/pycocotools-2.0.2/pycocotools-2.0.2' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/mmpycocotools-12.0.3/mmpycocotools-12.0.3' --no-deps\n\n!cp -r /kaggle/input/mmdetectionv2140/mmdetection-2.14.0 /kaggle/working/\n!mv /kaggle/working/mmdetection-2.14.0 /kaggle/working/mmdetection\n%cd /kaggle/working/mmdetection\n!pip install -e . --no-deps\n%cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:25:28.113014Z","iopub.execute_input":"2021-08-08T05:25:28.113394Z","iopub.status.idle":"2021-08-08T05:33:34.722762Z","shell.execute_reply.started":"2021-08-08T05:25:28.113352Z","shell.execute_reply":"2021-08-08T05:33:34.721898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_torch_384size = 384\nstudy_torch_480size = 480\nstudy_torch_440size = 440\nstudy_torch_540size = 540\nstudy_torch_640size = 640\nstudy_torch_size = 640\nstudy_tf_size = 512\nimage_size = 640\ntwoclass_size = 512\ntwoclass_480size = 480\ntwoclass_384size = 384\n\neffv_model_path_384 = \"../input/onecycle-effv2-384/\"\neffv_model_path_480 = \"../input/480imageeffv2cv376/\"\neffv_large_model_path_440 = \"../input/onecycle-effv2-large-440/\"\neffv_large_model_path_540 = \"../input/onecycle-effv2-large-540/\"\neffv_large_model_path_640 = \"../input/effvlarge640/\"\neffv_model_twoclass_384 = \"../input/efftwoclass384/\"\neffv_model_twoclass_480 = \"../input/efftwoclass480/\"","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:34.724929Z","iopub.execute_input":"2021-08-08T05:33:34.725283Z","iopub.status.idle":"2021-08-08T05:33:34.733359Z","shell.execute_reply.started":"2021-08-08T05:33:34.725242Z","shell.execute_reply":"2021-08-08T05:33:34.732472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm.auto import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-08T05:33:34.73506Z","iopub.execute_input":"2021-08-08T05:33:34.735378Z","iopub.status.idle":"2021-08-08T05:33:34.746126Z","shell.execute_reply.started":"2021-08-08T05:33:34.735345Z","shell.execute_reply":"2021-08-08T05:33:34.745361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\nif df.shape[0] == 2477:\n    fast_sub = True\n    fast_df = pd.DataFrame(([['00086460a852_study', 'negative 1 0 0 1 1'], \n                         ['000c9c05fd14_study', 'negative 1 0 0 1 1'], \n                         ['65761e66de9f_image', 'none 1 0 0 1 1'], \n                         ['51759b5579bc_image', 'none 1 0 0 1 1']]), \n                       columns=['id', 'PredictionString'])\nelse:\n    fast_sub = False","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:34.748121Z","iopub.execute_input":"2021-08-08T05:33:34.748355Z","iopub.status.idle":"2021-08-08T05:33:34.779855Z","shell.execute_reply.started":"2021-08-08T05:33:34.748333Z","shell.execute_reply":"2021-08-08T05:33:34.77911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# .dcm to .png","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:34.781418Z","iopub.execute_input":"2021-08-08T05:33:34.78165Z","iopub.status.idle":"2021-08-08T05:33:35.005333Z","shell.execute_reply.started":"2021-08-08T05:33:34.781628Z","shell.execute_reply":"2021-08-08T05:33:35.004513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:35.006566Z","iopub.execute_input":"2021-08-08T05:33:35.007345Z","iopub.status.idle":"2021-08-08T05:33:35.012879Z","shell.execute_reply.started":"2021-08-08T05:33:35.007296Z","shell.execute_reply":"2021-08-08T05:33:35.01199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = 'test'\nsave_dir = f'/kaggle/tmp/{split}/'\n\nos.makedirs(save_dir, exist_ok=True)\n\nsave_dir = f'/kaggle/tmp/{split}/study_torch_size/'\nos.makedirs(save_dir, exist_ok=True)\nif fast_sub:\n    xray = read_xray('/kaggle/input/siim-covid19-detection/train/00086460a852/9e8302230c91/65761e66de9f.dcm')\n    im = resize(xray, size=study_torch_size)  \n    study = '00086460a852' + '_study.png'\n    im.save(os.path.join(save_dir, study))\n    xray = read_xray('/kaggle/input/siim-covid19-detection/train/000c9c05fd14/e555410bd2cd/51759b5579bc.dcm')\n    #print(xray.shape)\n    im = resize(xray, size=study_torch_size)  \n    study = '000c9c05fd14' + '_study.png'\n    im.save(os.path.join(save_dir, study))\nelse:   \n    for dirname, _, filenames in tqdm(os.walk(f'/kaggle/input/siim-covid19-detection/{split}')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            xray = read_xray(os.path.join(dirname, file))\n            im = resize(xray, size=study_torch_size)  \n            study = dirname.split('/')[-2] + '_study.png'\n            im.save(os.path.join(save_dir, study))","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:35.015853Z","iopub.execute_input":"2021-08-08T05:33:35.016412Z","iopub.status.idle":"2021-08-08T05:33:36.638925Z","shell.execute_reply.started":"2021-08-08T05:33:35.016372Z","shell.execute_reply":"2021-08-08T05:33:36.62898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_id = []\ndim0 = []\ndim1 = []\nsplits = []\nsave_dir = f'/kaggle/tmp/{split}/image/'\nos.makedirs(save_dir, exist_ok=True)\nif fast_sub:\n    xray = read_xray('/kaggle/input/siim-covid19-detection/train/00086460a852/9e8302230c91/65761e66de9f.dcm')\n    im = resize(xray, size=image_size)  \n    im.save(os.path.join(save_dir,'65761e66de9f_image.png'))\n    image_id.append('65761e66de9f.dcm'.replace('.dcm', ''))\n    dim0.append(xray.shape[0])\n    dim1.append(xray.shape[1])\n    splits.append(split)\n    xray = read_xray('/kaggle/input/siim-covid19-detection/train/000c9c05fd14/e555410bd2cd/51759b5579bc.dcm')\n    im = resize(xray, size=image_size)  \n    im.save(os.path.join(save_dir, '51759b5579bc_image.png'))\n    image_id.append('51759b5579bc.dcm'.replace('.dcm', ''))\n    dim0.append(xray.shape[0])\n    dim1.append(xray.shape[1])\n    splits.append(split)\nelse:\n    for dirname, _, filenames in tqdm(os.walk(f'/kaggle/input/siim-covid19-detection/{split}')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            xray = read_xray(os.path.join(dirname, file))\n            im = resize(xray, size=image_size)  \n            im.save(os.path.join(save_dir, file.replace('.dcm', '_image.png')))\n            image_id.append(file.replace('.dcm', ''))\n            dim0.append(xray.shape[0])\n            dim1.append(xray.shape[1])\n            splits.append(split)\nmeta = pd.DataFrame.from_dict({'image_id': image_id, 'dim0': dim0, 'dim1': dim1, 'split': splits})","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:36.641162Z","iopub.execute_input":"2021-08-08T05:33:36.64166Z","iopub.status.idle":"2021-08-08T05:33:37.073153Z","shell.execute_reply.started":"2021-08-08T05:33:36.641621Z","shell.execute_reply":"2021-08-08T05:33:37.072239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# study predict","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nif fast_sub:\n    df = fast_df.copy()\nelse:\n    df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\nid_laststr_list  = []\nfor i in range(df.shape[0]):\n    id_laststr_list.append(df.loc[i,'id'][-1])\ndf['id_last_str'] = id_laststr_list\n\nstudy_len = df[df['id_last_str'] == 'y'].shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:37.074526Z","iopub.execute_input":"2021-08-08T05:33:37.074875Z","iopub.status.idle":"2021-08-08T05:33:37.095053Z","shell.execute_reply.started":"2021-08-08T05:33:37.074839Z","shell.execute_reply":"2021-08-08T05:33:37.093256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For Pytorch Markdown","metadata":{}},{"cell_type":"code","source":"if fast_sub:\n    sub_df = fast_df.copy()\nelse:\n    sub_df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\nsub_df = sub_df[:study_len]\ntest_paths = f'/kaggle/tmp/{split}/study_torch/' + sub_df['id'] +'.png'\n\nsub_df['negative'] = 0\nsub_df['typical'] = 0\nsub_df['indeterminate'] = 0\nsub_df['atypical'] = 0\n\n\nlabel_cols = sub_df.columns[2:]","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:37.096693Z","iopub.execute_input":"2021-08-08T05:33:37.097118Z","iopub.status.idle":"2021-08-08T05:33:37.119203Z","shell.execute_reply.started":"2021-08-08T05:33:37.097077Z","shell.execute_reply":"2021-08-08T05:33:37.118468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pytorch ImageLoader","metadata":{}},{"cell_type":"code","source":"# torch libs\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import *\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.nn.parallel.data_parallel import data_parallel\n\nfrom torch.nn.utils.rnn import *\n\n\ndef seed_torch(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    return seed\n\nimport os\nfrom datetime import datetime\n\n#-----------------------------------\n\n#numerical libs\nimport math\nimport numpy as np\nimport random\nimport PIL\nimport cv2\nimport matplotlib\n#matplotlib.use('TkAgg')\n#matplotlib.use('WXAgg')\n#matplotlib.use('Qt4Agg')\n#matplotlib.use('Qt5Agg')\n#print('matplotlib.get_backend : ', matplotlib.get_backend())\n#print(matplotlib.__version__)\n\n\n# std libs\nimport collections\nfrom collections import defaultdict\nimport copy\nimport numbers\nimport inspect\nimport shutil\nfrom timeit import default_timer as timer\nimport itertools\nfrom collections import OrderedDict\nfrom multiprocessing import Pool\nimport multiprocessing as mp\n\n#from pprintpp import pprint, pformat\nimport json\nimport zipfile\nfrom shutil import copyfile\n\nimport csv\nimport pandas as pd\nimport pickle\nimport glob\nimport sys\nfrom distutils.dir_util import copy_tree\nimport time\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:37.120406Z","iopub.execute_input":"2021-08-08T05:33:37.120773Z","iopub.status.idle":"2021-08-08T05:33:37.130498Z","shell.execute_reply.started":"2021-08-08T05:33:37.12074Z","shell.execute_reply":"2021-08-08T05:33:37.129509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def null_augment(r):\n    image = r['image']\n    # if image[:2].shape != (image_size, image_size):\n    #     r['image'] = cv2.resize(image, dsize=(image_size, image_size), interpolation=cv2.INTER_AREA)\n    return r\n\n\nclass SiimDataset(Dataset):\n    def __init__(self, df, image_size, augment=null_augment):\n        super().__init__()\n        self.df = df\n        self.augment = augment\n        self.length = len(df)\n        self.image_size = image_size\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        d = self.df.iloc[index]\n\n        #image_file = data_dir + '/%s_640/%s/%s/%s.png' % (d.set, d.study, d.series, d.image)\n        #image_file = data_dir + '/%s_full_512/%s.png' % (d.set, d.image)\n        image_file = f'/kaggle/tmp/{split}/study_torch_size/' + d['id'].split(\"_\")[0] + \"_study.png\"\n        image = cv2.imread(image_file,cv2.IMREAD_GRAYSCALE)\n        image = cv2.resize(image, (self.image_size, self.image_size))\n\n        r = {'image' : image, 'image_size' : self.image_size}\n        if self.augment is not None: r = self.augment(r)\n        return r\n    \ndef null_collate(batch):\n    collate = defaultdict(list)\n\n    for r in batch:\n        for k, v in r.items():\n            collate[k].append(v)\n\n    # ---\n    batch_size = len(batch)\n    image = np.stack(collate['image'])\n    image = image.reshape(batch_size, 1, collate['image_size'][0],collate['image_size'][0]).repeat(3,1)\n    image = np.ascontiguousarray(image)\n    image = image.astype(np.float32) / 255\n    collate['image'] = torch.from_numpy(image)\n\n    return collate\n\ntest_384dataset = SiimDataset(sub_df, study_torch_384size)\ntest_384loader  = DataLoader(\n                test_384dataset,\n                sampler = SequentialSampler(test_384dataset),\n                batch_size  = 32,#128, #\n                drop_last   = False,\n                num_workers = 0,\n                pin_memory  = True,\n                collate_fn  = null_collate,\n            )\n\ntest_480dataset = SiimDataset(sub_df, study_torch_480size)\ntest_480loader  = DataLoader(\n                test_480dataset,\n                sampler = SequentialSampler(test_480dataset),\n                batch_size  = 32,#128, #\n                drop_last   = False,\n                num_workers = 0,\n                pin_memory  = True,\n                collate_fn  = null_collate,\n            )\n\ntest_440dataset = SiimDataset(sub_df, study_torch_440size)\ntest_440loader  = DataLoader(\n                test_440dataset,\n                sampler = SequentialSampler(test_440dataset),\n                batch_size  = 32,#128, #\n                drop_last   = False,\n                num_workers = 0,\n                pin_memory  = True,\n                collate_fn  = null_collate,\n            )\n\ntest_540dataset = SiimDataset(sub_df, study_torch_540size)\ntest_540loader  = DataLoader(\n                test_540dataset,\n                sampler = SequentialSampler(test_540dataset),\n                batch_size  = 16,#128, #\n                drop_last   = False,\n                num_workers = 0,\n                pin_memory  = True,\n                collate_fn  = null_collate,\n            )\n","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:37.132172Z","iopub.execute_input":"2021-08-08T05:33:37.132561Z","iopub.status.idle":"2021-08-08T05:33:37.150685Z","shell.execute_reply.started":"2021-08-08T05:33:37.132527Z","shell.execute_reply":"2021-08-08T05:33:37.149869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Torch NetWork","metadata":{}},{"cell_type":"code","source":"sys.path.append(\"../input/timm-pytorch-image-models/pytorch-image-models-master/\")\nfrom timm.models.efficientnet import *\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        e = tf_efficientnetv2_m(pretrained=False)\n        self.b0 = nn.Sequential(\n            e.conv_stem,\n            e.bn1,\n            e.act1,\n        )\n        self.b1 = e.blocks[0]\n        self.b2 = e.blocks[1]\n        self.b3 = e.blocks[2]\n        self.b4 = e.blocks[3]\n        self.b5 = e.blocks[4]\n        self.b6 = e.blocks[5]\n        self.b7 = e.blocks[6]\n        self.b8 = nn.Sequential(\n            e.conv_head, #384, 1536\n            e.bn2,\n            e.act2,\n        )\n\n        self.logit = nn.Linear(1280,4)\n        self.mask = nn.Sequential(\n            nn.Conv2d(176, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 1, kernel_size=1, padding=0),\n        )\n\n\n    # @torch.cuda.amp.autocast()\n    def forward(self, image):\n        batch_size = len(image)\n        x = 2*image-1     # ; print('input ',   x.shape)\n\n        x = self.b0(x) #; print (x.shape)  # torch.Size([2, 40, 256, 256])\n        x = self.b1(x) #; print (x.shape)  # torch.Size([2, 24, 256, 256])\n        x = self.b2(x) #; print (x.shape)  # torch.Size([2, 32, 128, 128])\n        x = self.b3(x) #; print (x.shape)  # torch.Size([2, 48, 64, 64])\n        x = self.b4(x) #; print (x.shape)  # torch.Size([2, 96, 32, 32])\n        x = self.b5(x) #; print (x.shape)  # torch.Size([2, 136, 32, 32])\n        #------------\n        mask = self.mask(x)\n        #-------------\n        x = self.b6(x) #; print (x.shape)  # torch.Size([2, 232, 16, 16])\n        x = self.b7(x) #; print (x.shape)  # torch.Size([2, 384, 16, 16])\n        x = self.b8(x) #; print (x.shape)  # torch.Size([2, 1536, 16, 16])\n        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n        #x = F.dropout(x, 0.5, training=self.training)\n        logit = self.logit(x)\n        return logit, mask\n    \nclass NetL(nn.Module):\n    def __init__(self):\n        super(NetL, self).__init__()\n\n        e = tf_efficientnetv2_l(pretrained=False)\n        self.b0 = nn.Sequential(\n            e.conv_stem,\n            e.bn1,\n            e.act1,\n        )\n        self.b1 = e.blocks[0]\n        self.b2 = e.blocks[1]\n        self.b3 = e.blocks[2]\n        self.b4 = e.blocks[3]\n        self.b5 = e.blocks[4]\n        self.b6 = e.blocks[5]\n        self.b7 = e.blocks[6]\n        self.b8 = nn.Sequential(\n            e.conv_head, #384, 1536\n            e.bn2,\n            e.act2,\n        )\n\n        self.logit = nn.Linear(1280,4)\n        self.mask = nn.Sequential(\n            nn.Conv2d(224, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 1, kernel_size=1, padding=0),\n        )\n\n\n    # @torch.cuda.amp.autocast()\n    def forward(self, image):\n        batch_size = len(image)\n        x = 2*image-1     # ; print('input ',   x.shape)\n\n        x = self.b0(x) #; print (x.shape)  # torch.Size([2, 40, 256, 256])\n        x = self.b1(x) #; print (x.shape)  # torch.Size([2, 24, 256, 256])\n        x = self.b2(x) #; print (x.shape)  # torch.Size([2, 32, 128, 128])\n        x = self.b3(x) #; print (x.shape)  # torch.Size([2, 48, 64, 64])\n        x = self.b4(x) #; print (x.shape)  # torch.Size([2, 96, 32, 32])\n        x = self.b5(x) #; print (x.shape)  # torch.Size([2, 136, 32, 32])\n        #------------\n        mask = self.mask(x)\n        #-------------\n        x = self.b6(x) #; print (x.shape)  # torch.Size([2, 232, 16, 16])\n        x = self.b7(x) #; print (x.shape)  # torch.Size([2, 384, 16, 16])\n        x = self.b8(x) #; print (x.shape)  # torch.Size([2, 1536, 16, 16])\n        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n        #x = F.dropout(x, 0.5, training=self.training)\n        logit = self.logit(x)\n        return logit, mask","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:37.151787Z","iopub.execute_input":"2021-08-08T05:33:37.152266Z","iopub.status.idle":"2021-08-08T05:33:38.222227Z","shell.execute_reply.started":"2021-08-08T05:33:37.15223Z","shell.execute_reply":"2021-08-08T05:33:38.221242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Torch Get Pretrained Net","metadata":{}},{"cell_type":"code","source":"study_torch_384size = 384\nstudy_torch_480size = 480\nstudy_torch_440size = 440\nstudy_torch_540size = 540\nstudy_torch_640size = 640\nstudy_torch_size = 640\nstudy_tf_size = 512\nimage_size = 640\ntwoclass_size = 512\ntwoclass_480size = 480\ntwoclass_384size = 384\n\neffv_model_path_384 = \"../input/onecycle-effv2-384/\"\neffv_model_path_480 = \"../input/480imageeffv2cv376/\"\neffv_large_model_path_440 = \"../input/onecycle-effv2-large-440/\"\neffv_large_model_path_540 = \"../input/onecycle-effv2-large-540/\"\neffv_large_model_path_640 = \"../input/effvlarge640/\"\neffv_model_twoclass_384 = \"../input/efftwoclass384/\"\neffv_model_twoclass_480 = \"../input/efftwoclass480/\"\ntorch_model_384list = os.listdir(effv_model_path_384)\ntorch_model_480list = os.listdir(effv_model_path_480)\ntorch_model_large_440list = os.listdir(effv_large_model_path_440)\ntorch_model_large_540list = os.listdir(effv_large_model_path_540)\ntorch_model_large_640list = os.listdir(effv_large_model_path_640)\ndef get_pretrained_model(path, model_name='m'):\n    checkpoint = path\n    if model_name == 'm':\n        net = Net()\n    elif model_name == 'l':\n        net = NetL()\n    if torch.cuda.is_available():\n        net.load_state_dict(torch.load(path)['state_dict'], strict=True)\n        net.cuda()\n    else:\n        net.load_state_dict(torch.load(path, map_location=torch.device('cpu'))['state_dict'], strict=True)\n    return net\n\npretrained_384path = [effv_model_path_384 + x for x in torch_model_384list]\npretrained_480path = [effv_model_path_480 + x for x in torch_model_480list]\npretrained_large_440path = [effv_large_model_path_440 + x for x in torch_model_large_440list]\npretrained_large_540path = [effv_large_model_path_540 + x for x in torch_model_large_540list]\npretrained_440path = [effv_large_model_path_440 + x for x in torch_model_large_440list]\npretrained_540path = [effv_large_model_path_540 + x for x in torch_model_large_540list]\npretrained_large_640path = [effv_large_model_path_640 + x for x in torch_model_large_640list]\npretrained_640path = [effv_large_model_path_640 + x for x in torch_model_large_640list]","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:38.223718Z","iopub.execute_input":"2021-08-08T05:33:38.224135Z","iopub.status.idle":"2021-08-08T05:33:38.266213Z","shell.execute_reply.started":"2021-08-08T05:33:38.224094Z","shell.execute_reply":"2021-08-08T05:33:38.26547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Torch Study Prediction","metadata":{}},{"cell_type":"code","source":"def do_predict_384(net, valid_loader, tta=['flip','scale']): #flip\n    \n    valid_probability = []\n    \n\n    start_timer = timer()\n    for t, batch in enumerate(valid_loader):\n        \n        if torch.cuda.is_available():\n            image  = batch['image'].cuda()\n        else:\n            image  = batch['image']\n\n        #<todo> TTA\n        net.eval()\n        with torch.no_grad():\n            probability = []\n            logit, mask = net(image)\n            probability.append(torch.sigmoid(logit))\n\n            if 'flip' in tta:\n                logit, mask = net(torch.flip(image,dims=(3,)))\n                probability.append(torch.sigmoid(logit))\n\n            if 'scale' in tta:\n                # size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None):\n                logit, mask = net(F.interpolate(image, scale_factor=1.25, mode='bilinear', align_corners=False))\n                probability.append(torch.sigmoid(logit))\n\n            #--------------\n            probability = torch.stack(probability,0).mean(0)\n\n        \n        valid_probability.append(probability.data.cpu().numpy())\n\n    probability = np.concatenate(valid_probability)\n    return probability\n\ndef do_predict_480(net, valid_loader, tta=['flip','scale']): #flip\n    \n    valid_probability = []\n    \n\n    start_timer = timer()\n    for t, batch in enumerate(valid_loader):\n        \n        if torch.cuda.is_available():\n            image  = batch['image'].cuda()\n        else:\n            image  = batch['image']\n\n        #<todo> TTA\n        net.eval()\n        with torch.no_grad():\n            probability = []\n            logit, mask = net(image)\n            probability.append(torch.sigmoid(logit))\n\n            if 'flip' in tta:\n                logit, mask = net(torch.flip(image,dims=(3,)))\n                probability.append(torch.sigmoid(logit))\n\n            if 'scale' in tta:\n                # size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None):\n                size = random.uniform(1,1.1)\n                logit, mask = net(F.interpolate(image, scale_factor=size, mode='bilinear', align_corners=False))\n                probability.append(torch.sigmoid(logit))\n\n            #--------------\n            probability = torch.stack(probability,0).mean(0)\n\n        \n        valid_probability.append(probability.data.cpu().numpy())\n\n    probability = np.concatenate(valid_probability)\n    return probability\n\ndef do_predict_440(net, valid_loader, tta=['flip','scale']): #flip\n    \n    valid_probability = []\n    \n\n    start_timer = timer()\n    for t, batch in enumerate(valid_loader):\n        \n        if torch.cuda.is_available():\n            image  = batch['image'].cuda()\n        else:\n            image  = batch['image']\n\n        #<todo> TTA\n        net.eval()\n        with torch.no_grad():\n            probability = []\n            logit, mask = net(image)\n            probability.append(torch.sigmoid(logit))\n\n            if 'flip' in tta:\n                logit, mask = net(torch.flip(image,dims=(3,)))\n                probability.append(torch.sigmoid(logit))\n\n            if 'scale' in tta:\n                # size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None):\n                size = random.uniform(1,1.15)\n                logit, mask = net(F.interpolate(image, scale_factor=size, mode='bilinear', align_corners=False))\n                probability.append(torch.sigmoid(logit))\n\n            #--------------\n            probability = torch.stack(probability,0).mean(0)\n\n        \n        valid_probability.append(probability.data.cpu().numpy())\n\n    probability = np.concatenate(valid_probability)\n    return probability\n\n\ndef do_predict_540(net, valid_loader, tta=['flip','scale']): #flip\n    \n    valid_probability = []\n    \n\n    start_timer = timer()\n    for t, batch in enumerate(valid_loader):\n        \n        if torch.cuda.is_available():\n            image  = batch['image'].cuda()\n        else:\n            image  = batch['image']\n\n        #<todo> TTA\n        net.eval()\n        with torch.no_grad():\n            probability = []\n            logit, mask = net(image)\n            probability.append(torch.sigmoid(logit))\n\n            if 'flip' in tta:\n                logit, mask = net(torch.flip(image,dims=(3,)))\n                probability.append(torch.sigmoid(logit))\n\n            if 'scale' in tta:\n                # size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None):\n                size = random.uniform(0.95,1.05)\n                logit, mask = net(F.interpolate(image, scale_factor=size, mode='bilinear', align_corners=False))\n                probability.append(torch.sigmoid(logit))\n\n            #--------------\n            probability = torch.stack(probability,0).mean(0)\n\n        \n        valid_probability.append(probability.data.cpu().numpy())\n\n    probability = np.concatenate(valid_probability)\n    return probability\n\ndef do_predict_640(net, valid_loader, tta=['flip','scale']): #flip\n    \n    valid_probability = []\n    \n\n    start_timer = timer()\n    for t, batch in enumerate(valid_loader):\n        \n        if torch.cuda.is_available():\n            image  = batch['image'].cuda()\n        else:\n            image  = batch['image']\n\n        #<todo> TTA\n        net.eval()\n        with torch.no_grad():\n            probability = []\n            logit, mask = net(image)\n            probability.append(torch.sigmoid(logit))\n\n            if 'flip' in tta:\n                logit, mask = net(torch.flip(image,dims=(3,)))\n                probability.append(torch.sigmoid(logit))\n\n            if 'scale' in tta:\n                # size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None):\n                size = random.uniform(0.85,1.05)\n                logit, mask = net(F.interpolate(image, scale_factor=size, mode='bilinear', align_corners=False))\n                probability.append(torch.sigmoid(logit))\n\n            #--------------\n            probability = torch.stack(probability,0).mean(0)\n\n        \n        valid_probability.append(probability.data.cpu().numpy())\n\n    probability = np.concatenate(valid_probability)\n    return probability","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:38.26752Z","iopub.execute_input":"2021-08-08T05:33:38.267898Z","iopub.status.idle":"2021-08-08T05:33:38.298031Z","shell.execute_reply.started":"2021-08-08T05:33:38.26787Z","shell.execute_reply":"2021-08-08T05:33:38.296851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nmodel0 = pretrained_384path[0]\npredict0 = do_predict_384(get_pretrained_model(model0, 'm'), test_384loader, tta=['flip','scale'])\ndel model0\ngc.collect()\nmodel1 = pretrained_384path[1]\npredict1 = do_predict_384(get_pretrained_model(model1, 'm'), test_384loader, tta=['flip','scale'])\ndel model1\ngc.collect()\nmodel2 = pretrained_384path[2]\npredict2 = do_predict_384(get_pretrained_model(model2, 'm'), test_384loader, tta=['flip','scale'])\ndel model2\ngc.collect()\nmodel3 = pretrained_384path[3]\npredict3 = do_predict_384(get_pretrained_model(model3, 'm'), test_384loader, tta=['flip','scale'])\ndel model3\ngc.collect()\nmodel4 = pretrained_384path[4]\npredict4 = do_predict_384(get_pretrained_model(model4, 'm'), test_384loader, tta=['flip','scale'])\ngc.collect()\ndel model4","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:33:38.299873Z","iopub.execute_input":"2021-08-08T05:33:38.300728Z","iopub.status.idle":"2021-08-08T05:34:03.918786Z","shell.execute_reply.started":"2021-08-08T05:33:38.300688Z","shell.execute_reply":"2021-08-08T05:34:03.917844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_384value = ((predict0) + (predict1) + (predict2) + (predict3) + (predict4))*(0.2)\n#sub_df[label_cols] = \n\n#del model0, model1, model2, model3, model4\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:34:03.920216Z","iopub.execute_input":"2021-08-08T05:34:03.920617Z","iopub.status.idle":"2021-08-08T05:34:04.021111Z","shell.execute_reply.started":"2021-08-08T05:34:03.92058Z","shell.execute_reply":"2021-08-08T05:34:04.020057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import gc\n#model0 = pretrained_480path[0]\n#predict0 = do_predict_480(get_pretrained_model(model0, 'm'), test_480loader, tta=['flip','scale'])\n#del model0\n#gc.collect()\n#model1 = pretrained_480path[1]\n#predict1 = do_predict_480(get_pretrained_model(model1, 'm'), test_480loader, tta=['flip','scale'])\n#del model1\n#gc.collect()\n#model2 = pretrained_480path[2]\n#predict2 = do_predict_480(get_pretrained_model(model2, 'm'), test_480loader, tta=['flip','scale'])\n#del model2\n#gc.collect()\n#model3 = pretrained_480path[3]\n#predict3 = do_predict_480(get_pretrained_model(model3, 'm'), test_480loader, tta=['flip','scale'])\n#del model3\n#gc.collect()\n#model4 = pretrained_480path[4]\n#predict4 = do_predict_480(get_pretrained_model(model4, 'm'), test_480loader, tta=['flip','scale'])\n#gc.collect()\n#del model4","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:22:44.005636Z","iopub.status.idle":"2021-08-08T05:22:44.00623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch_480value = ((predict0) * (predict1)*(predict2) * (predict3) * (predict4))**(0.2)\n#sub_df[label_cols] = \n\n#del model0, model1, model2, model3, model4\n#import gc\n#gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:22:44.007742Z","iopub.status.idle":"2021-08-08T05:22:44.008335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nmodel0 = pretrained_440path[0]\npredict0 = do_predict_440(get_pretrained_model(model0, 'l'), test_440loader, tta=['flip','scale'])\ndel model0\ngc.collect()\nmodel1 = pretrained_440path[1]\npredict1 = do_predict_440(get_pretrained_model(model1, 'l'), test_440loader, tta=['flip','scale'])\ndel model1\ngc.collect()\nmodel2 = pretrained_440path[2]\npredict2 = do_predict_440(get_pretrained_model(model2, 'l'), test_440loader, tta=['flip','scale'])\ndel model2\ngc.collect()\nmodel3 = pretrained_440path[3]\npredict3 = do_predict_440(get_pretrained_model(model3, 'l'), test_440loader, tta=['flip','scale'])\ndel model3\ngc.collect()\nmodel4 = pretrained_440path[4]\npredict4 = do_predict_440(get_pretrained_model(model4, 'l'), test_440loader, tta=['flip','scale'])\ngc.collect()\ndel model4","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:34:04.022474Z","iopub.execute_input":"2021-08-08T05:34:04.022904Z","iopub.status.idle":"2021-08-08T05:34:46.918839Z","shell.execute_reply.started":"2021-08-08T05:34:04.022864Z","shell.execute_reply":"2021-08-08T05:34:46.918003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch_440value = np.stack([predict0**0.25, predict1**0.25, predict2**0.25, predict3**0.25, predict4**0.25]).mean(axis=0)\ntorch_440value = ((predict0) + (predict1) + (predict2) + (predict3) + (predict4))*(0.2)\n#sub_df[label_cols] = \n\n#del model0, model1, model2, model3, model4\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:34:46.920201Z","iopub.execute_input":"2021-08-08T05:34:46.920557Z","iopub.status.idle":"2021-08-08T05:34:47.015559Z","shell.execute_reply.started":"2021-08-08T05:34:46.920521Z","shell.execute_reply":"2021-08-08T05:34:47.014421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nmodel0 = pretrained_540path[0]\npredict0 = do_predict_540(get_pretrained_model(model0, 'l'), test_540loader, tta=['flip','scale'])\ndel model0\ngc.collect()\nmodel1 = pretrained_540path[1]\npredict1 = do_predict_540(get_pretrained_model(model1, 'l'), test_540loader, tta=['flip','scale'])\ndel model1\ngc.collect()\nmodel2 = pretrained_540path[2]\npredict2 = do_predict_540(get_pretrained_model(model2, 'l'), test_540loader, tta=['flip','scale'])\ndel model2\ngc.collect()\nmodel3 = pretrained_540path[3]\npredict3 = do_predict_540(get_pretrained_model(model3, 'l'), test_540loader, tta=['flip','scale'])\ndel model3\ngc.collect()\nmodel4 = pretrained_540path[4]\npredict4 = do_predict_540(get_pretrained_model(model4, 'l'), test_540loader, tta=['flip','scale'])\ngc.collect()\ndel model4","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:34:47.0172Z","iopub.execute_input":"2021-08-08T05:34:47.017787Z","iopub.status.idle":"2021-08-08T05:35:24.570072Z","shell.execute_reply.started":"2021-08-08T05:34:47.017747Z","shell.execute_reply":"2021-08-08T05:35:24.569219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_540value = ((predict0) + (predict1) + (predict2) + (predict3) + (predict4))*(0.2)\n#sub_df[label_cols] = \n\n#del model0, model1, model2, model3, model4\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:35:24.571416Z","iopub.execute_input":"2021-08-08T05:35:24.571757Z","iopub.status.idle":"2021-08-08T05:35:24.673956Z","shell.execute_reply.started":"2021-08-08T05:35:24.571724Z","shell.execute_reply":"2021-08-08T05:35:24.672914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as tfhub\n\nMODEL_ARCH = 'efficientnetv2-l-21k-ft1k'\n# Get the TensorFlow Hub model URL\nhub_type = 'feature_vector' # ['classification', 'feature_vector']\nMODEL_ARCH_PATH = f'/kaggle/input/efficientnetv2-tfhub-weight-files/tfhub_models/{MODEL_ARCH}/{hub_type}'\n\n# Custom wrapper class to load the right pretrained weights explicitly from the local directory\nclass KerasLayerWrapper(tfhub.KerasLayer):\n    def __init__(self, handle, **kwargs):\n        handle = tfhub.KerasLayer(tfhub.load(MODEL_ARCH_PATH))\n        super().__init__(handle, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:35:24.677522Z","iopub.execute_input":"2021-08-08T05:35:24.67796Z","iopub.status.idle":"2021-08-08T05:35:29.154157Z","shell.execute_reply.started":"2021-08-08T05:35:24.677922Z","shell.execute_reply":"2021-08-08T05:35:29.153162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install /kaggle/input/efficientnet-keras-source-code/ -q --no-deps\n\nimport os\n\nimport efficientnet.tfkeras as efn\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport random\n\nMODEL_PATH = '/kaggle/input/pesudo-512512'\n#test_paths = study_df.image_path.tolist()\nBATCH_SIZE = 32\n\ndef build_decoder(with_labels=True, target_size=(300, 300), ext='jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3)\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3)\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.image.resize(img, target_size)\n\n        return img\n\n    def decode_with_labels(path, label):\n        return decode(path), label\n\n    return decode_with_labels if with_labels else decode\n\ndef build_augmenter(with_labels=True):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        #img = tf.image.random_flip_up_down(img)\n        img = tf.image.random_brightness(img, 0.1)\n        return img\n\n    def augment_with_labels(img, label):\n        return augment(img), label\n\n    return augment_with_labels if with_labels else augment\n\ndef build_dataset(paths, labels=None, bsize=32, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=True, repeat=True, shuffle=1024, \n                  cache_dir=\"\"):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n\n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n\n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n\n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, labels)\n\n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n    dset = dset.cache(cache_dir) if cache else dset\n    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n    dset = dset.repeat() if repeat else dset\n    dset = dset.shuffle(shuffle) if shuffle else dset\n    dset = dset.batch(bsize).prefetch(AUTO)\n\n    return dset\n\n# strategy = auto_select_accelerator()\n# BATCH_SIZE = strategy.num_replicas_in_sync * 16\n\n#label_cols = ['negative', 'typical', 'indeterminate', 'atypical']\n#study_df[label_cols] = 0\ntest_paths = f'/kaggle/tmp/{split}/study_torch_size/' + sub_df['id'] +'.png'\ntest_decoder = build_decoder(with_labels=False,\n                             target_size=(512,\n                                          512), ext='png')\n\n\nwith tf.device('/device:GPU:0'):\n    models = []\n    models0 = tf.keras.models.load_model(f'{MODEL_PATH}/model0.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models1 = tf.keras.models.load_model(f'{MODEL_PATH}/model1.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models2 = tf.keras.models.load_model(f'{MODEL_PATH}/model2.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models3 = tf.keras.models.load_model(f'{MODEL_PATH}/model3.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models4 = tf.keras.models.load_model(f'{MODEL_PATH}/model4.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models.append(models0)\n    models.append(models1)\n    models.append(models2)\n    models.append(models3)\n    models.append(models4)\n\n#study_df[label_cols] = sum([model.predict(test_dataset, verbose=1) for model in models]) / len(models)\n#study_df['PredictionString'] = study_df[label_cols].apply(lambda row: f'negative {row.negative} 0 0 1 1 typical {row.typical} 0 0 1 1 indeterminate {row.indeterminate} 0 0 1 1 atypical {row.atypical} 0 0 1 1', axis=1)\n\ntest_dataset = build_dataset(\n    test_paths, bsize=BATCH_SIZE, repeat=False, \n    shuffle=False, augment=False, cache=False,\n    decode_fn=test_decoder\n)\ntensor_value_list1= [model.predict(test_dataset, verbose=1) for model in models]\ntest_dataset = build_dataset(\n    test_paths, bsize=BATCH_SIZE, repeat=False, \n    shuffle=False, augment=True, cache=False,\n    decode_fn=test_decoder\n)\ntensor_value_list2= [model.predict(test_dataset, verbose=1) for model in models]\n\ntensor_value_512 = (tensor_value_list1[0]+tensor_value_list1[1]+tensor_value_list1[2]+tensor_value_list1[3]+tensor_value_list1[4]+tensor_value_list2[0]+tensor_value_list2[1]+tensor_value_list2[2]+tensor_value_list2[3]+tensor_value_list2[4])*(0.1)\ndel models0, models1, models2, models3, models4\ndel models\ndel test_dataset, test_decoder\ndel tensor_value_list1\ndel tensor_value_list2\nimport gc\ngc.collect()\n\n\n##################################\n########### 640 SIZE #############\n##################################\nMODEL_PATH = '/kaggle/input/simm-640x640'\ntest_paths = f'/kaggle/tmp/{split}/study_torch_size/' + sub_df['id'] +'.png'\ntest_decoder = build_decoder(with_labels=False,\n                             target_size=(640,\n                                          640), ext='png')\n\n\nwith tf.device('/device:GPU:0'):\n    models = []\n    models0 = tf.keras.models.load_model(f'{MODEL_PATH}/model0.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models1 = tf.keras.models.load_model(f'{MODEL_PATH}/model1.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models2 = tf.keras.models.load_model(f'{MODEL_PATH}/model2.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models3 = tf.keras.models.load_model(f'{MODEL_PATH}/model3.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models4 = tf.keras.models.load_model(f'{MODEL_PATH}/model4.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models.append(models0)\n    models.append(models1)\n    models.append(models2)\n    models.append(models3)\n    models.append(models4)\n\n#study_df[label_cols] = sum([model.predict(test_dataset, verbose=1) for model in models]) / len(models)\n#study_df['PredictionString'] = study_df[label_cols].apply(lambda row: f'negative {row.negative} 0 0 1 1 typical {row.typical} 0 0 1 1 indeterminate {row.indeterminate} 0 0 1 1 atypical {row.atypical} 0 0 1 1', axis=1)\ntest_dataset = build_dataset(\n    test_paths, bsize=BATCH_SIZE, repeat=False, \n    shuffle=False, augment=False, cache=False,\n    decode_fn=test_decoder\n)\ntensor_value_list1= [model.predict(test_dataset, verbose=1) for model in models]\ntest_dataset = build_dataset(\n    test_paths, bsize=BATCH_SIZE, repeat=False, \n    shuffle=False, augment=True, cache=False,\n    decode_fn=test_decoder\n)\ntensor_value_list2= [model.predict(test_dataset, verbose=1) for model in models]\n\ntensor_value_640 = (tensor_value_list1[0]+tensor_value_list1[1]+tensor_value_list1[2]+tensor_value_list1[3]+tensor_value_list1[4]+tensor_value_list2[0]+tensor_value_list2[1]+tensor_value_list2[2]+tensor_value_list2[3]+tensor_value_list2[4])*(0.1)\ndel models0, models1, models2, models3, models4\ndel models\ndel test_dataset, test_decoder\ndel tensor_value_list1\ndel tensor_value_list2\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:37:42.644296Z","iopub.execute_input":"2021-08-08T05:37:42.644655Z","iopub.status.idle":"2021-08-08T05:45:17.66918Z","shell.execute_reply.started":"2021-08-08T05:37:42.644625Z","shell.execute_reply":"2021-08-08T05:45:17.668368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as tfhub\n\nMODEL_ARCH = 'efficientnetv2-m-21k-ft1k'\n# Get the TensorFlow Hub model URL\nhub_type = 'feature_vector' # ['classification', 'feature_vector']\nMODEL_ARCH_PATH = f'/kaggle/input/efficientnetv2-tfhub-weight-files/tfhub_models/{MODEL_ARCH}/{hub_type}'\n\n# Custom wrapper class to load the right pretrained weights explicitly from the local directory\nclass KerasLayerWrapper(tfhub.KerasLayer):\n    def __init__(self, handle, **kwargs):\n        handle = tfhub.KerasLayer(tfhub.load(MODEL_ARCH_PATH))\n        super().__init__(handle, **kwargs)\n\n##################################\n########### 480 SIZE #############\n##################################        \n\nMODEL_PATH = '/kaggle/input/covid-480x480-2st'\ntest_paths = f'/kaggle/tmp/{split}/study_torch_size/' + sub_df['id'] +'.png'\ntest_decoder = build_decoder(with_labels=False,\n                             target_size=(480,\n                                          480), ext='png')\n\n\nwith tf.device('/device:GPU:0'):\n    models = []\n    models0 = tf.keras.models.load_model(f'{MODEL_PATH}/model0.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models1 = tf.keras.models.load_model(f'{MODEL_PATH}/model1.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models2 = tf.keras.models.load_model(f'{MODEL_PATH}/model2.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models3 = tf.keras.models.load_model(f'{MODEL_PATH}/model3.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models4 = tf.keras.models.load_model(f'{MODEL_PATH}/model4.h5',\n                                         custom_objects={'KerasLayer': KerasLayerWrapper})\n    models.append(models0)\n    models.append(models1)\n    models.append(models2)\n    models.append(models3)\n    models.append(models4)\n\n#study_df[label_cols] = sum([model.predict(test_dataset, verbose=1) for model in models]) / len(models)\n#study_df['PredictionString'] = study_df[label_cols].apply(lambda row: f'negative {row.negative} 0 0 1 1 typical {row.typical} 0 0 1 1 indeterminate {row.indeterminate} 0 0 1 1 atypical {row.atypical} 0 0 1 1', axis=1)\ntest_dataset = build_dataset(\n    test_paths, bsize=BATCH_SIZE, repeat=False, \n    shuffle=False, augment=False, cache=False,\n    decode_fn=test_decoder\n)\ntensor_value_list1= [model.predict(test_dataset, verbose=1) for model in models]\ntest_dataset = build_dataset(\n    test_paths, bsize=BATCH_SIZE, repeat=False, \n    shuffle=False, augment=True, cache=False,\n    decode_fn=test_decoder\n)\ntensor_value_list2= [model.predict(test_dataset, verbose=1) for model in models]\n\n\ntensor_value_480 = (tensor_value_list1[0]+tensor_value_list1[1]+tensor_value_list1[2]+tensor_value_list1[3]+tensor_value_list1[4]+tensor_value_list2[0]+tensor_value_list2[1]+tensor_value_list2[2]+tensor_value_list2[3]+tensor_value_list2[4])*(0.1)\ndel models0, models1, models2, models3, models4\ndel models\ndel test_dataset, test_decoder\ndel tensor_value_list1\ndel tensor_value_list2\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:45:17.673027Z","iopub.execute_input":"2021-08-08T05:45:17.673342Z","iopub.status.idle":"2021-08-08T05:47:58.463163Z","shell.execute_reply.started":"2021-08-08T05:45:17.673312Z","shell.execute_reply":"2021-08-08T05:47:58.462377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install /kaggle/input/kerasapplications -q\n#!pip install /kaggle/input/efficientnet-keras-source-code/ -q --no-deps\n\nimport os\n\nimport efficientnet.tfkeras as efn\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport random\n\ndef auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n\n    return strategy\n\n\ndef build_decoder(with_labels=True, target_size=(300, 300), ext='jpg'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path)\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3)\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3)\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.image.resize(img, target_size)\n\n        return img\n\n    def decode_with_labels(path, label):\n        return decode(path), label\n\n    return decode_with_labels if with_labels else decode\n\n\ndef build_augmenter(with_labels=True):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_flip_up_down(img)\n        \n        random_number = random.randint(0,2)\n        if random_number == 1:\n            img = tf.image.adjust_brightness(img, 1.2)\n        if random_number == 2:\n            img = tf.image.adjust_brightness(img, 0.8)\n        return img\n\n    def augment_with_labels(img, label):\n        return augment(img), label\n\n    return augment_with_labels if with_labels else augment\n\n\ndef build_dataset(paths, labels=None, bsize=32, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=True, repeat=True, shuffle=1024, \n                  cache_dir=\"\"):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n\n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n\n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n\n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, labels)\n\n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n    dset = dset.cache(cache_dir) if cache else dset\n    dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n    dset = dset.repeat() if repeat else dset\n    dset = dset.shuffle(shuffle) if shuffle else dset\n    dset = dset.batch(bsize).prefetch(AUTO)\n\n    return dset\n\n#COMPETITION_NAME = \"siim-cov19-test-img512-study-600\"\nstrategy = auto_select_accelerator()\nBATCH_SIZE = strategy.num_replicas_in_sync * 8\n\nIMSIZE = (224, 240, 260, 300, 380, 456, 528, 600, 512)\n\n#load_dir = f\"/kaggle/input/{COMPETITION_NAME}/\"\n#if fast_sub:\n#    sub_df = fast_df.copy()\n#else:\n#    sub_df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\n#sub_df = sub_df[:study_len]\n#test_paths = f'/kaggle/tmp/{split}/study_tf/' + sub_df['id'] +'.png'\n\n#sub_df['negative'] = 0\n#sub_df['typical'] = 0\n#sub_df['indeterminate'] = 0\n#sub_df['atypical'] = 0\n\n\n#label_cols = sub_df.columns[2:]\n\n#test_decoder = build_decoder(with_labels=False, target_size=(IMSIZE[7], IMSIZE[7]), ext='png')\n#dtest = build_dataset(\n#    test_paths, bsize=BATCH_SIZE, repeat=False, \n#    shuffle=False, augment=False, cache=False,\n#    decode_fn=test_decoder\n#)\n\n#with strategy.scope():\n    \n#    models = []\n#    \n#    models0 = tf.keras.models.load_model(\n#        '../input/siim-covid19-efnb7-train-study/model0.h5'\n#    )\n#    models1 = tf.keras.models.load_model(\n#        '../input/siim-covid19-efnb7-train-study/model1.h5'\n#    )\n#    models2 = tf.keras.models.load_model(\n#        '../input/siim-covid19-efnb7-train-study/model2.h5'\n#    )\n#    models3 = tf.keras.models.load_model(\n#        '../input/siim-covid19-efnb7-train-study/model3.h5'\n#    )\n#    models4 = tf.keras.models.load_model(\n#        '../input/siim-covid19-efnb7-train-study/model4.h5'\n#    )\n    \n#    models.append(models0)\n#    models.append(models1)\n#    models.append(models2)\n#    models.append(models3)\n#    models.append(models4)\n    \n\n#tensor_value_list = [model.predict(dtest, verbose=1) for model in models]\n#tensor_value = (tensor_value_list[0]*tensor_value_list[1]*tensor_value_list[2]*tensor_value_list[3]*tensor_value_list[4])**(0.2)\n#del models0, models1, models2, models3, models4\n#del models\n#import gc\n#gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:47:58.465253Z","iopub.execute_input":"2021-08-08T05:47:58.465596Z","iopub.status.idle":"2021-08-08T05:47:58.485674Z","shell.execute_reply.started":"2021-08-08T05:47:58.46556Z","shell.execute_reply":"2021-08-08T05:47:58.484875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df[label_cols] = (torch_384value + torch_440value + tensor_value_480 + tensor_value_640 + torch_540value + tensor_value_512) * (1/6)","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:47:58.487512Z","iopub.execute_input":"2021-08-08T05:47:58.488085Z","iopub.status.idle":"2021-08-08T05:47:58.50963Z","shell.execute_reply.started":"2021-08-08T05:47:58.488049Z","shell.execute_reply":"2021-08-08T05:47:58.50881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.columns = ['id', 'PredictionString1', 'negative', 'typical', 'indeterminate', 'atypical']\ndf = pd.merge(df, sub_df, on = 'id', how = 'left')","metadata":{"execution":{"iopub.status.busy":"2021-08-08T05:47:58.511077Z","iopub.execute_input":"2021-08-08T05:47:58.511426Z","iopub.status.idle":"2021-08-08T05:47:58.536352Z","shell.execute_reply.started":"2021-08-08T05:47:58.511394Z","shell.execute_reply":"2021-08-08T05:47:58.535384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# study string","metadata":{}},{"cell_type":"code","source":"for i in range(study_len):\n    negative = df.loc[i,'negative']\n    typical = df.loc[i,'typical']\n    indeterminate = df.loc[i,'indeterminate']\n    atypical = df.loc[i,'atypical']\n    df.loc[i, 'PredictionString'] = f'negative {negative} 0 0 1 1 typical {typical} 0 0 1 1 indeterminate {indeterminate} 0 0 1 1 atypical {atypical} 0 0 1 1'","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:08.000557Z","iopub.execute_input":"2021-08-07T06:42:08.001067Z","iopub.status.idle":"2021-08-07T06:42:08.008259Z","shell.execute_reply.started":"2021-08-07T06:42:08.001026Z","shell.execute_reply":"2021-08-07T06:42:08.007162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_study = df[['id', 'PredictionString']]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:08.010051Z","iopub.execute_input":"2021-08-07T06:42:08.010597Z","iopub.status.idle":"2021-08-07T06:42:08.023811Z","shell.execute_reply.started":"2021-08-07T06:42:08.010558Z","shell.execute_reply":"2021-08-07T06:42:08.02284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 Class Prediction","metadata":{}},{"cell_type":"markdown","source":"# DataLoader Define","metadata":{}},{"cell_type":"code","source":"if fast_sub:\n    sub_df = fast_df.copy()\nelse:\n    sub_df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\nsub_df = sub_df[study_len:]\ntest_paths = f'/kaggle/tmp/{split}/image/' + sub_df['id'] +'.png'\nsub_df['none'] = 0\nlabel_cols = sub_df.columns[2]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:08.025637Z","iopub.execute_input":"2021-08-07T06:42:08.02621Z","iopub.status.idle":"2021-08-07T06:42:08.037171Z","shell.execute_reply.started":"2021-08-07T06:42:08.026156Z","shell.execute_reply":"2021-08-07T06:42:08.036168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def null_augment(r):\n    image = r['image']\n    # if image[:2].shape != (image_size, image_size):\n    #     r['image'] = cv2.resize(image, dsize=(image_size, image_size), interpolation=cv2.INTER_AREA)\n    return r\n\n\nclass TwoClassSiimDataset(Dataset):\n    def __init__(self, df, image_size, augment=null_augment):\n        super().__init__()\n        self.df = df\n        self.augment = augment\n        self.length = len(df)\n        self.image_size = image_size\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        d = self.df.iloc[index]\n\n        #image_file = data_dir + '/%s_640/%s/%s/%s.png' % (d.set, d.study, d.series, d.image)\n        #image_file = data_dir + '/%s_full_512/%s.png' % (d.set, d.image)\n        image_file = f'/kaggle/tmp/{split}/image/' + d['id'].split(\"_\")[0] + \"_image.png\"\n        #print(image_file)\n        image = cv2.imread(image_file,cv2.IMREAD_GRAYSCALE)\n        image = cv2.resize(image, (self.image_size, self.image_size))\n\n        r = {'image' : image, 'image_size' : self.image_size}\n        if self.augment is not None: r = self.augment(r)\n        return r\n    \ndef twoclass_null_collate(batch):\n    collate = defaultdict(list)\n\n    for r in batch:\n        for k, v in r.items():\n            collate[k].append(v)\n\n    # ---\n    batch_size = len(batch)\n    image = np.stack(collate['image'])\n    image = image.reshape(batch_size, 1, collate['image_size'][0],collate['image_size'][0]).repeat(3,1)\n    image = np.ascontiguousarray(image)\n    image = image.astype(np.float32) / 255\n    collate['image'] = torch.from_numpy(image)\n\n    return collate\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:08.039287Z","iopub.execute_input":"2021-08-07T06:42:08.03966Z","iopub.status.idle":"2021-08-07T06:42:08.052817Z","shell.execute_reply.started":"2021-08-07T06:42:08.039623Z","shell.execute_reply":"2021-08-07T06:42:08.051834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twoclass_384dataset = TwoClassSiimDataset(sub_df, 384)\ntwoclass_384loader  = DataLoader(\n                twoclass_384dataset,\n                sampler = SequentialSampler(twoclass_384dataset),\n                batch_size  = 16,#128, #\n                drop_last   = False,\n                num_workers = 0,\n                pin_memory  = True,\n                collate_fn  = twoclass_null_collate,\n            )\n\ntwoclass_480dataset = TwoClassSiimDataset(sub_df, 480)\ntwoclass_480loader  = DataLoader(\n                twoclass_480dataset,\n                sampler = SequentialSampler(twoclass_480dataset),\n                batch_size  = 16,#128, #\n                drop_last   = False,\n                num_workers = 0,\n                pin_memory  = True,\n                collate_fn  = twoclass_null_collate,\n            )","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:08.054372Z","iopub.execute_input":"2021-08-07T06:42:08.054794Z","iopub.status.idle":"2021-08-07T06:42:08.067168Z","shell.execute_reply.started":"2021-08-07T06:42:08.054753Z","shell.execute_reply":"2021-08-07T06:42:08.066097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get Two Class Pretrained model","metadata":{}},{"cell_type":"code","source":"class TwoClassNet(nn.Module):\n    def __init__(self):\n        super(TwoClassNet, self).__init__()\n\n        e = tf_efficientnetv2_m(pretrained=False)\n        self.b0 = nn.Sequential(\n            e.conv_stem,\n            e.bn1,\n            e.act1,\n        )\n        self.b1 = e.blocks[0]\n        self.b2 = e.blocks[1]\n        self.b3 = e.blocks[2]\n        self.b4 = e.blocks[3]\n        self.b5 = e.blocks[4]\n        self.b6 = e.blocks[5]\n        self.b7 = e.blocks[6]\n        self.b8 = nn.Sequential(\n            e.conv_head, #384, 1536\n            e.bn2,\n            e.act2,\n        )\n\n        self.logit = nn.Linear(1280,2)\n        self.mask = nn.Sequential(\n            nn.Conv2d(176, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 1, kernel_size=1, padding=0),\n        )\n\n\n    # @torch.cuda.amp.autocast()\n    def forward(self, image):\n        batch_size = len(image)\n        x = 2*image-1     # ; print('input ',   x.shape)\n\n        x = self.b0(x) #; print (x.shape)  # torch.Size([2, 40, 256, 256])\n        x = self.b1(x) #; print (x.shape)  # torch.Size([2, 24, 256, 256])\n        x = self.b2(x) #; print (x.shape)  # torch.Size([2, 32, 128, 128])\n        x = self.b3(x) #; print (x.shape)  # torch.Size([2, 48, 64, 64])\n        x = self.b4(x) #; print (x.shape)  # torch.Size([2, 96, 32, 32])\n        x = self.b5(x) #; print (x.shape)  # torch.Size([2, 136, 32, 32])\n        #------------\n        mask = self.mask(x)\n        #-------------\n        x = self.b6(x) #; print (x.shape)  # torch.Size([2, 232, 16, 16])\n        x = self.b7(x) #; print (x.shape)  # torch.Size([2, 384, 16, 16])\n        x = self.b8(x) #; print (x.shape)  # torch.Size([2, 1536, 16, 16])\n        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n        #x = F.dropout(x, 0.5, training=self.training)\n        logit = self.logit(x)\n        return logit, mask","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:08.068636Z","iopub.execute_input":"2021-08-07T06:42:08.069082Z","iopub.status.idle":"2021-08-07T06:42:08.08475Z","shell.execute_reply.started":"2021-08-07T06:42:08.068981Z","shell.execute_reply":"2021-08-07T06:42:08.083519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch_model_list = os.listdir(\"../input/effv2-cv375/\")\neffv_model_twoclass_384 = \"../input/efftwoclass384/\"\neffv_model_twoclass_480 = \"../input/efftwoclass480/\"\ntorch_twoclass_384 = os.listdir(effv_model_twoclass_384)\ntorch_twoclass_480 = os.listdir(effv_model_twoclass_480)\ndef twoclass_get_pretrained_model(path):\n    checkpoint = path\n    net = TwoClassNet()\n    if torch.cuda.is_available():\n        net.load_state_dict(torch.load(path)['state_dict'], strict=True)\n        net.cuda()\n    else:\n        net.load_state_dict(torch.load(path, map_location=torch.device('cpu'))['state_dict'], strict=True)\n    return net\n\ntwoclass_384 = [effv_model_twoclass_384 + x for x in torch_twoclass_384]\ntwoclass_480 = [effv_model_twoclass_480 + x for x in torch_twoclass_480]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:08.086332Z","iopub.execute_input":"2021-08-07T06:42:08.086764Z","iopub.status.idle":"2021-08-07T06:42:08.112499Z","shell.execute_reply.started":"2021-08-07T06:42:08.086723Z","shell.execute_reply":"2021-08-07T06:42:08.111596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Two Class Prediction define","metadata":{}},{"cell_type":"code","source":"def twoclass_predict_384(net, valid_loader, tta=['flip','scale']): #flip\n    \n    valid_probability = []\n    \n\n    start_timer = timer()\n    for t, batch in enumerate(valid_loader):\n        \n        if torch.cuda.is_available():\n            image  = batch['image'].cuda()\n        else:\n            image  = batch['image']\n\n        #<todo> TTA\n        net.eval()\n        with torch.no_grad():\n            probability = []\n            logit, mask = net(image)\n            probability.append(torch.sigmoid(logit))\n\n            if 'flip' in tta:\n                logit, mask = net(torch.flip(image,dims=(3,)))\n                probability.append(torch.sigmoid(logit))\n\n            if 'scale' in tta:\n                # size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None):\n                logit, mask = net(F.interpolate(image, scale_factor=1.25, mode='bilinear', align_corners=False))\n                probability.append(torch.sigmoid(logit))\n\n            #--------------\n            probability = torch.stack(probability,0).mean(0)\n\n        \n        valid_probability.append(probability.data.cpu().numpy())\n\n    probability = np.concatenate(valid_probability)\n    return probability\n\ndef twoclass_predict_480(net, valid_loader, tta=['flip','scale']): #flip\n    \n    valid_probability = []\n    \n\n    start_timer = timer()\n    for t, batch in enumerate(valid_loader):\n        \n        if torch.cuda.is_available():\n            image  = batch['image'].cuda()\n        else:\n            image  = batch['image']\n\n        #<todo> TTA\n        net.eval()\n        with torch.no_grad():\n            probability = []\n            logit, mask = net(image)\n            probability.append(torch.sigmoid(logit))\n\n            if 'flip' in tta:\n                logit, mask = net(torch.flip(image,dims=(3,)))\n                probability.append(torch.sigmoid(logit))\n\n            if 'scale' in tta:\n                # size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None):\n                size = random.uniform(1,1.1)\n                logit, mask = net(F.interpolate(image, scale_factor=size, mode='bilinear', align_corners=False))\n                probability.append(torch.sigmoid(logit))\n\n            #--------------\n            probability = torch.stack(probability,0).mean(0)\n\n        \n        valid_probability.append(probability.data.cpu().numpy())\n\n    probability = np.concatenate(valid_probability)\n    return probability","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:08.115097Z","iopub.execute_input":"2021-08-07T06:42:08.115639Z","iopub.status.idle":"2021-08-07T06:42:08.130526Z","shell.execute_reply.started":"2021-08-07T06:42:08.115598Z","shell.execute_reply":"2021-08-07T06:42:08.129468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Two class Prediction","metadata":{}},{"cell_type":"code","source":"import gc\nmodel0 = twoclass_384[0]\npredict0 = twoclass_predict_384(twoclass_get_pretrained_model(model0), twoclass_384loader, tta=['flip','scale'])\ndel model0\ngc.collect()\nimport gc\nmodel1 = twoclass_384[1]\npredict1 = twoclass_predict_384(twoclass_get_pretrained_model(model1), twoclass_384loader, tta=['flip','scale'])\ndel model1\ngc.collect()\nimport gc\nmodel2 = twoclass_384[2]\npredict2 = twoclass_predict_384(twoclass_get_pretrained_model(model2), twoclass_384loader, tta=['flip','scale'])\ndel model2\ngc.collect()\nimport gc\nmodel3 = twoclass_384[3]\npredict3 = twoclass_predict_384(twoclass_get_pretrained_model(model3), twoclass_384loader, tta=['flip','scale'])\ndel model3\ngc.collect()\nimport gc\nmodel4 = twoclass_384[4]\npredict4 = twoclass_predict_384(twoclass_get_pretrained_model(model4), twoclass_384loader, tta=['flip','scale'])\ndel model4\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:08.132148Z","iopub.execute_input":"2021-08-07T06:42:08.132703Z","iopub.status.idle":"2021-08-07T06:42:29.291144Z","shell.execute_reply.started":"2021-08-07T06:42:08.132664Z","shell.execute_reply":"2021-08-07T06:42:29.290205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twoclass_384value = np.stack([predict0, predict1, predict2, predict3, predict4]).mean(axis=0)\n#sub_df[label_cols] = \n\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:29.292603Z","iopub.execute_input":"2021-08-07T06:42:29.292992Z","iopub.status.idle":"2021-08-07T06:42:29.489459Z","shell.execute_reply.started":"2021-08-07T06:42:29.29295Z","shell.execute_reply":"2021-08-07T06:42:29.488152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nmodel0 = twoclass_480[0]\npredict0 = twoclass_predict_480(twoclass_get_pretrained_model(model0), twoclass_480loader, tta=['flip','scale'])\ndel model0\ngc.collect()\nimport gc\nmodel1 = twoclass_480[1]\npredict1 = twoclass_predict_480(twoclass_get_pretrained_model(model1), twoclass_480loader, tta=['flip','scale'])\ndel model1\ngc.collect()\nimport gc\nmodel2 = twoclass_480[2]\npredict2 = twoclass_predict_480(twoclass_get_pretrained_model(model2), twoclass_480loader, tta=['flip','scale'])\ndel model2\ngc.collect()\nimport gc\nmodel3 = twoclass_480[3]\npredict3 = twoclass_predict_480(twoclass_get_pretrained_model(model3), twoclass_480loader, tta=['flip','scale'])\ndel model3\ngc.collect()\nimport gc\nmodel4 = twoclass_480[4]\npredict4 = twoclass_predict_480(twoclass_get_pretrained_model(model4), twoclass_480loader, tta=['flip','scale'])\ndel model4\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:29.491106Z","iopub.execute_input":"2021-08-07T06:42:29.491562Z","iopub.status.idle":"2021-08-07T06:42:50.286784Z","shell.execute_reply.started":"2021-08-07T06:42:29.491497Z","shell.execute_reply":"2021-08-07T06:42:50.286027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twoclass_480value = np.stack([predict0, predict1, predict2, predict3, predict4]).mean(axis=0)\n#sub_df[label_cols] = \n\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:50.288138Z","iopub.execute_input":"2021-08-07T06:42:50.288462Z","iopub.status.idle":"2021-08-07T06:42:50.462715Z","shell.execute_reply.started":"2021-08-07T06:42:50.288427Z","shell.execute_reply":"2021-08-07T06:42:50.461809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TensorFLow TwoClass","metadata":{}},{"cell_type":"code","source":"test_decoder = build_decoder(with_labels=False, target_size=(512, 512), ext='png')\ndtest = build_dataset(\n    test_paths, bsize=BATCH_SIZE, repeat=False, \n    shuffle=False, augment=False, cache=False,\n    decode_fn=test_decoder\n)\n\nwith strategy.scope():\n    \n    models = []\n    \n    models0 = tf.keras.models.load_model(\n        '/kaggle/input/siim-covid19-efnb7-train-fold0-5-2class/model0.h5'\n    )\n    models1 = tf.keras.models.load_model(\n        '/kaggle/input/siim-covid19-efnb7-train-fold0-5-2class/model1.h5'\n    )\n    models2 = tf.keras.models.load_model(\n        '/kaggle/input/siim-covid19-efnb7-train-fold0-5-2class/model2.h5'\n    )\n    models3 = tf.keras.models.load_model(\n        '/kaggle/input/siim-covid19-efnb7-train-fold0-5-2class/model3.h5'\n    )\n    models4 = tf.keras.models.load_model(\n        '/kaggle/input/siim-covid19-efnb7-train-fold0-5-2class/model4.h5'\n    )\n    \n    models.append(models0)\n    models.append(models1)\n    models.append(models2)\n    models.append(models3)\n    models.append(models4)\n    \ntensor_value = sum([model.predict(dtest, verbose=1) for model in models]) / len(models)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:42:50.464231Z","iopub.execute_input":"2021-08-07T06:42:50.464841Z","iopub.status.idle":"2021-08-07T06:44:44.932158Z","shell.execute_reply.started":"2021-08-07T06:42:50.464799Z","shell.execute_reply":"2021-08-07T06:44:44.931109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df[label_cols] = ((twoclass_384value + twoclass_480value)[:,0] + tensor_value)/3","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:44.936687Z","iopub.execute_input":"2021-08-07T06:44:44.937166Z","iopub.status.idle":"2021-08-07T06:44:44.951279Z","shell.execute_reply.started":"2021-08-07T06:44:44.937122Z","shell.execute_reply":"2021-08-07T06:44:44.95011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv(\"/kaggle/working/df2class.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:44.955954Z","iopub.execute_input":"2021-08-07T06:44:44.958344Z","iopub.status.idle":"2021-08-07T06:44:44.979742Z","shell.execute_reply.started":"2021-08-07T06:44:44.958298Z","shell.execute_reply":"2021-08-07T06:44:44.978861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del models0, models1, models2, models3, models4\ndel models\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:44.983893Z","iopub.execute_input":"2021-08-07T06:44:44.985957Z","iopub.status.idle":"2021-08-07T06:44:46.735628Z","shell.execute_reply.started":"2021-08-07T06:44:44.98592Z","shell.execute_reply":"2021-08-07T06:44:46.734688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sub_df[label_cols] = sum([model.predict(dtest, verbose=1) for model in models]) / len(models)\ndf_2class = sub_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:46.74147Z","iopub.execute_input":"2021-08-07T06:44:46.741786Z","iopub.status.idle":"2021-08-07T06:44:46.762904Z","shell.execute_reply.started":"2021-08-07T06:44:46.741755Z","shell.execute_reply":"2021-08-07T06:44:46.761856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nfrom numba import cuda\nimport torch","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:46.76446Z","iopub.execute_input":"2021-08-07T06:44:46.765031Z","iopub.status.idle":"2021-08-07T06:44:47.534416Z","shell.execute_reply.started":"2021-08-07T06:44:46.764974Z","shell.execute_reply":"2021-08-07T06:44:47.533592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# yolov5 predict","metadata":{}},{"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\nimport torch","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.535737Z","iopub.execute_input":"2021-08-07T06:44:47.536111Z","iopub.status.idle":"2021-08-07T06:44:47.781859Z","shell.execute_reply.started":"2021-08-07T06:44:47.536067Z","shell.execute_reply":"2021-08-07T06:44:47.781086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta = meta[meta['split'] == 'test']\nif fast_sub:\n    test_df = fast_df.copy()\nelse:\n    test_df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\ntest_df = df[study_len:].reset_index(drop=True) \nmeta['image_id'] = meta['image_id'] + '_image'\nmeta.columns = ['id', 'dim0', 'dim1', 'split']\ntest_df = pd.merge(test_df, meta, on = 'id', how = 'left')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.783138Z","iopub.execute_input":"2021-08-07T06:44:47.783487Z","iopub.status.idle":"2021-08-07T06:44:47.806045Z","shell.execute_reply.started":"2021-08-07T06:44:47.783451Z","shell.execute_reply":"2021-08-07T06:44:47.805212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Yolo Prediction","metadata":{}},{"cell_type":"markdown","source":"# Load Pytorch Image Loader","metadata":{}},{"cell_type":"code","source":"# dim0 -> height\n# dim1 -> width\nmeta['image'] = meta['id'].map(lambda x: x.split(\"_\")[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.809416Z","iopub.execute_input":"2021-08-07T06:44:47.809694Z","iopub.status.idle":"2021-08-07T06:44:47.817267Z","shell.execute_reply.started":"2021-08-07T06:44:47.809668Z","shell.execute_reply":"2021-08-07T06:44:47.816261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Fold","metadata":{}},{"cell_type":"code","source":"study_name_to_predict_string = {\n    'Negative for Pneumonia'  :'negative',\n    'Typical Appearance'      :'typical',\n    'Indeterminate Appearance':'indeterminate',\n    'Atypical Appearance'     :'atypical',\n}\n\nstudy_name_to_label = {\n    'Negative for Pneumonia'  :0,\n    'Typical Appearance'      :1,\n    'Indeterminate Appearance':2,\n    'Atypical Appearance'     :3,\n}\nstudy_label_to_name = { v:k for k,v in study_name_to_label.items()}\nnum_study_label = len(study_name_to_label)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.818759Z","iopub.execute_input":"2021-08-07T06:44:47.819276Z","iopub.status.idle":"2021-08-07T06:44:47.827907Z","shell.execute_reply.started":"2021-08-07T06:44:47.819237Z","shell.execute_reply":"2021-08-07T06:44:47.82708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_meta  = meta.copy()\n\ndf_test = df_meta[df_meta['split']=='test'].copy()\n                   \ndf_annotate = pd.DataFrame()\ndf_annotate.loc[:,'image_id'] = df_test.image.values\ndf_annotate = df_annotate.merge(df_meta[['image','dim0','dim1']],left_on='image_id',right_on='image')\ndf_annotate.loc[:,'class_id'] = 0\ndf_annotate.loc[:, 'ncx'] = 0\ndf_annotate.loc[:, 'ncy'] = 0\ndf_annotate.loc[:, 'nw'] = 0\ndf_annotate.loc[:, 'nh'] = 0\n\n\nfor l in study_name_to_label.keys():\n    df_test.loc[:,l]=0\ndf_test = df_test.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.829511Z","iopub.execute_input":"2021-08-07T06:44:47.830136Z","iopub.status.idle":"2021-08-07T06:44:47.852909Z","shell.execute_reply.started":"2021-08-07T06:44:47.830094Z","shell.execute_reply":"2021-08-07T06:44:47.852054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Siim Dataset","metadata":{}},{"cell_type":"code","source":"##########################################\n# Imput으로 df_annotate과 df_test가 들어감 #\n##########################################\n\nclass YoloSiimDataset(Dataset):\n    def __init__(self, df_annotate, df, image_size=640, augment=null_augment):\n        super().__init__()\n        self.gb = df_annotate.groupby('image_id') #9eb725cdb713\n        self.df = df\n        self.augment = augment\n        self.length = len(df)\n        self.image_size = image_size\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        d = self.df.iloc[index]\n        df = self.gb.get_group(d.image).copy()\n\n        g = self.gb.get_group(d.image)\n        annotate = g[['class_id', 'ncx', 'ncy', 'nw', 'nh', ]].values\n        annotate = annotate[annotate[:, 0] == 1]\n\n        #image_file = data_dir + '/%s_640/%s/%s/%s.png' % (d.set, d.study, d.series, d.image)\n        image_file = f'/kaggle/tmp/{split}/image/' + d['id'].split(\"_\")[0] + \"_image.png\"\n        image  = cv2.imread(image_file,cv2.IMREAD_GRAYSCALE)\n        image  = cv2.resize(image, (self.image_size, self.image_size))\n\n        r = {\n            'd'  : d,\n            'df' : df,\n            'annotate' : annotate,\n            'image'  : image,\n            'image_size' : self.image_size\n        }\n        if self.augment is not None: r = self.augment(r)\n        return r","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.85557Z","iopub.execute_input":"2021-08-07T06:44:47.85604Z","iopub.status.idle":"2021-08-07T06:44:47.865886Z","shell.execute_reply.started":"2021-08-07T06:44:47.855994Z","shell.execute_reply":"2021-08-07T06:44:47.86491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def yolo_null_collate(batch):\n    collate = defaultdict(list)\n    for b,r in enumerate(batch):\n        for k, v in r.items():\n            if k=='annotate':\n                collate[k].append(np.hstack(\n                    [np.full((len(v),1),b),v]\n                ))\n            else:\n                collate[k].append(v)\n\n    # ---\n    batch_size = len(batch)\n\n    image = np.stack(collate['image'])\n    image = image.reshape(batch_size, 1, collate['image_size'][0],collate['image_size'][0]).repeat(3,1)\n    image = np.ascontiguousarray(image)\n    image = image.astype(np.float32) / 255\n    collate['image'] = torch.from_numpy(image)\n\n    annotate = np.concatenate(collate['annotate'])\n    collate['annotate'] = torch.from_numpy(annotate).float()\n    return collate","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.867427Z","iopub.execute_input":"2021-08-07T06:44:47.867823Z","iopub.status.idle":"2021-08-07T06:44:47.878892Z","shell.execute_reply.started":"2021-08-07T06:44:47.867788Z","shell.execute_reply":"2021-08-07T06:44:47.878183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_test = df_test.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.879799Z","iopub.execute_input":"2021-08-07T06:44:47.881755Z","iopub.status.idle":"2021-08-07T06:44:47.890583Z","shell.execute_reply.started":"2021-08-07T06:44:47.881729Z","shell.execute_reply":"2021-08-07T06:44:47.889806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yolo_test_dataset_640 = YoloSiimDataset(df_annotate, df_test, image_size = 640, augment=null_augment)\nyolo_test_loader_640  = DataLoader(\n                yolo_test_dataset_640,\n                sampler = SequentialSampler(yolo_test_dataset_640),\n                batch_size  = 8,#128, #\n                drop_last   = False,\n                num_workers = 0,\n                pin_memory  = True,\n                collate_fn  = yolo_null_collate,\n            )","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.891686Z","iopub.execute_input":"2021-08-07T06:44:47.892127Z","iopub.status.idle":"2021-08-07T06:44:47.901183Z","shell.execute_reply.started":"2021-08-07T06:44:47.892091Z","shell.execute_reply":"2021-08-07T06:44:47.900311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load YoloTorch Network","metadata":{}},{"cell_type":"code","source":"import sys","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.902695Z","iopub.execute_input":"2021-08-07T06:44:47.90318Z","iopub.status.idle":"2021-08-07T06:44:47.910674Z","shell.execute_reply.started":"2021-08-07T06:44:47.903146Z","shell.execute_reply":"2021-08-07T06:44:47.909699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.append(\"/kaggle/input/woongyolov5\")\nsys.path.append(\"kaggle/input/woong-yolov5\")","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.913845Z","iopub.execute_input":"2021-08-07T06:44:47.914225Z","iopub.status.idle":"2021-08-07T06:44:47.921199Z","shell.execute_reply.started":"2021-08-07T06:44:47.914183Z","shell.execute_reply":"2021-08-07T06:44:47.920097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from models.yolo import *\nfrom utils.torch_utils import *\nfrom utils.general import bbox_iou, xywh2xyxy","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:47.922747Z","iopub.execute_input":"2021-08-07T06:44:47.92317Z","iopub.status.idle":"2021-08-07T06:44:48.061388Z","shell.execute_reply.started":"2021-08-07T06:44:47.923129Z","shell.execute_reply":"2021-08-07T06:44:48.060431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = \"/kaggle/input/woong-yolov5/yolov5m.pt\"\npre_model_cfg_file = \"/kaggle/input/woong-yolov5/yolov5m.yaml\"\n\n#############################################################\n################### Configuration ###########################\n#############################################################\nimport numpy as np\n\n\nnum_class = 1\n\n# yolo net\nnum_head = 3\nimage_size = 640\nfeature_size = [\n    80, 40, 20\n]\nfeature_stride = [\n    8, 16, 32\n]\n\nanchor_size = [\n    [[ 10,13], [ 16, 30], [ 33, 23]],  # P3/8\n    [[ 30,61], [ 62, 45], [ 59,119]],  # P4/16\n    [[116,90], [156,198], [373,326]],  # P5/32\n]\nnum_anchor = 3 #per head\n\ndef make_norm_anchor_size():\n    norm_anchor_size = np.array(anchor_size)/np.array(feature_stride).reshape(3,1,1)\n    norm_anchor_size = norm_anchor_size.tolist()\n    return norm_anchor_size\n\n\n#----------------------------------------------\n\n# yolo loss\nanchor_match_ratio_threshold = 4\nloss_level_balance = [4.0, 1.0, 0.4]\nloss_obj_balance = 100\nloss_cls_balance = 1\nloss_box_balance = 0.05\n\n#nms\nnms_objectness_threshold = 0.001\nnms_iou_threshold = 0.5\nnms_pre_max_num   = 3000\nnms_post_max_num  = 100\n\nclass YoloNet(nn.Module):\n    def __init__(self):\n        super(YoloNet, self).__init__()\n\n        model_cfg_file = pre_model_cfg_file\n        pretrain_file  = pretrained_model\n        e = Model(\n            cfg=model_cfg_file,\n            ch=3,\n            nc=1,\n            anchors=np.array(anchor_size).reshape(num_head,-1).tolist(),\n        )\n        state_dict = torch.load(pretrain_file, map_location=lambda storage, loc: storage)['model'].float().state_dict()\n        state_dict = intersect_dicts(state_dict, e.state_dict(), exclude=['anchor'])  # intersect\n        e.load_state_dict(state_dict, strict=False)\n\n        #---\n        # remove detect layer\n        assert (e.save[-3:] == e.model[-1].f)\n        removed = list(e.model.children())[:-1]\n        self.backbone = torch.nn.Sequential(*removed)\n        self.index = e.save\n        #print(self.index)\n\n        #---\n        self.head = nn.ModuleList([\n            nn.Conv2d(192, num_anchor*6, kernel_size=1),\n            nn.Conv2d(384, num_anchor*6, kernel_size=1),\n            nn.Conv2d(768, num_anchor*6, kernel_size=1),\n        ])\n\n        #---\n        #<todo> add image classification head\n        # self.index.append( ... add feature layer no to use ...)\n        #self.logit=nn.Sequential(\n        #    nn.Conv2d(768, 1, kernel_size=1),\n        #)\n\n\n    def forward(self, image):\n        batch_size = len(image)\n        x = 2*image-1\n\n        # yolov5 backbone ----------------------\n        # predict = self.e(x)\n        z = []\n        for m in self.backbone:\n            if m.f != -1:  # if not from previous layer\n                if isinstance(m.f, int):\n                    x = z[m.f]\n                else:\n                    x = [x if i == -1 else z[i] for i in m.f]\n            x = m(x)  # run\n            z.append(x if m.i in self.index else None)  # cache output\n        z = [z[i] for i in self.index[-3:]]\n        #--------------------------------------\n        predict = []\n        for n in range(num_head):\n            p = self.head[n](z[n])\n            batch_size, num_anchor_dim, h, w = p.shape\n            dim = num_anchor_dim//num_anchor\n            p = p.reshape(batch_size, num_anchor, dim, h, w).permute(0, 1, 3, 4, 2).contiguous()\n            predict.append(p)\n\n        return predict","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:48.062837Z","iopub.execute_input":"2021-08-07T06:44:48.063262Z","iopub.status.idle":"2021-08-07T06:44:48.082439Z","shell.execute_reply.started":"2021-08-07T06:44:48.063219Z","shell.execute_reply":"2021-08-07T06:44:48.081354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Pretrained-File","metadata":{}},{"cell_type":"code","source":"#if (len(os.listdir(\"/kaggle/input/d/deepkim/woong-yolov5\")) != 0):\n#    yolo_pretrained_path = [\"/kaggle/input/d/deepkim/woong-yolov5/\"+x for x in os.listdir(\"/kaggle/input/d/deepkim/woong-yolov5/\") if x.startswith(\"yolov5_fold\")]\nyolo_pretrained_path_640 = [\"../input/woong-yolov5/\"+x for x in os.listdir(\"../input/woong-yolov5/\") if x.endswith(\"best_model.pth\")]\nyolo_pretrained_path_512 = [\"../input/woong-yolov5/\"+x for x in os.listdir(\"../input/woong-yolov5/\") if x.endswith(\"_512.pth\")]\n#yolo_pretrained_path_384 = [\"../input/woong-yolov5/\"+x for x in os.listdir(\"../input/woong-yolov5/\") if x.endswith(\"_384.pth\")]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:48.084109Z","iopub.execute_input":"2021-08-07T06:44:48.08461Z","iopub.status.idle":"2021-08-07T06:44:48.099807Z","shell.execute_reply.started":"2021-08-07T06:44:48.084571Z","shell.execute_reply":"2021-08-07T06:44:48.098879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def yolo_get_pretrained_model(path):\n    yolonet = YoloNet()\n    if torch.cuda.is_available():\n        yolonet.load_state_dict(torch.load(path)['state_dict'], strict=True)\n        yolonet.cuda()\n    else:\n        yolonet.load_state_dict(torch.load(path, map_location=torch.device('cpu'))['state_dict'], strict=True)\n    return yolonet","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:48.102359Z","iopub.execute_input":"2021-08-07T06:44:48.102609Z","iopub.status.idle":"2021-08-07T06:44:48.109757Z","shell.execute_reply.started":"2021-08-07T06:44:48.102585Z","shell.execute_reply":"2021-08-07T06:44:48.108945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:48.111141Z","iopub.execute_input":"2021-08-07T06:44:48.111758Z","iopub.status.idle":"2021-08-07T06:44:48.120135Z","shell.execute_reply.started":"2021-08-07T06:44:48.111721Z","shell.execute_reply":"2021-08-07T06:44:48.119095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yolonet0_640 = yolo_get_pretrained_model(yolo_pretrained_path_640[0])\nyolonet1_640 = yolo_get_pretrained_model(yolo_pretrained_path_640[1])\nyolonet2_640 = yolo_get_pretrained_model(yolo_pretrained_path_640[2])\nyolonet3_640 = yolo_get_pretrained_model(yolo_pretrained_path_640[3])\nyolonet4_640 = yolo_get_pretrained_model(yolo_pretrained_path_640[4])\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:48.121737Z","iopub.execute_input":"2021-08-07T06:44:48.122353Z","iopub.status.idle":"2021-08-07T06:44:57.031923Z","shell.execute_reply.started":"2021-08-07T06:44:48.122317Z","shell.execute_reply":"2021-08-07T06:44:57.030967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilitys","metadata":{}},{"cell_type":"code","source":"def do_non_max_suppression(\n    predict_flat,\n    nms_objectness_threshold = nms_objectness_threshold,\n    nms_iou_threshold = nms_iou_threshold,\n    nms_pre_max_num   = nms_pre_max_num,\n    nms_post_max_num  = nms_post_max_num,\n):\n    batch_size = len(predict_flat)\n    detection = []\n    for b in range(batch_size):\n        p = predict_flat[b]\n\n        i = p[..., 4] > nms_objectness_threshold\n        p = p[i]\n        num = len(p)\n        if num==0:\n            det = np.zeros((0,5),np.float32) #None\n            detection.append(det)\n            continue\n\n        box = xywh2xyxy(p[:, :4])\n        score = p[:,4]\n        #<todo> x[:, 5:] *= x[:, 4:5]  #conf = obj_conf * cls_conf\n\n        if num > nms_pre_max_num:  # excess boxes\n            i = score.argsort(descending=True)[:nms_pre_max_num]  # sort by confidence\n            box = box[i]\n            score = score[i]\n\n        i = torchvision.ops.nms(box, score, nms_iou_threshold)\n        if len(i) > nms_post_max_num:\n            i = i[:nms_post_max_num]\n\n        #<todo> merge NMS (boxes merged using weighted mean)  # sort by confidence\n        box = box[i]\n        score = score[i]\n        det = torch.cat([box,score[:,None]],-1)\n        det = det.data.cpu().numpy()\n        detection.append(det)\n\n    return detection\n\n\ndef pyramid_to_flat(data):\n    flat = []\n    for n in range(num_head):\n        p = data[n]\n        batch_size, num_anchor, w, h, dim = p.shape\n        flat.append(p.reshape(batch_size, -1, dim))\n    flat = torch.cat(flat, 1)\n    return flat\n\ndef infer_prediction(predict):\n    device = predict[0].device\n\n    z = []  # inference output\n    for n in range(num_head):\n        p = predict[n]\n        batch_size, num_anchor, w, h, dim = p.shape\n\n        # inference\n        aa = torch.FloatTensor(anchor_size[n]).to(device)\n        yy, xx = torch.meshgrid([torch.arange(h), torch.arange(w)])\n        grid = torch.stack((xx, yy), 2).reshape((1, 1, h, w, 2)).float().to(device)\n\n        d = p.sigmoid()  #objectivesness and class also use sigmoid ...\n        d[..., 0:2] = (d[..., 0:2] * 2 - 0.5 + grid) * feature_stride[n]  # xy\n        d[..., 2:4] = (d[..., 2:4] * 2) ** 2 * aa.reshape(1, num_anchor, 1, 1, 2)  # wh\n\n        z.append(d)\n    return z","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:57.033705Z","iopub.execute_input":"2021-08-07T06:44:57.034044Z","iopub.status.idle":"2021-08-07T06:44:57.049592Z","shell.execute_reply.started":"2021-08-07T06:44:57.034008Z","shell.execute_reply":"2021-08-07T06:44:57.048455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Do Predict","metadata":{}},{"cell_type":"code","source":"# do_predict(net, test_loader)\ndef do_predict(net, valid_loader, tta=[]): #flip\n\n    valid_detection = []\n\n    start_timer = timer()\n    for t, batch in enumerate(valid_loader):\n        if torch.cuda.is_available():\n            image  = batch['image'].cuda()\n        else:\n            image = batch['image']\n\n        #<todo> TTA\n        net.eval()\n        with torch.no_grad():\n            predict = net(image)\n            predict = infer_prediction(predict)\n            predict_flat = pyramid_to_flat(predict)\n\n            detection = do_non_max_suppression(\n                    predict_flat,\n                    nms_objectness_threshold=0.001,\n                    nms_iou_threshold=0.5,\n                    nms_pre_max_num=500,\n                    nms_post_max_num=25,\n            )\n\n            \n\n        valid_detection.extend(detection)\n\n\n    detection = valid_detection\n    return detection","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:57.050875Z","iopub.execute_input":"2021-08-07T06:44:57.05141Z","iopub.status.idle":"2021-08-07T06:44:57.061778Z","shell.execute_reply.started":"2021-08-07T06:44:57.051373Z","shell.execute_reply":"2021-08-07T06:44:57.060959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do_predict(net, test_loader)\nsys.path.append(\"/kaggle/input/weightedboxesfusion/\")\nfrom ensemble_boxes import *\ndef do_ensemble_predict(net, valid_loader, image_size, tta=[]): #flip\n\n    valid_detection = []\n\n    start_timer = timer()\n    for t, batch in enumerate(valid_loader):\n        if torch.cuda.is_available():\n            image  = batch['image'].cuda()\n        else:\n            image = batch['image']\n\n        #<todo> TTA\n        net.eval()\n        with torch.no_grad():\n            \n            ensemble_detection = []\n        \n            predict_flip = net(torch.flip(image, dims=(3,)))\n            predict_flip = infer_prediction(predict_flip)\n            predict_flip_flat = pyramid_to_flat(predict_flip)\n\n\n            detection_flip = do_non_max_suppression(\n                        predict_flip_flat,\n                        nms_objectness_threshold=0.001,\n                        nms_iou_threshold=0.4,\n                        nms_pre_max_num=500,\n                        nms_post_max_num=25)\n            \n            predict = net(image)\n            predict = infer_prediction(predict)\n            predict_flat = pyramid_to_flat(predict)\n\n            #print(predict_flat)\n            #print(predict_flat.shape)\n            detection = do_non_max_suppression(\n                        predict_flat,\n                        nms_objectness_threshold=0.001,\n                        nms_iou_threshold=0.4,\n                        nms_pre_max_num=500,\n                        nms_post_max_num=25)\n            \n            \n            for i, (det0, det1) in enumerate(zip(detection_flip, detection)):\n                iou_thr = 0.5\n                skip_box_thr = 0.0001\n                boxes_list = []\n                scores_list = []\n                weights = [1,1]\n                labels_list = [[1]*det0.shape[0], [1]*det1.shape[0]]\n\n                conf0 = det0[:,4].tolist()\n                conf1 = det1[:,4].tolist()\n                \n                det0 = det0.astype(int)\n                det1 = det1.astype(int)\n                tmp = det0[:,0].copy()\n                det0[:,0] = (image_size) - det0[:,2].astype(int)               \n                det0[:,2] = (image_size) - tmp.astype(int)\n\n                det0 = np.where(det0<0,0,det0)\n                det1 = np.where(det1<0,0,det1)\n                det0 = np.where(det0>image_size,1,det0)\n                det1 = np.where(det1>image_size,1,det1)\n                box0 = (det0[:,:4]/image_size).tolist()\n                box1 = (det1[:,:4]/image_size).tolist()\n                \n                \n                boxes_list.append(box0)\n                boxes_list.append(box1)\n\n                scores_list.append(conf0)\n                scores_list.append(conf1)\n\n                boxes, scores, labels = weighted_boxes_fusion(boxes_list,\n                                                  scores_list,\n                                                  labels_list,\n                                                  weights=weights,\n                                                  iou_thr = iou_thr,\n                                                  skip_box_thr = skip_box_thr)\n                processed_det0 = np.zeros(shape=(len(boxes), 5))\n                processed_det0[:,:4] = np.array(boxes)\n                processed_det0[:,4] = np.array(scores)\n                ensemble_detection.append(processed_det0)\n\n            \n\n        valid_detection.extend(ensemble_detection)\n\n\n    detection = valid_detection\n    return detection","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:57.069624Z","iopub.execute_input":"2021-08-07T06:44:57.069883Z","iopub.status.idle":"2021-08-07T06:44:57.118156Z","shell.execute_reply.started":"2021-08-07T06:44:57.069859Z","shell.execute_reply":"2021-08-07T06:44:57.117434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detection0_640 = do_ensemble_predict(yolonet0_640, yolo_test_loader_640, image_size=640)\ndel yolonet0_640\ngc.collect()\ndetection1_640 = do_ensemble_predict(yolonet1_640, yolo_test_loader_640, image_size=640)\ndel yolonet1_640\ngc.collect()\n\ndetection2_640 = do_ensemble_predict(yolonet2_640, yolo_test_loader_640, image_size=640)\ndel yolonet2_640\ngc.collect()\ndetection3_640 = do_ensemble_predict(yolonet3_640, yolo_test_loader_640, image_size=640)\ndel yolonet3_640\ngc.collect()\ndetection4_640 = do_ensemble_predict(yolonet4_640, yolo_test_loader_640, image_size=640)\ndel yolonet4_640\ngc.collect()\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:57.119729Z","iopub.execute_input":"2021-08-07T06:44:57.119961Z","iopub.status.idle":"2021-08-07T06:44:58.630012Z","shell.execute_reply.started":"2021-08-07T06:44:57.119939Z","shell.execute_reply":"2021-08-07T06:44:58.628921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert len(detection0_640) == len_test\nassert len(detection1_640) == len_test\nassert len(detection2_640) == len_test\nassert len(detection3_640) == len_test\nassert len(detection4_640) == len_test\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:58.631413Z","iopub.execute_input":"2021-08-07T06:44:58.631783Z","iopub.status.idle":"2021-08-07T06:44:58.63694Z","shell.execute_reply.started":"2021-08-07T06:44:58.631745Z","shell.execute_reply":"2021-08-07T06:44:58.635936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VFNet","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nsys.path.append('/kaggle/working/mmdetection')\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n\n# Check mmcv installation\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nprint(get_compiling_cuda_version())\nprint(get_compiler_version())\n\n# Check MMDetection installation\nfrom mmdet.apis import set_random_seed\n\n# Imports\nimport mmdet\nfrom mmdet.apis import set_random_seed\nfrom mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector\n\nimport random\nimport numpy as np\nfrom pathlib import Path","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:44:58.638558Z","iopub.execute_input":"2021-08-07T06:44:58.638982Z","iopub.status.idle":"2021-08-07T06:45:25.250137Z","shell.execute_reply.started":"2021-08-07T06:44:58.638941Z","shell.execute_reply":"2021-08-07T06:45:25.249115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"global_seed = 111\n\ndef set_seed(seed=global_seed):\n    \"\"\"Sets the random seeds.\"\"\"\n    set_random_seed(seed, deterministic=False)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.251682Z","iopub.execute_input":"2021-08-07T06:45:25.252111Z","iopub.status.idle":"2021-08-07T06:45:25.262573Z","shell.execute_reply.started":"2021-08-07T06:45:25.252067Z","shell.execute_reply":"2021-08-07T06:45:25.261363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/mmdetection\")","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.264393Z","iopub.execute_input":"2021-08-07T06:45:25.264937Z","iopub.status.idle":"2021-08-07T06:45:25.275762Z","shell.execute_reply.started":"2021-08-07T06:45:25.264894Z","shell.execute_reply":"2021-08-07T06:45:25.274562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mmcv import Config\ncfg = Config.fromfile('/kaggle/working/mmdetection/configs/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco.py')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.277511Z","iopub.execute_input":"2021-08-07T06:45:25.277984Z","iopub.status.idle":"2021-08-07T06:45:25.3126Z","shell.execute_reply.started":"2021-08-07T06:45:25.277942Z","shell.execute_reply":"2021-08-07T06:45:25.31186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\nnow = datetime.datetime.now()\n\ndata = dict(\n    info=dict(\n        description=None,\n        url=None,\n        version=None,\n        year=now.year,\n        contributor=None,\n        date_created=now.strftime('%Y-%m-%d %H:%M:%S.%f'),\n    ),\n    licenses=[dict(\n        url=None,\n        id=0,\n        name=None,\n    )],\n    images=[\n        # license, url, file_name, height, width, date_captured, id\n    ],\n    type='instances',\n    annotations=[\n        # segmentation, area, iscrowd, image_id, bbox, category_id, id\n    ],\n    categories=[\n        # supercategory, id, name\n    ],\n)\nlabels =  [\n            \"__ignore__\",\n            \"opacity\",\n            ]\nclass_name_to_id = {}\nfor i, each_label in enumerate(labels):\n    class_id = i - 1  # starts with -1\n    class_name = each_label\n    if class_id == -1:\n        assert class_name == '__ignore__'\n        continue\n    class_name_to_id[class_name] = class_id\n    data['categories'].append(dict(\n        supercategory=None,\n        id=class_id,\n        name=class_name,\n    ))","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.313727Z","iopub.execute_input":"2021-08-07T06:45:25.314079Z","iopub.status.idle":"2021-08-07T06:45:25.321373Z","shell.execute_reply.started":"2021-08-07T06:45:25.314036Z","shell.execute_reply":"2021-08-07T06:45:25.320419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mmcv import Config\nfrom mmdet.apis import set_random_seed\nfrom mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector, init_detector, inference_detector","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.322885Z","iopub.execute_input":"2021-08-07T06:45:25.323388Z","iopub.status.idle":"2021-08-07T06:45:25.337825Z","shell.execute_reply.started":"2021-08-07T06:45:25.323352Z","shell.execute_reply":"2021-08-07T06:45:25.336936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg.dataset_type = 'CocoDataset' # Dataset type, this will be used to define the dataset\ncfg.classes = (\"opacity\",)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.339014Z","iopub.execute_input":"2021-08-07T06:45:25.339447Z","iopub.status.idle":"2021-08-07T06:45:25.347302Z","shell.execute_reply.started":"2021-08-07T06:45:25.339413Z","shell.execute_reply":"2021-08-07T06:45:25.34643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mmdet.apis import set_random_seed\n\n# Imports\nimport mmdet\nfrom mmdet.apis import set_random_seed\nfrom mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\n\nimport mmcv\nfrom mmcv import Config\nfrom mmcv.runner import load_checkpoint\nfrom mmcv.parallel import MMDataParallel\nfrom mmdet.apis import inference_detector, init_detector, show_result_pyplot\nfrom mmdet.apis import single_gpu_test\nfrom mmdet.datasets import build_dataloader, build_dataset","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.348448Z","iopub.execute_input":"2021-08-07T06:45:25.349113Z","iopub.status.idle":"2021-08-07T06:45:25.356962Z","shell.execute_reply.started":"2021-08-07T06:45:25.349054Z","shell.execute_reply":"2021-08-07T06:45:25.35611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\nlabel2color = [[59, 238, 119]]\n\nviz_labels =  [\"Opacity\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.35854Z","iopub.execute_input":"2021-08-07T06:45:25.358945Z","iopub.status.idle":"2021-08-07T06:45:25.367228Z","shell.execute_reply.started":"2021-08-07T06:45:25.358912Z","shell.execute_reply":"2021-08-07T06:45:25.366381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nWEIGHTS_FILE_LIST = glob.glob(\"../input/rerenovated-vfnet-pretrained/*pth\")\noptions = dict(classes = (\"opacity\"))\nfrom mmdet.apis import inference_detector, init_detector, show_result_pyplot\nimport glob\n\nimport datetime\nnow = datetime.datetime.now()\n\ndata = dict(\n    info=dict(\n        description=None,\n        url=None,\n        version=None,\n        year=now.year,\n        contributor=None,\n        date_created=now.strftime('%Y-%m-%d %H:%M:%S.%f'),\n    ),\n    licenses=[dict(\n        url=None,\n        id=0,\n        name=None,\n    )],\n    images=[\n        # license, url, file_name, height, width, date_captured, id\n    ],\n    type='instances',\n    annotations=[\n        # segmentation, area, iscrowd, image_id, bbox, category_id, id\n    ],\n    categories=[\n        # supercategory, id, name\n    ],\n)\nlabels =  [\n            \"__ignore__\",\n            \"opacity\",\n            ]\nclass_name_to_id = {}\nfor i, each_label in enumerate(labels):\n    class_id = i - 1  # starts with -1\n    class_name = each_label\n    if class_id == -1:\n        assert class_name == '__ignore__'\n        continue\n    class_name_to_id[class_name] = class_id\n    data['categories'].append(dict(\n        supercategory=None,\n        id=class_id,\n        name=class_name,\n    ))\n    \n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.368462Z","iopub.execute_input":"2021-08-07T06:45:25.36877Z","iopub.status.idle":"2021-08-07T06:45:25.39518Z","shell.execute_reply.started":"2021-08-07T06:45:25.368742Z","shell.execute_reply":"2021-08-07T06:45:25.394338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport shutil\nfrom pathlib import Path\ndata_test = data.copy()\ndata_test['images'] = []\ndata_test['annotations'] = []\n\ndef get_test_annotation(image_paths, anno_name):\n        #test_output_dir = f\"train_{IMAGE_SIZE}\"\n#test_annotations = df_test.copy()\n        for i, path in tqdm(enumerate(image_paths)):  \n            #image_basename = Path(path).stem\n            ## Copy Image\n\n            ## Add Images to annotation\n            #print(os.path.isfile(path))\n            data_test['images'].append(dict(\n                license = 0,\n                url = None,\n                file_name = path,\n                height = 640,\n                width = 640,\n                data_captured = None,\n                id = i\n\n            ))\n        with open(anno_name, 'w') as f:\n            json.dump(data_test, f, indent=4)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.397039Z","iopub.execute_input":"2021-08-07T06:45:25.397311Z","iopub.status.idle":"2021-08-07T06:45:25.405378Z","shell.execute_reply.started":"2021-08-07T06:45:25.397285Z","shell.execute_reply":"2021-08-07T06:45:25.404445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['image_path'] = f'/kaggle/tmp/{split}/image/'+df_test['id']+\".png\"\ndf_test['image_path']","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.407072Z","iopub.execute_input":"2021-08-07T06:45:25.407858Z","iopub.status.idle":"2021-08-07T06:45:25.420476Z","shell.execute_reply.started":"2021-08-07T06:45:25.407766Z","shell.execute_reply":"2021-08-07T06:45:25.419331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vfnet_anno = [\"vfnet_anno_test.json\"]\ntest_annotation = get_test_annotation(df_test['image_path'].tolist(), vfnet_anno[0])\ndata_test['images'] = []","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.422042Z","iopub.execute_input":"2021-08-07T06:45:25.422428Z","iopub.status.idle":"2021-08-07T06:45:25.46934Z","shell.execute_reply.started":"2021-08-07T06:45:25.422358Z","shell.execute_reply":"2021-08-07T06:45:25.468245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET_TYPE = 'CocoDataset'\ncfg.data.test.img_prefix = \"\"\ncfg.data.test.classes = (\"opacity\",)\ncfg.data.test.ann_file = vfnet_anno[0]\ncfg.data.test.type = DATASET_TYPE","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.470865Z","iopub.execute_input":"2021-08-07T06:45:25.471315Z","iopub.status.idle":"2021-08-07T06:45:25.47865Z","shell.execute_reply.started":"2021-08-07T06:45:25.471274Z","shell.execute_reply":"2021-08-07T06:45:25.476534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseline_cfg_path = \"/kaggle/working/mmdetection/configs/vfnet/vfnet_r50_fpn_mdconv_c3-c5_mstrain_2x_coco.py\"\ncfg = Config.fromfile(baseline_cfg_path)\n\ncfg.classes = (\"opacity\")\ncfg.data.test.img_prefix = ''\ncfg.data.test.classes = cfg.classes\n\n\ncfg.model.bbox_head.num_classes=1\n\n\ncfg.seed = 211\nset_random_seed(211, deterministic=False)\ncfg.gpu_ids = [0]\n\ncfg.data.test.pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=[(640,640)],\n                flip=True,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip', direction='horizontal'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]\n\ncfg.test_pipeline = [\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=[(640,640)],\n                flip=True,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip', direction='horizontal'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]\n\ncfg.model.test_cfg.score_thr = 0.001","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.480615Z","iopub.execute_input":"2021-08-07T06:45:25.481211Z","iopub.status.idle":"2021-08-07T06:45:25.521104Z","shell.execute_reply.started":"2021-08-07T06:45:25.481172Z","shell.execute_reply":"2021-08-07T06:45:25.520378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_pred(boxes: np.ndarray, scores: np.ndarray, labels: np.ndarray) -> str:\n    pred_strings = []\n    label_str = ['opacity']\n    for label, score, bbox in zip(labels, scores, boxes):\n        xmin, ymin, xmax, ymax = bbox.astype(np.int64)\n        pred_strings.append(f\"{label_str[int(label)]} {score:.16f} {xmin} {ymin} {xmax} {ymax}\")\n    return \" \".join(pred_strings)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.523567Z","iopub.execute_input":"2021-08-07T06:45:25.523806Z","iopub.status.idle":"2021-08-07T06:45:25.529991Z","shell.execute_reply.started":"2021-08-07T06:45:25.523782Z","shell.execute_reply":"2021-08-07T06:45:25.528894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_results = []\ntotal_image_infos = []\niou_nms = 0.5\ndevice = 'cuda'\nfor fold in [0,1,2,3,4]:\n        cfg.data.test.ann_file = f\"vfnet_anno_test.json\"\n        viz_images = []\n        results = []\n        image_infos = []\n\n        model = init_detector(cfg, WEIGHTS_FILE_LIST[fold])\n        model.to(device)\n        model.eval()\n\n        with torch.no_grad():\n            for index, row in tqdm(df_test.iterrows(), total=df_test.shape[0], position=0, leave=True):\n                #original_H, original_W = (int(row.height), int(row.width))\n                predictions = inference_detector(model, row.image_path)\n\n                boxes = predictions[0][:,:4]\n\n                scores = predictions[0][:,4]\n\n                boxes = boxes / 640\n\n\n                skip_box_thr = 0.0001\n                iou_thr = iou_nms\n                boxes_list = []\n                scores_list = []\n                weights = [1]\n                labels_list = [[1]*boxes.shape[0]]\n\n\n                boxes_list.append(boxes.tolist())\n                scores_list.append(scores.tolist())\n                boxes, scores, labels = nms(boxes_list,\n                                                              scores_list,\n                                                              labels_list,\n                                                              weights=weights,\n                                                              iou_thr = iou_thr,)\n                                                          #skip_box_thr = skip_box_thr\n                processed_det0 = np.zeros(shape=(len(boxes), 5))\n                processed_det0[:,:4] = np.array(boxes)\n                processed_det0[:,4] = np.array(scores)\n                if processed_det0[0,0] > 2:\n                    print('error')\n                    break\n\n                results.append(processed_det0)\n                image_infos.append(row.id)\n            total_results.append(results)\n            total_image_infos.append(image_infos)\n            gc.collect()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:25.531571Z","iopub.execute_input":"2021-08-07T06:45:25.532161Z","iopub.status.idle":"2021-08-07T06:45:46.435834Z","shell.execute_reply.started":"2021-08-07T06:45:25.532125Z","shell.execute_reply":"2021-08-07T06:45:46.434926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CasCade","metadata":{}},{"cell_type":"code","source":"WEIGHTS_FILE_LIST = glob.glob(\"../input/covid-cascade/*pth\")\nWEIGHTS_FILE_LIST","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:46.437147Z","iopub.execute_input":"2021-08-07T06:45:46.437648Z","iopub.status.idle":"2021-08-07T06:45:46.450546Z","shell.execute_reply.started":"2021-08-07T06:45:46.437607Z","shell.execute_reply":"2021-08-07T06:45:46.449458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CASCADE CFG","metadata":{}},{"cell_type":"code","source":"cfg = Config.fromfile('/kaggle/working/mmdetection/configs/cascade_rcnn/cascade_rcnn_x101_32x4d_fpn_1x_coco.py')\nfor head in cfg.model.roi_head.bbox_head:\n    head.num_classes = 1\ncfg.dataset_type = 'CocoDataset' # Dataset type, this will be used to define the dataset\ncfg.classes = (\"opacity\",)\nDATASET_TYPE = 'CocoDataset'\ncfg.data.test.img_prefix = \"\"\ncfg.data.test.classes = (\"opacity\",)\ncfg.data.test.ann_file = vfnet_anno[0]\ncfg.data.test.type = DATASET_TYPE\ncfg.classes = (\"opacity\")\ncfg.data.test.classes = cfg.classes\n\n# cfg.model.roi_head.bbox_head.num_classes = 1\n# cfg.model.bbox_head.num_classes = 1\n#for head in cfg.model.roi_head.bbox_head:\n#    head.num_classes = 1\n#cfg.model.bbox_head.num_classes=1\n\n# Set seed thus the results are more reproducible\ncfg.seed = 211\nset_random_seed(211, deterministic=False)\ncfg.gpu_ids = [0]\n\ncfg.data.test.pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=[(640,640)],\n                flip=True,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip', direction='horizontal'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]\n\ncfg.test_pipeline = [\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=[(640,640)],\n                flip=True,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip', direction='horizontal'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]\n\n# cfg.data.samples_per_gpu = 4\n# cfg.data.workers_per_gpu = 4\n# cfg.model.test_cfg.nms.iou_threshold = 0.3\ncfg.model.test_cfg.score_thr = 0.001\ncfg.model.test_cfg.rcnn.score_thr = 0.001\ncfg.model.test_cfg.rcnn.nms.iou_threshold=1.1","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:45:46.451988Z","iopub.execute_input":"2021-08-07T06:45:46.452453Z","iopub.status.idle":"2021-08-07T06:45:46.49047Z","shell.execute_reply.started":"2021-08-07T06:45:46.452415Z","shell.execute_reply":"2021-08-07T06:45:46.489692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_results_cascade = []\ntotal_image_infos = []\niou_nms = 0.5\ndevice = 'cuda'\nfor fold in [0,1,2,3,4]:\n        cfg.data.test.ann_file = f\"vfnet_anno_test.json\"\n        viz_images = []\n        results = []\n        image_infos = []\n\n        model = init_detector(cfg, WEIGHTS_FILE_LIST[fold])\n        model.to(device)\n        model.eval()\n\n        with torch.no_grad():\n            for index, row in tqdm(df_test.iterrows(), total=df_test.shape[0], position=0, leave=True):\n                #original_H, original_W = (int(row.height), int(row.width))\n                predictions = inference_detector(model, row.image_path)\n\n                boxes = predictions[0][:,:4]\n\n                scores = predictions[0][:,4]\n\n                boxes = boxes / 640\n\n\n                skip_box_thr = 0.0001\n                iou_thr = iou_nms\n                boxes_list = []\n                scores_list = []\n                weights = [1]\n                labels_list = [[1]*boxes.shape[0]]\n\n\n                boxes_list.append(boxes.tolist())\n                scores_list.append(scores.tolist())\n                boxes, scores, labels = nms(boxes_list,\n                                                              scores_list,\n                                                              labels_list,\n                                                              weights=weights,\n                                                              iou_thr = iou_thr,)\n                                                          #skip_box_thr = skip_box_thr\n                processed_det0 = np.zeros(shape=(len(boxes), 5))\n                processed_det0[:,:4] = np.array(boxes)\n                processed_det0[:,4] = np.array(scores)\n                if processed_det0[0,0] > 2:\n                    print('error')\n                    break\n\n                results.append(processed_det0)\n                image_infos.append(row.id)\n            total_results_cascade.append(results)\n            total_image_infos.append(image_infos)\n            gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:49:10.070481Z","iopub.execute_input":"2021-08-07T06:49:10.070838Z","iopub.status.idle":"2021-08-07T06:49:43.310905Z","shell.execute_reply.started":"2021-08-07T06:49:10.070807Z","shell.execute_reply":"2021-08-07T06:49:43.310038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weighted Boxes Fusion","metadata":{}},{"cell_type":"code","source":"iou_thr = 0.6\nskip_box_thr = 0.1\nimage_size = 640\n\n#total_boxes = []\n#total_scores = []\n#total_labels = []\n\nensemble_detection = []\n\nfor i, (det0_640, det1_640, det2_640, det3_640, det4_640,\n        vfnet0,vfnet1,vfnet2,vfnet3,vfnet4,\n        cascade0,cascade1,cascade2,cascade3,cascade4) in enumerate(zip(detection0_640, detection1_640, detection2_640,\\\n                                                                            detection3_640, detection4_640,\\\n                                                                            total_results[0],total_results[1],\\\n                                                                            total_results[2],total_results[3],\\\n                                                                            total_results[4],\n                                                                            total_results_cascade[0],\\\n                                                                            total_results_cascade[1],\\\n                                                                            total_results_cascade[2],\\\n                                                                            total_results_cascade[3],\\\n                                                                            total_results_cascade[4])):\n                    #print(f\"i : f {i}\"+\" =\"*50)\n    \n    boxes_list = []\n    scores_list = []\n    weights=[1,1,1,1,1,\n             1,1,1,1,1,\n             1,1,1,1,1]\n    \n    labels_list = [[1]*len(det0_640),[1]*len(det1_640),[1]*len(det2_640),\\\n                   [1]*len(det3_640),[1]*len(det4_640),\n                   [1]*len(vfnet0),[1]*len(vfnet1),[1]*len(vfnet2),[1]*len(vfnet3),[1]*len(vfnet4),\n                   [1]*len(cascade0),[1]*len(cascade1),[1]*len(cascade2),[1]*len(cascade3),[1]*len(cascade4)]\n    \n    box0_640 = (det0_640[:,:4]).tolist()\n    box1_640 = (det1_640[:,:4]).tolist()\n    box2_640 = (det2_640[:,:4]).tolist()\n    box3_640 = (det3_640[:,:4]).tolist()\n    box4_640 = (det4_640[:,:4]).tolist()\n\n    box0_vfnet = (vfnet0[:,:4]).tolist()\n    box1_vfnet = (vfnet1[:,:4]).tolist()\n    box2_vfnet = (vfnet2[:,:4]).tolist()\n    box3_vfnet = (vfnet3[:,:4]).tolist()\n    box4_vfnet = (vfnet4[:,:4]).tolist()\n    \n    box0_cascade = (cascade0[:,:4]).tolist()\n    box1_cascade = (cascade1[:,:4]).tolist()\n    box2_cascade = (cascade2[:,:4]).tolist()\n    box3_cascade = (cascade3[:,:4]).tolist()\n    box4_cascade = (cascade4[:,:4]).tolist()\n    \n   \n    \n    conf0_640 = det0_640[:,4].tolist()\n    conf1_640 = det1_640[:,4].tolist()\n    conf2_640 = det2_640[:,4].tolist()\n    conf3_640 = det3_640[:,4].tolist()\n    conf4_640 = det4_640[:,4].tolist()\n\n    conf0_vfnet = vfnet0[:,4].tolist()\n    conf1_vfnet = vfnet1[:,4].tolist()\n    conf2_vfnet = vfnet2[:,4].tolist()\n    conf3_vfnet = vfnet3[:,4].tolist()\n    conf4_vfnet = vfnet4[:,4].tolist()\n    \n    conf0_cascade = cascade0[:,4].tolist()\n    conf1_cascade = cascade1[:,4].tolist()\n    conf2_cascade = cascade2[:,4].tolist()\n    conf3_cascade = cascade3[:,4].tolist()\n    conf4_cascade = cascade4[:,4].tolist()\n    \n    \n    boxes_list.append(box0_640)\n    boxes_list.append(box1_640)\n    boxes_list.append(box2_640)\n    boxes_list.append(box3_640)\n    boxes_list.append(box4_640)\n\n    boxes_list.append(box0_vfnet)\n    boxes_list.append(box1_vfnet)\n    boxes_list.append(box2_vfnet)\n    boxes_list.append(box3_vfnet)\n    boxes_list.append(box4_vfnet)\n    \n    boxes_list.append(box0_cascade)\n    boxes_list.append(box1_cascade)\n    boxes_list.append(box2_cascade)\n    boxes_list.append(box3_cascade)\n    boxes_list.append(box4_cascade)\n\n    \n    scores_list.append(conf0_640)\n    scores_list.append(conf1_640)\n    scores_list.append(conf2_640)\n    scores_list.append(conf3_640)\n    scores_list.append(conf4_640)\n\n    scores_list.append(conf0_vfnet)\n    scores_list.append(conf1_vfnet)\n    scores_list.append(conf2_vfnet)\n    scores_list.append(conf3_vfnet)\n    scores_list.append(conf4_vfnet)\n    \n    scores_list.append(conf0_cascade)\n    scores_list.append(conf1_cascade)\n    scores_list.append(conf2_cascade)\n    scores_list.append(conf3_cascade)\n    scores_list.append(conf4_cascade)\n\n    \n    boxes, scores, labels = weighted_boxes_fusion(boxes_list,\n                                                  scores_list,\n                                                  labels_list,\n                                                  weights=weights,\n                                                  iou_thr = iou_thr,\n                                                  skip_box_thr = skip_box_thr)\n    \n\n    processed_det0 = np.zeros(shape=(len(boxes), 5))\n    processed_det0[:,:4] = np.array(boxes)*image_size\n    processed_det0[:,4] = np.array(scores)\n    ensemble_detection.append(processed_det0)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:54:53.119759Z","iopub.execute_input":"2021-08-07T06:54:53.12017Z","iopub.status.idle":"2021-08-07T06:54:53.206804Z","shell.execute_reply.started":"2021-08-07T06:54:53.120133Z","shell.execute_reply":"2021-08-07T06:54:53.206033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_df_image(df_valid, detection):\n    df_image = pd.DataFrame()\n    df_image.loc[:,'id'] = df_valid.image + '_image'\n\n    predict_string = []\n    for i,det in enumerate(detection):\n        d = df_valid.iloc[i]\n\n        s = ''\n        for x0, y0, x1, y1, c in det:\n            x0 = int(x0/image_size*d.dim1 )\n            y0 = int(y0/image_size*d.dim0)\n            x1 = int(x1/image_size*d.dim1 )\n            y1 = int(y1/image_size*d.dim0)\n            s += ' opacity %0.5f %4d %4d %4d %4d'%(c,x0,y0,x1,y1)\n        predict_string.append(s)\n\n    df_image.loc[:, 'PredictionString'] = predict_string\n    return df_image","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:55:00.342493Z","iopub.execute_input":"2021-08-07T06:55:00.342835Z","iopub.status.idle":"2021-08-07T06:55:00.351875Z","shell.execute_reply.started":"2021-08-07T06:55:00.342803Z","shell.execute_reply":"2021-08-07T06:55:00.350966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = make_df_image(df_test, ensemble_detection)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:55:30.41581Z","iopub.execute_input":"2021-08-07T06:55:30.416156Z","iopub.status.idle":"2021-08-07T06:55:30.431881Z","shell.execute_reply.started":"2021-08-07T06:55:30.416124Z","shell.execute_reply":"2021-08-07T06:55:30.430863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = test_df.drop(['PredictionString'], axis=1)\nsub_df = pd.merge(test_df, pred_df, on = 'id', how = 'left').fillna(\"none 1 0 0 1 1\")","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:55:38.142879Z","iopub.execute_input":"2021-08-07T06:55:38.14325Z","iopub.status.idle":"2021-08-07T06:55:38.159292Z","shell.execute_reply.started":"2021-08-07T06:55:38.143218Z","shell.execute_reply":"2021-08-07T06:55:38.158395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = sub_df[['id', 'PredictionString']]","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:55:38.851664Z","iopub.execute_input":"2021-08-07T06:55:38.852096Z","iopub.status.idle":"2021-08-07T06:55:38.858343Z","shell.execute_reply.started":"2021-08-07T06:55:38.852057Z","shell.execute_reply":"2021-08-07T06:55:38.857205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df['none'] = df_2class['none']\nfor i in range(sub_df.shape[0]):\n    if sub_df.loc[i,'PredictionString'] == \"none 1 0 0 1 1\":\n        continue\n    sub_df_split = sub_df.loc[i,'PredictionString'].split()\n    twoclass_none = sub_df.loc[i,'none']\n    sub_df_list = []\n    final_sub_df_list = []\n    for j in range(int(len(sub_df_split) / 6)):\n        sub_df_list.append(['opacity',\n                                        (float(sub_df_split[6 * j + 1]))*(1-twoclass_none)**0.20,\n                                        sub_df_split[6 * j + 2],\n                                        sub_df_split[6 * j + 3],\n                                        sub_df_split[6 * j + 4],\n                                        sub_df_split[6 * j + 5]]\n                                    )\n    sub_df_list = sorted(sub_df_list, key = lambda x: x[1], reverse=True)\n    for zz in range(len(sub_df_list)):\n        sub_df_list[zz][1] = str(sub_df_list[zz][1])\n        final_sub_df_list.extend(sub_df_list[zz])\n\n    sub_df.loc[i,'PredictionString'] = ' '.join(final_sub_df_list)\n\nfor i in range(sub_df.shape[0]):\n    if sub_df.loc[i,'PredictionString'] != 'none 1 0 0 1 1':\n        sub_df.loc[i,'PredictionString'] = sub_df.loc[i,'PredictionString'] + ' none ' + str(sub_df.loc[i,'none']) + ' 0 0 1 1'\nsub_df = sub_df[['id', 'PredictionString']]   \ndf_study = df_study[:study_len]\ndf_study = df_study.append(sub_df).reset_index(drop=True)\ndf_study.to_csv('/kaggle/working/submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:55:43.815874Z","iopub.execute_input":"2021-08-07T06:55:43.816297Z","iopub.status.idle":"2021-08-07T06:55:43.838765Z","shell.execute_reply.started":"2021-08-07T06:55:43.816264Z","shell.execute_reply":"2021-08-07T06:55:43.837571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r /kaggle/working/mmdetection","metadata":{"execution":{"iopub.status.busy":"2021-08-07T06:55:45.231514Z","iopub.execute_input":"2021-08-07T06:55:45.231871Z","iopub.status.idle":"2021-08-07T06:55:45.698069Z","shell.execute_reply.started":"2021-08-07T06:55:45.231842Z","shell.execute_reply":"2021-08-07T06:55:45.696745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}