{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![TF](https://www.gstatic.com/devrel-devsite/prod/ve312520032ba2ac0c4d23f7b46fc670cbbe051886a2d1f04563a5e4768ad9787/tensorflow/images/lockup.svg)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T16:20:40.529016Z","iopub.execute_input":"2021-05-25T16:20:40.529499Z","iopub.status.idle":"2021-05-25T16:20:41.273387Z","shell.execute_reply.started":"2021-05-25T16:20:40.529396Z","shell.execute_reply":"2021-05-25T16:20:41.271896Z"}}},{"cell_type":"markdown","source":"# TFRecords\nThis notebook will create TFRecords of the SIIM COVID-19 X-rays for image classification. Object detection is also part of the task, but a separate set of TFRecords will be created for that. Images are resized to 1024 while preserving aspect ratio (change to any desired value of IMAGE_SIZE). The dataset will be split into K files (for K-Folds cross validation), and the data is stratified on multiple features from [this notebook](https://www.kaggle.com/mistag/k-folds-cv-stratification-on-multiple-features).","metadata":{}},{"cell_type":"code","source":"# need some additional libraries to process compressed dicom data\n!conda install '../input/pydicom-conda-helper/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '../input/pydicom-conda-helper/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '../input/pydicom-conda-helper/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '../input/pydicom-conda-helper/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '../input/pydicom-conda-helper/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '../input/pydicom-conda-helper/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-05-26T04:57:17.638164Z","iopub.execute_input":"2021-05-26T04:57:17.638839Z","iopub.status.idle":"2021-05-26T04:58:36.905656Z","shell.execute_reply.started":"2021-05-26T04:57:17.638804Z","shell.execute_reply":"2021-05-26T04:58:36.904557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image, ImageFont, ImageDraw\nimport pydicom\nfrom pydicom import dcmread\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport hashlib\nimport os\nimport glob\nfrom io import BytesIO\nimport cv2\nimport contextlib2\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-26T04:58:49.910926Z","iopub.execute_input":"2021-05-26T04:58:49.91131Z","iopub.status.idle":"2021-05-26T04:58:49.92188Z","shell.execute_reply.started":"2021-05-26T04:58:49.911275Z","shell.execute_reply":"2021-05-26T04:58:49.921101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the stratified dataset:","metadata":{}},{"cell_type":"code","source":"df = pd.read_pickle('../input/k-folds-cv-stratification-on-multiple-features/dataset.pkl')\ndf.sample(7)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T04:49:58.348935Z","iopub.execute_input":"2021-05-26T04:49:58.349563Z","iopub.status.idle":"2021-05-26T04:49:58.440992Z","shell.execute_reply.started":"2021-05-26T04:49:58.349526Z","shell.execute_reply":"2021-05-26T04:49:58.440193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions\nThe images are digitized in 12-14bit resolution - converting this to JPEG will cause quite a bit of information to be lost. To preserve all image information, the images could be saved as 16bit PNG. But here we use Contrast Limiting Adaptive Histogram Equalization (CLAHE). This image pre-processing step must then also be used during inference time.","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = 1024 # change this to desired value\nCLIP_LIMIT = 2.\nGRID_SIZE = (8,8)\n\ndef read_image(fname, target_size=IMAGE_SIZE, use_clahe=True):\n    ds = dcmread(fname)\n    data = apply_voi_lut(ds.pixel_array, ds)\n    im = data - np.min(data)\n    im = 255. * im / np.max(im)\n    if ds.PhotometricInterpretation == \"MONOCHROME1\": # check for inverted image\n        im = 255. - im\n    if use_clahe:\n        clahe = cv2.createCLAHE(clipLimit=CLIP_LIMIT, tileGridSize=GRID_SIZE)\n        climg = clahe.apply(im.astype('uint8'))\n        img = Image.fromarray(climg.astype('uint8'), 'L')\n    else:\n        img = Image.fromarray(im.astype('uint8'), 'L')\n    org_size = img.size\n    if max(img.size) > target_size:\n        img.thumbnail((target_size, target_size), Image.ANTIALIAS)\n    return img, org_size","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-26T04:58:54.864708Z","iopub.execute_input":"2021-05-26T04:58:54.865248Z","iopub.status.idle":"2021-05-26T04:58:54.873543Z","shell.execute_reply.started":"2021-05-26T04:58:54.865214Z","shell.execute_reply":"2021-05-26T04:58:54.872278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see what CLAHE does:","metadata":{}},{"cell_type":"code","source":"fname = glob.glob('../input/siim-covid19-detection/train/278fc970196c/**/*.dcm')[0]\nfig = plt.figure(figsize=(20,20))\naxes = fig.add_subplot(1, 2, 1)\nimg, size = read_image(fname, use_clahe=False)\naxes.set_title('Original')\nplt.imshow(img, cmap='gray')\naxes = fig.add_subplot(1, 2, 2)\nimg, size = read_image(fname, use_clahe=True)\naxes.set_title('CLAHE')\nplt.imshow(img, cmap='gray');","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-26T04:58:57.319843Z","iopub.execute_input":"2021-05-26T04:58:57.320523Z","iopub.status.idle":"2021-05-26T04:58:59.100231Z","shell.execute_reply.started":"2021-05-26T04:58:57.320492Z","shell.execute_reply":"2021-05-26T04:58:59.098948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TFRecord format\nThe TFRecords are protocol buffer format. A good overview of the different fields can be [found here](https://github.com/visipedia/tfrecords).","metadata":{}},{"cell_type":"code","source":"TPATH = '../input/siim-covid19-detection/train/'\nLABELS = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\n\ndef create_tf_example(study, label_idx, longest_edge=IMAGE_SIZE, use_clahe=True):  \n    fname = glob.glob(TPATH+study+'/**/*.dcm')[0]\n    filename=fname.split('/')[-1] # exclude path    \n    img, org_size = read_image(fname, target_size=longest_edge, use_clahe=use_clahe)\n    height = img.size[1] # Image height\n    width = img.size[0] # Image width\n    buf= BytesIO()\n    img.save(buf, format= 'JPEG') # encode to jpeg in memory\n    encoded_image_data= buf.getvalue()\n    image_format = b'jpeg'\n    source_id = study\n    # A hash of the image is used in some frameworks\n    key = hashlib.sha256(encoded_image_data).hexdigest()   \n\n    tf_record = tf.train.Example(features=tf.train.Features(feature={\n        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n        'image/channels': tf.train.Feature(int64_list=tf.train.Int64List(value=[1])),\n        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename.encode()])),\n        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[source_id.encode()])),\n        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_image_data])),\n        'image/key/sha256': tf.train.Feature(bytes_list=tf.train.BytesList(value=[key.encode()])),\n        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n        'image/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=[1+label_idx])),\n        'image/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=[LABELS[label_idx].encode()]))\n    }))\n    return tf_record\n\ndef open_sharded_tfrecords(exit_stack, base_path, num_shards):\n    tf_record_output_filenames = [\n        '{}-{:02d}-of-{:02}.tfrecord'.format(base_path, idx, num_shards)\n        for idx in range(num_shards)\n        ]\n    tfrecords = [\n        exit_stack.enter_context(tf.io.TFRecordWriter(file_name))\n        for file_name in tf_record_output_filenames\n    ]\n    return tfrecords","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-26T04:59:04.46129Z","iopub.execute_input":"2021-05-26T04:59:04.462234Z","iopub.status.idle":"2021-05-26T04:59:04.480165Z","shell.execute_reply.started":"2021-05-26T04:59:04.462197Z","shell.execute_reply":"2021-05-26T04:59:04.47869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Stratified TFRecords\nHere we stratify the studies based on the Group column created in [this notebook](https://www.kaggle.com/mistag/k-folds-cv-stratification-on-multiple-features).","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\nK_FOLDS = 5\nIMG_SIZES = [528, 600, 1024] # change list according to needs\n\nfor size in IMG_SIZES:\n    output_filebase='./covid-{}'.format(size)\n    labcnt = np.zeros((K_FOLDS, len(LABELS)), dtype=int)\n    with contextlib2.ExitStack() as tf_record_close_stack:\n        output_tfrecords = open_sharded_tfrecords(tf_record_close_stack, output_filebase, K_FOLDS)\n        idx = 0\n        skf = StratifiedKFold(n_splits=K_FOLDS)\n        for _, test_index in skf.split(df, df.Group):\n            for i in test_index:\n                study = df.Study.iloc[i]\n                for j in range(4):\n                    if df[LABELS[j]].iloc[i] == 1:\n                        label_idx = j\n                labcnt[idx, label_idx] += 1\n                tf_record = create_tf_example(study, label_idx, longest_edge=size)\n                output_tfrecords[idx].write(tf_record.SerializeToString())\n            idx += 1\n        \nldf = pd.DataFrame(labcnt, columns=LABELS)\nldf.to_pickle('fold_stats.pkl') # statistics for later use","metadata":{"execution":{"iopub.status.busy":"2021-05-26T04:59:06.492973Z","iopub.execute_input":"2021-05-26T04:59:06.493487Z","iopub.status.idle":"2021-05-26T04:59:08.039676Z","shell.execute_reply.started":"2021-05-26T04:59:06.493446Z","shell.execute_reply":"2021-05-26T04:59:08.037649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize result\nVerify the result by reading and plotting a few X-rays.","metadata":{}},{"cell_type":"code","source":"dataset = tf.data.TFRecordDataset('./covid-1024-00-of-05.tfrecord')\nfig = plt.figure(figsize=(18, 12))\nidx=1\nfor raw_record in dataset.take(6):\n    axes = fig.add_subplot(2, 3, idx)\n    example = tf.train.Example()\n    example.ParseFromString(raw_record.numpy())\n    classes=example.features.feature['image/class/text'].bytes_list.value[:]\n    study=example.features.feature['image/source_id'].bytes_list.value[:]\n    img_encoded=example.features.feature['image/encoded'].bytes_list.value[0]\n    img = Image.open(BytesIO(img_encoded))\n    #plot_img(img, axes, xmin, ymin, xmax, ymax, classes, class_label, \"\")\n    plt.setp(axes, xticks=[], yticks=[])\n    axes.set_title(study[0].decode()+'\\n'+classes[0].decode())\n    plt.imshow(img, cmap='gray')\n    idx=idx+1","metadata":{"execution":{"iopub.status.busy":"2021-05-25T19:47:35.777144Z","iopub.execute_input":"2021-05-25T19:47:35.777544Z","iopub.status.idle":"2021-05-25T19:47:36.949057Z","shell.execute_reply.started":"2021-05-25T19:47:35.777512Z","shell.execute_reply":"2021-05-25T19:47:36.947904Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}