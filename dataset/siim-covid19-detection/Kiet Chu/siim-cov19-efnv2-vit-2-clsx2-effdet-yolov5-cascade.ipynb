{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Version\n\n* `v6`: \n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch20_colab_uda_v8.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,37,37,37,37,37] 5_tta\n    \n    detection: yolov5x_fold0_nb_v14_best.pt + yolov5x_fold1_nb_v19_best.pt + yolov5x_fold2_nb_v20_best.pt + yolov5x_fold3_nb_v21_best.pt + yolov5x_fold4_nb_v22_best.pt + siimcovid19-effdet/effdet5_640_fold3_epoch20.bin + siimcovid19-effdet/effdet5_640_fold4_epoch24.bin nms_0.5 img_640 + cascade_rcnn_x101_32x4d_fpn_1x_fold0_epoch10_public_nb_v8.pth nms_iou_0.5 wbf iou=0.6 weight=[54,54,54,54,54,58,58,58,58,58,54] skipbox_thr=0.01 tta (1 - none_score) ** 0.5 img_640\n\n* `v5`: \n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch0_colab_v33.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,37,37,37,37,37] 5_tta\n    \n    detection: yolov5x_fold0_nb_v14_best.pt + yolov5x_fold1_nb_v19_best.pt + yolov5x_fold2_nb_v20_best.pt + yolov5x_fold3_nb_v21_best.pt + yolov5x_fold4_nb_v22_best.pt + siimcovid19-effdet/effdet5_640_fold3_epoch20.bin + siimcovid19-effdet/effdet5_640_fold4_epoch24.bin nms_0.5 img_640 + cascade_rcnn_x101_32x4d_fpn_1x_fold0_epoch10_public_nb_v8.pth nms_iou_0.5 wbf iou=0.6 weight=[54,54,54,54,54,58,58,58,58,58,54] skipbox_thr=0.01 tta (1 - none_score) ** 0.5 img_640\n\n* `v4`: \n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch0_colab_v33.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,37,37,37,37,37] 5_tta\n\n* `v3`: \n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch20_colab_uda_v8.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,37,37,37,37,37] 5_tta\n\n* `v2`:\n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch0_colab_v33.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,20,20,20,20,20] 3_tta\n\n* `v1`: \n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch0_colab_v33.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,37,37,37,37,37] 3_tta\n    \n    2-class: tf_efficientnet_v2 + vit_deit_base_patch16_384_fold0_epoch8_colab_v6.pt + vit_deit_base_patch16_384_fold1_epoch8_colab_v7.pt + vit_deit_base_patch16_384_fold2_epoch9_colab_v8.pt weights=[59,59,59,59,59,58,58,58] 3_tta\n    \n    detection: yolov5x_fold0_nb_v14_best.pt + yolov5x_fold1_nb_v19_best.pt + yolov5x_fold2_nb_v20_best.pt + siimcovid19-effdet/effdet5_640_fold3_epoch20.bin + siimcovid19-effdet/effdet5_640_fold4_epoch24.bin nms_0.5 img_640 + cascade_rcnn_x101_32x4d_fpn_1x_fold0_epoch10_public_nb_v8.pth nms_iou_0.5 wbf iou=0.6 weight=[54,54,54,54,54,58,58,58,58,58,54] skipbox_thr=0.01 tta (1 - none_score) ** 0.5 img_640","metadata":{}},{"cell_type":"markdown","source":"### Note:\n- df_2class and sub_df should be merged by id, not sub_df['none'] = df_2class['none'] because there might be a mix of ids","metadata":{}},{"cell_type":"markdown","source":"thanks to https://www.kaggle.com/xhlulu/siim-covid-19-convert-to-jpg-256px  \nthanks to https://www.kaggle.com/awsaf49/vinbigdata-cxr-ad-yolov5-14-class-infer  \ntrain_study: https://www.kaggle.com/h053473666/siim-covid19-efnb7-train-study  \ntrain_image: https://www.kaggle.com/h053473666/siim-cov19-yolov5-train  \ntrain_2class: https://www.kaggle.com/h053473666/siim-covid19-efnb7-train-fold0-5-2class  \n  \nversion1:Original hyperparameters (yolov5)  \nversion4:New hyperparameters (yolov5)\n","metadata":{}},{"cell_type":"code","source":"FINAL_COUNT= 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:22:53.293797Z","iopub.execute_input":"2021-08-09T07:22:53.294158Z","iopub.status.idle":"2021-08-09T07:22:53.304153Z","shell.execute_reply.started":"2021-08-09T07:22:53.294083Z","shell.execute_reply":"2021-08-09T07:22:53.30326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda install '/kaggle/input/pydicom-conda-helper/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y\n\nFINAL_COUNT += 1","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-09T07:22:53.631598Z","iopub.execute_input":"2021-08-09T07:22:53.631912Z","iopub.status.idle":"2021-08-09T07:24:02.185886Z","shell.execute_reply.started":"2021-08-09T07:22:53.631882Z","shell.execute_reply":"2021-08-09T07:24:02.184747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:02.191148Z","iopub.execute_input":"2021-08-09T07:24:02.193455Z","iopub.status.idle":"2021-08-09T07:24:24.472171Z","shell.execute_reply.started":"2021-08-09T07:24:02.19338Z","shell.execute_reply":"2021-08-09T07:24:24.471133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"package_paths = [\n    '../input/weighted-boxes-fusion/Weighted-Boxes-Fusion-master',\n    '../input/pytorchimagemodels/pytorch-image-models-master', #'../input/efficientnet-pytorch-07/efficientnet_pytorch-0.7.0'\n    \"../input/timmeffdetclsv2/timm-efficientdet-pytorch\",\n    \"../input/omegaconf\"\n#     '../input/image-fmix/FMix-master'\n]\nimport sys; \n\nfor pth in package_paths:\n    sys.path.append(pth)\n    \nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:24.474343Z","iopub.execute_input":"2021-08-09T07:24:24.474713Z","iopub.status.idle":"2021-08-09T07:24:24.482219Z","shell.execute_reply.started":"2021-08-09T07:24:24.474672Z","shell.execute_reply":"2021-08-09T07:24:24.481329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom ensemble_boxes import *\nimport matplotlib.pyplot as plt\n\nFINAL_COUNT += 1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T07:24:24.483953Z","iopub.execute_input":"2021-08-09T07:24:24.484374Z","iopub.status.idle":"2021-08-09T07:24:25.396299Z","shell.execute_reply.started":"2021-08-09T07:24:24.484326Z","shell.execute_reply":"2021-08-09T07:24:25.395432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nif df.shape[0] == 2477:\n    fast_sub = True\n    fast_df = pd.DataFrame(([['00086460a852_study', 'negative 1 0 0 1 1'], \n                         ['000c9c05fd14_study', 'negative 1 0 0 1 1'], \n                         ['65761e66de9f_image', 'none 1 0 0 1 1'], \n                         ['51759b5579bc_image', 'none 1 0 0 1 1']]), \n                       columns=['id', 'PredictionString'])\nelse:\n    fast_sub = False\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:25.397648Z","iopub.execute_input":"2021-08-09T07:24:25.397981Z","iopub.status.idle":"2021-08-09T07:24:25.418157Z","shell.execute_reply.started":"2021-08-09T07:24:25.397945Z","shell.execute_reply":"2021-08-09T07:24:25.417431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# .dcm to .png","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:25.419404Z","iopub.execute_input":"2021-08-09T07:24:25.419898Z","iopub.status.idle":"2021-08-09T07:24:25.671966Z","shell.execute_reply.started":"2021-08-09T07:24:25.41985Z","shell.execute_reply":"2021-08-09T07:24:25.670992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:25.673553Z","iopub.execute_input":"2021-08-09T07:24:25.673948Z","iopub.status.idle":"2021-08-09T07:24:25.680532Z","shell.execute_reply.started":"2021-08-09T07:24:25.673895Z","shell.execute_reply":"2021-08-09T07:24:25.679066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = 'test'\nsave_dir = f'/kaggle/tmp/{split}/'\n\nos.makedirs(save_dir, exist_ok=True)\n\nsave_dir = f'/kaggle/tmp/{split}/study/'\nos.makedirs(save_dir, exist_ok=True)\nif fast_sub:\n    xray = read_xray('../input/siim-covid19-detection/train/00086460a852/9e8302230c91/65761e66de9f.dcm')\n    im = resize(xray, size=640)  \n    study = '00086460a852' + '_study.png'\n    im.save(os.path.join(save_dir, study))\n    xray = read_xray('../input/siim-covid19-detection/train/000c9c05fd14/e555410bd2cd/51759b5579bc.dcm')\n    im = resize(xray, size=640)  \n    study = '000c9c05fd14' + '_study.png'\n    im.save(os.path.join(save_dir, study))\nelse:   \n    for dirname, _, filenames in tqdm(os.walk(f'../input/siim-covid19-detection/{split}')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            xray = read_xray(os.path.join(dirname, file))\n            im = resize(xray, size=640)  \n            study = dirname.split('/')[-2] + '_study.png'\n            im.save(os.path.join(save_dir, study))\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:25.684078Z","iopub.execute_input":"2021-08-09T07:24:25.684633Z","iopub.status.idle":"2021-08-09T07:24:27.329763Z","shell.execute_reply.started":"2021-08-09T07:24:25.684598Z","shell.execute_reply":"2021-08-09T07:24:27.328758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_id = []\ndim0 = []\ndim1 = []\nsplits = []\nsave_dir = f'/kaggle/tmp/{split}/image/'\nos.makedirs(save_dir, exist_ok=True)\nif fast_sub:\n    xray = read_xray('../input/siim-covid19-detection/train/00086460a852/9e8302230c91/65761e66de9f.dcm')\n    im = resize(xray, size=640)  \n    im.save(os.path.join(save_dir,'65761e66de9f_image.png'))\n    image_id.append('65761e66de9f.dcm'.replace('.dcm', ''))\n    dim0.append(xray.shape[0])\n    dim1.append(xray.shape[1])\n    splits.append(split)\n    xray = read_xray('../input/siim-covid19-detection/train/000c9c05fd14/e555410bd2cd/51759b5579bc.dcm')\n    im = resize(xray, size=640)  \n    im.save(os.path.join(save_dir, '51759b5579bc_image.png'))\n    image_id.append('51759b5579bc.dcm'.replace('.dcm', ''))\n    dim0.append(xray.shape[0])\n    dim1.append(xray.shape[1])\n    splits.append(split)\nelse:\n    for dirname, _, filenames in tqdm(os.walk(f'../input/siim-covid19-detection/{split}')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            xray = read_xray(os.path.join(dirname, file))\n            im = resize(xray, size=640)  \n            im.save(os.path.join(save_dir, file.replace('.dcm', '_image.png')))\n            image_id.append(file.replace('.dcm', ''))\n            dim0.append(xray.shape[0])\n            dim1.append(xray.shape[1])\n            splits.append(split)\nmeta = pd.DataFrame.from_dict({'image_id': image_id, 'dim0': dim0, 'dim1': dim1, 'split': splits})\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:27.331737Z","iopub.execute_input":"2021-08-09T07:24:27.332087Z","iopub.status.idle":"2021-08-09T07:24:27.829834Z","shell.execute_reply.started":"2021-08-09T07:24:27.332051Z","shell.execute_reply":"2021-08-09T07:24:27.828664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# study predict","metadata":{}},{"cell_type":"code","source":"from glob import glob\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nimport cv2\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport torchvision\nfrom torchvision import transforms\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom  torch.cuda.amp import autocast, GradScaler\n\nimport sklearn\nimport warnings\nimport joblib\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn import metrics\nimport warnings\nimport cv2\nimport pydicom\nimport timm #from efficientnet_pytorch import EfficientNet\nfrom scipy.ndimage.interpolation import zoom\nfrom sklearn.metrics import log_loss\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:27.831695Z","iopub.execute_input":"2021-08-09T07:24:27.832251Z","iopub.status.idle":"2021-08-09T07:24:33.207392Z","shell.execute_reply.started":"2021-08-09T07:24:27.832212Z","shell.execute_reply":"2021-08-09T07:24:33.206336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG = {\n    'fold_num': 5,\n    'seed': 2011,\n    'model_arch': 'tf_efficientnetv2_m_in21k',\n    'vit_model_arch': 'vit_deit_base_patch16_384',\n    'img_size': 640,\n    'epochs': 10,\n    'train_bs': 32,\n    'valid_bs': 32,\n    'lr': 1e-4,\n    'num_workers': 4,\n    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n    'device': 'cuda:0',\n    'tta': 3,\n    'model_paths': ['../input/siimcovid19-pseudo-labeling-classification/tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt',\n                   '../input/siimcovid19-classification-models/tf_efficientnetv2_m_in21k_fold1_epoch20_colab_uda_v8.pt',\n                   '../input/siimcovid19-pseudo-labeling-classification/tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt',\n                   '../input/siimcovid19-pseudo-labeling-classification/tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt',\n                   '../input/siimcovid19-pseudo-labeling-classification/tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt',\n                   '../input/siimcovid19-vision-transformer-model/vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt',\n                   '../input/siimcovid19-vision-transformer-model/vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt',\n                   '../input/siimcovid19-vision-transformer-model/vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt',\n                   '../input/siimcovid19-vision-transformer-model/vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt',\n                   '../input/siimcovid19-vision-transformer-model/vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt'],\n    'num_classes': 4,\n    'weights': [38, 38, 38, 38, 38, 37, 37, 37, 37, 37]\n}\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:33.209113Z","iopub.execute_input":"2021-08-09T07:24:33.209476Z","iopub.status.idle":"2021-08-09T07:24:33.218241Z","shell.execute_reply.started":"2021-08-09T07:24:33.209434Z","shell.execute_reply":"2021-08-09T07:24:33.217286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    #print(im_rgb)\n    return im_rgb\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:33.219757Z","iopub.execute_input":"2021-08-09T07:24:33.220262Z","iopub.status.idle":"2021-08-09T07:24:33.253474Z","shell.execute_reply.started":"2021-08-09T07:24:33.220221Z","shell.execute_reply":"2021-08-09T07:24:33.252554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"class ChestXRayDataset(Dataset):\n    def __init__(\n        self, df, data_root, transforms=None, output_label=True\n    ):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        self.output_label = output_label\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        \n        # get labels\n        if self.output_label:\n            target = self.df.iloc[index]['target']\n        \n        study_id = self.df.loc[index]['image_id']\n        image_path = \"{}/{}.png\".format(self.data_root, study_id)\n        if (image_path.find('.png') == -1):\n            image_path = image_path + '.png'\n        \n        img = get_img(image_path)\n        \n        if self.transforms:\n            img = self.transforms(image=img)['image']\n            \n        # do label smoothing\n        if self.output_label == True:\n            return img, target\n        else:\n            return img\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:33.256581Z","iopub.execute_input":"2021-08-09T07:24:33.256875Z","iopub.status.idle":"2021-08-09T07:24:33.268056Z","shell.execute_reply.started":"2021-08-09T07:24:33.256841Z","shell.execute_reply":"2021-08-09T07:24:33.267111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Train\\Validation Image Augmentations","metadata":{}},{"cell_type":"code","source":"from albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_train_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            #VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            #CoarseDropout(p=0.5),\n            #Cutout(p=0.5),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n  \n        \ndef get_valid_transforms():\n    return Compose([\n            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\ndef get_inference_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size'], scale=(0.8, 1.0)),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\n# def get_inference_transforms():\n#     return Compose([\n#             Resize(CFG['img_size'], CFG['img_size']),\n#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n#             ToTensorV2(p=1.0),\n#         ], p=1.)\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:33.269641Z","iopub.execute_input":"2021-08-09T07:24:33.270096Z","iopub.status.idle":"2021-08-09T07:24:34.036683Z","shell.execute_reply.started":"2021-08-09T07:24:33.269954Z","shell.execute_reply":"2021-08-09T07:24:34.035769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class ChestXrayImgClassifier(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        # tf_efficientnet_b6_ns\n        self.model = timm.create_model(model_arch, pretrained=pretrained, drop_rate=0.5, drop_path_rate=0.2)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(n_features, n_class, bias=True)\n        )\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:34.040973Z","iopub.execute_input":"2021-08-09T07:24:34.043014Z","iopub.status.idle":"2021-08-09T07:24:34.052505Z","shell.execute_reply.started":"2021-08-09T07:24:34.042972Z","shell.execute_reply":"2021-08-09T07:24:34.051755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChestXrayImgClassifierVIT(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        # vit_base_patch16\n        self.model = timm.create_model(model_arch, pretrained=pretrained, img_size=640)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(n_features, n_class, bias=True)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:34.056676Z","iopub.execute_input":"2021-08-09T07:24:34.059213Z","iopub.status.idle":"2021-08-09T07:24:34.068744Z","shell.execute_reply.started":"2021-08-09T07:24:34.059172Z","shell.execute_reply":"2021-08-09T07:24:34.067795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main Loop","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nif fast_sub:\n    df = fast_df.copy()\nelse:\n    df = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nid_laststr_list  = []\nfor i in range(df.shape[0]):\n    id_laststr_list.append(df.loc[i,'id'][-1])\ndf['id_last_str'] = id_laststr_list\n\nstudy_len = df[df['id_last_str'] == 'y'].shape[0]\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:34.072901Z","iopub.execute_input":"2021-08-09T07:24:34.075339Z","iopub.status.idle":"2021-08-09T07:24:34.094095Z","shell.execute_reply.started":"2021-08-09T07:24:34.075299Z","shell.execute_reply":"2021-08-09T07:24:34.093121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FINAL_COUNT += 1\ndf","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:34.098331Z","iopub.execute_input":"2021-08-09T07:24:34.100347Z","iopub.status.idle":"2021-08-09T07:24:34.121269Z","shell.execute_reply.started":"2021-08-09T07:24:34.100307Z","shell.execute_reply":"2021-08-09T07:24:34.120514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_lst = df[df['id_last_str'] == 'y']['id'].values\n\nFINAL_COUNT += 1\nstudy_lst","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:34.125062Z","iopub.execute_input":"2021-08-09T07:24:34.127037Z","iopub.status.idle":"2021-08-09T07:24:34.13778Z","shell.execute_reply.started":"2021-08-09T07:24:34.126998Z","shell.execute_reply":"2021-08-09T07:24:34.136868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        image_preds = model(imgs)   #output = model(input)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:34.14194Z","iopub.execute_input":"2021-08-09T07:24:34.14401Z","iopub.status.idle":"2021-08-09T07:24:34.152802Z","shell.execute_reply.started":"2021-08-09T07:24:34.143972Z","shell.execute_reply":"2021-08-09T07:24:34.152002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold = 0\ntest_dataset_dir = f'/kaggle/tmp/{split}/study'\n # for training only, need nightly build pytorch\n\nseed_everything(CFG['seed'])\n\nprint('Inference fold {} started'.format(fold))\n\ntest = pd.DataFrame()\ntest['image_id'] = study_lst\ntest_ds = ChestXRayDataset(test, test_dataset_dir, transforms=get_inference_transforms(), output_label=False)\n\ntst_loader = torch.utils.data.DataLoader(\n    test_ds, \n    batch_size=CFG['valid_bs'],\n    num_workers=CFG['num_workers'],\n    shuffle=False,\n    pin_memory=False,\n)\n\ndevice = torch.device(CFG['device'])\nmodel = ChestXrayImgClassifier(CFG['model_arch'], CFG['num_classes']).to(device)\n\ntst_preds = []\n\nfor i, model_path in enumerate(CFG['model_paths']):\n    if i > 4:\n        break\n    model.load_state_dict(torch.load(model_path))\n\n    with torch.no_grad():\n        for _ in range(CFG['tta']):\n            infer_one = inference_one_epoch(model, tst_loader, device)\n    #         print(infer_one)\n            tst_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*infer_one]\n\n# print('tst_preds')\n# print(tst_preds[:10])\n# print(len(tst_preds))\n# tst_preds = np.sum(tst_preds, axis=0)\n# print(tst_preds[:10])\n# print(len(tst_preds))\n\ndel model\ntorch.cuda.empty_cache()\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:24:34.157141Z","iopub.execute_input":"2021-08-09T07:24:34.159467Z","iopub.status.idle":"2021-08-09T07:25:05.270538Z","shell.execute_reply.started":"2021-08-09T07:24:34.15943Z","shell.execute_reply":"2021-08-09T07:25:05.269483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_vit = ChestXrayImgClassifierVIT(CFG['vit_model_arch'], CFG['num_classes']).to(device)\nVIT_model_paths = CFG['model_paths'][5:]\n\nfor i, model_path in enumerate(VIT_model_paths):\n    model_vit.load_state_dict(torch.load(model_path))\n\n    with torch.no_grad():\n        for _ in range(CFG['tta']):\n            infer_one = inference_one_epoch(model_vit, tst_loader, device)\n    #         print(infer_one)\n            tst_preds += [CFG['weights'][i+5]/sum(CFG['weights'])/CFG['tta']*infer_one]\n\ntst_preds = np.sum(tst_preds, axis=0)\ndel model_vit\ntorch.cuda.empty_cache()\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:05.272379Z","iopub.execute_input":"2021-08-09T07:25:05.272768Z","iopub.status.idle":"2021-08-09T07:25:28.636717Z","shell.execute_reply.started":"2021-08-09T07:25:05.272726Z","shell.execute_reply":"2021-08-09T07:25:28.635701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(tst_preds))\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.641894Z","iopub.execute_input":"2021-08-09T07:25:28.642156Z","iopub.status.idle":"2021-08-09T07:25:28.646602Z","shell.execute_reply.started":"2021-08-09T07:25:28.642128Z","shell.execute_reply":"2021-08-09T07:25:28.645682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FINAL_COUNT += 1\ntst_preds[:10]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.650114Z","iopub.execute_input":"2021-08-09T07:25:28.650556Z","iopub.status.idle":"2021-08-09T07:25:28.659489Z","shell.execute_reply.started":"2021-08-09T07:25:28.650475Z","shell.execute_reply":"2021-08-09T07:25:28.658247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"debug = False\nclass_labels = ['0', '1', '2', '3']\n\ntest.loc[:99 if debug else test.shape[0], class_labels] = tst_preds\n\nFINAL_COUNT += 1\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.661261Z","iopub.execute_input":"2021-08-09T07:25:28.661898Z","iopub.status.idle":"2021-08-09T07:25:28.685775Z","shell.execute_reply.started":"2021-08-09T07:25:28.661792Z","shell.execute_reply":"2021-08-09T07:25:28.684619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# study string","metadata":{}},{"cell_type":"code","source":"name2label = { \n    'negative': 0,\n    'indeterminate': 1,\n    'typical': 2,\n    'atypical': 3}\nlabel2name  = {v:k for k, v in name2label.items()}\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.687546Z","iopub.execute_input":"2021-08-09T07:25:28.68799Z","iopub.status.idle":"2021-08-09T07:25:28.69396Z","shell.execute_reply.started":"2021-08-09T07:25:28.687946Z","shell.execute_reply":"2021-08-09T07:25:28.692708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_PredictionString(row, thr=0):\n    string = ''\n    for idx in range(4):\n        conf =  row[str(idx)]\n        if conf>thr:\n            string+=f'{label2name[idx]} {conf:0.3f} 0 0 1 1 '\n    string = string.strip()\n    return string\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.695793Z","iopub.execute_input":"2021-08-09T07:25:28.696413Z","iopub.status.idle":"2021-08-09T07:25:28.705248Z","shell.execute_reply.started":"2021-08-09T07:25:28.696337Z","shell.execute_reply":"2021-08-09T07:25:28.703986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['PredictionString'] = test.apply(get_PredictionString, axis=1)\ntest = test.drop(class_labels, axis=1)\ntest.rename(columns={'image_id':'id'}, inplace=True)\n\nFINAL_COUNT += 1\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.706716Z","iopub.execute_input":"2021-08-09T07:25:28.707163Z","iopub.status.idle":"2021-08-09T07:25:28.729041Z","shell.execute_reply.started":"2021-08-09T07:25:28.707105Z","shell.execute_reply":"2021-08-09T07:25:28.728287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_study = test\n\nFINAL_COUNT += 1\ndf_study","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.730102Z","iopub.execute_input":"2021-08-09T07:25:28.730466Z","iopub.status.idle":"2021-08-09T07:25:28.741652Z","shell.execute_reply.started":"2021-08-09T07:25:28.730427Z","shell.execute_reply":"2021-08-09T07:25:28.740501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 class","metadata":{}},{"cell_type":"code","source":"package_paths = [\n    '../input/pytorchimagemodels/pytorch-image-models-master', #'../input/efficientnet-pytorch-07/efficientnet_pytorch-0.7.0'\n#     '../input/image-fmix/FMix-master'\n]\nimport sys; \n\nfor pth in package_paths:\n    sys.path.append(pth)\n    \nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.743319Z","iopub.execute_input":"2021-08-09T07:25:28.743758Z","iopub.status.idle":"2021-08-09T07:25:28.750714Z","shell.execute_reply.started":"2021-08-09T07:25:28.743724Z","shell.execute_reply":"2021-08-09T07:25:28.749804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from glob import glob\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nimport cv2\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport torchvision\nfrom torchvision import transforms\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom  torch.cuda.amp import autocast, GradScaler\n\nimport sklearn\nimport warnings\nimport joblib\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn import metrics\nimport warnings\nimport cv2\nimport pydicom\nimport timm #from efficientnet_pytorch import EfficientNet\nfrom scipy.ndimage.interpolation import zoom\nfrom sklearn.metrics import log_loss\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.752571Z","iopub.execute_input":"2021-08-09T07:25:28.752843Z","iopub.status.idle":"2021-08-09T07:25:28.763557Z","shell.execute_reply.started":"2021-08-09T07:25:28.752819Z","shell.execute_reply":"2021-08-09T07:25:28.76268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG = {\n    'fold_num': 5,\n    'seed': 2021,\n    'model_arch': 'tf_efficientnetv2_m_in21k',\n    'vit_model_arch': 'vit_deit_base_patch16_384',\n    'img_size': 640,\n    'epochs': 10,\n    'train_bs': 32,\n    'valid_bs': 32,\n    'lr': 1e-4,\n    'num_workers': 4,\n    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n    'device': 'cuda:0',\n    'tta': 3,\n    'model_paths': ['../input/siimcovid192classes-classifier/tf_efficientnetv2_m_in21k_fold0_epoch1_colab_v1.pt',\n                   '../input/siimcovid192classes-classifier/tf_efficientnetv2_m_in21k_fold1_epoch20_colab_v2.pt',\n                   '../input/siimcovid192classes-classifier/tf_efficientnetv2_m_in21k_fold2_epoch14_colab_v3.pt',\n                   '../input/siimcovid192classes-classifier/tf_efficientnetv2_m_in21k_fold3_epoch13_colab_v4.pt',\n                   '../input/siimcovid192classes-classifier/tf_efficientnetv2_m_in21k_fold4_epoch5_colab_v5.pt',\n                   '../input/siimcovid192classes-classifier/vit_deit_base_patch16_384_fold0_epoch8_colab_v6.pt',\n                   '../input/siimcovid192classes-classifier/vit_deit_base_patch16_384_fold1_epoch8_colab_v7.pt',\n                   '../input/siimcovid192classes-classifier/vit_deit_base_patch16_384_fold2_epoch9_colab_v8.pt'],\n    'num_classes': 2,\n    'weights': [59, 59, 59, 59, 59, 58, 58, 58]\n}\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.765097Z","iopub.execute_input":"2021-08-09T07:25:28.765617Z","iopub.status.idle":"2021-08-09T07:25:28.776331Z","shell.execute_reply.started":"2021-08-09T07:25:28.765565Z","shell.execute_reply":"2021-08-09T07:25:28.775272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    return im_rgb\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.777841Z","iopub.execute_input":"2021-08-09T07:25:28.778235Z","iopub.status.idle":"2021-08-09T07:25:28.789862Z","shell.execute_reply.started":"2021-08-09T07:25:28.778196Z","shell.execute_reply":"2021-08-09T07:25:28.788814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_train_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            #VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            #CoarseDropout(p=0.5),\n            #Cutout(p=0.5),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n  \n        \ndef get_valid_transforms():\n    return Compose([\n            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\ndef get_inference_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size'], scale=(0.8, 1.0)),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\n# def get_inference_transforms():\n#     return Compose([\n#             Resize(CFG['img_size'], CFG['img_size']),\n#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n#             ToTensorV2(p=1.0),\n#         ], p=1.)\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.791278Z","iopub.execute_input":"2021-08-09T07:25:28.791724Z","iopub.status.idle":"2021-08-09T07:25:28.810444Z","shell.execute_reply.started":"2021-08-09T07:25:28.791682Z","shell.execute_reply":"2021-08-09T07:25:28.809518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChestXrayImgClassifier(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        # tf_efficientnet_b6_ns\n        self.model = timm.create_model(model_arch, pretrained=pretrained, drop_rate=0.3, drop_path_rate=0.2)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(n_features, n_class, bias=True)\n        )\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.81397Z","iopub.execute_input":"2021-08-09T07:25:28.814288Z","iopub.status.idle":"2021-08-09T07:25:28.823261Z","shell.execute_reply.started":"2021-08-09T07:25:28.814262Z","shell.execute_reply":"2021-08-09T07:25:28.822303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        image_preds = model(imgs)   #output = model(input)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.824793Z","iopub.execute_input":"2021-08-09T07:25:28.825249Z","iopub.status.idle":"2021-08-09T07:25:28.832424Z","shell.execute_reply.started":"2021-08-09T07:25:28.82521Z","shell.execute_reply":"2021-08-09T07:25:28.831381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChestXRayDataset(Dataset):\n    def __init__(\n        self, df, data_root, transforms=None, output_label=True\n    ):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        self.output_label = output_label\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        \n        # get labels\n        if self.output_label:\n            target = self.df.iloc[index]['target']\n        \n        image_id = self.df.loc[index]['id']\n        image_path = \"{}/{}\".format(self.data_root, image_id)\n        if (image_path.find('.png') == -1):\n            image_path = image_path + '.png'\n        img = get_img(image_path)\n        \n        if self.transforms:\n            img = self.transforms(image=img)['image']\n            \n        # do label smoothing\n        if self.output_label == True:\n            return img, target\n        else:\n            return img\n        \n        \nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.833943Z","iopub.execute_input":"2021-08-09T07:25:28.834387Z","iopub.status.idle":"2021-08-09T07:25:28.845148Z","shell.execute_reply.started":"2021-08-09T07:25:28.834339Z","shell.execute_reply":"2021-08-09T07:25:28.843923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if fast_sub:\n    sub_df = fast_df.copy()\nelse:\n    sub_df = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nsub_df = sub_df[study_len:]\ntest_paths = f'/kaggle/tmp/{split}/image/' + sub_df['id'] +'.png'\nsub_df['none'] = 0\n\nFINAL_COUNT += 1\nsub_df","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.846302Z","iopub.execute_input":"2021-08-09T07:25:28.846729Z","iopub.status.idle":"2021-08-09T07:25:28.875928Z","shell.execute_reply.started":"2021-08-09T07:25:28.846653Z","shell.execute_reply":"2021-08-09T07:25:28.875074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold = 0\ntest_dataset_dir = f'/kaggle/tmp/{split}/image'\n # for training only, need nightly build pytorch\n\nseed_everything(CFG['seed'])\n\nprint('Inference fold {} started'.format(fold))\n\ntest = pd.DataFrame()\ntest_ds = ChestXRayDataset(sub_df, test_dataset_dir, transforms=get_inference_transforms(), output_label=False)\n\ntst_loader = torch.utils.data.DataLoader(\n    test_ds, \n    batch_size=CFG['valid_bs'],\n    num_workers=CFG['num_workers'],\n    shuffle=False,\n    pin_memory=False,\n)\n\ndevice = torch.device(CFG['device'])\nmodel = ChestXrayImgClassifier(CFG['model_arch'], CFG['num_classes']).to(device)\n\ntst_preds = []\n\nfor i, model_path in enumerate(CFG['model_paths'][0:5]):\n    model.load_state_dict(torch.load(model_path))\n\n    with torch.no_grad():\n        for _ in range(CFG['tta']):\n            infer_one = inference_one_epoch(model, tst_loader, device)\n    #         print(infer_one)\n            tst_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*infer_one]\n\n# tst_preds = np.sum(tst_preds, axis=0)\n\ndel model\ntorch.cuda.empty_cache()\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:28.878301Z","iopub.execute_input":"2021-08-09T07:25:28.878856Z","iopub.status.idle":"2021-08-09T07:25:47.594284Z","shell.execute_reply.started":"2021-08-09T07:25:28.878815Z","shell.execute_reply":"2021-08-09T07:25:47.59327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_vit = ChestXrayImgClassifierVIT(CFG['vit_model_arch'], CFG['num_classes']).to(device)\nVIT_model_paths = CFG['model_paths'][5:]\n\nfor i, model_path in enumerate(VIT_model_paths):\n    model_vit.load_state_dict(torch.load(model_path))\n\n    with torch.no_grad():\n        for _ in range(CFG['tta']):\n            infer_one = inference_one_epoch(model_vit, tst_loader, device)\n    #         print(infer_one)\n            tst_preds += [CFG['weights'][i+5]/sum(CFG['weights'])/CFG['tta']*infer_one]\n\ntst_preds = np.sum(tst_preds, axis=0)\ndel model_vit\ntorch.cuda.empty_cache()\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:25:47.595974Z","iopub.execute_input":"2021-08-09T07:25:47.59637Z","iopub.status.idle":"2021-08-09T07:26:01.927599Z","shell.execute_reply.started":"2021-08-09T07:25:47.596314Z","shell.execute_reply":"2021-08-09T07:26:01.926611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FINAL_COUNT += 1\ntst_preds","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:01.929308Z","iopub.execute_input":"2021-08-09T07:26:01.929673Z","iopub.status.idle":"2021-08-09T07:26:01.936972Z","shell.execute_reply.started":"2021-08-09T07:26:01.929634Z","shell.execute_reply":"2021-08-09T07:26:01.93604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"debug = False\nclass_labels = ['none']\n\nsub_df[class_labels] = tst_preds[:, 1]\n\nFINAL_COUNT += 1\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:01.93833Z","iopub.execute_input":"2021-08-09T07:26:01.938683Z","iopub.status.idle":"2021-08-09T07:26:01.955934Z","shell.execute_reply.started":"2021-08-09T07:26:01.938647Z","shell.execute_reply":"2021-08-09T07:26:01.954951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_2class = sub_df.reset_index(drop=True)\n\nFINAL_COUNT += 1\ndf_2class.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:01.957256Z","iopub.execute_input":"2021-08-09T07:26:01.957663Z","iopub.status.idle":"2021-08-09T07:26:01.97051Z","shell.execute_reply.started":"2021-08-09T07:26:01.957622Z","shell.execute_reply":"2021-08-09T07:26:01.969551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Effdet predict","metadata":{}},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:01.971945Z","iopub.execute_input":"2021-08-09T07:26:01.972283Z","iopub.status.idle":"2021-08-09T07:26:02.132047Z","shell.execute_reply.started":"2021-08-09T07:26:01.972246Z","shell.execute_reply":"2021-08-09T07:26:02.13121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.135305Z","iopub.execute_input":"2021-08-09T07:26:02.135579Z","iopub.status.idle":"2021-08-09T07:26:02.184207Z","shell.execute_reply.started":"2021-08-09T07:26:02.135554Z","shell.execute_reply":"2021-08-09T07:26:02.183446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ensemble_boxes import *\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nimport gc\n\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.187491Z","iopub.execute_input":"2021-08-09T07:26:02.187812Z","iopub.status.idle":"2021-08-09T07:26:02.243051Z","shell.execute_reply.started":"2021-08-09T07:26:02.187783Z","shell.execute_reply":"2021-08-09T07:26:02.241983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_copy = meta.copy()\n\nFINAL_COUNT += 1\nmeta_copy","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.244528Z","iopub.execute_input":"2021-08-09T07:26:02.24499Z","iopub.status.idle":"2021-08-09T07:26:02.301147Z","shell.execute_reply.started":"2021-08-09T07:26:02.244941Z","shell.execute_reply":"2021-08-09T07:26:02.300197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta = meta_copy.copy()\nmeta = meta[meta['split'] == 'test']\nif fast_sub:\n    test_df = fast_df.copy()\nelse:\n    test_df = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\ntest_df = df[study_len:].reset_index(drop=True) \nmeta['image_id'] = meta['image_id'] + '_image'\nmeta.columns = ['id', 'dim0', 'dim1', 'split']\n# print(test_df)\ntest_df = pd.merge(test_df, meta, on = 'id', how = 'left')\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.302551Z","iopub.execute_input":"2021-08-09T07:26:02.30295Z","iopub.status.idle":"2021-08-09T07:26:02.359876Z","shell.execute_reply.started":"2021-08-09T07:26:02.302914Z","shell.execute_reply":"2021-08-09T07:26:02.358958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FINAL_COUNT += 1\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.361429Z","iopub.execute_input":"2021-08-09T07:26:02.36192Z","iopub.status.idle":"2021-08-09T07:26:02.415071Z","shell.execute_reply.started":"2021-08-09T07:26:02.361882Z","shell.execute_reply":"2021-08-09T07:26:02.414035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_valid_transforms_effdet():\n    return A.Compose(\n        [\n            A.Resize(height=640, width=640, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.416726Z","iopub.execute_input":"2021-08-09T07:26:02.417178Z","iopub.status.idle":"2021-08-09T07:26:02.463887Z","shell.execute_reply.started":"2021-08-09T07:26:02.417141Z","shell.execute_reply":"2021-08-09T07:26:02.462954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_ROOT_PATH = f'/kaggle/tmp/{split}/image/'\n\nclass DatasetRetriever(Dataset):\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.png', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.46512Z","iopub.execute_input":"2021-08-09T07:26:02.465418Z","iopub.status.idle":"2021-08-09T07:26:02.514739Z","shell.execute_reply.started":"2021-08-09T07:26:02.465376Z","shell.execute_reply":"2021-08-09T07:26:02.513844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_dataset = DatasetRetriever(\n    image_ids=meta[\"id\"].values,\n    transforms=get_valid_transforms_effdet()\n)\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.515815Z","iopub.execute_input":"2021-08-09T07:26:02.516381Z","iopub.status.idle":"2021-08-09T07:26:02.563152Z","shell.execute_reply.started":"2021-08-09T07:26:02.516319Z","shell.execute_reply":"2021-08-09T07:26:02.562137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSKIP_BOX_THR = 0.001\nIMAGE_ID_TO_SIZE = {}\nfor i in range(len(meta)):\n    IMAGE_ID_TO_SIZE[meta.at[i, \"id\"]] = {\"width\": meta.at[i, \"dim1\"], \"height\": meta.at[i, \"dim0\"]}\n    \ndef get_boxes_classes_from_preds(preds, score_threshold=SKIP_BOX_THR):\n    det = preds\n    predictions = []\n    for i in range(len(det)):\n        boxes = det[i].detach().cpu().numpy()[:,:4]    \n        scores = det[i].detach().cpu().numpy()[:,4]\n        indexes = np.where(scores > score_threshold)[0]\n        boxes = boxes[indexes]\n        boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n        predictions.append({\n            'boxes': boxes[indexes],\n            'scores': scores[indexes],\n        })\n    return [predictions]\n\n\ndef run_wbf(predictions, image_index, image_size=512, iou_thr=0.44, skip_box_thr=SKIP_BOX_THR, weights=None):\n    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels\n\ndef run_nms(predictions, image_index, image_size=640, iou_thr=0.5, weights=None):\n    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n    boxes, scores, labels = nms(boxes, scores, labels, weights=None, iou_thr=iou_thr)\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels\n\ndef box_label_conf_to_pred_str(boxes, labels, confs):\n    pred_str = []\n    for i in range(len(boxes)):\n        x1, y1, x2, y2 = boxes[i]\n        pred_str.append(f\"{labels[i]} {confs[i]} {x1} {y1} {x2} {y2}\")\n    pred_str = \" \".join(pred_str)\n    return pred_str\n\ndef box_to_original_size(boxes, image_id, image_id_to_size, imsize=512):\n    boxes = boxes.copy()\n    for i, box in enumerate(boxes):\n        w = image_id_to_size[image_id][\"width\"]\n        h = image_id_to_size[image_id][\"height\"]\n        box[[0,2]] = box[[0,2]] / imsize * w\n        box[[1,3]] = box[[1,3]] / imsize * h\n        boxes[i] = box\n    return boxes    \n\nclass Fitter:\n    def __init__(self, model, device, config):\n        self.config = config\n\n        self.base_dir = f'./{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n        self.log_path = f'{self.base_dir}/log.txt'\n\n        self.model = model\n        self.device = device\n\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def infer(self, val_loader, iou_threshold, visualize=False):\n        print(\"iou threshold:\", iou_threshold)\n        self.log(f\"iou threshold: {iou_threshold}\")\n        self.model.eval()\n        t = time.time()\n        results = []\n        for step, (images, image_ids) in tqdm(enumerate(val_loader)):            \n            with torch.no_grad():\n                images = torch.stack(images)\n                batch_size = images.shape[0]\n                images = images.to(self.device).float()\n\n                pred = self.model(images, torch.tensor([1]*images.shape[0]).float().cuda())\n                \n                predictions = get_boxes_classes_from_preds(pred)\n                for i, image in enumerate(images):\n#                     boxes, scores, labels = run_wbf(predictions, image_index=i, iou_thr=iou_threshold)\n                    boxes, scores, labels = run_nms(predictions, image_index=i, iou_thr=iou_threshold)\n                    boxes = boxes.astype(np.int32).clip(min=0, max=640)\n                    image_id = image_ids[i]\n                    text_labels = [\"opacity\"] * len(boxes)\n\n                    boxes_ori_size = box_to_original_size(boxes, image_id, IMAGE_ID_TO_SIZE, imsize=640)\n                    result = {\n                        'id': image_id,\n                        'PredictionString': box_label_conf_to_pred_str(boxes_ori_size, text_labels, scores)\n                    }\n                    results.append(result)\n                    \n\n                    #visualize\n                    if visualize:\n                      if i==0 and np.random.uniform() < 0.02:\n                          sample = images[i].permute(1,2,0).cpu().numpy()\n                          sample = np.ascontiguousarray(sample)\n                    \n                          fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n                          for score, box in zip(scores, boxes):\n                              sample[int(box[1]):int(box[3]), int(box[0]):int(box[2]), 1] += score\n\n                          for score, box in zip(scores, boxes):\n                              sample = cv2.rectangle(sample, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (1, 0, 0, min(score*3,1)), 2)\n                              \n                          ax.set_axis_off()\n                          ax.imshow(sample)\n                          plt.show()\n                          print(result)\n        \n        result_df = pd.DataFrame(results, columns=['id', 'PredictionString'])\n        os.makedirs('/kaggle/working/effdet_results', exist_ok=True)\n        result_file = f'/kaggle/working/effdet_results/result.csv'\n        result_df.to_csv(result_file, index=False)\n        return result_df\n\n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')\n            \nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.566988Z","iopub.execute_input":"2021-08-09T07:26:02.567294Z","iopub.status.idle":"2021-08-09T07:26:02.643406Z","shell.execute_reply.started":"2021-08-09T07:26:02.567267Z","shell.execute_reply":"2021-08-09T07:26:02.642386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainGlobalConfig:\n    num_workers = 2\n    batch_size = 4\n    folder = 'effdet5-cutmix-augmix'\n    verbose = True\n    verbose_step = 10\n    \nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.647252Z","iopub.execute_input":"2021-08-09T07:26:02.647591Z","iopub.status.idle":"2021-08-09T07:26:02.694126Z","shell.execute_reply.started":"2021-08-09T07:26:02.647544Z","shell.execute_reply":"2021-08-09T07:26:02.693198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef run_inference():\n    device = torch.device('cuda:0')\n    net.to(device)\n\n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n        collate_fn=collate_fn,\n    )\n\n    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n    result_df = fitter.infer(val_loader, iou_threshold=0.5, visualize=True)\n    return result_df\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.695683Z","iopub.execute_input":"2021-08-09T07:26:02.696085Z","iopub.status.idle":"2021-08-09T07:26:02.743834Z","shell.execute_reply.started":"2021-08-09T07:26:02.696046Z","shell.execute_reply":"2021-08-09T07:26:02.742902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchEval\nfrom effdet.efficientdet import HeadNet\n\ndef load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size=640\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.745329Z","iopub.execute_input":"2021-08-09T07:26:02.745687Z","iopub.status.idle":"2021-08-09T07:26:02.984271Z","shell.execute_reply.started":"2021-08-09T07:26:02.745652Z","shell.execute_reply":"2021-08-09T07:26:02.983445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# net = load_net('/kaggle/input/siimcovid19-effdet/effdet5_640_fold0_epoch20.bin')\n# effdet_fold0 = run_inference()\n# effdet_fold0","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:02.98558Z","iopub.execute_input":"2021-08-09T07:26:02.985916Z","iopub.status.idle":"2021-08-09T07:26:03.032157Z","shell.execute_reply.started":"2021-08-09T07:26:02.98588Z","shell.execute_reply":"2021-08-09T07:26:03.030978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# net = load_net('/kaggle/input/siimcovid19-effdet/effdet5_640_fold1_epoch21.bin')\n# effdet_fold1 = run_inference()\n# effdet_fold1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:03.035967Z","iopub.execute_input":"2021-08-09T07:26:03.036436Z","iopub.status.idle":"2021-08-09T07:26:03.084323Z","shell.execute_reply.started":"2021-08-09T07:26:03.036399Z","shell.execute_reply":"2021-08-09T07:26:03.08334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# net = load_net('/kaggle/input/siimcovid19-effdet/effdet5_640_fold2_epoch24.bin')\n# effdet_fold2 = run_inference()\n# effdet_fold2","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:03.085822Z","iopub.execute_input":"2021-08-09T07:26:03.086202Z","iopub.status.idle":"2021-08-09T07:26:03.132759Z","shell.execute_reply.started":"2021-08-09T07:26:03.086164Z","shell.execute_reply":"2021-08-09T07:26:03.131916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = load_net('/kaggle/input/siimcovid19-effdet/effdet5_640_fold3_epoch20.bin')\neffdet_fold3 = run_inference()\neffdet_fold3","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:03.134289Z","iopub.execute_input":"2021-08-09T07:26:03.134723Z","iopub.status.idle":"2021-08-09T07:26:14.123807Z","shell.execute_reply.started":"2021-08-09T07:26:03.134684Z","shell.execute_reply":"2021-08-09T07:26:14.122999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = load_net('/kaggle/input/siimcovid19-effdet/effdet5_640_fold4_epoch24.bin')\neffdet_fold4 = run_inference()\neffdet_fold4\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:14.126588Z","iopub.execute_input":"2021-08-09T07:26:14.126844Z","iopub.status.idle":"2021-08-09T07:26:22.937752Z","shell.execute_reply.started":"2021-08-09T07:26:14.126818Z","shell.execute_reply":"2021-08-09T07:26:22.934776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# yolov5 predict","metadata":{}},{"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\nimport torch\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:22.939441Z","iopub.execute_input":"2021-08-09T07:26:22.939795Z","iopub.status.idle":"2021-08-09T07:26:23.058283Z","shell.execute_reply.started":"2021-08-09T07:26:22.939755Z","shell.execute_reply":"2021-08-09T07:26:23.05731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FINAL_COUNT += 1\nmeta","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:23.061782Z","iopub.execute_input":"2021-08-09T07:26:23.06208Z","iopub.status.idle":"2021-08-09T07:26:23.119816Z","shell.execute_reply.started":"2021-08-09T07:26:23.062049Z","shell.execute_reply":"2021-08-09T07:26:23.118916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta = meta[meta['split'] == 'test']\nif fast_sub:\n    test_df = fast_df.copy()\nelse:\n    test_df = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\ntest_df = df[study_len:].reset_index(drop=True) \n# meta['image_id'] = meta['image_id'] + '_image'\n# meta.columns = ['id', 'dim0', 'dim1', 'split']\ntest_df = pd.merge(test_df, meta, on = 'id', how = 'left')\ntest_df.head()\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:23.122332Z","iopub.execute_input":"2021-08-09T07:26:23.123222Z","iopub.status.idle":"2021-08-09T07:26:23.175824Z","shell.execute_reply.started":"2021-08-09T07:26:23.123176Z","shell.execute_reply":"2021-08-09T07:26:23.174856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5')\nos.chdir('/kaggle/working/yolov5') # install dependencies\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:26:23.177303Z","iopub.execute_input":"2021-08-09T07:26:23.177788Z","iopub.status.idle":"2021-08-09T07:26:23.603752Z","shell.execute_reply.started":"2021-08-09T07:26:23.177744Z","shell.execute_reply":"2021-08-09T07:26:23.60278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## yolov5 fold 0","metadata":{}},{"cell_type":"code","source":"dim = 640 #1024, 256, 'original'\ntest_dir = f'/kaggle/tmp/{split}/image'\nweights_dir = '/kaggle/input/siimcovid19-detection-models/yolov5x_fold0_nb_v14_best.pt'\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python detect.py --weights $weights_dir\\\n--img 640\\\n--conf 0.001\\\n--iou 0.5\\\n--source $test_dir\\\n--save-txt --save-conf --augment\n\n\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\n\nimage_ids = [] # (num_images, )\nPredictionStrings = [] # (num_images, boxes_lst)\n\nfor file_path in tqdm(glob('runs/detect/exp/labels/*.txt')):\n    image_id = file_path.split('/')[-1].split('.')[0]\n    w, h = test_df.loc[test_df.id==image_id,['dim1', 'dim0']].values[0]\n    f = open(file_path, 'r')\n    data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n    data = data[:, [0, 5, 1, 2, 3, 4]]\n    bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n    new_bboxes = []\n    for idx in range(len(bboxes)):\n        if idx%6 == 0:\n            new_bboxes.append('opacity')\n        else :\n            new_bboxes.append(bboxes[idx])\n    image_ids.append(image_id)\n    PredictionStrings.append(' '.join(new_bboxes))\n\n\npred_yolov5_fold0_df = pd.DataFrame({'id':image_ids,\n                        'PredictionString':PredictionStrings})\n\nFINAL_COUNT += 1\npred_yolov5_fold0_df.head()","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-09T07:26:23.605175Z","iopub.execute_input":"2021-08-09T07:26:23.605939Z","iopub.status.idle":"2021-08-09T07:26:34.306819Z","shell.execute_reply.started":"2021-08-09T07:26:23.605903Z","shell.execute_reply":"2021-08-09T07:26:34.305803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## yolov5 fold 1","metadata":{}},{"cell_type":"code","source":"dim = 640 #1024, 256, 'original'\ntest_dir = f'/kaggle/tmp/{split}/image'\nweights_dir = '/kaggle/input/siimcovid19-detection-models/yolov5x_fold1_nb_v19_best.pt'\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python detect.py --weights $weights_dir\\\n--img 640\\\n--conf 0.001\\\n--iou 0.5\\\n--source $test_dir\\\n--save-txt --save-conf --augment\n\n\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\n\nimage_ids = [] # (num_images, )\nPredictionStrings = [] # (num_images, boxes_lst)\n\nfor file_path in tqdm(glob('runs/detect/exp2/labels/*.txt')):\n    image_id = file_path.split('/')[-1].split('.')[0]\n    w, h = test_df.loc[test_df.id==image_id,['dim1', 'dim0']].values[0]\n    f = open(file_path, 'r')\n    data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n    data = data[:, [0, 5, 1, 2, 3, 4]]\n    bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n    new_bboxes = []\n    for idx in range(len(bboxes)):\n        if idx%6 == 0:\n            new_bboxes.append('opacity')\n        else :\n            new_bboxes.append(bboxes[idx])\n    image_ids.append(image_id)\n    PredictionStrings.append(' '.join(new_bboxes))\n\n\npred_yolov5_fold1_df = pd.DataFrame({'id':image_ids,\n                        'PredictionString':PredictionStrings})\n\nFINAL_COUNT += 1\npred_yolov5_fold1_df.head()","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-09T07:26:34.308537Z","iopub.execute_input":"2021-08-09T07:26:34.3089Z","iopub.status.idle":"2021-08-09T07:26:44.481553Z","shell.execute_reply.started":"2021-08-09T07:26:34.308859Z","shell.execute_reply":"2021-08-09T07:26:44.480484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## yolov5 fold 2","metadata":{}},{"cell_type":"code","source":"dim = 640 #1024, 256, 'original'\ntest_dir = f'/kaggle/tmp/{split}/image'\nweights_dir = '/kaggle/input/siimcovid19-detection-models/yolov5x_fold2_nb_v20_best.pt'\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python detect.py --weights $weights_dir\\\n--img 640\\\n--conf 0.001\\\n--iou 0.5\\\n--source $test_dir\\\n--save-txt --save-conf --augment\n\n\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\n\nimage_ids = [] # (num_images, )\nPredictionStrings = [] # (num_images, boxes_lst)\n\nfor file_path in tqdm(glob('runs/detect/exp3/labels/*.txt')):\n    image_id = file_path.split('/')[-1].split('.')[0]\n    w, h = test_df.loc[test_df.id==image_id,['dim1', 'dim0']].values[0]\n    f = open(file_path, 'r')\n    data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n    data = data[:, [0, 5, 1, 2, 3, 4]]\n    bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n    new_bboxes = []\n    for idx in range(len(bboxes)):\n        if idx%6 == 0:\n            new_bboxes.append('opacity')\n        else :\n            new_bboxes.append(bboxes[idx])\n    image_ids.append(image_id)\n    PredictionStrings.append(' '.join(new_bboxes))\n\n\npred_yolov5_fold2_df = pd.DataFrame({'id':image_ids,\n                        'PredictionString':PredictionStrings})\n\nFINAL_COUNT += 1\npred_yolov5_fold2_df.head()","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-09T07:26:44.488192Z","iopub.execute_input":"2021-08-09T07:26:44.488477Z","iopub.status.idle":"2021-08-09T07:26:54.716461Z","shell.execute_reply.started":"2021-08-09T07:26:44.488447Z","shell.execute_reply":"2021-08-09T07:26:54.715379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## yolov5 fold 3","metadata":{}},{"cell_type":"code","source":"dim = 640 #1024, 256, 'original'\ntest_dir = f'/kaggle/tmp/{split}/image'\nweights_dir = '/kaggle/input/siimcovid19-detection-models/yolov5x_fold3_nb_v21_best.pt'\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python detect.py --weights $weights_dir\\\n--img 640\\\n--conf 0.001\\\n--iou 0.5\\\n--source $test_dir\\\n--save-txt --save-conf --augment\n\n\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\n\nimage_ids = [] # (num_images, )\nPredictionStrings = [] # (num_images, boxes_lst)\n\nfor file_path in tqdm(glob('runs/detect/exp4/labels/*.txt')):\n    image_id = file_path.split('/')[-1].split('.')[0]\n    w, h = test_df.loc[test_df.id==image_id,['dim1', 'dim0']].values[0]\n    f = open(file_path, 'r')\n    data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n    data = data[:, [0, 5, 1, 2, 3, 4]]\n    bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n    new_bboxes = []\n    for idx in range(len(bboxes)):\n        if idx%6 == 0:\n            new_bboxes.append('opacity')\n        else :\n            new_bboxes.append(bboxes[idx])\n    image_ids.append(image_id)\n    PredictionStrings.append(' '.join(new_bboxes))\n\n\npred_yolov5_fold3_df = pd.DataFrame({'id':image_ids,\n                        'PredictionString':PredictionStrings})\n\nFINAL_COUNT += 1\npred_yolov5_fold3_df.head()","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-09T07:26:54.718898Z","iopub.execute_input":"2021-08-09T07:26:54.719326Z","iopub.status.idle":"2021-08-09T07:27:05.149621Z","shell.execute_reply.started":"2021-08-09T07:26:54.719279Z","shell.execute_reply":"2021-08-09T07:27:05.148555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## yolov5 fold 4","metadata":{}},{"cell_type":"code","source":"dim = 640 #1024, 256, 'original'\ntest_dir = f'/kaggle/tmp/{split}/image'\nweights_dir = '/kaggle/input/siimcovid19-detection-models/yolov5x_fold4_nb_v22_best.pt'\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python detect.py --weights $weights_dir\\\n--img 640\\\n--conf 0.001\\\n--iou 0.5\\\n--source $test_dir\\\n--save-txt --save-conf --augment\n\n\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\n\nimage_ids = [] # (num_images, )\nPredictionStrings = [] # (num_images, boxes_lst)\n\nfor file_path in tqdm(glob('runs/detect/exp5/labels/*.txt')):\n    image_id = file_path.split('/')[-1].split('.')[0]\n    w, h = test_df.loc[test_df.id==image_id,['dim1', 'dim0']].values[0]\n    f = open(file_path, 'r')\n    data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n    data = data[:, [0, 5, 1, 2, 3, 4]]\n    bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n    new_bboxes = []\n    for idx in range(len(bboxes)):\n        if idx%6 == 0:\n            new_bboxes.append('opacity')\n        else :\n            new_bboxes.append(bboxes[idx])\n    image_ids.append(image_id)\n    PredictionStrings.append(' '.join(new_bboxes))\n\n\npred_yolov5_fold4_df = pd.DataFrame({'id':image_ids,\n                        'PredictionString':PredictionStrings})\n\nFINAL_COUNT += 1\npred_yolov5_fold4_df.head()","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-09T07:27:05.151288Z","iopub.execute_input":"2021-08-09T07:27:05.151673Z","iopub.status.idle":"2021-08-09T07:27:15.168722Z","shell.execute_reply.started":"2021-08-09T07:27:05.151634Z","shell.execute_reply":"2021-08-09T07:27:15.167395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MMDetection","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/mmdet2100/mmdetection-2.10.0/addict-2.4.0-py3-none-any.whl\n!pip install /kaggle/input/mmdet2100/mmdetection-2.10.0/yapf-0.31.0-py2.py3-none-any.whl\n!pip install /kaggle/input/pycocotools202/pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl\n!pip install /kaggle/input/mmcvfull134/mmcv_full-1.3.4-cp37-cp37m-manylinux1_x86_64.whl\n!pip install /kaggle/input/mmdetection2120/mmdetection-2.12.0 -f ./ --no-index\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:27:15.170292Z","iopub.execute_input":"2021-08-09T07:27:15.170699Z","iopub.status.idle":"2021-08-09T07:29:18.04108Z","shell.execute_reply.started":"2021-08-09T07:27:15.170659Z","shell.execute_reply":"2021-08-09T07:29:18.040056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport yaml\nimport time\nimport json\nimport cv2\nimport random\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\nimport torch\nfrom IPython.display import Image, clear_output\nfrom collections import Counter\nfrom ensemble_boxes import *\nimport copy\nimport os.path as osp\nimport mmcv\nimport mmdet\nimport numpy as np\nimport albumentations as A\nfrom mmdet.datasets.builder import DATASETS\nfrom mmdet.datasets.custom import CustomDataset\nfrom mmcv import Config\nfrom mmdet.apis import set_random_seed\nfrom mmdet.apis import inference_detector, init_detector, show_result_pyplot\nfrom mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector\nprint(mmdet.__version__)\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:29:18.044265Z","iopub.execute_input":"2021-08-09T07:29:18.044713Z","iopub.status.idle":"2021-08-09T07:29:37.449386Z","shell.execute_reply.started":"2021-08-09T07:29:18.04467Z","shell.execute_reply":"2021-08-09T07:29:37.448456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = '/kaggle/input/siimcovid19-detection-models/cascade_rcnn_x101_32x4d_fpn_1x_fold0_epoch10_public_nb_v8.pth'\ncfg = '/kaggle/input/siimcovid19-detection-models/cascade_rcnn_x101_32x4d_fpn_1x_fold0_epoch10_public_nb_v8_config.py'\n\ncfg = Config.fromfile(cfg)\n\ncfg.classes = (\"Covid_Abnormality\")\ncfg.data.test.img_prefix = ''\ncfg.data.test.classes = cfg.classes\n\n# cfg.model.roi_head.bbox_head.num_classes = 1\n# cfg.model.bbox_head.num_classes = 1\nfor head in cfg.model.roi_head.bbox_head:\n    head.num_classes = 1\n\n# Set seed thus the results are more reproducible\ncfg.seed = 211\nset_random_seed(211, deterministic=False)\ncfg.gpu_ids = [0]\n\ncfg.data.test.pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(1333, 800),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip', direction='horizontal'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]\n\ncfg.test_pipeline = [\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(1333, 800),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip', direction='horizontal'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]\n\n# cfg.data.samples_per_gpu = 4\n# cfg.data.workers_per_gpu = 4\n# cfg.model.test_cfg.nms.iou_threshold = 0.3\ncfg.model.test_cfg.rcnn.score_thr = 0.001\n\nmodel_test = init_detector(cfg, checkpoint, device='cuda:0')\nprint(model_test)\n\nFINAL_COUNT += 1","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-09T07:29:37.450817Z","iopub.execute_input":"2021-08-09T07:29:37.451315Z","iopub.status.idle":"2021-08-09T07:29:47.31993Z","shell.execute_reply.started":"2021-08-09T07:29:37.451273Z","shell.execute_reply":"2021-08-09T07:29:47.318134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nms_one_img(preds, img_path, img_width, img_height):\n    # print('img width:', img_width)\n    # print('img height:', img_height)\n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = [1]\n    # print(preds)\n    for i, pred in enumerate(preds):\n        if len(pred):\n            for p in pred:\n                box = [0, 0, 0, 0]\n                box[0] = min(1.0, p[0] / img_width)\n                box[1] = min(1.0, p[1] / img_height)\n                box[2] = min(1.0, p[2] / img_width)\n                box[3] = min(1.0, p[3] / img_height)\n                # print(box)\n                for b in box:\n                    if b > 1:\n                        print(img_path)\n                boxes_list.append(box)\n                score = p[4].astype(float)\n                scores_list.append(score)\n                labels_list.append(i)\n    # print('Before:')\n    # print(boxes_list)\n    boxes_list, scores_list, labels_list = nms([boxes_list], [scores_list], [labels_list], weights=weights, iou_thr=0.6)\n    # print('After:')\n    # print(boxes_list)\n    return boxes_list, scores_list, labels_list\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:29:47.32127Z","iopub.execute_input":"2021-08-09T07:29:47.321698Z","iopub.status.idle":"2021-08-09T07:29:47.405784Z","shell.execute_reply.started":"2021-08-09T07:29:47.321658Z","shell.execute_reply":"2021-08-09T07:29:47.404763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def infer_img(model, img_path):\n    img = mmcv.imread(img_path)\n    result = inference_detector(model, img_path)\n    # print(img.shape)\n    # print(img.shape[0])\n    # print(img.shape[1])\n    boxes_list, scores_list, labels_list = nms_one_img(result, img_path, img.shape[1], img.shape[0])\n    return boxes_list, scores_list, labels_list\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:29:47.407169Z","iopub.execute_input":"2021-08-09T07:29:47.407558Z","iopub.status.idle":"2021-08-09T07:29:47.465169Z","shell.execute_reply.started":"2021-08-09T07:29:47.40752Z","shell.execute_reply":"2021-08-09T07:29:47.464209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def round_float(str_float, dec=3):\n    return str(round(float(str_float), 3))\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:29:47.466501Z","iopub.execute_input":"2021-08-09T07:29:47.466848Z","iopub.status.idle":"2021-08-09T07:29:47.518969Z","shell.execute_reply.started":"2021-08-09T07:29:47.466811Z","shell.execute_reply":"2021-08-09T07:29:47.518157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_df_row_list(img_id, boxes_list, scores_list, labels_list):\n    res = []\n    res.append(img_id)\n    prediction_string = ''\n    for i, label in enumerate(labels_list):\n        if int(label) == 1:\n            prediction_string = 'none 1 0 0 1 1'\n            continue\n        else:\n            prediction_string += 'opacity'\n        prediction_string += ' '\n        prediction_string += str(round(float(scores_list[i]), 3))\n        prediction_string += ' '\n        prediction_string += round_float(boxes_list[i][0], 3) + ' ' + round_float(boxes_list[i][1], 3) + ' ' + round_float(boxes_list[i][2], 3) + ' ' + round_float(boxes_list[i][3], 3) + ' '\n    res.append(prediction_string.rstrip())\n    return res\n\nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:29:47.520268Z","iopub.execute_input":"2021-08-09T07:29:47.520645Z","iopub.status.idle":"2021-08-09T07:29:47.575564Z","shell.execute_reply.started":"2021-08-09T07:29:47.520609Z","shell.execute_reply":"2021-08-09T07:29:47.574705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vfnet_pred_lst = []\ntest_dir = f'/kaggle/tmp/{split}/image'\ncount = 0\n\nfor img_name in os.listdir(test_dir):\n    img_path = test_dir + '/' + img_name\n    # boxes_list, scores_list, labels_list = infer_img(model, '../vinbigdata-cocodataset/ori_vinbigdata_3xdownsampled/test/test/b461cd28bc17c294dd986d0d91577ac3.jpg')\n    boxes_list, scores_list, labels_list = infer_img(model_test, img_path)\n    # print(boxes_list)\n    # print(scores_list)\n    # print(labels_list)\n    image_id = img_name.split('.png')[0]\n    ori_img_width = test_df[test_df.id == image_id]['dim1'].iloc[0]\n    ori_img_height = test_df[test_df.id == image_id]['dim0'].iloc[0]\n    boxes_list[:, 0] = boxes_list[:, 0] * ori_img_width\n    boxes_list[:, 2] = boxes_list[:, 2] * ori_img_width\n    boxes_list[:, 1] = boxes_list[:, 1] * ori_img_height\n    boxes_list[:, 3] = boxes_list[:, 3] * ori_img_height\n    df_row_lst = convert_to_df_row_list(image_id, boxes_list, scores_list, labels_list)\n    vfnet_pred_lst.append(df_row_lst)\n    \nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:29:47.576957Z","iopub.execute_input":"2021-08-09T07:29:47.577391Z","iopub.status.idle":"2021-08-09T07:29:48.994379Z","shell.execute_reply.started":"2021-08-09T07:29:47.577339Z","shell.execute_reply":"2021-08-09T07:29:48.993383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vfnet_pred_df = pd.DataFrame(vfnet_pred_lst, columns=['id', 'PredictionString'])\n\nFINAL_COUNT += 1\nvfnet_pred_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:29:48.995591Z","iopub.execute_input":"2021-08-09T07:29:48.995937Z","iopub.status.idle":"2021-08-09T07:29:49.055897Z","shell.execute_reply.started":"2021-08-09T07:29:48.995899Z","shell.execute_reply":"2021-08-09T07:29:49.05489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Detection Emsemble","metadata":{}},{"cell_type":"code","source":"# %%\nimport numpy as np\nimport pandas as pd\nfrom ensemble_boxes import nms, soft_nms, non_maximum_weighted, weighted_boxes_fusion\n\n# %%\n''' Merge output of two models after 2cls filter\n- Detectron2: https://www.kaggle.com/corochann/vinbigdata-detectron2-prediction\n- Yolov5: https://www.kaggle.com/awsaf49/vinbigdata-2-class-filter\nReference:\n- https://github.com/ZFTurbo/Weighted-Boxes-Fusion\n'''\n\n# %%\nweights = [54, 54, 54, 54, 54, 58, 58, 54]\niou_thr = 0.6\nskip_box_thr = 0.01 # 0.0001 for non soft-nms\nsigma = 0.1\n\n# %%\ntest_meta = test_df\nmerged_df = pd.DataFrame(columns=['id', 'PredictionString'])\n\n# %%\n'''Weighted Boxes Fusion'''\nimage_id_lst = pred_yolov5_fold0_df['id'].unique()\n\n# %%\n# Helper functions\ndef extract_data(data, img_height, img_width):\n    boxes_lst = []\n    scores_lst = []\n    labels_lst = []\n    data_lst = data.split(' ')\n    for i in range(0, len(data_lst), 6):\n        labels_lst.append(0)\n        scores_lst.append(float(data_lst[i + 1]))\n        x_min = float(data_lst[i + 2]) / img_width\n        y_min = float(data_lst[i + 3]) / img_height\n        x_max = float(data_lst[i + 4]) / img_width\n        y_max = float(data_lst[i + 5]) / img_height\n        boxes_lst.append([x_min, y_min, x_max, y_max])\n    return boxes_lst, scores_lst, labels_lst\n\ndef convert_data_to_row(boxes, scores, labels):\n    data_lst = []\n    for i in range(len(boxes)):\n        data_lst.append('opacity')\n        data_lst.append(str(scores[i]))\n        data_lst.append(str(boxes[i][0]))\n        data_lst.append(str(boxes[i][1]))\n        data_lst.append(str(boxes[i][2]))\n        data_lst.append(str(boxes[i][3]))\n    data = ' '.join(data_lst)\n    return data\n\ndef get_height_width(image_id):\n    # dim0: heigth, dim1: width\n    height = test_meta[test_meta['id'] == image_id]['dim0'].values[0]\n    width = test_meta[test_meta['id'] == image_id]['dim1'].values[0]\n    return height, width\n\ndef scale_data(boxes, img_height, img_width):\n    res = []\n    for box in boxes:\n        temp = []\n        temp.append(box[0] * img_width)\n        temp.append(box[1] * img_height)\n        temp.append(box[2] * img_width)\n        temp.append(box[3] * img_height)\n        res.append(temp)\n    return res\n\n# %%\ndef wbf(image_id):\n    img_height, img_width = get_height_width(image_id)\n    boxes_lst, scores_lst, labels_lst = [], [], []\n\n    pred_yolov5_fold0_data = pred_yolov5_fold0_df[pred_yolov5_fold0_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(pred_yolov5_fold0_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n\n    pred_yolov5_fold1_data = pred_yolov5_fold1_df[pred_yolov5_fold1_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(pred_yolov5_fold1_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n\n    pred_yolov5_fold2_data = pred_yolov5_fold2_df[pred_yolov5_fold2_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(pred_yolov5_fold2_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n\n    pred_yolov5_fold3_data = pred_yolov5_fold3_df[pred_yolov5_fold3_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(pred_yolov5_fold3_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n\n    pred_yolov5_fold4_data = pred_yolov5_fold4_df[pred_yolov5_fold4_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(pred_yolov5_fold4_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n    \n#     effdet_fold0_data = effdet_fold0[effdet_fold0['id'] == image_id]['PredictionString'].values[0]\n#     model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(effdet_fold0_data, img_height, img_width)\n#     boxes_lst.append(model_boxes_lst)\n#     scores_lst.append(model_scores_lst)\n#     labels_lst.append(model_labels_lst)\n    \n#     effdet_fold1_data = effdet_fold1[effdet_fold1['id'] == image_id]['PredictionString'].values[0]\n#     model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(effdet_fold1_data, img_height, img_width)\n#     boxes_lst.append(model_boxes_lst)\n#     scores_lst.append(model_scores_lst)\n#     labels_lst.append(model_labels_lst)\n    \n#     effdet_fold2_data = effdet_fold2[effdet_fold2['id'] == image_id]['PredictionString'].values[0]\n#     model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(effdet_fold2_data, img_height, img_width)\n#     boxes_lst.append(model_boxes_lst)\n#     scores_lst.append(model_scores_lst)\n#     labels_lst.append(model_labels_lst)\n    \n    effdet_fold3_data = effdet_fold3[effdet_fold3['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(effdet_fold3_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n    \n    effdet_fold4_data = effdet_fold4[effdet_fold4['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(effdet_fold4_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n    \n    vfnet_pred_data = vfnet_pred_df[vfnet_pred_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(vfnet_pred_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n\n    boxes, scores, labels = weighted_boxes_fusion(boxes_lst, scores_lst, labels_lst, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    # boxes, scores, labels = non_maximum_weighted(boxes_lst, scores_lst, labels_lst, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    # boxes, scores, labels = nms(boxes_lst, scores_lst, labels_lst, weights=weights, iou_thr=iou_thr)\n    # boxes, scores, labels = soft_nms(boxes_lst, scores_lst, labels_lst, weights=weights, iou_thr=iou_thr, sigma=sigma, thresh=skip_box_thr)\n    boxes = scale_data(boxes, img_height, img_width)\n    # return boxes, scores, labels\n    merged_data = convert_data_to_row(boxes, scores, labels)\n    merged_data = pd.DataFrame([[image_id, merged_data]], columns=['id', 'PredictionString'])\n    return merged_data\n\n# %%\n# Test\n# test_boxes, test_scores, test_labels = wbf(image_id_lst[0])\ntest_wbf = wbf(image_id_lst[0])\n\n# %%\nfor image_id in image_id_lst:\n    merged_data = wbf(image_id)\n    merged_df = merged_df.append(merged_data, ignore_index=True)\n    \nFINAL_COUNT += 1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:31:22.50496Z","iopub.execute_input":"2021-08-09T07:31:22.505328Z","iopub.status.idle":"2021-08-09T07:31:23.579389Z","shell.execute_reply.started":"2021-08-09T07:31:22.505292Z","shell.execute_reply":"2021-08-09T07:31:23.578514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FINAL_COUNT += 1\nmerged_df","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:31:23.580965Z","iopub.execute_input":"2021-08-09T07:31:23.581307Z","iopub.status.idle":"2021-08-09T07:31:23.642078Z","shell.execute_reply.started":"2021-08-09T07:31:23.58127Z","shell.execute_reply":"2021-08-09T07:31:23.640922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FINAL_COUNT += 1\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:31:23.644098Z","iopub.execute_input":"2021-08-09T07:31:23.644537Z","iopub.status.idle":"2021-08-09T07:31:23.708657Z","shell.execute_reply.started":"2021-08-09T07:31:23.644493Z","shell.execute_reply":"2021-08-09T07:31:23.707497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = test_df.drop(['PredictionString'], axis=1)\nsub_df = pd.merge(test_df, merged_df, on = 'id', how = 'left').fillna(\"none 1 0 0 1 1\")\nFINAL_COUNT += 1\nsub_df","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:31:24.086928Z","iopub.execute_input":"2021-08-09T07:31:24.087273Z","iopub.status.idle":"2021-08-09T07:31:24.156156Z","shell.execute_reply.started":"2021-08-09T07:31:24.08724Z","shell.execute_reply":"2021-08-09T07:31:24.155154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = sub_df[['id', 'PredictionString']]\nsub_df['none'] = df_2class['none']\nFINAL_COUNT += 1\nsub_df","metadata":{"execution":{"iopub.status.busy":"2021-08-09T07:31:24.943527Z","iopub.execute_input":"2021-08-09T07:31:24.943891Z","iopub.status.idle":"2021-08-09T07:31:25.011569Z","shell.execute_reply.started":"2021-08-09T07:31:24.943857Z","shell.execute_reply":"2021-08-09T07:31:25.010683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(sub_df.shape[0]):\n    if sub_df.loc[i,'PredictionString'] == \"none 1 0 0 1 1\":\n        continue\n    sub_df_split = sub_df.loc[i,'PredictionString'].split()\n    none_score = float(sub_df.loc[i, 'none'])\n    sub_df_list = []\n    for j in range(int(len(sub_df_split) / 6)):\n        sub_df_list.append('opacity')\n        opacity_score = float(sub_df_split[6 * j + 1])\n        calibrated_opacity_score = opacity_score * ((1 - none_score) ** 0.5) \n        sub_df_list.append(str(calibrated_opacity_score))\n        sub_df_list.append(sub_df_split[6 * j + 2])\n        sub_df_list.append(sub_df_split[6 * j + 3])\n        sub_df_list.append(sub_df_split[6 * j + 4])\n        sub_df_list.append(sub_df_split[6 * j + 5])\n    sub_df.loc[i,'PredictionString'] = ' '.join(sub_df_list)\nfor i in range(sub_df.shape[0]):\n    if sub_df.loc[i,'PredictionString'] != 'none 1 0 0 1 1':\n        sub_df.loc[i,'PredictionString'] = sub_df.loc[i,'PredictionString'] + ' none ' + str(sub_df.loc[i,'none']) + ' 0 0 1 1'\nsub_df = sub_df[['id', 'PredictionString']]   \ndf_study = df_study[:study_len]\ndf_study = df_study.append(sub_df).reset_index(drop=True)\nif FINAL_COUNT==81:\n    df_study.to_csv('/kaggle/working/submission.csv',index = False)  \nshutil.rmtree('/kaggle/working/yolov5')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:22:13.581584Z","iopub.execute_input":"2021-08-01T23:22:13.582263Z","iopub.status.idle":"2021-08-01T23:22:13.660883Z","shell.execute_reply.started":"2021-08-01T23:22:13.58222Z","shell.execute_reply":"2021-08-01T23:22:13.660127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_study","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:22:13.775955Z","iopub.execute_input":"2021-08-01T23:22:13.776235Z","iopub.status.idle":"2021-08-01T23:22:13.83193Z","shell.execute_reply.started":"2021-08-01T23:22:13.776208Z","shell.execute_reply":"2021-08-01T23:22:13.830969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}