{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"!conda install '/kaggle/input/pydicom-conda-helper/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-16T21:16:29.424767Z","iopub.execute_input":"2021-08-16T21:16:29.425144Z","iopub.status.idle":"2021-08-16T21:17:18.94101Z","shell.execute_reply.started":"2021-08-16T21:16:29.425112Z","shell.execute_reply":"2021-08-16T21:17:18.938428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install /kaggle/input/kerasapplications -q\n!pip install /kaggle/input/efficientnet-keras-source-code/ -q --no-deps","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:17:18.945993Z","iopub.execute_input":"2021-08-16T21:17:18.946486Z","iopub.status.idle":"2021-08-16T21:17:30.812177Z","shell.execute_reply.started":"2021-08-16T21:17:18.946403Z","shell.execute_reply":"2021-08-16T21:17:30.810882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport efficientnet.tfkeras as efn\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport random\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom skimage import exposure\nfrom skimage import io","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-16T21:17:30.814345Z","iopub.execute_input":"2021-08-16T21:17:30.814648Z","iopub.status.idle":"2021-08-16T21:17:30.820588Z","shell.execute_reply.started":"2021-08-16T21:17:30.814614Z","shell.execute_reply":"2021-08-16T21:17:30.819865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Processing","metadata":{}},{"cell_type":"code","source":"def read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:17:30.821772Z","iopub.execute_input":"2021-08-16T21:17:30.822057Z","iopub.status.idle":"2021-08-16T21:17:30.835079Z","shell.execute_reply.started":"2021-08-16T21:17:30.822031Z","shell.execute_reply":"2021-08-16T21:17:30.83434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:17:30.836212Z","iopub.execute_input":"2021-08-16T21:17:30.83651Z","iopub.status.idle":"2021-08-16T21:17:30.84518Z","shell.execute_reply.started":"2021-08-16T21:17:30.836483Z","shell.execute_reply":"2021-08-16T21:17:30.844353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_image(img):\n    equ_img = exposure.equalize_adapthist(img/255, clip_limit=0.05, kernel_size=24)\n    return equ_img","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:17:30.846365Z","iopub.execute_input":"2021-08-16T21:17:30.84665Z","iopub.status.idle":"2021-08-16T21:17:30.858462Z","shell.execute_reply.started":"2021-08-16T21:17:30.846623Z","shell.execute_reply":"2021-08-16T21:17:30.857616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder_path = \"../input/siim-covid19-detection\"\ntrain_path = folder_path + \"/train/\"\ntest_path = folder_path + \"/test/\"\n    \ntrain_image_data = pd.read_csv(\"../input/siim-covid19-detection/train_image_level.csv\")\n#display(train_image_data)\n#train_image_data = train_image_data.drop(labels=['boxes', 'label'], axis=1)\ntrain_image_data = train_image_data.rename(columns = {'id':'image_id', \"StudyInstanceUID\": \"study_id\"})\ntrain_image_data[\"study_id\"] = [item + \"_study\" for item in train_image_data['study_id']]\ntrain_image_data = train_image_data[[\"study_id\", \"image_id\", \"boxes\", \"label\"]]\n\nimage_paths_arr = []\nfor i in range(0, len(train_image_data)):\n    study_path = train_path + str(train_image_data['study_id'][i][:-6])\n    series = os.listdir(study_path)\n    for j in range(0, len(series)):\n        series_path = study_path + \"/\" + series[j]\n        images = os.listdir(series_path)\n        for k in range(0, len(images)):\n            if images[k][:-4] in train_image_data['image_id'][i]:\n                image_path = series_path + \"/\" + images[k]\n                image_paths_arr.append(image_path)\n                \ntrain_image_data['path'] = image_paths_arr\n\ntrain_study_data = pd.read_csv(\"../input/siim-covid19-detection/train_study_level.csv\")\n\nvariables = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\nfor var in variables:\n    train_image_data[var] = \"\"\n    for i in range(0, len(train_image_data)):\n        train_image_data[var][i] = int(train_study_data[var][train_study_data.index[train_study_data['id'] == train_image_data['study_id'][i]]])\n        \ntrain_image_data['Final Label'] = \"\"\nfor i in range(0, len(train_image_data)):\n    for j in range(0, 4):\n        if (train_image_data[variables[j]][i] == 1):\n            train_image_data['Final Label'][i] = j\n\ntrain_image_data = train_image_data[['path', 'study_id', 'image_id', 'Final Label', 'boxes', 'label']]\n        \ndisplay(train_image_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:17:30.8602Z","iopub.execute_input":"2021-08-16T21:17:30.860646Z","iopub.status.idle":"2021-08-16T21:18:41.942998Z","shell.execute_reply.started":"2021-08-16T21:17:30.860617Z","shell.execute_reply":"2021-08-16T21:18:41.941691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"this_image_info = train_image_data[10:20]\ndisplay(this_image_info)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:18:41.945819Z","iopub.execute_input":"2021-08-16T21:18:41.946233Z","iopub.status.idle":"2021-08-16T21:18:41.965624Z","shell.execute_reply.started":"2021-08-16T21:18:41.94619Z","shell.execute_reply":"2021-08-16T21:18:41.964468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = this_image_info['path'][10]","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:18:41.967331Z","iopub.execute_input":"2021-08-16T21:18:41.967714Z","iopub.status.idle":"2021-08-16T21:18:41.972796Z","shell.execute_reply.started":"2021-08-16T21:18:41.967673Z","shell.execute_reply":"2021-08-16T21:18:41.97158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir('/kaggle/working/normaldir')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:18:41.974011Z","iopub.execute_input":"2021-08-16T21:18:41.974552Z","iopub.status.idle":"2021-08-16T21:18:42.371472Z","shell.execute_reply.started":"2021-08-16T21:18:41.974514Z","shell.execute_reply":"2021-08-16T21:18:42.369997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xray = read_xray(path)\nim = resize(xray, size=600)  \nim = np.array(im)\nio.imsave('/kaggle/working/normaldir/normal.png', im)\n#im = preprocess_image(im)\n#io.imsave('test2.png', im)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:19:23.851906Z","iopub.execute_input":"2021-08-16T21:19:23.852268Z","iopub.status.idle":"2021-08-16T21:19:24.340706Z","shell.execute_reply.started":"2021-08-16T21:19:23.852237Z","shell.execute_reply":"2021-08-16T21:19:24.339739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd\nfrom tqdm import tqdm\nimport json\n\ndef dicom2array(path, voi_lut=True, fix_monochrome=True):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n\ndef get_bbox (image_path, save_path):\n\n    thickness = 10\n\n    #print(image_path)\n    img = dicom2array(path=image_path)\n    img = np.stack([img, img, img], axis=-1)\n    \n    l = this_image_info.loc[this_image_info['path'] == image_path].values[0][5]\n    if 'none' in l:\n        #plt.figure()\n        #plt.imshow(img)\n        img = tf.keras.preprocessing.image.array_to_img(img)\n        img.save(save_path)\n        return\n    \n    b = this_image_info.loc[this_image_info['path'] == image_path].values[0][4][1:-1]\n    b = b.split('}, {')\n    for i in range(len(b)):\n        if b[i][0] is not '{':\n            b[i] = '{' + b[i]\n        if b[i][-1] is not '}':\n            b[i] = b[i] + '}'\n        b[i] = b[i].replace(\"\\'\", \"\\\"\")\n        b[i] = json.loads(b[i])\n    \n    for i in b:\n        img = cv2.rectangle(img, (int(float(i['x'])), int(float(i['y']))), (int(float(i['x'] + i['width'])), int(float(i['y']+i['height']))), [255,0,0], thickness)\n    \n    #plt.figure()\n    #plt.imshow(img)\n    img = tf.keras.preprocessing.image.array_to_img(img)\n    img.save(save_path)\n    return","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:19:30.85484Z","iopub.execute_input":"2021-08-16T21:19:30.855413Z","iopub.status.idle":"2021-08-16T21:19:30.868918Z","shell.execute_reply.started":"2021-08-16T21:19:30.855379Z","shell.execute_reply":"2021-08-16T21:19:30.868171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_bbox(path, 'bbox.jpg')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:19:37.155852Z","iopub.execute_input":"2021-08-16T21:19:37.156258Z","iopub.status.idle":"2021-08-16T21:19:38.049505Z","shell.execute_reply.started":"2021-08-16T21:19:37.15622Z","shell.execute_reply":"2021-08-16T21:19:38.048526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nimport cv2\nimport warnings\nwarnings.filterwarnings('ignore')\nimport shutil \nimport tensorflow as tf\n%matplotlib inline\n\n\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nimport pprint\nimport pydicom as dicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport wandb\n\nimport PIL\nfrom PIL import Image\nfrom colorama import Fore, Back, Style\nviz_counter=0\n\ndef create_dir(dir, v=1):\n    \"\"\"\n    Creates a directory without throwing an error if directory already exists.\n    dir : The directory to be created.\n    v : Verbosity\n    \"\"\"\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n        if v:\n            print(\"Created Directory : \", dir)\n        return 1\n    else:\n        if v:\n            print(\"Directory already existed : \", dir)\n        return 0\n\nvoi_lut=True\nfix_monochrome=True\n\ndef dicom_dataset_to_dict(filename):\n    \"\"\"Credit: https://github.com/pydicom/pydicom/issues/319\n               https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    \"\"\"\n    \n    dicom_header = dicom.dcmread(filename) \n    \n    #====== DICOM FILE DATA ======\n    dicom_dict = {}\n    repr(dicom_header)\n    for dicom_value in dicom_header.values():\n        if dicom_value.tag == (0x7fe0, 0x0010):\n            #discard pixel data\n            continue\n        if type(dicom_value.value) == dicom.dataset.Dataset:\n            dicom_dict[dicom_value.name] = dicom_dataset_to_dict(dicom_value.value)\n        else:\n            v = _convert_value(dicom_value.value)\n            dicom_dict[dicom_value.name] = v\n      \n    del dicom_dict['Pixel Representation']\n    \n    #====== DICOM IMAGE DATA ======\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom_header.pixel_array, dicom_header)\n    else:\n        data = dicom_header.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom_header.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data / np.max(data)\n    modified_image_data = (data * 255).astype(np.uint8)\n    \n    return dicom_dict, modified_image_data\n\ndef _sanitise_unicode(s):\n    return s.replace(u\"\\u0000\", \"\").strip()\n\ndef _convert_value(v):\n    t = type(v)\n    if t in (list, int, float):\n        cv = v\n    elif t == str:\n        cv = _sanitise_unicode(v)\n    elif t == bytes:\n        s = v.decode('ascii', 'replace')\n        cv = _sanitise_unicode(s)\n    elif t == dicom.valuerep.DSfloat:\n        cv = float(v)\n    elif t == dicom.valuerep.IS:\n        cv = int(v)\n    else:\n        cv = repr(v)\n    return cv\n\n\nimport os, fnmatch\ndef find(pattern, path):\n    \"\"\"Utility to find files wrt a regex search\"\"\"\n    result = []\n    for root, dirs, files in os.walk(path):\n        for name in files:\n            if fnmatch.fnmatch(name, pattern):\n                result.append(os.path.join(root, name))\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:19:40.320986Z","iopub.execute_input":"2021-08-16T21:19:40.321349Z","iopub.status.idle":"2021-08-16T21:19:40.873015Z","shell.execute_reply.started":"2021-08-16T21:19:40.321313Z","shell.execute_reply":"2021-08-16T21:19:40.871901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz_counter=0","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:19:43.094989Z","iopub.execute_input":"2021-08-16T21:19:43.095355Z","iopub.status.idle":"2021-08-16T21:19:43.102386Z","shell.execute_reply.started":"2021-08-16T21:19:43.095316Z","shell.execute_reply":"2021-08-16T21:19:43.101436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras import backend as keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\n\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = keras.flatten(y_true)\n    y_pred_f = keras.flatten(y_pred)\n    intersection = keras.sum(y_true_f * y_pred_f)\n    return (2. * intersection + 1) / (keras.sum(y_true_f) + keras.sum(y_pred_f) + 1)\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\n\ndef unet(input_size=(256,256,1)):\n    inputs = Input(input_size)\n    \n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n\n    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\n    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\n    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\n    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n\n    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n\n    return Model(inputs=[inputs], outputs=[conv10])","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:19:43.974681Z","iopub.execute_input":"2021-08-16T21:19:43.975202Z","iopub.status.idle":"2021-08-16T21:19:44.041858Z","shell.execute_reply.started":"2021-08-16T21:19:43.975169Z","shell.execute_reply":"2021-08-16T21:19:44.040925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = unet(input_size=(512,512,1))\nmodel.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss,\n                  metrics=[dice_coef, 'binary_accuracy'])\n#model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:19:47.483422Z","iopub.execute_input":"2021-08-16T21:19:47.483928Z","iopub.status.idle":"2021-08-16T21:19:47.857786Z","shell.execute_reply.started":"2021-08-16T21:19:47.483884Z","shell.execute_reply":"2021-08-16T21:19:47.856994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_weights_path = \"../input/unet-lung-segmentation-weights-for-chest-x-rays/cxr_reg_weights.best.hdf5\"\n\nmodel.load_weights(model_weights_path)\n\nShape_X,Shape_Y=512,512","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:19:48.74201Z","iopub.execute_input":"2021-08-16T21:19:48.742513Z","iopub.status.idle":"2021-08-16T21:19:49.95639Z","shell.execute_reply.started":"2021-08-16T21:19:48.742484Z","shell.execute_reply":"2021-08-16T21:19:49.955297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir('/kaggle/working/segmentdir')","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:19:51.565153Z","iopub.execute_input":"2021-08-16T21:19:51.56561Z","iopub.status.idle":"2021-08-16T21:19:51.570162Z","shell.execute_reply.started":"2021-08-16T21:19:51.565569Z","shell.execute_reply":"2021-08-16T21:19:51.569125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filenames = './normaldir/normal.png'\nmodified_image_data= cv2.imread(filenames, 0)\nresized_image_data = cv2.resize(modified_image_data,(Shape_Y,Shape_X)) # cv2 has this opposite\n# props(resized_image_data)\nprep_unet_input_img_1 = resized_image_data.reshape(1,Shape_X,Shape_Y,1)\nprep_unet_input_img = (prep_unet_input_img_1-127.0)/127.0\npred_img = model.predict(prep_unet_input_img)\npred_img_preprocessed_1 = np.squeeze(pred_img)\npred_img_preprocessed = (pred_img_preprocessed_1*255>127).astype(np.int8)\n# props(pred_img_preprocessed)\n# print(\"Unique Values :\",np.unique(pred_img_preprocessed))\nres = cv2.bitwise_and(resized_image_data,resized_image_data,mask = pred_img_preprocessed)\nfile_name = filenames.split(\"/\")[-1]\nsave_path = '/kaggle/working/segmentdir/segment.png'\ncv2.imwrite(save_path,res)","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:20:10.481824Z","iopub.execute_input":"2021-08-16T21:20:10.482224Z","iopub.status.idle":"2021-08-16T21:20:12.084038Z","shell.execute_reply.started":"2021-08-16T21:20:10.482193Z","shell.execute_reply":"2021-08-16T21:20:12.082939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\nimport torch","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:20:17.630058Z","iopub.execute_input":"2021-08-16T21:20:17.630459Z","iopub.status.idle":"2021-08-16T21:20:18.638738Z","shell.execute_reply.started":"2021-08-16T21:20:17.630426Z","shell.execute_reply":"2021-08-16T21:20:18.637971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim = 512 #1024, 256, 'original'\n#save_dir = f'/kaggle/tmp/segmented_data/{split}/segmented'\ntest_dir = '/kaggle/working/segmentdir'\n#weights_dir = '/kaggle/input/siim-cov19-yolov5-train/yolov5/runs/train/exp/weights/best.pt'\nweights_dir = '/kaggle/input/yolov5-segmented/segmentedbest.pt'\n\n#shutil.copytree('/kaggle/input/yolov5-repo/yolov5-master', '/kaggle/working/yolov5')\n#os.chdir('/kaggle/working/yolov5') # install dependencies\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python ../input/yolov5-repo/yolov5-master/detect.py --weights $weights_dir\\\n--img 512\\\n--conf 0.1\\\n--iou 0.5\\\n--augment\\\n--source $test_dir\\\n--save-txt --save-conf --exist-ok\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\nimage_ids = []\nPredictionStrings = []\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:20:23.936774Z","iopub.execute_input":"2021-08-16T21:20:23.937546Z","iopub.status.idle":"2021-08-16T21:20:37.458152Z","shell.execute_reply.started":"2021-08-16T21:20:23.937494Z","shell.execute_reply":"2021-08-16T21:20:37.457058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim = 512 #1024, 256, 'original'\n#save_dir = f'/kaggle/tmp/segmented_data/{split}/segmented'\ntest_dir = '/kaggle/working/normaldir'\n#weights_dir = '/kaggle/input/siim-cov19-yolov5-train/yolov5/runs/train/exp/weights/best.pt'\nweights_dir = '/kaggle/input/weights-of-yolov5-150-epochs/best.pt'\n\n#shutil.copytree('/kaggle/input/yolov5-repo/yolov5-master', '/kaggle/working/yolov5')\n#os.chdir('/kaggle/working/yolov5') # install dependencies\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python ../input/yolov5-repo/yolov5-master/detect.py --weights $weights_dir\\\n--img 512\\\n--conf 0.1\\\n--iou 0.5\\\n--augment\\\n--source $test_dir\\\n--save-txt --save-conf --exist-ok\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\nimage_ids = []\nPredictionStrings = []","metadata":{"execution":{"iopub.status.busy":"2021-08-16T21:20:41.954409Z","iopub.execute_input":"2021-08-16T21:20:41.95478Z","iopub.status.idle":"2021-08-16T21:20:55.364303Z","shell.execute_reply.started":"2021-08-16T21:20:41.954745Z","shell.execute_reply":"2021-08-16T21:20:55.363475Z"},"trusted":true},"execution_count":null,"outputs":[]}]}