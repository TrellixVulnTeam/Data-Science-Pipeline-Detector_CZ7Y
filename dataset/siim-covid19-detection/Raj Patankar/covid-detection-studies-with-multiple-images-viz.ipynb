{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align: center; font-family: Verdana; font-size: 32px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: #186b75; background-color: #ffffff;\">SIIM â€“ COVID19 Detection<br><br><font color=\"red\">EfficientNetV2-B3</font></h1>\n\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\">Study Level Image Exploration</h2>\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: DARIEN SCHETTLER</h5><br>\n\n<br>\n\n<center>ğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ </center>\n<center>ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«</center>\n<center>ğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ </center>\n<center>ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ªğŸ”¬ğŸ©ºğŸ’ŠğŸ§¬ğŸ¦ ğŸ§«ğŸ§ª</center>\n<center>ğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ¦ ğŸ¦ ğŸ¦ ğŸ©ºğŸ©ºğŸ©ºğŸ©º</center>\n<center>ğŸ©ºğŸ©ºğŸ©º</center>\n\n<br>\n","metadata":{"papermill":{"duration":0.044805,"end_time":"2021-02-05T02:17:44.538251","exception":false,"start_time":"2021-02-05T02:17:44.493446","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>\n\n<h2 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: darkred; background-color: #ffffff;\">TABLE OF CONTENTS</h2>\n\n---\n\n<h2 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h2>\n\n---\n\n<h2 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\"><a href=\"#background_information\">1&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION</a></h2>\n\n---\n\n<h2 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\"><a href=\"#setup\">2&nbsp;&nbsp;&nbsp;&nbsp;SETUP</a></h2>\n\n---\n\n<h2 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\"><a href=\"#helper_functions\">3&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS</a></h2>\n\n---\n\n<h2 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\"><a href=\"#tabular_data\">4&nbsp;&nbsp;&nbsp;&nbsp;TABULAR DATA</a></h2>\n\n---\n\n<h2 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\"><a href=\"#image_data\">5&nbsp;&nbsp;&nbsp;&nbsp;IMAGE DATA</a></h2>\n\n---\n\n<h2 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\"><a href=\"#find_duplicates\">6&nbsp;&nbsp;&nbsp;&nbsp;IDENTIFY DUPLICATES</a></h2>\n\n---\n\n<br>","metadata":{"papermill":{"duration":0.043006,"end_time":"2021-02-05T02:17:44.624815","exception":false,"start_time":"2021-02-05T02:17:44.581809","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"imports\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: darkred;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS</h1>","metadata":{"papermill":{"duration":0.04267,"end_time":"2021-02-05T02:17:44.71043","exception":false,"start_time":"2021-02-05T02:17:44.66776","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Installs\n!cp /kaggle/input/gdcm-conda-install/gdcm.tar .\n!tar -xvzf gdcm.tar\n!conda install --offline ./gdcm/gdcm-2.8.9-py37h71b2a6d_0.tar.bz2\n!rm -rf ./gdcm.tar\n\nprint(\"\\n... IMPORTS STARTING ...\\n\")\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\n##import tensorflow as tf; print(f\"\\t\\tâ€“ TENSORFLOW VERSION: {tf.__version__}\");\n##import tensorflow_addons as tfa; print(f\"\\t\\tâ€“ TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\tâ€“ NUMPY VERSION: {np.__version__}\");\n\nimport pydicom\nfrom pydicom import dcmread\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport imageio\nimport IPython\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image\nimport matplotlib; print(f\"\\t\\tâ€“ MATPLOTLIB VERSION: {matplotlib.__version__}\");\nimport plotly\nimport PIL\nimport cv2\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")\n\n# To give access to automl files\nsys.path.append(\"/kaggle/input/automl-efficientdet-efficientnetv2/automl\")\nsys.path.append(\"/kaggle/input/automl-efficientdet-efficientnetv2/automl/brain_automl\")\nsys.path.append(\"/kaggle/input/automl-efficientdet-efficientnetv2/automl/brain_automl/efficientdet\")\nsys.path.append(\"/kaggle/input/automl-efficientdet-efficientnetv2/automl/brain_automl/efficientnetv2\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":41.017298,"end_time":"2021-02-05T02:18:25.770952","exception":false,"start_time":"2021-02-05T02:17:44.753654","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n!pip install tensorflow==1.15","metadata":{"execution":{"iopub.status.busy":"2021-06-15T08:40:59.010742Z","iopub.execute_input":"2021-06-15T08:40:59.01109Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow==1.15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf; print(f\"\\t\\tâ€“ TENSORFLOW VERSION: {tf.__version__}\");\n##import tensorflow_addons as tfa; print(f\"\\t\\tâ€“ TENSORFLOW ADDONS VERSION: {tfa.__version__}\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras==2.2.5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"background_information\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: darkred; background-color: #ffffff;\" id=\"background_information\">1&nbsp;&nbsp;BACKGROUND INFORMATION</h1>","metadata":{"papermill":{"duration":0.045609,"end_time":"2021-02-05T02:18:25.862323","exception":false,"start_time":"2021-02-05T02:18:25.816714","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**<br>\n\n<h2 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\">1.1  THE DATA</h2>\n\n---\n\n<b style=\"text-decoration: underline; font-family: Verdana;\">BACKGROUND INFORMATION</b>\n\nIn this competition, we are identifying and localizing COVID-19 abnormalities on chest radiographs. <br>**This is an object detection and classification problem.**\n\nFor each test image, you will be predicting a bounding box and class for all findings. \n* If you predict that there are no findings, you should create a prediction of **`\"none 1 0 0 1 1\"`** \n    * \"none\" is the class ID for no finding, and this provides a one-pixel bounding box with a confidence of 1.0\n\nFurther, for each test study, you should make a determination within the following labels:\n\n> **`'Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance'`**\n\nTo make a prediction of one of the above labels, create a prediction string similar to the \"none\" class above: \n* i.e. **`atypical 1 0 0 1 1`**\n\n---\n\n**MESSAGE FROM THE COMPETITION HOST ON LABEL AND BBOX DETAILS:**\n\nIn this challenge, the chest radiographs (CXRs) were categorized using a specific grading schema, based on a published paper:\n\n[**Litmanovich DE, Chung M, Kirkbride RR, Kicska G, Kanne JP. Review of chest radiograph findings of COVID-19 pneumonia and suggested reporting language. Journal of thoracic imaging. 2020 Nov 14;35(6):354-60.**](https://journals.lww.com/thoracicimaging/Fulltext/2020/11000/Review_of_Chest_Radiograph_Findings_of_COVID_19.4.aspx)\n\nPer the grading schema, chest radiographs are classified into one of four categories, which are mutually exclusive:\n\n1. **Typical Appearance**: Multifocal bilateral, peripheral opacities with rounded morphology, lower lungâ€“predominant distribution\n2. **Indeterminate Appearance**: Absence of typical findings AND unilateral, central or upper lung predominant distribution\n3. **Atypical Appearance**: Pneumothorax, pleural effusion, pulmonary edema, lobar consolidation, solitary lung nodule or mass, diffuse tiny nodules, cavity\n4. **Negative for Pneumonia**: No lung opacities\n\nBounding boxes were placed on lung opacities, whether typical or indeterminate. Bounding boxes were also placed on some atypical findings including solitary lobar consolidation, nodules/masses, and cavities. Bounding boxes were not placed on pleural effusions, or pneumothoraces. No bounding boxes were placed for the negative for pneumonia category.\n\nIn cases of multiple adjacent opacities, we opted for one large bounding box, rather than multiple adjacent smaller boxes, to improve consistency in the labeling.\n\nAnnotators did have access to the COVID status for each patient, but were asked to adhere to the grading system above irrespective of the status. As such, some patients who were COVID negative still had chest radiographs with typical appearances. Similarly, some patients who were COVID positive had atypical appearances, or were negative for pneumonia (no lung opacities), because the grading system is based off the chest radiographic findings alone.\n\nThe goal in this challenge is to determine the appropriate category for each radiograph, as well as localize the lung opacities with a bounding box prediction.\n\n---\n\nThe images are in DICOM format, which means they contain additional data that might be useful for visualizing and classifying.\nNote that the images are in **DICOM** format, which means they contain additional data that might be useful for visualizing and classifying.\n\n![Example Radiographs](https://i.imgur.com/QWmbhXx.png)\n\n<br>\n\n<b style=\"text-decoration: underline; font-family: Verdana;\">DATASET INFORMATION</b>\n\nThe **train dataset** comprises **`6,334`** chest scans in **DICOM** format, which were de-identified to protect patient privacy. \n\nNote that all images are stored in paths with the form **`study/series/image`**. \n* The **`study`** ID here relates directly to the study-level predictions\n* the **`image`** ID is the ID used for image-level predictions\n\nThe **test dataset** is of roughly the same scale as the training dataset. \n* As this is a kernels only competition we shsould plan accordingly\n* i.e. we should be able to infer on the entirety of the training dataset within the submission kernel\n\n<br>\n\n<b style=\"text-decoration: underline; font-family: Verdana;\">DATA FILES</b>\n> **`train_study_level.csv`**\n> * **`id`** - unique study identifier\n> * **Negative for Pneumonia** - **`1`** if the study is negative for pneumonia, **`0`** otherwise\n> * **Typical Appearance** - **`1`** if the study has this appearance, **`0`** otherwise\n> * **Indeterminate Appearance**  - **`1`** if the study has this appearance, **`0`** otherwise\n> * **Atypical Appearance**  - **`1`** if the study has this appearance, **`0`** otherwise\n\n> **`train_image_level.csv`**\n> * **`id`** - unique image identifier\n> * **`boxes`** - bounding boxes in easily-readable dictionary format\n> * **`label`** - the correct prediction label for the provided bounding boxes","metadata":{"papermill":{"duration":0.04413,"end_time":"2021-02-05T02:18:25.953034","exception":false,"start_time":"2021-02-05T02:18:25.908904","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>\n\n<h2 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\">1.2  THE GOAL</h2>\n\n---\n\nIn this competition, youâ€™ll identify and localize COVID-19 abnormalities on chest radiographs. In particular, you'll categorize the radiographs as one of a possible **`4`** categories. \n\nIn this competition, we are making predictions at both a study (multi-image) and image level.\n* **`negative for pneumonia`** or **`typical`**, **`indeterminate`**, or **`atypical`** \n     \nYou'll work with a dataset consisting of **`8,781`** scans that have been annotated by experienced radiologists. You can train your model with **`6,334`** independently-labeled images and you will be evaluated on a test set of **`2,447`** images. \n\nThe challenge uses the standard PASCAL VOC 2010 mean Average Precision (mAP) at IoU > 0.5.\n* Note that the linked document describes VOC 2012, which differs in some minor ways (e.g. there is no concept of \"difficult\" classes in VOC 2010). The P/R curve and AP calculations remain the same.\n\n<br>\n\n<center>ğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©º</center>\n\n<center><font color=\"red\"><b>If successful, you'll help radiologists diagnose the millions of COVID-19 patients more confidently and quickly.</b></font></center>\n\n<center>ğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©ºğŸ©º</center>","metadata":{"papermill":{"duration":0.044853,"end_time":"2021-02-05T02:18:26.044977","exception":false,"start_time":"2021-02-05T02:18:26.000124","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>\n\n<h2 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\">1.3  ADDITIONAL INFORMATION ON ABNORMALITIES</h2>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">Negative for Pneumonia</b>\n* No lung opacities\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">Typical Appearance</b>\n* Multifocal bilateral, peripheral opacities with rounded morphology, lower lungâ€“predominant distribution\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">Indeterminate Appearance</b>\n* Absence of typical findings AND unilateral, central or upper lung predominant distribution\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">Atypical Appearance</b>\n* Pneumothorax, pleural effusion, pulmonary edema, lobar consolidation, solitary lung nodule or mass, diffuse tiny nodules, cavity","metadata":{"papermill":{"duration":0.046066,"end_time":"2021-02-05T02:18:26.137054","exception":false,"start_time":"2021-02-05T02:18:26.090988","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"setup\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: darkred; background-color: #ffffff;\" id=\"setup\">2&nbsp;&nbsp;NOTEBOOK SETUP</h1>","metadata":{"papermill":{"duration":0.045324,"end_time":"2021-02-05T02:18:26.226344","exception":false,"start_time":"2021-02-05T02:18:26.18102","status":"completed"},"tags":[]}},{"cell_type":"code","source":"TRAIN_CSV_PATH = \"/kaggle/input/siim-covid19-updated-train-labels/updated_train_labels.csv\"\nSS_CSV_PATH = \"/kaggle/input/siim-covid19-updated-train-labels/updated_sample_submission.csv\"\n\nprint(\"\\n\\nCOMBINED AND EXPLODED TRAIN DATAFRAME\\n\\n\")\ntrain_df = pd.read_csv(TRAIN_CSV_PATH)\ndisplay(train_df)\n\nprint(\"\\n\\nSAMPLE SUBMISSION DATAFRAME\\n\\n\")\nss_df = pd.read_csv(SS_CSV_PATH)\ndisplay(ss_df)\n\nimage_df = pd.read_csv(\"/kaggle/input/siim-covid19-detection/train_image_level.csv\")\nall_image_ids = image_df.id.str.replace(\"_image\", \"\")\nbbox_image_ids = image_df.dropna().id.str.replace(\"_image\", \"\")","metadata":{"papermill":{"duration":0.717244,"end_time":"2021-02-05T02:18:26.987446","exception":false,"start_time":"2021-02-05T02:18:26.270202","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"helper_functions\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: darkred; background-color: #ffffff;\" id=\"helper_functions\">3&nbsp;&nbsp;HELPER FUNCTIONS</h1>","metadata":{"papermill":{"duration":0.045205,"end_time":"2021-02-05T02:18:27.079444","exception":false,"start_time":"2021-02-05T02:18:27.034239","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def dicom2array(path, voi_lut=True, fix_monochrome=True):\n    \"\"\" Convert dicom file to numpy array \n    \n    Args:\n        path (str): Path to the dicom file to be converted\n        voi_lut (bool): Whether or not VOI LUT is available\n        fix_monochrome (bool): Whether or not to apply monochrome fix\n        \n    Returns:\n        Numpy array of the respective dicom file \n        \n    \"\"\"\n    # Use the pydicom library to read the dicom file\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to \n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n        \n    # The XRAY may look inverted\n    #   - If we want to fix this we can\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    \n    # Normalize the image array and return\n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n\n\ndef dicom2array_2(fname, target_size=512, use_clahe=True, clip_limit=2., grid_size=(8,8)):\n    dicom = pydicom.dcmread(fname)\n    data = apply_voi_lut(dicom.pixel_array, dicom)\n    im = data - np.min(data)\n    im = 255. * im / np.max(im)\n    \n    if dicom.PhotometricInterpretation == \"MONOCHROME1\": # check for inverted image\n        im = 255. - im\n    \n    if use_clahe:\n        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)\n        climg = clahe.apply(im.astype('uint8'))\n        img = Image.fromarray(climg.astype('uint8'), 'L')\n    else:\n        img = Image.fromarray(im.astype('uint8'), 'L')\n    org_size = img.size\n    \n    if max(img.size) > target_size:\n        img.thumbnail((target_size, target_size), Image.ANTIALIAS)\n    \n    return np.asarray(img)\n\ndef get_absolute_file_paths(directory):\n    all_abs_file_paths = []\n    for dirpath,_,filenames in os.walk(directory):\n        for f in filenames:\n            all_abs_file_paths.append(os.path.abspath(os.path.join(dirpath, f)))\n    print(all_abs_file_paths)        \n    return all_abs_file_paths","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    display(study_df)\nexcept:\n    study_df = pd.read_csv(\"../input/siim-covid19-detection/train_study_level.csv\")\n    study_df = study_df[study_df.id.str.contains(\"study\")]\n    study_df[\"id\"] = study_df[\"id\"].str.replace(\"_study\", \"\")\n    study_df[\"study_dir\"] = \"/kaggle/input/siim-covid19-detection/train/\"+study_df[\"id\"]\n    study_df[\"images_per_study\"] = study_df.study_dir.progress_apply(lambda x: len(get_absolute_file_paths(x)))\nmultiple_images_per_study_df = study_df[study_df.images_per_study>1].reset_index(drop=True)\n\nfor dir_path in multiple_images_per_study_df.study_dir.values:\n    image_paths = get_absolute_file_paths(dir_path)\n    if len(image_paths)<=4:\n        plt.figure(figsize=(18,4))\n        plt.suptitle(f\"\\n\\nSTUDY: {dir_path.rsplit('/', 1)[1]}\", fontsize=16, fontweight=\"bold\")\n        for j, x in enumerate(image_paths):\n            if any(True for xx in all_image_ids if xx in x):\n                title = \"\\nINCLUDED IN IMAGE LEVEL\"\n                if any(True for xx in bbox_image_ids if xx in x):\n                    title += \" - W/ BBOX!!!\"\n            else:\n                title = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n            plt.subplot(1,4,j+1)\n            plt.imshow(dicom2array_2(x))\n            plt.axis(False)\n            plt.title(title, fontweight=\"bold\")\n    elif len(image_paths)<=8:\n        plt.figure(figsize=(18,8))\n        plt.suptitle(f\"\\n\\nSTUDY: {dir_path.rsplit('/', 1)[1]}\", fontsize=16, fontweight=\"bold\")\n        for j, x in enumerate(image_paths):\n            if any(True for xx in all_image_ids if xx in x):\n                title = \"\\nINCLUDED IN IMAGE LEVEL\"\n                if any(True for xx in bbox_image_ids if xx in x):\n                    title += \" - W/ BBOX!!!\"\n            else:\n                title = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n            plt.subplot(2,4,j+1)\n            plt.imshow(dicom2array_2(x))\n            plt.axis(False)\n            plt.title(title, fontweight=\"bold\")\n    else:\n        plt.figure(figsize=(18,12))\n        plt.suptitle(f\"\\n\\nSTUDY: {dir_path.rsplit('/', 1)[1]}\", fontsize=16, fontweight=\"bold\")\n        for j, x in enumerate(image_paths):\n            if any(True for xx in all_image_ids if xx in x):\n                title = \"\\nINCLUDED IN IMAGE LEVEL\"\n                if any(True for xx in bbox_image_ids if xx in x):\n                    title += \" - W/ BBOX!!!\"\n            else:\n                title = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n            plt.subplot(3,4,j+1)\n            plt.imshow(dicom2array_2(x))\n            plt.title(title, fontweight=\"bold\")\n            plt.axis(False)\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":0.083595,"end_time":"2021-02-05T02:18:27.209879","exception":false,"start_time":"2021-02-05T02:18:27.126284","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \nimport sys\nimport random\nimport math\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nimport pydicom\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nimport pandas as pd \nimport glob ","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":0.083595,"end_time":"2021-02-05T02:18:27.209879","exception":false,"start_time":"2021-02-05T02:18:27.126284","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input'\n\n# Directory to save logs and trained model\nROOT_DIR = '/kaggle/working'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://www.github.com/matterport/Mask_RCNN.git\nos.chdir('Mask_RCNN')\n#!python setup.py -q install","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import Mask RCNN\nsys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dicom_dir = os.path.join(DATA_DIR, 'train')\ntest_dicom_dir = os.path.join(DATA_DIR, 'test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training dataset\nanns = pd.read_csv(os.path.join(DATA_DIR, '../input/siim-covid19-updated-train-labels/updated_train_labels.csv'))\nanns.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##image_fps, image_annotations = parse_dataset(train_dicom_dir, anns=anns)\ndef get_absolute_file_paths(directory):\n    all_abs_file_paths = []\n    for dirpath,_,filenames in os.walk(directory):\n        for f in filenames:\n            all_abs_file_paths.append(os.path.abspath(os.path.join(dirpath, f)))\n    return all_abs_file_paths\n##image_fps[]\n\nimage_fps=anns.dcm_path.tolist()\n##print(image_fps)\n\n\nds = pydicom.read_file(image_fps[0])         # read dicom image from filepath \nimage = ds.pixel_array     \nds\n\n# Original DICOM image size: 1024 x 1024\nORIG_SIZE = 2048\n\n######################################################################\n# Modify this line to use more or fewer images for training/validation. \n# To use all images, do: image_fps_list = list(image_fps)\nimage_fps_list = list(image_fps[:1000]) \n#####################################################################\n\n# split dataset into training vs. validation dataset \n# split ratio is set to 0.9 vs. 0.1 (train vs. validation, respectively)\nsorted(image_fps_list)\nrandom.seed(42)\nrandom.shuffle(image_fps_list)\n\nvalidation_split = 0.1\nsplit_index = int((1 - validation_split) * len(image_fps_list))\n\nimage_fps_train = image_fps_list[:split_index]\nimage_fps_val = image_fps_list[split_index:]\n\nprint(len(image_fps_train), len(image_fps_val))\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_annotations = {fp: [] for fp in image_fps}\nfor index, row in anns.iterrows():\n    image_annotations[row[1]].append(row)\n##print(image_annotations)        \n       \n   \n\n     ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare the training dataset\ndataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_train.prepare()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(dataset_train)\nprint(dataset_train.__dict__)\nprint(dir(dataset_train))\n\n\n# Show annotation(s) for a DICOM image \ntest_fp = random.choice(image_fps_train)\nimage_annotations[test_fp]\n\n# prepare the validation dataset\ndataset_val = DetectorDataset(image_fps_val, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_val.prepare()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and display random samples and their bounding boxes\n# Suggestion: Run this a few times to see different examples. \n\nimage_id = random.choice(dataset_train.image_ids)\nimage_fp = dataset_train.image_reference(image_id)\n\n\nimage = dataset_train.load_image(image_id)\n##print(image)\nmask, class_ids = dataset_train.load_mask(image_id)\n##print(image_id)\nprint(mask.shape)\n####image.resize((1024, 1024,3))\n##print(mask)\nplt.figure(figsize=(10, 10))\nplt.subplot(1, 2, 1)\nplt.imshow(image[:, :, 0], cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nmasked = np.zeros(image.shape[:2])\n#3print(masked)\nfor i in range(mask.shape[2]):\n    masked += image[:, :, 0] * mask[:, :, i]\nplt.imshow(masked, cmap='gray')\nplt.axis('off')\n\nprint(image_fp)\nprint(class_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------Another way\n","metadata":{}},{"cell_type":"code","source":"train_image_path = os.path.join('../input/siim-covid19-detection/train')\ntest_image_path = os.path.join('../input/siim-covid19-detection/test')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DetectorDataset(utils.Dataset):\n    \"\"\"Dataset class for training pneumonia detection on the RSNA pneumonia dataset.\n    \"\"\"\n    \n    def load_labels(self, labels_list):\n        for i, label in enumerate(labels_list):\n            self.add_class('covid', i + 1, label)\n            \n    def load_dataset(self, images_obj):\n        for image_obj in images_obj:\n            image_id = image_obj['image_id']\n            image_path = image_obj['image_path']\n            num_ids = image_obj['num_ids']\n            polygons = image_obj['polygons']\n            width = image_obj['width']\n            height = image_obj['height']\n            self.add_image(\"covid\", image_id=image_id, path=image_path,\n                           width=width, height=height, polygons=polygons,num_ids=num_ids)\n            \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n\n    def draw_shape(self, image, shape, dims, color):\n        \"\"\"Draws a shape from the given specs.\"\"\"\n        # Get the center x, y and the size s\n        x, y, s = dims\n        if shape == 'square':\n            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n        elif shape == \"circle\":\n            cv2.circle(image, (x, y), s, color, -1)\n        elif shape == \"triangle\":\n            points = np.array([[(x, y-s),\n                                (x-s/math.sin(math.radians(60)), y+s),\n                                (x+s/math.sin(math.radians(60)), y+s),\n                                ]], dtype=np.int32)\n            cv2.fillPoly(image, points, color)\n        return image\n\n    def load_mask(self, image_id):\n        \"\"\"Generate instance masks for an image.\n       Returns:\n        masks: A bool array of shape [height, width, instance count] with\n            one mask per instance.\n        class_ids: a 1D array of class IDs of the instance masks.\n        \"\"\"\n        info = self.image_info[image_id]\n        num_ids = info['num_ids']\n        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n                        dtype=np.uint8)\n\n        for i, p in enumerate(info[\"polygons\"]):\n            # Get indexes of pixels inside the polygon and set them to 1\n            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n            mask[rr, cc, i] = 1\n\n        num_ids = np.array(num_ids, dtype=np.int32)\n        return mask, num_ids","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install xmltodict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = [\"negetive\", \"Typical\", \"Indeterminate\",\"Atypical\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_single_annotation(label_obj):\n    #print(label_obj)\n    name = label_obj['name']\n    # Get label\n    num_id = labels.index(name) + 1\n    bb_box = label_obj['bndbox']\n    # Extract the xmin xmax ymin and ymax of bounding box\n    xmin = int(bb_box['xmin'])\n    xmax = int(bb_box['xmax'])\n    ymin = int(bb_box['ymin'])\n    ymax = int(bb_box['ymax'])\n    # Convert it into polygon format. So we need 5 points for both x and y\n    all_points_x = [xmin, xmax, xmax, xmin, xmin]\n    all_points_y = [ymin, ymin, ymax, ymax, ymin]\n    return all_points_x, all_points_y, num_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = transform_annotations(image_annotations)\nprint(train_images[0:5])\ndataset_train = DetectorDataset()\ndataset_train.load_labels(labels)\ndataset_train.load_dataset(train_images)\ndataset_train.prepare()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"##1\n! conda install -c conda-forge gdcm -y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nIMG_FORMAT = \".dcm\"\nIMG_PATHS = []\nIMAGE_IDS = []\nIMAGE_NAMES = []\nSETS = []\nSERIES = []\nSTUDIES = []\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.endswith(IMG_FORMAT):\n            img_path = os.path.join(dirname, filename)\n            Splitted = img_path.split('/')\n            # print(Splitted)\n            img_name = os.path.basename(img_path)\n            img_id = img_name.rstrip(IMG_FORMAT)\n            series_name = Splitted[-2]\n            study_name = Splitted[-3]\n            set_name = Splitted[-4]\n            IMG_PATHS.append(img_path)\n            IMAGE_NAMES.append(img_name)\n            IMAGE_IDS.append(img_id)\n            SETS.append(set_name)\n            SERIES.append(series_name)\n            STUDIES.append(study_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##3\ndf_ext = pd.DataFrame.from_dict({\"Image_Path\":IMG_PATHS,\n                             \"Image_Name\":IMAGE_NAMES,\n                             \"Image_ID\":IMAGE_IDS,\n                             \"Set_Name\": SETS,\n                             \"Series_Name\":SERIES,\n                             \"Study_Name\":STUDIES})\ndf_ext.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##4\nimg_lvl_pth = \"/kaggle/input/siim-covid19-detection/train_image_level.csv\"\ndf_img = pd.read_csv(img_lvl_pth)\ndf_img.sort_values(by=['id'],inplace=True)\ndf_img.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##5\nstd_lvl_pth = \"/kaggle/input/siim-covid19-detection/train_study_level.csv\"\ndf_std = pd.read_csv(std_lvl_pth)\ndf_std['id'] = df_std['id'].str.replace('_study',\"\")\ndf_std.rename({'id': 'StudyInstanceUID'},axis=1, inplace=True)\ndf_std.head(3)\n# df_std.sort_values(by=['StudyInstanceUID'],inplace=True)\ndf_std.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##6\ndf = df_img.merge(df_std, on='StudyInstanceUID')\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##7\nfrom copy import deepcopy\ndf_train = deepcopy(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"string = \"a29c5a68b07b\"\nstring.zfill(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#*\ndf_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##8\ndf_ext[\"id\"] = df_ext[\"Image_Name\"].str.zfill(19)\ndf_ext.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##9\ndf_ext[\"Set_Name\"].value_counts()\ndf_test = df_ext[df_ext[\"Set_Name\"]==\"test\"]\ndf_test.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##10\ndf_train_test = deepcopy(df_train)\nCOLS = list(df_train_test.columns)\ndef subtract_lists(x,y):\n    \"\"\"Subtract Two Lists (List Difference)\"\"\"\n    return [item for item in x if item not in y]\ndef merge_list_to_dict(test_keys,test_values):\n    \"\"\"Using dictionary comprehension to merge two lists to dictionary\"\"\"\n    merged_dict = {test_keys[i]: test_values[i] for i in range(len(test_keys))}\n    return merged_dict\n# NAN_COLS = subtract_lists(COLS,[\"id\"])\nTO_ATTACH = merge_list_to_dict(COLS,[np.nan]*len(COLS))\nfor index, row in df_test.iterrows():\n    TO_ATTACH[\"id\"] = row[\"id\"]\n    df_train_test = df_train_test.append(TO_ATTACH, ignore_index = True)\ndf_train_test.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\ndf_ext[\"id\"] = df_ext[\"id\"].str.zfill(19)\ndf_ext.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\ndf = df_ext.merge(df_train_test, on='id')\ndf.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nimport cv2\nimport warnings\nwarnings.filterwarnings('ignore')\nimport shutil \nimport tensorflow as tf\n%matplotlib inline\n\n\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nimport pprint\nimport pydicom as dicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport wandb\n\nimport PIL\nfrom PIL import Image\nfrom colorama import Fore, Back, Style\nviz_counter=0\n\ndef create_dir(dir, v=1):\n    \"\"\"\n    Creates a directory without throwing an error if directory already exists.\n    dir : The directory to be created.\n    v : Verbosity\n    \"\"\"\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n        if v:\n            print(\"Created Directory : \", dir)\n        return 1\n    else:\n        if v:\n            print(\"Directory already existed : \", dir)\n        return 0\n\nvoi_lut=True\nfix_monochrome=True\n\ndef dicom_dataset_to_dict(filename):\n    \"\"\"Credit: https://github.com/pydicom/pydicom/issues/319\n               https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    \"\"\"\n    \n    dicom_header = dicom.dcmread(filename) \n    \n    #====== DICOM FILE DATA ======\n    dicom_dict = {}\n    repr(dicom_header)\n    for dicom_value in dicom_header.values():\n        if dicom_value.tag == (0x7fe0, 0x0010):\n            #discard pixel data\n            continue\n        if type(dicom_value.value) == dicom.dataset.Dataset:\n            dicom_dict[dicom_value.name] = dicom_dataset_to_dict(dicom_value.value)\n        else:\n            v = _convert_value(dicom_value.value)\n            dicom_dict[dicom_value.name] = v\n      \n    del dicom_dict['Pixel Representation']\n    \n    #====== DICOM IMAGE DATA ======\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom_header.pixel_array, dicom_header)\n    else:\n        data = dicom_header.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom_header.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data / np.max(data)\n    modified_image_data = (data * 255).astype(np.uint8)\n    \n    return dicom_dict, modified_image_data\n\ndef _sanitise_unicode(s):\n    return s.replace(u\"\\u0000\", \"\").strip()\n\ndef _convert_value(v):\n    t = type(v)\n    if t in (list, int, float):\n        cv = v\n    elif t == str:\n        cv = _sanitise_unicode(v)\n    elif t == bytes:\n        s = v.decode('ascii', 'replace')\n        cv = _sanitise_unicode(s)\n    elif t == dicom.valuerep.DSfloat:\n        cv = float(v)\n    elif t == dicom.valuerep.IS:\n        cv = int(v)\n    else:\n        cv = repr(v)\n    return cv\n\n\nimport os, fnmatch\ndef find(pattern, path):\n    \"\"\"Utility to find files wrt a regex search\"\"\"\n    result = []\n    for root, dirs, files in os.walk(path):\n        for name in files:\n            if fnmatch.fnmatch(name, pattern):\n                result.append(os.path.join(root, name))\n    return result\ndef props(arr):\n    print(\"Shape :\",arr.shape,\"Maximum :\",arr.max(),\"Minimum :\",arr.min(),\"Data Type :\",arr.dtype)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install pylibjpeg pylibjpeg-libjpeg pydicom","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pydicom import dcmread\nimport pylibjpeg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\n\"\"\"\nShapes that you wish to resize to\n\"\"\"\n\nShape_X = 1024\nShape_Y = 1024\nimage_id = []\ndim0 = []\ndim1 = []\nsplits = []\nimg_paths = []\n\nfor split in ['test', 'train']:\n    # save_dir = f'/kaggle/tmp/{split}/'\n    save_dir = f'/kaggle/working/resized_data/{split}/'\n    print(split)\n    os.makedirs(save_dir, exist_ok=True)\n    \n    for dirname, _, filenames in tqdm(os.walk(f'/kaggle/input/siim-covid19-detection/{split}')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            fpath = os.path.join(dirname, file)\n            dicom_dict, modified_image_data = dicom_dataset_to_dict(fpath)\n            res = cv2.resize(modified_image_data,(Shape_Y,Shape_X)) # cv2 has this opposite\n            save_path = os.path.join(save_dir, file.replace('dcm', 'png'))\n            cv2.imwrite(save_path,res)\n            img_id = file.replace('.dcm', '')\n            image_id.append(img_id)\n            dim0.append(modified_image_data.shape[0])\n            dim1.append(modified_image_data.shape[1])\n            img_paths.append(fpath)\n            splits.append(split)\n\"\"\"\n2475/?\n12386/?\n07:34 | 5.38it/s\n36:51 | 8.13it/s\n\"\"\"\nprint(\"Generation Complete!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nnew_df = pd.DataFrame.from_dict({'Image_Path': img_paths, 'dim0': dim0, 'dim1': dim1})\nnew_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nfinal_df = df.merge(new_df,on=\"Image_Path\")\nfinal_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ast import literal_eval","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(final_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nn = len(final_df)\n# Already defined above : Shape_Y,Shape_X = 512,512\nNEW_BOXES = []\nfor i in range(n):\n    if type(final_df['boxes'][i])==str:\n        boxes = literal_eval(final_df['boxes'][i])\n        BIG_BOX = []\n        for box in boxes:\n            xbase,ybase = (box['x']*(Shape_Y/final_df['dim1'][i]), box['y']*(Shape_X/final_df['dim0'][i]))\n            new_width,new_height = box['width']*(Shape_Y/final_df['dim1'][i]), box['height']*(Shape_X/final_df['dim0'][i])\n            CURR_BOX = {\"x\": xbase,\n                        \"y\" : ybase,\n                        \"width\" : new_width,\n                        \"height\" : new_height}\n            BIG_BOX.append(CURR_BOX)\n    else:\n        BIG_BOX = \"\"\n    NEW_BOXES.append(str(BIG_BOX))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nfinal_df['corrected_boxes'] = NEW_BOXES","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(final_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*# Correction Factors\nfinal_df['cfy'] = Shape_Y/final_df['dim1']\nfinal_df['cfx'] = Shape_X/final_df['dim0']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install openpyxl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nfinal_df.to_csv('Extracted_Study_Series_Img.csv',index=False)\nfinal_df.to_excel('Extracted_Study_Series_Img.xlsx',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nimport os \nimport sys\nimport random\nimport math\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nimport pydicom\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nimport pandas as pd \nimport glob ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nDATA_DIR = '/kaggle/working'\n\n# Directory to save logs and trained model\nROOT_DIR = '/kaggle/working'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\n!git clone https://www.github.com/matterport/Mask_RCNN.git\n####https://github.com/matterport/Mask_RCNN\n##!git clone https://github.com/leekunhee/Mask_RCNN\nos.chdir('Mask_RCNN')\n#!python setup.py -q install","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\n# Import Mask RCNN\nsys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nclass DetectorConfig(Config):\n    \"\"\"Configuration for training pneumonia detection on the RSNA pneumonia dataset.\n    Overrides values in the base Config class.\n    \"\"\"\n    \n    # Give the configuration a recognizable name  \n    NAME = 'covid'\n    \n    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 8 \n    \n    BACKBONE = 'resnet50'\n    \n    NUM_CLASSES = 1+4  # background + 1 pneumonia classes\n    \n    IMAGE_MIN_DIM = 256\n    IMAGE_MAX_DIM = 256\n    ##RPN_ANCHOR_SCALES = (32, 64, 128, 256)\n    TRAIN_ROIS_PER_IMAGE = 32\n    MAX_GT_INSTANCES = 3\n    DETECTION_MAX_INSTANCES = 3\n    DETECTION_MIN_CONFIDENCE = 0.9\n    DETECTION_NMS_THRESHOLD = 0.1\n    LEARNING_RATE  = 0.1\n    STEPS_PER_EPOCH = 100\n    \nconfig = DetectorConfig()\nconfig.display()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DetectorDataset(utils.Dataset):\n    \"\"\"Dataset class for training pneumonia detection on the RSNA pneumonia dataset.\n    \"\"\"\n\n    def __init__(self, image_fps, image_annotations, orig_height, orig_width):\n        super().__init__(self)\n        \n        # Add classes\n        self.add_class('covid', 1, 'negetive')\n        self.add_class('covid', 2, 'typical')\n        self.add_class('covid', 3, 'intermediate')\n        self.add_class('covid', 4, 'atypical')\n   \n        # add images \n        for i, fp in enumerate(image_fps):\n            print(i)\n            annotations = image_annotations[fp]\n            self.add_image('covid', image_id=i, path=fp, \n                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n            \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n\n    def load_image(self, image_id):\n        info = self.image_info[image_id]\n        fp = info['path']\n        ds = pydicom.read_file(fp)\n        image = ds.pixel_array\n        # If grayscale. Convert to RGB for consistency.\n        if len(image.shape) != 3 or image.shape[2] != 3:\n            image = np.stack((image,) * 3, -1)\n        return image\n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        annotations = info['annotations']\n        count = len(annotations)\n        if count == 0:\n            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n            class_ids = np.zeros((1,), dtype=np.int32)\n        else:\n            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n            class_ids = np.zeros((count,), dtype=np.int32)\n            for i, a in enumerate(annotations):\n                    x = int(a['x'])\n                    y = int(a['y'])\n                    w = int(a['width'])\n                    h = int(a['height'])\n                    mask_instance = mask[:, :, i].copy()\n                    cv2.rectangle(mask_instance, (x, y), (x+w, y+h), 255, -1)\n                    mask[:, :, i] = mask_instance\n                    class_ids[i] = i\n        return mask.astype(np.bool), class_ids.astype(np.int32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dicom_fps(dicom_dir):\n    dicom_fps = glob.glob(dicom_dir+'/'+'*.png')\n    return list(set(dicom_fps))\n\ndef parse_dataset(dicom_dir, anns): \n    image_fps = get_dicom_fps(dicom_dir)\n    ##print(image_fps)\n    image_annotations = {}\n    for index, row in anns.iterrows(): \n        fp = os.path.join(dicom_dir, row['Image_ID']+'.png')\n        print(fp)\n        print(row)\n        image_annotations[fp]=(row)\n    return image_fps, image_annotations ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimage_fps, image_annotations = parse_dataset(train_dicom_dir, anns=anns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\n# training dataset\nDTA_DIR= '/kaggle/input/siim-covid19-updated-train-labels'\n##../input/siim-covid19-updated-train-labels/updated_train_labels.csv\nanns = pd.read_csv(os.path.join(DTA_DIR, 'updated_train_labels.csv'))\nanns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\ndef get_dicom_fps(dicom_dir):\n    dicom_fps = glob.glob(dicom_dir+'/'+'*.png')\n    return list(set(dicom_fps))\n\ndef parse_dataset(dicom_dir, anns): \n    image_fps = get_dicom_fps(dicom_dir)\n    image_annotations = {fp: [] for fp in image_fps}\n    for index, row in anns.iterrows(): \n        fp = os.path.join(dicom_dir, row['id']+'.png')\n        ##print(row)\n        image_annotations[fp].append(row)\n    return image_fps, image_annotations ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\ntrain_dicom_dir = os.path.join(DATA_DIR, 'resized_data/train')\ntest_dicom_dir = os.path.join(DATA_DIR, 'resized_data/test')\nimage_fps, image_annotations = parse_dataset(train_dicom_dir, anns=anns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##image_fpst,image_annotationst=parse_dataset(test_dicom_dir,anns=anns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nprint(image_annotations['/kaggle/working/resized_data/train/149356f04849.png'])\n\nimport numpy as np\n\nimage = cv2.imread(image_fps[1])\nprint(image_annotations[image_fps[1]])\nplt.imshow(image)\nplt.show()\ndata = np.asarray( image, dtype='uint8' )\ndata\n\n# Original DICOM image size: 1024 x 1024\nORIG_SIZE = 1024\n\n##print(image_annotations)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(image_fps)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nimage_fps_list = list(image_fps[:6334]) \n#####################################################################\n\n# split dataset into training vs. validation dataset \n# split ratio is set to 0.9 vs. 0.1 (train vs. validation, respectively)\nsorted(image_fps_list)\nrandom.seed(42)\nrandom.shuffle(image_fps_list)\n\nvalidation_split = 0.1\nsplit_index = int((1 - validation_split) * len(image_fps_list))\n\nimage_fps_train = image_fps_list[:split_index]\nimage_fps_val = image_fps_list[split_index:]\n\nprint(len(image_fps_train), len(image_fps_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(image_fps_train)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_train.prepare()\n\ntest_fp = random.choice(image_fps_train)\nimage_annotations[test_fp]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndataset_val = DetectorDataset(image_fps_val, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_val.prepare()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nclasslabel={'negative':1,'typical':2,'indeterminate':3,'atypical':4}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nclass DetectorDataset(utils.Dataset):\n    \n    \"\"\"Dataset class for training pneumonia detection on the RSNA pneumonia dataset.\n    \"\"\"\n    def __init__(self, image_fps, image_annotations, orig_height, orig_width):\n        super().__init__(self)\n        # Add classes\n        self.add_class('covid', 1, 'negative')\n        self.add_class('covid', 2, 'typical')\n        self.add_class('covid', 3, 'indeterminate')\n        self.add_class('covid', 4, 'atypical')\n   \n        # add images \n        for i, fp in enumerate(image_fps):\n            annotations = image_annotations[fp]\n            ##print(fp)\n            self.add_image('covid', image_id=i, path=fp, \n                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n            \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n    \n    def load_image(self, image_id):\n        info = self.image_info[image_id]\n        fp = info['path']\n        image = cv2.imread(fp)\n        ##plt.imshow(image)\n        ##plt.show()\n        data = np.asarray( image, dtype='uint8' )\n        # If grayscale. Convert to RGB for consistency.\n        if len(data.shape) != 3 or data.shape[2] != 3:\n            data = np.stack((data,) * 3, -1)\n        return data\n    \n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        annotations = info['annotations']\n        ##print(annotations)\n        fp = info['path']\n        image = cv2.imread(fp)\n        data = np.asarray( image, dtype='uint8' )\n        im_h,im_w,_=image.shape\n        s_x=info['orig_width']\n        s_y=info['orig_height']\n        ##print(s_x,s_y)\n        count = len(annotations)\n        if count == 0:\n            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n            class_ids = np.zeros((1,), dtype=np.int32)\n        else:\n            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n            class_ids = np.zeros((count,), dtype=np.int32)\n            for i, a in enumerate(annotations):\n                print(classlabel[a[6]])\n                X = (a[2])\n                Y = (a[3])\n                ## print(x)\n                ##print(y)\n                W = a[4]\n                H = a[5]\n                final_d=a[15]  ##final_df['dim1'][i]\n                final_f=a[16]  ##final_df['dim0'][i]\n                xbase,ybase = (X*(s_y/final_d), Y*(s_x/final_f))\n                new_width,new_height = W*(s_y/final_d), H*(s_x/final_f)\n                ##print(xbase,\" \",ybase,\" \",new_width,\" \",new_height)\n                x = int(xbase)\n                y = int(ybase)\n                w = int(new_width)\n                h = int(new_height)\n                mask_instance = mask[:, :, i].copy()\n                ##cv2.rectangle(frame, (startX, startY), (endX, endY), (155, 255, 0), 2)\"\"\"\n                cv2.rectangle(mask_instance, (x, y), (w, h), 255, -1)\n                mask[:, :, i] = mask_instance\n                clas=a[6]\n                class_ids[i] = classlabel[clas]\n        return mask.astype(np.bool), class_ids.astype(np.int32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\ndataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_train.prepare()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ValueError: 'white' is not a valid value for name; supported values are 'Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap', 'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', 'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', 'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', 'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', 'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', 'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', 'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', 'coolwarm_r', 'copper', 'copper_r', 'crest', 'crest_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'flare', 'flare_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', 'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', 'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', 'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', 'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', 'rocket_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', 'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', 'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r'","metadata":{}},{"cell_type":"code","source":"!pip install keras==2.2.5\n\n!pip uninstall tensorflow\n!pip install tensorflow== 2.4.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!pip install tensorflow== 2.4.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nimage_id = random.choice(dataset_train.image_ids)\nimage_fp = dataset_train.image_reference(image_id)\nimage = dataset_train.load_image(image_id)\nmask, class_ids = dataset_train.load_mask(image_id)\n\nprint(image.shape)\n\nplt.figure(figsize=(10, 10))\nplt.subplot(1, 2, 1)\nplt.imshow(image[:, :, 0], cmap='gray')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nmasked = np.zeros(image.shape[:2])\nfor i in range(mask.shape[2]):\n    masked += image[:, :, 0] * mask[:, :, i]\nplt.imshow(masked, cmap='gray')\nplt.axis('off')\n\nprint(image_fp)\nprint(class_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\n# prepare the validation dataset\ndataset_val = DetectorDataset(image_fps_val, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_val.prepare()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dataset_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nROOT_DIR = '/kaggle/working'\nmodel = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\n# Image augmentation \naugmentation = iaa.SomeOf((0, 1), [\n    iaa.Fliplr(0.5),\n    iaa.Affine(\n        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n        rotate=(-25, 25),\n        shear=(-8, 8)\n    ),\n    iaa.Multiply((0.9, 1.1))\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nNUM_EPOCHS = 1\n\n# Train Mask-RCNN Model \nimport warnings \nwarnings.filterwarnings(\"ignore\")\nmodel.train(dataset_train, dataset_val, \n            learning_rate=config.LEARNING_RATE, \n            epochs=NUM_EPOCHS, \n            layers='all',\n           augmentation=augmentation)\n","metadata":{"execution":{"iopub.status.idle":"2021-06-20T13:21:09.812545Z","shell.execute_reply.started":"2021-06-20T12:31:56.67122Z","shell.execute_reply":"2021-06-20T13:21:09.811146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##/kaggle/working/covid20210618T0559/mask_rcnn_covid_{epoch:04d}.h5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\n# select trained model \ndir_names = next(os.walk(model.model_dir))[1]\nkey = config.NAME.lower()\ndir_names = filter(lambda f: f.startswith(key), dir_names)\ndir_names = sorted(dir_names)\n\nif not dir_names:\n    import errno\n    raise FileNotFoundError(\n        errno.ENOENT,\n        \"Could not find model directory under {}\".format(self.model_dir))\n    \nfps = []\n# Pick last directory\nfor d in dir_names: \n    dir_name = os.path.join(model.model_dir, d)\n    # Find the last checkpoint\n    checkpoints = next(os.walk(dir_name))[2]\n    checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n    checkpoints = sorted(checkpoints)\n    if not checkpoints:\n        print('No weight files in {}'.format(dir_name))\n    else: \n      \n      checkpoint = os.path.join(dir_name, checkpoints[-1])\n      fps.append(checkpoint)\n\nmodel_path = sorted(fps)[0]\nprint('Found model {}'.format(model_path))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T13:23:14.639559Z","iopub.execute_input":"2021-06-20T13:23:14.639974Z","iopub.status.idle":"2021-06-20T13:23:14.65061Z","shell.execute_reply.started":"2021-06-20T13:23:14.639923Z","shell.execute_reply":"2021-06-20T13:23:14.649958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\nclass InferenceConfig(DetectorConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\ninference_config = InferenceConfig()\n\n# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=ROOT_DIR)\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodal_path='/kaggle/working/covid20210618T0559/mask_rcnn_covid_{epoch:04d}.h5';\nmodel.load_weights(model_path, by_name=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T13:23:19.574154Z","iopub.execute_input":"2021-06-20T13:23:19.574793Z","iopub.status.idle":"2021-06-20T13:23:29.325231Z","shell.execute_reply.started":"2021-06-20T13:23:19.574757Z","shell.execute_reply":"2021-06-20T13:23:29.324524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\n# set color for class\ndef get_colors_for_class_ids(class_ids):\n    colors = []\n    for class_id in class_ids:\n        if class_id == 1:\n            colors.append((.941, .204, .204))\n    return colors","metadata":{"execution":{"iopub.status.busy":"2021-06-20T13:23:29.32635Z","iopub.execute_input":"2021-06-20T13:23:29.326722Z","iopub.status.idle":"2021-06-20T13:23:29.330486Z","shell.execute_reply.started":"2021-06-20T13:23:29.326693Z","shell.execute_reply":"2021-06-20T13:23:29.329847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\n# Show few example of ground truth vs. predictions on the validation dataset \ndataset = dataset_val\nfig = plt.figure(figsize=(10, 30))\n\nfor i in range(4):\n\n    image_id = random.choice(dataset.image_ids)\n    \n    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n        modellib.load_image_gt(dataset_val, inference_config, \n                               image_id, use_mini_mask=False)\n    \n    print(original_image.shape)\n    plt.subplot(6, 2, 2*i + 1)\n    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n                                dataset.class_names,\n                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n    \n    plt.subplot(6, 2, 2*i + 2)\n    results = model.detect([original_image]) #, verbose=1)\n    r = results[0]\n    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], \n                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])","metadata":{"execution":{"iopub.status.busy":"2021-06-20T13:23:33.029531Z","iopub.execute_input":"2021-06-20T13:23:33.03003Z","iopub.status.idle":"2021-06-20T13:23:39.086022Z","shell.execute_reply.started":"2021-06-20T13:23:33.029999Z","shell.execute_reply":"2021-06-20T13:23:39.085233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##*\ntest_image_fps = get_dicom_fps(test_dicom_dir)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T13:24:06.078822Z","iopub.execute_input":"2021-06-20T13:24:06.079544Z","iopub.status.idle":"2021-06-20T13:24:06.095513Z","shell.execute_reply.started":"2021-06-20T13:24:06.079492Z","shell.execute_reply":"2021-06-20T13:24:06.094545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions on test images, write out sample submission \ndef predict(image_fps, filepath='submission.csv', min_conf=0.95): \n    \n    # assume square image\n    resize_factor = ORIG_SIZE / config.IMAGE_SHAPE[0]\n    #resize_factor = ORIG_SIZE \n    with open(filepath, 'w') as file:\n     for image_id in tqdm(image_fps): \n        ds = pydicom.read_file(image_id)\n        image = ds.pixel_array\n        # If grayscale. Convert to RGB for consistency.\n        if len(image.shape) != 3 or image.shape[2] != 3:\n            image = np.stack((image,) * 3, -1) \n        image, window, scale, padding, crop = utils.resize_image(\n            image,\n            min_dim=config.IMAGE_MIN_DIM,\n            min_scale=config.IMAGE_MIN_SCALE,\n            max_dim=config.IMAGE_MAX_DIM,\n            mode=config.IMAGE_RESIZE_MODE)\n            \n        patient_id = os.path.splitext(os.path.basename(image_id))[0]\n\n        results = model.detect([image])\n        r = results[0]\n\n        out_str = \"\"\n        out_str += patient_id \n        out_str += \",\"\n        assert( len(r['rois']) == len(r['class_ids']) == len(r['scores']) )\n        if len(r['rois']) == 0: \n            pass\n        else: \n            num_instances = len(r['rois'])\n  \n            for i in range(num_instances): \n                if r['scores'][i] > min_conf: \n                    out_str += ' '\n                    out_str += str(round(r['scores'][i], 2))\n                    out_str += ' '\n\n                    # x1, y1, width, height \n                    x1 = r['rois'][i][1]\n                    y1 = r['rois'][i][0]\n                    width = r['rois'][i][3] - x1 \n                    height = r['rois'][i][2] - y1 \n                    bboxes_str = \"{} {} {} {}\".format(x1*resize_factor, y1*resize_factor, \\\n                                                       width*resize_factor, height*resize_factor)   \n#                     bboxes_str = \"{} {} {} {}\".format(x1, y1, \\\n#                                                       width, height)\n                    out_str += bboxes_str\n\n        file.write(out_str+\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T13:25:35.413514Z","iopub.execute_input":"2021-06-20T13:25:35.413893Z","iopub.status.idle":"2021-06-20T13:25:35.427723Z","shell.execute_reply.started":"2021-06-20T13:25:35.413855Z","shell.execute_reply":"2021-06-20T13:25:35.426577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict only the first 50 entries\nsubmission_fp = os.path.join(ROOT_DIR, 'submission.csv')\nprint(submission_fp)\npredict(test_image_fps, filepath=submission_fp)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T13:26:03.908357Z","iopub.execute_input":"2021-06-20T13:26:03.908778Z","iopub.status.idle":"2021-06-20T13:26:04.015294Z","shell.execute_reply.started":"2021-06-20T13:26:03.908739Z","shell.execute_reply":"2021-06-20T13:26:04.013457Z"},"trusted":true},"execution_count":null,"outputs":[]}]}