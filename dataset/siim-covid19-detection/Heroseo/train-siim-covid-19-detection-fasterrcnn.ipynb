{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"width: 100%\">\n     <center>\n    <img style=\"width: 100%\" src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/26680/logos/header.png?t=2021-04-23-22-04-05\"/>\n    </center>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<h1 id=\"title\" style=\"color:white;background:black;\">\n    </br>\n    <center>\n        SIIM-FISABIO-RSNA COVID-19 Detection\n    </center>\n</h1>\n<h1>\n    <center>\n        [Train] Starter using Faster-RCNNðŸ”¥\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"Hi, This is starter notebook using `Faster-RCNN for train`.\n\nThere are a lot of parts to improve. :)\n\np.s. The `inference notebook` will be released later.\n\n\n> [Credits]\n> - https://www.kaggle.com/shonenkov/training-efficientdet\n> - https://www.kaggle.com/xhlulu/siim-covid19-resized-to-256px-jpg\n> - https://www.kaggle.com/dschettler8845/siim-covid19-updated-train-labels\n\n## If this kernel is useful, <font color='orange'>please upvote</font>!","metadata":{}},{"cell_type":"markdown","source":"## My other notebook\n- [SIIM-FISABIO-RSNA COVID-19 Detection - Basic EDAðŸ”Ž](https://www.kaggle.com/piantic/siim-fisabio-rsna-covid-19-detection-basic-eda)","metadata":{}},{"cell_type":"markdown","source":"`V2` - Initial version\n\n`V3` - Fix a bug for image view.","metadata":{}},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"def get_train_file_path(image_id):\n    return \"../input/siim-covid19-resized-to-256px-jpg/train/{}.jpg\".format(image_id)\n\ndef get_test_file_path(image_id):\n    return \"../input/siim-covid19-resized-to-256px-jpg/test/{}.jpg\".format(image_id)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:32.204105Z","iopub.execute_input":"2021-05-21T11:21:32.204546Z","iopub.status.idle":"2021-05-21T11:21:32.217317Z","shell.execute_reply.started":"2021-05-21T11:21:32.204456Z","shell.execute_reply":"2021-05-21T11:21:32.21656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nupdated_train_labels = pd.read_csv('../input/siim-covid19-updated-train-labels/updated_train_labels.csv')\n\nupdated_train_labels['jpg_path'] = updated_train_labels['id'].apply(get_train_file_path)\ntrain = updated_train_labels.copy()\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:32.219296Z","iopub.execute_input":"2021-05-21T11:21:32.21967Z","iopub.status.idle":"2021-05-21T11:21:32.305829Z","shell.execute_reply.started":"2021-05-21T11:21:32.219632Z","shell.execute_reply":"2021-05-21T11:21:32.304866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nfrom datetime import datetime\nimport time\nimport random\nfrom tqdm import tqdm_notebook as tqdm # progress bar\nimport matplotlib.pyplot as plt\n\n# torch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\n\n# torchvision\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\n# sklearn\nfrom sklearn.model_selection import StratifiedKFold\n\n# CV\nimport cv2\n\n# Albumenatations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n#from pycocotools.coco import COCO\nfrom sklearn.model_selection import StratifiedKFold\n\n# glob\nfrom glob import glob\n\n# numba\nimport numba\nfrom numba import jit\n\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:32.307893Z","iopub.execute_input":"2021-05-21T11:21:32.30825Z","iopub.status.idle":"2021-05-21T11:21:33.631675Z","shell.execute_reply.started":"2021-05-21T11:21:32.308211Z","shell.execute_reply":"2021-05-21T11:21:33.630819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CFG","metadata":{}},{"cell_type":"code","source":"class DefaultConfig:\n    n_folds: int = 5\n    seed: int = 2021\n    num_classes: int = 4 # \"negative\", \"typical\", \"indeterminate\", \"atypical\"\n    img_size: int = 256\n    fold_num: int = 0\n    device: str = 'cuda:0'","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:33.633275Z","iopub.execute_input":"2021-05-21T11:21:33.633598Z","iopub.status.idle":"2021-05-21T11:21:33.640109Z","shell.execute_reply.started":"2021-05-21T11:21:33.633561Z","shell.execute_reply":"2021-05-21T11:21:33.63932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(DefaultConfig.device) if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:33.641568Z","iopub.execute_input":"2021-05-21T11:21:33.641947Z","iopub.status.idle":"2021-05-21T11:21:33.68011Z","shell.execute_reply.started":"2021-05-21T11:21:33.641909Z","shell.execute_reply":"2021-05-21T11:21:33.679289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Choose your optimizers:\nAdam = False\nif Adam: \n    Adam_config = {\"lr\" : 0.001, \"betas\" : (0.9, 0.999), \"eps\" : 1e-08}\nelse:\n    SGD_config = {\"lr\" : 0.001, \"momentum\" : 0.9, \"weight_decay\" : 0.001}","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:33.682022Z","iopub.execute_input":"2021-05-21T11:21:33.682594Z","iopub.status.idle":"2021-05-21T11:21:33.690308Z","shell.execute_reply.started":"2021-05-21T11:21:33.682556Z","shell.execute_reply":"2021-05-21T11:21:33.689393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(DefaultConfig.seed)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:33.691583Z","iopub.execute_input":"2021-05-21T11:21:33.692018Z","iopub.status.idle":"2021-05-21T11:21:33.70223Z","shell.execute_reply.started":"2021-05-21T11:21:33.691982Z","shell.execute_reply":"2021-05-21T11:21:33.701424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split","metadata":{}},{"cell_type":"code","source":"df_folds = train.copy()\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=DefaultConfig.seed)\nfor n, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds.integer_label)):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = int(n)\ndf_folds['fold'] = df_folds['fold'].astype(int)\nprint(df_folds.groupby(['fold', df_folds.integer_label]).size())","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:33.70491Z","iopub.execute_input":"2021-05-21T11:21:33.705218Z","iopub.status.idle":"2021-05-21T11:21:33.729772Z","shell.execute_reply.started":"2021-05-21T11:21:33.705192Z","shell.execute_reply":"2021-05-21T11:21:33.728973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Albumentations","metadata":{}},{"cell_type":"code","source":"def get_train_transforms():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5), \n        A.VerticalFlip(p=0.5),\n        A.OneOf([\n            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2,\n                                 val_shift_limit=0.2, p=0.3), \n            A.RandomBrightnessContrast(brightness_limit=0.2,  \n                                       contrast_limit=0.2, p=0.3),\n        ], p=0.2),\n        A.Resize(height=DefaultConfig.img_size, width=DefaultConfig.img_size, p=1.0),\n        #A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0),\n    ],\n    p=1.0, bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transforms():\n    return A.Compose([\n        A.Resize(height=DefaultConfig.img_size, width=DefaultConfig.img_size, p=1.0),\n        #A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:33.732798Z","iopub.execute_input":"2021-05-21T11:21:33.733067Z","iopub.status.idle":"2021-05-21T11:21:33.74004Z","shell.execute_reply.started":"2021-05-21T11:21:33.733038Z","shell.execute_reply":"2021-05-21T11:21:33.738984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset & DataLoader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n\n    def __init__(self, image_ids, df, transforms=None, test=False):\n        super().__init__()\n\n        self.image_ids = image_ids\n        self.df = df\n        self.file_names = df['jpg_path'].values\n        self.transforms = transforms\n        self.test = test\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        \n        image, boxes, labels = self.load_image_and_boxes(index)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = torch.tensor(labels)\n        target['image_id'] = torch.tensor([index])\n\n        if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                })\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    break\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def load_image_and_boxes(self, index):\n        image_id = self.image_ids[index]\n        image = cv2.imread(self.file_names[index], cv2.IMREAD_COLOR).copy().astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        records = self.df[self.df['id'] == image_id]       \n        boxes = []\n        for bbox in records[['frac_xmin', 'frac_ymin', 'frac_xmax', 'frac_ymax']].values:\n            bbox = np.clip(bbox, 0, 1.0)\n            temp = A.convert_bbox_from_albumentations(bbox, 'pascal_voc', image.shape[0], image.shape[0]) \n            boxes.append(temp)\n        '''\n        [0: 'atypical', 1: 'indeterminate', 2: 'negative', 3: 'typical']\n        '''\n        labels = records['integer_label'].values\n        return image, boxes, labels","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:33.742282Z","iopub.execute_input":"2021-05-21T11:21:33.743011Z","iopub.status.idle":"2021-05-21T11:21:33.757225Z","shell.execute_reply.started":"2021-05-21T11:21:33.742972Z","shell.execute_reply":"2021-05-21T11:21:33.756374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_folds = df_folds.set_index('id')\n\ndef get_train_dataset(fold_number):    \n    return CustomDataset(\n        image_ids = df_folds[df_folds['fold'] != fold_number].index.values,\n        df = train,\n        transforms = get_train_transforms()\n    )\n\ndef get_validation_dataset(fold_number):\n    return CustomDataset(\n        image_ids = df_folds[df_folds['fold'] == fold_number].index.values,\n        df = train,\n        transforms = get_valid_transforms()\n    )\n\ndef get_train_data_loader(train_dataset, batch_size=16):\n    return DataLoader(\n        train_dataset,\n        batch_size = batch_size,\n        shuffle = False,\n        num_workers = 4,\n        collate_fn = collate_fn\n    )\n\ndef get_validation_data_loader(valid_dataset, batch_size=16):\n    return DataLoader(\n        valid_dataset,\n        batch_size = batch_size,\n        shuffle = False,\n        num_workers = 4,\n        collate_fn = collate_fn\n    )    \n\ndef collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:33.758432Z","iopub.execute_input":"2021-05-21T11:21:33.758802Z","iopub.status.idle":"2021-05-21T11:21:33.77313Z","shell.execute_reply.started":"2021-05-21T11:21:33.758766Z","shell.execute_reply":"2021-05-21T11:21:33.772324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Show One Image using Dataset","metadata":{}},{"cell_type":"code","source":"train_dataset = get_train_dataset(0)\n\nimage, target, image_id = train_dataset[2]\nboxes = target['boxes'].cpu().numpy().astype(np.int32)\n\nnumpy_image = image.permute(1,2,0).cpu().numpy()\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(numpy_image, (box[0], box[1]), (box[2],  box[3]), (0, 255, 0), 2)\n    \nax.set_axis_off()\nax.imshow(numpy_image);","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:33.77443Z","iopub.execute_input":"2021-05-21T11:21:33.774773Z","iopub.status.idle":"2021-05-21T11:21:33.948894Z","shell.execute_reply.started":"2021-05-21T11:21:33.774739Z","shell.execute_reply":"2021-05-21T11:21:33.948049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:33.949858Z","iopub.execute_input":"2021-05-21T11:21:33.9501Z","iopub.status.idle":"2021-05-21T11:21:33.957592Z","shell.execute_reply.started":"2021-05-21T11:21:33.95007Z","shell.execute_reply":"2021-05-21T11:21:33.956708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Show Images using Dataloader","metadata":{}},{"cell_type":"code","source":"n_rows=4\nn_cols=4\n\n# create train dataset and data-loader\ntrain_dataset = get_train_dataset(fold_number=DefaultConfig.fold_num)\ntrain_data_loader = get_train_data_loader(train_dataset, batch_size=16)\n\nimages, targets, image_ids = next(iter(train_data_loader))\n\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n# plot some augmentations!\nfig, ax = plt.subplots(figsize=(20, 20),  nrows=n_rows, ncols=n_cols)\nfor i in range (n_rows*n_cols):    \n    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n    sample = images[i].permute(1,2,0).cpu().numpy()\n    for box in boxes:\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (255, 0, 0), 3)\n    \n    ax[i // n_rows][i % n_cols].imshow(sample)   ","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:33.958937Z","iopub.execute_input":"2021-05-21T11:21:33.959547Z","iopub.status.idle":"2021-05-21T11:21:38.481005Z","shell.execute_reply.started":"2021-05-21T11:21:33.959507Z","shell.execute_reply":"2021-05-21T11:21:38.480021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metric","metadata":{}},{"cell_type":"markdown","source":"I will use the metric of `Global Wheat Detection` for implementing it easily.","metadata":{}},{"cell_type":"code","source":"'''\nhttps://www.kaggle.com/pestipeti/competition-metric-details-script\n'''\n\n@jit(nopython=True)\ndef calculate_iou(gt, pr, form='pascal_voc') -> float:\n    \"\"\"Calculates the Intersection over Union.\n\n    Args:\n        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n        form: (str) gt/pred coordinates format\n            - pascal_voc: [xmin, ymin, xmax, ymax]\n            - coco: [xmin, ymin, w, h]\n    Returns:\n        (float) Intersection over union (0.0 <= iou <= 1.0)\n    \"\"\"\n    if form == 'coco':\n        gt = gt.copy()\n        pr = pr.copy()\n\n        gt[2] = gt[0] + gt[2]\n        gt[3] = gt[1] + gt[3]\n        pr[2] = pr[0] + pr[2]\n        pr[3] = pr[1] + pr[3]\n\n    # Calculate overlap area\n    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n    \n    if dx < 0:\n        return 0.0\n    \n    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n\n    if dy < 0:\n        return 0.0\n\n    overlap_area = dx * dy\n\n    # Calculate union area\n    union_area = (\n            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n            overlap_area\n    )\n\n    return overlap_area / union_area\n\n@jit(nopython=True)\ndef find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None) -> int:\n    \"\"\"Returns the index of the 'best match' between the\n    ground-truth boxes and the prediction. The 'best match'\n    is the highest IoU. (0.0 IoUs are ignored).\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        pred: (List[Union[int, float]]) Coordinates of the predicted box\n        pred_idx: (int) Index of the current predicted box\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (int) Index of the best match GT box (-1 if no match above threshold)\n    \"\"\"\n    best_match_iou = -np.inf\n    best_match_idx = -1\n\n    for gt_idx in range(len(gts)):\n        \n        if gts[gt_idx][0] < 0:\n            # Already matched GT-box\n            continue\n        \n        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n\n        if iou < 0:\n            iou = calculate_iou(gts[gt_idx], pred, form=form)\n            \n            if ious is not None:\n                ious[gt_idx][pred_idx] = iou\n\n        if iou < threshold:\n            continue\n\n        if iou > best_match_iou:\n            best_match_iou = iou\n            best_match_idx = gt_idx\n\n    return best_match_idx\n\n@jit(nopython=True)\ndef calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None) -> float:\n    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n = len(preds)\n    tp = 0\n    fp = 0\n    \n    # for pred_idx, pred in enumerate(preds_sorted):\n    for pred_idx in range(n):\n\n        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n                                            threshold=threshold, form=form, ious=ious)\n\n        if best_match_gt_idx >= 0:\n            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n            tp += 1\n            # Remove the matched GT box\n            gts[best_match_gt_idx] = -1\n\n        else:\n            # No match\n            # False positive: indicates a predicted box had no associated gt box.\n            fp += 1\n\n    # False negative: indicates a gt box had no associated predicted box.\n    fn = (gts.sum(axis=1) > 0).sum()\n\n    return tp / (tp + fp + fn)\n\n\n@jit(nopython=True)\ndef calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco') -> float:\n    \"\"\"Calculates image precision.\n       The mean average precision at different intersection over union (IoU) thresholds.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        thresholds: (float) Different thresholds\n        form: (str) Format of the coordinates\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n_threshold = len(thresholds)\n    image_precision = 0.0\n    \n    ious = np.ones((len(gts), len(preds))) * -1\n    # ious = None\n\n    for threshold in thresholds:\n        precision_at_threshold = calculate_precision(gts.copy(), preds, threshold=threshold,\n                                                     form=form, ious=ious)\n        image_precision += precision_at_threshold / n_threshold\n\n    return image_precision","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T11:21:38.48271Z","iopub.execute_input":"2021-05-21T11:21:38.483065Z","iopub.status.idle":"2021-05-21T11:21:38.510667Z","shell.execute_reply.started":"2021-05-21T11:21:38.483027Z","shell.execute_reply":"2021-05-21T11:21:38.509679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitter","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:38.512928Z","iopub.execute_input":"2021-05-21T11:21:38.513557Z","iopub.status.idle":"2021-05-21T11:21:38.52022Z","shell.execute_reply.started":"2021-05-21T11:21:38.513518Z","shell.execute_reply":"2021-05-21T11:21:38.519446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iou_thresholds = [0.5]\n\nclass EvalMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.image_precision = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, gt_boxes, pred_boxes, n=1):       \n        \"\"\" pred_boxes : need to be sorted.\"\"\"\n        \n        self.image_precision = calculate_image_precision(pred_boxes,\n                                                         gt_boxes,\n                                                         thresholds=iou_thresholds,\n                                                         form='pascal_voc')\n        self.count += n\n        self.sum += self.image_precision * n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:38.521539Z","iopub.execute_input":"2021-05-21T11:21:38.521909Z","iopub.status.idle":"2021-05-21T11:21:38.531462Z","shell.execute_reply.started":"2021-05-21T11:21:38.521872Z","shell.execute_reply":"2021-05-21T11:21:38.530594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Fitter:\n    def __init__(self, model, device, config):\n        self.config = config\n        self.epoch = 0\n\n        self.base_dir = f'./{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n        self.log_path = f'{self.base_dir}/log.txt'\n        self.best_summary_loss = 10**5\n        self.best_score = 0\n\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n        \n        # get the configured optimizer\n        if Adam:\n            self.optimizer = torch.optim.Adam(self.model.parameters(), **Adam_config)\n        else:\n            self.optimizer = torch.optim.SGD(self.model.parameters(), **SGD_config)\n\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n        self.log(f'Fold num is {DefaultConfig.fold_num}')\n\n    def fit(self, train_loader, validation_loader):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss = self.train_one_epoch(train_loader)\n            \n            if e == 0:\n                self.best_summary_loss = summary_loss.avg\n\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n\n            t = time.time()\n            _, eval_scores  = self.validation(validation_loader)\n\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, image_precision: {eval_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n            \n            #if summary_loss.avg < self.best_summary_loss:\n            if eval_scores.avg > self.best_score:\n                self.best_summary_loss = summary_loss.avg\n                self.best_score = eval_scores.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n                    os.remove(path)\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=eval_scores.avg)\n                #self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        \n        # model.eval() mode --> it will return boxes and scores.\n        # in this part, just print train_loss\n        summary_loss = AverageMeter()\n        summary_loss.update(self.best_summary_loss, self.config.batch_size)\n        \n        eval_scores = EvalMeter()\n        validation_image_precisions = []\n        \n        t = time.time()\n        for step, (images, targets, image_ids) in enumerate(val_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'val_precision: {eval_scores.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            with torch.no_grad():\n                images = torch.stack(images)\n                batch_size = images.shape[0]\n                images = images.to(self.device).float()\n                labels = [target['labels'].float() for target in targets]\n\n                \"\"\"\n                In model.train() mode, model(images)  is returning losses.\n                We are using model.eval() mode --> it will return boxes and scores. \n                \"\"\"\n                outputs = self.model(images)               \n                \n                for i, image in enumerate(images):               \n                    gt_boxes = targets[i]['boxes'].data.cpu().numpy()\n                    boxes = outputs[i]['boxes'].data.cpu().numpy()\n                    scores = outputs[i]['scores'].detach().cpu().numpy()\n                    \n                    preds_sorted_idx = np.argsort(scores)[::-1]\n                    preds_sorted_boxes = boxes[preds_sorted_idx]\n\n                    eval_scores.update(pred_boxes=preds_sorted_boxes, gt_boxes=gt_boxes)\n\n        return summary_loss, eval_scores\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n\n        t = time.time()\n        for step, (images, targets, image_ids) in enumerate(train_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            \n            images = torch.stack(images)\n            images = images.to(self.device).float()\n            batch_size = images.shape[0]\n            boxes = [target['boxes'].to(self.device).float() for target in targets]\n            labels = [target['labels'].to(self.device).float() for target in targets]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets] \n\n            self.optimizer.zero_grad()\n            \n            outputs = self.model(images, targets)\n            \n            loss = sum(loss for loss in outputs.values())\n            \n            loss.backward()\n\n            summary_loss.update(loss.detach().item(), batch_size)\n\n            self.optimizer.step()\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n        return summary_loss\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.state_dict(), #'model_state_dict': self.model.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:38.533091Z","iopub.execute_input":"2021-05-21T11:21:38.533461Z","iopub.status.idle":"2021-05-21T11:21:38.564652Z","shell.execute_reply.started":"2021-05-21T11:21:38.533426Z","shell.execute_reply":"2021-05-21T11:21:38.563801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainGlobalConfig:\n    num_workers: int = 4\n    batch_size: int = 16\n    n_epochs: int = 2 #40\n    lr: float = 0.0002\n\n    img_size = DefaultConfig.img_size\n        \n    folder = '/kaggle/working/' #folder_name \n\n    # -------------------\n    verbose = True\n    verbose_step = 1\n    # -------------------\n\n    # --------------------\n    step_scheduler = False  # do scheduler.step after optimizer.step\n    validation_scheduler = False  # do scheduler.step after validation stage loss\n\n#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n#     scheduler_params = dict(\n#         max_lr=0.001,\n#         epochs=n_epochs,\n#         steps_per_epoch=int(len(train_dataset) / batch_size),\n#         pct_start=0.1,\n#         anneal_strategy='cos', \n#         final_div_factor=10**5\n#     )\n    \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.5,\n        patience=1,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )\n    # --------------------","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:38.565936Z","iopub.execute_input":"2021-05-21T11:21:38.566323Z","iopub.status.idle":"2021-05-21T11:21:38.578086Z","shell.execute_reply.started":"2021-05-21T11:21:38.566285Z","shell.execute_reply":"2021-05-21T11:21:38.577183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class FasterRCNNDetector(torch.nn.Module):\n    def __init__(self, pretrained=False, **kwargs):\n        super(FasterRCNNDetector, self).__init__()\n        # load pre-trained model incl. head\n        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained, pretrained_backbone=pretrained)\n        \n        # get number of input features for the classifier custom head\n        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n        \n        # replace the pre-trained head with a new one\n        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, DefaultConfig.num_classes)\n        \n    def forward(self, images, targets=None):\n        return self.model(images, targets)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:38.57941Z","iopub.execute_input":"2021-05-21T11:21:38.579822Z","iopub.status.idle":"2021-05-21T11:21:38.591079Z","shell.execute_reply.started":"2021-05-21T11:21:38.579786Z","shell.execute_reply":"2021-05-21T11:21:38.590289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ndef get_model(checkpoint_path=None, pretrained=False):\n    model = FasterRCNNDetector(pretrained=pretrained)\n    \n    # Load the trained weights\n    if checkpoint_path is not None:\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n\n        del checkpoint\n        gc.collect()\n        \n    return model.cuda()\n\nnet = get_model(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:38.59204Z","iopub.execute_input":"2021-05-21T11:21:38.592357Z","iopub.status.idle":"2021-05-21T11:21:39.369772Z","shell.execute_reply.started":"2021-05-21T11:21:38.592319Z","shell.execute_reply":"2021-05-21T11:21:39.368876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def run_training(fold=0):\n    net.to(device)\n    \n    train_dataset = get_train_dataset(fold_number=fold)\n    train_data_loader = get_train_data_loader(\n        train_dataset,\n        batch_size=TrainGlobalConfig.batch_size\n    )\n    \n    validation_dataset = get_validation_dataset(fold_number=fold)\n    validation_data_loader = get_validation_data_loader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size\n    )\n\n    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n    fitter.fit(train_data_loader, validation_data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:39.372097Z","iopub.execute_input":"2021-05-21T11:21:39.372724Z","iopub.status.idle":"2021-05-21T11:21:39.378896Z","shell.execute_reply.started":"2021-05-21T11:21:39.372678Z","shell.execute_reply":"2021-05-21T11:21:39.378081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#run_training(fold=DefaultConfig.fold_num)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:39.382142Z","iopub.execute_input":"2021-05-21T11:21:39.382466Z","iopub.status.idle":"2021-05-21T11:21:39.389716Z","shell.execute_reply.started":"2021-05-21T11:21:39.382436Z","shell.execute_reply":"2021-05-21T11:21:39.388876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = open('../input/coviddetection-temp/log.txt', 'r')\nfor line in file.readlines():\n    print(line[:-1])\nfile.close()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-05-21T11:21:39.39201Z","iopub.execute_input":"2021-05-21T11:21:39.393627Z","iopub.status.idle":"2021-05-21T11:21:39.403245Z","shell.execute_reply.started":"2021-05-21T11:21:39.393587Z","shell.execute_reply":"2021-05-21T11:21:39.40221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Inference","metadata":{}},{"cell_type":"code","source":"validation_dataset = get_validation_dataset(fold_number=DefaultConfig.fold_num)\nvalidation_data_loader = get_validation_data_loader(\n    validation_dataset, \n    batch_size=TrainGlobalConfig.batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:39.404667Z","iopub.execute_input":"2021-05-21T11:21:39.405185Z","iopub.status.idle":"2021-05-21T11:21:39.415397Z","shell.execute_reply.started":"2021-05-21T11:21:39.405132Z","shell.execute_reply":"2021-05-21T11:21:39.414473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, targets, image_id = next(iter(validation_data_loader))\n\nimages = list(img.to(device) for img in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nboxes = targets[1]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[1].permute(1,2,0).cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:39.416817Z","iopub.execute_input":"2021-05-21T11:21:39.417113Z","iopub.status.idle":"2021-05-21T11:21:40.185922Z","shell.execute_reply.started":"2021-05-21T11:21:39.417088Z","shell.execute_reply":"2021-05-21T11:21:40.184909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = get_model('../input/coviddetection-temp/best-checkpoint-005epoch.bin')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:40.189964Z","iopub.execute_input":"2021-05-21T11:21:40.190243Z","iopub.status.idle":"2021-05-21T11:21:41.276719Z","shell.execute_reply.started":"2021-05-21T11:21:40.190212Z","shell.execute_reply":"2021-05-21T11:21:41.27588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net.eval()\ncpu_device = torch.device(\"cpu\")\n\noutputs = net(images)\noutputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:41.278194Z","iopub.execute_input":"2021-05-21T11:21:41.278517Z","iopub.status.idle":"2021-05-21T11:21:44.337118Z","shell.execute_reply.started":"2021-05-21T11:21:41.278479Z","shell.execute_reply":"2021-05-21T11:21:44.336275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 2)\n    \nax.set_axis_off()\nax.imshow(sample)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T11:21:50.263796Z","iopub.execute_input":"2021-05-21T11:21:50.264122Z","iopub.status.idle":"2021-05-21T11:21:50.406293Z","shell.execute_reply.started":"2021-05-21T11:21:50.26409Z","shell.execute_reply":"2021-05-21T11:21:50.405318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thank you!","metadata":{}},{"cell_type":"markdown","source":"# References\n- https://www.kaggle.com/artgor/object-detection-with-pytorch-lightning\n- https://www.kaggle.com/pestipeti/vinbigdata-fasterrcnn-pytorch-train","metadata":{}}]}