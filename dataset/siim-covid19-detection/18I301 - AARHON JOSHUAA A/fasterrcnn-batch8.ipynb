{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"width: 100%\">\n     <center>\n    <img style=\"width: 100%\" src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/26680/logos/header.png?t=2021-04-23-22-04-05\"/>\n    </center>\n</div>","metadata":{}},{"cell_type":"code","source":"# import torch\n# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:13.071618Z","iopub.execute_input":"2021-11-15T15:50:13.071933Z","iopub.status.idle":"2021-11-15T15:50:13.097157Z","shell.execute_reply.started":"2021-11-15T15:50:13.071898Z","shell.execute_reply":"2021-11-15T15:50:13.096105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"title\" style=\"color:white;background:black;\">\n    </br>\n    <center>\n        SIIM-FISABIO-RSNA COVID-19 Detection\n    </center>\n</h1>\n<h1>\n    <center>\n        Faster-RCNN\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"def get_train_file_path(image_id):\n    return \"../input/siim-covid19-resized-to-256px-jpg/train/{}.jpg\".format(image_id)\n\ndef get_test_file_path(image_id):\n    return \"../input/siim-covid19-resized-to-256px-jpg/test/{}.jpg\".format(image_id)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:13.09935Z","iopub.execute_input":"2021-11-15T15:50:13.099881Z","iopub.status.idle":"2021-11-15T15:50:13.12395Z","shell.execute_reply.started":"2021-11-15T15:50:13.099835Z","shell.execute_reply":"2021-11-15T15:50:13.12304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nupdated_train_labels = pd.read_csv('../input/siim-covid19-updated-train-labels/updated_train_labels.csv')\n\nupdated_train_labels['jpg_path'] = updated_train_labels['id'].apply(get_train_file_path)\ntrain = updated_train_labels.copy()\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:13.125106Z","iopub.execute_input":"2021-11-15T15:50:13.12597Z","iopub.status.idle":"2021-11-15T15:50:13.253677Z","shell.execute_reply.started":"2021-11-15T15:50:13.125937Z","shell.execute_reply":"2021-11-15T15:50:13.252804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nfrom datetime import datetime\nimport time\nimport random\nfrom tqdm import tqdm_notebook as tqdm # progress bar\nimport matplotlib.pyplot as plt\n\n# torch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\n\n# torchvision\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\n# sklearn\nfrom sklearn.model_selection import StratifiedKFold\n\n# CV\nimport cv2\n\n# Albumenatations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n#from pycocotools.coco import COCO\nfrom sklearn.model_selection import StratifiedKFold\n\n# glob\nfrom glob import glob\n\n# numba\nimport numba\nfrom numba import jit\n\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:13.256067Z","iopub.execute_input":"2021-11-15T15:50:13.256975Z","iopub.status.idle":"2021-11-15T15:50:13.287768Z","shell.execute_reply.started":"2021-11-15T15:50:13.256925Z","shell.execute_reply":"2021-11-15T15:50:13.286541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade wandb\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:13.289146Z","iopub.execute_input":"2021-11-15T15:50:13.289837Z","iopub.status.idle":"2021-11-15T15:50:22.525239Z","shell.execute_reply.started":"2021-11-15T15:50:13.289787Z","shell.execute_reply":"2021-11-15T15:50:22.524267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"class DefaultConfig:\n    n_folds: int = 5\n    seed: int = 2021\n    num_classes: int = 4 # \"negative\", \"typical\", \"indeterminate\", \"atypical\"\n    img_size: int = 256\n    fold_num: int = 0\n    device: str = 'cuda:0'\nconfig_default = {  \"num_workers\":4,\n    \"batch_size\":16,\n    \"n_epochs\": 1,\n    \"lr\": 0.0002,\n    }\nlog_interval =  1\nimage_log_interval = 10","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:22.526946Z","iopub.execute_input":"2021-11-15T15:50:22.527831Z","iopub.status.idle":"2021-11-15T15:50:22.556868Z","shell.execute_reply.started":"2021-11-15T15:50:22.527784Z","shell.execute_reply":"2021-11-15T15:50:22.556135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(DefaultConfig.device) if torch.cuda.is_available() else torch.device('cpu')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:22.557993Z","iopub.execute_input":"2021-11-15T15:50:22.55822Z","iopub.status.idle":"2021-11-15T15:50:22.581993Z","shell.execute_reply.started":"2021-11-15T15:50:22.558193Z","shell.execute_reply":"2021-11-15T15:50:22.58092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:22.583713Z","iopub.execute_input":"2021-11-15T15:50:22.584175Z","iopub.status.idle":"2021-11-15T15:50:22.608205Z","shell.execute_reply.started":"2021-11-15T15:50:22.584142Z","shell.execute_reply":"2021-11-15T15:50:22.607143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Choosing optimizers:\nAdam = False\nif Adam: \n    Adam_config = {\"lr\" : 0.001, \"betas\" : (0.9, 0.999), \"eps\" : 1e-08}\nelse:\n    SGD_config = {\"lr\" : 0.001, \"momentum\" : 0.9, \"weight_decay\" : 0.001}","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:22.609806Z","iopub.execute_input":"2021-11-15T15:50:22.610273Z","iopub.status.idle":"2021-11-15T15:50:22.638203Z","shell.execute_reply.started":"2021-11-15T15:50:22.610225Z","shell.execute_reply":"2021-11-15T15:50:22.637147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(DefaultConfig.seed)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:22.642415Z","iopub.execute_input":"2021-11-15T15:50:22.642938Z","iopub.status.idle":"2021-11-15T15:50:22.670741Z","shell.execute_reply.started":"2021-11-15T15:50:22.642887Z","shell.execute_reply":"2021-11-15T15:50:22.669835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split","metadata":{}},{"cell_type":"code","source":"df_folds = train.copy()\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=DefaultConfig.seed)\nfor n, (train_index, val_index) in enumerate(skf.split(X=df_folds.index, y=df_folds.integer_label)):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = int(n)\ndf_folds['fold'] = df_folds['fold'].astype(int)\nprint(df_folds.groupby(['fold', df_folds.integer_label]).size())","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:22.672202Z","iopub.execute_input":"2021-11-15T15:50:22.672505Z","iopub.status.idle":"2021-11-15T15:50:22.725357Z","shell.execute_reply.started":"2021-11-15T15:50:22.672471Z","shell.execute_reply":"2021-11-15T15:50:22.724754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Augmentation using albumentations","metadata":{}},{"cell_type":"code","source":"def get_train_transforms():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5), \n        A.VerticalFlip(p=0.5),\n        A.OneOf([\n            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2,\n                                 val_shift_limit=0.2, p=0.3), \n            A.RandomBrightnessContrast(brightness_limit=0.2,  \n                                       contrast_limit=0.2, p=0.3),\n        ], p=0.2),\n        A.Resize(height=DefaultConfig.img_size, width=DefaultConfig.img_size, p=1.0),\n        #A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0),\n    ],\n    p=1.0, bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transforms():\n    return A.Compose([\n        A.Resize(height=DefaultConfig.img_size, width=DefaultConfig.img_size, p=1.0),\n        #A.Normalize(mean=(0, 0, 0), std=(1, 1, 1), max_pixel_value=255.0, p=1.0),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:22.726541Z","iopub.execute_input":"2021-11-15T15:50:22.726888Z","iopub.status.idle":"2021-11-15T15:50:22.754491Z","shell.execute_reply.started":"2021-11-15T15:50:22.726857Z","shell.execute_reply":"2021-11-15T15:50:22.753912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset & DataLoader","metadata":{}},{"cell_type":"code","source":"class CustomDataset(Dataset):\n\n    def __init__(self, image_ids, df, transforms=None, test=False):\n        super().__init__()\n\n        self.image_ids = image_ids\n        self.df = df\n        self.file_names = df['jpg_path'].values\n        self.transforms = transforms\n        self.test = test\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        \n        image, boxes, labels = self.load_image_and_boxes(index)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = torch.tensor(labels)\n        target['image_id'] = torch.tensor([index])\n\n        if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                })\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    break\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def load_image_and_boxes(self, index):\n        image_id = self.image_ids[index]\n        image = cv2.imread(self.file_names[index], cv2.IMREAD_COLOR).copy().astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        records = self.df[self.df['id'] == image_id]       \n        boxes = []\n        for bbox in records[['frac_xmin', 'frac_ymin', 'frac_xmax', 'frac_ymax']].values:\n            bbox = np.clip(bbox, 0, 1.0)\n            temp = A.convert_bbox_from_albumentations(bbox, 'pascal_voc', image.shape[0], image.shape[0]) \n            boxes.append(temp)\n        '''\n        [0: 'atypical', 1: 'indeterminate', 2: 'negative', 3: 'typical']\n        '''\n        labels = records['integer_label'].values\n        return image, boxes, labels","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:22.755815Z","iopub.execute_input":"2021-11-15T15:50:22.756169Z","iopub.status.idle":"2021-11-15T15:50:22.793124Z","shell.execute_reply.started":"2021-11-15T15:50:22.756138Z","shell.execute_reply":"2021-11-15T15:50:22.792369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_folds = df_folds.set_index('id')\n\ndef get_train_dataset(fold_number):    \n    return CustomDataset(\n        image_ids = df_folds[df_folds['fold'] != fold_number].index.values,\n        df = train,\n        transforms = get_train_transforms()\n    )\n\ndef get_validation_dataset(fold_number):\n    return CustomDataset(\n        image_ids = df_folds[df_folds['fold'] == fold_number].index.values,\n        df = train,\n        transforms = get_valid_transforms()\n    )\n\ndef get_train_data_loader(train_dataset, batch_size=16):\n    return DataLoader(\n        train_dataset,\n        batch_size = batch_size,\n        shuffle = False,\n        num_workers = 4,\n        collate_fn = collate_fn\n    )\n\ndef get_validation_data_loader(valid_dataset, batch_size=16):\n    return DataLoader(\n        valid_dataset,\n        batch_size = batch_size,\n        shuffle = False,\n        num_workers = 4,\n        collate_fn = collate_fn\n    )    \n\ndef collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:22.7942Z","iopub.execute_input":"2021-11-15T15:50:22.794876Z","iopub.status.idle":"2021-11-15T15:50:22.827954Z","shell.execute_reply.started":"2021-11-15T15:50:22.794701Z","shell.execute_reply":"2021-11-15T15:50:22.82719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Show One Image using Dataset","metadata":{}},{"cell_type":"code","source":"train_dataset = get_train_dataset(0)\n\nimage, target, image_id = train_dataset[2]\nboxes = target['boxes'].cpu().numpy().astype(np.int32)\n\nnumpy_image = image.permute(1,2,0).cpu().numpy()\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(numpy_image, (box[0], box[1]), (box[2],  box[3]), (0, 255, 0), 2)\n    \nax.set_axis_off()\nax.imshow(numpy_image);","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:22.829093Z","iopub.execute_input":"2021-11-15T15:50:22.829819Z","iopub.status.idle":"2021-11-15T15:50:23.136747Z","shell.execute_reply.started":"2021-11-15T15:50:22.829781Z","shell.execute_reply":"2021-11-15T15:50:23.135806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:23.138046Z","iopub.execute_input":"2021-11-15T15:50:23.138279Z","iopub.status.idle":"2021-11-15T15:50:23.165989Z","shell.execute_reply.started":"2021-11-15T15:50:23.138249Z","shell.execute_reply":"2021-11-15T15:50:23.164966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Show Images using Dataloader","metadata":{}},{"cell_type":"code","source":"n_rows=4\nn_cols=4\n\n# create train dataset and data-loader\ntrain_dataset = get_train_dataset(fold_number=DefaultConfig.fold_num)\ntrain_data_loader = get_train_data_loader(train_dataset, batch_size=16)\n\nimages, targets, image_ids = next(iter(train_data_loader))\n\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n# plot some augmentations!\nfig, ax = plt.subplots(figsize=(20, 20),  nrows=n_rows, ncols=n_cols)\nfor i in range (n_rows*n_cols):    \n    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n    sample = images[i].permute(1,2,0).cpu().numpy()\n    for box in boxes:\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (255, 0, 0), 3)\n    \n    ax[i // n_rows][i % n_cols].imshow(sample)   ","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:23.167299Z","iopub.execute_input":"2021-11-15T15:50:23.167505Z","iopub.status.idle":"2021-11-15T15:50:27.417199Z","shell.execute_reply.started":"2021-11-15T15:50:23.167479Z","shell.execute_reply":"2021-11-15T15:50:27.416545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metric","metadata":{}},{"cell_type":"markdown","source":"IOU\n","metadata":{}},{"cell_type":"code","source":"\n@jit(nopython=True)\ndef calculate_iou(gt, pr, form='pascal_voc') -> float:\n    \"\"\"Calculates the Intersection over Union.\n\n    Args:\n        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n        form: (str) gt/pred coordinates format\n            - pascal_voc: [xmin, ymin, xmax, ymax]\n            - coco: [xmin, ymin, w, h]\n    Returns:\n        (float) Intersection over union (0.0 <= iou <= 1.0)\n    \"\"\"\n    if form == 'coco':\n        gt = gt.copy()\n        pr = pr.copy()\n\n        gt[2] = gt[0] + gt[2]\n        gt[3] = gt[1] + gt[3]\n        pr[2] = pr[0] + pr[2]\n        pr[3] = pr[1] + pr[3]\n\n    # Calculate overlap area\n    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n    \n    if dx < 0:\n        return 0.0\n    \n    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n\n    if dy < 0:\n        return 0.0\n\n    overlap_area = dx * dy\n\n    # Calculate union area\n    union_area = (\n            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n            overlap_area\n    )\n\n    return overlap_area / union_area\n\n@jit(nopython=True)\ndef find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None) -> int:\n    \"\"\"Returns the index of the 'best match' between the\n    ground-truth boxes and the prediction. The 'best match'\n    is the highest IoU. (0.0 IoUs are ignored).\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        pred: (List[Union[int, float]]) Coordinates of the predicted box\n        pred_idx: (int) Index of the current predicted box\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (int) Index of the best match GT box (-1 if no match above threshold)\n    \"\"\"\n    best_match_iou = -np.inf\n    best_match_idx = -1\n\n    for gt_idx in range(len(gts)):\n        \n        if gts[gt_idx][0] < 0:\n            # Already matched GT-box\n            continue\n        \n        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n\n        if iou < 0:\n            iou = calculate_iou(gts[gt_idx], pred, form=form)\n            \n            if ious is not None:\n                ious[gt_idx][pred_idx] = iou\n\n        if iou < threshold:\n            continue\n\n        if iou > best_match_iou:\n            best_match_iou = iou\n            best_match_idx = gt_idx\n\n    return best_match_idx\n\n@jit(nopython=True)\ndef calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None) -> float:\n    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n = len(preds)\n    tp = 0\n    fp = 0\n    \n    # for pred_idx, pred in enumerate(preds_sorted):\n    for pred_idx in range(n):\n\n        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n                                            threshold=threshold, form=form, ious=ious)\n\n        if best_match_gt_idx >= 0:\n            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n            tp += 1\n            # Remove the matched GT box\n            gts[best_match_gt_idx] = -1\n\n        else:\n            # No match\n            # False positive: indicates a predicted box had no associated gt box.\n            fp += 1\n\n    # False negative: indicates a gt box had no associated predicted box.\n    fn = (gts.sum(axis=1) > 0).sum()\n\n    return tp / (tp + fp + fn)\n\n\n@jit(nopython=True)\ndef calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco') -> float:\n    \"\"\"Calculates image precision.\n       The mean average precision at different intersection over union (IoU) thresholds.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        thresholds: (float) Different thresholds\n        form: (str) Format of the coordinates\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n_threshold = len(thresholds)\n    image_precision = 0.0\n    \n    ious = np.ones((len(gts), len(preds))) * -1\n    # ious = None\n\n    for threshold in thresholds:\n        precision_at_threshold = calculate_precision(gts.copy(), preds, threshold=threshold,\n                                                     form=form, ious=ious)\n        image_precision += precision_at_threshold / n_threshold\n\n    return image_precision","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-15T15:50:27.418846Z","iopub.execute_input":"2021-11-15T15:50:27.4191Z","iopub.status.idle":"2021-11-15T15:50:27.466323Z","shell.execute_reply.started":"2021-11-15T15:50:27.419067Z","shell.execute_reply":"2021-11-15T15:50:27.465719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fitter","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:27.467213Z","iopub.execute_input":"2021-11-15T15:50:27.467838Z","iopub.status.idle":"2021-11-15T15:50:27.496303Z","shell.execute_reply.started":"2021-11-15T15:50:27.467797Z","shell.execute_reply":"2021-11-15T15:50:27.495552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iou_thresholds = [0.5]\n\nclass EvalMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.image_precision = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, gt_boxes, pred_boxes, n=1):       \n        \"\"\" pred_boxes : need to be sorted.\"\"\"\n        \n        self.image_precision = calculate_image_precision(pred_boxes,\n                                                         gt_boxes,\n                                                         thresholds=iou_thresholds,\n                                                         form='pascal_voc')\n        self.count += n\n        self.sum += self.image_precision * n\n        self.avg = self.sum / self.count\n       ","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:27.497961Z","iopub.execute_input":"2021-11-15T15:50:27.498233Z","iopub.status.idle":"2021-11-15T15:50:27.526199Z","shell.execute_reply.started":"2021-11-15T15:50:27.498203Z","shell.execute_reply":"2021-11-15T15:50:27.525426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainGlobalConfig:\n    num_workers: int = 4\n    batch_size: int = 16\n    n_epochs: int = 1 #40\n    lr: float = 0.0002\n\n    img_size = DefaultConfig.img_size\n        \n    folder = '/kaggle/working/' #folder_name \n\n    # -------------------\n    verbose = True\n    verbose_step = 1\n    # -------------------\n\n    # --------------------\n    step_scheduler = False  # do scheduler.step after optimizer.step\n    validation_scheduler = False  # do scheduler.step after validation stage loss\n\n#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n#     scheduler_params = dict(\n#         max_lr=0.001,\n#         epochs=n_epochs,\n#         steps_per_epoch=int(len(train_dataset) / batch_size),\n#         pct_start=0.1,\n#         anneal_strategy='cos', \n#         final_div_factor=10**5\n#     )\n    \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='min',\n        factor=0.5,\n        patience=1,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )\n    # --------------------","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:27.527605Z","iopub.execute_input":"2021-11-15T15:50:27.527919Z","iopub.status.idle":"2021-11-15T15:50:27.552438Z","shell.execute_reply.started":"2021-11-15T15:50:27.527881Z","shell.execute_reply":"2021-11-15T15:50:27.551862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class FasterRCNNDetector(torch.nn.Module):\n    def __init__(self, pretrained=False, **kwargs):\n        super(FasterRCNNDetector, self).__init__()\n        # load pre-trained model incl. head\n        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained, pretrained_backbone=pretrained)\n        \n        # get number of input features for the classifier custom head\n        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n        \n        # replace the pre-trained head with a new one\n        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, DefaultConfig.num_classes)\n        \n    def forward(self, images, targets=None):\n        return self.model(images, targets)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:27.553392Z","iopub.execute_input":"2021-11-15T15:50:27.553975Z","iopub.status.idle":"2021-11-15T15:50:27.580378Z","shell.execute_reply.started":"2021-11-15T15:50:27.553939Z","shell.execute_reply":"2021-11-15T15:50:27.579511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ndef get_model(checkpoint_path=None, pretrained=False):\n    model = FasterRCNNDetector(pretrained=pretrained)\n    \n    # Load the trained weights\n    if checkpoint_path is not None:\n        checkpoint = torch.load(checkpoint_path)\n        model.load_state_dict(checkpoint['model_state_dict'])\n\n        del checkpoint\n        gc.collect()\n        \n    return model\n\nnet = get_model(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:27.581522Z","iopub.execute_input":"2021-11-15T15:50:27.582152Z","iopub.status.idle":"2021-11-15T15:50:28.322581Z","shell.execute_reply.started":"2021-11-15T15:50:27.58212Z","shell.execute_reply":"2021-11-15T15:50:28.321684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"sweep_config = {\n    'method': 'grid', #grid, random (random will continue running until you terminate it or specify a target field for metric)\n    'metric': {\n      'name': 'train_loss',\n      'goal': 'minimize'   \n    },\n    'parameters': {\n        'epochs': {\n            'values': [2, 4]\n        },\n        'batch_size': {\n            'values': [4, 8]\n        },\n        'lr': {\n            'values': [1e-3, 5e-4] \n        }\n    }\n}\n\n#initialize the sweep\n#Running this line will ask you to log into your W&B account\nsweep_id = wandb.sweep(sweep_config, project=\"CovidDetection-fasterRCNN\")","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:28.32389Z","iopub.execute_input":"2021-11-15T15:50:28.324217Z","iopub.status.idle":"2021-11-15T15:50:28.577609Z","shell.execute_reply.started":"2021-11-15T15:50:28.324183Z","shell.execute_reply":"2021-11-15T15:50:28.576726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Result Graphs\nfrom sklearn.metrics import PrecisionRecallDisplay\ndef run_training(fold=0):\n    net.to(device)\n    wandb.watch(net, log=\"all\")\n    \n    train_dataset = get_train_dataset(fold_number=fold)\n    train_data_loader = get_train_data_loader(\n        train_dataset,\n        batch_size=TrainGlobalConfig.batch_size\n    )\n    \n    validation_dataset = get_validation_dataset(fold_number=fold)\n    validation_data_loader = get_validation_data_loader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size\n    )\n\n    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n    fitter.fit(train_data_loader, validation_data_loader)\n    \n   ","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:28.579253Z","iopub.execute_input":"2021-11-15T15:50:28.579775Z","iopub.status.idle":"2021-11-15T15:50:28.614653Z","shell.execute_reply.started":"2021-11-15T15:50:28.579728Z","shell.execute_reply":"2021-11-15T15:50:28.613746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_boxes_format(boxes):\n    boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n    boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n    return boxes\n\nclass_id_to_label = {\n    0: 'atypical', 1: 'indeterminate', 2: 'negative', 3: 'typical'\n}\n\n\nclass Fitter:\n    def __init__(self, model, device, config):\n        self.config = config\n        self.epoch = 0\n\n        self.base_dir = f'./{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n        self.log_path = f'{self.base_dir}/log.txt'\n        self.best_summary_loss = 10**5\n        self.best_score = 0\n\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n        \n        # get the configured optimizer\n        if Adam:\n            self.optimizer = torch.optim.Adam(self.model.parameters(), **Adam_config)\n        else:\n            self.optimizer = torch.optim.SGD(self.model.parameters(), **SGD_config)\n\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n        self.log(f'Fitter prepared. Device is {self.device}')\n        self.log(f'Fold num is {DefaultConfig.fold_num}')\n\n    def fit(self, train_loader, validation_loader):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            summary_loss = self.train_one_epoch(train_loader)\n            \n            if e == 0:\n                self.best_summary_loss = summary_loss.avg\n\n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n            self.save(f'{self.base_dir}/last-checkpoint.bin')\n\n            t = time.time()\n            _, eval_scores  = self.validation(validation_loader)\n\n            self.log(f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, image_precision: {eval_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n            \n            epoch_precision_score = []\n            iteration = 0\n            for batch_idx, (images, targets, image_ids) in enumerate(validation_loader):\n                images = list(image.to(device) for image in images)\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n                outputs = self.model(images)\n\n                #Calculate Precision Score Per Batch\n                batch_precision_score = 0\n                for i, image in enumerate(images):\n            \n                    #predicted bounding boxes\n                    boxes = outputs[i]['boxes'].data.cpu().numpy()\n                    boxes = convert_boxes_format(boxes)\n\n                    scores = outputs[i]['scores'].data.cpu().numpy()\n\n                    #ground truth bounding boxes\n                    gt_boxes = targets[i][\"boxes\"].cpu().numpy()\n                    gt_boxes = convert_boxes_format(gt_boxes)\n\n                    image_id = image_ids[i]\n\n                    # Sort highest confidence -> lowest confidence\n                    preds_sorted_idx = np.argsort(scores)[::-1]\n                    preds_sorted = boxes[preds_sorted_idx]\n\n                    iou_thresholds = [x for x in np.arange(0.5, 0.76, 0.05)]\n                    image_precision = calculate_image_precision(preds_sorted, gt_boxes,\n                                                    thresholds=iou_thresholds,\n                                                    form='coco')\n\n                    print(\"Average Precision of image: {0:.4f}\".format(image_precision))\n                    batch_precision_score +=  image_precision\n        \n                batch_precision_score = batch_precision_score / len(images) #average precision score for the batch\n                epoch_precision_score.append(batch_precision_score)\n\n                if batch_idx % log_interval == 0:\n                    #logs the precision score per batch and also the iteration\n                    wandb.log({\"batch_score_validation\": batch_precision_score, \"iteration_validation\": iteration})\n\n                #Log bounding boxes\n                if batch_idx % image_log_interval == 0:\n\n                    #Log 1 image with bounding boxes for this batch\n                    for i, image in enumerate(images[:1]): \n\n                        image_id = image_ids[i]\n                        scores = outputs[i]['scores'].data.cpu().numpy().astype(np.float64)\n\n                        #predicted bounding boxes\n                        boxes = outputs[i]['boxes'].data.cpu().numpy().astype(np.float64)\n                        predicted_boxes = []\n                        for b_i, box in enumerate(boxes):\n\n                            box_data = {\"position\" : {\n                                  \"minX\" : box[0],\n                                  \"maxX\" : box[2],\n                                  \"minY\" : box[1],\n                                  \"maxY\" : box[3] \n                                },\n                              \"class_id\" : 1,\n                              \"box_caption\" : \"particle: (%.3f)\" % (scores[b_i]),\n                              \"domain\": \"pixel\",\n                              \"scores\" : { \"score\" : scores[b_i] }\n                            }\n                            predicted_boxes.append(box_data)\n\n\n                        #ground truth bounding boxes\n                        gt_boxes = targets[i][\"boxes\"].cpu().numpy().astype(np.float64)\n                        target_boxes = [] \n                        for b_i, box in enumerate(gt_boxes):\n\n                            box_data = {\"position\" : {\n                                  \"minX\" : box[0],\n                                  \"maxX\" : box[2],\n                                  \"minY\" : box[1],\n                                  \"maxY\" : box[3] \n                                },\n                              \"class_id\" : 0,\n                              \"domain\": \"pixel\",\n                              \"box_caption\" : \"ground_truth\"\n                            }\n                            target_boxes.append(box_data)\n\n\n                        image = image.permute(1,2,0).cpu().numpy().astype(np.float64)\n\n                        #create image object and log\n                        img = wandb.Image(image, boxes = \n                                          {\"predictions\": \n                                           {\"box_data\": predicted_boxes, \n                                            \"class_labels\" : class_id_to_label},\"ground_truth\": {\"box_data\": target_boxes}})\n\n                        wandb.log({\"bounding_boxes\": img})\n\n                iteration += 1\n    \n            epoch_precision_score = sum(epoch_precision_score) / len(epoch_precision_score) \n            wandb.log({\"epoch_score_validation\": epoch_precision_score, \"epoch\": epoch})\n            \n            #if summary_loss.avg < self.best_summary_loss:\n            if eval_scores.avg > self.best_score:\n                self.best_summary_loss = summary_loss.avg\n                self.best_score = eval_scores.avg\n                self.model.eval()\n                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n                    os.remove(path)\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=eval_scores.avg)\n                #self.scheduler.step(metrics=summary_loss.avg)\n\n            self.epoch += 1\n\n    def validation(self, val_loader):\n        self.model.eval()\n        \n        # model.eval() mode --> it will return boxes and scores.\n        # in this part, just print train_loss\n        summary_loss = AverageMeter()\n        summary_loss.update(self.best_summary_loss, self.config.batch_size)\n        \n        eval_scores = EvalMeter()\n        validation_image_precisions = []\n        \n        t = time.time()\n        for step, (images, targets, image_ids) in enumerate(val_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Val Step {step}/{len(val_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'val_precision: {eval_scores.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n            with torch.no_grad():\n                images = torch.stack(images)\n                batch_size = images.shape[0]\n                images = images.to(self.device).float()\n                labels = [target['labels'].float() for target in targets]\n\n                \"\"\"\n                In model.train() mode, model(images)  is returning losses.\n                We are using model.eval() mode --> it will return boxes and scores. \n                \"\"\"\n                outputs = self.model(images)               \n                \n                for i, image in enumerate(images):               \n                    gt_boxes = targets[i]['boxes'].data.cpu().numpy()\n                    boxes = outputs[i]['boxes'].data.cpu().numpy()\n                    scores = outputs[i]['scores'].detach().cpu().numpy()\n                    \n                    preds_sorted_idx = np.argsort(scores)[::-1]\n                    preds_sorted_boxes = boxes[preds_sorted_idx]\n\n                    eval_scores.update(pred_boxes=preds_sorted_boxes, gt_boxes=gt_boxes)\n\n        return summary_loss, eval_scores\n\n    def train_one_epoch(self, train_loader):\n        self.model.train()\n        summary_loss = AverageMeter()\n\n        t = time.time()\n        for step, (images, targets, image_ids) in enumerate(train_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    print(\n                        f'Train Step {step}/{len(train_loader)}, ' + \\\n                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}', end='\\r'\n                    )\n                    wandb.log({\"train_step\": step/len(train_loader), \"summary_loss\":summary_loss.avg, \"time\":(time.time() - t)})\n            \n            images = torch.stack(images)\n            images = images.to(self.device).float()\n            batch_size = images.shape[0]\n            boxes = [target['boxes'].to(self.device).float() for target in targets]\n            labels = [target['labels'].to(self.device).float() for target in targets]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets] \n\n            self.optimizer.zero_grad()\n            \n            outputs = self.model(images, targets)\n            \n            loss = sum(loss for loss in outputs.values())\n            \n            loss.backward()\n\n            summary_loss.update(loss.detach().item(), batch_size)\n\n            self.optimizer.step()\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n\n        return summary_loss\n    \n    def save(self, path):\n        self.model.eval()\n        torch.save({\n            'model_state_dict': self.model.state_dict(), #'model_state_dict': self.model.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'best_summary_loss': self.best_summary_loss,\n            'epoch': self.epoch,\n        }, path)\n\n    def load(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_summary_loss = checkpoint['best_summary_loss']\n        self.epoch = checkpoint['epoch'] + 1\n        \n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:28.616846Z","iopub.execute_input":"2021-11-15T15:50:28.617726Z","iopub.status.idle":"2021-11-15T15:50:28.701325Z","shell.execute_reply.started":"2021-11-15T15:50:28.617652Z","shell.execute_reply":"2021-11-15T15:50:28.700634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#run_training(fold=DefaultConfig.fold_num)\n#run = wandb.init(config=config_default, project=\"CovidDetection-fasterRCNN\")\nwandb.agent(sweep_id, run_training(fold=DefaultConfig.fold_num))\n\n#artifact = run.use_artifact('joshuaa/CovidDetection-fasterRCNN/Faster-RCNN:v0', type='model')\n#artifact_dir = artifact.download()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:28.70511Z","iopub.execute_input":"2021-11-15T15:50:28.705578Z","iopub.status.idle":"2021-11-15T15:50:30.020652Z","shell.execute_reply.started":"2021-11-15T15:50:28.705541Z","shell.execute_reply":"2021-11-15T15:50:30.019292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:30.022096Z","iopub.status.idle":"2021-11-15T15:50:30.022469Z","shell.execute_reply.started":"2021-11-15T15:50:30.022261Z","shell.execute_reply":"2021-11-15T15:50:30.022279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#artifact = wandb.Artifact('Faster-RCNN', type='model')\n\n# Add a file to the artifact's contents\n#artifact.add_file('/joshuaa/Projects/Faster-RCNN.h5')\n\n# Save the artifact version to W&B and mark it as the output of this run\n#run.log_artifact(artifact)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:30.024023Z","iopub.status.idle":"2021-11-15T15:50:30.024373Z","shell.execute_reply.started":"2021-11-15T15:50:30.024205Z","shell.execute_reply":"2021-11-15T15:50:30.024223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade scikit-learn","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:30.025516Z","iopub.status.idle":"2021-11-15T15:50:30.025876Z","shell.execute_reply.started":"2021-11-15T15:50:30.025695Z","shell.execute_reply":"2021-11-15T15:50:30.025715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = open('kaggle/working/log.txt', 'r')\nfor line in file.readlines():\n    print(line[:-1])\nfile.close()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-11-15T15:50:30.027025Z","iopub.status.idle":"2021-11-15T15:50:30.027754Z","shell.execute_reply.started":"2021-11-15T15:50:30.027494Z","shell.execute_reply":"2021-11-15T15:50:30.027521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"validation_dataset = get_validation_dataset(fold_number=DefaultConfig.fold_num)\nvalidation_data_loader = get_validation_data_loader(\n    validation_dataset, \n    batch_size=TrainGlobalConfig.batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:30.02891Z","iopub.status.idle":"2021-11-15T15:50:30.029791Z","shell.execute_reply.started":"2021-11-15T15:50:30.029458Z","shell.execute_reply":"2021-11-15T15:50:30.029509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, targets, image_id = next(iter(validation_data_loader))\n\nimages = list(img.to(device) for img in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nboxes = targets[1]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[1].permute(1,2,0).cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:30.031996Z","iopub.status.idle":"2021-11-15T15:50:30.032348Z","shell.execute_reply.started":"2021-11-15T15:50:30.032166Z","shell.execute_reply":"2021-11-15T15:50:30.03219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:30.03376Z","iopub.status.idle":"2021-11-15T15:50:30.034385Z","shell.execute_reply.started":"2021-11-15T15:50:30.034172Z","shell.execute_reply":"2021-11-15T15:50:30.034196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net = get_model('kaggle/working/best-checkpoint-001epoch.bin')","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:30.035859Z","iopub.status.idle":"2021-11-15T15:50:30.036639Z","shell.execute_reply.started":"2021-11-15T15:50:30.036362Z","shell.execute_reply":"2021-11-15T15:50:30.036396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net.eval()\ncpu_device = torch.device(\"cpu\")\n\noutputs = net(images)\noutputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:30.038088Z","iopub.status.idle":"2021-11-15T15:50:30.038433Z","shell.execute_reply.started":"2021-11-15T15:50:30.038247Z","shell.execute_reply":"2021-11-15T15:50:30.038272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 2)\n    \nax.set_axis_off()\nax.imshow(sample)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T15:50:30.039835Z","iopub.status.idle":"2021-11-15T15:50:30.040195Z","shell.execute_reply.started":"2021-11-15T15:50:30.040018Z","shell.execute_reply":"2021-11-15T15:50:30.040038Z"},"trusted":true},"execution_count":null,"outputs":[]}]}