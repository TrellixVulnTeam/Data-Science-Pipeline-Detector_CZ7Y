{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2\n\n## MMDetection compatible torch installation\n!pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torch-1.7.0+cu110-cp37-cp37m-linux_x86_64.whl' --no-deps\n!pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torchvision-0.8.1+cu110-cp37-cp37m-linux_x86_64.whl' --no-deps\n# !pip install '/kaggle/input/pytorch-170-cuda-toolkit-110221/torchaudio-0.7.0-cp37-cp37m-linux_x86_64.whl' --no-deps\n\n## Compatible Cuda Toolkit installation\n!mkdir -p /kaggle/tmp && cp /kaggle/input/pytorch-170-cuda-toolkit-110221/cudatoolkit-11.0.221-h6bb024c_0 /kaggle/tmp/cudatoolkit-11.0.221-h6bb024c_0.tar.bz2 && conda install /kaggle/tmp/cudatoolkit-11.0.221-h6bb024c_0.tar.bz2 -y --offline\n\n## MMDetection Offline Installation\n!pip install '/kaggle/input/mmdetectionv2140/addict-2.4.0-py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/yapf-0.31.0-py2.py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/terminal-0.4.0-py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/terminaltables-3.1.0-py3-none-any.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/mmcv_full-1_3_8-cu110-torch1_7_0/mmcv_full-1.3.8-cp37-cp37m-manylinux1_x86_64.whl' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/pycocotools-2.0.2/pycocotools-2.0.2' --no-deps\n!pip install '/kaggle/input/mmdetectionv2140/mmpycocotools-12.0.3/mmpycocotools-12.0.3' --no-deps\n\n!cp -r /kaggle/input/mmdetectionv2140/mmdetection-2.14.0 /kaggle/working/\n!mv /kaggle/working/mmdetection-2.14.0 /kaggle/working/mmdetection\n%cd /kaggle/working/mmdetection\n!pip install -e . --no-deps\n%cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:51:05.02536Z","iopub.execute_input":"2021-08-09T09:51:05.025845Z","iopub.status.idle":"2021-08-09T09:59:28.577316Z","shell.execute_reply.started":"2021-08-09T09:51:05.025764Z","shell.execute_reply":"2021-08-09T09:59:28.576161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda install '/kaggle/input/pydicom-conda-helper/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y > /dev/null 2>&1\n!conda install '/kaggle/input/pydicom-conda-helper/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y > /dev/null 2>&1\n!conda install '/kaggle/input/pydicom-conda-helper/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y > /dev/null 2>&1\n!conda install '/kaggle/input/pydicom-conda-helper/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y > /dev/null 2>&1\n!conda install '/kaggle/input/pydicom-conda-helper/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y > /dev/null 2>&1\n!conda install '/kaggle/input/pydicom-conda-helper/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y > /dev/null 2>&1","metadata":{"execution":{"iopub.status.busy":"2021-08-09T09:59:28.581315Z","iopub.execute_input":"2021-08-09T09:59:28.581646Z","iopub.status.idle":"2021-08-09T10:00:31.361304Z","shell.execute_reply.started":"2021-08-09T09:59:28.58161Z","shell.execute_reply":"2021-08-09T10:00:31.359695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install /kaggle/input/weightedboxesfusion","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:00:31.36593Z","iopub.execute_input":"2021-08-09T10:00:31.366307Z","iopub.status.idle":"2021-08-09T10:01:01.120935Z","shell.execute_reply.started":"2021-08-09T10:00:31.366273Z","shell.execute_reply":"2021-08-09T10:01:01.11963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\n# add smp & timm into kernel w/o internet\nsys.path = [\n    '/kaggle/input/smp20210726/segmentation_models.pytorch-master/segmentation_models.pytorch-master/',\n    '/kaggle/input/smp20210726/EfficientNet-PyTorch-master/EfficientNet-PyTorch-master',\n    '/kaggle/input/smp20210726/pytorch-image-models-master/pytorch-image-models-master',\n    '/kaggle/input/smp20210726/pretrained-models.pytorch-master/pretrained-models.pytorch-master',\n] + sys.path\nfrom ensemble_boxes import *","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:01:01.125539Z","iopub.execute_input":"2021-08-09T10:01:01.125874Z","iopub.status.idle":"2021-08-09T10:01:01.201928Z","shell.execute_reply.started":"2021-08-09T10:01:01.125839Z","shell.execute_reply":"2021-08-09T10:01:01.200616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntemp = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\nDEBUG = temp.shape[0] == 2477\n# DEBUG = False\nTTA = False\n# IMAGE_MODELPATH = [f'/kaggle/input/yolov5x6-e-10-img-640/yolov5x6-e-10-img-640-fold-{fold}.pt' for fold in range(5)]\n# IMAGE_MODELPATH = \"/kaggle/input/siimcovid19yolov5test/5s-512px-29epochs.pt\"\n# IMAGE_MODELPATH = [\n#     (512, 'yolov5x-e-40-img-512', [f'/kaggle/input/yolov5x-e-40-img-512/yolov5x-e-40-img-512-fold-{fold}.pt' for fold in range(5)]),\n#     (640, 'yolov5x6-e-10-img-640', [f'/kaggle/input/yolov5x6-e-10-img-640/yolov5x6-e-10-img-640-fold-{fold}.pt' for fold in range(5)]),\n#     (1280, 'yolov5x6-e-40-img-1280', [f'/kaggle/input/yolov5x6-e-40-img-1280/yolov5x6-e-40-img-1280-fold-{fold}.pt' for fold in range(5)]),\n# ]\nIMAGE_IMG_SIZE = [512, 640, 1280]\nMODELPATH = [\n#     [\n#         [\"../input/efficientnetv2-sgkf/tu_tf_efficientnetv2_m-640-aug-epoch04-sgkf4-val_loss0.7334.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n#         [\"../input/efficientnetv2-sgkf/tu_tf_efficientnetv2_m-640-aug-epoch09-sgkf0-val_loss0.7218.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n#         [\"../input/efficientnetv2-sgkf/tu_tf_efficientnetv2_m-640-aug-epoch09-sgkf1-val_loss0.7143.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n#         [\"../input/efficientnetv2-sgkf/tu_tf_efficientnetv2_m-640-aug-epoch09-sgkf2-val_loss0.7196.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n#         [\"../input/efficientnetv2-sgkf/tu_tf_efficientnetv2_m-640-aug-epoch09-sgkf3-val_loss0.7625.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n#     ],\n    [\n        [\"../input/efficientnetv2-m-640-nih/tu_tf_efficientnetv2_m-640-aug-epoch04-sgkf0-val_loss0.7173.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n        [\"../input/efficientnetv2-m-640-nih/tu_tf_efficientnetv2_m-640-aug-epoch04-sgkf1-val_loss0.6988.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n        [\"../input/efficientnetv2-m-640-nih/tu_tf_efficientnetv2_m-640-aug-epoch04-sgkf2-val_loss0.7106.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n        [\"../input/efficientnetv2-m-640-nih/tu_tf_efficientnetv2_m-640-aug-epoch04-sgkf3-val_loss0.7454.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n        [\"../input/efficientnetv2-m-640-nih/tu_tf_efficientnetv2_m-640-aug-epoch08-sgkf4-val_loss0.7338.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n    ],\n    [\n        [\"../input/efficientnetv2-sgkf-512/tu_tf_efficientnetv2_m-512-aug-epoch04-sgkf0-val_loss0.7306.ckpt\", 512, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n        [\"../input/efficientnetv2-sgkf-512/tu_tf_efficientnetv2_m-512-aug-epoch04-sgkf3-val_loss0.7475.ckpt\", 512, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n        [\"../input/efficientnetv2-sgkf-512/tu_tf_efficientnetv2_m-512-aug-epoch09-sgkf1-val_loss0.7077.ckpt\", 512, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n        [\"../input/efficientnetv2-sgkf-512/tu_tf_efficientnetv2_m-512-aug-epoch09-sgkf2-val_loss0.7330.ckpt\", 512, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n        [\"../input/efficientnetv2-sgkf-512/tu_tf_efficientnetv2_m-512-aug-epoch09-sgkf4-val_loss0.7431.ckpt\", 512, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n    ],\n    [\n        [\"../input/efficientnetv2-l-384/tu_tf_efficientnetv2_l-384-aug-epoch03-sgkf4-val_loss0.7819.ckpt\", 384, \"tu-tf_efficientnetv2_l\", \"unet_smp\"],\n        [\"../input/efficientnetv2-l-384/tu_tf_efficientnetv2_l-384-aug-epoch04-sgkf1-val_loss0.7402.ckpt\", 384, \"tu-tf_efficientnetv2_l\", \"unet_smp\"],\n        [\"../input/efficientnetv2-l-384/tu_tf_efficientnetv2_l-384-aug-epoch08-sgkf0-val_loss0.7614.ckpt\", 384, \"tu-tf_efficientnetv2_l\", \"unet_smp\"],\n        [\"../input/efficientnetv2-l-384/tu_tf_efficientnetv2_l-384-aug-epoch09-sgkf3-val_loss0.7793.ckpt\", 384, \"tu-tf_efficientnetv2_l\", \"unet_smp\"],\n        [\"../input/efficientnetv2-l-384/tu_tf_efficientnetv2_l-384-aug-epoch14-sgkf2-val_loss0.7636.ckpt\", 384, \"tu-tf_efficientnetv2_l\", \"unet_smp\"],\n    ],\n#     [\n#         [\"../input/swin-nihft-384/timm-swin_large_patch4_window12_384_in22k-384-aug-epoch04-sgkf2-val_loss0.8851.ckpt\", 384, \"swin_large_patch4_window12_384_in22k\", \"timm\"],\n#         [\"../input/swin-nihft-384/timm-swin_large_patch4_window12_384_in22k-384-aug-epoch04-sgkf4-val_loss0.8842.ckpt\", 384, \"swin_large_patch4_window12_384_in22k\", \"timm\"],\n#         [\"../input/swin-nihft-384/timm-swin_large_patch4_window12_384_in22k-384-aug-epoch05-sgkf0-val_loss0.8677.ckpt\", 384, \"swin_large_patch4_window12_384_in22k\", \"timm\"],\n#         [\"../input/swin-nihft-384/timm-swin_large_patch4_window12_384_in22k-384-aug-epoch06-sgkf1-val_loss0.8142.ckpt\", 384, \"swin_large_patch4_window12_384_in22k\", \"timm\"],\n#         [\"../input/swin-nihft-384/timm-swin_large_patch4_window12_384_in22k-384-aug-epoch07-sgkf3-val_loss0.8961.ckpt\", 384, \"swin_large_patch4_window12_384_in22k\", \"timm\"],\n#     ],\n#     [\n#         [\"../input/study-unet-smp-vin/tu_tf_efficientnetv2_m-640-aug-epoch04-sgkf0-val_loss0.7173.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n#         [\"../input/study-unet-smp-vin/tu_tf_efficientnetv2_m-640-aug-epoch04-sgkf3-val_loss0.7513.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n#         [\"../input/study-unet-smp-vin/tu_tf_efficientnetv2_m-640-aug-epoch04-sgkf4-val_loss0.7326.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n#         [\"../input/study-unet-smp-vin/tu_tf_efficientnetv2_m-640-aug-epoch09-sgkf1-val_loss0.6999.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n#         [\"../input/study-unet-smp-vin/tu_tf_efficientnetv2_m-640-aug-epoch09-sgkf2-val_loss0.7127.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp\"],\n#     ],\n#     [\n#         [\"../input/efficientnetv2-m-downconv-1280/unet_smp_downconv-1280-aug-epoch04-sgkf0-val_loss0.7329.ckpt\", 1280, \"tu-tf_efficientnetv2_m\", \"unet_smp_downconv\"],\n#         [\"../input/efficientnetv2-m-downconv-1280/unet_smp_downconv-1280-aug-epoch04-sgkf4-val_loss0.7403.ckpt\", 1280, \"tu-tf_efficientnetv2_m\", \"unet_smp_downconv\"],\n#         [\"../input/efficientnetv2-m-downconv-1280/unet_smp_downconv-1280-aug-epoch08-sgkf1-val_loss0.7113.ckpt\", 1280, \"tu-tf_efficientnetv2_m\", \"unet_smp_downconv\"],\n#         [\"../input/efficientnetv2-m-downconv-1280/unet_smp_downconv-1280-aug-epoch09-sgkf2-val_loss0.7333.ckpt\", 1280, \"tu-tf_efficientnetv2_m\", \"unet_smp_downconv\"],\n#         [\"../input/efficientnetv2-m-downconv-1280/unet_smp_downconv-1280-aug-epoch09-sgkf3-val_loss0.7494.ckpt\", 1280, \"tu-tf_efficientnetv2_m\", \"unet_smp_downconv\"],\n#     ],\n#     [\n#         [\"../input/efficientnetv2-m-mask-cond-640/unet_smp_mask_cond-640-aug-epoch04-sgkf0-val_loss0.7194.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp_mask_cond\"],\n#         [\"../input/efficientnetv2-m-mask-cond-640/unet_smp_mask_cond-640-aug-epoch04-sgkf1-val_loss0.7133.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp_mask_cond\"],\n#         [\"../input/efficientnetv2-m-mask-cond-640/unet_smp_mask_cond-640-aug-epoch09-sgkf2-val_loss0.7328.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp_mask_cond\"],\n#         [\"../input/efficientnetv2-m-mask-cond-640/unet_smp_mask_cond-640-aug-epoch09-sgkf3-val_loss0.7451.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp_mask_cond\"],\n#         [\"../input/efficientnetv2-m-mask-cond-640/unet_smp_mask_cond-640-aug-epoch09-sgkf4-val_loss0.7396.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp_mask_cond\"],\n#     ],\n]\nMODELPATH2C = [\n#     [\n#         [\"../input/study-unet-smp-2c/unet_smp_2c-640-aug-epoch00-sgkf0-val_loss0.4086.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp_2c\"],\n#         [\"../input/study-unet-smp-2c/unet_smp_2c-640-aug-epoch00-sgkf1-val_loss0.3669.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp_2c\"],\n#         [\"../input/study-unet-smp-2c/unet_smp_2c-640-aug-epoch00-sgkf2-val_loss0.3641.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp_2c\"],\n#         [\"../input/study-unet-smp-2c/unet_smp_2c-640-aug-epoch00-sgkf3-val_loss0.4120.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp_2c\"],\n#         [\"../input/study-unet-smp-2c/unet_smp_2c-640-aug-epoch05-sgkf4-val_loss0.3831.ckpt\", 640, \"tu-tf_efficientnetv2_m\", \"unet_smp_2c\"],\n#     ],\n    [\n        [\"../input/2class-all/timm_2c-384-aug-epoch01-sgkf0-val_loss0.4083.ckpt\", 384, \"swin_large_patch4_window12_384_in22k\", \"timm_2c\"],\n        [\"../input/2class-all/timm_2c-384-aug-epoch01-sgkf3-val_loss0.4137.ckpt\", 384, \"swin_large_patch4_window12_384_in22k\", \"timm_2c\"],\n        [\"../input/2class-all/timm_2c-384-aug-epoch02-sgkf4-val_loss0.3985.ckpt\", 384, \"swin_large_patch4_window12_384_in22k\", \"timm_2c\"],\n        [\"../input/2class-all/timm_2c-384-aug-epoch03-sgkf2-val_loss0.4303.ckpt\", 384, \"swin_large_patch4_window12_384_in22k\", \"timm_2c\"],\n        [\"../input/2class-all/timm_2c-384-aug-epoch04-sgkf1-val_loss0.3927.ckpt\", 384, \"swin_large_patch4_window12_384_in22k\", \"timm_2c\"],\n    ],\n    [\n        [\"../input/2class-all/unet_smp_2c-384-aug-epoch00-sgkf0-val_loss0.3606.ckpt\", 384, \"tu-tf_efficientnetv2_l\", \"unet_smp_2c\"],\n        [\"../input/2class-all/unet_smp_2c-384-aug-epoch00-sgkf1-val_loss0.3577.ckpt\", 384, \"tu-tf_efficientnetv2_l\", \"unet_smp_2c\"],\n        [\"../input/2class-all/unet_smp_2c-384-aug-epoch00-sgkf4-val_loss0.3659.ckpt\", 384, \"tu-tf_efficientnetv2_l\", \"unet_smp_2c\"],\n        [\"../input/2class-all/unet_smp_2c-384-aug-epoch02-sgkf3-val_loss0.3676.ckpt\", 384, \"tu-tf_efficientnetv2_l\", \"unet_smp_2c\"],\n        [\"../input/2class-all/unet_smp_2c-384-aug-epoch04-sgkf2-val_loss0.3854.ckpt\", 384, \"tu-tf_efficientnetv2_l\", \"unet_smp_2c\"],\n    ],\n]\nIMAGE_SCORE_THRES = 0.5\n# STUDY_IMG_SIZE = int(MODELPATH[0].split(\"/\")[-1].split(\"-\")[1])\nSTUDY_IMG_SIZE = 640\nTWOCLASS_IMG_SIZE = 640\n# AUG = \"aug\" in MODELPATH[0]\nAUG = True\n\nclass dotdict(dict):\n    \"\"\"dot.notation access to dictionary attributes\"\"\"\n    __getattr__ = dict.get\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n    \nconfig = dotdict({\n    \"model_name\": \"unet_smp\",\n    \"unet_smp\": dotdict({\n      \"backbone_name\": None,\n      \"model_type\": \"unetplusplus\",\n      \"neck_type\": \"F\",\n      \"gem_pooling\": False,\n      \"hidden_dim\": 512,\n      \"encoder_weights\": \"imagenet\",\n      \"mask_type\": \"both\",\n      \"decoder_blocks\": 3,\n      \"decoder_channels\": None,\n      \"classes\": 2,\n    }),\n})\n\nconfig_timm = dotdict({\n    \"model_name\": \"timm\",\n    \"model_config\": dotdict({\n        \"backbone_name\": \"swin_large_patch4_window12_384_in22k\",\n        \"neck_type\": \"F\",\n        \"hidden_dim\": 192,\n        \"num_classes\": 4,\n        \"pretrained_path\": None\n    })\n})\n\nfrom torchvision.datasets.folder import default_loader\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom tqdm.auto import tqdm\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom typing import Optional, List\nfrom glob import glob\n\nimport albumentations as A\nimport os\nimport gc\nimport copy\nimport torch\nimport timm\nimport pydicom\nimport multiprocessing\nimport shutil\n\nimport segmentation_models_pytorch as smp\nimport numpy as np\nimport pandas as pd\nimport torchvision.datasets as datasets\nimport torchvision.transforms as T\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data\n\n\ndef resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T10:01:01.204884Z","iopub.execute_input":"2021-08-09T10:01:01.205338Z","iopub.status.idle":"2021-08-09T10:01:08.81368Z","shell.execute_reply.started":"2021-08-09T10:01:01.205279Z","shell.execute_reply":"2021-08-09T10:01:08.812534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_study_transform(img_size):\n    train_transform = A.Compose([\n        A.Resize(img_size,img_size),\n        A.HorizontalFlip(p=0.5),\n#         A.ShiftScaleRotate(shift_limit=0.0, scale_limit=0.2, rotate_limit=0, border_mode=0, p=0.75),\n        ToTensorV2(p=1.0),\n    ])\n    val_transform = A.Compose([\n        A.Resize(img_size,img_size),\n        ToTensorV2(p=1.0),\n    ])\n    return train_transform, val_transform","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:01:08.815365Z","iopub.execute_input":"2021-08-09T10:01:08.815839Z","iopub.status.idle":"2021-08-09T10:01:08.873779Z","shell.execute_reply.started":"2021-08-09T10:01:08.815771Z","shell.execute_reply":"2021-08-09T10:01:08.872604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\n\ndef resize_bboxes(coordinates_abs, original_x, original_y, img_size):\n    w = coordinates_abs[2] - coordinates_abs[0]\n    h = coordinates_abs[3] - coordinates_abs[1]\n\n    coordinates_abs[0] = coordinates_abs[0] * (img_size / original_x)\n    coordinates_abs[2] = coordinates_abs[0] + w * (img_size / original_x)\n\n    coordinates_abs[1] = coordinates_abs[1] * (img_size / original_y)\n    coordinates_abs[3] = coordinates_abs[1] + h * (img_size / original_y)\n    \n    return coordinates_abs\n\n\ndef reconstruct_bboxes(coordinates_abs, original_x, original_y, img_size):\n    w = coordinates_abs[2] - coordinates_abs[0]\n    h = coordinates_abs[3] - coordinates_abs[1]\n\n    coordinates_abs[0] = coordinates_abs[0] * (original_x / img_size)\n    coordinates_abs[2] = coordinates_abs[0] + w * (original_x / img_size)\n\n    coordinates_abs[1] = coordinates_abs[1] * (original_y / img_size)\n    coordinates_abs[3] = coordinates_abs[1] + h * (original_y / img_size)\n    \n    return coordinates_abs\n\n\nclass COVIDDataset(datasets.VisionDataset):\n    def __init__(self, root, df, mode=\"train\", img_size=800, transform=None, target_transform=None, xy=True):\n        super().__init__(root, transform=transform, target_transform=target_transform)\n        \n        self.mode = mode\n\n        self.img_size = img_size\n        self.root = root\n        self.xy = xy\n\n        if self.mode == \"train\":\n            self.id_original_img_size_map = self.get_img_meta_map(root)\n\n        self.data = self.get_data(df)\n        \n        self.image_label_map = {0: \"none\", 1: \"opacity\"}\n        self.study_label_map = {0: 'Negative for Pneumonia', 1: 'Typical Appearance', 2: 'Indeterminate Appearance', 3: 'Atypical Appearance'}\n        self.image_label_map_inv = dict([(v, k) for k, v in self.image_label_map.items()])\n        self.study_label_map_inv = dict([(v, k) for k, v in self.study_label_map.items()])\n\n        self.loader = default_loader\n\n    def __getitem__(self, index):\n        if self.mode == \"train\":\n            img_id, study_id, label, boxes, study_targets = self.data[index]\n            path = os.path.join(self.root, self.mode, f\"{img_id.replace('_image','')}.png\")\n            img = self.loader(path)\n\n            if self.transform is not None:\n                if type(self.transform) == A.Compose:\n                    img = self.transform(image=np.array(img))[\"image\"]\n                else:\n                    img = self.transform(img)\n\n            image_level_target, boxes = self.parse_label(label, boxes, img_id)\n            if not self.xy:\n                boxes[:,[0,1,2,3]] = boxes[:,[1,0,3,2]]\n            study_level_target = torch.as_tensor(study_targets, dtype=torch.int64)\n\n            return img, img_id, study_id, boxes, image_level_target, study_level_target\n        else:\n            pred_id, img_id, study_id, dim0, dim1 = self.data[index]\n            path = os.path.join(self.root, self.mode, f\"{img_id.replace('_image','')}.png\")\n            img = self.loader(path)\n            \n            if self.transform is not None:\n                if type(self.transform) == A.Compose:\n                    img = self.transform(image=np.array(img))[\"image\"]\n                else:\n                    img = self.transform(img)\n            \n            return pred_id, img, img_id, study_id, dim0, dim1\n\n    def __len__(self):\n        return len(self.data)\n\n    def get_img_meta_map(self, root):\n        meta_df = pd.read_csv(os.path.join(root, \"meta.csv\"))\n        meta_df = meta_df[meta_df[\"split\"] == \"train\"]\n\n        id_original_img_size_map = {}\n        for row in meta_df.to_numpy():\n            img_id, dim0, dim1, split = row\n            id_original_img_size_map[img_id] = (dim0, dim1)\n\n        return id_original_img_size_map\n\n    def get_data(self, df):\n        data = []\n        for row in df.to_numpy():\n            if self.mode == \"train\":\n                img_id, boxes, label, study_id, nfp, ta, ia, aa = row\n                data.append((img_id, study_id, label, boxes, np.argmax([nfp, ta, ia, aa])))\n            else:\n                pred_id, img_id, study_id, dim0, dim1 = row\n                data.append((pred_id, img_id, study_id, dim0, dim1))\n\n        return data\n\n    def parse_label(self, label, boxes, img_id):\n        splits = label.split(\" \")\n        num_labels = len(splits) // 6\n        boxes_parsed = []\n        for idx in range(num_labels):\n            coordinates_abs = [float(splits[idx*6+2]), float(splits[idx*6+3]), float(splits[idx*6+4]), float(splits[idx*6+5])]\n            # original_coordinates_abs = np.array([float(splits[idx*6+2]), float(splits[idx*6+3]), float(splits[idx*6+4]), float(splits[idx*6+5])])\n\n            if not pd.isna(boxes):\n                box = eval(boxes)[idx]\n                assert box[\"x\"] + box[\"width\"] == coordinates_abs[-2]\n                assert box[\"y\"] + box[\"height\"] == coordinates_abs[-1]\n\n                original_y, original_x = self.id_original_img_size_map[img_id.replace(\"_image\", \"\")]\n                coordinates_abs = resize_bboxes(coordinates_abs, original_x, original_y, self.img_size)\n                boxes_parsed.append(coordinates_abs)\n\n        if len(boxes_parsed) > 0:\n            boxes_parsed = torch.stack(tuple(map(torch.tensor, boxes_parsed)))\n            empty = torch.as_tensor([1]*len(boxes_parsed), dtype=torch.int64)\n        else:\n            boxes_parsed.append([0,0,1,1])\n            boxes_parsed = torch.as_tensor(boxes_parsed, dtype=torch.float32)\n            empty = torch.as_tensor([0], dtype=torch.int64)\n\n        return empty, boxes_parsed\n    \n    \ndef get_batch(batch):\n#     imgs, img_id, study_id, boxes, targets_image, targets_study = batch\n    pred_ids, imgs, image_ids, study_ids, dim0s, dim1s = batch\n    \n    imgs = [img.float().cuda() for img in imgs]\n    \n    return pred_ids, imgs, image_ids, study_ids, dim0s, dim1s\n\n\ndef gen_pred_string(label, score, box):\n    if label == 1:\n        result_label = \"opacity\"\n    else:\n        result_label = \"none\"\n    \n    return f\"{result_label} {str(score)} {int(box[0])} {int(box[1])} {int(box[2])} {int(box[3])}\"\n    \n    \ndef plot_boxes(img, boxes, rsz=None):\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n    from skimage.transform import resize\n               \n    fig, ax = plt.subplots()\n    if rsz is not None:\n        img = resize(img, (rsz[0], rsz[1]))\n    \n    ax.imshow(img)\n    for box in boxes:\n        x, y, x1, y1 = box\n        w = x1 - x\n        h = y1 - y\n        ax.add_patch(\n            patches.Rectangle(\n                (x, y), w, h,\n                edgecolor='green',\n                linewidth=4,\n                fill=True))\n    \n    plt.show()\n\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.xavier_normal_(m.weight)\n        nn.init.constant_(m.bias, 0)\n\n    if type(m) == nn.BatchNorm1d:\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n        \nclass SMPModelDownConv(nn.Module):\n    def __init__(self, config):\n        super(SMPModelDownConv, self).__init__()\n        in_features = get_in_features(config.backbone_name, config.model_type)\n\n        self.hidden_dim = config.hidden_dim\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        if config.decoder_channels:\n            assert len(config.decoder_channels.split(\",\")) == config.decoder_blocks\n            decoder_channels = list(map(int, config.decoder_channels.split(\",\")))\n        else:\n            if config.decoder_blocks == 3:\n                decoder_channels = (512, 32, 16)\n            elif config.decoder_blocks == 4:\n                decoder_channels = (512, 64, 32, 16)\n            elif config.decoder_blocks == 5:\n                decoder_channels = (512, 128, 64, 32, 16)\n\n        self.downconv = nn.Sequential(\n            nn.Conv2d(1,2,kernel_size=5, stride=2, padding=2, bias=False),\n            nn.BatchNorm2d(2),\n            nn.ReLU(),\n        )\n\n        self.seg = smp.UnetPlusPlus(\n            encoder_name=config.backbone_name,\n            encoder_weights=None,\n            classes=config.classes,\n            activation=None,\n        )\n\n        delattr(self.seg, \"decoder\")\n        delattr(self.seg, \"segmentation_head\")\n\n        self.seg.decoder = smp.unetplusplus.decoder.UnetPlusPlusDecoder(\n            encoder_channels=self.seg.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=config.decoder_blocks,\n            use_batchnorm=True,\n            center=False,\n            attention_type=None,\n        )\n\n        self.seg.segmentation_head = smp.unetplusplus.model.SegmentationHead(\n            in_channels=decoder_channels[-1],\n            out_channels=config.classes,\n            activation=None,\n            kernel_size=3,\n        )\n\n        self.mask_type = config.mask_type\n\n        if config.neck_type == \"D\":\n            self.neck = nn.Sequential(\n                nn.Dropout(0.3),\n            )\n            head_feature_dim = in_features\n        elif config.neck_type == \"F\":\n            self.neck = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(in_features, self.hidden_dim, bias=True),\n                nn.BatchNorm1d(self.hidden_dim),\n                torch.nn.PReLU(),\n            )\n            head_feature_dim = self.hidden_dim\n\n        self.neck.apply(init_weights)\n        self.head = nn.Linear(head_feature_dim, 4)\n\n    def forward(self,x):\n        x = x[:,0,:,:].unsqueeze(1)\n        x = torch.cat([self.downconv(x), F.avg_pool2d(x, 2)], dim=1)\n        x = self.seg.encoder(x)[-1]\n        x = self.global_pool(x)\n        x = x[:,:,0,0]\n        x = self.neck(x)\n        logits = self.head(x)\n        return logits\n\n    def forward_mask(self, x):\n        x = x[:,0,:,:].unsqueeze(1)\n        x = torch.cat([self.downconv(x), F.avg_pool2d(x, 2)], dim=1)\n        global_features = self.seg.encoder(x)\n        seg_features = self.seg.decoder(*global_features)\n        seg_features = self.seg.segmentation_head(seg_features)\n        return seg_features\n\n\nclass SMPModelMaskCond(nn.Module):\n    def __init__(self, config):\n        super(SMPModelMaskCond, self).__init__()\n        in_features = get_in_features(config.backbone_name, config.model_type) + 16\n\n        self.hidden_dim = config.hidden_dim\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        if config.decoder_channels:\n            assert len(config.decoder_channels.split(\",\")) == config.decoder_blocks\n            decoder_channels = list(map(int, config.decoder_channels.split(\",\")))\n        else:\n            if config.decoder_blocks == 3:\n                decoder_channels = (512, 32, 16)\n            elif config.decoder_blocks == 4:\n                decoder_channels = (512, 64, 32, 16)\n            elif config.decoder_blocks == 5:\n                decoder_channels = (512, 128, 64, 32, 16)\n\n        self.seg = smp.UnetPlusPlus(\n            encoder_name=config.backbone_name,\n            encoder_weights=None,\n            classes=config.classes,\n            activation=None,\n        )\n\n        delattr(self.seg, \"decoder\")\n        delattr(self.seg, \"segmentation_head\")\n\n        self.seg.decoder = smp.unetplusplus.decoder.UnetPlusPlusDecoder(\n            encoder_channels=self.seg.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=config.decoder_blocks,\n            use_batchnorm=True,\n            center=False,\n            attention_type=None,\n        )\n\n        self.seg.segmentation_head = smp.unetplusplus.model.SegmentationHead(\n            in_channels=decoder_channels[-1],\n            out_channels=config.classes,\n            activation=None,\n            kernel_size=3,\n        )\n\n        self.mask_type = config.mask_type\n\n        if config.neck_type == \"D\":\n            self.neck = nn.Sequential(\n                nn.Dropout(0.3),\n            )\n            head_feature_dim = in_features\n        elif config.neck_type == \"F\":\n            self.neck = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(in_features, self.hidden_dim, bias=True),\n                nn.BatchNorm1d(self.hidden_dim),\n                torch.nn.PReLU(),\n            )\n            head_feature_dim = self.hidden_dim\n\n        self.neck.apply(init_weights)\n        self.head = nn.Linear(head_feature_dim, 4)\n\n        self.mask_features = nn.Sequential(\n            nn.Conv2d(2,4,kernel_size=5, stride=2, padding=2, bias=False),\n            nn.BatchNorm2d(4),\n            nn.ReLU(),\n            nn.Conv2d(4,8,kernel_size=5, stride=2, padding=2, bias=False),\n            nn.BatchNorm2d(8),\n            nn.ReLU(),\n            nn.Conv2d(8,16,kernel_size=5, stride=2, padding=2, bias=False),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n        )\n\n    def forward(self, x, mask_output):\n        mask_output = self.mask_features(mask_output)\n        mask_output = self.global_pool(mask_output)\n        mask_output = mask_output[:,:,0,0]\n        x = self.seg.encoder(x)[-1]\n        x = self.global_pool(x)\n        x = x[:,:,0,0]\n        x = self.neck(torch.cat([x, mask_output], dim=1))\n        logits = self.head(x)\n        return logits\n\n    def forward_mask(self, x):\n        global_features = self.seg.encoder(x)\n        seg_features = self.seg.decoder(*global_features)\n        seg_features = self.seg.segmentation_head(seg_features)\n        return seg_features\n\n    \n\nclass TimmModel(nn.Module):\n    def __init__(self, config):\n        super(TimmModel, self).__init__()\n        self.backbone_name = config.backbone_name\n        self.hidden_dim = config.hidden_dim\n        in_features = get_in_features(self.backbone_name, None)\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        self.base = timm.create_model(\n            self.backbone_name,\n            pretrained=False,\n            num_classes=0,\n            global_pool='',\n        )\n        if config.pretrained_path:\n            self.base.load_state_dict(torch.load(config.pretrained_path))\n\n        if config.neck_type == \"D\":\n            self.neck = nn.Sequential(\n                nn.Dropout(0.3),\n            )\n            head_feature_dim = in_features\n        elif config.neck_type == \"E\":\n            self.neck = nn.Identity()\n            head_feature_dim = in_features\n        elif config.neck_type == \"F\":\n            self.neck = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(in_features, self.hidden_dim, bias=True),\n                nn.BatchNorm1d(self.hidden_dim),\n                torch.nn.PReLU(),\n            )\n            head_feature_dim = self.hidden_dim\n\n        self.neck.apply(init_weights)\n        self.head = nn.Linear(head_feature_dim, config.num_classes)\n\n    def forward(self, x):\n        x = self.base(x)\n\n        # print(x.shape)\n        if 'vit' not in self.backbone_name and 'swin' not in self.backbone_name:\n            x = self.global_pool(x)\n            x = x[:,:,0,0]\n        # print(x.shape)\n        x = self.neck(x)\n\n        logits = self.head(x)\n\n        return logits\n    \n\nclass TimmModel2C(nn.Module):\n    def __init__(self, config, hidden_dim=192):\n        super(TimmModel2C, self).__init__()\n        self.model = TimmModel(config.model_config)\n        if config.model_path:\n            self.model.load_state_dict(torch.load(config.model_path))\n\n        self.model.head = nn.Linear(hidden_dim, 1)\n\n    def forward(self,x):\n        x = self.model(x)\n        return x\n\n    \nclass SMPModel(nn.Module):\n    def __init__(self, config):\n        super(SMPModel, self).__init__()\n        in_features = get_in_features(config.backbone_name, config.model_type)\n\n        self.hidden_dim = config.hidden_dim\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n\n        if config.decoder_channels:\n            assert len(config.decoder_channels.split(\",\")) == config.decoder_blocks\n            decoder_channels = list(map(int, config.decoder_channels.split(\",\")))\n        else:\n            if config.decoder_blocks == 3:\n                decoder_channels = (512, 32, 16)\n            elif config.decoder_blocks == 4:\n                decoder_channels = (512, 64, 32, 16)\n            elif config.decoder_blocks == 5:\n                decoder_channels = (512, 128, 64, 32, 16)\n\n        self.seg = smp.UnetPlusPlus(\n            encoder_name=config.backbone_name,\n            encoder_weights=None,\n            classes=config.classes,\n            activation=None,\n        )\n\n        delattr(self.seg, \"decoder\")\n        delattr(self.seg, \"segmentation_head\")\n\n        self.seg.decoder = smp.unetplusplus.decoder.UnetPlusPlusDecoder(\n            encoder_channels=self.seg.encoder.out_channels,\n            decoder_channels=decoder_channels,\n            n_blocks=config.decoder_blocks,\n            use_batchnorm=True,\n            center=False,\n            attention_type=None,\n        )\n\n        self.seg.segmentation_head = smp.unetplusplus.model.SegmentationHead(\n            in_channels=decoder_channels[-1],\n            out_channels=config.classes,\n            activation=None,\n            kernel_size=3,\n        )\n\n        self.mask_type = config.mask_type\n\n        if config.neck_type == \"D\":\n            self.neck = nn.Sequential(\n                nn.Dropout(0.3),\n            )\n            head_feature_dim = in_features\n        elif config.neck_type == \"F\":\n            self.neck = nn.Sequential(\n                nn.Dropout(0.3),\n                nn.Linear(in_features, self.hidden_dim, bias=True),\n                nn.BatchNorm1d(self.hidden_dim),\n                torch.nn.PReLU(),\n            )\n            head_feature_dim = self.hidden_dim\n\n        self.neck.apply(init_weights)\n        self.head = nn.Linear(head_feature_dim, 4)\n\n    def forward(self,x):\n        x = self.seg.encoder(x)[-1]\n        x = self.global_pool(x)\n        x = x[:,:,0,0]\n        x = self.neck(x)\n        logits = self.head(x)\n        return logits\n\n    def forward_mask(self, x):\n        global_features = self.seg.encoder(x)\n        seg_features = self.seg.decoder(*global_features)\n        seg_features = self.seg.segmentation_head(seg_features)\n        return seg_features\n\n\nclass SMP2CModel(nn.Module):\n    def __init__(self, config, hidden_dim=512):\n        super(SMP2CModel, self).__init__()\n\n        self.smp_model = SMPModel(config.unet_smp)\n        if config.smp_path:\n            self.smp_model.load_state_dict(torch.load(config.smp_path))\n\n        self.smp_model.head = nn.Linear(hidden_dim, 1)\n\n    def forward(self,x):\n        x = self.smp_model(x)\n        return x\n\n    def forward_mask(self, x):\n        seg_features = self.smp_model.forward_mask(x)\n        return seg_features\n\n\ndef get_in_features(backbone, model_type):\n    if 'tf_efficientnetv2_m' in backbone:\n        if model_type == \"pspnet\":\n            in_features = 80\n        else:\n            in_features = 512\n    elif 'tf_efficientnetv2_l' in backbone:\n        if model_type == \"pspnet\":\n            in_features = 80\n        else:\n            in_features = 640\n    elif 'efficientnet' in backbone:\n        if \"b7\" in backbone:\n            in_features = 640\n        if \"b6\" in backbone:\n            in_features = 576\n        else:\n            in_features = 512\n    elif 'inceptionresnet' in backbone:\n        if model_type == \"pspnet\":\n            in_features = 320\n        else:\n            in_features = 1536\n    elif 'resnet' in backbone:\n        in_features = 2048\n    elif 'densenet169' in backbone:\n        in_features = 1664\n    elif 'densenet161' in backbone:\n        in_features = 2208\n    elif 'vit' in backbone:\n        in_features = 768\n    elif 'swin' in backbone:\n        in_features = 1536\n    elif 'xception' in backbone:\n        in_features = 2048\n    elif 'inception' in backbone:\n        in_features = 1536\n    elif 'resnext' in backbone:\n        in_features = 2048\n    else:\n        in_features = 2048\n\n    return in_features\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:01:08.876162Z","iopub.execute_input":"2021-08-09T10:01:08.876771Z","iopub.status.idle":"2021-08-09T10:01:09.03079Z","shell.execute_reply.started":"2021-08-09T10:01:08.876726Z","shell.execute_reply":"2021-08-09T10:01:09.029565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuples = []\nsplit = \"test\"\nsave_dir = f\"/kaggle/tmp/{split}\"\nos.makedirs(save_dir, exist_ok=True)\nif DEBUG:\n    tuples = [\n        (\"65761e66de9f.dcm\", \"/kaggle/input/siim-covid19-detection/train/00086460a852/9e8302230c91\", save_dir, \"test\"),\n        (\"51759b5579bc.dcm\", \"/kaggle/input/siim-covid19-detection/train/000c9c05fd14/e555410bd2cd\", save_dir, \"test\"),\n    ]\nelse:\n    for dirname, _, filenames in tqdm(os.walk(f'/kaggle/input/siim-covid19-detection/{split}')):\n        for file in filenames:\n            tuples.append((file, dirname, save_dir, split))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:01:09.03618Z","iopub.execute_input":"2021-08-09T10:01:09.0368Z","iopub.status.idle":"2021-08-09T10:01:09.086736Z","shell.execute_reply.started":"2021-08-09T10:01:09.036748Z","shell.execute_reply":"2021-08-09T10:01:09.085495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Study-level","metadata":{}},{"cell_type":"code","source":"def resize_and_save_study(t):\n    file, dirname, save_dir, _ = t\n    # set keep_ratio=True to have original aspect ratio\n    xray = read_xray(os.path.join(dirname, file))\n    im = resize(xray, size=STUDY_IMG_SIZE) \n    im.save(os.path.join(save_dir, file.replace('dcm', 'png')))\n#     image_id.append(f\"{file.replace('.dcm', '')}\")\n#     study_id.append(dirname.split('/')[-2])\n#     dim0.append(xray.shape[0])\n#     dim1.append(xray.shape[1])\n    \n    with open(\"/kaggle/working/temp-study\", \"a\") as f:\n        f.write(f\"{file.replace('.dcm', '')} {dirname.split('/')[-2]} {xray.shape[0]} {xray.shape[1]}\\n\")\n    \npool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n# for t in tqdm(tuples):\nfor _ in tqdm(pool.imap_unordered(resize_and_save_study, tuples), total=len(tuples)):\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:01:09.089949Z","iopub.execute_input":"2021-08-09T10:01:09.090492Z","iopub.status.idle":"2021-08-09T10:01:09.915053Z","shell.execute_reply.started":"2021-08-09T10:01:09.090446Z","shell.execute_reply":"2021-08-09T10:01:09.91314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_and_meta = pd.read_csv(\"/kaggle/working/temp-study\", sep=\" \", header=None).rename(columns={0: \"image_id\", 1: \"study_id\", 2: \"dim0\", 3: \"dim1\"})\n\ndf_and_meta_study = copy.deepcopy(df_and_meta)\ndf_and_meta_study[\"id\"] = df_and_meta_study[\"study_id\"] + \"_study\"\ndf_and_meta_image = copy.deepcopy(df_and_meta)\ndf_and_meta_image[\"id\"] = df_and_meta_image[\"image_id\"] + \"_image\"\ndf_and_meta = pd.concat([df_and_meta_study, df_and_meta_image])\n\nif DEBUG:\n    df = pd.DataFrame(([['00086460a852_study', 'negative 1 0 0 1 1'], \n                         ['000c9c05fd14_study', 'negative 1 0 0 1 1'], \n                         ['65761e66de9f_image', 'none 1 0 0 1 1'], \n                         ['51759b5579bc_image', 'none 1 0 0 1 1']]), \n                       columns=['id', 'PredictionString'])\nelse:\n    df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\nid_laststr_list  = []\nfor i in range(df.shape[0]):\n    id_laststr_list.append(df.loc[i,'id'][-1])\ndf['id_last_str'] = id_laststr_list\ndf = df.merge(df_and_meta, on=\"id\")\n\n# if AUG:\n#     tta_transform, transform = get_study_transform(STUDY_IMG_SIZE)\n#     if TTA:\n#         transform = tta_transform\n# else:\n#     transform = T.Compose([\n#         T.ToTensor(),\n#         T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n#     ])\n\ndf_study = df[df[\"id_last_str\"] == \"y\"]\ndf_study = df_study[[\"id\", \"image_id\", \"study_id\", \"dim0\", \"dim1\"]]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:01:09.917789Z","iopub.execute_input":"2021-08-09T10:01:09.918283Z","iopub.status.idle":"2021-08-09T10:01:10.097331Z","shell.execute_reply.started":"2021-08-09T10:01:09.918222Z","shell.execute_reply":"2021-08-09T10:01:10.096201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx_map = {0: \"negative\", 1: \"typical\", 2: \"indeterminate\", 3: \"atypical\"}\nresult = {\n    \"id\": [],\n    \"PredictionString\": []\n}","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:01:10.099202Z","iopub.execute_input":"2021-08-09T10:01:10.099619Z","iopub.status.idle":"2021-08-09T10:01:10.152269Z","shell.execute_reply.started":"2021-08-09T10:01:10.099572Z","shell.execute_reply":"2021-08-09T10:01:10.1511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TTA:\n    tta_result = []\n    for i in range(3):\n        dataset_study = COVIDDataset(root=\"/kaggle/tmp\", df=df_study, mode=\"test\", transform=transform)\n        dataloader_study = DataLoader(dataset_study, batch_size=2, collate_fn=collate_fn, pin_memory=True, num_workers=8)\n        for idx, batch in enumerate(dataloader_study):\n            pred_ids, imgs, image_ids, study_ids, dim0s, dim1s = get_batch(batch)\n\n            imgs = torch.stack(imgs)\n            \n            outputs_study = []\n            for idx, (img_size, study_model) in enumerate(zip(img_sizes, study_models)):\n                imgs = F.interpolate(imgs, (img_size, img_size))\n                outputs_study.append(F.softmax(study_model(imgs), dim=1).cpu().detach().numpy())\n                \n            outputs_study = np.mean(np.array(outputs_study), axis=0)\n            for idx, (pred_id, _, study_id, output_study) in enumerate(zip(pred_ids, image_ids, study_ids, outputs_study)):\n                tta_result.append({\"output_study\": output_study, \"id\": f\"{study_id}_study\"})\n        if DEBUG:\n            if i == 1:\n                break\n    tta_result = pd.DataFrame(tta_result).groupby(\"id\").apply(np.average).reset_index().rename(columns={0: \"PredictionString\"})\n    tta_result[\"PredictionString\"] = tta_result[\"PredictionString\"].apply(lambda x: \" \".join([f\"{idx_map[i]} {val:.5f} 0 0 1 1\" for i, val in enumerate(x)]))\n    df_result = pd.concat((pd.DataFrame(result), tta_result))\n    \nelse:\n    ensemble_result = []\n    for modelpaths in MODELPATH:\n        study_models = []\n        img_sizes = []\n        model_names = []\n        for model_path, img_size, backbone_name, model_name in modelpaths:\n            if model_name == \"unet_smp\":\n                config.unet_smp.backbone_name = backbone_name\n                study_model = SMPModel(config.unet_smp)\n            elif model_name == \"unet_smp_downconv\":\n                config.unet_smp.backbone_name = backbone_name\n                study_model = SMPModelDownConv(config.unet_smp)\n            elif model_name == \"unet_smp_mask_cond\":\n                config.unet_smp.backbone_name = backbone_name\n                study_model = SMPModelMaskCond(config.unet_smp)\n            elif model_name == \"timm\":\n                config_timm.model_config.backbone_name = backbone_name\n                study_model = TimmModel(config_timm.model_config)\n                \n            study_model.load_state_dict(torch.load(model_path)) \n            study_model.to(\"cuda:0\")\n            study_model.eval()\n            study_models.append(study_model)\n            img_sizes.append(img_size)\n            model_names.append(model_name)\n        \n        _, transform = get_study_transform(img_sizes[0])\n        dataset_study = COVIDDataset(root=\"/kaggle/tmp\", df=df_study, mode=\"test\", transform=transform)\n        dataloader_study = DataLoader(dataset_study, batch_size=2, collate_fn=collate_fn, pin_memory=True, num_workers=8)\n        for idx, batch in enumerate(dataloader_study):\n            pred_ids, imgs, image_ids, study_ids, dim0s, dim1s = get_batch(batch)\n            imgs = torch.stack(imgs)\n\n            outputs_study = []\n            for idx, (img_size, study_model, model_name) in enumerate(zip(img_sizes, study_models, model_names)):\n                imgs = F.interpolate(imgs, (img_size, img_size))\n                if model_name == \"unet_smp_mask_cond\":\n                    outputs_masks = study_model.forward_mask(imgs)\n                    outputs_study.append(F.softmax(study_model(imgs, outputs_masks), dim=1).cpu().detach().numpy())\n                else:\n                    outputs_study.append(F.softmax(study_model(imgs), dim=1).cpu().detach().numpy())\n#             print(outputs_study)\n            outputs_study = np.mean(np.array(outputs_study), axis=0)\n            for idx, (pred_id, _, study_id, output_study) in enumerate(zip(pred_ids, image_ids, study_ids, outputs_study)):\n                ensemble_result.append({\"output_study\": output_study, \"id\": f\"{study_id}_study\"})\n        del study_models\n        gc.collect()\n#                 pred_string_study = \" \".join([f\"{idx_map[i]} {val:.5f} 0 0 1 1\" for i, val in enumerate(output_study)])\n#                 result[\"id\"].append(f\"{study_id}_study\")\n#                 result[\"PredictionString\"].append(pred_string_study)\n#     df_result = pd.DataFrame(result)\n\n\n    ensemble_result = pd.DataFrame(ensemble_result).groupby(\"id\").apply(np.average).reset_index().rename(columns={0: \"PredictionString\"})\n    ensemble_result[\"PredictionString\"] = ensemble_result[\"PredictionString\"].apply(lambda x: \" \".join([f\"{idx_map[i]} {val:.5f} 0 0 1 1\" for i, val in enumerate(x)]))\n    df_result = pd.concat((pd.DataFrame(result), ensemble_result))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:01:10.154228Z","iopub.execute_input":"2021-08-09T10:01:10.154864Z","iopub.status.idle":"2021-08-09T10:03:40.112711Z","shell.execute_reply.started":"2021-08-09T10:01:10.154818Z","shell.execute_reply":"2021-08-09T10:03:40.111539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 Class","metadata":{}},{"cell_type":"code","source":"df_2c = df[df[\"id_last_str\"] == \"e\"]\ndf_2c = df_2c[[\"id\", \"image_id\", \"study_id\", \"dim0\", \"dim1\"]]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:03:40.114401Z","iopub.execute_input":"2021-08-09T10:03:40.11486Z","iopub.status.idle":"2021-08-09T10:03:40.607751Z","shell.execute_reply.started":"2021-08-09T10:03:40.114812Z","shell.execute_reply":"2021-08-09T10:03:40.606279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# study_models_2class = []\n# image_sizes = []\n# for model_path, image_size, backbone_name, model_name in MODELPATH2C:\n#     if model_name == \"unet_smp_2c\":\n#         config.unet_smp.backbone_name = backbone_name\n#         study_model = SMP2CModel(config)\n#         study_model.load_state_dict(torch.load(model_path)) \n#         study_model.to(\"cuda:0\")\n#         study_model.eval()\n#         study_models_2class.append(study_model)\n#     elif model_name == \"timm_2c\":\n#         config_timm.model_config.backbone_name = backbone_name\n#         study_model = TimmModel2C(config_timm)\n#         study_model.load_state_dict(torch.load(model_path)) \n#         study_model.to(\"cuda:0\")\n#         study_model.eval()\n#         study_models_2class.append(study_model)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:03:40.611986Z","iopub.execute_input":"2021-08-09T10:03:40.612317Z","iopub.status.idle":"2021-08-09T10:03:40.667893Z","shell.execute_reply.started":"2021-08-09T10:03:40.612283Z","shell.execute_reply":"2021-08-09T10:03:40.666604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_2class = {}\nensemble_result = []\nfor model_paths in MODELPATH2C:\n    study_models = []\n    for model_path, image_size, backbone_name, model_name in model_paths:\n        if model_name == \"unet_smp_2c\":\n            config.unet_smp.backbone_name = backbone_name\n            study_model = SMP2CModel(config)\n            study_model.load_state_dict(torch.load(model_path)) \n            study_model.to(\"cuda:0\")\n            study_model.eval()\n        elif model_name == \"timm_2c\":\n            config_timm.model_config.backbone_name = backbone_name\n            study_model = TimmModel2C(config_timm)\n            study_model.load_state_dict(torch.load(model_path)) \n            study_model.to(\"cuda:0\")\n            study_model.eval()\n        study_models.append(study_model)\n        \n    _, transform = get_study_transform(image_size)\n    dataset_2c = COVIDDataset(root=\"/kaggle/tmp\", df=df_2c, mode=\"test\", transform=transform)\n    dataloader_2c = DataLoader(dataset_2c, batch_size=2, collate_fn=collate_fn, pin_memory=True, num_workers=8)\n    for idx, batch in enumerate(dataloader_2c):\n        pred_ids, imgs, image_ids, study_ids, dim0s, dim1s = get_batch(batch)\n        imgs = torch.stack(imgs)\n        outputs_2c = []\n        for study_model in study_models:\n            assert imgs[0].shape[-1] == image_size\n            pred = torch.sigmoid(study_model(imgs))\n            outputs_2c.append(pred.cpu().detach().numpy())\n        outputs_2c = np.mean(np.array(outputs_2c), axis=0)\n        for image_id, output_2c in zip(image_ids, outputs_2c):\n            ensemble_result.append({\"output_study\": output_2c, \"id\": f\"{image_id}_image\"})\n    del study_models\ndf_2class = pd.DataFrame(ensemble_result).groupby(\"id\").apply(np.average).reset_index().rename(columns={0: \"PredictionString\"})\ndf_2class = dict(zip(df_2class[\"id\"], [i[0] for i in df_2class[\"PredictionString\"]]))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:03:40.669718Z","iopub.execute_input":"2021-08-09T10:03:40.670235Z","iopub.status.idle":"2021-08-09T10:05:33.496222Z","shell.execute_reply.started":"2021-08-09T10:03:40.670171Z","shell.execute_reply":"2021-08-09T10:05:33.494908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_2class","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:05:33.498304Z","iopub.execute_input":"2021-08-09T10:05:33.498789Z","iopub.status.idle":"2021-08-09T10:05:33.745467Z","shell.execute_reply.started":"2021-08-09T10:05:33.498736Z","shell.execute_reply":"2021-08-09T10:05:33.74423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_2class = {}\n# _, transform = get_study_transform(TWOCLASS_IMG_SIZE)\n# dataset_2c = COVIDDataset(root=\"/kaggle/tmp\", df=df_2c, mode=\"test\", transform=transform)\n# dataloader_2c = DataLoader(dataset_2c, batch_size=2, collate_fn=collate_fn, pin_memory=True, num_workers=8)\n# for idx, batch in enumerate(dataloader_2c):\n#     pred_ids, imgs, image_ids, study_ids, dim0s, dim1s = get_batch(batch)\n\n#     outputs_2c = []\n#     for image_size, study_model in zip(image_sizes, study_models_2class):\n#         assert imgs[0].shape[-1] == image_size\n#         pred = torch.sigmoid(study_model(torch.stack(imgs)))\n#         outputs_2c.append(pred.cpu().detach().numpy())\n#     outputs_2c = np.mean(np.array(outputs_2c), axis=0)\n#     for idx, (pred_id, image_id, _, output_2c) in enumerate(zip(pred_ids, image_ids, study_ids, outputs_2c)):\n#         df_2class[f\"{image_id}_image\"] = output_2c[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:05:33.747468Z","iopub.execute_input":"2021-08-09T10:05:33.748121Z","iopub.status.idle":"2021-08-09T10:05:33.80252Z","shell.execute_reply.started":"2021-08-09T10:05:33.74807Z","shell.execute_reply":"2021-08-09T10:05:33.801136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image-level","metadata":{}},{"cell_type":"code","source":"tuples = []\nsplit = \"test\"\nIMAGE_IMG_SIZE = [1280, 640, 512]\nsave_dirs = [f\"/kaggle/tmp/{split}/{img_size}\" for img_size in IMAGE_IMG_SIZE]\nfor save_dir in save_dirs:\n    os.makedirs(save_dir, exist_ok=True)\nif DEBUG:\n    tuples = [\n        (\"65761e66de9f.dcm\", \"/kaggle/input/siim-covid19-detection/train/00086460a852/9e8302230c91\", save_dirs, \"test\"),\n        (\"51759b5579bc.dcm\", \"/kaggle/input/siim-covid19-detection/train/000c9c05fd14/e555410bd2cd\", save_dirs, \"test\"),\n    ]\nelse:\n    for dirname, _, filenames in tqdm(os.walk(f'/kaggle/input/siim-covid19-detection/{split}')):\n        for file in filenames:\n            tuples.append((file, dirname, save_dirs, split))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:05:33.804655Z","iopub.execute_input":"2021-08-09T10:05:33.805403Z","iopub.status.idle":"2021-08-09T10:05:33.863353Z","shell.execute_reply.started":"2021-08-09T10:05:33.805346Z","shell.execute_reply":"2021-08-09T10:05:33.862185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize_and_save_image(t):\n    file, dirname, save_dirs, _ = t\n    # set keep_ratio=True to have original aspect ratio\n    xray = read_xray(os.path.join(dirname, file))\n    for img_size, save_dir in zip(IMAGE_IMG_SIZE, save_dirs):\n        im = resize(xray, size=img_size) \n        im.save(os.path.join(save_dir, file.replace('dcm', 'png')))\n#     image_id.append(f\"{file.replace('.dcm', '')}\")\n#     study_id.append(dirname.split('/')[-2])\n#     dim0.append(xray.shape[0])\n#     dim1.append(xray.shape[1])\n    \n    with open(\"/kaggle/working/temp-image\", \"a\") as f:\n        f.write(f\"{file.replace('.dcm', '')} {dirname.split('/')[-2]} {xray.shape[0]} {xray.shape[1]}\\n\")\n    \npool = multiprocessing.Pool(processes=multiprocessing.cpu_count())\n# for t in tqdm(tuples):\nfor _ in tqdm(pool.imap_unordered(resize_and_save_image, tuples), total=len(tuples)):\n    pass","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:05:33.865294Z","iopub.execute_input":"2021-08-09T10:05:33.865868Z","iopub.status.idle":"2021-08-09T10:05:35.930037Z","shell.execute_reply.started":"2021-08-09T10:05:33.865775Z","shell.execute_reply":"2021-08-09T10:05:35.928352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_and_meta = pd.read_csv(\"/kaggle/working/temp-image\", sep=\" \", header=None).rename(columns={0: \"image_id\", 1: \"study_id\", 2: \"dim0\", 3: \"dim1\"})\n\ndf_and_meta_study = copy.deepcopy(df_and_meta)\ndf_and_meta_study[\"id\"] = df_and_meta_study[\"study_id\"] + \"_study\"\ndf_and_meta_image = copy.deepcopy(df_and_meta)\ndf_and_meta_image[\"id\"] = df_and_meta_image[\"image_id\"] + \"_image\"\ndf_and_meta = pd.concat([df_and_meta_study, df_and_meta_image])\n\nif DEBUG:\n    df = pd.DataFrame(([['00086460a852_study', 'negative 1 0 0 1 1'], \n                         ['000c9c05fd14_study', 'negative 1 0 0 1 1'], \n                         ['65761e66de9f_image', 'none 1 0 0 1 1'], \n                         ['51759b5579bc_image', 'none 1 0 0 1 1']]), \n                       columns=['id', 'PredictionString'])\nelse:\n    df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\nid_laststr_list  = []\nfor i in range(df.shape[0]):\n    id_laststr_list.append(df.loc[i,'id'][-1])\ndf['id_last_str'] = id_laststr_list\ndf = df.merge(df_and_meta, on=\"id\")\n\ndf_image = df[df[\"id_last_str\"] == \"e\"]\ndf_image = df_image[[\"id\", \"image_id\", \"study_id\", \"dim0\", \"dim1\"]]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:05:35.932811Z","iopub.execute_input":"2021-08-09T10:05:35.933557Z","iopub.status.idle":"2021-08-09T10:05:36.05813Z","shell.execute_reply.started":"2021-08-09T10:05:35.933473Z","shell.execute_reply":"2021-08-09T10:05:36.056844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_image","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:05:36.059937Z","iopub.execute_input":"2021-08-09T10:05:36.060394Z","iopub.status.idle":"2021-08-09T10:05:36.136078Z","shell.execute_reply.started":"2021-08-09T10:05:36.060347Z","shell.execute_reply":"2021-08-09T10:05:36.13436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CascadeRCNN","metadata":{}},{"cell_type":"code","source":"bbox_preds_df = []","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:30:54.576721Z","iopub.execute_input":"2021-08-09T10:30:54.577136Z","iopub.status.idle":"2021-08-09T10:30:54.855216Z","shell.execute_reply.started":"2021-08-09T10:30:54.577101Z","shell.execute_reply":"2021-08-09T10:30:54.853825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/mmdetection')\n\nfrom tqdm.notebook import tqdm\n\nimport torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device.type)\n\nimport torchvision\nprint(torch.__version__, torch.cuda.is_available())\n\n# Check mmcv installation\nfrom mmcv.ops import get_compiling_cuda_version, get_compiler_version\nprint(get_compiling_cuda_version())\nprint(get_compiler_version())\n\n# Check MMDetection installation\nfrom mmdet.apis import set_random_seed\n\n# Imports\nimport mmdet\nfrom mmdet.apis import set_random_seed\nfrom mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\n\nimport mmcv\nfrom mmcv import Config\nfrom mmcv.runner import load_checkpoint\nfrom mmcv.parallel import MMDataParallel\nfrom mmdet.apis import inference_detector, init_detector, show_result_pyplot\nfrom mmdet.apis import single_gpu_test\nfrom mmdet.datasets import build_dataloader, build_dataset\n\nimport cv2\nimport matplotlib.pyplot as plt\n\nlabel2color = [[59, 238, 119]]\n\nviz_labels =  [\"Covid_Abnormality\"]\n\ndef plot_img(img, size=(18, 18), is_rgb=True, title=\"\", cmap=None):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n    \ndef plot_imgs(imgs, cols=2, size=10, is_rgb=True, title=\"\", cmap=None, img_size=None):\n    rows = len(imgs)//cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    return fig\n    \ndef draw_bbox(image, box, label, color):   \n    alpha = 0.1\n    alpha_font = 0.6\n    thickness = 8\n    font_size = 2.0\n    font_weight = 1\n    overlay_bbox = image.copy()\n    overlay_text = image.copy()\n    output = image.copy()\n\n    text_width, text_height = cv2.getTextSize(label.upper(), cv2.FONT_HERSHEY_SIMPLEX, font_size, font_weight)[0]\n    cv2.rectangle(overlay_bbox, (box[0], box[1]), (box[2], box[3]),\n                color, -1)\n    cv2.addWeighted(overlay_bbox, alpha, output, 1 - alpha, 0, output)\n    cv2.rectangle(overlay_text, (box[0], box[1]-18-text_height), (box[0]+text_width+8, box[1]),\n                (0, 0, 0), -1)\n    cv2.addWeighted(overlay_text, alpha_font, output, 1 - alpha_font, 0, output)\n    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]),\n                    color, thickness)\n    cv2.putText(output, label.upper(), (box[0], box[1]-12),\n            cv2.FONT_HERSHEY_SIMPLEX, font_size, (255, 255, 255), font_weight, cv2.LINE_AA)\n    return output\n\ndef draw_bbox_small(image, box, label, color):   \n    alpha = 0.1\n    alpha_text = 0.3\n    thickness = 1\n    font_size = 0.4\n    overlay_bbox = image.copy()\n    overlay_text = image.copy()\n    output = image.copy()\n\n    text_width, text_height = cv2.getTextSize(label.upper(), cv2.FONT_HERSHEY_SIMPLEX, font_size, thickness)[0]\n    cv2.rectangle(overlay_bbox, (box[0], box[1]), (box[2], box[3]),\n                color, -1)\n    cv2.addWeighted(overlay_bbox, alpha, output, 1 - alpha, 0, output)\n    cv2.rectangle(overlay_text, (box[0], box[1]-7-text_height), (box[0]+text_width+2, box[1]),\n                (0, 0, 0), -1)\n    cv2.addWeighted(overlay_text, alpha_text, output, 1 - alpha_text, 0, output)\n    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]),\n                    color, thickness)\n    cv2.putText(output, label.upper(), (box[0], box[1]-5),\n            cv2.FONT_HERSHEY_SIMPLEX, font_size, (255, 255, 255), thickness, cv2.LINE_AA)\n    return output\n\nbaseline_cfg_path = \"/kaggle/input/siimcovid19mmdet/job4_cascade_rcnn_x101_32x4d_fpn_1x_coco.py\"\ncfg = Config.fromfile(baseline_cfg_path)\n\ncfg.classes = (\"Covid_Abnormality\")\ncfg.data.test.img_prefix = ''\ncfg.data.test.classes = cfg.classes\n\n# cfg.model.roi_head.bbox_head.num_classes = 1\n# cfg.model.bbox_head.num_classes = 1\nfor head in cfg.model.roi_head.bbox_head:\n    head.num_classes = 1\n\n# Set seed thus the results are more reproducible\ncfg.seed = 211\nset_random_seed(211, deterministic=False)\ncfg.gpu_ids = [0]\n\ncfg.data.test.pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(1333, 800),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip', direction='horizontal'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]\n\ncfg.test_pipeline = [\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(1333, 800),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip', direction='horizontal'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]\n\n# cfg.data.samples_per_gpu = 4\n# cfg.data.workers_per_gpu = 4\n# cfg.model.test_cfg.nms.iou_threshold = 0.3\ncfg.model.test_cfg.rcnn.score_thr = 0.001\n\nWEIGHTS_FILES = [f'/kaggle/input/siimcovid19mmdet/mmdet-fold{fold}.pth' for fold in range(5)]\nMODELS = [init_detector(cfg, WEIGHTS_FILES[fold], device='cuda:0') for fold in range(5)]\n\nmmdet_df = df_image.copy()\nif len(save_dirs) > 1 and save_dirs[2][-3:] == '512':\n    save_dir = save_dirs[2]\nelse:\n    save_dir = '/kaggle/tmp/test/512'\nmmdet_df['image_path'] = save_dir+'/'+mmdet_df['image_id']+'.png'\nmmdet_df\n\nscore_threshold = cfg.model.test_cfg.rcnn.score_thr\n\ndef format_pred(boxes: np.ndarray, scores: np.ndarray, labels: np.ndarray) -> str:\n    pred_strings = []\n#     label_str = ['opacity']\n    label_str = ['0']\n    for label, score, bbox in zip(labels, scores, boxes):\n        xmin, ymin, xmax, ymax = bbox.astype(np.int64)\n        pred_strings.append(f\"{label_str[int(label)]} {score:.16f} {xmin} {ymin} {xmax} {ymax}\")\n    return \" \".join(pred_strings)\n\n\nfor fold, model in enumerate(MODELS):\n    \n    results = []\n    viz_images = []\n    \n    model.to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        for index, row in tqdm(mmdet_df.iterrows(), total=mmdet_df.shape[0]):\n            original_H, original_W = (int(row.dim0), int(row.dim1))\n            predictions = inference_detector(model, row.image_path)\n            boxes, scores, labels = (list(), list(), list())\n\n            for k, cls_result in enumerate(predictions):\n    #             print(\"cls_result\", cls_result)\n                if cls_result.size != 0:\n                    if len(labels)==0:\n                        boxes = np.array(cls_result[:, :4])\n                        scores = np.array(cls_result[:, 4])\n                        labels = np.array([k]*len(cls_result[:, 4]))\n                    else:    \n                        boxes = np.concatenate((boxes, np.array(cls_result[:, :4])))\n                        scores = np.concatenate((scores, np.array(cls_result[:, 4])))\n                        labels = np.concatenate((labels, [k]*len(cls_result[:, 4])))\n                        \n                    \n                if DEBUG:\n                    img_viz = cv2.imread(row.image_path)\n                    for box, label, score in zip(boxes, labels, scores):\n                        color = label2color[int(label)]\n                        img_viz = draw_bbox_small(img_viz, box.astype(np.int32), f'opacity_{score:.4f}', color)\n                    viz_images.append(img_viz)\n\n            indexes = np.where(scores > score_threshold)\n    #         print(indexes)\n            boxes = boxes[indexes]\n            scores = scores[indexes]\n            labels = labels[indexes]\n\n            if len(labels) != 0:\n                h_ratio = original_H/512\n                w_ratio = original_W/512\n                boxes[:, [0, 2]] *= w_ratio\n                boxes[:, [1, 3]] *= h_ratio\n\n                result = {\n                    \"id\": row.id,\n                    \"PredictionString\": format_pred(boxes, scores, labels),\n                }\n\n                results.append(result)\n                \n    \n    detection_df = pd.DataFrame(results, columns=['id', 'PredictionString'])\n    detection_df = pd.merge(df_image[['id']].copy(), detection_df, on='id', how='left').fillna('0 0.0001 0 0 1 1')\n    bbox_preds_df.append(detection_df)\n    \ndel model, MODELS\ngc.collect()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:30:58.944223Z","iopub.execute_input":"2021-08-09T10:30:58.944635Z","iopub.status.idle":"2021-08-09T10:31:11.222126Z","shell.execute_reply.started":"2021-08-09T10:30:58.944594Z","shell.execute_reply":"2021-08-09T10:31:11.220743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numba import cuda\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:31:11.224373Z","iopub.execute_input":"2021-08-09T10:31:11.224851Z","iopub.status.idle":"2021-08-09T10:31:14.154463Z","shell.execute_reply.started":"2021-08-09T10:31:11.224805Z","shell.execute_reply":"2021-08-09T10:31:14.153413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## YOLO","metadata":{}},{"cell_type":"code","source":"rm -rf '/kaggle/working/yolov5'","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:31:14.157006Z","iopub.execute_input":"2021-08-09T10:31:14.157478Z","iopub.status.idle":"2021-08-09T10:31:14.589149Z","shell.execute_reply.started":"2021-08-09T10:31:14.15743Z","shell.execute_reply":"2021-08-09T10:31:14.587792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shutil.copytree('/kaggle/input/siimcovid19yolov5test/yolov5', '/kaggle/working/yolov5')\n# shutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5')\nshutil.copytree('/kaggle/input/siimcovid19yolov5test/yolov5', '/kaggle/working/yolov5')\nos.chdir('/kaggle/working/yolov5')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:31:14.593441Z","iopub.execute_input":"2021-08-09T10:31:14.593858Z","iopub.status.idle":"2021-08-09T10:31:15.763699Z","shell.execute_reply.started":"2021-08-09T10:31:14.593792Z","shell.execute_reply":"2021-08-09T10:31:15.762534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_HYP = [\n    (512, 'yolov5x-e-40-img-512', [f'/kaggle/input/yolov5x-e-40-img-512/yolov5x-e-40-img-512-fold-{fold}.pt' for fold in range(5)]),\n    (640, 'yolov5x6-e-10-img-640-b48', [f'/kaggle/input/yolov5x6-e-10-img-640-b48/yolov5x6-e-10-img-640-fold-{fold}-b48.pt' for fold in range(5)]),\n    (640, 'yolov5x6-e-10-img-640', [f'/kaggle/input/yolov5x6-e-10-img-640/yolov5x6-e-10-img-640-fold-{fold}.pt' for fold in range(5)]),\n    (640, 'yolov5x-e-10-img-640', [f'/kaggle/input/yolov5x-e-10-img-640/yolov5x-e-10-img-640-fold-{fold}.pt' for fold in range(5)]),\n    (1280, 'yolov5x6-e-40-img-1280', [f'/kaggle/input/yolov5x6-e-40-img-1280/yolov5x6-e-40-img-1280-fold-{fold}.pt' for fold in range(5)]),\n]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:31:15.765559Z","iopub.execute_input":"2021-08-09T10:31:15.766032Z","iopub.status.idle":"2021-08-09T10:31:15.833246Z","shell.execute_reply.started":"2021-08-09T10:31:15.765983Z","shell.execute_reply":"2021-08-09T10:31:15.832024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img_size, model, model_path in MODEL_HYP:\n    source_path = f'/kaggle/tmp/test/{img_size}'\n    for fold in range(5):\n        !python detect.py --weights {model_path[fold]} \\\n                          --img {img_size} \\\n                          --source {source_path} \\\n                          --conf 0.001 --iou 0.5 \\\n                          --name {model}-{fold} \\\n                          --save-txt --save-conf --exist-ok --nosave","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:31:15.834978Z","iopub.execute_input":"2021-08-09T10:31:15.835488Z","iopub.status.idle":"2021-08-09T10:35:54.420092Z","shell.execute_reply.started":"2021-08-09T10:31:15.835437Z","shell.execute_reply":"2021-08-09T10:35:54.418416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:54.425348Z","iopub.execute_input":"2021-08-09T10:35:54.425773Z","iopub.status.idle":"2021-08-09T10:35:54.952061Z","shell.execute_reply.started":"2021-08-09T10:35:54.425736Z","shell.execute_reply":"2021-08-09T10:35:54.950472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_image","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:54.957974Z","iopub.execute_input":"2021-08-09T10:35:54.958395Z","iopub.status.idle":"2021-08-09T10:35:55.037372Z","shell.execute_reply.started":"2021-08-09T10:35:54.958349Z","shell.execute_reply":"2021-08-09T10:35:55.036145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for img_size, model, model_path in MODEL_HYP:\n    \n    pred_path = [f'/kaggle/working/yolov5/runs/detect/{model}-{fold}/labels' for fold in range(5)]\n    \n    for fold in range(5):\n        image_ids = []\n        PredictionStrings = []\n\n        for file_path in tqdm(glob(f'{pred_path[fold]}/*.txt')):\n            image_id = file_path.split('/')[-1].split('.')[0] + '_image'\n            w, h = df_image.loc[df_image.id==image_id,['dim1', 'dim0']].values[0]\n            f = open(file_path, 'r')\n            data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n            data = data[:, [0, 5, 1, 2, 3, 4]]\n            bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n            for idx in range(len(bboxes)):\n                bboxes[idx] = str(int(float(bboxes[idx]))) if idx%6!=1 else bboxes[idx]\n            image_ids.append(image_id)\n            PredictionStrings.append(' '.join(bboxes))\n            f.close()\n\n\n        pred_df = pd.DataFrame({'id':image_ids, 'PredictionString':PredictionStrings})\n        yolo_df = pd.merge(df_image[['id']].copy(), pred_df, on='id', how='left').fillna('0 0.0001 0 0 1 1')\n\n        bbox_preds_df.append(yolo_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:55.040219Z","iopub.execute_input":"2021-08-09T10:35:55.04099Z","iopub.status.idle":"2021-08-09T10:35:56.694914Z","shell.execute_reply.started":"2021-08-09T10:35:55.040936Z","shell.execute_reply":"2021-08-09T10:35:56.693267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(bbox_preds_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:56.697041Z","iopub.execute_input":"2021-08-09T10:35:56.697625Z","iopub.status.idle":"2021-08-09T10:35:56.773296Z","shell.execute_reply.started":"2021-08-09T10:35:56.697553Z","shell.execute_reply":"2021-08-09T10:35:56.771976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weighted Boxes Fusion (opacity only)","metadata":{}},{"cell_type":"code","source":"def convert_label(label):\n    if label == 'none': convert = 1\n    elif label == 'opacity': convert = 0\n    elif label == 1: convert = 'none'\n    elif label == 0: convert = 'opacity'\n    return convert\n\ndef normalize_bbox(bbox, w, h):\n    x1, y1, x2, y2 = [float(num) for num in bbox]\n    return [x1/w, y1/h, x2/w, y2/h]\n\ndef resize_bbox(bbox, w, h):\n    x1, y1, x2, y2 = [float(num) for num in bbox]\n    return [x1*w, y1*h, x2*w, y2*h]\n\ndef gen_one_pred_string(bbox, score, label):\n    x1, y1, x2, y2 = bbox\n    return f'{label} {score} {x1} {y1} {x2} {y2}'\n\ndef gen_pred_string(boxes, scores, labels):\n    strings = [gen_one_pred_string(boxes[i], scores[i], labels[i]) for i in range(len(boxes))]\n    return ' '.join(strings)\n\ndef wbf_one_image(df, meta,\n                  image_idx,\n                  weights=None,\n                  iou_thr=0.5,\n                  skip_box_thr=0.001):\n\n    labels_list, scores_list, boxes_list = [], [], []\n    none_exist = 0\n    for fold in range(len(df)):\n        image_id = df[fold].iloc[image_idx, 0]\n        image_meta = meta[meta.id == image_id]\n        h, w = image_meta.iloc[0, 1:3].tolist()\n        pred_string = df[fold].iloc[image_idx, 1]\n        pred_splits = pred_string.split()\n        labels, scores, boxes = [], [], []\n        for j in range(int(len(pred_splits) / 6)):\n            label, score, *bbox = pred_splits[6*j:6*j+6]\n            if label == '1':\n                none_exist += 1\n            elif label == '0':\n                labels.append(int(label))\n                scores.append(float(score))\n                boxes.append(normalize_bbox(bbox, w, h))\n        labels_list.append(labels)\n        scores_list.append(scores)\n        boxes_list.append(boxes)\n\n    boxes, scores, labels = weighted_boxes_fusion(boxes_list, scores_list, labels_list, weights, iou_thr, skip_box_thr)\n    if len(boxes) == 0: # only none case\n        return None, None, None\n    resized_boxes = [resize_bbox(bbox, w, h) for bbox in boxes]\n    converted_labels = [convert_label(label) for label in labels]\n    scores = scores.tolist()\n    \n    resized_boxes.append([0, 0, 1, 1])\n    scores.append(df_2class[image_id])\n    converted_labels.append('none')\n        \n    return resized_boxes, scores, converted_labels","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:56.775517Z","iopub.execute_input":"2021-08-09T10:35:56.776734Z","iopub.status.idle":"2021-08-09T10:35:56.858307Z","shell.execute_reply.started":"2021-08-09T10:35:56.776681Z","shell.execute_reply":"2021-08-09T10:35:56.85698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_strings = []\nfor image_idx in range(df_image.shape[0]):\n    boxes, scores, labels = wbf_one_image(bbox_preds_df, df_image[['id', 'dim0', 'dim1']], image_idx)\n    if boxes is None:\n        pred_string = 'none 1 0 0 1 1'\n    else:\n        pred_string = gen_pred_string(boxes, scores, labels) \n    pred_strings.append(pred_string)\n        \ndf_image['PredictionString'] = pred_strings","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:56.860297Z","iopub.execute_input":"2021-08-09T10:35:56.860876Z","iopub.status.idle":"2021-08-09T10:35:58.175752Z","shell.execute_reply.started":"2021-08-09T10:35:56.860824Z","shell.execute_reply":"2021-08-09T10:35:58.174515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_df = df_image[['id', 'PredictionString']].copy()\nimage_df","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:58.177378Z","iopub.execute_input":"2021-08-09T10:35:58.177871Z","iopub.status.idle":"2021-08-09T10:35:58.256024Z","shell.execute_reply.started":"2021-08-09T10:35:58.177803Z","shell.execute_reply":"2021-08-09T10:35:58.254503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_df.to_csv(\"/kaggle/working/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:58.258038Z","iopub.execute_input":"2021-08-09T10:35:58.25856Z","iopub.status.idle":"2021-08-09T10:35:58.671143Z","shell.execute_reply.started":"2021-08-09T10:35:58.258489Z","shell.execute_reply":"2021-08-09T10:35:58.670055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx, pred in enumerate(image_df.PredictionString):\n    if pred[0] != 'o':\n        print(idx)\n        print(pred)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:58.672993Z","iopub.execute_input":"2021-08-09T10:35:58.673458Z","iopub.status.idle":"2021-08-09T10:35:58.749784Z","shell.execute_reply.started":"2021-08-09T10:35:58.673403Z","shell.execute_reply":"2021-08-09T10:35:58.748533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_result = pd.concat((df_result, image_df))\ndf_result.to_csv(\"/kaggle/working/test.csv\")\ndf_result","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:58.751483Z","iopub.execute_input":"2021-08-09T10:35:58.752177Z","iopub.status.idle":"2021-08-09T10:35:58.82984Z","shell.execute_reply.started":"2021-08-09T10:35:58.752125Z","shell.execute_reply":"2021-08-09T10:35:58.828457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/siim-covid19-detection/sample_submission.csv')\ndf.pop(\"PredictionString\")\nNone","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:58.831643Z","iopub.execute_input":"2021-08-09T10:35:58.832129Z","iopub.status.idle":"2021-08-09T10:35:58.913263Z","shell.execute_reply.started":"2021-08-09T10:35:58.832079Z","shell.execute_reply":"2021-08-09T10:35:58.911906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.merge(df_result, on=\"id\", how=\"left\").drop_duplicates(\"id\").reset_index(drop=True)\ndf.to_csv(\"/kaggle/working/submission.csv\", index=False)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:58.914973Z","iopub.execute_input":"2021-08-09T10:35:58.915703Z","iopub.status.idle":"2021-08-09T10:35:59.009151Z","shell.execute_reply.started":"2021-08-09T10:35:58.915652Z","shell.execute_reply":"2021-08-09T10:35:59.007688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.rmtree('/kaggle/working/yolov5')\nos.chdir('/kaggle/working')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:59.011063Z","iopub.execute_input":"2021-08-09T10:35:59.011584Z","iopub.status.idle":"2021-08-09T10:35:59.095333Z","shell.execute_reply.started":"2021-08-09T10:35:59.011509Z","shell.execute_reply":"2021-08-09T10:35:59.094225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_2class","metadata":{"execution":{"iopub.status.busy":"2021-08-09T10:35:59.097041Z","iopub.execute_input":"2021-08-09T10:35:59.097611Z","iopub.status.idle":"2021-08-09T10:35:59.166607Z","shell.execute_reply.started":"2021-08-09T10:35:59.097557Z","shell.execute_reply":"2021-08-09T10:35:59.165267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r /kaggle/working/mmdetection","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}