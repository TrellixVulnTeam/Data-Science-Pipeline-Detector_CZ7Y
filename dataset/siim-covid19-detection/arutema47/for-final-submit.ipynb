{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predictions of arutema47","metadata":{}},{"cell_type":"code","source":"!conda install '/kaggle/input/pydicom-conda-helper/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '/kaggle/input/pydicom-conda-helper/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-09T04:20:29.596128Z","iopub.execute_input":"2021-08-09T04:20:29.596413Z","iopub.status.idle":"2021-08-09T04:21:37.060112Z","shell.execute_reply.started":"2021-08-09T04:20:29.596344Z","shell.execute_reply":"2021-08-09T04:21:37.059163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\ndebug = True # if true, run all public data for action.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T04:21:37.063377Z","iopub.execute_input":"2021-08-09T04:21:37.063646Z","iopub.status.idle":"2021-08-09T04:21:37.074321Z","shell.execute_reply.started":"2021-08-09T04:21:37.063618Z","shell.execute_reply":"2021-08-09T04:21:37.073602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\nif (df.shape[0] == 2477) and (debug==False):\n    fast_sub = True\n    fast_df = pd.DataFrame(([['00086460a852_study', 'negative 1 0 0 1 1'], \n                         ['000c9c05fd14_study', 'negative 1 0 0 1 1'], \n                         ['65761e66de9f_image', 'none 1 0 0 1 1'], \n                         ['51759b5579bc_image', 'none 1 0 0 1 1']]), \n                       columns=['id', 'PredictionString'])\nelse:\n    fast_sub = False","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:37.077953Z","iopub.execute_input":"2021-08-09T04:21:37.078236Z","iopub.status.idle":"2021-08-09T04:21:37.107654Z","shell.execute_reply.started":"2021-08-09T04:21:37.07821Z","shell.execute_reply":"2021-08-09T04:21:37.106897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files = []\nsplit = \"test\"\n\nif not fast_sub:\n    for studyname, _, filenames in tqdm(os.walk(f'../input/siim-covid19-detection/{split}')):\n        for file in filenames:\n            files.append({\"img_id\":file.split(\".\")[0], \"study_id\":studyname.split(\"/\")[-2]})\n            #print(file, studyname.split(\"/\")[-2])\nelse:\n    files.append({\"img_id\": \"65761e66de9f\", \"study_id\": \"00086460a852\"})\n    files.append({\"img_id\": \"51759b5579bc\", \"study_id\": \"000c9c05fd14\"})\ndf_test = pd.DataFrame(files)\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:37.109191Z","iopub.execute_input":"2021-08-09T04:21:37.109536Z","iopub.status.idle":"2021-08-09T04:21:37.133704Z","shell.execute_reply.started":"2021-08-09T04:21:37.109498Z","shell.execute_reply":"2021-08-09T04:21:37.132952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:37.134942Z","iopub.execute_input":"2021-08-09T04:21:37.135321Z","iopub.status.idle":"2021-08-09T04:21:37.144195Z","shell.execute_reply.started":"2021-08-09T04:21:37.135287Z","shell.execute_reply":"2021-08-09T04:21:37.143167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# .dcm to .png","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:37.145709Z","iopub.execute_input":"2021-08-09T04:21:37.146349Z","iopub.status.idle":"2021-08-09T04:21:37.3963Z","shell.execute_reply.started":"2021-08-09T04:21:37.146313Z","shell.execute_reply":"2021-08-09T04:21:37.395474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https://www.kaggle.com/xhlulu/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:37.397594Z","iopub.execute_input":"2021-08-09T04:21:37.397949Z","iopub.status.idle":"2021-08-09T04:21:37.403286Z","shell.execute_reply.started":"2021-08-09T04:21:37.397911Z","shell.execute_reply":"2021-08-09T04:21:37.402363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make image imgs only to save time.\nsplit = 'test'\nsave_dir = f'/kaggle/tmp/{split}/'\n\nos.makedirs(save_dir, exist_ok=True)\n\nsave_dir = f'/kaggle/tmp/{split}/study/'\nos.makedirs(save_dir, exist_ok=True)\nimage_id = []\ndim0 = []\ndim1 = []\nsplits = []\nsave_dir = f'/kaggle/tmp/{split}/image/'\nos.makedirs(save_dir, exist_ok=True)\n\nif fast_sub:\n    xray = read_xray('../input/siim-covid19-detection/train/00086460a852/9e8302230c91/65761e66de9f.dcm')\n    im = resize(xray, size=512)  \n    im.save(os.path.join(save_dir,'65761e66de9f_image.png'))\n    image_id.append('65761e66de9f.dcm'.replace('.dcm', ''))\n    dim0.append(xray.shape[0])\n    dim1.append(xray.shape[1])\n    splits.append(split)\n    xray = read_xray('../input/siim-covid19-detection/train/000c9c05fd14/e555410bd2cd/51759b5579bc.dcm')\n    im = resize(xray, size=512)  \n    im.save(os.path.join(save_dir, '51759b5579bc_image.png'))\n    image_id.append('51759b5579bc.dcm'.replace('.dcm', ''))\n    dim0.append(xray.shape[0])\n    dim1.append(xray.shape[1])\n    splits.append(split)\nelse:\n    for dirname, _, filenames in tqdm(os.walk(f'../input/siim-covid19-detection/{split}')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            xray = read_xray(os.path.join(dirname, file))\n            im = resize(xray, size=512)  \n            im.save(os.path.join(save_dir, file.replace('.dcm', '_image.png')))\n            image_id.append(file.replace('.dcm', ''))\n            dim0.append(xray.shape[0])\n            dim1.append(xray.shape[1])\n            splits.append(split)\n            \nmeta = pd.DataFrame.from_dict({'image_id': image_id, 'dim0': dim0, 'dim1': dim1, 'split': splits})","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:37.406076Z","iopub.execute_input":"2021-08-09T04:21:37.40645Z","iopub.status.idle":"2021-08-09T04:21:38.048716Z","shell.execute_reply.started":"2021-08-09T04:21:37.406412Z","shell.execute_reply":"2021-08-09T04:21:38.047845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# meta image data for detections\nmeta","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:38.052296Z","iopub.execute_input":"2021-08-09T04:21:38.052593Z","iopub.status.idle":"2021-08-09T04:21:38.066148Z","shell.execute_reply.started":"2021-08-09T04:21:38.052545Z","shell.execute_reply":"2021-08-09T04:21:38.065312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/tmp/test/image","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:38.067324Z","iopub.execute_input":"2021-08-09T04:21:38.067672Z","iopub.status.idle":"2021-08-09T04:21:38.755082Z","shell.execute_reply.started":"2021-08-09T04:21:38.067644Z","shell.execute_reply":"2021-08-09T04:21:38.754174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# study predict","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\ndf = df_test\n\nstudy_len = len(df.study_id.unique())\n\nimsize = 384\nstudy_len","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:38.758158Z","iopub.execute_input":"2021-08-09T04:21:38.758416Z","iopub.status.idle":"2021-08-09T04:21:38.767745Z","shell.execute_reply.started":"2021-08-09T04:21:38.758388Z","shell.execute_reply":"2021-08-09T04:21:38.766754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:38.769283Z","iopub.execute_input":"2021-08-09T04:21:38.769967Z","iopub.status.idle":"2021-08-09T04:21:38.780949Z","shell.execute_reply.started":"2021-08-09T04:21:38.769925Z","shell.execute_reply":"2021-08-09T04:21:38.779884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_df = df\nstudy_df","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:38.782307Z","iopub.execute_input":"2021-08-09T04:21:38.782648Z","iopub.status.idle":"2021-08-09T04:21:38.793514Z","shell.execute_reply.started":"2021-08-09T04:21:38.782615Z","shell.execute_reply":"2021-08-09T04:21:38.792507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/d/kyoshioka47/siim-arutema/pytorch-image-models -q","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:38.794833Z","iopub.execute_input":"2021-08-09T04:21:38.795359Z","iopub.status.idle":"2021-08-09T04:22:10.012852Z","shell.execute_reply.started":"2021-08-09T04:21:38.795324Z","shell.execute_reply":"2021-08-09T04:22:10.011602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader, Dataset\n\ndef double_conv(in_channels, out_channels):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True)\n    )\n\n\ndef up_conv(in_channels, out_channels):\n    return nn.ConvTranspose2d(\n        in_channels, out_channels, kernel_size=2, stride=2\n    )\n\n\nclass unet(nn.Module):\n    def __init__(self, kernel_type=\"swin\", out_channels=1, pretrained = False):\n        super(unet, self).__init__()\n        self.kernel_type = kernel_type\n        if \"swinl\" in kernel_type:\n            self.back = timm.create_model(\"swin_large_patch4_window12_384\",pretrained=pretrained, return_dense=True, num_classes=0,)\n            ch = 1536\n            self.start = nn.Sequential(*list(self.back.children())[:-3])\n        elif \"swin\" in kernel_type:\n            self.back = timm.create_model(\"swin_base_patch4_window12_384\",pretrained=pretrained, return_dense=True, num_classes=0,)\n            ch = 1024\n            self.start = nn.Sequential(*list(self.back.children())[:-3])\n        elif \"effv2l\" in kernel_type:\n            self.back = timm.create_model(\"tf_efficientnetv2_l_in21ft1k\", pretrained=pretrained,  num_classes=0,)\n            ch = 1280\n            self.start = nn.Sequential(*list(self.back.children())[:-2])      \n        elif \"effv2s\" in kernel_type:\n            self.back = timm.create_model(\"tf_efficientnetv2_s_in21ft1k\", pretrained=pretrained,  num_classes=0,)\n            ch = 1280\n            self.start = nn.Sequential(*list(self.back.children())[:-2])        \n        \n        elif \"eff\" in kernel_type:\n            self.back = timm.create_model(\"tf_efficientnetv2_m_in21ft1k\", pretrained=pretrained,  num_classes=0,)\n            ch = 1280\n            self.start = nn.Sequential(*list(self.back.children())[:-2])\n        elif \"res50\" in kernel_type:\n            self.back = timm.create_model(\"resnet50d\", pretrained=pretrained,  num_classes=0,)\n            ch = 2048\n            self.start = nn.Sequential(*list(self.back.children())[:-2])\n        \n        self.ch = ch\n        self.up_conv1 = up_conv(ch, 512)\n        self.double_conv1 = double_conv(512, 256)\n        \n        self.do = nn.Dropout(0.5)\n        self.fc = nn.Linear(ch, 4)\n        \n        self.do2 = nn.Dropout(0.2)\n        self.fc2 = nn.Linear(ch, 3)\n\n        self.final_conv = nn.Conv2d(256, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        input_ = x\n        x = self.start(x)\n        # classifier head        \n        if \"swin\" in self.kernel_type:\n            tmp = x.reshape(-1, self.ch, 12, 12)            \n        else:\n            tmp = x\n        \n        # cls head\n        cls = F.adaptive_avg_pool2d(tmp, 1).squeeze(-1).squeeze(-1)\n        cls = self.do(cls)\n        cls = self.fc(cls)\n        \n        # cls head2\n        cls2 = F.adaptive_avg_pool2d(tmp, 1).squeeze(-1).squeeze(-1)\n        cls2 = self.do2(cls2)\n        cls2 = self.fc2(cls2)\n\n        # seg head\n        x = self.up_conv1(tmp)\n        x = self.double_conv1(x)\n        x = self.final_conv(x)\n            \n        return x, cls, cls2","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:22:10.014644Z","iopub.execute_input":"2021-08-09T04:22:10.014974Z","iopub.status.idle":"2021-08-09T04:22:12.014726Z","shell.execute_reply.started":"2021-08-09T04:22:10.014932Z","shell.execute_reply":"2021-08-09T04:22:12.013647Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = unet()\nmodel(torch.rand(1,3,384,384))[0].size()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:22:12.016264Z","iopub.execute_input":"2021-08-09T04:22:12.016659Z","iopub.status.idle":"2021-08-09T04:22:16.384007Z","shell.execute_reply.started":"2021-08-09T04:22:12.016609Z","shell.execute_reply":"2021-08-09T04:22:16.383216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class cassavaDataset(Dataset):\n    def __init__(self,\n                 df,\n                 rand=False,\n                 transform=None,\n                ):\n\n        self.df = df.reset_index(drop=True)\n        self.rand = rand\n        self.transform = transform\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        img_id = row.img_id\n        \n        tiff_file = os.path.join(image_folder, f'{img_id}_image.png')\n        images = cv2.imread(tiff_file)   \n        \n        # aug\n        if self.transform is not None:\n            out = self.transform(image=images,)\n            images = out[\"image\"]\n              \n        images = images.astype(np.float32)\n        images /= 255\n        images = images.transpose(2, 0, 1)\n        \n        return torch.tensor(images), row.study_id, row.img_id\n    \nimport albumentations\nimport albumentations as A\n\ntransforms_val = albumentations.Compose([albumentations.Resize(imsize, imsize, p=1.0)])","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:22:16.385206Z","iopub.execute_input":"2021-08-09T04:22:16.385524Z","iopub.status.idle":"2021-08-09T04:22:18.110978Z","shell.execute_reply.started":"2021-08-09T04:22:16.385497Z","shell.execute_reply":"2021-08-09T04:22:18.110088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score\nfrom torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\nimport cv2\n\ndef val_epoch(loader, get_output=True):\n    model.eval()\n    val_loss = []\n    LOGITS = []\n    PREDS = []\n    TARGETS = []\n    study = []\n    img = []\n\n    with torch.no_grad():\n        for (data, s, i) in tqdm(loader):\n            data = data.to(device)\n            _, logits, _ = model(data)\n            _, logits2, _ = model(data.flip(3))\n            logits = logits.squeeze(1)\n            logits2 = logits2.squeeze(1)\n            logits = (logits+logits2)/2\n            study.extend(s)\n            img.extend(i)\n            \n            pred = F.softmax(logits).detach()\n            LOGITS.append(logits)\n            PREDS.append(pred)\n\n    LOGITS = torch.cat(LOGITS).cpu().numpy()   \n    PREDS = torch.cat(PREDS).cpu().numpy()\n    PREDS_ARG = PREDS.argmax(1)\n    \n    if get_output:\n        return LOGITS, PREDS, study, img\n    else:\n        return val_loss, acc, np.mean(map)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:22:18.112355Z","iopub.execute_input":"2021-08-09T04:22:18.112724Z","iopub.status.idle":"2021-08-09T04:22:18.249284Z","shell.execute_reply.started":"2021-08-09T04:22:18.112689Z","shell.execute_reply":"2021-08-09T04:22:18.248479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########### infer swin\nimsize = 384\nimage_folder = \"/kaggle/tmp/test/image/\"\ndevice =\"cuda\"\n\ntransforms_val = albumentations.Compose([albumentations.Resize(imsize, imsize, p=1.0)])\ndataset_valid = cassavaDataset(study_df, transform=transforms_val)\n# Setup dataloader\nvalid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=16, sampler=SequentialSampler(dataset_valid), num_workers=2)\n\n# Initialize model\npreds = []\n\nfor fold in range(4):\n    model = unet(\"swin\")\n    model = model.to(device)\n    model.load_state_dict(torch.load(\"../input/siimfinal/unet-swin-4class-clean_fold{}_imsize384_dec0_scaler0.5_smooth_best_r7_fold{}.pth\".format(fold, fold)))\n\n    LOGITS, PREDS, study, image  = val_epoch(valid_loader)\n    preds.append(PREDS)\n    \nfor fold in range(5):\n    kernel_type = \"swin\"\n    model = unet(\"swin\")\n    model = model.to(device)\n    model.load_state_dict(torch.load(\"../input/siimfinal/unet-swin-4class-r8_fold{}_imsize384_dec0_scaler0.5_smooth_best_r7_fold{}.pth\".format(fold, fold)))\n\n    LOGITS, PREDS, study, image  = val_epoch(valid_loader)\n    preds.append(PREDS)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:22:18.250872Z","iopub.execute_input":"2021-08-09T04:22:18.251237Z","iopub.status.idle":"2021-08-09T04:23:14.921246Z","shell.execute_reply.started":"2021-08-09T04:22:18.251199Z","shell.execute_reply":"2021-08-09T04:23:14.920131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kernel_type = \"eff\"\ntransforms_val = albumentations.Compose([albumentations.Resize(512, 512, p=1.0)])\ndataset_valid = cassavaDataset(study_df, transform=transforms_val)\n# Setup dataloader\nvalid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=8, sampler=SequentialSampler(dataset_valid), num_workers=2)\n\n# memory clean\nimport gc\ngc.collect()\n\nfor fold in range(4):\n    model = unet(\"eff\")\n    model = model.to(device)\n    model.load_state_dict(torch.load(\"../input/siimfinal/unet-eff-4class-clean_fold{}_imsize512_dec0_scaler0.5_smooth_best_r7_fold{}.pth\".format(fold, fold)))\n\n    LOGITS, PREDS, study, image  = val_epoch(valid_loader)\n    preds.append(PREDS)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:23:14.923308Z","iopub.execute_input":"2021-08-09T04:23:14.923731Z","iopub.status.idle":"2021-08-09T04:23:36.407131Z","shell.execute_reply.started":"2021-08-09T04:23:14.923663Z","shell.execute_reply":"2021-08-09T04:23:36.406046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# memory clean\nimport gc\ngc.collect()\n\nkernel_type = \"eff\"\ntransforms_val = albumentations.Compose([albumentations.Resize(512, 512, p=1.0)])\ndataset_valid = cassavaDataset(study_df, transform=transforms_val)\n# Setup dataloader\nvalid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=8, sampler=SequentialSampler(dataset_valid), num_workers=2)\n    \nfor fold in range(5):\n    model = unet(\"eff\")\n    model = model.to(device)\n    model.load_state_dict(torch.load(\"../input/siimfinal/unet-effv2m-4class-r8_fold{}_imsize512_dec0_scaler0.5_smooth_best_r7_fold{}.pth\".format(fold, fold)))\n\n    LOGITS, PREDS, study, image  = val_epoch(valid_loader)\n    preds.append(PREDS)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:23:36.409195Z","iopub.execute_input":"2021-08-09T04:23:36.409672Z","iopub.status.idle":"2021-08-09T04:24:02.538237Z","shell.execute_reply.started":"2021-08-09T04:23:36.409614Z","shell.execute_reply":"2021-08-09T04:24:02.536769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model, valid_loader\n\n# memory clean\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:02.540663Z","iopub.execute_input":"2021-08-09T04:24:02.541285Z","iopub.status.idle":"2021-08-09T04:24:02.762002Z","shell.execute_reply.started":"2021-08-09T04:24:02.541229Z","shell.execute_reply":"2021-08-09T04:24:02.760742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Process study inference","metadata":{}},{"cell_type":"code","source":"PREDS = np.mean(np.array(preds),axis=0)\nPREDS[:5]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:02.767767Z","iopub.execute_input":"2021-08-09T04:24:02.768387Z","iopub.status.idle":"2021-08-09T04:24:02.776041Z","shell.execute_reply.started":"2021-08-09T04:24:02.768316Z","shell.execute_reply":"2021-08-09T04:24:02.775021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_PredictionString(row, thr=0):\n    string = ''\n    for idx in range(4):\n        conf =  row[str(idx)]\n        if conf>thr:\n            string+=f'{label2name[idx]} {conf:0.2f} 0 0 1 1 '\n    string = string.strip()\n    return string\n\nname2label = { \n    'negative': 0,\n    'indeterminate': 2,\n    'atypical': 3,\n    'typical': 1}\nlabel2name  = {v:k for k, v in name2label.items()}","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:02.77843Z","iopub.execute_input":"2021-08-09T04:24:02.778913Z","iopub.status.idle":"2021-08-09T04:24:02.786937Z","shell.execute_reply.started":"2021-08-09T04:24:02.778872Z","shell.execute_reply":"2021-08-09T04:24:02.785719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outs = []\nfor i in range(len(study)):\n    out = {\"id\": study[i]+\"_study\", \"img_id\": image[i]+\"_image\", \"0\": PREDS[i][3], \"1\": PREDS[i][0], \"2\": PREDS[i][1], \"3\": PREDS[i][2]}\n    outs.append(out)\nouts[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:02.788645Z","iopub.execute_input":"2021-08-09T04:24:02.789069Z","iopub.status.idle":"2021-08-09T04:24:02.800327Z","shell.execute_reply.started":"2021-08-09T04:24:02.789029Z","shell.execute_reply":"2021-08-09T04:24:02.79922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(outs) # store predictions as df\ndf_study = df # save for image predictions.\n\ndf = df[~df.id.duplicated()].reset_index(drop=True) # simply drop multi-images..(やりようはある気がするが時間ねえ)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:02.802251Z","iopub.execute_input":"2021-08-09T04:24:02.802807Z","iopub.status.idle":"2021-08-09T04:24:02.82445Z","shell.execute_reply.started":"2021-08-09T04:24:02.802766Z","shell.execute_reply":"2021-08-09T04:24:02.823158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make submission string\ndf['PredictionString'] = df.apply(get_PredictionString, axis=1)\ndf = df.drop([\"0\",\"1\",\"2\",\"3\"], axis=1)\nstudy_sub = df\ndf","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:02.826248Z","iopub.execute_input":"2021-08-09T04:24:02.82675Z","iopub.status.idle":"2021-08-09T04:24:02.847826Z","shell.execute_reply.started":"2021-08-09T04:24:02.826709Z","shell.execute_reply":"2021-08-09T04:24:02.846529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del preds, outs\n# memory clean\nimport gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:02.849438Z","iopub.execute_input":"2021-08-09T04:24:02.849901Z","iopub.status.idle":"2021-08-09T04:24:02.969277Z","shell.execute_reply.started":"2021-08-09T04:24:02.849859Z","shell.execute_reply":"2021-08-09T04:24:02.968365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image Inference (yolov5)","metadata":{}},{"cell_type":"code","source":"!pip install ../input/odachkaggle/ODA-Object-Detection-ttA-main -q","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:02.970779Z","iopub.execute_input":"2021-08-09T04:24:02.971402Z","iopub.status.idle":"2021-08-09T04:24:31.682965Z","shell.execute_reply.started":"2021-08-09T04:24:02.97136Z","shell.execute_reply":"2021-08-09T04:24:31.681882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"################## boxes\nfrom odach.wbf import *\nimport os\nimport cv2\n\ndef run_wbf(predictions, iou_thr=0.4, skip_box_thr=0.2, weights=None):\n    boxes = [(prediction['boxes']).tolist()  for prediction in predictions]\n    scores = [prediction['scores'].tolist()  for prediction in predictions]    \n    labels = [prediction['labels'].tolist() for prediction in predictions]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    return boxes, scores, labels\n          \ndef process_box(boxes, iou, skip):\n    preds1 = {\"boxes\": boxes[:,2:],\n                  \"labels\": boxes[:,0],\n                  \"scores\": boxes[:,1]}\n    preds = [preds1,]\n\n    boxes, scores, labels = run_wbf(preds, iou_thr=iou, skip_box_thr=skip)\n    return np.stack([labels.reshape(-1), scores.reshape(-1), boxes[:,0],boxes[:,1],boxes[:,2],boxes[:,3]]).T\n          \ndef yolo2voc(bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n    \n    return bboxes\n          \ndef get_boxes(filepath,train=False,iou=0.5,skip=0.):\n    if os.path.exists(filepath):\n        w = 1\n        h = 1\n        # read results\n        f = open(filepath, 'r')\n        data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n        data = data[:, [0, 5, 1, 2, 3, 4]]\n\n        # to voc\n        bboxes = np.concatenate((data[:, :2], yolo2voc(data[:, 2:])), axis =1) \n        # nms\n        bboxes = process_box(bboxes,iou,skip) \n\n        # resize boxes to original size\n        bboxes[:,2] = w*bboxes[:,2]\n        bboxes[:,4] = w*bboxes[:,4]\n        bboxes[:,3] = h*bboxes[:,3]\n        bboxes[:,5] = h*bboxes[:,5]\n        return bboxes","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:31.686692Z","iopub.execute_input":"2021-08-09T04:24:31.686965Z","iopub.status.idle":"2021-08-09T04:24:32.300848Z","shell.execute_reply.started":"2021-08-09T04:24:31.686936Z","shell.execute_reply":"2021-08-09T04:24:32.300016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get negative prediction from study models\nimport glob\nfiles = sorted(glob.glob(\"/kaggle/tmp/test/image/*\"))\n\n# inference by study model\ninferences = [f.split(\"/\")[-1].split(\".\")[0] for f in files]\nPREDS = [df_study[df_study.img_id==inferences[i]].reset_index()[\"0\"][0] for i,_ in enumerate(inferences)]\nPREDS[:5]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:32.302027Z","iopub.execute_input":"2021-08-09T04:24:32.302358Z","iopub.status.idle":"2021-08-09T04:24:32.317795Z","shell.execute_reply.started":"2021-08-09T04:24:32.302322Z","shell.execute_reply":"2021-08-09T04:24:32.31678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:32.319222Z","iopub.execute_input":"2021-08-09T04:24:32.319668Z","iopub.status.idle":"2021-08-09T04:24:32.331991Z","shell.execute_reply.started":"2021-08-09T04:24:32.31963Z","shell.execute_reply":"2021-08-09T04:24:32.331105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get meta info\nmeta = meta[meta['split'] == 'test']\n\nif fast_sub:\n    test_df = fast_df.copy()\nelse:\n    test_df = pd.read_csv('../input/siim-covid19-detection/sample_submission.csv')\n\ntest_df = test_df[study_len:].reset_index(drop=True) \nprint(test_df)\nmeta.columns = ['id', 'dim0', 'dim1', 'split']\ntest_df = pd.merge(test_df, meta, on = 'id', how = 'left')\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:32.333116Z","iopub.execute_input":"2021-08-09T04:24:32.333725Z","iopub.status.idle":"2021-08-09T04:24:32.364523Z","shell.execute_reply.started":"2021-08-09T04:24:32.333673Z","shell.execute_reply":"2021-08-09T04:24:32.36378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run yolov5 inference\ndim = 512 #1024, 256, 'original'\ntest_dir = f'/kaggle/tmp/{split}/image'\nimport shutil\ntry:\n    shutil.copytree('/kaggle/input/yolov5-official-v31-dataset/yolov5', '/kaggle/working/yolov5')\nexcept:\n    print(\"oops\")\n    \nos.chdir('/kaggle/working/yolov5') # install dependencies\n\nimport torch\n\nfor fold in range(5):\n    for model in [\"l6\", \"x6\"]:\n        weights_dir = '/kaggle/input/siimfinal/{}_{}.pt'.format(model, fold)\n        name = \"{}_{}\".format(model, fold)\n        !python detect.py --weights $weights_dir --name $name\\\n        --img 512\\\n        --conf 0.001\\\n        --iou 0.6\\\n        --source $test_dir\\\n        --save-txt --save-conf --exist-ok","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:24:32.365575Z","iopub.execute_input":"2021-08-09T04:24:32.365892Z","iopub.status.idle":"2021-08-09T04:25:49.9227Z","shell.execute_reply.started":"2021-08-09T04:24:32.365868Z","shell.execute_reply":"2021-08-09T04:25:49.921687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls runs/detect/l6_0/labels","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:25:49.924183Z","iopub.execute_input":"2021-08-09T04:25:49.924529Z","iopub.status.idle":"2021-08-09T04:25:50.629592Z","shell.execute_reply.started":"2021-08-09T04:25:49.924486Z","shell.execute_reply":"2021-08-09T04:25:50.628692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:25:50.631084Z","iopub.execute_input":"2021-08-09T04:25:50.631425Z","iopub.status.idle":"2021-08-09T04:25:50.645114Z","shell.execute_reply.started":"2021-08-09T04:25:50.631395Z","shell.execute_reply":"2021-08-09T04:25:50.643837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boxes to predictions","metadata":{}},{"cell_type":"code","source":"import glob\n\npreds = []\n\nfor i,file in tqdm(enumerate(files)):\n    filepath = file\n    id = filepath.split(\"/\")[-1].split(\".\")[0]\n    print(id)\n    bbox = []\n    for fold in range(5):\n        for model in [\"l6\",\"x6\"]:\n            path = 'runs/detect/{}_{}/labels/{}.txt'.format(model, fold, id)\n            if os.path.exists(path):\n                #try:\n                    boxes = get_boxes(path,iou=1)\n                    bbox.append({\"boxes\": boxes[:,2:],\n                      \"labels\": boxes[:,0],\n                      \"scores\": boxes[:,1]})\n                #except:\n                #    print(\"not found {}\".format(path))\n    # ensemble boxes\n    boxes, bscores, labels = run_wbf(bbox, iou_thr=0.6, skip_box_thr=0.)\n    box = np.stack([labels.reshape(-1), bscores.reshape(-1), boxes[:,0],boxes[:,1],boxes[:,2],boxes[:,3]]).T\n\n    w = int(meta[meta.id+\"_image\"==id].dim0)\n    h = int(meta[meta.id+\"_image\"==id].dim1)\n\n    box[:,2] *= h\n    box[:,4] *= h\n    box[:,3] *= w\n    box[:,5] *= w\n    string = \"\"\n    new = []\n    for b in box:\n        string += \"opacity {} {} {} {} {} \".format(b[1], b[2], b[3], b[4], b[5])\n    \n    # append none\n    string += \"none {} {} {} {} {} \".format(PREDS[i], 0,0,1,1)\n    preds.append({\"id\": id, \"PredictionString\": string})","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:25:50.647102Z","iopub.execute_input":"2021-08-09T04:25:50.647394Z","iopub.status.idle":"2021-08-09T04:25:51.59108Z","shell.execute_reply.started":"2021-08-09T04:25:50.64737Z","shell.execute_reply":"2021-08-09T04:25:51.590096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds[0]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:25:51.592634Z","iopub.execute_input":"2021-08-09T04:25:51.592985Z","iopub.status.idle":"2021-08-09T04:25:51.599789Z","shell.execute_reply.started":"2021-08-09T04:25:51.592948Z","shell.execute_reply":"2021-08-09T04:25:51.598753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2 = pd.DataFrame(preds) # get image detections","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:25:51.601246Z","iopub.execute_input":"2021-08-09T04:25:51.601618Z","iopub.status.idle":"2021-08-09T04:25:51.610475Z","shell.execute_reply.started":"2021-08-09T04:25:51.601582Z","shell.execute_reply":"2021-08-09T04:25:51.609672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare final submission","metadata":{}},{"cell_type":"code","source":"sub = pd.concat([study_sub, df2]).reset_index(drop=True).drop([\"img_id\"], axis=1)\nsub","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:25:51.611777Z","iopub.execute_input":"2021-08-09T04:25:51.612108Z","iopub.status.idle":"2021-08-09T04:25:51.628965Z","shell.execute_reply.started":"2021-08-09T04:25:51.612072Z","shell.execute_reply":"2021-08-09T04:25:51.628157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv('/kaggle/working/submission.csv',index = False)  \nshutil.rmtree('/kaggle/working/yolov5')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:25:51.630122Z","iopub.execute_input":"2021-08-09T04:25:51.630442Z","iopub.status.idle":"2021-08-09T04:25:52.581988Z","shell.execute_reply.started":"2021-08-09T04:25:51.63041Z","shell.execute_reply":"2021-08-09T04:25:52.581175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}