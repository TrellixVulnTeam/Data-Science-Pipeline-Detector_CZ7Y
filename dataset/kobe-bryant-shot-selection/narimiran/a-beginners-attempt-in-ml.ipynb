{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Intro\n\nI'm just a beginner who started using `pandas` one week ago and now trying to do some basic data analysis for the first time.  \nI would very much like to hear your feedback, comments and improvements to my code.\n\n---\n\nThis notebook continues from [my last one](https://www.kaggle.com/narimiran/kobe-bryant-shot-selection/beginners-first-time) stopped, so I'll just quickly repeat all modifications I've already done, and then continue with the new stuff."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set_style('whitegrid')\n\ndf = pd.read_csv('../input/data.csv')\n\nnot_needed = []\n\nnot_needed.extend(['game_event_id', 'game_id'])\n\nnot_needed.extend(['lon', 'lat'])\n\ndf['time_remaining'] = 60 * df.loc[:, 'minutes_remaining'] + df.loc[:, 'seconds_remaining']\nnot_needed.extend(['minutes_remaining', 'seconds_remaining'])\n\ndf['season'] = df['season'].apply(lambda x: x[:4])\ndf['season'] = pd.to_numeric(df['season'])\n\ndist = pd.DataFrame({'true_dist': np.sqrt((df['loc_x']/10)**2 + (df['loc_y']/10)**2), \n                     'shot_dist': df['shot_distance']})\ndf['shot_distance_'] = dist['true_dist']\nnot_needed.append('shot_distance')\n\ndf['3pt_goal'] = df['shot_type'].str.contains('3PT').astype('int')\nnot_needed.append('shot_type')\n\nnot_needed.append('shot_zone_range')\n\nnot_needed.extend(['team_id', 'team_name'])\n\ndf['game_date'] = pd.to_datetime(df['game_date'])\ndf['game_year'] = df['game_date'].dt.year\ndf['game_month'] = df['game_date'].dt.month\ndf['game_day'] = df['game_date'].dt.dayofweek\nnot_needed.append('game_date')\n\ndf['home_game'] = df['matchup'].str.contains('vs.').astype(int)\nnot_needed.append('matchup')\n\ndf.set_index('shot_id', inplace=True)\n\ndf = df.drop(not_needed, axis=1)\n\nrandom_sample = df.take(np.random.permutation(len(df))[:10])\nrandom_sample.head(10)"},{"cell_type":"markdown","metadata":{},"source":"# New stuff\n\nAfter we've explored the data [last time](https://www.kaggle.com/narimiran/kobe-bryant-shot-selection/beginners-first-time), it's time to analyze some more."},{"cell_type":"markdown","metadata":{},"source":"## Action types"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"df['action_type'].value_counts()"},{"cell_type":"markdown","metadata":{},"source":"There are too many (57) different action types, and many of them have only few shots, so we'll keep first 25 action types (with most of shot attempts), and all other action types will be under `other` category."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"rare_action_types = df['action_type'].value_counts()[25:]\nrare_actions = rare_action_types.index.values\n\ndf.loc[df['action_type'].isin(rare_actions), 'action_type'] = 'other'\ndf['action_type'].value_counts()"},{"cell_type":"markdown","metadata":{},"source":"## Periods and overtime"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"df['period'].value_counts()"},{"cell_type":"markdown","metadata":{},"source":"Under 400 shot attempts (with similar accuracy) were made in overtime periods (periods 5, 6, 7), so we'll combine them in one category: `overtime`."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"overtime = np.array([5, 6, 7])\ndf.loc[df['period'].isin(overtime), 'period'] = 'overtime'\ndf['period'].value_counts()"},{"cell_type":"markdown","metadata":{},"source":"## Playoffs\n\nAs we've seen earlier there's no difference in accuracy between regular season and playoffs, so column `playoffs` won't be needed."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"df = df.drop('playoffs', axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Creating dummies for categorical features\n\nWe can't use categorical features so we'll convert them to dummies."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"df.dtypes"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"categorical = ['action_type', 'combined_shot_type', 'shot_zone_area', 'shot_zone_basic', \n               'opponent', 'period', 'season', 'game_year', 'game_month', 'game_day']\n\nfor column in categorical:\n    dummy = pd.get_dummies(df[column], prefix=column)\n    df = df.join(dummy)\n    df.drop(column, axis=1, inplace=True)\n\ndf.head()"},{"cell_type":"markdown","metadata":{},"source":"# Separating the data\n\nSplitting the data in two parts - one for our learning and other for submission."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"unknown_shots = df['shot_made_flag'].isnull()\n\nsubmission_data = df[unknown_shots].drop('shot_made_flag', 1)\ndata = df[~unknown_shots]\n\nX = data.drop('shot_made_flag', 1)\ny = data['shot_made_flag']"},{"cell_type":"markdown","metadata":{},"source":"# Feature selection\n\nWe have 146 features, but would like to reduce that number to only most important features.\n\n---\n\n***Big THANK YOU goes to [Norbert Kozlowski](https://www.kaggle.com/khozzy) and [his script](https://www.kaggle.com/khozzy/kobe-bryant-shot-selection/kobe-shots-show-me-your-best-model/) which helped me a lot to make all of the code from now till the end of the notebook.***\n\n---"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.feature_selection import VarianceThreshold, RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression"},{"cell_type":"markdown","metadata":{},"source":"## Variance Threshold"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"threshold = 0.9\nvt = VarianceThreshold().fit(X)\n\nfeat_var_threshold = X.columns[vt.variances_ > threshold * (1-threshold)].values\nfeat_var_threshold"},{"cell_type":"markdown","metadata":{},"source":"## Random Forest Classifier"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"model = RandomForestClassifier()\nmodel.fit(X, y)\n\nfeature_imp = pd.DataFrame(model.feature_importances_, index=X.columns, columns=[\"importance\"])\nfeat_RFC = feature_imp.sort_values(\"importance\", ascending=False).head(35)\n\nfeat_RFC = feat_RFC.index.values\nfeat_RFC"},{"cell_type":"markdown","metadata":{},"source":"## Recursive feature elimination (RFE)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"rfe = RFE(LogisticRegression(), 35)\nrfe.fit(X, y)\n\nfeature_rfe_scoring = pd.DataFrame({'feature': X.columns, 'score': rfe.ranking_})\n\nfeat_rfe = feature_rfe_scoring[feature_rfe_scoring['score'] == 1]['feature'].values\nfeat_rfe"},{"cell_type":"markdown","metadata":{},"source":"## Putting it all together"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"features = np.hstack([feat_var_threshold, feat_RFC, feat_rfe])\n\nfeatures = np.unique(features)\nprint('Final features set:\\n')\nfor f in features:\n    print(\"-{}\".format(f))\n    \nlen(features)"},{"cell_type":"markdown","metadata":{},"source":"We'll make new datasets with only those columns."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"submission_data = submission_data.ix[:, features]\ndata = data.ix[:, features]\nX = X.ix[:, features]"},{"cell_type":"markdown","metadata":{},"source":"# Testing different algorithms"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.cross_validation import KFold, cross_val_score\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"seed = 2016\nnum_folds = 5\nnum_instances = len(X)\njobs = -1\n\nscoring = 'log_loss'\n\nkfold = KFold(n=num_instances, n_folds=num_folds, random_state=seed)"},{"cell_type":"markdown","metadata":{},"source":"## Logistic regression"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"model = LogisticRegression()\n\nresult = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\nprint(\"({0:.4f}) +/- ({1:.4f})\".format(result.mean(), result.std()))"},{"cell_type":"markdown","metadata":{},"source":"## K-nearest neighbors"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"model = KNeighborsClassifier(n_neighbors=20, n_jobs=jobs)\n\nresult = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\nprint(\"({0:.4f}) +/- ({1:.4f})\".format(result.mean(), result.std()))"},{"cell_type":"markdown","metadata":{},"source":"## Random forest"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"model = RandomForestClassifier(n_estimators=200, n_jobs=jobs)\n\nresult = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\nprint(\"({0:.4f}) +/- ({1:.4f})\".format(result.mean(), result.std()))"},{"cell_type":"markdown","metadata":{},"source":"## Ada boost"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"model = AdaBoostClassifier(random_state=seed)\n\nresults = cross_val_score(model, X, y, cv=kfold, scoring=scoring, n_jobs=jobs)\nprint(\"({0:.4f}) +/- ({1:.4f})\".format(results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{},"source":"## Gradient Boosting"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"model = GradientBoostingClassifier(random_state=seed)\n\nresults = cross_val_score(model, X, y, cv=kfold, scoring=scoring, n_jobs=jobs)\nprint(\"({0:.4f}) +/- ({1:.4f})\".format(results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{},"source":"## Linear Discriminant Analysis (LDA)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"model = LinearDiscriminantAnalysis(solver='lsqr')\n\nresults = cross_val_score(model, X, y, cv=kfold, scoring=scoring, n_jobs=jobs)\nprint(\"({0:.4f}) +/- ({1:.4f})\".format(results.mean(), results.std()))"},{"cell_type":"markdown","metadata":{},"source":"# Grid search\n\nWe'll use `GridSearchCV` to find the best parameters for each algorithm that we used above."},{"cell_type":"markdown","metadata":{},"source":"## Logistic regression"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"lr_grid = GridSearchCV(estimator = LogisticRegression(random_state=seed),\n                       param_grid = {'penalty': ['l1', 'l2'], \n                                     'C': [0.01, 0.1, 1, 10]}, \n                       cv = kfold, \n                       scoring = scoring)\n\nlr_grid.fit(X, y)\n\nprint(lr_grid.best_score_)\nprint(lr_grid.best_params_)"},{"cell_type":"markdown","metadata":{},"source":"## K-nearest neighbors"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"knn_grid = GridSearchCV(estimator = KNeighborsClassifier(n_jobs=jobs),\n                        param_grid = {'n_neighbors': [50, 80],\n                                      'weights': ['uniform'],\n                                      'algorithm': ['ball_tree'],\n                                      'leaf_size': [2, 10], \n                                      'p': [1]}, \n                        cv = kfold, \n                        scoring = scoring)\n\nknn_grid.fit(X, y)\n\nprint(knn_grid.best_score_)\nprint(knn_grid.best_params_)"},{"cell_type":"markdown","metadata":{},"source":"## Random forest"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"rf_grid = GridSearchCV(estimator = RandomForestClassifier(warm_start=True, random_state=seed, n_jobs=jobs), \n                       param_grid = {'n_estimators': [100, 200],\n                                     'criterion': ['entropy'], \n                                     'max_features': ['auto', 20], \n                                     'max_depth': [None, 10]}, \n                       cv = kfold, \n                       scoring = scoring)\n\nrf_grid.fit(X, y)\n\nprint(rf_grid.best_score_)\nprint(rf_grid.best_params_)"},{"cell_type":"markdown","metadata":{},"source":"## Ada boost"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"ada_grid = GridSearchCV(estimator = AdaBoostClassifier(random_state=seed), \n                        param_grid = {'n_estimators': [10, 25, 50, 100, 150],\n                                      'learning_rate': [1e-3, 1e-2, 1e-1, 1]},\n                        cv = kfold, \n                        scoring = scoring, \n                        n_jobs = jobs)\n\nada_grid.fit(X, y)\n\nprint(ada_grid.best_score_)\nprint(ada_grid.best_params_)"},{"cell_type":"markdown","metadata":{},"source":"## Gradient Boosting"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"gbm_grid = GridSearchCV(estimator = GradientBoostingClassifier(warm_start=True, random_state=seed),\n                        param_grid = {'n_estimators': [100, 200],\n                                      'max_depth': [5],\n                                      'max_features': ['auto', 'log2'],\n                                      'learning_rate': [0.1]}, \n                        cv = kfold, \n                        scoring = scoring, \n                        n_jobs = jobs)\n\ngbm_grid.fit(X, y)\n\nprint(gbm_grid.best_score_)\nprint(gbm_grid.best_params_)"},{"cell_type":"markdown","metadata":{},"source":"## LDA"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"lda_grid = GridSearchCV(estimator = LinearDiscriminantAnalysis(),\n                        param_grid = {'solver': ['lsqr'], \n                                      'shrinkage': [None, 'auto'],\n                                      'n_components': [None, 2, 5, 10]},\n                        cv = kfold, \n                        scoring = scoring,\n                        n_jobs = jobs)\n\nlda_grid.fit(X, y)\n\nprint(lda_grid.best_score_)\nprint(lda_grid.best_params_)"},{"cell_type":"markdown","metadata":{},"source":"## Grid search summary"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print('lr', lr_grid.best_score_)\nprint(lr_grid.best_params_)\nprint()\nprint('knn', knn_grid.best_score_)\nprint(knn_grid.best_params_)\nprint()\nprint('rf', rf_grid.best_score_)\nprint(rf_grid.best_params_)\nprint()\nprint('ada', ada_grid.best_score_)\nprint(ada_grid.best_params_)\nprint()\nprint('gbm', gbm_grid.best_score_)\nprint(gbm_grid.best_params_)\nprint()\nprint('lda', lda_grid.best_score_)\nprint(lda_grid.best_params_)"},{"cell_type":"markdown","metadata":{},"source":"# Voting classifier\n\nAfter lots of trial-and-error, I decided not to use KNN and ADA (two algorithms with the worst score).\n\nWeights were chosen based on same principle - trying a lot of values."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"estimators = [('lr', LogisticRegression(C=1, penalty='l2', random_state=seed)), \n              ('rf', RandomForestClassifier(warm_start=True, max_features=20, n_estimators=200, \n                                            max_depth=5, criterion='entropy', random_state=seed)),\n              ('gbm', GradientBoostingClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, max_features='log2')),\n              ('lda', LinearDiscriminantAnalysis(solver='lsqr', shrinkage=None))]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"voters = VotingClassifier(estimators, voting='soft', weights=[1, 2, 2, 1])\n\nresults = cross_val_score(voters, X, y, cv=kfold, scoring=scoring, n_jobs=jobs)\nprint(\"({0:.4}) +/- ({1:.4f})\".format(results.mean(), results.std()))"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}