{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> Kobe Bryant Shot Selection Analysis </center>\n\n### <center> Exploratory Analysis and Ensemble Prediction of Shots made and missed</center>","metadata":{}},{"cell_type":"markdown","source":"![cover](https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/1a5b131c-c4f2-4f4b-8587-945e38919401/d2omfj6-684f32d6-3706-4426-b693-a407dbfc93b3.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJ1cm46YXBwOjdlMGQxODg5ODIyNjQzNzNhNWYwZDQxNWVhMGQyNmUwIiwic3ViIjoidXJuOmFwcDo3ZTBkMTg4OTgyMjY0MzczYTVmMGQ0MTVlYTBkMjZlMCIsImF1ZCI6WyJ1cm46c2VydmljZTpmaWxlLmRvd25sb2FkIl0sIm9iaiI6W1t7InBhdGgiOiIvZi8xYTViMTMxYy1jNGYyLTRmNGItODU4Ny05NDVlMzg5MTk0MDEvZDJvbWZqNi02ODRmMzJkNi0zNzA2LTQ0MjYtYjY5My1hNDA3ZGJmYzkzYjMuanBnIn1dXX0.ngz3uDrOtN-k3aoSIGlCYWj3i_DDleRiIIdZ8JcO8F0)","metadata":{}},{"cell_type":"markdown","source":"### Introduction\n\nWelcome Kagglers,\n\nAll basketball fans and, in general, sports lovers were devastated about the loss in the tragic 2020 of one of the greatest players ever. Kobe Bryant was an off guard who spent his entire career in Los Angeles Lakers, winning for them five championships.\n\nThe Black Mamba also was an 18-time All-Star and won 2 gold medals in the Olympic Games of London and Beijing representing its country: the United States, and beating both times my country... hehe\n\nWith all these achievements and his unforgetable moves, he is considered one of the best in the game. After the tragic event, Kobe received the recognition, affection, and the warmest possible farewell from the fans all over the world.\n\nIn this notebook, I will make an exploratory analysis of the shots made by this player throughout his entire career, including interesting visualizations and extracting some insights about them. Furthermore, I will make a model that predicts whether a shot was successful or not given some features of this same shot. For doing so, an ensemble of different models will be implemented. So if you are not familiar with this kind of procedure, stick with the reading, and I will explain everything you need.\n\nOn the other side, I would like to thank and recognize the effort of other kagglers, whose works were a great inspiration for doing this notebook:\n- kevins's Kobe Shots - Show Me Your Best Model: https://www.kaggle.com/kevins/kobe-shots-show-me-your-best-model\n- Xavier's Kobe Bryant Shot Selection: https://www.kaggle.com/xvivancos/kobe-bryant-shot-selection\n\nFinally, the data description of Kaggle recommends avoiding leakage by only trining on events that occurred prior to the shot for which we are predicting. It is said that is up to us to abide by this rule, and having taken a look at other libraries the general rule is to disregard this restriction. So, for the sake of simplicity, we will predict the shots on all the train observations, with prior and later events of the shot.\n\nSaid this, I hope you enjoy the notebook, don't forget to upvote if you like it, and remember that any advice or guidance will be welcome and appreciatted.","metadata":{}},{"cell_type":"markdown","source":"### Index\n\n[The data](#section0)\n\n1. [Loading the necessary libraries](#section1)\n2. [Loading the dataset itself](#section2)\n3. [Correct variable types](#section3)\n4. [Data fast summary](#section4)\n5. [Some exploration](#section5)\n6. [Preprocess the data](#section6)\n7. [Separate train and test sets](#section7)\n8. [Feature Selection](#section8)\n9. [Prepare dataset for futher analysis](#section9)\n10. [Evaluate Algorithms](#section10)\n11. [Hyperparameter tuning](#section11)\n12. [Final model: Voting Ensemble](#section12)\n13. [Final predictions and submission](#section13)","metadata":{}},{"cell_type":"markdown","source":"### <a id='section0'>The data</a>\n\nThe data is from the Kaggle's Playground Prediction Competition, it can be found [here](https://www.kaggle.com/c/kobe-bryant-shot-selection/data). As its data description states:\n\nThis data contains the location and circumstances of every field goal attempted by Kobe Bryant took during his 20-year career. The task is to predict whether the basket went in (shot_made_flag).\n\n5000 of the shot_made_flags have been removed and represented as missing values in the csv file. These are the test set shots for which we must submit a prediction.\n\nThe field names are self-explanatory and contain the following attributes:\n\n    action_type\n    combined_shot_type\n    game_event_id\n    game_id\n    lat\n    loc_x\n    loc_y\n    lon\n    minutes_remaining\n    period\n    playoffs\n    season \n    seconds_remaining\n    shot_distance\n    shot_made_flag (this is what you are predicting)\n    shot_type\n    shot_zone_area\n    shot_zone_basic\n    shot_zone_range\n    team_id\n    team_name\n    game_date\n    matchup\n    opponent\n    shot_id\n","metadata":{}},{"cell_type":"markdown","source":"### <a id='section1'>1. Loading the necessary libraries</a>","metadata":{}},{"cell_type":"code","source":"# For processing the data\nimport numpy as np\nimport pandas as pd\n\n# Visualization tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.lines import Line2D\n%matplotlib inline\nsns.set_style(\"white\") # set style for seaborn plots\n\n# Machine learning\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.feature_selection import VarianceThreshold, RFE, SelectKBest, chi2\nfrom sklearn.metrics import make_scorer, log_loss\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (BaggingClassifier, ExtraTreesClassifier, \n                              GradientBoostingClassifier, VotingClassifier, \n                              RandomForestClassifier, AdaBoostClassifier)\n\n# Ignore warnings\nimport warnings \nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='section2'>2. Loading the dataset itself</a>\n\nAs stated in the data section, the dataset consists of only one csv file. There are 5000 of the shot_made_flags observations as missing values. These values represent our test set, and our goal here is to predict them.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/kobe-bryant-shot-selection/data.csv.zip\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We set the index using the existing column `shot_id`:","metadata":{}},{"cell_type":"code","source":"df.set_index('shot_id', inplace=True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='section3'>3. Correct variable types</a>","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all, we will transform the `period` column into an `object`. We won't be doing mathematical operations with it so it is not necessary to maintain it as an integer.\n\nOn the other side, there are several variables that can be encoded as `category`. This will let us interact with a more efficient DataFrame in terms of running speed and memory usage.","metadata":{}},{"cell_type":"code","source":"df[\"period\"] = df[\"period\"].astype('object')\n\nvars_to_category = [\"combined_shot_type\", \"game_event_id\", \"game_id\", \"playoffs\", \n                    \"season\", \"shot_made_flag\", \"shot_type\", \"team_id\"]\nfor col in vars_to_category:\n    df[col] = df[col].astype('category')\n\n# Let us check the final types\ndf.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id='section4'>4. Data fast summary</a>","metadata":{}},{"cell_type":"code","source":"print(\"Dimensions of out DataFrame:\", df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we knew, `shot_made_flag` has null values corresponding to the test observations. But surprisingly, this is the only variable with missing data, so no imputation will be needed.","metadata":{}},{"cell_type":"code","source":"df.describe(include=['number'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(include=['object', 'category'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"section5\">5. Some exploration</a>\n\nIn this section, we will further explore our dataset. Primarily, we will display visualizations, which are a very effective way to get insights into our data.\n\nSimilarly, we will try to identify variables that can have a significant impact on the explainability of our dependant variable: `shot_made_flag`. We start with our target class distribution:","metadata":{}},{"cell_type":"code","source":"ax = plt.axes()\nsns.countplot(\"shot_made_flag\", data=df, ax=ax, palette=(\"#552583\", \"#FDB927\"))\nax.set_title(\"Distribution of the dependent variable\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At first, we can see that the target variable is distributed quite equally. We won't perform any actions to deal with imbalanced datasets.\n\nNow we continue with the shots made or missed in connection with the position they were taken. The next graph will display exactly this:","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10, 7))\nscatter = sns.scatterplot(x=df[\"lon\"], y=df[\"lat\"], hue=df['shot_made_flag'],\n                                    alpha=0.55, ax=ax, palette=(\"#552583\", \"#FDB927\"))\nscatter.set_xlim(left=-118.54, right=-118)\nscatter.set_ylim(bottom=33.6, top=34.1)\nax.set_title(\"Shots made and missed based on court position\")\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nlegend_elemnts = [Line2D([0], [0], marker=\"o\", color='w', label=\"Made\",\n                         markerfacecolor=\"#FDB927\", markersize=10),\n                  Line2D([0], [0], marker=\"o\", color='w', label=\"Missed\",\n                         markerfacecolor=\"#552583\", markersize=10)]\nplt.legend(handles=legend_elemnts, title=\"Shot missed/made\", \n           ncol=2, fontsize='small', fancybox=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that there is a clear cluster of points made next to the basket. On the other hand, there is a clear trend in the central area: it seems that Kobe was more accurate there. I think the right side of the court has a little bit more yellow in it, less perceptible though. Let us check these two last assumptions:","metadata":{}},{"cell_type":"code","source":"# We don't want to modify the original DataFrame\nsubset = df.copy()\nsubset[\"x_zones\"] = pd.cut(df[\"loc_x\"], bins=25)\ndf_grouped1 = subset.groupby(\"x_zones\").agg({\"shot_made_flag\": \"count\"}).reset_index()\ndf_shots_made = subset[subset[\"shot_made_flag\"]==1]\ndf_grouped2 = df_shots_made.groupby(\"x_zones\").agg({\"shot_made_flag\": \"count\"}).reset_index()\nproportions = round(df_grouped2[\"shot_made_flag\"] / df_grouped1[\"shot_made_flag\"], 2)\n\nf, ax = plt.subplots(figsize=(12, 6))\n# Plot total shots\ng1 = sns.barplot(x=\"x_zones\", y=\"shot_made_flag\", data=df_grouped1,\n                 label=\"Total\", color=\"#552583\")\n\n# Plot shots made\ng2 = sns.barplot(x=\"x_zones\", y=\"shot_made_flag\", data=df_grouped2,\n                 label=\"Made\", color=\"#FDB927\")\n\nidx = 0\nfor p in g1.patches:\n    g1.annotate(proportions[idx],\n               (p.get_x() + p.get_width() / 2., p.get_height()-80), \n                ha=\"center\", va=\"center\", \n                xytext=(0, 9), fontsize=9,\n                textcoords=\"offset points\")\n    if idx < 24: idx += 1\n    else: break\n    \nplt.yticks(ticks=[0, 2000, 4000, 6000])\nplt.xticks(fontsize=8, rotation=90)\nax.set_title(\"Proportion of shots made by total considering x court strips\")\nax.set_xlabel(\"x zones of the court\")\nax.set_ylabel(\"Number of shots\")\nax.legend(ncol=2, loc=\"upper right\", frameon=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is now clear that taken central shots have more accuracy than lateral ones. Specifically, 60% of the shots made in the central strip are successful, while 40% are missed. Shots in the corner are the ones that Kobe had lower precision, which is a normal phenomenon among the great majority of players.\n\nWhat is interesting is that there is a better performance in some lateral zones than in others closer to the center. It also seems that the right-court shots had better results by a narrow margin.","metadata":{}},{"cell_type":"code","source":"def make_zone_scatter(var, ax):\n    sns.scatterplot(x=df[\"lon\"], y=df[\"lat\"], \n                    hue=df[var], ax=ax,\n                    palette=\"Dark2\")\n    ax.legend(ncol=len(df[var].unique())//3, fontsize='small', fancybox=True)\n\n    \ndef make_zone_countplot(var, ax):\n    sns.countplot(x=var, data=df, \n              order=df[var].value_counts().index, \n              ax=ax, palette=\"Dark2\")\n    ax.set_xlabel(\"\")\n    ax.set_xticklabels(df[var].unique(), fontsize=8, rotation=90)\n    \n    \ndef make_acc_lollipop(var, ax):\n    subset = df[[var, \"shot_made_flag\"]].dropna()\n    subset[\"shot_made_flag\"] = pd.to_numeric(subset[\"shot_made_flag\"])\n    df_grouped = subset.groupby(var).agg({\"shot_made_flag\": \"mean\"}).reset_index()\n    df_grouped = df_grouped.sort_values(by=\"shot_made_flag\")\n    ax.hlines(y=df_grouped[var], xmin=0,\n               xmax=df_grouped[\"shot_made_flag\"], color=\"#552583\", linewidth=3)\n    ax.plot(df_grouped[\"shot_made_flag\"], range(0,len(df_grouped.index)), \"o\", color=\"#FDB927\")\n    ax.set_xlim([0, .7])\n    ax.set_xlabel(\"Accuracy\")\n\n    \nf, ((ax0, ax1, ax2), (ax3, ax4, ax5), (ax6, ax7, ax8)) = plt.subplots(3, 3, figsize=(16, 16))\nmake_zone_scatter(\"shot_zone_area\", ax0)\nmake_zone_scatter(\"shot_zone_basic\", ax1)\nmake_zone_scatter(\"shot_zone_range\", ax2)\n\nmake_zone_countplot(\"shot_zone_area\", ax3)\nmake_zone_countplot(\"shot_zone_basic\", ax4)\nmake_zone_countplot(\"shot_zone_range\", ax5)\n\nmake_acc_lollipop(\"shot_zone_area\", ax6)\nmake_acc_lollipop(\"shot_zone_basic\", ax7)\nmake_acc_lollipop(\"shot_zone_range\", ax8)\n\nf.tight_layout()\nf.suptitle(\"Distribution of shots by zone-related variable\", fontsize=16, y=1.03);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this combined figure we can understand how the zone-related variables are situated among the basketball court, how are their distributions (i.e. how many shots took place in each area), and how these areas affect our dependent binary variable `shot_made_flag`.","metadata":{}},{"cell_type":"markdown","source":"Besides that, different types of shots have been categorized in the variables `combined_shot_type` and `action_type`. Here we examine these features, providing their impact on the accuracy of the shot metric.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10, 8))\njump_shot_df = df[df[\"combined_shot_type\"] == \"Jump Shot\"]\nscatter_jumpshots = sns.scatterplot(x=jump_shot_df[\"lon\"], y=jump_shot_df[\"lat\"], \n                                    alpha=0.1, ax=ax, color=\"grey\")\n\nnot_jump_shot_df = df[df[\"combined_shot_type\"] != \"Jump Shot\"]\nscatter = sns.scatterplot(x=not_jump_shot_df[\"lon\"], y=not_jump_shot_df[\"lat\"], \n                          hue=not_jump_shot_df[\"combined_shot_type\"], \n                          palette=[\"C8\", \"#552583\", \"C3\", \"#000000\", \"#FDB927\"],\n                          ax=ax)\n\nscatter.set_xlim(left=-118.54, right=-118)\nscatter.set_ylim(bottom=33.65, top=34.1);\nax.set_title(\"Shots made by type/kind\")\nax.set_xlabel(\"\")\nax.set_ylabel(\"\")\nplt.legend(ncol=len(df[\"combined_shot_type\"].unique())+1, fontsize='small', fancybox=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,8))\nmake_acc_lollipop(\"action_type\", ax)\nax.set_xlim([0, 1.05])\nax.tick_params(axis=\"y\", labelsize=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We continue with the accuracy exploration, now considering the seconds remaining for the last fourth quarter or the extra ones. They should be shots with a lot of pressure, which could lead to worse performance, but Kobe has overall good stats. Despite there is a decrease in accuracy after the 5 seconds remaining.","metadata":{}},{"cell_type":"code","source":"subset = df[df[\"period\"]<=4][[\"seconds_remaining\", \"shot_made_flag\"]].dropna()\nsubset = pd.DataFrame(subset, dtype=int)\ndf_grouped3 = subset.groupby(\"seconds_remaining\").agg({\"shot_made_flag\": \"mean\"}).reset_index()\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 4))\nsns.barplot(x=\"seconds_remaining\", y=\"shot_made_flag\", data=df_grouped3, \n            palette=(\"#552583\", \"#FDB927\", \"#000000\"))\nax.set_title(\"Proportion of shots converted by seconds remaining of fourth or extra quarter\")\nax.set_xlabel(\"Seconds remaining\")\nax.set_ylabel(\"Precision percentage\")\nplt.xticks(fontsize=8, rotation=90);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The performance of Kobe dropped in his last three years in the league. At least in terms of shot precision, this graph shows it:","metadata":{}},{"cell_type":"code","source":"subset4 = df[[\"season\", \"shot_made_flag\"]].dropna()\nsubset4[\"shot_made_flag\"] = pd.to_numeric(subset4[\"shot_made_flag\"])\ndf_grouped4 = subset4.groupby(\"season\").agg({\"shot_made_flag\": \"mean\"}).reset_index()\n\nf, ax = plt.subplots(1, 1, figsize=(8,8))\nsns.lineplot(x=\"season\", y=\"shot_made_flag\", data=df_grouped4, color=\"#552583\", ax=ax);\nsns.scatterplot(x=\"season\", y=\"shot_made_flag\", data=df_grouped4, s=100, color=\"#FDB927\", ax=ax)\nax.set_title(\"Accuracy per season\")\nax.set_xlabel(\"Accuracy\")\nax.set_ylabel(\"Season\")\nplt.xticks(fontsize=8, rotation=90);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we will visualize three extra features: period, playoffs, shot_type. We can extract some surprising and valuable insights from them: Kobe had incredible accuracy stats in playoffs and in the extra times, big moment player.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(3, figsize=(12, 10))\n\nfor var, i in zip([\"period\", \"playoffs\", \"shot_type\"], range(0,3)):\n    sns.countplot(x=var, hue=\"shot_made_flag\", data=df, ax=ax[i], palette=(\"#552583\", \"#FDB927\"))\n    ax[i].set_title(var)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"section6\">6. Preprocess the data</a>\n\nWe will now make some modifications to the data. To keep the original DataFrame integrity, we will copy it into a new one called: `copy_df`. This is considered a good practice and can be helpful to prevent undesired problems.","metadata":{}},{"cell_type":"code","source":"copy_df = df.copy()\ntarget = copy_df['shot_made_flag'].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.1 Remove useless columns\n\nLet us start removing some columns that do not provide any informative benefit. \n\n- `team_id` and `team_name` are quite useless features considering Kobe only played in one team L.A. Lakers: Their values have just one unique value.\n\n\n- For `game_id` and `game_event_id`, they are independent variables that have null relation with whether a shot is made or missed. They would add noise to our model.\n\n\n- `lat` and `long` are highly correlated with `loc_x` and `loc_y`, we could be adding multicollinearity problems to our set.\n\n\n- Ultimately, `shot_made_flag` is our dependent variable, and we have already stored it in the `target` series.","metadata":{}},{"cell_type":"code","source":"vars_to_remove = [\"team_id\", \"team_name\", \"game_id\", \"game_event_id\", \n                  \"lat\", \"lon\", \"shot_made_flag\"]\n\nfor var in vars_to_remove:\n    copy_df = copy_df.drop(var, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.2 Variable's transformation\n\n##### 6.2.1 Action types","metadata":{}},{"cell_type":"markdown","source":"There are way too many action types. We need to encode those values with fewer occurrences as a new category: \"Other\" or \"Rare actions\". Otherwise, when we one-hot-encode, we will experience a great increase in the columns' dimension.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({\"counts\": copy_df[\"action_type\"].value_counts().sort_values()[:25]})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rare_action_types = copy_df[\"action_type\"].value_counts().sort_values().index.values[:20]\ncopy_df.loc[copy_df[\"action_type\"].isin(rare_action_types), \"action_type\"] = \"Other\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 6.2.2 Game date\nWe will separate the month and year from the date. As we will see later on, this will contribute to the explainability of the target.","metadata":{}},{"cell_type":"code","source":"copy_df[\"game_date\"] = pd.to_datetime(copy_df[\"game_date\"])\ncopy_df[\"game_year\"] = copy_df[\"game_date\"].dt.year\ncopy_df[\"game_month\"] = copy_df[\"game_date\"].dt.month\ncopy_df = copy_df.drop(\"game_date\", axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 6.2.3 Last seconds\nAs we observed in the exploratory analysis section, there was a significant decrease in the shots taken with less than 5 seconds remaining. And similarly, the accuracy with more seconds was quite uniform. We will perform a transformation to include this phenomenon and reduce the number of future columns.","metadata":{}},{"cell_type":"code","source":"copy_df[\"seconds_from_period_end\"] = 60 * copy_df[\"minutes_remaining\"] + copy_df[\"seconds_remaining\"]\ncopy_df[\"last_5_sec_in_period\"] = copy_df[\"seconds_from_period_end\"] < 5\n\n# We can drop the rest of time related fields\ncopy_df = copy_df.drop(\"minutes_remaining\", axis=1)\ncopy_df = copy_df.drop(\"seconds_remaining\", axis=1)\ncopy_df = copy_df.drop(\"seconds_from_period_end\", axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 6.2.4 x and y zones\n\nWe already did something similar in the data visualization section. Now we will include these strips in our training set for the x-axis and y-axis. But we won't drop `loc_x` and `loc_y`.","metadata":{}},{"cell_type":"code","source":"copy_df[\"x_zones\"] = pd.cut(copy_df[\"loc_x\"], bins=25)\ncopy_df[\"y_zones\"] = pd.cut(copy_df[\"loc_y\"], bins=25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 6.2.5 Home games\nIt will be clearer if we set a binary variable that will determine if a game was played at home or away with the classic 1 or 0 values.","metadata":{}},{"cell_type":"code","source":"copy_df[\"home_play\"] = copy_df[\"matchup\"].str.contains(\"vs\").astype(\"int\")\ncopy_df = copy_df.drop(\"matchup\", axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.3 Encode the categorical variables\n\nWe are finally in a position to one-hot-encode our categorical variables.","metadata":{}},{"cell_type":"code","source":"pd.get_dummies(copy_df[\"action_type\"]).add_prefix(\"{}#\".format(\"action_type\"))\n\ncategorial_vars = [\n    'action_type', 'combined_shot_type', 'period', 'season', 'shot_type',\n    'shot_zone_area', 'shot_zone_basic', 'shot_zone_range', 'game_year',\n    'game_month', 'opponent', 'loc_x', 'loc_y', 'x_zones', 'y_zones']\n\nfor var in categorial_vars:\n    dummies = pd.get_dummies(copy_df[var])\n    dummies = dummies.add_prefix(\"{}#\".format(var))\n    copy_df.drop(var, axis=1, inplace=True)\n    copy_df = copy_df.join(dummies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"section7\">7. Separate train and test sets</a>\n","metadata":{}},{"cell_type":"code","source":"missing = target.isnull()\n\ndata_submit = copy_df[missing]\nX = copy_df[~missing]\nY = target[~missing]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape, Y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"copy_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"section8\">8. Feature Selection</a>\n\nWhen we one-hot-encoded, we abruptly increased the columns of our set, we drew from less than 1106 features and we now have 208. This happened even with all the hard work of variable's disregard and transformation we did before.\n\nWell, this is fairly normal when the number of categories in the variables is high. Fortunately, we have enough observations to deal with all these columns; and, more importantly, with techniques to reduce them. We will be doing so in this section by selecting those more informative variables.","metadata":{}},{"cell_type":"markdown","source":"Let us start with this reduction of features. We will implement different techniques and combine them in a final selection stage.","metadata":{}},{"cell_type":"markdown","source":"#### 8.1 Variance Threshold\nWe will find all features with a training-set variance greater than 90%.","metadata":{}},{"cell_type":"code","source":"threshold = 0.9\nvt = VarianceThreshold().fit(X)\n\n# Find feature names\nfeat_var_threshold = copy_df.columns[vt.variances_ > threshold * (1-threshold)]\nfeat_var_threshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8.2 Most important features \n\n`RandomForestClassifier` allows us to get the feature's importances. According to them, we will select the top 30.","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.fit(X, Y)\n\nfeature_imp = pd.DataFrame(model.feature_importances_, index=X.columns, columns=[\"importance\"])\nfeat_imp_30 = feature_imp.sort_values(\"importance\", ascending=False).head(30).index\nfeat_imp_30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8.3 Univariate feature selection\n\nWith this procedure, we will also select the top 30 features but using a chi2 test. The features must be positive before applying this test.","metadata":{}},{"cell_type":"code","source":"X_minmax = MinMaxScaler(feature_range=(0,1)).fit_transform(X)\nX_scored = SelectKBest(score_func=chi2, k=\"all\").fit(X_minmax, Y)\nfeature_scoring = pd.DataFrame({\n    \"feature\": X.columns,\n    \"score\": X_scored.scores_\n})\n\nfeat_scored_30 = feature_scoring.sort_values(\"score\", ascending=False).head(30)[\"feature\"].values\nfeat_scored_30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8.4 Recursive Feature Elimination\n\nWe now select the best 30 features by using recursive feature elimination (RFE) with a logistic regression model.","metadata":{}},{"cell_type":"code","source":"# Running time can take several minutes\n# You can ignore this method and don't include it in the final feature selection\nrfe = RFE(LogisticRegression(), 30)\nrfe.fit(X, Y)\n\nfeature_rfe_scoring = pd.DataFrame({\n    \"feature\": X.columns, \n    \"score\": rfe.ranking_\n})\n\nfeat_rfe_30 = feature_rfe_scoring[feature_rfe_scoring[\"score\"] == 1][\"feature\"].values\nfeat_rfe_30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8.5 Final feature selection \n\nFinally, we will get our selection of features by merging all methods above. In a nutshell, we will keep those variables that, at least, appear as the best variable in one of the techniques.","metadata":{}},{"cell_type":"code","source":"features = np.hstack([\n    feat_var_threshold,\n    feat_imp_30,\n    feat_scored_30,\n    feat_rfe_30\n])\n\nfeatures = np.unique(features)\nprint(\"Final features set:\\n\")\nfor f in features:\n    print(\"\\t-{}\".format(f))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"section9\">9. Prepare dataset for futher analysis</a>","metadata":{}},{"cell_type":"code","source":"copy_df = copy_df.loc[:, features]\ndata_submit = data_submit.loc[:, features]\nX = X.loc[:, features]\n\nprint(\"Clean dataset shape: {}\".format(copy_df.shape))\nprint(\"Subbmitable dataset shape: {}\".format(data_submit.shape))\nprint(\"Train features shape: {}\".format(X.shape))\nprint(\"Target label shape: {}\".format(Y.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here I show you the actual version of sklearn used to help solve compatibility problems. After that, we set some variables that we will be using through the model construction. The first one is the random seed: to get reproducible results is a must. \n\nThe number of processors is set to -1, this means that your computer will use all its cores to parallel process the code. `n_folds` is the number of partitions we want when we perform cross-validation. Log loss is the metric chosen to get the scoring performance of the models.","metadata":{}},{"cell_type":"code","source":"seed = 2666\nprocessors = -1\nnum_folds = 3\nscoring=\"neg_log_loss\"\n\nkfold = KFold(n_splits=num_folds, random_state=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 10.1 Algorithms spot-check\n\nNow we will fast-prepare some basic models and see how they behave in our particular dataset.","metadata":{}},{"cell_type":"code","source":"models = []\nmodels.append((\"LR\", LogisticRegression()))\nmodels.append((\"LDA\", LinearDiscriminantAnalysis()))\nmodels.append((\"K-NN\", KNeighborsClassifier(n_neighbors=5)))\nmodels.append((\"CART\", DecisionTreeClassifier()))\nmodels.append((\"NB\", GaussianNB()))\n\n\nresults = []\nnames = []\nfor name, model in models:\n    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\n    results.append(cv_results)\n    names.append(name)\n    print(\"{0}:({1:.3f}) +/- ({2:.3f})\".format(name, cv_results.mean(), cv_results.std()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By looking at these results, Logistic Regression and Linear Discriminant Analysis are providing decent results and are worth further examination.\n\nBut apart from these simple algorithms, let's look at some ensemble models before to see if we can find some more interesting models: ","metadata":{}},{"cell_type":"markdown","source":"#### 10.2 Ensembles\n\n##### 10.2.1 Bagging (Bootstrap Aggregation)\n\nIt involves taking multiple samples with replacement from the training dataset, and training a model for each one of them.\nThe final output prediction is averaged across the predictions of all of the sampled-based-models.","metadata":{}},{"cell_type":"markdown","source":"###### Bagged Decision Trees","metadata":{}},{"cell_type":"code","source":"cart = DecisionTreeClassifier()\nnum_trees = 100\n\nmodel = BaggingClassifier(base_estimator = cart, n_estimators = num_trees, random_state=seed)\n\nresult = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\nprint(\"({0:.3f}) +/- ({1:.3f})\".format(np.mean(results), np.std(results)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Random Forest ","metadata":{}},{"cell_type":"code","source":"num_features = 10\n\nmodel = RandomForestClassifier(n_estimators=num_trees, max_features=num_features)\n\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\nprint(\"({0:.3f}) +/- ({1:.3f})\".format(np.mean(results), np.std(results)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Extra Trees","metadata":{}},{"cell_type":"code","source":"model = ExtraTreesClassifier(n_estimators=num_trees, max_features=num_features)\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\nresult = np.array(result)\nprint(\"({0:.3f}) +/- ({1:.3f})\".format(np.mean(results), np.std(results)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 10.2.2 Boosting\n\nBoosting algorithms seek to improve the prediction power by training a sequence of weak models, each compensating the weaknesses of its predecessors. To understand Boosting, it is crucial to recognize that boosting is a generic algorithm rather than a specific model. Boosting needs you to specify a weak model (e.g. regression, shallow decision trees, etc) and then improves it.","metadata":{}},{"cell_type":"markdown","source":"###### AdaBoost","metadata":{}},{"cell_type":"code","source":"model = AdaBoostClassifier(n_estimators=100, random_state=seed)\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\nprint(\"({0:.3f}) +/- ({1:.3f})\".format(np.mean(results), np.std(results)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"###### Stochastic Gradient Boosting ","metadata":{}},{"cell_type":"code","source":"model = GradientBoostingClassifier(n_estimators=100, random_state=seed)\n\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\nprint(\"({0:.3f}) +/- ({1:.3f})\".format(np.mean(results), np.std(results)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"section11\">11. Hyperparameter tuning</a>\n\nWe are left with all those models that got better results. But they could be getting even better performance if we would have defined their optimal architecture of the models. This is what we will be doing here: selecting from a specific list of hyperparameters for each model the ones that work better for our data.\n\nThis selection procedure for hyperparameter is known as Hyperparameter Tuning, and `GridSearchCV()` will be our best friend.","metadata":{}},{"cell_type":"markdown","source":"#### 11.1 Logistic Regression","metadata":{}},{"cell_type":"code","source":"lr_grid = GridSearchCV(\n    estimator = LogisticRegression(random_state=seed),\n    param_grid = {\n        'penalty': ['l1', 'l2'],\n        'C': [0.001, 0.01, 1, 10, 100, 1000]\n    },\n    cv = kfold,\n    scoring=scoring,\n    n_jobs=processors)\n\nlr_grid.fit(X, Y)\n\nprint(lr_grid.best_score_)\nprint(lr_grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.2 Linear Discriminant Analysis","metadata":{}},{"cell_type":"code","source":"lda_grid = GridSearchCV(\n    estimator = LinearDiscriminantAnalysis(),\n    param_grid = {\n        'solver': ['lsqr'],\n        'shrinkage':[0, 0.25, 0.5, 0.75, 1],\n        'n_components':[None, 2, 5, 10]\n    },\n    cv=kfold,\n    scoring=scoring,\n    n_jobs=processors)\n\nlda_grid.fit(X, Y)\n\nprint(lr_grid.best_score_)\nprint(lr_grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.3 K-NN\n","metadata":{}},{"cell_type":"code","source":"knn_grid = GridSearchCV(\n    estimator = Pipeline([\n        ('min_max_scaler', MinMaxScaler()),\n        ('knn', KNeighborsClassifier())\n    ]),\n    param_grid = {\n        'knn__n_neighbors': [25],\n        'knn__algorithm': ['ball_tree'],\n        'knn__leaf_size': [2, 3, 4],\n        'knn__p': [1]\n    },\n    cv = kfold,\n    scoring = scoring,\n    n_jobs=processors\n    )\n\nknn_grid.fit(X, Y)\n\nprint(knn_grid.best_score_)\nprint(knn_grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.4 Random Forest","metadata":{}},{"cell_type":"code","source":"rf_grid = GridSearchCV(\n    estimator = RandomForestClassifier(warm_start=True, random_state=seed),\n    param_grid = {\n        'n_estimators': [100, 200],\n        'criterion': ['gini', 'entropy'],\n        'max_features': [18, 20],\n        'max_depth': [8, 10],\n        'bootstrap': [True]\n    }, \n    cv = kfold, \n    scoring = scoring, \n    n_jobs = processors)\n\nrf_grid.fit(X, Y)\n\nprint(rf_grid.best_score_)\nprint(rf_grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.5 AdaBoost ","metadata":{}},{"cell_type":"code","source":"ada_grid = GridSearchCV(\n    estimator = AdaBoostClassifier(random_state=seed),\n    param_grid = {\n        'algorithm': ['SAMME', 'SAMME.R'],\n        'n_estimators': [10, 25, 50],\n        'learning_rate': [1e-3, 1e-2, 1e-1]\n    }, \n    cv = kfold, \n    scoring = scoring, \n    n_jobs = processors)\n\nada_grid.fit(X, Y)\n\nprint(ada_grid.best_score_)\nprint(ada_grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11.6 Gradient Boosting","metadata":{}},{"cell_type":"code","source":"gbm_grid = GridSearchCV(\n    estimator = GradientBoostingClassifier(warm_start=True, random_state=seed),\n    param_grid = {\n        'n_estimators': [100, 200],\n        'max_depth': [2, 3, 4],\n        'max_features': [10, 15, 20],\n        'learning_rate': [1e-1, 1]\n    }, \n    cv = kfold, \n    scoring = scoring, \n    n_jobs = processors)\n\ngbm_grid.fit(X, Y)\n\nprint(gbm_grid.best_score_)\nprint(gbm_grid.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"section12\">12. Final Model: Voting Ensemble</a>\n\nWe are on our last step in the model development. We select our four best models based on the log loss scoring, with their best possible hyperparameters, and combine them in an ensemble called a Voting classifier. \n\nVoting is one of the simplest ways of combining the predictions from multiple machine learning algorithms. The voting classifier isn’t an actual classifier but a wrapper for a set of different algorithms that are trained and evaluated in parallel, in order to exploit the different peculiarities of each of them.\n\nIn the soft voting (the modality we have chosen), the probability vector for each predicted class (for all classifiers) are summed up and averaged. The winning class is the one corresponding to the highest value. We also set different weights depending on the results of the models: for example, gradient boosting and random forest, the two models that achieved better log loss, have a weight of 3.\n\nBy this way of proceeding, we have more robust models.","metadata":{}},{"cell_type":"code","source":"estimators = []\nestimators.append(('lr', LogisticRegression(penalty='l2', C=1)))\nestimators.append(('gbm', GradientBoostingClassifier(n_estimators=200, max_depth=3, learning_rate=0.1, max_features=15, warm_start=True, random_state=seed)))\nestimators.append(('rf', RandomForestClassifier(bootstrap=True, max_depth=8, n_estimators=200, max_features=20, criterion='entropy', random_state=seed)))\nestimators.append(('ada', AdaBoostClassifier(algorithm='SAMME.R', learning_rate=1e-2, n_estimators=10, random_state=seed)))\n\n\n# Create the ensemble model\nensemble = VotingClassifier(estimators, voting='soft', weights=[2,3,3,1])\n\nresults = cross_val_score(ensemble, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\nprint(\"({0:.3f}) +/- ({1:.3f})\".format(np.mean(results), np.std(results)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a id=\"section13\">13. Final predictions and submission</a>","metadata":{}},{"cell_type":"code","source":"model = ensemble\nmodel.fit(X, Y)\npreds = model.predict_proba(data_submit)\npreds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission[\"shot_id\"] = data_submit.index\nsubmission[\"shot_made_flag\"]= preds[:,0]\n\nsubmission.to_csv(\"sub.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}