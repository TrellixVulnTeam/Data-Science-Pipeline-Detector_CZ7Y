{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os \nimport sys\nimport random\nimport math\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nimport pydicom\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nimport pandas as pd \nimport glob\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_file_path = '/kaggle/input/rsna-pneumonia-detection-challenge'\n# to save the model to the current directory\nfile_path = '/kaggle/working'\n# Temporaary files stores just for this session\ntemp_file_path = '/kaggle/temp'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mask R-CNN (Region based CNN)\nThe following section uses the Matterports Mask R-CNN. This is used for the transfer learning of the model to autommatically locate lung opacities ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://www.github.com/matterport/Mask_RCNN.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow-gpu==1.13.1\n!pip install tensorflow==1.13.1\n!pip install keras==2.0.8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir('Mask_RCNN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python3 setup.py install","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Assigning the Root Directory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sys.path.append(os.path.join(file_path, 'Mask_RCNN'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mrcnn import utils\nfrom mrcnn import visualize\nfrom mrcnn.visualize import display_images\nimport mrcnn.model as modellib\nfrom mrcnn.model import log\nfrom mrcnn.config import Config\nimport mrcnn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Assigning the Data Directory","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dicom_dir = os.path.join(data_file_path, 'stage_2_train_images')\ntest_dicom_dir = os.path.join(data_file_path, 'stage_2_test_images')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Downloading the COCO weights","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget --quiet https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n!ls -lh mask_rcnn_coco.h5\nWeights_file_path = \"mask_rcnn_coco.h5\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dicom_fps(dicom_dir):\n    # To get a list of dicom images\n    dicom_img = glob.glob(dicom_dir+'/'+'*.dcm')\n    return list(set(dicom_img))\n\ndef parse_dataset(dicom_dir, annotations): \n    # returns a list of all images\n    image_fps = get_dicom_fps(dicom_dir)\n    # annotates the list of images obtained\n    image_annotations = {fp: [] for fp in image_fps}\n    \n    for index, row in annotations.iterrows(): \n        \n        fp = os.path.join(dicom_dir, row['patientId']+'.dcm')\n        image_annotations[fp].append(row)\n        \n    return image_fps, image_annotations ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PneumoniaConfig(Config):\n    \"\"\"Configuration for training Lung Opacity location detection on the RSNA pneumonia dataset. Created a\n    sub-class that inherits from config class and override properties that need to be changed.\n    \"\"\"\n    # Name the configurations. \n    NAME = 'Lung_Opacity'  # Override in sub-classes\n\n    # NUMBER OF GPUs to use. When using only a CPU, this needs to be set to 1.\n    GPU_COUNT = 1\n    # Number of images to train with on each GPU. A 12GB GPU can typically\n    # handle 2 images of 1024x1024px.\n    # Adjust based on your GPU memory and image sizes. Use the highest\n    # number that your GPU can handle for best performance.\n    # Train on 2 GPU and 8 images per GPU. We can put multiple images on each\n    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n    IMAGES_PER_GPU = 8\n    \n    \n\n    # Number of training steps per epoch\n    # This doesn't need to match the size of the training set. Tensorboard\n    # updates are saved at the end of each epoch, so setting this to a\n    # smaller number means getting more frequent TensorBoard updates.\n    # Validation stats are also calculated at each epoch end and they\n    # might take a while, so don't set this too small to avoid spending\n    # a lot of time on validation stats.\n    STEPS_PER_EPOCH = 32\n    VALIDATION_STEPS = 8\n\n    \n    # Backbone network architecture\n    # Supported values are: resnet50, resnet101.\n    # You can also provide a callable that should have the signature\n    # of model.resnet_graph. If you do so, you need to supply a callable\n    # to COMPUTE_BACKBONE_SHAPE as well\n    # Here, we leave it as resnet 101\n    BACKBONE = 'resnet50'\n    \n    # Length of square anchor side in pixels\n#     RPN_ANCHOR_SCALES = (32, 64, 128, 256)\n#     BACKBONE_STRIDES = [4, 8, 16, 32]\n    \n\n    # Number of classification classes (including background)\n    NUM_CLASSES = 2  # background and 1 pneumonia class\n    \n    # Input image resizing\n    # Generally, use the \"square\" resizing mode for training and predicting\n    # and it should work well in most cases. In this mode, images are scaled\n    # up such that the small side is = IMAGE_MIN_DIM, but ensuring that the\n    # scaling doesn't make the long side > IMAGE_MAX_DIM. Then the image is\n    # padded with zeros to make it a square so multiple images can be put\n    # in one batch.\n    # Available resizing modes:\n    # none:   No resizing or padding. Return the image unchanged.\n    # square: Resize and pad with zeros to get a square image\n    #         of size [max_dim, max_dim].\n    # pad64:  Pads width and height with zeros to make them multiples of 64.\n    #         If IMAGE_MIN_DIM or IMAGE_MIN_SCALE are not None, then it scales\n    #         up before padding. IMAGE_MAX_DIM is ignored in this mode.\n    #         The multiple of 64 is needed to ensure smooth scaling of feature\n    #         maps up and down the 6 levels of the FPN pyramid (2**6=64).\n    # crop:   Picks random crops from the image. First, scales the image based\n    #         on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of\n    #         size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.\n    #         IMAGE_MAX_DIM is not used in this mode.\n    IMAGE_MIN_DIM = 256\n    IMAGE_MAX_DIM = 256\n    \n    # Number of ROIs per image to feed to classifier/mask heads\n    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n    # enough positive proposals to fill this and keep a positive:negative\n    # ratio of 1:3. You can increase the number of proposals by adjusting\n    # the RPN NMS threshold.\n    TRAIN_ROIS_PER_IMAGE = 32\n\n    # Maximum number of ground truth instances to use in one image\n    MAX_GT_INSTANCES = 3\n    \n    # Max number of final detections\n    DETECTION_MAX_INSTANCES = 3\n    \n    # Minimum probability value to accept a detected instance\n    # ROIs below this threshold are skipped\n    DETECTION_MIN_CONFIDENCE = 0.8\n\n    # Non-maximum suppression threshold for detection\n    DETECTION_NMS_THRESHOLD = 0.1\n        \nconfig = PneumoniaConfig()\nconfig.display()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PneumoniaDataset(utils.Dataset):\n    \"\"\"\n    Dataset class for training automated loation detection for lung opacities on the RSNA pneumonia dataset. For overriding the Base dataset class in Matterport's Mask R-CNN.\n    \n    \"\"\"\n\n    def __init__(self, image_fps, image_annotations, orig_height, orig_width):\n        # For parent class\n        super().__init__(self)\n        \n        # Add classes\n        self.add_class('pneumonia', 1, 'Lung Opacity')\n        \n        # add images \n        for i, fp in enumerate(image_fps):\n            \n            # Adding annotations\n            annotations = image_annotations[fp]\n            \n            # Add image function (self, source, image_id, path, anntations)\n            self.add_image('pneumonia', \n                           image_id=i, \n                           path=fp, \n                           annotations=annotations, \n                           orig_height=orig_height, orig_width=orig_width)\n            \n    def image_reference(self, image_id):\n        \"\"\"Return a link to the image in its source Website or details about\n        the image that help looking it up or debugging it.\n        Override for pneumonia dataset, and pass to this function\n        if you encounter images not in your dataset.\n        \"\"\"\n        info = self.image_info[image_id]\n        return info['path']\n\n    \n    def load_image(self, image_id):\n        \"\"\"\n        Load the specified image and return a [H,W,3] Numpy array.\n        \"\"\"\n        info = self.image_info[image_id]\n        \n        fp = info['path']\n        \n        ds = pydicom.read_file(fp)\n        image = ds.pixel_array\n        \n        # If grayscale. Convert to RGB for consistency.\n        if len(image.shape) != 3 or image.shape[2] != 3:\n            image = np.stack((image,) * 3, -1)\n \n\n        return image\n\n    \n    def load_mask(self, image_id):\n        \"\"\"\n        Load instance masks for the given image.\n        Different datasets use different ways to store masks. Override this\n        method to load instance masks and return them in the form of am\n        array of binary masks of shape [height, width, instances].\n        Returns:\n            masks: A bool array of shape [height, width, instance count] with\n                a binary mask per instance.\n            class_ids: a 1D array of class IDs of the instance masks.\n        \"\"\"\n\n\n        info = self.image_info[image_id]\n        \n        annotations = info['annotations']\n        \n        count = len(annotations)\n        \n        if count == 0:\n            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n            class_ids = np.zeros((1,), dtype=np.int32)\n            \n        else:\n            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n            class_ids = np.zeros((count,), dtype=np.int32)\n            \n            for i, a in enumerate(annotations):\n                \n                if a['Target'] == 1:\n                    \n                    x = int(a['x'])\n                    y = int(a['y'])\n                    w = int(a['width'])\n                    h = int(a['height'])\n                    mask_instance = mask[:, :, i].copy()\n                    cv2.rectangle(mask_instance, (x, y), (x+w, y+h), 255, -1)\n                    mask[:, :, i] = mask_instance\n                    class_ids[i] = 1\n                    \n        return mask.astype(np.bool), class_ids.astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annotations = pd.read_csv(os.path.join(data_file_path, 'stage_2_train_labels.csv'))\nannotations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annotations.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_fps, image_annotations = parse_dataset(train_dicom_dir, annotations=annotations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = pydicom.read_file(image_fps[0]) # read dicom image from filepath \nimage = ds.pixel_array # get image array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Original DICOM image size: 1024 x 1024\nORIG_SIZE = 1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_fps_list = list(image_fps)\n\n# Seeding to get same results in case of shuffling the data\nrandom.seed(42)\n\nrandom.shuffle(image_fps_list)\n\n# Validation size\nval_size = 8100\nimage_fps_validate1 = image_fps_list[:val_size]\nimage_fps_training = image_fps_list[val_size:]\n\nrandom.shuffle(image_fps_list)\n\n# Validation size\ntest_size = 4050\nimage_fps_validate = image_fps_validate1[:test_size]\nimage_fps_test = image_fps_validate1[test_size:]\n\nprint(len(image_fps_training), len(image_fps_validate))\nprint(image_fps_validate[:5])\nprint(image_fps_test[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the training dataset\ndataset_train = PneumoniaDataset(image_fps_training, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_train.prepare()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show annotation(s) for a DICOM image \ntest_fp = random.choice(image_fps_training)\nimage_annotations[test_fp]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare the validation dataset\ndataset_val = PneumoniaDataset(image_fps_validate, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_val.prepare()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load and display random sample and their bounding boxes\n\nclass_ids = [0]\nwhile class_ids[0] == 0:  ## look for a mask\n    image_id = random.choice(dataset_train.image_ids)\n    image_fp = dataset_train.image_reference(image_id)\n    image = dataset_train.load_image(image_id)\n    mask, class_ids = dataset_train.load_mask(image_id)\n\nprint(image.shape)\n\nplt.figure(figsize=(10, 10))\n\nplt.subplot(1, 2, 1)\n\nplt.imshow(image)\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nmasked = np.zeros(image.shape[:2])\n\nfor i in range(mask.shape[2]):\n    masked += image[:,:,0] * mask[:,:,i]\n    \nplt.imshow(masked, cmap='gray')\nplt.axis('off')\n\nprint(image_fp)\nprint(class_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Image augmentation (light but constant)\naugmentation = iaa.Sequential([\n    iaa.OneOf([ ## geometric transform\n        iaa.Affine(\n            scale={\"x\": (0.98, 1.02), \"y\": (0.98, 1.02)},\n            translate_percent={\"x\": (-0.02, 0.02), \"y\": (-0.04, 0.04)},\n            rotate=(-2, 2),\n            shear=(-1, 1),\n        ),\n        iaa.PiecewiseAffine(scale=(0.001, 0.025)),\n    ]),\n    iaa.OneOf([ ## brightness or contrast\n        iaa.Multiply((0.9, 1.1)),\n        iaa.ContrastNormalization((0.9, 1.1)),\n    ]),\n    iaa.OneOf([ ## blur or sharpen\n        iaa.GaussianBlur(sigma=(0.0, 0.1)),\n        iaa.Sharpen(alpha=(0.0, 0.1)),\n    ]),\n])\n\n# test on the same image as above\nimggrid = augmentation.draw_grid(image[:, :, 0], cols=5, rows=2)\nplt.figure(figsize=(30, 12))\n_ = plt.imshow(imggrid[:, :, 0], cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = modellib.MaskRCNN(mode='training', config=config, model_dir=file_path)\n\n# Exclude the last layers because they require a matching\n# number of classes\nmodel.load_weights(Weights_file_path, by_name=True, exclude=[\n    \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n    \"mrcnn_bbox\", \"mrcnn_mask\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nLEARNING_RATE = 0.005\n\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE,\n            epochs=7,\n            layers='all',\n            augmentation=augmentation)\nhistory = model.keras_model.history.history.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--------","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE,\n            epochs=15,\n            layers='all',\n            augmentation=augmentation)\ns = model.keras_model.history.history\nfor k in s: history[k] = history[k] + s[k]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE,\n            epochs=20,\n            layers='all',\n            augmentation=augmentation)\ns = model.keras_model.history.history\nfor k in s: history[k] = history[k] + s[k]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = range(1,len(next(iter(history.values())))+1)\npd.DataFrame(history, index=epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.subplot(111)\nplt.plot(epochs, history[\"loss\"], label=\"Training loss\")\nplt.plot(epochs, history[\"val_loss\"], label=\"Validation loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select trained model \ndir_names = next(os.walk(model.model_dir))[1]\nkey = config.NAME.lower()\ndir_names = filter(lambda f: f.startswith(key), dir_names)\ndir_names = sorted(dir_names)\n\nif not dir_names:\n    import errno\n    raise FileNotFoundError(\n        errno.ENOENT,\n        \"Could not find model directory under {}\".format(self.model_dir))\n    \nfps = []\n# Pick last directory\nfor d in dir_names:\n    dir_name = os.path.join(model.model_dir, d)\n    # Find the last checkpoint\n    checkpoints = next(os.walk(dir_name))[2]\n    checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n    checkpoints = sorted(checkpoints)\n    \n    if not checkpoints:\n        print('No weight files in {}'.format(dir_name))\n    else:\n        checkpoint = os.path.join(dir_name, checkpoints[-1])\n        fps.append(checkpoint)\n\nmodel_path = sorted(fps)[-1]\nprint('Found model {}'.format(model_path))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class InferenceConfig(PneumoniaConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\ninference_config = InferenceConfig()\n\n# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=file_path)\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set color for class\ndef get_colors_for_class_ids(class_ids):\n    colors = []\n    for class_id in class_ids:\n        if class_id == 1:\n            colors.append((.941, .204, .204))\n    return colors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset_val\nfig = plt.figure(figsize=(10, 30))\n\nfor i in range(6):\n\n    image_id = random.choice(dataset.image_ids)\n    \n    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n        modellib.load_image_gt(dataset_val, inference_config, \n                               image_id, use_mini_mask=False)\n    \n    print(original_image.shape)\n    plt.subplot(6, 2, 2*i + 1)\n    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n                                dataset.class_names,\n                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n    \n    plt.subplot(6, 2, 2*i + 2)\n    results = model.detect([original_image]) #, verbose=1)\n    r = results[0]\n    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], \n                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have used the test dataset that was taken from the stage 2 dataset separately and used it to get a score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get filenames of test dataset DICOM images\ntest_image_fps = get_dicom_fps(test_dicom_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on test images, write out sample submission\ndef predict(image_fps, filepath='submission.csv', min_conf=0.95):\n    resize_factor = ORIG_SIZE / config.IMAGE_SHAPE[0]\n    with open(filepath, 'w') as file:\n        file.write(\"patientId,PredictionString\\n\")\n\n        for image_id in tqdm(image_fps):\n            ds = pydicom.read_file(image_id)\n            image = ds.pixel_array\n            # If grayscale. Convert to RGB for consistency.\n            if len(image.shape) != 3 or image.shape[2] != 3:\n                image = np.stack((image,) * 3, -1)\n            image, window, scale, padding, crop = utils.resize_image(\n                image,\n                min_dim=config.IMAGE_MIN_DIM,\n                min_scale=config.IMAGE_MIN_SCALE,\n                max_dim=config.IMAGE_MAX_DIM,\n                mode=config.IMAGE_RESIZE_MODE)\n\n            patient_id = os.path.splitext(os.path.basename(image_id))[0]\n\n            results = model.detect([image])\n            r = results[0]\n\n            out_str = \"\"\n            out_str += patient_id\n            out_str += \",\"\n            assert( len(r['rois']) == len(r['class_ids']) == len(r['scores']) )\n            if len(r['rois']) == 0:\n                pass\n            else:\n                num_instances = len(r['rois'])\n\n                for i in range(num_instances):\n                    if r['scores'][i] > min_conf:\n                        out_str += ' '\n                        out_str += str(round(r['scores'][i], 2))\n                        out_str += ' '\n\n                        # x1, y1, width, height\n                        x1 = r['rois'][i][1]\n                        y1 = r['rois'][i][0]\n                        width = r['rois'][i][3] - x1\n                        height = r['rois'][i][2] - y1\n                        bboxes_str = \"{} {} {} {}\".format(x1*resize_factor, y1*resize_factor, \\\n                                                           width*resize_factor, height*resize_factor)\n                        out_str += bboxes_str\n\n            file.write(out_str+\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_results = os.path.join(file_path, 'test_results.csv')\npredict(image_fps_test, filepath=test_results)\nprint(test_results)\n# Against Submission\nsubmission_fp = os.path.join(file_path, 'submission.csv')\npredict(test_image_fps, filepath=submission_fp)\nprint(submission_fp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(test_results, names=['patientId', 'PredictionString'])\ntest.head(60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.read_csv(submission_fp, names=['patientId', 'PredictionString'])\noutput.head(60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize(): \n    image_id = random.choice(test_image_fps)\n    ds = pydicom.read_file(image_id)\n    \n    # original image \n    image = ds.pixel_array\n    \n    # assume square image \n    resize_factor = ORIG_SIZE / config.IMAGE_SHAPE[0]\n    \n    # If grayscale. Convert to RGB for consistency.\n    if len(image.shape) != 3 or image.shape[2] != 3:\n        image = np.stack((image,) * 3, -1) \n    resized_image, window, scale, padding, crop = utils.resize_image(\n        image,\n        min_dim=config.IMAGE_MIN_DIM,\n        min_scale=config.IMAGE_MIN_SCALE,\n        max_dim=config.IMAGE_MAX_DIM,\n        mode=config.IMAGE_RESIZE_MODE)\n\n    patient_id = os.path.splitext(os.path.basename(image_id))[0]\n    print(patient_id)\n\n    results = model.detect([resized_image])\n    r = results[0]\n    for bbox in r['rois']: \n        print(bbox)\n        x1 = int(bbox[1] * resize_factor)\n        y1 = int(bbox[0] * resize_factor)\n        x2 = int(bbox[3] * resize_factor)\n        y2 = int(bbox[2]  * resize_factor)\n        cv2.rectangle(image, (x1,y1), (x2,y2), (77, 255, 9), 3, 1)\n        width = x2 - x1 \n        height = y2 - y1 \n        print(\"x {} y {} h {} w {}\".format(x1, y1, width, height))\n    plt.figure() \n    plt.imshow(image, cmap=plt.cm.gist_gray)\n\nvisualize()\nvisualize()\nvisualize()\nvisualize()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf /kaggle/working/Mask_RCNN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----\n----","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}