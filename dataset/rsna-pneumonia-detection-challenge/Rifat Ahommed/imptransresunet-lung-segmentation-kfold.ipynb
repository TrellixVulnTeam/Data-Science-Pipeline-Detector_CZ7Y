{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"CFVTA-wLHmm_","trusted":true},"cell_type":"code","source":"!mkdir ../input_c\n!mkdir ../input_c/segmentation\n!mkdir ../input_c/segmentation/test\n!mkdir ../input_c/segmentation/train\n!mkdir ../input_c/segmentation/train/augmentation\n!mkdir ../input_c/segmentation/train/image\n!mkdir ../input_c/segmentation/train/mask\n!mkdir ../input_c/segmentation/train/dilate","execution_count":null,"outputs":[]},{"metadata":{"id":"m_1aEoZdHmnW","outputId":"4dcecd2d-e041-4db0-b3d5-1fa6d205ead2","trusted":true},"cell_type":"code","source":"!ls ../input_c/segmentation","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"hM2RJRjaHmn3","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom skimage.color import rgb2gray\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model, load_model, save_model\nfrom tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom glob import glob\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"id":"FP19bB2yHmoL","trusted":true},"cell_type":"code","source":"INPUT_C_DIR = os.path.join(\"..\", \"input_c\")\nINPUT_DIR = os.path.join(\"..\", \"input\")\n\nSEGMENTATION_DIR = os.path.join(INPUT_C_DIR, \"segmentation\")\nSEGMENTATION_TEST_DIR = os.path.join(SEGMENTATION_DIR, \"test\")\nSEGMENTATION_TRAIN_DIR = os.path.join(SEGMENTATION_DIR, \"train\")\nSEGMENTATION_AUG_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"augmentation\")\nSEGMENTATION_IMAGE_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"image\")\nSEGMENTATION_MASK_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"mask\")\nSEGMENTATION_DILATE_DIR = os.path.join(SEGMENTATION_TRAIN_DIR, \"dilate\")\nSEGMENTATION_SOURCE_DIR = os.path.join(INPUT_DIR, \\\n                                       \"pulmonary-chest-xray-abnormalities\")\n\nSHENZHEN_TRAIN_DIR = os.path.join(SEGMENTATION_SOURCE_DIR, \"ChinaSet_AllFiles\", \\\n                                  \"ChinaSet_AllFiles\")\nSHENZHEN_IMAGE_DIR = os.path.join(SHENZHEN_TRAIN_DIR, \"CXR_png\")\nSHENZHEN_MASK_DIR = os.path.join(INPUT_DIR, \"shcxr-lung-mask\", \"mask\", \"mask\")\n\nMONTGOMERY_TRAIN_DIR = os.path.join(SEGMENTATION_SOURCE_DIR, \\\n                                    \"Montgomery\", \"MontgomerySet\")\nMONTGOMERY_IMAGE_DIR = os.path.join(MONTGOMERY_TRAIN_DIR, \"CXR_png\")\nMONTGOMERY_LEFT_MASK_DIR = os.path.join(MONTGOMERY_TRAIN_DIR, \\\n                                        \"ManualMask\", \"leftMask\")\nMONTGOMERY_RIGHT_MASK_DIR = os.path.join(MONTGOMERY_TRAIN_DIR, \\\n                                         \"ManualMask\", \"rightMask\")\n\nDILATE_KERNEL = np.ones((15, 15), np.uint8)\n\nBATCH_SIZE=2\n\n#Prod\nEPOCHS=35\n\n#Desv\n#EPOCHS=16","execution_count":null,"outputs":[]},{"metadata":{"id":"2KdJW-hKHmog","outputId":"9aa942db-c857-4b06-b5c2-ace854d8194b","trusted":true},"cell_type":"code","source":"print(MONTGOMERY_LEFT_MASK_DIR)\n!ls MONTGOMERY_LEFT_MASK_DIR","execution_count":null,"outputs":[]},{"metadata":{"id":"I8GTKQFeHmo1","outputId":"6df7e318-646e-4c0a-e3bd-d1b464fbfeb3","trusted":true},"cell_type":"code","source":"montgomery_left_mask_dir = glob(os.path.join(MONTGOMERY_LEFT_MASK_DIR, '*.png'))\n#montgomery_test = montgomery_left_mask_dir[0:50]\nmontgomery_train= montgomery_left_mask_dir[:]\n\nfor left_image_file in tqdm(montgomery_left_mask_dir):\n    base_file = os.path.basename(left_image_file)\n    image_file = os.path.join(MONTGOMERY_IMAGE_DIR, base_file)\n    right_image_file = os.path.join(MONTGOMERY_RIGHT_MASK_DIR, base_file)\n\n    image = cv2.imread(image_file)\n    left_mask = cv2.imread(left_image_file, cv2.IMREAD_GRAYSCALE)\n    right_mask = cv2.imread(right_image_file, cv2.IMREAD_GRAYSCALE)\n    \n    image = cv2.resize(image, (512, 512))\n    left_mask = cv2.resize(left_mask, (512, 512))\n    right_mask = cv2.resize(right_mask, (512, 512))\n    \n    mask = np.maximum(left_mask, right_mask)\n    mask_dilate = cv2.dilate(mask, DILATE_KERNEL, iterations=1)\n    \n    if (left_image_file in montgomery_train):\n        cv2.imwrite(os.path.join(SEGMENTATION_IMAGE_DIR, base_file), \\\n                    image)\n        cv2.imwrite(os.path.join(SEGMENTATION_MASK_DIR, base_file), \\\n                    mask)\n        cv2.imwrite(os.path.join(SEGMENTATION_DILATE_DIR, base_file), \\\n                    mask_dilate)","execution_count":null,"outputs":[]},{"metadata":{"id":"n-WSOhPDHmpG","trusted":true},"cell_type":"code","source":"def add_colored_dilate(image, mask_image, dilate_image):\n    mask_image_gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)\n    dilate_image_gray = cv2.cvtColor(dilate_image, cv2.COLOR_BGR2GRAY)\n    \n    mask = cv2.bitwise_and(mask_image, mask_image, mask=mask_image_gray)\n    dilate = cv2.bitwise_and(dilate_image, dilate_image, mask=dilate_image_gray)\n    \n    mask_coord = np.where(mask!=[0,0,0])\n    dilate_coord = np.where(dilate!=[0,0,0])\n\n    mask[mask_coord[0],mask_coord[1],:]=[255,0,0]\n    dilate[dilate_coord[0],dilate_coord[1],:] = [0,0,255]\n\n    ret = cv2.addWeighted(image, 0.7, dilate, 0.3, 0)\n    ret = cv2.addWeighted(ret, 0.7, mask, 0.3, 0)\n\n    return ret\n\ndef add_colored_mask(image, mask_image):\n    mask_image_gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)\n                                        \n    mask = cv2.bitwise_and(mask_image, mask_image, mask=mask_image_gray)\n    \n    mask_coord = np.where(mask!=[0,0,0])\n\n    mask[mask_coord[0],mask_coord[1],:]=[255,0,0]\n\n    ret = cv2.addWeighted(image, 0.7, mask, 0.3, 0)\n\n    return ret\n\ndef diff_mask(ref_image, mask_image):\n    mask_image_gray = cv2.cvtColor(mask_image, cv2.COLOR_BGR2GRAY)\n    \n    mask = cv2.bitwise_and(mask_image, mask_image, mask=mask_image_gray)\n    \n    mask_coord = np.where(mask!=[0,0,0])\n\n    mask[mask_coord[0],mask_coord[1],:]=[255,0,0]\n\n    ret = cv2.addWeighted(ref_image, 0.7, mask, 0.3, 0)\n    return ret","execution_count":null,"outputs":[]},{"metadata":{"id":"p3j2aG03HmpS","outputId":"7130f807-cbbd-4f1b-cb91-2347a58fa2f9","trusted":true},"cell_type":"code","source":"shenzhen_mask_dir = glob(os.path.join(SHENZHEN_MASK_DIR, '*.png'))\n#shenzhen_test = shenzhen_mask_dir[0:50]\nshenzhen_train= shenzhen_mask_dir[:]\n\nfor mask_file in tqdm(shenzhen_mask_dir):\n    base_file = os.path.basename(mask_file).replace(\"_mask\", \"\")\n    image_file = os.path.join(SHENZHEN_IMAGE_DIR, base_file)\n\n    image = cv2.imread(image_file)\n    mask = cv2.imread(mask_file, cv2.IMREAD_GRAYSCALE)\n        \n    image = cv2.resize(image, (512, 512))\n    mask = cv2.resize(mask, (512, 512))\n    mask_dilate = cv2.dilate(mask, DILATE_KERNEL, iterations=1)\n    \n    if (mask_file in shenzhen_train):\n        cv2.imwrite(os.path.join(SEGMENTATION_IMAGE_DIR, base_file), \\\n                    image)\n        cv2.imwrite(os.path.join(SEGMENTATION_MASK_DIR, base_file), \\\n                    mask)\n        cv2.imwrite(os.path.join(SEGMENTATION_DILATE_DIR, base_file), \\\n                    mask_dilate)","execution_count":null,"outputs":[]},{"metadata":{"id":"_o6rO-alHmpe","outputId":"0e02ea35-2a1a-4ab4-d9fd-5980cd6593b9","trusted":true},"cell_type":"code","source":"train_files = glob(os.path.join(SEGMENTATION_IMAGE_DIR, \"*.png\"))\n#test_files = glob(os.path.join(SEGMENTATION_TEST_DIR, \"*.png\"))\nmask_files = glob(os.path.join(SEGMENTATION_MASK_DIR, \"*.png\"))\ndilate_files = glob(os.path.join(SEGMENTATION_DILATE_DIR, \"*.png\"))\n\n(len(train_files), \\\n len(mask_files), \\\n len(dilate_files))","execution_count":null,"outputs":[]},{"metadata":{"id":"Yfw6LSelHmpr","trusted":true},"cell_type":"code","source":"# From: https://github.com/zhixuhao/unet/blob/master/data.py\nfrom tensorflow.keras.applications.vgg16 import preprocess_input \n\ndef train_generator(data_frame, batch_size, train_path, aug_dict,\n        image_color_mode=\"rgb\",\n        mask_color_mode=\"grayscale\",\n        image_save_prefix=\"image\",\n        mask_save_prefix=\"mask\",\n        save_to_dir=None,\n        target_size=(256,256),\n        seed=1):\n    '''\n    can generate image and mask at the same time use the same seed for\n    image_datagen and mask_datagen to ensure the transformation for image\n    and mask is the same if you want to visualize the results of generator,\n    set save_to_dir = \"your path\"\n    '''\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow_from_dataframe(\n        data_frame,\n        directory = train_path,\n        x_col = \"filename\",\n        class_mode = None,\n        color_mode = image_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = image_save_prefix,\n        seed = seed)\n\n    mask_generator = mask_datagen.flow_from_dataframe(\n        data_frame,\n        directory = train_path,\n        x_col = \"dilate\",\n        class_mode = None,\n        color_mode = mask_color_mode,\n        target_size = target_size,\n        batch_size = batch_size,\n        save_to_dir = save_to_dir,\n        save_prefix  = mask_save_prefix,\n        seed = seed)\n\n    train_gen = zip(image_generator, mask_generator)\n    \n    for (img, mask) in train_gen:\n        img, mask = adjust_data(img, mask)\n        yield (img,mask)\n\ndef adjust_data(img,mask):\n    img = preprocess_input(img)\n    mask = mask / 255\n    mask[mask > 0.5] = 1\n    mask[mask <= 0.5] = 0\n    \n    return (img, mask)","execution_count":null,"outputs":[]},{"metadata":{"id":"kmEp8m3AHmp5","trusted":true},"cell_type":"code","source":"# From: https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\ndef iou(y_true, y_pred):\n    intersection = K.sum(K.abs(y_true * y_pred))\n    sum_ = K.sum(K.square(y_true)) + K.sum(K.square(y_pred))\n    jac = (intersection) / (sum_ - intersection)\n    return jac","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import *\n\ndef expend_as(tensor, rep):\n     return Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n                    arguments={'repnum': rep})(tensor)\n\ndef gating_signal(inputs, out_size, batch_norm=False):\n    \"\"\"\n    resize the down layer feature map into the same dimension as the up layer feature map\n    using 1x1 conv\n    :param input:   down-dim feature map\n    :param out_size:output channel number\n    :return: the gating feature map with the same dimension of the up layer feature map\n    \"\"\"\n    x = Conv2D(out_size, (1, 1), padding='same')(inputs)\n    if batch_norm:\n        x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    return x\n\ndef attention_block(x, gating, inter_shape):\n    shape_x = K.int_shape(x)\n    shape_g = K.int_shape(gating)\n\n    theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n    shape_theta_x = K.int_shape(theta_x)\n\n    phi_g = Conv2D(inter_shape, (1, 1), padding='same')(gating)\n    upsample_g = Conv2DTranspose(inter_shape, (3, 3),\n                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n                                 padding='same')(phi_g)  # 16\n\n    concat_xg = add([upsample_g, theta_x])\n    act_xg = Activation('relu')(concat_xg)\n    psi = Conv2D(1, (1, 1), padding='same')(act_xg)\n    sigmoid_xg = Activation('sigmoid')(psi)\n    shape_sigmoid = K.int_shape(sigmoid_xg)\n    upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n\n    upsample_psi = expend_as(upsample_psi, shape_x[3])\n\n    y = multiply([upsample_psi, x])\n\n    result = Conv2D(shape_x[3], (1, 1), padding='same')(y)\n    result_bn = BatchNormalization()(result)\n    return result_bn","execution_count":null,"outputs":[]},{"metadata":{"id":"vW7yYbMwHmqH","trusted":true},"cell_type":"code","source":"from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.layers import *\n\ndef res_block(inputs,filter_size):\n    \"\"\"\n    res_block -- Residual block for building res path\n    \n    Arguments:\n    inputs {<class 'tensorflow.python.framework.ops.Tensor'>} -- input for residual block\n    filter_size {int} -- convolutional filter size \n    \n    Returns:\n    add {<class 'tensorflow.python.framework.ops.Tensor'>} -- addition of two convolutional filter output  \n    \"\"\"\n    # First Conv2D layer\n    cb1 = Conv2D(filter_size,(3,3),padding = 'same',activation=\"relu\")(inputs)\n    # Second Conv2D layer parallel to the first one\n    cb2 = Conv2D(filter_size,(1,1),padding = 'same',activation=\"relu\")(inputs)\n    # Addition of cb1 and cb2\n    add = Add()([cb1,cb2])\n    \n    return add\n\ndef res_path(inputs,filter_size,path_number):\n    \"\"\"\n    res_path -- residual path / modified skip connection\n    \n    Arguments:\n    inputs {<class 'tensorflow.python.framework.ops.Tensor'>} -- input for res path\n    filter_size {int} -- convolutional filter size \n    path_number {int} -- path identifier \n    \n    Returns:\n    skip_connection {<class 'tensorflow.python.framework.ops.Tensor'>} -- final res path\n    \"\"\"\n    # Minimum one residual block for every res path\n    skip_connection = res_block(inputs, filter_size)\n\n    # Two serial residual blocks for res path 2\n    if path_number == 2:\n        skip_connection = res_block(skip_connection,filter_size)\n    \n    # Three serial residual blocks for res path 1\n    elif path_number == 1:\n        skip_connection = res_block(skip_connection,filter_size)\n        skip_connection = res_block(skip_connection,filter_size)\n    \n    return skip_connection\n\ndef decoder_block(inputs, res, out_channels, depth):\n    \n    \"\"\"\n    decoder_block -- decoder block formation\n    \n    Arguments:\n    inputs {<class 'tensorflow.python.framework.ops.Tensor'>} -- input for decoder block\n    mid_channels {int} -- no. of mid channels \n    out_channels {int} -- no. of out channels\n    \n    Returns:\n    db {<class 'tensorflow.python.framework.ops.Tensor'>} -- returning the decoder block\n    \"\"\"\n    conv_kwargs = dict(\n        activation='relu',\n        padding='same',\n        kernel_initializer='he_normal',\n        data_format='channels_last'  \n    )\n    \n    # attention block\n    gating = gating_signal(inputs, out_channels)\n    att = attention_block(res, gating, out_channels)\n    \n    # UpConvolutional layer\n    db = Conv2DTranspose(out_channels, (2, 2), strides=(2, 2))(inputs)\n    db = concatenate([db, att], axis=3)\n    # First conv2D layer \n    db = Conv2D(out_channels, 3, **conv_kwargs)(db)\n    # Second conv2D layer\n    db = Conv2D(out_channels, 3, **conv_kwargs)(db)\n\n    if depth > 2:\n        # Third conv2D layer\n        db = Conv2D(out_channels, 3, **conv_kwargs)(db)\n\n    return db\n\ndef TransResUNet(input_size=(512, 512, 1)):\n    \"\"\"\n    TransResUNet -- main architecture of TransResUNet\n    \n    Arguments:\n    input_size {tuple} -- size of input image\n    \n    Returns:\n    model {<class 'tensorflow.python.keras.engine.training.Model'>} -- final model\n    \"\"\"\n    \n    # Input \n    inputs = Input(input_size)\n    \n    # Handling input channels \n    # input with 1 channel will be converted to 3 channels to be compatible with VGG16 pretrained encoder \n#     if input_size[-1] < 3:\n#         inp = Conv2D(3, 1)(inputs)                         \n#         input_shape = (input_size[0], input_size[0], 3)  \n#     else:\n#         inp = inputs\n#         input_shape = input_size\n\n    # VGG16 with imagenet weights\n    encoder = VGG16(include_top=False, weights='imagenet', input_shape=input_size)\n       \n    # First encoder block\n    enc1 = encoder.get_layer(name='block1_conv1')(inputs)\n    enc1 = encoder.get_layer(name='block1_conv2')(enc1)\n    enc2 = MaxPooling2D(pool_size=(2, 2))(enc1)\n    \n    # Second encoder block\n    enc2 = encoder.get_layer(name='block2_conv1')(enc2)\n    enc2 = encoder.get_layer(name='block2_conv2')(enc2)\n    enc3 = MaxPooling2D(pool_size=(2, 2))(enc2)\n    \n    # Third encoder block\n    enc3 = encoder.get_layer(name='block3_conv1')(enc3)\n    enc3 = encoder.get_layer(name='block3_conv2')(enc3)\n    enc3 = encoder.get_layer(name='block3_conv3')(enc3)\n    center = MaxPooling2D(pool_size=(2, 2))(enc3)\n\n    # Center block\n    center = Conv2D(512, (3, 3), activation='relu', padding='same')(center)\n    center = Conv2D(512, (3, 3), activation='relu', padding='same')(center)\n\n    # Decoder block corresponding to third encoder\n    res_path3 = res_path(enc3,128,3)\n    dec3 = decoder_block(center, res_path3, 256, 3)\n    \n    # Decoder block corresponding to second encoder\n    res_path2 = res_path(enc2,64,2)\n    dec2 = decoder_block(dec3, res_path2, 128, 2)\n    \n    # Final Block concatenation with first encoded feature \n    res_path1 = res_path(enc1,32,1)\n    dec1 = decoder_block(dec2, res_path1, 64, 1)\n\n    # Output\n    out = Conv2D(1, 1)(dec1)\n    out = Activation('sigmoid')(out)  \n    \n    # Final model\n    model = Model(inputs=[inputs], outputs=[out])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"iMnzlFs-HmqX","trusted":true},"cell_type":"code","source":"import pandas\nfrom sklearn.model_selection import KFold\n\ndf = pandas.DataFrame(data={\"filename\": train_files, 'mask' : mask_files, 'dilate' : dilate_files})\n\nkf = KFold(n_splits = 5, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"IsgDvg0NHmqm","outputId":"060c9dfd-64d7-4f52-e71b-e4c73fc4e8de","trusted":true},"cell_type":"code","source":"train_generator_args = dict(rotation_range=0.2,\n                            width_shift_range=0.05,\n                            height_shift_range=0.05,\n                            shear_range=0.05,\n                            zoom_range=0.05,\n                            horizontal_flip=True,\n                            fill_mode='nearest')\n\nhistories = []\nlosses = []\naccuracies = []\ndicecoefs = []\nious = []\n\nfor k, (train_index, test_index) in enumerate(kf.split(df)):\n    train_data_frame = df.iloc[train_index]\n    test_data_frame = df.iloc[test_index]\n    \n    train_gen = train_generator(train_data_frame, BATCH_SIZE,\n                                None,\n                                train_generator_args,\n                                target_size=(512,512))\n\n    test_gener = train_generator(test_data_frame, BATCH_SIZE,\n                                None,\n                                train_generator_args,\n                                target_size=(512,512))\n\n    model = TransResUNet(input_size=(512,512,3))\n    model.compile(optimizer=Adam(lr=1e-5), loss=dice_coef_loss, \\\n                      metrics=[iou, dice_coef, 'binary_accuracy'])\n#    model.summary()\n\n    model_checkpoint = ModelCheckpoint(str(k+1) + '_unet_lung_seg.hdf5', \n                                       monitor='loss', \n                                       verbose=1, \n                                       save_best_only=True)\n\n    history = model.fit(train_gen,\n                                  steps_per_epoch=len(train_data_frame) / BATCH_SIZE, \n                                  epochs=32, \n                                  callbacks=[model_checkpoint],\n                                  validation_data = test_gener,\n                                  validation_steps=len(test_data_frame) / BATCH_SIZE)\n    \n    model = load_model(str(k+1) + '_unet_lung_seg.hdf5', custom_objects={'dice_coef_loss': dice_coef_loss, 'iou': iou, 'dice_coef': dice_coef})\n    \n    #test_gen = test_generator(test_files, target_size=(512,512))\n    test_gen = train_generator(test_data_frame, BATCH_SIZE,\n                                None,\n                                train_generator_args,\n                                target_size=(512,512))\n    results = model.evaluate(test_gen, steps=len(test_data_frame))\n    results = dict(zip(model.metrics_names,results))\n    \n    histories.append(history)\n    accuracies.append(results['binary_accuracy'])\n    losses.append(results['loss'])\n    dicecoefs.append(results['dice_coef'])\n    ious.append(results['iou'])\n","execution_count":null,"outputs":[]},{"metadata":{"id":"2CJVybJ4Hmqx","outputId":"a82a0b3b-0997-4e58-cd8b-ceeee68d74fb","trusted":false},"cell_type":"code","source":"print('accuracies : ', accuracies)\nprint('losses : ', losses)\nprint('dicecoefs : ', dicecoefs)\nprint('ious : ', ious)\n\nprint('-----------------------------------------------------------------------------')\nprint('-----------------------------------------------------------------------------')\n\nprint('average accuracy : ', np.mean(np.array(accuracies)))\nprint('average loss : ', np.mean(np.array(losses)))\nprint('average dicecoefs : ', np.mean(np.array(dicecoefs)))\nprint('average ious : ', np.mean(np.array(ious)))\nprint()\n\nprint('standard deviation of accuracy : ', np.std(np.array(accuracies)))\nprint('standard deviation of loss : ', np.std(np.array(losses)))\nprint('standard deviation of dicecoefs : ', np.std(np.array(dicecoefs)))\nprint('standard deviation of ious : ', np.std(np.array(ious)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"CMz25SLxHmq8","outputId":"8bacade1-ae58-46c0-bdb7-a3110f33004c","trusted":false},"cell_type":"code","source":"import pickle\n\nfor h, history in enumerate(histories):\n\n    keys = history.history.keys()\n    fig, axs = plt.subplots(1, len(keys)//2, figsize = (25, 5))\n    fig.suptitle('No. ' + str(h+1) + ' Fold Results', fontsize=30)\n\n    for k, key in enumerate(list(keys)[:len(keys)//2]):\n        training = history.history[key]\n        validation = history.history['val_' + key]\n\n        epoch_count = range(1, len(training) + 1)\n\n        axs[k].plot(epoch_count, training, 'r--')\n        axs[k].plot(epoch_count, validation, 'b-')\n        axs[k].legend(['Training ' + key, 'Validation ' + key])\n        \n    with open(str(h+1) + '_lungs_trainHistoryDict', 'wb') as file_pi:\n        pickle.dump(history.history, file_pi)","execution_count":null,"outputs":[]},{"metadata":{"id":"lT2X8MIMHmrH","trusted":false},"cell_type":"code","source":"model = load_model('1_unet_lung_seg.hdf5', custom_objects={'dice_coef_loss': dice_coef_loss, 'iou': iou, 'dice_coef': dice_coef})","execution_count":null,"outputs":[]},{"metadata":{"id":"oSBZ2CImHmrQ","outputId":"210193a4-5d5b-42ad-ea5b-0c555e1f7a97","trusted":false},"cell_type":"code","source":"for i in range(10):\n    index=np.random.randint(1,len(train_files))\n    img = cv2.imread(train_files[index])\n    img = cv2.resize(img, (512,512))\n    img = preprocess_input(img)\n    img=img[np.newaxis, :, :, :]\n    pred = model.predict(img)\n\n    plt.figure(figsize=(12,12))\n    plt.subplot(1,3,1)\n    plt.imshow(cv2.imread(train_files[index]))\n    plt.title('Original Image')\n    plt.subplot(1,3,2)\n    plt.imshow(np.squeeze(cv2.imread(dilate_files[index])))\n    plt.title('Original Mask')\n    plt.subplot(1,3,3)\n    plt.imshow(np.squeeze(pred) > .5)\n    plt.title('Prediction')\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}