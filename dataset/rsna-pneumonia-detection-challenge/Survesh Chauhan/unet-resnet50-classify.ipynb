{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nfrom matplotlib import pyplot\nimport matplotlib.patches as patches\nfrom collections import Counter\nfrom keras.preprocessing.image import ImageDataGenerator\nimport pydicom as dcm\nfrom pydicom import dcmread\nimport glob \nimport pylab\nimport seaborn as sns\n# !pip install -q pydicom\n\n# After installing pydicom. This is needed to load .dcm files\nimport pydicom","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Setting up project path**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load train set image metadata\ndataDirPath = '../input/rsna-pneumonia-detection-challenge/'\n\nTrain_Image_path = dataDirPath + 'stage_2_train_images'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Creating a function to load the metadata from images****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImageMetadata():\n    def __init__(self, setName, file):\n        # print(name, file)\n        # dataset name(train or test)\n        self.setName = setName\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.imagePath()\n\n    def imagePath(self):\n        return os.path.join(self.setName, self.file) \n    \n\n# function to load image metadada   \ndef loadImageMetadata(dataSetName):\n    imageMetadata = []\n    for f in os.listdir(dataSetName):\n        # Check file extension. Allow only .dcm files.\n        ext = os.path.splitext(f)[1]\n        if ext == '.dcm' :\n            imageMetadata.append(ImageMetadata(dataSetName, f))\n    return np.array(imageMetadata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Getting Metadata Information**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainSetImageMetadata = loadImageMetadata(dataDirPath + 'stage_2_train_images')\n\nprint(\"trainSetImageMetadata.shape : \", trainSetImageMetadata.shape)\n\nprint(\"Sample image path : \", trainSetImageMetadata[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"******Insights from Metadata******\n\nThere are 26684 images in the Training data\nThese are DICOM Images which has pixel information as well as several tags added to it like patientid, age,gender etc.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Create a function to load image and loaging a sample Image**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadImage(path):\n    img = pydicom.dcmread(path)\n    return img\n\nimgIndex = 4\nimgPath = trainSetImageMetadata[imgIndex]\nimgPath = imgPath.imagePath()\nimgData = loadImage(imgPath)\n\npyplot.imshow(imgData.pixel_array, cmap=pyplot.cm.bone)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preparing Dataset with patient id and respective image paths**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainSetImageMetadata_df = pd.DataFrame(trainSetImageMetadata, columns=[\"Path\"])\ntrainSetImageMetadata_df.head(2)\n\ndef getImgId(_imgData) :\n    return str(_imgData).split(\".dcm\")[0].split(\"/\")[4]\n\nimageIdPaths = pd.DataFrame(columns=[\"patientId\", \"imgPath\"])\nimageIdPaths[\"patientId\"] = trainSetImageMetadata_df[\"Path\"].apply(getImgId)\nimageIdPaths[\"imgPath\"] = trainSetImageMetadata_df[\"Path\"]\n\nprint(\"imageIdPaths\", imageIdPaths.shape)\nimageIdPaths.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exloratory Data Analysis on Train Labels and Detail Info CSV data sets","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Analyzing Detailed Classes CSV file**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Step 1 -> Define and read the Detail_Info CSV File ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classesPath =  dataDirPath + 'stage_2_detailed_class_info.csv'\n\ndetailedClasses = pd.read_csv(classesPath)\n\ndetailedClasses.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 2 -> Check for Missing Values\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"detailedClasses.isna().apply(pd.value_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No Missing Values found","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Step 3 -> Checking the shape of data frame","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"detailedClasses.shape : \", detailedClasses.shape, )\n\n# File has 30227 rows and 2 columns - PatientID & Class","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 3 -> Checking Unique Patients","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique patientIds : \", detailedClasses['patientId'].nunique(), )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Total number of unique patients in data - 26684\n\n**Observation - As we have total 30227 records and out of that 26684 are unique records, this shows presence of multiple records for some patients**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Step 4 -> Checking unique Classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique patientIds : \", detailedClasses['class'].nunique(), )\n\nprint(detailedClasses['class'].unique)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 Unique classes observed \n1 - No Lung Opacity/Not Normal,\n2 - Normal,\n3 - Lung Opacity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"class\",hue=\"class\",data=detailedClasses)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analyzing Train Lables Dataset**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Step 1 -> Reading the data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"labelsPath = dataDirPath + 'stage_2_train_labels.csv'\n\ntrainLabels = pd.read_csv(labelsPath)\n\ntrainLabels.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 2 -> Checking the missing values if any","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainLabels.isna().apply(pd.value_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Observation -For around 20672 patients Bounding box cordinates not available where as for 9555 patients its avaialable**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainLabels[trainLabels['Target']==0].head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Few records have observed with missing values in x,y,width and height coulmn, but no missing values observed in patientid and Target.\n\nAlso this is observed such missing columns are present for those records with Target as '0'.\n\nx,y,width and height columns have the information for bounding boxes in Images where Penumonia is detected.\n\n**Explaination on missing values - These are not the missing values instead it is expected not to have Bounding Box co-ordinates for those images where Pneumonia is not detected (Target - '0')**\n\nHence concluding there are no missing values in this dataset as well","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Step 3 -> Checking unique Patients","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique patientIds : \", trainLabels['patientId'].nunique(), )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Total Unique patients found - 26684\nThis is same as the number of patients in Detailed CSV sheet hence both sheets share the information for same patients","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Step 4 -> Checkin unqiue Targets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique patientIds : \", trainLabels['Target'].nunique(), )\n\nprint(\"Unique patientIds : \", trainLabels['Target'].unique(), )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trail label has only 2 target variables [0 & 1] \n\n**Conclusion - In Train labels only two target variables are present 0 & 1, where as in Detailed_Info sheet we have 3 classes.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**As we have 3 Classes in Detailed_Info dataset and 2 Target Variables in Train_Labels, concatenating to get better insight into the data**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Step 1 -> Sorting both the datasets based on patientId","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"trainLabels.sort_values(\"patientId\", inplace=True)\ndetailedClasses.sort_values(\"patientId\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 2 -> Concatenating the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Combined_Data = pd.concat([trainLabels, detailedClasses[\"class\"]], axis=1, sort=False)\nCombined_Data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Validating the concatenation results**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Combined_Data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Combined_Data.isna().apply(pd.value_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Combined_Data[Combined_Data[\"Target\"] == 1].isna().apply(pd.value_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Combined_Data[Combined_Data[\"Target\"] == 0].isna().apply(pd.value_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Combined_Data[Combined_Data[\"class\"] == \"Lung Opacity\"].isna().apply(pd.value_counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From Above Analysis our concatenated data is correct","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Prepare data for training\n\n\nStep 1 -> \n* Conver data to only two classes, 'Normal' and 'Lung Opacity'\n* Splitting the data in three parts, train, test and validation sets.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Conver data to only two classes, 'Normal' and 'Lung Opacity'\nCombined_Data[\"class\"].replace(\"No Lung Opacity / Not Normal\", \"Normal\", inplace=True)\nCombined_Data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imageIdPaths.sort_values(\"patientId\", inplace=True)\ntrain_CombinedData = Combined_Data[0:25000]\nvalidate_CombinedData = Combined_Data[25000:30227]\n\nprint(\"train_CombinedData.shape : \", train_CombinedData.shape)\nprint(\"validate_CombinedData.shape : \", validate_CombinedData.shape)\n\nprint(\"\\nunique train patients : \", train_CombinedData[\"patientId\"].nunique())\nprint(\"unique validate patients : \", validate_CombinedData[\"patientId\"].nunique())\n\nprint(\"\\nTotal unique patients : \", imageIdPaths[\"patientId\"].nunique())\nprint(\"Total of unique train and test : \", train_CombinedData[\"patientId\"].nunique() + validate_CombinedData[\"patientId\"].nunique())\n\nprint(\"\\nLast from train set : \", train_CombinedData.iloc[24999][\"patientId\"])\nprint(\"First from validate set : \", validate_CombinedData.iloc[0][\"patientId\"])\n\n# Set all NaN values to 0 in train and test data sets. While training NaN will not have any meaning.\n#    * x, y, width and hight values as zero(0) means no bounding box.\ntrain_CombinedData.fillna(0, inplace=True)\nvalidate_CombinedData.fillna(0, inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_imageIdPaths = imageIdPaths[0:21764]\nvalidate_imageIdPaths = imageIdPaths[21764:26684]\n\nprint(\"train_imageIdPaths.shape : \", train_imageIdPaths.shape)\nprint(\"validate_imageIdPaths.shape : \", validate_imageIdPaths.shape)\n\nprint(\"\\nunique train patients : \", train_imageIdPaths[\"patientId\"].nunique())\nprint(\"unique validate patients : \", validate_imageIdPaths[\"patientId\"].nunique())\n\nprint(\"\\nTotal unique patients : \", imageIdPaths[\"patientId\"].nunique())\nprint(\"Total of unique train and test : \", train_imageIdPaths[\"patientId\"].nunique() + validate_imageIdPaths[\"patientId\"].nunique())\n\nprint(\"\\nLast from train set : \", train_imageIdPaths.iloc[21763][\"patientId\"])\nprint(\"First from validate set : \", validate_imageIdPaths.iloc[0][\"patientId\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wer = wer r","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build UNet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Layer, Convolution2D, Flatten, Dense\nfrom tensorflow.keras.layers import Concatenate, UpSampling2D, Conv2D, Reshape, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\n\nimport cv2\n\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.applications.mobilenet import preprocess_input \n\nimport tensorflow.keras.utils as pltUtil\nfrom tensorflow.keras.utils import Sequence\n\nimport math\n\nfrom tensorflow.keras.applications.resnet import ResNet50\nfrom tensorflow.keras.applications.resnet import preprocess_input as resnetProcess_input\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SIZE = 224\n\n\nIMG_WIDTH = 1024\nIMG_HEIGHT = 1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 10\n\nclass UNetTrainGenerator(Sequence):\n\n    def __init__(self, _imageIdPaths, _CombinedData):       \n        self.pids = _CombinedData[\"patientId\"].to_numpy()\n        self.imgIdPaths = _imageIdPaths\n        self.coords = _CombinedData[[\"x\", \"y\", \"width\", \"height\"]].to_numpy()\n        # Resize Bounding box\n        self.coords = self.coords * IMAGE_SIZE / IMG_WIDTH\n        \n\n    def __len__(self):\n        return math.ceil(len(self.coords) / BATCH_SIZE)\n    \n\n    def __getitem__(self, idx): # Get a batch\n        batch_coords = self.coords[idx * BATCH_SIZE:(idx + 1) * BATCH_SIZE] # Image coords\n        batch_pids = self.pids[idx * BATCH_SIZE:(idx + 1) * BATCH_SIZE] # Image pids    \n        \n        batch_images = np.zeros((len(batch_pids), IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.float32)\n        batch_masks = np.zeros((len(batch_pids), IMAGE_SIZE, IMAGE_SIZE))\n        for _indx, _pid in enumerate(batch_pids):\n            _path = self.imgIdPaths[self.imgIdPaths[\"patientId\"] == _pid][\"imgPath\"].array[0]\n            _imgData = loadImage(str(_path)) # Read image\n            img = _imgData.pixel_array \n            \n            # Resize image\n            resized_img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\n            #resized_img = cv2.resize(img[200:824, 200:824], dsize=(IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)\n    \n            #print(\"batch_images[_indx] shape :\", batch_images[_indx][:,:,0].shape)\n            # preprocess image for the batch\n            batch_images[_indx][:,:,0] = preprocess_input(np.array(resized_img[:,:], dtype=np.float32)) # Convert to float32 array\n            batch_images[_indx][:,:,1] = preprocess_input(np.array(resized_img[:,:], dtype=np.float32)) # Convert to float32 array\n            batch_images[_indx][:,:,2] = preprocess_input(np.array(resized_img[:,:], dtype=np.float32)) # Convert to float32 array  \n            \n            x = int(batch_coords[_indx, 0])\n            y = int(batch_coords[_indx, 1])\n            width = int(batch_coords[_indx, 2])\n            height = int(batch_coords[_indx, 3])\n            \n            batch_masks[_indx][y:y+height, x:x+width] = 1\n\n        return batch_images, batch_masks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainUNetDataGen = UNetTrainGenerator(train_imageIdPaths, train_CombinedData)\nvalidateUNetDataGen = UNetTrainGenerator(validate_imageIdPaths, validate_CombinedData)\n\nprint(len(trainUNetDataGen), \"# of iterations in one train epoch\")\nprint(len(validateUNetDataGen), \"# of iterations in one validate epoch\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To show image with mask\ndef showMaskedImage(_imageSet, _maskSet, _index) :\n    maskImage = _imageSet[_index]\n\n    #pyplot.imshow(maskImage[:,:,0], cmap=pyplot.cm.bone)\n    maskImage[:,:,0] = _maskSet[_index] * _imageSet[_index][:,:,0]\n    maskImage[:,:,1] = _maskSet[_index] * _imageSet[_index][:,:,1]\n    maskImage[:,:,2] = _maskSet[_index] * _imageSet[_index][:,:,2]\n\n    pyplot.imshow(maskImage[:,:,0], cmap=pyplot.cm.bone)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imageSet0 = trainUNetDataGen[0][0]\nmaskSet0 = trainUNetDataGen[0][1]    \nshowMaskedImage(imageSet0, maskSet0, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nALPHA = 1.0\n\ndef create_UNetModel(trainable=True):\n    model = MobileNet(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False, alpha=ALPHA, weights=\"imagenet\") \n    # Top layer is last layer of the model\n\n    for layer in model.layers:\n        layer.trainable = trainable\n\n    # Add all the UNET layers here\n    #### Add your code here ####\n    convLayer_112by112 = model.get_layer(\"conv_pw_1_relu\").output\n    convLayer_56by56 = model.get_layer(\"conv_pw_3_relu\").output\n    convLayer_28by28 = model.get_layer(\"conv_pw_5_relu\").output\n    convLayer_14by14 = model.get_layer(\"conv_pw_11_relu\").output\n    convLayer_7by7 = model.get_layer(\"conv_pw_13_relu\").output\n    # The last layer of mobilenet model is of dimensions (7x7x1024)\n\n    # Start upsampling from 7x7 to 14x14 ...up to 224x224 to form UNET\n    # concatinate with the original image layer of the same size from MobileNet\n    x = Concatenate()([UpSampling2D()(convLayer_7by7), convLayer_14by14])\n    x = Concatenate()([UpSampling2D()(x), convLayer_28by28])\n    x = Concatenate()([UpSampling2D()(x), convLayer_56by56])\n    x = Concatenate()([UpSampling2D()(x), convLayer_112by112])\n    x = UpSampling2D()(x) # upsample to 224x224\n\n    # Add classification layer\n    x = Conv2D(1, kernel_size=1, activation=\"sigmoid\", name=\"masks\")(x)\n    x = Reshape((IMAGE_SIZE, IMAGE_SIZE))(x) \n    \n    # To join UNet output to ResNet50 need this lines to match input and output shates. And comment the 'Reshape' line.\n    #x = Conv2D(3, kernel_size=1, activation=\"sigmoid\", name=\"masks\")(x)\n    #x = Reshape((IMAGE_SIZE, IMAGE_SIZE))(x)   \n\n    return Model(inputs=model.input, outputs=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras.backend import log, epsilon\n\ndef dice_coefficient(y_true, y_pred):\n    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n    denominator = tf.reduce_sum(y_true + y_pred)\n\n    return numerator / (denominator + tf.keras.backend.epsilon())\n\n\n\ndef losses(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - log(dice_coefficient(y_true, y_pred) + epsilon())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nadamOptimizer = Adam(lr=1e-6, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n\ncheckpoint = ModelCheckpoint(\"model-{loss:.2f}.h5\", monitor=\"loss\", verbose=1, save_best_only=True,\n                             save_weights_only=True, mode=\"min\", period=1)\nstop = EarlyStopping( monitor=\"loss\", patience=5, mode=\"min\")\nreduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.2, patience=5, min_lr=1e-8, verbose=1, mode=\"min\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainUnetModel = True\n\nEPOCHS = 10\n\n#### Add your code here ####\n\n#WEIGHTS_FILE = \"../input/unwets/model-2.56.h5\"\nUNetModel = create_UNetModel()\nUNetModel.compile(loss=losses, optimizer=adamOptimizer, metrics=[dice_coefficient]) \n#UNetModel.load_weights(WEIGHTS_FILE)\n\nif trainUnetModel==True :\n    # Make layers trainable\n    for layer in UNetModel.layers:\n        layer.trainable = True\n\n    history=UNetModel.fit_generator(generator=trainUNetDataGen,\n                        epochs=1,\n                        validation_data=validateUNetDataGen,\n                        callbacks=[checkpoint, reduce_lr, stop],\n                        shuffle=True,\n                        verbose=1)\n    unet_history = np.array(history.history)\n    np.save(\"unetTrain_hist3\", unet_history, allow_pickle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check sample ground truth masked image and predicted masked image \nimageSet0 = trainUNetDataGen[0][0]\nmaskSet0 = trainUNetDataGen[0][1]\nshowMaskedImage(imageSet0, maskSet0, 5)\n\npredMasks = UNetModel.predict(imageSet0)\nshowMaskedImage(imageSet0, predMasks, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qwrv =drtery ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_resNetModel(trainable=True) :\n    \n    input_img_shape = (IMAGE_SIZE, IMAGE_SIZE, 3)\n    \n    # Load pre-trained ResNet50 with 'imagenet' weights\n    resNetLayers = ResNet50(weights='imagenet', input_shape = input_img_shape, include_top=False) \n\n    # Will train some layers and leave other frozen, as we have small dataset and different from trained dataset.\n    layer_names = [layer.name for layer in resNetLayers.layers]\n    layer_idx = layer_names.index(\"conv2_block3_out\") # index of \"conv2_block3_out\" = 38\n\n    # to freeze layers, except the last 38 layers out of total 175 layers\n    for layer in resNetLayers.layers[:-(layer_idx)]:\n        layer.trainable = trainable\n        \n    # Append classifire\n    classLayers = resNetLayers.layers[-1].output\n    classLayers = GlobalAveragePooling2D()(classLayers)\n    classLayers = Dense(1024, activation='relu')(classLayers)\n    classLayers = Dense(2, activation='softmax', kernel_initializer='zero', name='dense_class_{}'.format(2))(classLayers)\n    #classLayers = Convolution2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='class_conv1')(UNet_ResNet_Layers)\n    #classLayers = Convolution2D(2, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='class_out')(classLayers)\n    #flattenOut = Flatten()(classLayers)\n    #out_class = Dense(2, activation='softmax', kernel_initializer='zero', name='dense_class_{}'.format(2))(flattenOut)\n    \n    return Model(inputs=resNetLayers.input, outputs=classLayers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclsCheckpoint = ModelCheckpoint(\"nemoResnetModel-{loss:.2f}.h5\", monitor=\"loss\", verbose=1, save_best_only=True,\n                                 save_weights_only=True, mode=\"min\", period=1)\nclsStop = EarlyStopping(monitor=\"loss\", patience=5, mode=\"min\")\nclsReduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.2, patience=5, min_lr=1e-6, verbose=1, mode=\"min\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainResNetClassifierModel = True\n\nCLSI_EPOCHS = 1\n\n    \n# This is for training the model.   \nif(trainResNetClassifierModel == True) :   \n    resNetClassifierModel = create_resNetModel()\n    resNetClassifierModel.compile(optimizer='SGD', loss='binary_crossentropy', metrics = ['accuracy'])\n    \n    print(\"Training model... \")\n    resnet_history = resNetClassifierModel.fit_generator(trainClasiDataGen,\n                                                    epochs=CLSI_EPOCHS,\n                                                    validation_data=validateClasiDataGen,\n                                                    callbacks=[clsCheckpoint, clsReduce_lr, clsStop],\n                                                    verbose=1)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#UNetModel\n#resNetClassifierModel\npltUtil.plot_model(resNetClassifierModel,\n                    to_file=\"resNetClassifierModel.png\",\n                    show_shapes=True,\n                    show_layer_names=True,\n                    expand_nested=False,\n                    dpi=70)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add UNetModel and resNetClassifierModel to get UNetResNetModel\n\nUNet_ResNet_Layers = Concatenate()([UNetModel.output, resNetClassifierModel.input])\n\n#UNetResNetModel = Add(Model(inputs=UNetModel.input, outputs=resNetClassifierModel)\n\nUNetResNetModel = Model(inputs=UNetModel.input, outputs=UNet_ResNet_Layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass JoinModels(Layer):\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(MyLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        input_dim = input_shape[1]\n        initial_weight_value = np.random.random((input_dim, output_dim))\n        self.W = K.variable(initial_weight_value)\n        self.trainable_weights = [self.W]\n\n    def call(self, x, mask=None):\n        return K.dot(x, self.W)\n\n    def get_output_shape_for(self, input_shape):\n        return (input_shape[0], self.output_dim)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}