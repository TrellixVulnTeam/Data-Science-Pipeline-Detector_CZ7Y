{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Modules","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import log_loss\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pprint import pprint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,r2_score\nimport warnings\nfrom mlxtend.classifier import StackingClassifier\nimport missingno as msno\nfrom sklearn.ensemble import VotingClassifier\nimport shap\nshap.initjs()\nimport lime\nfrom lime import lime_tabular\nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:03.736924Z","iopub.execute_input":"2021-11-26T17:19:03.737473Z","iopub.status.idle":"2021-11-26T17:19:13.065228Z","shell.execute_reply.started":"2021-11-26T17:19:03.737347Z","shell.execute_reply":"2021-11-26T17:19:13.064442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-26T17:19:13.066688Z","iopub.execute_input":"2021-11-26T17:19:13.067042Z","iopub.status.idle":"2021-11-26T17:19:13.073083Z","shell.execute_reply.started":"2021-11-26T17:19:13.066981Z","shell.execute_reply":"2021-11-26T17:19:13.072242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading DataSet","metadata":{}},{"cell_type":"code","source":"insurance_df = pd.read_csv('../input/prudential-life-insurance-assessment/train.csv.zip', index_col='Id')\ninsurance_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:13.074364Z","iopub.execute_input":"2021-11-26T17:19:13.074759Z","iopub.status.idle":"2021-11-26T17:19:13.945716Z","shell.execute_reply.started":"2021-11-26T17:19:13.074729Z","shell.execute_reply":"2021-11-26T17:19:13.944734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:13.947711Z","iopub.execute_input":"2021-11-26T17:19:13.948281Z","iopub.status.idle":"2021-11-26T17:19:13.954989Z","shell.execute_reply.started":"2021-11-26T17:19:13.948236Z","shell.execute_reply":"2021-11-26T17:19:13.953911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distribution of Target Variabels","metadata":{}},{"cell_type":"code","source":"insurance_df['Response'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:13.956432Z","iopub.execute_input":"2021-11-26T17:19:13.956792Z","iopub.status.idle":"2021-11-26T17:19:13.971781Z","shell.execute_reply.started":"2021-11-26T17:19:13.95675Z","shell.execute_reply":"2021-11-26T17:19:13.971121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Class imbalance can be seen here. Also there 8 categories, lets combine them to 3 categories","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=insurance_df['Response']);","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:13.973081Z","iopub.execute_input":"2021-11-26T17:19:13.973507Z","iopub.status.idle":"2021-11-26T17:19:14.25846Z","shell.execute_reply.started":"2021-11-26T17:19:13.973476Z","shell.execute_reply":"2021-11-26T17:19:14.257424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Response 8 has highest values and 3 has the least","metadata":{}},{"cell_type":"code","source":"#Combining the Categores to 3 categories\ninsurance_df['Modified_Response']  = insurance_df['Response'].apply(lambda x : 0 if x<=7 and x>=0 else (1 if x==8 else -1))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:14.260031Z","iopub.execute_input":"2021-11-26T17:19:14.260489Z","iopub.status.idle":"2021-11-26T17:19:14.30904Z","shell.execute_reply.started":"2021-11-26T17:19:14.26044Z","shell.execute_reply":"2021-11-26T17:19:14.308255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x= insurance_df['Modified_Response']);","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:14.310519Z","iopub.execute_input":"2021-11-26T17:19:14.310839Z","iopub.status.idle":"2021-11-26T17:19:14.514222Z","shell.execute_reply.started":"2021-11-26T17:19:14.310797Z","shell.execute_reply":"2021-11-26T17:19:14.513401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Still some imbalance can be seen","metadata":{}},{"cell_type":"markdown","source":"# Droping old Target variabels","metadata":{}},{"cell_type":"code","source":"# Dropping old response columns\ninsurance_df.drop('Response',axis = 1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:14.515375Z","iopub.execute_input":"2021-11-26T17:19:14.515659Z","iopub.status.idle":"2021-11-26T17:19:14.578809Z","shell.execute_reply.started":"2021-11-26T17:19:14.515579Z","shell.execute_reply":"2021-11-26T17:19:14.577948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Seperating Categorical and Numerical Variables","metadata":{}},{"cell_type":"code","source":"# Making lists with categorical and numerical features.\ncategorical =  [col for col in insurance_df.columns if insurance_df[col].dtype =='object']\n\nnumerical = categorical =  [col for col in insurance_df.columns if insurance_df[col].dtype !='object']","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:14.581438Z","iopub.execute_input":"2021-11-26T17:19:14.581678Z","iopub.status.idle":"2021-11-26T17:19:14.59511Z","shell.execute_reply.started":"2021-11-26T17:19:14.581649Z","shell.execute_reply":"2021-11-26T17:19:14.594379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualising Categorical variabels","metadata":{}},{"cell_type":"code","source":"# Doing count plots for categorical\nfor col in categorical:\n    counts = insurance_df[col].value_counts().sort_index()\n    if len(counts) > 10 and len(counts) < 50 :\n      fig = plt.figure(figsize=(30, 10))\n    elif len(counts) >50 :\n      continue\n    else:\n      fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    counts.plot.bar(ax = ax, color='steelblue')\n    ax.set_title(col + ' counts')\n    ax.set_xlabel(col) \n    ax.set_ylabel(\"Frequency\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:14.596105Z","iopub.execute_input":"2021-11-26T17:19:14.596316Z","iopub.status.idle":"2021-11-26T17:19:32.872126Z","shell.execute_reply.started":"2021-11-26T17:19:14.59629Z","shell.execute_reply":"2021-11-26T17:19:32.8713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* D3 has the highest frequencies\n* Most of the features here are unbalanced.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_1'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_1'], ax=axes[1])","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:32.873831Z","iopub.execute_input":"2021-11-26T17:19:32.874091Z","iopub.status.idle":"2021-11-26T17:19:33.662111Z","shell.execute_reply.started":"2021-11-26T17:19:32.874052Z","shell.execute_reply":"2021-11-26T17:19:33.661088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Right skewed.\n* Outliers can be seen.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_4'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_4'], ax=axes[1])","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:33.663584Z","iopub.execute_input":"2021-11-26T17:19:33.663914Z","iopub.status.idle":"2021-11-26T17:19:34.414318Z","shell.execute_reply.started":"2021-11-26T17:19:33.663871Z","shell.execute_reply":"2021-11-26T17:19:34.413524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_6'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_6'], ax=axes[1])","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:34.415433Z","iopub.execute_input":"2021-11-26T17:19:34.416055Z","iopub.status.idle":"2021-11-26T17:19:35.135413Z","shell.execute_reply.started":"2021-11-26T17:19:34.415995Z","shell.execute_reply":"2021-11-26T17:19:35.134475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Family_Hist_4'], ax=axes[0])\nsns.boxplot(insurance_df['Family_Hist_4'], ax=axes[1])","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:35.136785Z","iopub.execute_input":"2021-11-26T17:19:35.137122Z","iopub.status.idle":"2021-11-26T17:19:35.850751Z","shell.execute_reply.started":"2021-11-26T17:19:35.137077Z","shell.execute_reply":"2021-11-26T17:19:35.849939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# visualising Correlation grater than .80","metadata":{}},{"cell_type":"code","source":"# I just checked correlated feature with greater than .8 here \ncorr = insurance_df.corr()\ncorr_greater_than_80 = corr[corr>=.8]\ncorr_greater_than_80","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:35.852335Z","iopub.execute_input":"2021-11-26T17:19:35.85282Z","iopub.status.idle":"2021-11-26T17:19:38.420954Z","shell.execute_reply.started":"2021-11-26T17:19:35.852775Z","shell.execute_reply":"2021-11-26T17:19:38.420173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(corr_greater_than_80, cmap=\"Reds\");","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:38.421977Z","iopub.execute_input":"2021-11-26T17:19:38.422198Z","iopub.status.idle":"2021-11-26T17:19:39.946729Z","shell.execute_reply.started":"2021-11-26T17:19:38.422173Z","shell.execute_reply":"2021-11-26T17:19:39.945915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"CONCLUSION¶\nBMI and Weight are highly correlated, which makes sense also as these 2 features are directly proprtional.\n\nIns_Age and Family_Hist_4, Family_Hist_2 highly correlated\n\nAlthough, I am not going to perform any transformation on any feature or drop any as these are tree based models and they don't get affected by correlation much because of their non parametric nature.","metadata":{}},{"cell_type":"code","source":"#setting max columns to 200\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:39.948258Z","iopub.execute_input":"2021-11-26T17:19:39.948467Z","iopub.status.idle":"2021-11-26T17:19:39.952548Z","shell.execute_reply.started":"2021-11-26T17:19:39.948442Z","shell.execute_reply":"2021-11-26T17:19:39.951548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking Null values in a Dataset","metadata":{}},{"cell_type":"code","source":"#checking percentage of missing values in a column\nmissing_val_count_by_column = insurance_df.isnull().sum()/len(insurance_df)\n\nprint(missing_val_count_by_column[missing_val_count_by_column > 0.4].sort_values(ascending=False))","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:39.953671Z","iopub.execute_input":"2021-11-26T17:19:39.953881Z","iopub.status.idle":"2021-11-26T17:19:39.98908Z","shell.execute_reply.started":"2021-11-26T17:19:39.953856Z","shell.execute_reply":"2021-11-26T17:19:39.988451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Removing unimportant columns","metadata":{}},{"cell_type":"code","source":"# Dropping all columns in which greater than 40 percent null values\ninsurance_df = insurance_df.dropna(thresh=insurance_df.shape[0]*0.4,how='all',axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:39.990171Z","iopub.execute_input":"2021-11-26T17:19:39.990826Z","iopub.status.idle":"2021-11-26T17:19:40.040376Z","shell.execute_reply.started":"2021-11-26T17:19:39.990792Z","shell.execute_reply":"2021-11-26T17:19:40.039562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Does not contain important information\ninsurance_df.drop('Product_Info_2',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:40.041693Z","iopub.execute_input":"2021-11-26T17:19:40.04251Z","iopub.status.idle":"2021-11-26T17:19:40.066968Z","shell.execute_reply.started":"2021-11-26T17:19:40.042473Z","shell.execute_reply":"2021-11-26T17:19:40.066023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting data X and Y","metadata":{}},{"cell_type":"code","source":"# Data for all the independent variables\nX = insurance_df.drop(labels='Modified_Response',axis=1)\n\n# Data for the dependent variable\nY = insurance_df['Modified_Response']","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:40.068146Z","iopub.execute_input":"2021-11-26T17:19:40.068367Z","iopub.status.idle":"2021-11-26T17:19:40.09907Z","shell.execute_reply.started":"2021-11-26T17:19:40.06834Z","shell.execute_reply":"2021-11-26T17:19:40.098404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filling Missing values with Mean","metadata":{}},{"cell_type":"code","source":"# Filling remaining missing values with mean\nX = X.fillna(X.mean())","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:40.099959Z","iopub.execute_input":"2021-11-26T17:19:40.100297Z","iopub.status.idle":"2021-11-26T17:19:40.169171Z","shell.execute_reply.started":"2021-11-26T17:19:40.10027Z","shell.execute_reply":"2021-11-26T17:19:40.168471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing data","metadata":{}},{"cell_type":"code","source":"# Train-test split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.25, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:40.170253Z","iopub.execute_input":"2021-11-26T17:19:40.1707Z","iopub.status.idle":"2021-11-26T17:19:40.239488Z","shell.execute_reply.started":"2021-11-26T17:19:40.170665Z","shell.execute_reply":"2021-11-26T17:19:40.238704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of train dataset\nprint(X_train.shape,Y_train.shape)\n\n# Check the shape of test dataset\nprint(X_test.shape, Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:40.240899Z","iopub.execute_input":"2021-11-26T17:19:40.241533Z","iopub.status.idle":"2021-11-26T17:19:40.24794Z","shell.execute_reply.started":"2021-11-26T17:19:40.241485Z","shell.execute_reply":"2021-11-26T17:19:40.247071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility Functions\ndef check_scores(model, X_train, X_test ):\n  # Making predictions on train and test data\n\n  train_class_preds = model.predict(X_train)\n  test_class_preds = model.predict(X_test)\n\n\n  # Get the probabilities on train and test\n  train_preds = model.predict_proba(X_train)[:,1]\n  test_preds = model.predict_proba(X_test)[:,1]\n\n\n  # Calculating accuracy on train and test\n  train_accuracy = accuracy_score(Y_train,train_class_preds)\n  test_accuracy = accuracy_score(Y_test,test_class_preds)\n\n  print(\"The accuracy on train dataset is\", train_accuracy)\n  print(\"The accuracy on test dataset is\", test_accuracy)\n  print()\n  # Get the confusion matrices for train and test\n  train_cm = confusion_matrix(Y_train,train_class_preds)\n  test_cm = confusion_matrix(Y_test,test_class_preds )\n\n  print('Train confusion matrix:')\n  print( train_cm)\n  print()\n  print('Test confusion matrix:')\n  print(test_cm)\n  print()\n\n  # Get the roc_auc score for train and test dataset\n  train_auc = roc_auc_score(Y_train,train_preds)\n  test_auc = roc_auc_score(Y_test,test_preds)\n\n  print('ROC on train data:', train_auc)\n  print('ROC on test data:', test_auc)\n  \n  # Fscore, precision and recall on test data\n  f1 = f1_score(Y_test, test_class_preds)\n  precision = precision_score(Y_test, test_class_preds)\n  recall = recall_score(Y_test, test_class_preds) \n  \n  \n  #R2 score on train and test data\n  train_log = log_loss(Y_train,train_preds)\n  test_log = log_loss(Y_test, test_preds)\n\n  print()\n  print('Train log loss:', train_log)\n  print('Test log loss:', test_log)\n  print()\n  print(\"F score is:\",f1 )\n  print(\"Precision is:\",precision)\n  print(\"Recall is:\", recall)\n  return model, train_auc, test_auc, train_accuracy, test_accuracy,f1, precision,recall, train_log, test_log\n\n\ndef check_importance(model, X_train):\n  #Checking importance of features\n  importances = model.feature_importances_\n  \n  #List of columns and their importances\n  importance_dict = {'Feature' : list(X_train.columns),\n                    'Feature Importance' : importances}\n  #Creating a dataframe\n  importance_df = pd.DataFrame(importance_dict)\n  \n  #Rounding it off to 2 digits as we might get exponential numbers\n  importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n  return importance_df.sort_values(by=['Feature Importance'],ascending=False)\n\ndef grid_search(model, parameters, X_train, Y_train):\n  #Doing a grid\n  grid = GridSearchCV(estimator=model,\n                       param_grid = parameters,\n                       cv = 2, verbose=2, scoring='roc_auc')\n  #Fitting the grid \n  grid.fit(X_train,Y_train)\n  print()\n  print()\n  # Best model found using grid search\n  optimal_model = grid.best_estimator_\n  print('Best parameters are: ')\n  pprint( grid.best_params_)\n\n  return optimal_model\n\n\n\n# This function will show how a feature is pushing towards 0 or 1\ndef interpret_with_lime(model, X_test):\n  # New data\n  interpretor = lime_tabular.LimeTabularExplainer(\n    training_data=np.array(X_train),\n    feature_names=X_train.columns,\n    mode='classification')\n  \n\n  exp = interpretor.explain_instance(\n      data_row=X_test.iloc[10], \n      predict_fn=model.predict_proba\n  )\n\n  exp.show_in_notebook(show_table=True)\n\n# This gives feature importance\ndef plot_feature_importance(model, X_train):\n  # PLotting features vs their importance factors\n  fig = plt.figure(figsize = (15, 8))\n  \n  # Extracting importance values\n  values =check_importance(model, X_train)[check_importance(model, X_train)['Feature Importance']>0]['Feature Importance'].values\n  \n  \n  # Extracting importance features\n  features = check_importance(model, X_train)[check_importance(model, X_train)['Feature Importance']>0]['Feature'].values\n\n  plt.bar(features, values, color ='blue',\n          width = 0.4)\n  plt.xticks( rotation='vertical')\n  plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:40.249656Z","iopub.execute_input":"2021-11-26T17:19:40.250215Z","iopub.status.idle":"2021-11-26T17:19:40.274226Z","shell.execute_reply.started":"2021-11-26T17:19:40.25017Z","shell.execute_reply":"2021-11-26T17:19:40.273313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"# Number of trees\nn_estimators = [50,80,100]\n\n# Maximum depth of trees\nmax_depth = [4,6,8]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [50,100,150]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [40,50]\n\n# Hyperparameter Grid\nrf_parameters = {'n_estimators' : n_estimators,\n              'max_depth' : max_depth,\n              'min_samples_split' : min_samples_split,\n              'min_samples_leaf' : min_samples_leaf}\n\npprint(rf_parameters)\n\n#finding the best model\nrf_optimal_model = grid_search(RandomForestClassifier(), rf_parameters, X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:19:40.275802Z","iopub.execute_input":"2021-11-26T17:19:40.276348Z","iopub.status.idle":"2021-11-26T17:22:06.167862Z","shell.execute_reply.started":"2021-11-26T17:19:40.276288Z","shell.execute_reply":"2021-11-26T17:22:06.166906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting scores from all the metrices\nrf_model, rf_train_auc, rf_test_auc, rf_train_accuracy, rf_test_accuracy,rf_f1, rf_precision,rf_recall,rf_train_log, rf_test_log = check_scores(rf_optimal_model, X_train, X_test )","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:22:06.173323Z","iopub.execute_input":"2021-11-26T17:22:06.173594Z","iopub.status.idle":"2021-11-26T17:22:07.773637Z","shell.execute_reply.started":"2021-11-26T17:22:06.173563Z","shell.execute_reply":"2021-11-26T17:22:07.772719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importance of Random Forest","metadata":{}},{"cell_type":"code","source":"#Getting the feature importance for all the features\ncheck_importance(rf_model, X_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:22:07.774916Z","iopub.execute_input":"2021-11-26T17:22:07.775278Z","iopub.status.idle":"2021-11-26T17:22:07.812557Z","shell.execute_reply.started":"2021-11-26T17:22:07.775237Z","shell.execute_reply":"2021-11-26T17:22:07.811697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ploting only which are necessary","metadata":{}},{"cell_type":"code","source":"# PLotting only those features which are contributing something\nplot_feature_importance(rf_model, X_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:22:07.813715Z","iopub.execute_input":"2021-11-26T17:22:07.814037Z","iopub.status.idle":"2021-11-26T17:22:08.155791Z","shell.execute_reply.started":"2021-11-26T17:22:07.813977Z","shell.execute_reply":"2021-11-26T17:22:08.154945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"CONCLUSION:¶\nBMI, weight, Medical_History_23, Medical_History_4 and Medical_Keyword_15 seems to be important features according to random forest.\n\nAlso, only these features are contributing to the model prediction. Some features can be elmininated which are not contributing on further investigation.","metadata":{}},{"cell_type":"markdown","source":"# Model intrepretability from Random Forest\n* Using Lime","metadata":{}},{"cell_type":"code","source":"# Interpretting the model using lime\ninterpret_with_lime(rf_model,X_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:22:08.156928Z","iopub.execute_input":"2021-11-26T17:22:08.157231Z","iopub.status.idle":"2021-11-26T17:22:36.893476Z","shell.execute_reply.started":"2021-11-26T17:22:08.1572Z","shell.execute_reply":"2021-11-26T17:22:36.892457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using shape","metadata":{}},{"cell_type":"code","source":"# Interpretting the model using shaply\nX_shap=X_train\n\nrf_explainer = shap.TreeExplainer(rf_model)\nrf_shap_values = rf_explainer.shap_values(X_shap)\nshap.summary_plot(rf_shap_values, X_shap, plot_type=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:22:36.895522Z","iopub.execute_input":"2021-11-26T17:22:36.896209Z","iopub.status.idle":"2021-11-26T17:25:06.1702Z","shell.execute_reply.started":"2021-11-26T17:22:36.896159Z","shell.execute_reply":"2021-11-26T17:25:06.169275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Findings\nMedical keyword 15,medical history 9, Wt, medical history 3 all pushing towards 1.\n\nOrange ones are pusing towards 1.","metadata":{}},{"cell_type":"markdown","source":"# Dependence Plots","metadata":{}},{"cell_type":"code","source":"# Plotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Wt','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, rf_shap_values[0], X_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:25:06.171445Z","iopub.execute_input":"2021-11-26T17:25:06.171703Z","iopub.status.idle":"2021-11-26T17:25:24.096413Z","shell.execute_reply.started":"2021-11-26T17:25:06.171638Z","shell.execute_reply":"2021-11-26T17:25:24.09535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Findings¶\n* With high medical history 23 and low bmi we get class 1","metadata":{}},{"cell_type":"markdown","source":"# Gradient Boosting","metadata":{}},{"cell_type":"code","source":"#finding the best model\ngb_parameters ={\n    \"n_estimators\":[5,50,250],\n    \"max_depth\":[1,3,5,7],\n    \"learning_rate\":[0.01,0.1,1]\n}\n\npprint(gb_parameters)\n\ngb_optimal_model = grid_search(GradientBoostingClassifier(), gb_parameters, X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:25:24.097895Z","iopub.execute_input":"2021-11-26T17:25:24.098234Z","iopub.status.idle":"2021-11-26T17:41:17.338826Z","shell.execute_reply.started":"2021-11-26T17:25:24.098193Z","shell.execute_reply":"2021-11-26T17:41:17.337951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature importance for Gradient Boosting","metadata":{}},{"cell_type":"code","source":"# Getting the scpres for all the score metrics used here\ngb_model, gb_train_auc, gb_test_auc, gb_train_accuracy, gb_test_accuracy,gb_f1, gb_precision,gb_recall,gb_train_log, gb_test_log = check_scores(gb_optimal_model, X_train, X_test )","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:41:17.34015Z","iopub.execute_input":"2021-11-26T17:41:17.340548Z","iopub.status.idle":"2021-11-26T17:41:18.755771Z","shell.execute_reply.started":"2021-11-26T17:41:17.340502Z","shell.execute_reply":"2021-11-26T17:41:18.754659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting feature importance\ncheck_importance(gb_model, X_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:41:18.757165Z","iopub.execute_input":"2021-11-26T17:41:18.757401Z","iopub.status.idle":"2021-11-26T17:41:18.794069Z","shell.execute_reply.started":"2021-11-26T17:41:18.757367Z","shell.execute_reply":"2021-11-26T17:41:18.79304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLotting only those features which are contributing something\nplot_feature_importance(gb_model, X_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:41:18.795205Z","iopub.execute_input":"2021-11-26T17:41:18.79565Z","iopub.status.idle":"2021-11-26T17:41:19.149882Z","shell.execute_reply.started":"2021-11-26T17:41:18.795603Z","shell.execute_reply":"2021-11-26T17:41:19.149029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"CONCLUSION:\nBMI, weight, Medical_History_23, Medical_History_4 and Medical_Keyword_15 seems to be the most important 5 features according to Gradient boosting.","metadata":{}},{"cell_type":"markdown","source":"# Model intrepetability for Gradient Boostong\n* using Lime","metadata":{}},{"cell_type":"code","source":"# Interpretting the model using lime\ninterpret_with_lime(gb_model,X_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:41:19.151534Z","iopub.execute_input":"2021-11-26T17:41:19.151843Z","iopub.status.idle":"2021-11-26T17:41:47.530947Z","shell.execute_reply.started":"2021-11-26T17:41:19.151802Z","shell.execute_reply":"2021-11-26T17:41:47.529984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## using shape","metadata":{}},{"cell_type":"code","source":"# Interpretting the model using shaply\nX_shap=X_train\n\ngb_explainer = shap.TreeExplainer(gb_model)\ngb_shap_values = gb_explainer.shap_values(X_shap)\nshap.summary_plot(gb_shap_values, X_shap, plot_type=\"dot\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:41:47.53279Z","iopub.execute_input":"2021-11-26T17:41:47.533091Z","iopub.status.idle":"2021-11-26T17:42:46.948627Z","shell.execute_reply.started":"2021-11-26T17:41:47.533054Z","shell.execute_reply":"2021-11-26T17:42:46.947774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Findings¶\nBMI is pushing models prediction towards 0.\n\nMedical keyword 15 is pushing towards 1. However, medical keyword 4 is pushing towards 0.\n\nAlso, according to feature plot Wt. was in top 5 most important features, same isn't followed here.","metadata":{}},{"cell_type":"markdown","source":"# Dependence Plot","metadata":{}},{"cell_type":"code","source":"#PLotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Product_Info_4','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, gb_shap_values, X_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:42:46.94979Z","iopub.execute_input":"2021-11-26T17:42:46.950494Z","iopub.status.idle":"2021-11-26T17:43:04.84535Z","shell.execute_reply.started":"2021-11-26T17:42:46.950455Z","shell.execute_reply":"2021-11-26T17:43:04.844469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Findings¶\n* For low BMI and high medical history 23 we get class as 1.","metadata":{}},{"cell_type":"markdown","source":"# XGBOOST","metadata":{}},{"cell_type":"code","source":"# Parameter grid for xgboost\nxgb_parameters = {'max_depth': [1,3,5], 'n_estimators': [2,5,10], 'learning_rate': [.01 , .1, .5]}\nprint('XGB parameters areL:')\npprint(xgb_parameters)\n#finding the best model\nxgb_optimal_model = grid_search(XGBClassifier(), xgb_parameters, X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:43:04.846468Z","iopub.execute_input":"2021-11-26T17:43:04.846774Z","iopub.status.idle":"2021-11-26T17:43:22.062259Z","shell.execute_reply.started":"2021-11-26T17:43:04.846743Z","shell.execute_reply":"2021-11-26T17:43:22.061611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the scores for all the score metrics used here\nxgb_model, xgb_train_auc, xgb_test_auc, xgb_train_accuracy, xgb_test_accuracy,xgb_f1, xgb_precision,xgb_recall,xgb_train_log, xgb_test_log= check_scores(xgb_optimal_model, X_train, X_test )","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:43:22.063686Z","iopub.execute_input":"2021-11-26T17:43:22.064193Z","iopub.status.idle":"2021-11-26T17:43:22.364588Z","shell.execute_reply.started":"2021-11-26T17:43:22.064155Z","shell.execute_reply":"2021-11-26T17:43:22.363943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature importance for XGBOOST","metadata":{}},{"cell_type":"code","source":"\n# Getting feature importance\n\ncheck_importance(xgb_model, X_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:43:22.365904Z","iopub.execute_input":"2021-11-26T17:43:22.366526Z","iopub.status.idle":"2021-11-26T17:43:22.400679Z","shell.execute_reply.started":"2021-11-26T17:43:22.366478Z","shell.execute_reply":"2021-11-26T17:43:22.399866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion:¶\n* Same trend is seen here.\n* They all are giving similar scores also so it could be that same features are contributing the most thus similar scores.","metadata":{}},{"cell_type":"markdown","source":"# Model intrepebility for XGBOOST\n* using shap","metadata":{}},{"cell_type":"code","source":"# Interpretting the model using shaply\n\nxgb_explainer = shap.TreeExplainer(xgb_model)\nxgb_shap_values = xgb_explainer.shap_values(X_shap)\nshap.summary_plot(xgb_shap_values, X_shap, plot_type=\"dot\")","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:43:22.40181Z","iopub.execute_input":"2021-11-26T17:43:22.402063Z","iopub.status.idle":"2021-11-26T17:43:33.166182Z","shell.execute_reply.started":"2021-11-26T17:43:22.402026Z","shell.execute_reply":"2021-11-26T17:43:33.165306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Again BMI is pushing towards class 0.\n* MEdical history 4 pushing towards class 1.","metadata":{}},{"cell_type":"markdown","source":"# Dependence Plot","metadata":{}},{"cell_type":"code","source":"#PLotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Product_Info_4','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, xgb_shap_values, X_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:43:33.167661Z","iopub.execute_input":"2021-11-26T17:43:33.16798Z","iopub.status.idle":"2021-11-26T17:43:53.281067Z","shell.execute_reply.started":"2021-11-26T17:43:33.167939Z","shell.execute_reply":"2021-11-26T17:43:53.280084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* For product info 4 and wt we see some interesting trend","metadata":{}},{"cell_type":"markdown","source":"# Logestic Regression","metadata":{}},{"cell_type":"code","source":"# Parameter grid for Logistic Regression\nsolvers = ['lbfgs']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\nlr_parameters = dict(solver=solvers,penalty=penalty,C=c_values)# define grid search\n\n#finding the best model\nlr_optimal_model = grid_search(LogisticRegression( max_iter=5000), lr_parameters, X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:43:53.282629Z","iopub.execute_input":"2021-11-26T17:43:53.28294Z","iopub.status.idle":"2021-11-26T17:47:44.382683Z","shell.execute_reply.started":"2021-11-26T17:43:53.282898Z","shell.execute_reply":"2021-11-26T17:47:44.381835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the scores for all the score metrics used here\n\nlr_model, lr_train_auc, lr_test_auc, lr_train_accuracy, lr_test_accuracy,lr_f1, lr_precision, lr_recall,lr_train_log, lr_test_log = check_scores(lr_optimal_model, X_train, X_test )","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:47:44.384275Z","iopub.execute_input":"2021-11-26T17:47:44.384745Z","iopub.status.idle":"2021-11-26T17:47:44.691166Z","shell.execute_reply.started":"2021-11-26T17:47:44.384704Z","shell.execute_reply":"2021-11-26T17:47:44.69005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature importance for Logestic Regression","metadata":{}},{"cell_type":"code","source":"# Making a dataframe with coefficients and the feature names respectively\nimportance_df_lr = pd.concat([ pd.DataFrame(data =((X_train.columns).values).reshape(-1,1), columns = ['Feature']), pd.DataFrame(data =np.round(lr_optimal_model.coef_,2).reshape(-1,1), columns = ['Feature Importance'])], axis=1 )\nimportance_df_lr.sort_values(by=['Feature Importance'],ascending=False, inplace = True)\nimportance_df_lr","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:47:44.692787Z","iopub.execute_input":"2021-11-26T17:47:44.693105Z","iopub.status.idle":"2021-11-26T17:47:44.724688Z","shell.execute_reply.started":"2021-11-26T17:47:44.693063Z","shell.execute_reply":"2021-11-26T17:47:44.723812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting feature vs importance\nfig = plt.figure(figsize = (15, 8))\n\nvalues =importance_df_lr[importance_df_lr['Feature Importance']>0]['Feature Importance'].values\n\nfeatures = importance_df_lr[importance_df_lr['Feature Importance']>0]['Feature'].values\n\nplt.bar(features, values, color ='blue',\n          width = 0.4)\nplt.xticks( rotation='vertical')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:47:44.725975Z","iopub.execute_input":"2021-11-26T17:47:44.726633Z","iopub.status.idle":"2021-11-26T17:47:46.043128Z","shell.execute_reply.started":"2021-11-26T17:47:44.726594Z","shell.execute_reply":"2021-11-26T17:47:46.042264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Conclusion¶\n* And again the same pattern when doing feature importance","metadata":{}},{"cell_type":"markdown","source":"# Model interpretability for Logestic Regression\n* Using Lime","metadata":{}},{"cell_type":"code","source":"# Interpretting the model using lime\ninterpret_with_lime(lr_model,X_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:47:46.04422Z","iopub.execute_input":"2021-11-26T17:47:46.044429Z","iopub.status.idle":"2021-11-26T17:48:14.177251Z","shell.execute_reply.started":"2021-11-26T17:47:46.044404Z","shell.execute_reply":"2021-11-26T17:48:14.17605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Findings\n* Only BMI and medical history 4 pushing towards class 0","metadata":{}},{"cell_type":"markdown","source":"# Max Voting Model","metadata":{}},{"cell_type":"code","source":"# Appending all the models to estimators list\nestimators = []\n\nestimators.append(('logistic', lr_optimal_model))\nestimators.append(('XGB', xgb_optimal_model))\nestimators.append(('GB', gb_optimal_model))\nestimators.append(('rf', rf_optimal_model))\n\n# create the voting model\nvoting_model = VotingClassifier(estimators, voting='soft')\n\nvoting_model.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:48:14.17884Z","iopub.execute_input":"2021-11-26T17:48:14.179507Z","iopub.status.idle":"2021-11-26T17:50:16.260744Z","shell.execute_reply.started":"2021-11-26T17:48:14.179463Z","shell.execute_reply":"2021-11-26T17:50:16.259815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting all the scores and errors\nvoting_model, voting_train_auc, voting_test_auc, voting_train_accuracy, voting_test_accuracy, voting_f1, voting_precision, voting_recall, voting_train_log, voting_test_log = check_scores(voting_model, X_train, X_test )","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:50:16.261973Z","iopub.execute_input":"2021-11-26T17:50:16.262238Z","iopub.status.idle":"2021-11-26T17:50:19.735713Z","shell.execute_reply.started":"2021-11-26T17:50:16.262207Z","shell.execute_reply":"2021-11-26T17:50:19.734864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Stacked Model","metadata":{}},{"cell_type":"code","source":"#Building a stacked classifier\nstacked_classifier = StackingClassifier(classifiers =[lr_optimal_model, xgb_optimal_model, gb_model], meta_classifier = RandomForestClassifier(), use_probas = True, use_features_in_secondary = True)\n\n# training of stacked model\nstacked_model = stacked_classifier.fit(X_train, Y_train)  ","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:50:19.736844Z","iopub.execute_input":"2021-11-26T17:50:19.737122Z","iopub.status.idle":"2021-11-26T17:52:27.730945Z","shell.execute_reply.started":"2021-11-26T17:50:19.737081Z","shell.execute_reply":"2021-11-26T17:52:27.730313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked_model, stacked_train_auc, stacked_test_auc, stacked_train_accuracy, stacked_test_accuracy, stacked_f1, stacked_precision, stacked_recall, stacked_train_log, stacked_test_log = check_scores(stacked_model, X_train, X_test )","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:52:27.732348Z","iopub.execute_input":"2021-11-26T17:52:27.732841Z","iopub.status.idle":"2021-11-26T17:52:32.432467Z","shell.execute_reply.started":"2021-11-26T17:52:27.732797Z","shell.execute_reply":"2021-11-26T17:52:32.431498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Model and their Accuracy DataFrame","metadata":{}},{"cell_type":"code","source":"# Making a dataframe of all the scores for every model\n\nscores_ = [(\"Random Forest\", rf_train_auc, rf_test_auc, rf_train_accuracy, rf_test_accuracy,rf_train_log, rf_test_log,rf_f1, rf_precision, rf_recall),\n(\"Gradient Boosting\",  gb_train_auc, gb_test_auc, gb_train_accuracy, gb_test_accuracy,gb_train_log, gb_test_log,gb_f1, gb_precision,gb_recall,),\n(\"XG Boost\", xgb_train_auc, xgb_test_auc, xgb_train_accuracy, xgb_test_accuracy,xgb_train_log, xgb_test_log,xgb_f1, xgb_precision, xgb_recall),\n(\"Logistic Regression\", lr_train_auc, lr_test_auc, lr_train_accuracy, lr_test_accuracy,lr_train_log, lr_test_log,lr_f1, lr_precision, lr_recall,),\n(\"Voting Classifier\", voting_train_auc, voting_test_auc, voting_train_accuracy, voting_test_accuracy, voting_train_log, voting_test_log, voting_f1, voting_precision, voting_recall),\n(\"Stacked Model\", stacked_train_auc, stacked_test_auc, stacked_train_accuracy, stacked_test_accuracy, stacked_train_log, stacked_test_log, stacked_f1, stacked_precision, stacked_recall)]\n\nScores_ =pd.DataFrame(data = scores_, columns=['Model Name', 'Train ROC', 'Test ROC', 'Train Accuracy', 'Test Accuracy', 'Train Log Loss','Test Log Loss','F-Score', 'Precision','Recall',])\nScores_.set_index('Model Name', inplace = True)\n\nScores_","metadata":{"execution":{"iopub.status.busy":"2021-11-26T17:52:32.434067Z","iopub.execute_input":"2021-11-26T17:52:32.434299Z","iopub.status.idle":"2021-11-26T17:52:32.460239Z","shell.execute_reply.started":"2021-11-26T17:52:32.434268Z","shell.execute_reply":"2021-11-26T17:52:32.459469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Results¶\n* Gradient Boosting, Voting Classifier and Stacked models are performing really well. Their train and test errors and also the roc scores and f scores are really close and good.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}