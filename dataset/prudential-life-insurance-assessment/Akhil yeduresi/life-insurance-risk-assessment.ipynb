{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n#loading libraries\n\nimport os\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve\nfrom sklearn.metrics import recall_score, classification_report, auc, roc_curve\nfrom sklearn.metrics import precision_recall_fscore_support, f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n\nfrom keras import losses\nfrom keras.utils import to_categorical\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.models import Model, Sequential \nfrom keras.optimizers import Adam\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.callbacks import Callback\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom scipy import stats\nimport tensorflow as tf\nimport pickle\n\nfrom pylab import rcParams","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"#Loading train data\ntrain=pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Exploration"},{"metadata":{"_uuid":"ee8cb7933996db2cc7e1893d0c812bb0d3d2fb15","trusted":false},"cell_type":"code","source":"#Data\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Shape of data\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73f01cc7a52560d940a79fbe7611ad16868db99f","trusted":false},"cell_type":"code","source":"#Exploring missing values\ntrain.isnull().sum()[train.isnull().sum() !=0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"894fe7bff6368117131409e43a6263f61f38538a","trusted":false},"cell_type":"code","source":"#Exploring missing values\ntrain_missing= train.isnull().sum()[train.isnull().sum() !=0]\ntrain_missing=pd.DataFrame(train_missing.reset_index())\ntrain_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)\ntrain_missing['missing_count_percentage']=((train_missing['missing_count'])/59381)*100\nplt.figure(figsize=(20,8))\nsns.barplot(y=train_missing['features'],x=train_missing['missing_count_percentage'])\ntrain_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking data types\ntrain.dtypes.unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f7eea1d4dc8b50b49c05e009283dccfdb7c1b98","trusted":false},"cell_type":"code","source":"#Outliers detection\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de93f695e88d8e320bf0efab985b9a461effc7ae","trusted":false},"cell_type":"code","source":"#Responce variable\naixs1 = plt.subplots(1,1,figsize=(10,5))\nsns.countplot(x='Response',data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data PreProcessing"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Categorical codes\ntrain['Product_Info_2'] = train['Product_Info_2'].astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing Value Treatment"},{"metadata":{"trusted":false},"cell_type":"code","source":"# missing values\ntrain_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#dropping columns containing missing values more than 80%\ntrain = train.drop(['Medical_History_10','Medical_History_24','Medical_History_32'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#missing values AGAIN\ntrain_missing= train.isnull().sum()[train.isnull().sum() !=0]\ntrain_missing=pd.DataFrame(train_missing.reset_index())\ntrain_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)\ntrain_missing['missing_count_percentage']=((train_missing['missing_count'])/59381)*100\ntrain_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Mean Imputation fro continous variables\nContinuos = ['Employment_Info_1','Employment_Info_4', 'Employment_Info_6', 'Insurance_History_5',\n                    'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5']\ntrain[Continuos] = train[Continuos].fillna(train[Continuos].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Mode Imputation fro continous variables\nCategorical = ['Medical_History_1', 'Medical_History_15']\ntrain[Categorical] = train[Categorical].apply(lambda x:x.fillna(x.value_counts().index[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Missing values again\ntrain_missing= train.isnull().sum()[train.isnull().sum() !=0]\ntrain_missing=pd.DataFrame(train_missing.reset_index())\ntrain_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)\ntrain_missing['missing_count_percentage']=((train_missing['missing_count'])/59381)*100\ntrain_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#train data\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Modelling\n\n1)Dataset split"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Dataset split\ntrain_data, test_data = train_test_split(train, test_size = 0.15)\nprint(train_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#traindata\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#traindata\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Predictor and responce variables\ntrain_x = train_data.drop(['Id', 'Response'], axis=1)\ntrain_y = train_data['Response']\ntest_x = test_data.drop(['Id', 'Response'], axis=1)\ntest_y = test_data['Response']\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#train responce\ntrain_y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#test responce\ntest_y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#converting to responce categorical class labels(0-7)\ntrain_y = train_y-1\ntrain_y = to_categorical(train_y, num_classes= 8)\n\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2)Normalization"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Function for normalization\ndef normalization(data):\n    return (data - data.min())/(data.max() - data.min())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#normalizing data\ntrain_x = normalization(train_x)\ntest_x = normalization(test_x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#traindata\ntrain_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#testdata\ntest_x.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n3)Models and evaluation"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Train and test data shapes\nprint(train_x.shape)\nprint(test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#assigning static parameter\nnb_epoch = 20\nbatch_size = 512\ninput_dim = train_x.shape[1]\nhidden_dim1 = 64 \nhidden_dim2 = 32\nhidden_dim3 = 16\nlearning_rate = 1e-7","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Function for auto encoder to get and fit model\ndef get_fit_encoder(xs_train,xs_cv,test_x):\n    input_layer = Input(shape=(input_dim, ))\n    encoder = Dense(input_dim, activation=\"relu\",activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n    \n    encoder = Dense(hidden_dim1, activation=\"relu\")(encoder)\n    encoder = Dense(hidden_dim2, activation=\"relu\")(encoder)\n    encoder = Dense(hidden_dim3, activation=\"relu\", name=\"encoder\")(encoder)\n    \n    decoder = Dense(hidden_dim3, activation=\"relu\")(encoder)\n    decoder = Dense(hidden_dim2, activation='relu')(decoder)\n    decoder = Dense(hidden_dim1, activation='relu')(decoder)\n    \n    decoder = Dense(input_dim, activation='relu')(decoder)\n    decoder = Dense(input_dim, activation='sigmoid')(decoder)\n    autoencoder = Model(inputs=input_layer, outputs=decoder)\n    #autoencoder.summary()\n    autoencoder.compile(optimizer='adam',\n                        loss='binary_crossentropy')\n    \n    history = autoencoder.fit(x=xs_train, y=xs_train,\n                          epochs=nb_epoch,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          validation_data=(xs_cv, xs_cv),\n                          verbose=1)\n    encoder = Model(autoencoder.input, autoencoder.get_layer('encoder').output)\n    x_auto_train= encoder.predict(xs_train)\n    x_auto_cv= encoder.predict(xs_cv)\n    x_auto_test= encoder.predict(test_x)\n    return x_auto_train,x_auto_cv,x_auto_test\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Function for Neural network to get and fit model\ndef get_fit_neuralnetwork(xs_encoder_train,xs_encoder_cv,xs_encoder_test,ys_train,ys_cv):\n    classifier = Sequential()\n    classifier.add(Dense(output_dim = input_dim , init = 'uniform', activation = 'relu', input_dim = 16))\n    classifier.add(Dense(output_dim = 16 , init = 'uniform', activation = 'relu'))\n    classifier.add(Dense(output_dim = 8 , init = 'uniform', activation = 'relu'))\n    classifier.add(Dense(output_dim = 8, init = 'uniform', activation = 'softmax'))\n    classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    cp = ModelCheckpoint(filepath=\"autoencoder_data.h5\",\n                         save_best_only=True,\n                         verbose=0)\n    tb = TensorBoard(log_dir='./logs',\n                     histogram_freq=0,\n                     write_graph=True,\n                     write_images=True)\n    history = classifier.fit(xs_encoder_train, ys_train,\n                             batch_size=batch_size ,\n                             epochs=nb_epoch ,\n                             shuffle=True,\n                             validation_data=(xs_encoder_cv,ys_cv),\n                             verbose=1,\n                            callbacks=[cp, tb]).history\n    y_pred_NN = classifier.predict(xs_encoder_test, batch_size=batch_size, verbose=1)\n    y_pred_NN = np.argmax(y_pred_NN,axis = 1) + 1\n    return y_pred_NN","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Function for State of art model to get and fit model\ndef get_fit_SOA_Models(x_sampletrain,y_sampletrain,test_x):\n    model1 = RandomForestClassifier()\n    \n    inside_train_y = np.argmax(y_sampletrain, axis = 1) + 1   \n    \n    model1.fit(x_sampletrain, inside_train_y)\n    \n    y_pred1 = model1.predict(test_x) \n    return y_pred1","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#function for model evaluation\ndef model_evaluation (test_y,y_pred_NN,y_pred1):\n   \n    accuracy_NN = accuracy_score(test_y,y_pred_NN)\n    F1_score_NN=f1_score(test_y, y_pred_NN,average='weighted')\n    Precision_NN=precision_score(test_y, y_pred_NN,average='weighted')\n    Recall_score_NN=recall_score(test_y, y_pred_NN,average='weighted')\n    \n    accuracy_SOAM1 = accuracy_score(test_y, y_pred1)\n    F1_score_SOAM1=f1_score(test_y, y_pred1,average='weighted')\n    Precision_SOAM1=precision_score(test_y, y_pred1,average='weighted')\n    Recall_score_SOAM1=recall_score(test_y, y_pred1,average='weighted')\n    \n    \n    print(\"Classification score for NN:\", classification_report(test_y,y_pred_NN))\n    print(\"Classification score for SOAM1:\", classification_report(test_y, y_pred1))\n       \n    return accuracy_NN,F1_score_NN,Precision_NN,Recall_score_NN,accuracy_SOAM1,F1_score_SOAM1,Precision_SOAM1,Recall_score_SOAM1\n    ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":false},"cell_type":"code","source":"#Function to pass sample data to autoencoder and neural network functions\ndef data_sampling(train_x, train_y, test_x, test_y):\n    accuracy_list_NN= []\n    F1_score_list_NN=[]\n    Precision_list_NN=[]\n    Recall_list_NN=[]\n    \n    accuracy_list_SOAM1= []\n    F1_score_list_SOAM1=[]\n    Precision_list_SOAM1=[]\n    Recall_list_SOAM1=[]\n    \n    \n    for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6,0.7, 0.8, 0.9, 0.99]:\n        print(\"data sample {}\".format(i*100))\n        x_sampletrain, _, y_sampletrain, _ = train_test_split(train_x, train_y, stratify= train_y, train_size=i)\n        xs_train, xs_cv, ys_train, ys_cv = train_test_split(x_sampletrain, y_sampletrain, stratify=y_sampletrain, train_size=0.9)\n        xs_train.shape, xs_cv.shape, ys_train.shape, ys_cv.shape\n        xs_encoder_train,xs_encoder_cv,xs_encoder_test=get_fit_encoder(xs_train,xs_cv,test_x)\n        y_pred_NN=get_fit_neuralnetwork(xs_encoder_train,xs_encoder_cv,xs_encoder_test,ys_train,ys_cv)\n        \n        y_pred1=get_fit_SOA_Models(x_sampletrain,y_sampletrain,test_x)\n        \n        accuracy_NN,F1_score_NN,Precision_NN,Recall_NN,accuracy_SOAM1,F1_score_SOAM1,Precision_SOAM1,Recall_SOAM1=model_evaluation(test_y,y_pred_NN,y_pred1)\n        \n        \n        accuracy_list_NN.append(accuracy_NN)\n        F1_score_list_NN.append(F1_score_NN)\n        Precision_list_NN.append(Precision_NN)\n        Recall_list_NN.append(Recall_NN)\n        \n        accuracy_list_SOAM1.append(accuracy_SOAM1)\n        F1_score_list_SOAM1.append(F1_score_SOAM1)\n        Precision_list_SOAM1.append(Precision_SOAM1)\n        Recall_list_SOAM1.append(Recall_SOAM1)\n        \n        \n    return accuracy_list_NN,F1_score_list_NN,Precision_list_NN,Recall_list_NN,accuracy_list_SOAM1,F1_score_list_SOAM1,Precision_list_SOAM1,Recall_list_SOAM1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#main code to run all functions to reach objective\naccuracy_list_NN,F1_score_list_NN,Precision_list_NN,Recall_list_NN,accuracy_list_SOAM1,F1_score_list_SOAM1,Precision_list_SOAM1,Recall_list_SOAM1=data_sampling(train_x, train_y, test_x, test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evalution output for Neural network\naccuracy_list_NN","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Evalution output for SOAM network\naccuracy_list_SOAM1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving output to a file\nwith open('Accuracy_NN.txt', 'w') as f:\n    print(accuracy_list_NN, file=f)\nwith open('Accuracy_SOAM.txt', 'w') as f:\n    print(accuracy_list_SOAM1, file=f)\n\nwith open('F1_score_NN.txt', 'w') as f:\n    print(F1_score_list_NN, file=f)\nwith open('F1_score_SOAM1.txt', 'w') as f:\n    print(F1_score_list_SOAM1, file=f)\n\nwith open('Precision_Score_NN.txt', 'w') as f:\n    print(Precision_list_NN, file=f)\nwith open('Precision_Score_SOAM1.txt', 'w') as f:\n    print(Precision_list_SOAM1, file=f)\n    \nwith open('Recall_Recall_NN.txt', 'w') as f:\n    print(Recall_list_NN, file=f)\nwith open('Recall_Recall_SOAM1.txt', 'w') as f:\n    print(Recall_list_SOAM1, file=f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#output comparitive visualization\nfrom matplotlib.pyplot import figure\nplt.figure(figsize=(15, 5))\nplt.plot(accuracy_list_NN,label='NN')\nplt.plot(accuracy_list_SOAM1,label='SOAM1')\n#plt.plot([10,20], accuracy_list_SOAM3,label='SOAM2')\nplt.legend(loc='lower right')\nplt.xlabel(\"data fraction\")\nplt.ylabel(\"Accuaracy Value\")\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":1}