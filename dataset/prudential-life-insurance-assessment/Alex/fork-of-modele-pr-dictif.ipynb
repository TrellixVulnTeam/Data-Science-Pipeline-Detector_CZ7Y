{"cells":[{"metadata":{},"cell_type":"markdown","source":"On souhaite prédire les valeurs de Response avec un modèle Bayésien"},{"metadata":{},"cell_type":"markdown","source":"L'objectif de ce notebook est de tester l'efficacité d'un modèle Bayésien pour prédire la catégorie à laquelle une personne sera associée en fonction des information qui le caractérisent.\n<br>\nCe notebook fait suite a l'analyse exploratoire faite au lien suivant: https://www.kaggle.com/alexdarge/approche-bay-sienne"},{"metadata":{},"cell_type":"markdown","source":"# Plan\n<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Sommaire</h3>\n    \n<font size=+1><b>Feuille de route</b></font>\n    \n<font size=+1><b>Chargement des données</b></font>\n* [Import des fichiers](#0)\n* [Import des librairies](#1)\n\n<font size=+1><b>Préparation avant modélisation</b></font>\n* [Nettoyage de données](#2)\n* [Encodage de la variable catégorielle](#3)\n* [Sélection des variables pour modélisation](#4)\n\n    \n<font size=+1><b>Bayésien naïf</b></font>\n* [Une 1ère prédiction](#5)\n* [Modèle sans la variable Product_Info_2](#6)\n* [Matrice de confusion sur le 2ème modèle](#7)\n \n        \n<font size=+1><b>Réduction dimensionnelles</b></font>\n* [t-SNE](#8)\n* [ACP](#9)\n\n    \n<font size=+1><b>Tests</b></font>\n* [Tests infructueux](#10)\n"},{"metadata":{},"cell_type":"markdown","source":"# Feuille de route\n## Fait:\n* Bayésien naif sur l'ensemble des variables\n* Bayésien sur les variables numériques\n* Réduction dimensionnelle avec PCA et t-SNE\n* Représentation \n\n## A faire:\n* Identifier une métrique de contrôle afin de comparer avec d'autres modèles prédictifs\n* Prédiction avec les features réduites\n* Améliorer la performance du modèle"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"0\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Import des fichiers"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Import des librairies"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn import manifold\n\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nfrom sklearn import decomposition\n\n\n\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Préparation avant modélisation"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Nettoyage des données"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/prudential-life-insurance-assessment/train.csv.zip')\ntest = pd.read_csv('../input/prudential-life-insurance-assessment/test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fonction données manquantes\ndef missing(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Pourcent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return tt\nmissing(train)['Pourcent'].sort_values(ascending=False)\n# suppression des features données manquantes\ntrain_modified = train[train.columns[train.isnull().mean() <= 0.75]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop données non renseignées\ncols_with_missing = [col for col in train_modified.columns \n                                 if train_modified[col].isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_modified.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Encodage de la variable catégorielle"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On voit que la variable Product_Info_2 est catégorielle, on s'y intéresse en vue de l'encoder numériquement"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train.Product_Info_2.unique()))\ntrain.Product_Info_2.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On voit que la variable Product_Info_2 est définie comme une catégorie, elle prend 19 valeurs différentes. Ces valeurs sont l'association d'une lettre et d'un chiffre. Pour l'interpréter numériquement on doit l'encoder c'est à dire créer des features supplémentaires (pour chacune des 19 valeurs). Ces variables seront des booléens qui indiquent quelle valeur de Product_Info_2 la personne a indiqué.\n<br>\nPar exemple si une personne est caracterisée par la variable Product_Info_2 renseignée comme: 'D3', alors l'encodage des variables encodée sera comme suit: Product_Info_2_D3 vaudra 1 et tous les autres vaudront 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"# encodage pour la variable catégorielle\nencoded_train = pd.get_dummies(train_modified)\nencoded_train.sample(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Sélection des variables pour la modélisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# suppression des colonnes données manquantes\ntrain_modified = encoded_train.drop(cols_with_missing, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selection de la variable cible pour la modélisation\ny=train_modified.Response","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputation des données manquantes\nmy_imputer = SimpleImputer()\nimputed_data_train = my_imputer.fit_transform(train_modified)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sélection des variables pour la modélisation\ndf_features = train_modified.loc[:, train_modified.columns != 'Response']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# formalisation et vérification des dimensions avant modélisation\nfeatures=list(df_features.columns)\nX=train_modified[features]\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# suppression de la variable id qui est attribué pour chaque individu \nX=X.drop(['Id'], axis=1)\nX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bayésien naïf"},{"metadata":{"trusted":true},"cell_type":"code","source":"# configuation du modèle\nBayes = MultinomialNB()\nBayes.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Un premier résultat"},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage comparatif\npredict_df=pd.DataFrame(data=Bayes.predict(X))\npredict_df.rename(columns={0:'Predicted'}, inplace=True)\npredict_df['Response']=y\npredict_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nombre des valeurs pour la variable prédite\npredict_df.Predicted.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valeur des prédictions\nsns.countplot(data=predict_df, x='Predicted').set_title(\"Prédiction pour chaque catégorie avec variables encodées\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparaison prédiction avec les valeurs réelles\nfig, axes = plt.subplots(1,2,figsize=(16,6))\nfig.suptitle('Comparaison classes prédites vs classes réelles pour le 1er modèle')\nsns.countplot(ax=axes[0], data=predict_df, x='Predicted')\nsns.countplot(ax=axes[1], data=predict_df, x='Response')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# différence entre la variable prédite et Response\n# intégration de la variable de classe d'écart entre prédiction et valeur réelle\npredict_df['diff']=abs(predict_df.Predicted-predict_df.Response)\npredict_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# différence de classe entre prédiction et Response\npredict_df['diff'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quantité de données différence classes prédiction/réelles\nsns.countplot(data=predict_df, x='diff').set_title(\"Classes d'écart entre valeurs prédites et valeurs réelles 1er modèle\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quantité de classe d'écart\npredict_df['diff'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On n'obtient que 37% de résultat exact.\nOn peut expliquer cela à cause de l'encodage de la variable catégorielle qui créé beaucoup de variables numériques, ce qui peut biaiser la prédiction.\n<br>\nOn constate cependant qu'en cumulé on obtient près de 71% de précision à deux classes d'écart et 54% à une classe d'écart.\n\nOn ne prendra pas en considération la variable catégorielle dans le test suivant."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Modèle sans la variable Product_Info_2"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop('Product_Info_2', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# suppression des données non renseignées\ncols_with_missing = [col for col in data.columns \n                                 if data[col].isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(cols_with_missing, axis=1)\nfinal_df = data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=data.Response","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=list(data.columns)\nX=data[features]\nX=X.drop(['Id'], axis=1)\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bayes2 = MultinomialNB()\nBayes2.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage comparatif\npredict2=pd.DataFrame(data=Bayes2.predict(X))\npredict2.rename(columns={0:'Predicted'}, inplace=True)\npredict2['Response']=y\npredict2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nombre des valeurs pour la variable prédite\npredict_df.Predicted.value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valeur des prédictions\nsns.countplot(data=predict2, x='Predicted').set_title(\"Prédiction pour chaque catégorie pour le deuxieme modèle\")\nplt.grid(linestyle='dotted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparaison du modèle avec la valeurs réelles\nfig, axes = plt.subplots(1,2,figsize=(16,6))\nfig.suptitle('Comparaison classes prédites vs classes réelles 2ème modèle')\nsns.countplot(ax=axes[0], data=predict2, x='Predicted')\nsns.countplot(ax=axes[1], data=predict2, x='Response')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# différence entre la variable prédite et Response\npredict2['diff']=abs(predict2.Predicted-predict2.Response)\npredict2.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# différence de classe entre prédiction et Response\npredict2['diff'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quantité de données différence classes prédiction/réelles\nsns.countplot(data=predict2, x='diff').set_title(\"classes d'écart entre valeurs prédites et valeurs réelles 2ème modèle\")\nplt.grid(linestyle='dotted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pourcentage de classes écart entre prédiction et 'Response'\npredict2['diff'].value_counts(normalize=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On a réussi à améliorer la performance de notre prédiction, en effet elle était de 37% dans notre premier résultat et est dorénavent de 50%.\n<br>\nEn cumul à une classe d'écart on obtient 74% de précision avec le modèle.\nDe plus en cumulé à deux classes d'écart on obtient 89% de précision. C'est près de 18 points de pourcentage de plus que dans le modèle précédent."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Matrice de confusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"# matrice de confusion\nconfusion_matrix=confusion_matrix(predict2.Response, predict2.Predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage du rapport de la matrice de confusion\nprint(classification_report(predict2.Response, predict2.Predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_actu=predict2.Response\ny_pred=predict2.Predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_actu=pd.Series(predict2.Response, name='Réelle')\ny_pred=pd.Series(predict2.Predicted, name='Prédite')\ndf_confusion=pd.crosstab(y_actu, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_confusion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(df_confusion, title='Confusion matrix', cmap=plt.cm.Reds):\n    plt.matshow(df_confusion, cmap=cmap)\n    plt.title('Matrice de confusion')\n    plt.colorbar()\n    tick_marks = np.arange(len(df_confusion.columns))\n    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n    plt.yticks(tick_marks, df_confusion.index)\n    #plt.tight_layout()\n    plt.ylabel(df_confusion.index.name)\n    plt.xlabel(df_confusion.columns.name)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap matrice de confusion\nplot_confusion_matrix(df_confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage des prédictions pour comparaison avec les \nplt.figure(figsize=(8, 8))\nsns.heatmap(df_confusion, annot=True, fmt='d', cmap=plt.cm.Reds).set_title('Matrice des classes prédites / classes obtenues')\nplt.xlabel('Classe Prédite', size=16)\nplt.ylabel('Classe Réelle', size=16)\nplt.title('Matrice des classes prédites / classes obtenues', size=20)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Réduction avec SNE/ACP"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## t-SNE"},{"metadata":{},"cell_type":"markdown","source":"t-SNE est un algorithme de réduction de dimensions basé sur de l'apprentissage non supervisé. Il est utilisé pour de la visualisation de données ayant beaucoup de descripteurs.\n\nIl permet de représenter les données dans un nouvel espace interprétable (2 ou 3 dimensions)\nLes données proches dans l'espace original auront une probabilité élevée d'avoir une représentation proche dans le nouvel espace et à l'inverse les données éloignées ont une faible probabilité d'avoir une représentation proche dans le nouvel espace."},{"metadata":{"trusted":true},"cell_type":"code","source":"# création d'un pipeline: évite la fuite de données\ndef define_preprocessor(X):\n   \n    # Pipeline features catégorielles\n    categorical_transformer = Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # simple imputation \n            ('target_encoder', TargetEncoder()), \n            ('scaler', StandardScaler()), # standardizsation apres encodage\n            ])\n    \n    # pipeline features numériques\n    numeric_transformer = Pipeline(steps=[\n            ('imputer', IterativeImputer(max_iter=10)), \n            ('scaler', StandardScaler()), # standardisation\n             ])\n\n    # pipelines features numériques et catégorielles\n    preprocessor = ColumnTransformer(transformers=[\n            ('cat', categorical_transformer, list(X.select_dtypes(include=['category', 'bool']).columns)),\n            ('num', numeric_transformer, list(X.select_dtypes(include='number').columns)),\n            ])\n    \n    return preprocessor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fonction préprocessing pour le SNE\ndef preprocessing_tSNE(dataframe, target_name='TARGET'):\n    \n    X = dataframe.copy()\n\n    # suppression lignes ou il manque la valeur cible\n    X = X.dropna(subset=[target_name])\n\n    # définition variable cible\n    y = X[target_name]\n\n    # retire variable cible des feautures interprétées\n    X = X.drop(columns=[target_name])\n\n    # applique la fonction de préprocessing\n    preprocessor = define_preprocessor(X)\n\n    # applique process\n    X_std = preprocessor.fit_transform(X, y)\n    \n    return (X_std, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fonction SNE, permet aussi l'affichage\ndef tSNE(dataframe, target_name='TARGET'):\n    \n    # tritement pour tSNE\n    (X_std, y) = preprocessing_tSNE(dataframe, target_name)\n\n    # Instanciation tSNE\n    tsne = manifold.TSNE(n_components=2,\n                         perplexity=30,\n                         n_iter=300,\n                         init='pca', # initialisation avec une PCA\n                         random_state=0\n                        )\n\n    # Applying tSNE\n    X_projected = tsne.fit_transform(X_std) \n    \n    # Affichage\n    plt.figure(figsize=(14,8))\n\n    # limites graphe\n    plt.xlim(X_projected[:,0].min()*1.1, X_projected[:,0].max()*1.1)\n    plt.ylim(X_projected[:,1].min()*1.1, X_projected[:,1].max()*1.1)\n\n    # définition des axes\n    plt.title(\"t-SNE\\n\", fontsize=20)\n    plt.xlabel(\"t-SNE feature 1\")\n    plt.ylabel(\"t-SNE feature 2\")\n\n    # Def nuages de points\n    sc = plt.scatter(X_projected[:,0], # x\n                 X_projected[:,1], #y\n                 c=y,\n                 cmap=plt.cm.get_cmap('RdYlGn_r'), # couleur\n                 marker='.'\n        )\n    \n    # configuration et échelle\n    cbar = plt.colorbar(sc)\n    cbar.ax.get_yaxis().set_ticks([])\n    cbar.ax.get_yaxis().labelpad = 15\n    cbar.set_label(target_name, rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(['Id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage\ntSNE(data, target_name='Response')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## ACP"},{"metadata":{},"cell_type":"markdown","source":"On veut déterminer un nombre minimal de composantes à partir duquel on peut considérer que l'étude prédictive est fiable à partir d'un certain seuil (que je définis ici à 90%).\nIl est important de normaliser les données pour faire une PCA (pour la conservation de la distance vectorielle).\nLes données fournies ont déja été normalisée il n'est donc pas nécessaire de le faire ici. \n\nIl faudra néanmoins faire attention aux outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing  ACP\ndataframe = data\ntarget_name = 'Response'\n(X_std, y) = preprocessing_tSNE(dataframe, target_name)\n\n# Calcul des composantes principales\n\nn_components=2\npca = decomposition.PCA(n_components=n_components)\npca.fit(X_std)\n\nprint(\"Pourcentage variance expliquée par composante:\", pca.explained_variance_ratio_)\nprint(\"Pourcentage total variance expliquée:\", pca.explained_variance_ratio_.sum()) #  somme cumulée ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# représentation de l'ACP avec deux composantes\npca=PCA(n_components=2)\ncomponents=pca.fit_transform(X_std)\nfig = px.scatter(components, x=0, y=1, color=data.Response, title='Représentation PCA 2 composantes')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# représentation de l'ACP avec 3 composantes\npca = PCA(n_components=3)\ncomponents = pca.fit_transform(X_std)\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter_3d(\n    components, x=0, y=1, z=2, color=data.Response,\n    title='PCA avec 3 composantes\\nVariance explicative cumulée:: {}%'.format(total_var),\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fonction explicative des valeur propres pour PCA\ndef display_scree_plot(X_std):\n    pca = decomposition.PCA()\n    pca.fit(X_std)\n    scree = pca.explained_variance_ratio_*100\n    \n    plt.bar(np.arange(len(scree))+1, scree)\n    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n    plt.xlabel(\"Quantité de composantes\")\n    plt.ylabel(\"pourcentage de variance cumulée\")\n    plt.title(\"Etude du seuil de variance en fonction du nombre de composantes pour la PCA\", fontsize=15)\n    plt.grid(linestyle='dotted')\n    plt.show(block=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage valeurs propres PCA\ndisplay_scree_plot(X_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fonction donne les composantes principales de l'ACP, jusqu'au seuil de variance\ndef PCA_features_reduction(X_std, var_threshold=0.9): \n    # PCA\n    pca = decomposition.PCA()\n    pca.fit(X_std)\n    \n    # ratio de variance expliqué pour chaque composante principale\n    scree = pca.explained_variance_ratio_\n    # rend le nombre de composants principaux pour atteindre les seuils de variance\n    mask = scree.cumsum() > var_threshold\n    nb_selected_features = len(scree[~mask]) + 1\n    print(\"Nombre de features selectionnées:\", nb_selected_features)\n    \n    # Calcul du ratio\n    explained_variance_sum = scree.cumsum()[nb_selected_features-1]\n    print(\"Valeur cumulée de variance expliquée:  {:.2f}%\".format(explained_variance_sum*100))\n    \n    # projection sur les 1ers composant\n    X_projected = pca.transform(X_std)[:,:nb_selected_features]\n    \n    return X_projected","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_projected = PCA_features_reduction(X_std, var_threshold=0.9)\nX_projected.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Le seuil de 90% de variance expliquée est atteint avec 78 composantes principales.\nOn avait initialement près de 120 variables."},{"metadata":{},"cell_type":"markdown","source":"## Feature importance pour la PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df=data.copy()\n# crée objet standardscaler\nsc=StandardScaler()\n#stardisation données\nX_test_std=sc.fit_transform(test_df)\n# applique PCA\npca=PCA()\nX_test_pca=pca.fit(X_test_std)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# reduction dimensionnelle à 78 composantes\nnum_components = 78\npca = PCA(num_components)  \nX_test_pca = pca.fit_transform(X_test_std)\n# recherche des features le plus influentes\nn_pcs= pca.n_components_ # get number of component\n# get the index of the most important feature on EACH component\nmost_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\ninitial_feature_names = test_df.columns\n# get the most important feature names\nmost_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\nmost_important_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pour 3 composantes\nmodel = PCA(n_components=3).fit(X_std)\nX_pc = model.transform(X_std)\nn_pcs = model.components_.shape[0]\nmost_important = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\ndic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\ndic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# matrice de covariance\ncov_matrix=np.cov(X_std.T)\nprint('Dimensions de la matrice:', cov_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valeur propres de la matrice et vecteurs propres\neigenvalues, eigenvectors=np.linalg.eig(cov_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classement des valeurs propres\n\n#on crée une paire valeur propre/vecteur propre\neig_pairs=[(eigenvalues[index], eigenvectors[:, index]) for index in range(len(eigenvalues))]\n\n# classement des paire décroissant\neig_pairs.sort()\neig_pairs.reverse()\n\n#extraction\neigvalues_sorted = [eig_pairs[index][0] for index in range(len(eigenvalues))]\neigvectors_sorted = [eig_pairs[index][1] for index in range(len(eigenvalues))]\n\nprint('Valeurs propres décroissantes:\\n {}' .format(eigvalues_sorted))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# variance expliquée de chaque composante (numériquement ordonnée)\ntot = sum(eigenvalues)\nvar_explained = [(i / tot) for i in sorted(eigenvalues, reverse=True)]\ncum_var_exp = np.cumsum(var_explained)\nprint(var_explained)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# vérification variance cumulée\nprint('Variance cumulée: {}'.format(cum_var_exp))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# on réduit a 8 dimension pour l'affichage du pairplot\n\nP_reduce = np.array(eigvectors_sorted[0:8])\nX_std_8D = np.dot(X_std,P_reduce.T)\nreduced_pca = pd.DataFrame(X_std_8D)\nreduced_pca","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage pairplot\nsns.pairplot(reduced_pca, diag_kind='kde') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importance des variables avec un arbre de regression aléatoire\n\ndata_prediction=data.copy()\ndata_prediction=data_prediction.drop(['Response'], axis=1)\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(random_state=42, max_depth=10)\nmodel.fit(data_prediction,y)\nfeatures = data.columns\nimportances = model.feature_importances_\nindices = np.argsort(importances)[-9:]  # top 10 des variables\n\nplt.title('Importance des variables donné par le Random Forest', size=20)\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Importance relative')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# indice des 34 variables les moins influentes selon le Random Forest \nvar_to_drop=np.argsort(importances)[78:]\nvar_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_drop=[\n    'Medical_History_35',\n    'Medical_History_38',\n\n    'Medical_Keyword_20',\n    'Medical_Keyword_44',\n    'Medical_Keyword_8',\n    'Medical_Keyword_13',\n    'Medical_Keyword_26',\n    'Medical_Keyword_39',\n    'Medical_Keyword_6',\n    'Medical_Keyword_5',\n    'Medical_Keyword_18',\n    'Medical_Keyword_35',\n    'Medical_Keyword_21',\n    'Medical_Keyword_19',\n    'Medical_Keyword_36',\n    'Medical_Keyword_12',\n    'Medical_Keyword_45',\n    'Medical_Keyword_27',\n    'Medical_Keyword_17',\n    'Medical_Keyword_14',\n    'Medical_Keyword_16',\n    'Medical_Keyword_2',\n    'Medical_Keyword_29',\n    'Medical_Keyword_32',\n    'Medical_Keyword_30',\n    'Medical_Keyword_31',\n    'Medical_Keyword_11',\n    'Medical_Keyword_34',\n    'Medical_Keyword_7',\n    'Medical_Keyword_24',\n\n    'Product_Info_5',\n    'Product_Info_1',\n    'Product_Info_7'\n\n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# suppression des variables \nfor feature in features_to_drop:\n    data_prediction=data_prediction.drop(feature, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_prediction.shape\ndata_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_prediction=list(data_prediction.columns)\nX_prediction=data[features_prediction]\nprint(X_prediction.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bayes3 = MultinomialNB()\nBayes3.fit(X_prediction,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage comparatif\npredict3=pd.DataFrame(data=Bayes3.predict(X_prediction))\npredict3.rename(columns={0:'Predicted'}, inplace=True)\npredict3['Response']=y\npredict3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nombre des valeurs pour la variable prédite\npredict3.Predicted.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valeur des prédictions\nsns.countplot(data=predict3, x='Predicted').set_title(\"Prédiction pour chaque catégorie pour le 3ème modèle\")\nplt.grid(linestyle='dotted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparaison du modèle avec la valeurs réelles\nfig, axes = plt.subplots(1,2,figsize=(16,6))\nfig.suptitle('Comparaison classes prédites vs classes réelles 3eme modele')\nsns.countplot(ax=axes[0], data=predict3, x='Predicted')\nsns.countplot(ax=axes[1], data=predict3, x='Response')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# différence entre la variable prédite et Response\npredict3['diff']=abs(predict3.Predicted-predict3.Response)\npredict3.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# différence de classe entre prédiction et Response\npredict3['diff'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quantité de données différence classes prédiction/réelles\nsns.countplot(data=predict3, x='diff').set_title(\"classes d'écart entre valeurs prédites et valeurs réelles 3eme modele\")\nplt.grid(linestyle='dotted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pourcentage de classes écart entre prédiction et 'Response'\npredict3['diff'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interprétation des résultats: \n- 36% de prédiction exacte\n- 17% de prédiction à 2 classes d'écart\n- 17% de prédiction à 1 classe d'écart\n- on n'a pas plus de 7% prédiction à plus de 2 classes d'écart"},{"metadata":{},"cell_type":"markdown","source":"## Métrique de comparaison"},{"metadata":{"trusted":true},"cell_type":"code","source":"from ml_metrics import quadratic_weighted_kappa\ndef eval_wrapper(yhat, y):  \n    y = np.array(y)\n    y = y.astype(int)\n    yhat = np.array(yhat)\n    yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)   \n    return quadratic_weighted_kappa(yhat, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Autres modèles prédictifs"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install xgboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\n\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, recall_score, accuracy_score, precision_score\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nX_full = pd.read_csv('../input/prudential-life-insurance-assessment/train.csv.zip', index_col='Id')\nX_test_full = pd.read_csv('../input/prudential-life-insurance-assessment/test.csv.zip', index_col='Id')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = X_full.Response\nX = X_full.drop(labels=['Response'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=.30,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# suppression variable catégorielle\nX_dropped_train=X_train.drop(axis=1,labels=[\"Product_Info_2\"]).copy()\nX_dropped_valid=X_valid.drop(axis=1,labels=[\"Product_Info_2\"]).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#suppression des colonnes avec des null\nX_dropped_train.dropna(axis=1,inplace=True)\nX_dropped_valid.dropna(axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dimension X_train {}\".format(X_dropped_train.shape))\nprint(\"Dimension X_test {}\".format(X_dropped_valid.shape))\n\nprint(\"Dimension y_train {}\".format(y_train.shape))\nprint(\"Dimension y_valid {}\".format(y_valid.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed=42\n\n# liste des modèles\ndt=DecisionTreeClassifier(random_state=seed)\nrf=RandomForestClassifier(random_state=seed)\nlr=LogisticRegression(random_state=seed)\nadb=ensemble.AdaBoostClassifier()\nbgc=ensemble.BaggingClassifier()\ngbc=ensemble.GradientBoostingClassifier()\nxgb=XGBClassifier(random_state=seed)\nsvc=SVC(random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train, X_valid, y_train, y_valid = train_test_split(data_prediction,y,test_size=.30,random_state=42)\n\nX_dropped_train=X_train.dropna(axis=1,inplace=True)\nX_dropped_valid=X_valid.dropna(axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_wrapper(predict2.Response, predict2.Predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_wrapper(predict_df.Response, predict_df.Predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_wrapper(predict3.Response, predict3.Predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxgb_num_rounds = 720\nnum_classes = 8\nmissing_indicator = -1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/prudential-life-insurance-assessment/train.csv.zip')\ntest = pd.read_csv('../input/prudential-life-insurance-assessment/test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_params():\n    \n    params = {}\n    params[\"objective\"] = \"reg:squarederror\"     \n    params[\"eta\"] = 0.05\n    params[\"min_child_weight\"] = 360\n    params[\"subsample\"] = 0.85\n    params[\"colsample_bytree\"] = 0.3\n    params[\"silent\"] = 1\n    params[\"max_depth\"] = 7\n    plst = list(params.items())\n\n    return plst\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plst=get_params()\nplst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = train.append(test)\nall_data=all_data.drop(['Product_Info_2'], axis=1)\nall_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = all_data.drop(cols_with_missing, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.fillna(missing_indicator, inplace=True)\nall_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['Response'] = all_data['Response'].astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain = all_data[all_data['Response']>0].copy()\ntest = all_data[all_data['Response']<1].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_to_drop=['Response']\nxgtrain = xgb.DMatrix(train.drop(columns_to_drop, axis=1), train['Response'].values, \n                        missing=missing_indicator)\nxgtest = xgb.DMatrix(test.drop(columns_to_drop, axis=1), label=test['Response'].values, \n                        missing=missing_indicator) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.train(plst, xgtrain, xgb_num_rounds) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preds = model.predict(xgtrain, ntree_limit=model.best_iteration)\nprint('Train score is:', eval_wrapper(train_preds, train['Response'])) \ntest_preds = model.predict(xgtest, ntree_limit=model.best_iteration)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.optimize import fmin_powell\n\ndef apply_offsets(data, offsets):\n    for j in range(num_classes):\n        data[1, data[0].astype(int)==j] = data[0, data[0].astype(int)==j] + offsets[j]\n    return data\n\ndef score_offset(data, bin_offset, sv, scorer=eval_wrapper):\n    # data has the format of pred=0, offset_pred=1, labels=2 in the first dim\n    data[1, data[0].astype(int)==sv] = data[0, data[0].astype(int)==sv] + bin_offset\n    score = scorer(data[1], data[2])\n    return score\n\n\noffsets = np.array([0.1, -1, -2, -1, -0.8, 0.02, 0.8, 1])\noffset_preds = np.vstack((train_preds, train_preds, train['Response'].values))\noffset_preds = apply_offsets(offset_preds, offsets)\nopt_order = [6,4,5,3]\nfor j in opt_order:\n    train_offset = lambda x: -score_offset(offset_preds, x, j) * 100\n    offsets[j] = fmin_powell(train_offset, offsets[j], disp=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Offset Train score is:', eval_wrapper(offset_preds[1], train['Response'])) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = np.vstack((test_preds, test_preds, test['Response'].values))\ndata = apply_offsets(data, offsets)\n\nfinal_test_preds = np.round(np.clip(data[1], 1, 8)).astype(int)\n\npreds_out = pd.DataFrame({\"Id\": test['Id'].values, \"Predicted\": final_test_preds})\npreds_out = preds_out.set_index('Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}