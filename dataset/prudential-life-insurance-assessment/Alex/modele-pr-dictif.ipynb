{"cells":[{"metadata":{},"cell_type":"markdown","source":"On souhaite prédire les valeurs de Response avec un modèle Bayésien"},{"metadata":{},"cell_type":"markdown","source":"L'objectif de ce notebook est de tester l'efficacité d'un modèle Bayésien pour prédire la catégorie à laquelle une personne sera associée en fonction des information qui le caractérisent.\n<br>\nCe notebook fait suite a l'analyse exploratoire faite au lien suivant: https://www.kaggle.com/alexdarge/approche-bay-sienne"},{"metadata":{},"cell_type":"markdown","source":"# Plan\n<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Sommaire</h3>\n    \n<font size=+1><b>Feuille de route</b></font>\n    \n<font size=+1><b>Chargement des données</b></font>\n* [Import des fichiers](#0)\n* [Import des librairies](#1)\n\n<font size=+1><b>Préparation avant modélisation</b></font>\n* [Nettoyage de données](#2)\n* [Encodage de la variable catégorielle](#3)\n* [Sélection des variables pour modélisation](#4)\n\n    \n<font size=+1><b>Bayésien naïf</b></font>\n* [Une 1ère prédiction](#5)\n* [Modèle sans la variable Product_Info_2](#6)\n* [Matrice de confusion sur le 2ème modèle](#7)\n \n        \n<font size=+1><b>Réduction dimensionnelles</b></font>\n* [t-SNE](#8)\n* [ACP](#9)\n\n    \n<font size=+1><b>Tests</b></font>\n* [Tests infructueux](#10)\n"},{"metadata":{},"cell_type":"markdown","source":"# Feuille de route\n## Fait:\n* Bayésien naif sur l'ensemble des variables\n* Bayésien sur les variables numériques\n* Réduction dimensionnelle avec PCA et t-SNE\n* Représentation \n\n## A faire:\n* Identifier une métrique de contrôle afin de comparer avec d'autres modèles prédictifs\n* Prédiction avec les features réduites\n* Améliorer la performance du modèle"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"0\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Import des fichiers"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Import des librairies"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn import manifold\n\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nfrom sklearn import decomposition\n\n\n\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Préparation avant modélisation"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Nettoyage des données"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/prudential-life-insurance-assessment/train.csv.zip')\ntest = pd.read_csv('../input/prudential-life-insurance-assessment/test.csv.zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fonction données manquantes\ndef missing(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Pourcent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return tt\nmissing(train)['Pourcent'].sort_values(ascending=False)\n# suppression des features données manquantes\ntrain_modified = train[train.columns[train.isnull().mean() <= 0.75]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop données non renseignées\ncols_with_missing = [col for col in train_modified.columns \n                                 if train_modified[col].isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = train_modified.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Encodage de la variable catégorielle"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On voit que la variable Product_Info_2 est catégorielle, on s'y intéresse en vue de l'encoder numériquement"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train.Product_Info_2.unique()))\ntrain.Product_Info_2.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On voit que la variable Product_Info_2 est définie comme une catégorie, elle prend 19 valeurs différentes. Ces valeurs sont l'association d'une lettre et d'un chiffre. Pour l'interpréter numériquement on doit l'encoder c'est à dire créer des features supplémentaires (pour chacune des 19 valeurs). Ce variables seront des booléens qui indiquent quelle valeur de Product_Info_2 la personne a indiqué.\n<br>\nPar exemple un si une personne est caracterisée par la variable Product_Info_2 renseignée comme: 'D3', alors l'encodage des variables encodée sera comme suit: Product_Info_2_D3 vaudra 1 et tous les autres vaudront 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"# encodage pour la variable catégorielle\nencoded_train = pd.get_dummies(train_modified)\nencoded_train.sample(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Sélection des variables pour la modélisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# suppression des colonnes données manquantes\ntrain_modified = encoded_train.drop(cols_with_missing, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# selection de la variable cible pour la modélisation\ny=train_modified.Response","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputation des données manquantes\nmy_imputer = SimpleImputer()\nimputed_data_train = my_imputer.fit_transform(train_modified)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sélection des variables pour la modélisation\ndf_features = train_modified.loc[:, train_modified.columns != 'Response']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# formalisation et vérification des dimensions avant modélisation\nfeatures=list(df_features.columns)\nX=train_modified[features]\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bayésien naïf"},{"metadata":{"trusted":true},"cell_type":"code","source":"# configuation du modèle\nBayes = MultinomialNB()\nBayes.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Un premier résultat"},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage comparatif\npredict_df=pd.DataFrame(data=Bayes.predict(X))\npredict_df.rename(columns={0:'Predicted'}, inplace=True)\npredict_df['Response']=y\npredict_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nombre des valeurs pour la variable prédite\npredict_df.Predicted.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valeur des prédictions\nsns.countplot(data=predict_df, x='Predicted').set_title(\"Prédiction pour chaque catégorie\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparaison prédiction avec les valeurs réelles\nfig, axes = plt.subplots(1,2,figsize=(16,6))\nfig.suptitle('Comparaison classes prédites vs classes réelles')\nsns.countplot(ax=axes[0], data=predict_df, x='Predicted')\nsns.countplot(ax=axes[1], data=predict_df, x='Response')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict_df.countplot(x='predict_df', y=['Predicted','Response'], figsize=(10,5), grid=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"width = 0.3\nfig, ax = plt.subplots(figsize=(12,8))\nrects1 = ax.bar(x - width/2, predict_df['Response'], width)\nrects2 = ax.bar(x + width/2, predict_df['Predicted'], width)\n\nplt.title('Comparaison Response/Predicted', fontsize = 20)\nplt.xlabel('Nombre', fontsize = 15)\nplt.ylabel('Catégories', fontsize = 15)\n\nax.set_ylim(top=1)\n\nplt.xticks(x, results_df.index)\nax.legend()\nplt.grid(linestyle='dotted')\nplt.show()\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# différence entre la variable prédite et Response\npredict_df['diff']=abs(predict_df.Predicted-predict_df.Response)\npredict_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# différence de classe entre prédiction et Response\npredict_df['diff'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quantité de données différence classes prédiction/réelles\nsns.countplot(data=predict_df, x='diff').set_title(\"Classes d'écart entre valeurs prédites et valeurs réelles\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_df['diff'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On n'obtient que 30% de résultat exact.\nOn peut expliquer cela à cause de l'encodage de la variable catégorielle qui créé beaucoup de variables numérique et qui peut biaiser la prédiction.\n<br>\nOn constate cependant qu'en cumulé on obtient près de 72% de précision à deux classes d'écart et 48% à une classe d'écart.\n\nOn ne prendra pas en considération cette variable dans le test suivant."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Modèle sans la variable Product_Info_2"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop('Product_Info_2', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# suppression des données non renseignées\ncols_with_missing = [col for col in data.columns \n                                 if data[col].isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(cols_with_missing, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=data.Response","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features=list(data.columns)\nX=data[features]\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bayes2 = MultinomialNB()\nBayes2.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage comparatif\npredict2=pd.DataFrame(data=Bayes2.predict(X))\npredict2.rename(columns={0:'Predicted'}, inplace=True)\npredict2['Response']=y\npredict2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nombre des valeurs pour la variable prédite\npredict_df.Predicted.value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# valeur des prédictions\nsns.countplot(data=predict2, x='Predicted').set_title(\"Prédiction pour chaque catégorie\")\nplt.grid(linestyle='dotted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# comparaison du modèle avec la valeurs réelles\nfig, axes = plt.subplots(1,2,figsize=(16,6))\nfig.suptitle('Comparaison classes prédites vs classes réelles')\nsns.countplot(ax=axes[0], data=predict2, x='Predicted')\nsns.countplot(ax=axes[1], data=predict2, x='Response')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# différence entre la variable prédite et Response\npredict2['diff']=abs(predict2.Predicted-predict2.Response)\npredict2.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# différence de classe entre prédiction et Response\npredict2['diff'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quantité de données différence classes prédiction/réelles\nsns.countplot(data=predict2, x='diff').set_title(\"classes d'écart entre valeurs prédites et valeurs réelles\")\nplt.grid(linestyle='dotted')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pourcentage de classes écart entre prédiction et 'Response'\npredict2['diff'].value_counts(normalize=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On a réussi à améliorer la performance de notre prédiction, en effet elle était de 30% dans notre premier résultat et est dorénavent de 36%.\n<br>\nDe plus en cumulé à deux classe d'écart on obtient 83% de précision. C'est près de 11 points de pourcentage de plus que dans le modèle précédent."},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage pourcentage camenbert\n#predict2.plot(kind='pie', y = 'diff', legend = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot = predict2.plot.pie(subplots=True, figsize=(8, 8))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Matrice de confusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"# matrice de confusion\nconfusion_matrix=confusion_matrix(predict2.Response, predict2.Predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage du rapport de la matrice de confusion\nprint(classification_report(predict2.Response, predict2.Predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_actu=predict2.Response\ny_pred=predict2.Predicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_actu=pd.Series(predict2.Response, name='Réelle')\ny_pred=pd.Series(predict2.Predicted, name='Prédite')\ndf_confusion=pd.crosstab(y_actu, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_confusion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalisation de la matrice de confusion\ndf_conf_norm = df_confusion / df_confusion.sum(axis=1)\ndf_conf_norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(df_confusion, title='Confusion matrix', cmap=plt.cm.Reds):\n    plt.matshow(df_confusion, cmap=cmap)\n    plt.title('Matrice de confusion')\n    plt.colorbar()\n    tick_marks = np.arange(len(df_confusion.columns))\n    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n    plt.yticks(tick_marks, df_confusion.index)\n    #plt.tight_layout()\n    plt.ylabel(df_confusion.index.name)\n    plt.xlabel(df_confusion.columns.name)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap matrice de confusion\nplot_confusion_matrix(df_confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap matrice de confusion normalisée\nplot_confusion_matrix(df_conf_norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Réduction avec SNE/ACP"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## t-SNE"},{"metadata":{},"cell_type":"markdown","source":"t-SNE est un algorithme de réduction de dimensions basé sur de l'apprentissage non supervisé. Il est utilisé pour de la visualisation de données ayant beaucoup de descripteurs.\n\nIl permet de représenter les données dans un nouvel espace interprétable (2 ou 3 dimensions)\nLes données proches dans l'espace original auront une probabilité élevée d'avoir une représentation proche dans le nouvel espace et à l'inverse les données éloignées ont une faible probabilité d'avoir une représentation proche dans le nouvel espace."},{"metadata":{"trusted":true},"cell_type":"code","source":"# création d'un pipeline: évite la fuite de données\ndef define_preprocessor(X):\n   \n    # Pipeline features catégorielles\n    categorical_transformer = Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), # simple imputation \n            ('target_encoder', TargetEncoder()), \n            ('scaler', StandardScaler()), # standardizsation apres encodage\n            ])\n    \n    # pipeline features numériques\n    numeric_transformer = Pipeline(steps=[\n            ('imputer', IterativeImputer(max_iter=10)), \n            ('scaler', StandardScaler()), # standardisation\n             ])\n\n    # pipelines features numériques et catégorielles\n    preprocessor = ColumnTransformer(transformers=[\n            ('cat', categorical_transformer, list(X.select_dtypes(include=['category', 'bool']).columns)),\n            ('num', numeric_transformer, list(X.select_dtypes(include='number').columns)),\n            ])\n    \n    return preprocessor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fonction préprocessing pour le SNE\ndef preprocessing_tSNE(dataframe, target_name='TARGET'):\n    \n    X = dataframe.copy()\n\n    # suppression lignes ou il manque la valeur cible\n    X = X.dropna(subset=[target_name])\n\n    # définition variable cible\n    y = X[target_name]\n\n    # retire variable cible des feautures interprétées\n    X = X.drop(columns=[target_name])\n\n    # applique la fonction de préprocessing\n    preprocessor = define_preprocessor(X)\n\n    # applique process\n    X_std = preprocessor.fit_transform(X, y)\n    \n    return (X_std, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fonction SNE, permet aussi l'affichage\ndef tSNE(dataframe, target_name='TARGET'):\n    \n    # tritement pour tSNE\n    (X_std, y) = preprocessing_tSNE(dataframe, target_name)\n\n    # Instanciation tSNE\n    tsne = manifold.TSNE(n_components=2,\n                         perplexity=30,\n                         n_iter=300,\n                         init='pca', # initialisation avec une PCA\n                         random_state=0\n                        )\n\n    # Applying tSNE\n    X_projected = tsne.fit_transform(X_std) \n    \n    # Affichage\n    plt.figure(figsize=(14,8))\n\n    # limites graphe\n    plt.xlim(X_projected[:,0].min()*1.1, X_projected[:,0].max()*1.1)\n    plt.ylim(X_projected[:,1].min()*1.1, X_projected[:,1].max()*1.1)\n\n    # définition des axes\n    plt.title(\"t-SNE\\n\", fontsize=20)\n    plt.xlabel(\"t-SNE feature 1\")\n    plt.ylabel(\"t-SNE feature 2\")\n\n    # Def nuages de points\n    sc = plt.scatter(X_projected[:,0], # x\n                 X_projected[:,1], #y\n                 c=y,\n                 cmap=plt.cm.get_cmap('RdYlGn_r'), # couleur\n                 marker='.'\n        )\n    \n    # configuration et échelle\n    cbar = plt.colorbar(sc)\n    cbar.ax.get_yaxis().set_ticks([])\n    cbar.ax.get_yaxis().labelpad = 15\n    cbar.set_label(target_name, rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage\ntSNE(data, target_name='Response')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"8\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## ACP"},{"metadata":{},"cell_type":"markdown","source":"On veut déterminer un nombre minimal de composantes à partir duquel on peut considérer que l'étude prédictive est fiable à partir d'un certain seuil (que je définis ici à 90%).\nIl est important de normaliser les données pour faire une PCA (pour la conservation de la distance vectorielle).\nLes données fournies ont déja été normalisée il n'est donc pas nécessaire de le faire ici. \n\nIl faudra néanmoins faire attention aux outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing  ACP\ndataframe = data\ntarget_name = 'Response'\n(X_std, y) = preprocessing_tSNE(dataframe, target_name)\n\n# Calcul des composantes principales\n\nn_components=2\npca = decomposition.PCA(n_components=n_components)\npca.fit(X_std)\n\nprint(\"Pourcentage variance expliquée par composante:\", pca.explained_variance_ratio_)\nprint(\"Pourcentage total variance expliquée:\", pca.explained_variance_ratio_.sum()) #  somme cumulée ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# représentation de l'ACP avec deux composantes\npca=PCA(n_components=2)\ncomponents=pca.fit_transform(X_std)\nfig = px.scatter(components, x=0, y=1, color=data.Response, title='Représentation PCA 2 components')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport plotly.graph_objects as go\ncomponents=pca.fit_transform(X_std)\nfig=go.Scattergl(components, x=0, y=1, color=data.Response, title='Représentation PCA 2 components')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# représentation de l'ACP avec 3 composantes\npca = PCA(n_components=3)\ncomponents = pca.fit_transform(X_std)\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter_3d(\n    components, x=0, y=1, z=2, color=data.Response,\n    title='PCA avec 3 composantes\\nTotal Explained Variance: {}%'.format(total_var),\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fonction explicative des valeur propres pour PCA\ndef display_scree_plot(X_std):\n    pca = decomposition.PCA()\n    pca.fit(X_std)\n    scree = pca.explained_variance_ratio_*100\n    \n    plt.bar(np.arange(len(scree))+1, scree)\n    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n    plt.xlabel(\"Quantité de composantes\")\n    plt.ylabel(\"pourcentage de variance cumulée\")\n    plt.title(\"Etude du seuil de variance en fonction du nombre de composantes pour la PCA\", fontsize=15)\n    plt.grid(linestyle='dotted')\n    plt.show(block=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# affichage valeurs propres PCA\ndisplay_scree_plot(X_std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fonction donne les composantes principales de l'ACP, jusqu'au seuil de variance\ndef PCA_features_reduction(X_std, var_threshold=0.9): \n    # PCA\n    pca = decomposition.PCA()\n    pca.fit(X_std)\n    \n    # ratio de variance expliqué pour chaque composante principale\n    scree = pca.explained_variance_ratio_\n    # rend le nombre de composants principaux pour atteindre les seuils de variance\n    mask = scree.cumsum() > var_threshold\n    nb_selected_features = len(scree[~mask]) + 1\n    print(\"Nombre de features selectionnées:\", nb_selected_features)\n    \n    # Calcul du ratio\n    explained_variance_sum = scree.cumsum()[nb_selected_features-1]\n    print(\"Valeur cumulée de variance expliquée:  {:.2f}%\".format(explained_variance_sum*100))\n    \n    # projection sur les 1ers composant\n    X_projected = pca.transform(X_std)[:,:nb_selected_features]\n    \n    return X_projected","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_projected = PCA_features_reduction(X_std, var_threshold=0.9)\nX_projected.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Le seuil de 90% de variance expliquée est atteint avec 78 composantes principales.\nOn avait initialement près de 120 variables."},{"metadata":{},"cell_type":"markdown","source":"On souhaite connaître ces 78 composantes identifiées par la PCA afin de refaire un modèle prédictif sur celles-ci."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tests"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Sommaire</a>\n## Tests infructueux"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_modified2 = train[train.columns[train.isnull().mean() <= 0.75]]\ntrain_modified2 = train.drop(cols_with_missing, axis=1)\n\ntrain_modified2 = train_modified2.loc[:, train_modified2.columns != 'Response']\ntrain_modified2 = train_modified2.loc[:, train_modified2.columns != 'Product_Info_2']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_modified2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" df_features2 = train_modified2.loc[:, train_modified2.columns != 'Response']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features2=list(df_features2.columns)\nX2=train_modified2[features2]\nprint(X2.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Bayes2 = MultinomialNB()\nBayes2.fit(X2,y)'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Bayes Gaussian Mixture"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.mixture import BayesianGaussianMixture\nBayes_mix = BayesianGaussianMixture()\nBayes_mix.fit(X,y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Bayes_mix.fit_predict((X,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_mix=pd.DataFrame(data=Bayes_mix.fit_predict(X))\npredict_mix.rename(columns={0:'Predicted'}, inplace=True)\npredict_mix['Response']=y\npredict_mix","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}