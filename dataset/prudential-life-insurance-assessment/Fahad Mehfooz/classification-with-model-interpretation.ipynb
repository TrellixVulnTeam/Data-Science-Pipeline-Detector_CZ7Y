{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Classification With Model Interpretation**  ðŸ’¯ ðŸ’¯","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-30T07:18:51.653688Z","iopub.execute_input":"2021-09-30T07:18:51.654028Z","iopub.status.idle":"2021-09-30T07:19:00.571233Z","shell.execute_reply.started":"2021-09-30T07:18:51.65394Z","shell.execute_reply":"2021-09-30T07:19:00.570246Z"},"_kg_hide-input":false,"_kg_hide-output":true}},{"cell_type":"markdown","source":"# **Importing Modules**\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import log_loss\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pprint import pprint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,r2_score\nimport warnings\nfrom mlxtend.classifier import StackingClassifier\nimport missingno as msno\nfrom sklearn.ensemble import VotingClassifier\nimport shap\nshap.initjs()\nimport lime\nfrom lime import lime_tabular\nwarnings.simplefilter('ignore')\nimport os\nplt.style.use('fivethirtyeight')\nplt.style.use('dark_background')\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:33:50.285122Z","iopub.execute_input":"2022-01-06T08:33:50.285632Z","iopub.status.idle":"2022-01-06T08:33:55.210635Z","shell.execute_reply.started":"2022-01-06T08:33:50.285527Z","shell.execute_reply":"2022-01-06T08:33:55.209956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Reading Data**","metadata":{}},{"cell_type":"code","source":"insurance_df = pd.read_csv('../input/prudential-life-insurance-assessment/train.csv.zip', index_col='Id')\ninsurance_df.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:33:55.211878Z","iopub.execute_input":"2022-01-06T08:33:55.21255Z","iopub.status.idle":"2022-01-06T08:33:56.082231Z","shell.execute_reply.started":"2022-01-06T08:33:55.212515Z","shell.execute_reply":"2022-01-06T08:33:56.081587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Shape**","metadata":{}},{"cell_type":"code","source":"insurance_df.shape","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:33:56.083225Z","iopub.execute_input":"2022-01-06T08:33:56.083605Z","iopub.status.idle":"2022-01-06T08:33:56.089642Z","shell.execute_reply.started":"2022-01-06T08:33:56.083577Z","shell.execute_reply":"2022-01-06T08:33:56.088604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Distribution of Target Variable**","metadata":{}},{"cell_type":"code","source":"insurance_df['Response'].value_counts()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:33:56.092106Z","iopub.execute_input":"2022-01-06T08:33:56.0929Z","iopub.status.idle":"2022-01-06T08:33:56.108798Z","shell.execute_reply.started":"2022-01-06T08:33:56.092844Z","shell.execute_reply":"2022-01-06T08:33:56.107828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Class imbalance can be seen here. Also there 8 categories, lets combine them to 3 categories**\n","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=insurance_df['Response']);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:33:56.110457Z","iopub.execute_input":"2022-01-06T08:33:56.111411Z","iopub.status.idle":"2022-01-06T08:33:56.35635Z","shell.execute_reply.started":"2022-01-06T08:33:56.111369Z","shell.execute_reply":"2022-01-06T08:33:56.355439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Response 8 has highest values and 3 has the least**","metadata":{}},{"cell_type":"markdown","source":"# **Processing Target Variable**","metadata":{}},{"cell_type":"code","source":"#Combining the Categores to 3 categories\ninsurance_df['Modified_Response']  = insurance_df['Response'].apply(lambda x : 0 if x<=7 and x>=0 else (1 if x==8 else -1))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:33:56.357716Z","iopub.execute_input":"2022-01-06T08:33:56.358053Z","iopub.status.idle":"2022-01-06T08:33:56.387002Z","shell.execute_reply.started":"2022-01-06T08:33:56.358012Z","shell.execute_reply":"2022-01-06T08:33:56.386173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x= insurance_df['Modified_Response']);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:33:56.388306Z","iopub.execute_input":"2022-01-06T08:33:56.388572Z","iopub.status.idle":"2022-01-06T08:33:56.561749Z","shell.execute_reply.started":"2022-01-06T08:33:56.388544Z","shell.execute_reply":"2022-01-06T08:33:56.56089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Still some imbalance can be seen**","metadata":{}},{"cell_type":"markdown","source":"# **Removing old target variable**","metadata":{}},{"cell_type":"code","source":"# Dropping old response columns\ninsurance_df.drop('Response',axis = 1, inplace=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:33:56.562953Z","iopub.execute_input":"2022-01-06T08:33:56.563203Z","iopub.status.idle":"2022-01-06T08:33:56.642968Z","shell.execute_reply.started":"2022-01-06T08:33:56.563177Z","shell.execute_reply":"2022-01-06T08:33:56.641853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Making categorical and numerical columns list**","metadata":{}},{"cell_type":"code","source":"# Making lists with categorical and numerical features.\ncategorical =  [col for col in insurance_df.columns if insurance_df[col].dtype =='object']\n\nnumerical = categorical =  [col for col in insurance_df.columns if insurance_df[col].dtype !='object']","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:33:56.644341Z","iopub.execute_input":"2022-01-06T08:33:56.644639Z","iopub.status.idle":"2022-01-06T08:33:56.655551Z","shell.execute_reply.started":"2022-01-06T08:33:56.644609Z","shell.execute_reply":"2022-01-06T08:33:56.654442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualizations On Categorical Features**","metadata":{}},{"cell_type":"code","source":"# Doing count plots for categorical\nfor col in categorical:\n    counts = insurance_df[col].value_counts().sort_index()\n    if len(counts) > 10 and len(counts) < 50 :\n      fig = plt.figure(figsize=(30, 10))\n    elif len(counts) >50 :\n      continue\n    else:\n      fig = plt.figure(figsize=(9, 6))\n    ax = fig.gca()\n    counts.plot.bar(ax = ax, color='steelblue')\n    ax.set_title(col + ' counts')\n    ax.set_xlabel(col) \n    ax.set_ylabel(\"Frequency\")\nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-06T08:33:56.658048Z","iopub.execute_input":"2022-01-06T08:33:56.658735Z","iopub.status.idle":"2022-01-06T08:34:13.933835Z","shell.execute_reply.started":"2022-01-06T08:33:56.658698Z","shell.execute_reply":"2022-01-06T08:34:13.932749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **D3 has the highest frequencies**\n\n> Most of the features here are unbalanced.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_1'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_1'], ax=axes[1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:13.935149Z","iopub.execute_input":"2022-01-06T08:34:13.935604Z","iopub.status.idle":"2022-01-06T08:34:14.679002Z","shell.execute_reply.started":"2022-01-06T08:34:13.935568Z","shell.execute_reply":"2022-01-06T08:34:14.678116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Right skewed.\n\n> Outliers can be seen.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_4'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_4'], ax=axes[1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:14.680493Z","iopub.execute_input":"2022-01-06T08:34:14.681014Z","iopub.status.idle":"2022-01-06T08:34:15.369446Z","shell.execute_reply.started":"2022-01-06T08:34:14.680968Z","shell.execute_reply":"2022-01-06T08:34:15.368439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Employment_Info_6'], ax=axes[0])\nsns.boxplot(insurance_df['Employment_Info_6'], ax=axes[1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:15.370851Z","iopub.execute_input":"2022-01-06T08:34:15.37126Z","iopub.status.idle":"2022-01-06T08:34:16.031288Z","shell.execute_reply.started":"2022-01-06T08:34:15.371218Z","shell.execute_reply":"2022-01-06T08:34:16.030242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize=(10,5))\nsns.distplot(insurance_df['Family_Hist_4'], ax=axes[0])\nsns.boxplot(insurance_df['Family_Hist_4'], ax=axes[1])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:16.033053Z","iopub.execute_input":"2022-01-06T08:34:16.033376Z","iopub.status.idle":"2022-01-06T08:34:16.66107Z","shell.execute_reply.started":"2022-01-06T08:34:16.033309Z","shell.execute_reply":"2022-01-06T08:34:16.660257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Checking Correlation For Features greater than .8**","metadata":{}},{"cell_type":"code","source":"# I just checked correlated feature with greater than .8 here \ncorr = insurance_df.corr()\ncorr_greater_than_80 = corr[corr>=.8]\ncorr_greater_than_80\n","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:16.662502Z","iopub.execute_input":"2022-01-06T08:34:16.66342Z","iopub.status.idle":"2022-01-06T08:34:19.266624Z","shell.execute_reply.started":"2022-01-06T08:34:16.663376Z","shell.execute_reply":"2022-01-06T08:34:19.265164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(corr_greater_than_80, cmap=\"Reds\");","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:19.268008Z","iopub.execute_input":"2022-01-06T08:34:19.268924Z","iopub.status.idle":"2022-01-06T08:34:20.392603Z","shell.execute_reply.started":"2022-01-06T08:34:19.268877Z","shell.execute_reply":"2022-01-06T08:34:20.391409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **CONCLUSION**\n\n> BMI and Weight are highly correlated, which makes sense also as these 2 features are directly proprtional.\n\n> Ins_Age and Family_Hist_4, Family_Hist_2 highly correlated\n\n> Although, I am not going to perform any transformation on any feature or drop any as these are tree based models and they don't get affected by correlation much because of their non parametric nature.","metadata":{}},{"cell_type":"code","source":"#setting max columns to 200\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 200)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:20.394721Z","iopub.execute_input":"2022-01-06T08:34:20.395099Z","iopub.status.idle":"2022-01-06T08:34:20.401489Z","shell.execute_reply.started":"2022-01-06T08:34:20.395061Z","shell.execute_reply":"2022-01-06T08:34:20.400096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Null Value Check**","metadata":{}},{"cell_type":"code","source":"#checking percentage of missing values in a column\nmissing_val_count_by_column = insurance_df.isnull().sum()/len(insurance_df)\n\nprint(missing_val_count_by_column[missing_val_count_by_column > 0.4].sort_values(ascending=False))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:20.403579Z","iopub.execute_input":"2022-01-06T08:34:20.40384Z","iopub.status.idle":"2022-01-06T08:34:20.437016Z","shell.execute_reply.started":"2022-01-06T08:34:20.403815Z","shell.execute_reply":"2022-01-06T08:34:20.436087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Removing unimportant column**","metadata":{}},{"cell_type":"code","source":"# Dropping all columns in which greater than 40 percent null values\ninsurance_df = insurance_df.dropna(thresh=insurance_df.shape[0]*0.4,how='all',axis=1)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:20.438239Z","iopub.execute_input":"2022-01-06T08:34:20.438995Z","iopub.status.idle":"2022-01-06T08:34:20.500977Z","shell.execute_reply.started":"2022-01-06T08:34:20.438931Z","shell.execute_reply":"2022-01-06T08:34:20.499342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Does not contain important information\ninsurance_df.drop('Product_Info_2',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T08:34:20.502747Z","iopub.execute_input":"2022-01-06T08:34:20.503715Z","iopub.status.idle":"2022-01-06T08:34:20.540147Z","shell.execute_reply.started":"2022-01-06T08:34:20.503657Z","shell.execute_reply":"2022-01-06T08:34:20.53894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **X and Y split**","metadata":{}},{"cell_type":"code","source":"# Data for all the independent variables\nX = insurance_df.drop(labels='Modified_Response',axis=1)\n\n# Data for the dependent variable\nY = insurance_df['Modified_Response']","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:20.541679Z","iopub.execute_input":"2022-01-06T08:34:20.54199Z","iopub.status.idle":"2022-01-06T08:34:20.58159Z","shell.execute_reply.started":"2022-01-06T08:34:20.541953Z","shell.execute_reply":"2022-01-06T08:34:20.579711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Filling Remaining Missing Values**","metadata":{}},{"cell_type":"code","source":"# Filling remaining missing values with mean\nX = X.fillna(X.mean())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:20.583205Z","iopub.execute_input":"2022-01-06T08:34:20.583564Z","iopub.status.idle":"2022-01-06T08:34:20.709334Z","shell.execute_reply.started":"2022-01-06T08:34:20.583531Z","shell.execute_reply":"2022-01-06T08:34:20.708073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Train Test Split**","metadata":{}},{"cell_type":"code","source":"# Train-test split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.25, random_state=1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:20.710643Z","iopub.execute_input":"2022-01-06T08:34:20.710896Z","iopub.status.idle":"2022-01-06T08:34:20.782266Z","shell.execute_reply.started":"2022-01-06T08:34:20.710869Z","shell.execute_reply":"2022-01-06T08:34:20.781234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Shapes of Train and Test Data**","metadata":{}},{"cell_type":"code","source":"# Check the shape of train dataset\nprint(X_train.shape,Y_train.shape)\n\n# Check the shape of test dataset\nprint(X_test.shape, Y_test.shape)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:20.783698Z","iopub.execute_input":"2022-01-06T08:34:20.783948Z","iopub.status.idle":"2022-01-06T08:34:20.790058Z","shell.execute_reply.started":"2022-01-06T08:34:20.783919Z","shell.execute_reply":"2022-01-06T08:34:20.789392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Some Important functions that I will be using throughout**","metadata":{}},{"cell_type":"code","source":"# Utility Functions\ndef check_scores(model, X_train, X_test ):\n  # Making predictions on train and test data\n\n  train_class_preds = model.predict(X_train)\n  test_class_preds = model.predict(X_test)\n\n\n  # Get the probabilities on train and test\n  train_preds = model.predict_proba(X_train)[:,1]\n  test_preds = model.predict_proba(X_test)[:,1]\n\n\n  # Calculating accuracy on train and test\n  train_accuracy = accuracy_score(Y_train,train_class_preds)\n  test_accuracy = accuracy_score(Y_test,test_class_preds)\n\n  print(\"The accuracy on train dataset is\", train_accuracy)\n  print(\"The accuracy on test dataset is\", test_accuracy)\n  print()\n  # Get the confusion matrices for train and test\n  train_cm = confusion_matrix(Y_train,train_class_preds)\n  test_cm = confusion_matrix(Y_test,test_class_preds )\n\n  print('Train confusion matrix:')\n  print( train_cm)\n  print()\n  print('Test confusion matrix:')\n  print(test_cm)\n  print()\n\n  # Get the roc_auc score for train and test dataset\n  train_auc = roc_auc_score(Y_train,train_preds)\n  test_auc = roc_auc_score(Y_test,test_preds)\n\n  print('ROC on train data:', train_auc)\n  print('ROC on test data:', test_auc)\n  \n  # Fscore, precision and recall on test data\n  f1 = f1_score(Y_test, test_class_preds)\n  precision = precision_score(Y_test, test_class_preds)\n  recall = recall_score(Y_test, test_class_preds) \n  \n  \n  #R2 score on train and test data\n  train_log = log_loss(Y_train,train_preds)\n  test_log = log_loss(Y_test, test_preds)\n\n  print()\n  print('Train log loss:', train_log)\n  print('Test log loss:', test_log)\n  print()\n  print(\"F score is:\",f1 )\n  print(\"Precision is:\",precision)\n  print(\"Recall is:\", recall)\n  return model, train_auc, test_auc, train_accuracy, test_accuracy,f1, precision,recall, train_log, test_log\n\n\ndef check_importance(model, X_train):\n  #Checking importance of features\n  importances = model.feature_importances_\n  \n  #List of columns and their importances\n  importance_dict = {'Feature' : list(X_train.columns),\n                    'Feature Importance' : importances}\n  #Creating a dataframe\n  importance_df = pd.DataFrame(importance_dict)\n  \n  #Rounding it off to 2 digits as we might get exponential numbers\n  importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)\n  return importance_df.sort_values(by=['Feature Importance'],ascending=False)\n\ndef grid_search(model, parameters, X_train, Y_train):\n  #Doing a grid\n  grid = GridSearchCV(estimator=model,\n                       param_grid = parameters,\n                       cv = 2, verbose=2, scoring='roc_auc')\n  #Fitting the grid \n  grid.fit(X_train,Y_train)\n  print()\n  print()\n  # Best model found using grid search\n  optimal_model = grid.best_estimator_\n  print('Best parameters are: ')\n  pprint( grid.best_params_)\n\n  return optimal_model\n\n\n\n# This function will show how a feature is pushing towards 0 or 1\ndef interpret_with_lime(model, X_test):\n  # New data\n  interpretor = lime_tabular.LimeTabularExplainer(\n    training_data=np.array(X_train),\n    feature_names=X_train.columns,\n    mode='classification')\n  \n\n  exp = interpretor.explain_instance(\n      data_row=X_test.iloc[10], \n      predict_fn=model.predict_proba\n  )\n\n  exp.show_in_notebook(show_table=True)\n\n# This gives feature importance\ndef plot_feature_importance(model, X_train):\n  # PLotting features vs their importance factors\n  fig = plt.figure(figsize = (15, 8))\n  \n  # Extracting importance values\n  values =check_importance(model, X_train)[check_importance(model, X_train)['Feature Importance']>0]['Feature Importance'].values\n  \n  \n  # Extracting importance features\n  features = check_importance(model, X_train)[check_importance(model, X_train)['Feature Importance']>0]['Feature'].values\n\n  plt.bar(features, values, color ='blue',\n          width = 0.4)\n  plt.xticks( rotation='vertical')\n  plt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-01-06T08:34:20.791924Z","iopub.execute_input":"2022-01-06T08:34:20.792397Z","iopub.status.idle":"2022-01-06T08:34:20.814661Z","shell.execute_reply.started":"2022-01-06T08:34:20.792234Z","shell.execute_reply":"2022-01-06T08:34:20.813436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Random Forest**\n","metadata":{}},{"cell_type":"code","source":"\n# Number of trees\nn_estimators = [50,80,100]\n\n# Maximum depth of trees\nmax_depth = [4,6,8]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [50,100,150]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [40,50]\n\n# Hyperparameter Grid\nrf_parameters = {'n_estimators' : n_estimators,\n              'max_depth' : max_depth,\n              'min_samples_split' : min_samples_split,\n              'min_samples_leaf' : min_samples_leaf}\n\npprint(rf_parameters)\n\n#finding the best model\nrf_optimal_model = grid_search(RandomForestClassifier(), rf_parameters, X_train, Y_train)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-06T08:34:20.815808Z","iopub.execute_input":"2022-01-06T08:34:20.816452Z","iopub.status.idle":"2022-01-06T08:36:49.721665Z","shell.execute_reply.started":"2022-01-06T08:34:20.816283Z","shell.execute_reply":"2022-01-06T08:36:49.72062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting scores from all the metrices\nrf_model, rf_train_auc, rf_test_auc, rf_train_accuracy, rf_test_accuracy,rf_f1, rf_precision,rf_recall,rf_train_log, rf_test_log = check_scores(rf_optimal_model, X_train, X_test )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:36:49.722856Z","iopub.execute_input":"2022-01-06T08:36:49.723072Z","iopub.status.idle":"2022-01-06T08:36:51.114747Z","shell.execute_reply.started":"2022-01-06T08:36:49.723047Z","shell.execute_reply":"2022-01-06T08:36:51.113962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Importance For Random Forest**","metadata":{}},{"cell_type":"code","source":"# Getting the feature importance for all the features\ncheck_importance(rf_model, X_train)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:36:51.119737Z","iopub.execute_input":"2022-01-06T08:36:51.120014Z","iopub.status.idle":"2022-01-06T08:36:51.163542Z","shell.execute_reply.started":"2022-01-06T08:36:51.119985Z","shell.execute_reply":"2022-01-06T08:36:51.162512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Plotting only those features which are contributing something**\n","metadata":{}},{"cell_type":"code","source":"# PLotting only those features which are contributing something\nplot_feature_importance(rf_model, X_train)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:36:51.164905Z","iopub.execute_input":"2022-01-06T08:36:51.165219Z","iopub.status.idle":"2022-01-06T08:36:51.536811Z","shell.execute_reply.started":"2022-01-06T08:36:51.165181Z","shell.execute_reply":"2022-01-06T08:36:51.536061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **CONCLUSION:**\n\n> BMI, weight, Medical_History_23, Medical_History_4 and Medical_Keyword_15 seems to be important features according to random forest.\n\n> Also, only these features are contributing to the model prediction. Some features can be elmininated which are not contributing on further investigation.","metadata":{}},{"cell_type":"markdown","source":"## **Model Interpretability For Random Forest**\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"### **Using Lime**","metadata":{}},{"cell_type":"code","source":"\n# Interpretting the model using lime\ninterpret_with_lime(rf_model,X_test)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:36:51.537756Z","iopub.execute_input":"2022-01-06T08:36:51.538071Z","iopub.status.idle":"2022-01-06T08:37:13.517146Z","shell.execute_reply.started":"2022-01-06T08:36:51.538046Z","shell.execute_reply":"2022-01-06T08:37:13.516146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Using Shap**","metadata":{}},{"cell_type":"code","source":"\n# Interpretting the model using shaply\nX_shap=X_train\n\nrf_explainer = shap.TreeExplainer(rf_model)\nrf_shap_values = rf_explainer.shap_values(X_shap)\nshap.summary_plot(rf_shap_values, X_shap, plot_type=\"bar\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:37:13.518707Z","iopub.execute_input":"2022-01-06T08:37:13.519247Z","iopub.status.idle":"2022-01-06T08:38:56.250058Z","shell.execute_reply.started":"2022-01-06T08:37:13.519203Z","shell.execute_reply":"2022-01-06T08:38:56.249049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Findings**\n\n> Medical keyword 15,medical history 9,  Wt, medical history 3 all pushing towards 1.\n\n> Orange ones are pusing towards 1.","metadata":{}},{"cell_type":"markdown","source":"## **Dependence Plots**","metadata":{}},{"cell_type":"code","source":"# Plotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Wt','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, rf_shap_values[0], X_train)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:38:56.251186Z","iopub.execute_input":"2022-01-06T08:38:56.25142Z","iopub.status.idle":"2022-01-06T08:39:11.66423Z","shell.execute_reply.started":"2022-01-06T08:38:56.251394Z","shell.execute_reply":"2022-01-06T08:39:11.663392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Findings**\n\n> With high medical history 23 and low bmi we get class 1","metadata":{}},{"cell_type":"markdown","source":"# **Gradient Boosting**\n\n\n","metadata":{}},{"cell_type":"code","source":"#finding the best model\ngb_parameters ={\n    \"n_estimators\":[5,50,250],\n    \"max_depth\":[1,3,5,7],\n    \"learning_rate\":[0.01,0.1,1]\n}\n\npprint(gb_parameters)\n\ngb_optimal_model = grid_search(GradientBoostingClassifier(), gb_parameters, X_train, Y_train)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-06T08:39:11.66523Z","iopub.execute_input":"2022-01-06T08:39:11.66547Z","iopub.status.idle":"2022-01-06T08:55:32.649999Z","shell.execute_reply.started":"2022-01-06T08:39:11.665444Z","shell.execute_reply":"2022-01-06T08:55:32.648944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Importance For Gradient Boosting**","metadata":{}},{"cell_type":"code","source":"# Getting the scpres for all the score metrics used here\ngb_model, gb_train_auc, gb_test_auc, gb_train_accuracy, gb_test_accuracy,gb_f1, gb_precision,gb_recall,gb_train_log, gb_test_log = check_scores(gb_optimal_model, X_train, X_test )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:55:32.651337Z","iopub.execute_input":"2022-01-06T08:55:32.651598Z","iopub.status.idle":"2022-01-06T08:55:34.188018Z","shell.execute_reply.started":"2022-01-06T08:55:32.65157Z","shell.execute_reply":"2022-01-06T08:55:34.18694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting feature importance\ncheck_importance(gb_model, X_train)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:55:34.189671Z","iopub.execute_input":"2022-01-06T08:55:34.18998Z","iopub.status.idle":"2022-01-06T08:55:34.216064Z","shell.execute_reply.started":"2022-01-06T08:55:34.18994Z","shell.execute_reply":"2022-01-06T08:55:34.215122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PLotting only those features which are contributing something\nplot_feature_importance(gb_model, X_train)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:55:34.217659Z","iopub.execute_input":"2022-01-06T08:55:34.21797Z","iopub.status.idle":"2022-01-06T08:55:34.548773Z","shell.execute_reply.started":"2022-01-06T08:55:34.21793Z","shell.execute_reply":"2022-01-06T08:55:34.547845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **CONCLUSION:**\n\n> BMI, weight, Medical_History_23, Medical_History_4 and Medical_Keyword_15 seems to be the most important 5 features according to Gradient boosting.\n","metadata":{}},{"cell_type":"markdown","source":"# **Model Interpretability For Gradient Boosting**","metadata":{}},{"cell_type":"markdown","source":"### **Using Lime**","metadata":{}},{"cell_type":"code","source":"# Interpretting the model using lime\ninterpret_with_lime(gb_model,X_test)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:55:34.550488Z","iopub.execute_input":"2022-01-06T08:55:34.5508Z","iopub.status.idle":"2022-01-06T08:55:56.281445Z","shell.execute_reply.started":"2022-01-06T08:55:34.55076Z","shell.execute_reply":"2022-01-06T08:55:56.280384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Using Shap**","metadata":{}},{"cell_type":"code","source":"\n# Interpretting the model using shaply\nX_shap=X_train\n\ngb_explainer = shap.TreeExplainer(gb_model)\ngb_shap_values = gb_explainer.shap_values(X_shap)\nshap.summary_plot(gb_shap_values, X_shap, plot_type=\"dot\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:55:56.283408Z","iopub.execute_input":"2022-01-06T08:55:56.284146Z","iopub.status.idle":"2022-01-06T08:56:53.482532Z","shell.execute_reply.started":"2022-01-06T08:55:56.284087Z","shell.execute_reply":"2022-01-06T08:56:53.481446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Findings**\n> BMI is pushing models prediction towards 0.\n\n>Medical keyword 15 is pushing towards 1. However, medical keyword 4 is pushing towards 0.\n\n> Also, according to feature plot Wt. was in top 5 most important features, same isn't followed here.","metadata":{}},{"cell_type":"markdown","source":"## **Dependence Plots**","metadata":{}},{"cell_type":"code","source":"#PLotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Product_Info_4','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, gb_shap_values, X_train)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:56:53.484069Z","iopub.execute_input":"2022-01-06T08:56:53.484397Z","iopub.status.idle":"2022-01-06T08:57:08.632156Z","shell.execute_reply.started":"2022-01-06T08:56:53.484359Z","shell.execute_reply":"2022-01-06T08:57:08.63121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Findings**\n\n> For low BMI and high medical history 23 we get class as 1.\n\n","metadata":{}},{"cell_type":"markdown","source":"# **XGBOOST**","metadata":{}},{"cell_type":"code","source":"# Parameter grid for xgboost\nxgb_parameters = {'max_depth': [1,3,5], 'n_estimators': [2,5,10], 'learning_rate': [.01 , .1, .5]}\nprint('XGB parameters areL:')\npprint(xgb_parameters)\n#finding the best model\nxgb_optimal_model = grid_search(XGBClassifier(), xgb_parameters, X_train, Y_train)\n\n\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-06T08:57:08.633564Z","iopub.execute_input":"2022-01-06T08:57:08.633889Z","iopub.status.idle":"2022-01-06T08:57:25.815974Z","shell.execute_reply.started":"2022-01-06T08:57:08.633848Z","shell.execute_reply":"2022-01-06T08:57:25.81494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the scores for all the score metrics used here\nxgb_model, xgb_train_auc, xgb_test_auc, xgb_train_accuracy, xgb_test_accuracy,xgb_f1, xgb_precision,xgb_recall,xgb_train_log, xgb_test_log= check_scores(xgb_optimal_model, X_train, X_test )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:57:25.817152Z","iopub.execute_input":"2022-01-06T08:57:25.817397Z","iopub.status.idle":"2022-01-06T08:57:26.082669Z","shell.execute_reply.started":"2022-01-06T08:57:25.81737Z","shell.execute_reply":"2022-01-06T08:57:26.081721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Importance For XGBoost**","metadata":{}},{"cell_type":"code","source":"# Getting feature importance\n\ncheck_importance(xgb_model, X_train)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:57:26.084037Z","iopub.execute_input":"2022-01-06T08:57:26.084418Z","iopub.status.idle":"2022-01-06T08:57:26.113998Z","shell.execute_reply.started":"2022-01-06T08:57:26.084379Z","shell.execute_reply":"2022-01-06T08:57:26.113081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Conclusion:**\n\n> Same trend is seen here.\n\n> They all are giving similar scores also so it could be that same features are contributing the most thus similar scores.\n","metadata":{}},{"cell_type":"markdown","source":"# **Model Interpretability for XGBoost**","metadata":{}},{"cell_type":"markdown","source":"## **Using Shap**","metadata":{}},{"cell_type":"code","source":"\n# Interpretting the model using shaply\n\nxgb_explainer = shap.TreeExplainer(xgb_model)\nxgb_shap_values = xgb_explainer.shap_values(X_shap)\nshap.summary_plot(xgb_shap_values, X_shap, plot_type=\"dot\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:57:26.115146Z","iopub.execute_input":"2022-01-06T08:57:26.11551Z","iopub.status.idle":"2022-01-06T08:57:35.000867Z","shell.execute_reply.started":"2022-01-06T08:57:26.115478Z","shell.execute_reply":"2022-01-06T08:57:35.000236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Again BMI is pushing towards class 0.\n\n> MEdical history 4 pushing towards class 1.","metadata":{}},{"cell_type":"markdown","source":"## **Dependence Plots**","metadata":{}},{"cell_type":"code","source":"#PLotting for top 5 features\ntop_vars = ['BMI','Medical_Keyword_15','Medical_History_4','Product_Info_4','Medical_History_23']\nindex_top_vars =[list(X_train.columns).index(var) for var in top_vars]\n\nfor elem in index_top_vars:\n    shap.dependence_plot(elem, xgb_shap_values, X_train)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T08:57:35.001748Z","iopub.execute_input":"2022-01-06T08:57:35.002388Z","iopub.status.idle":"2022-01-06T08:57:52.09626Z","shell.execute_reply.started":"2022-01-06T08:57:35.002357Z","shell.execute_reply":"2022-01-06T08:57:52.095442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> For product info 4 and wt we see some interesting trend","metadata":{}},{"cell_type":"markdown","source":"# **Logistic Regression**","metadata":{}},{"cell_type":"code","source":"\n# Parameter grid for Logistic Regression\nsolvers = ['lbfgs']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\nlr_parameters = dict(solver=solvers,penalty=penalty,C=c_values)# define grid search\n\n#finding the best model\nlr_optimal_model = grid_search(LogisticRegression( max_iter=5000), lr_parameters, X_train, Y_train)\n\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-06T08:57:52.097839Z","iopub.execute_input":"2022-01-06T08:57:52.098304Z","iopub.status.idle":"2022-01-06T09:01:59.964282Z","shell.execute_reply.started":"2022-01-06T08:57:52.098261Z","shell.execute_reply":"2022-01-06T09:01:59.963436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting the scores for all the score metrics used here\n\nlr_model, lr_train_auc, lr_test_auc, lr_train_accuracy, lr_test_accuracy,lr_f1, lr_precision, lr_recall,lr_train_log, lr_test_log = check_scores(lr_optimal_model, X_train, X_test )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T09:01:59.965671Z","iopub.execute_input":"2022-01-06T09:01:59.966274Z","iopub.status.idle":"2022-01-06T09:02:00.252211Z","shell.execute_reply.started":"2022-01-06T09:01:59.96623Z","shell.execute_reply":"2022-01-06T09:02:00.251296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Importance For Logistic Regression**","metadata":{}},{"cell_type":"code","source":"# Making a dataframe with coefficients and the feature names respectively\nimportance_df_lr = pd.concat([ pd.DataFrame(data =((X_train.columns).values).reshape(-1,1), columns = ['Feature']), pd.DataFrame(data =np.round(lr_optimal_model.coef_,2).reshape(-1,1), columns = ['Feature Importance'])], axis=1 )\nimportance_df_lr.sort_values(by=['Feature Importance'],ascending=False, inplace = True)\nimportance_df_lr","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T09:02:00.253648Z","iopub.execute_input":"2022-01-06T09:02:00.254153Z","iopub.status.idle":"2022-01-06T09:02:00.281308Z","shell.execute_reply.started":"2022-01-06T09:02:00.254107Z","shell.execute_reply":"2022-01-06T09:02:00.280357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting feature vs importance\nfig = plt.figure(figsize = (15, 8))\n\nvalues =importance_df_lr[importance_df_lr['Feature Importance']>0]['Feature Importance'].values\n\nfeatures = importance_df_lr[importance_df_lr['Feature Importance']>0]['Feature'].values\n\nplt.bar(features, values, color ='blue',\n          width = 0.4)\nplt.xticks( rotation='vertical')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T09:02:00.282667Z","iopub.execute_input":"2022-01-06T09:02:00.283206Z","iopub.status.idle":"2022-01-06T09:02:01.506631Z","shell.execute_reply.started":"2022-01-06T09:02:00.283163Z","shell.execute_reply":"2022-01-06T09:02:01.505725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Conclusion**\n\n> **And again the same pattern when doing feature importance**","metadata":{}},{"cell_type":"markdown","source":"# **Model Interpretability for logistic regression**","metadata":{}},{"cell_type":"markdown","source":"## **Using Lime**","metadata":{}},{"cell_type":"code","source":"\n# Interpretting the model using lime\ninterpret_with_lime(lr_model,X_test)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T09:02:01.507881Z","iopub.execute_input":"2022-01-06T09:02:01.508176Z","iopub.status.idle":"2022-01-06T09:02:23.121779Z","shell.execute_reply.started":"2022-01-06T09:02:01.508138Z","shell.execute_reply":"2022-01-06T09:02:23.120835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Findings**\n\n> Only BMI and medical history 4 pushing towards class 0","metadata":{}},{"cell_type":"markdown","source":"# **Max Voting Model**","metadata":{}},{"cell_type":"code","source":"# Appending all the models to estimators list\nestimators = []\n\nestimators.append(('logistic', lr_optimal_model))\nestimators.append(('XGB', xgb_optimal_model))\nestimators.append(('GB', gb_optimal_model))\nestimators.append(('rf', rf_optimal_model))\n\n# create the voting model\nvoting_model = VotingClassifier(estimators, voting='soft')\n\nvoting_model.fit(X_train, Y_train)\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-06T09:02:23.12392Z","iopub.execute_input":"2022-01-06T09:02:23.124587Z","iopub.status.idle":"2022-01-06T09:04:26.283728Z","shell.execute_reply.started":"2022-01-06T09:02:23.124537Z","shell.execute_reply":"2022-01-06T09:04:26.282892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting all the scores and errors\nvoting_model, voting_train_auc, voting_test_auc, voting_train_accuracy, voting_test_accuracy, voting_f1, voting_precision, voting_recall, voting_train_log, voting_test_log = check_scores(voting_model, X_train, X_test )","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T09:04:26.285353Z","iopub.execute_input":"2022-01-06T09:04:26.285896Z","iopub.status.idle":"2022-01-06T09:04:29.529731Z","shell.execute_reply.started":"2022-01-06T09:04:26.285853Z","shell.execute_reply":"2022-01-06T09:04:29.529064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Stacked Model**","metadata":{}},{"cell_type":"code","source":"#Building a stacked classifier\nstacked_classifier = StackingClassifier(classifiers =[lr_optimal_model, xgb_optimal_model, gb_model], meta_classifier = RandomForestClassifier(), use_probas = True, use_features_in_secondary = True)\n\n# training of stacked model\nstacked_model = stacked_classifier.fit(X_train, Y_train)   \n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-06T09:04:29.531047Z","iopub.execute_input":"2022-01-06T09:04:29.53156Z","iopub.status.idle":"2022-01-06T09:06:40.96553Z","shell.execute_reply.started":"2022-01-06T09:04:29.531522Z","shell.execute_reply":"2022-01-06T09:06:40.96452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked_model, stacked_train_auc, stacked_test_auc, stacked_train_accuracy, stacked_test_accuracy, stacked_f1, stacked_precision, stacked_recall, stacked_train_log, stacked_test_log = check_scores(stacked_model, X_train, X_test )","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-01-06T09:06:40.966653Z","iopub.execute_input":"2022-01-06T09:06:40.966923Z","iopub.status.idle":"2022-01-06T09:06:45.886844Z","shell.execute_reply.started":"2022-01-06T09:06:40.966893Z","shell.execute_reply":"2022-01-06T09:06:45.885889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Models And Their Accuracies**","metadata":{}},{"cell_type":"code","source":"# Making a dataframe of all the scores for every model\n\nscores_ = [(\"Random Forest\", rf_train_auc, rf_test_auc, rf_train_accuracy, rf_test_accuracy,rf_train_log, rf_test_log,rf_f1, rf_precision, rf_recall),\n(\"Gradient Boosting\",  gb_train_auc, gb_test_auc, gb_train_accuracy, gb_test_accuracy,gb_train_log, gb_test_log,gb_f1, gb_precision,gb_recall,),\n(\"XG Boost\", xgb_train_auc, xgb_test_auc, xgb_train_accuracy, xgb_test_accuracy,xgb_train_log, xgb_test_log,xgb_f1, xgb_precision, xgb_recall),\n(\"Logistic Regression\", lr_train_auc, lr_test_auc, lr_train_accuracy, lr_test_accuracy,lr_train_log, lr_test_log,lr_f1, lr_precision, lr_recall,),\n(\"Voting Classifier\", voting_train_auc, voting_test_auc, voting_train_accuracy, voting_test_accuracy, voting_train_log, voting_test_log, voting_f1, voting_precision, voting_recall),\n(\"Stacked Model\", stacked_train_auc, stacked_test_auc, stacked_train_accuracy, stacked_test_accuracy, stacked_train_log, stacked_test_log, stacked_f1, stacked_precision, stacked_recall)]\n\nScores_ =pd.DataFrame(data = scores_, columns=['Model Name', 'Train ROC', 'Test ROC', 'Train Accuracy', 'Test Accuracy', 'Train Log Loss','Test Log Loss','F-Score', 'Precision','Recall',])\nScores_.set_index('Model Name', inplace = True)\n\nScores_\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-06T09:06:45.888083Z","iopub.execute_input":"2022-01-06T09:06:45.888363Z","iopub.status.idle":"2022-01-06T09:06:45.911257Z","shell.execute_reply.started":"2022-01-06T09:06:45.888313Z","shell.execute_reply":"2022-01-06T09:06:45.910597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Final Results**\n\n> **Gradient Boosting, Voting Classifier and Stacked models are performing really well. Their train and test errors and also the roc scores and f scores are really close and good.**","metadata":{}}]}