{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"# Importing the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport statsmodels.api as sm\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import r2_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\n# To ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc = pd.read_csv(\"C://Users/pchadha/Boosting_Kaggle_Practice/Prudential_Life_insurance/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test = pd.read_csv(\"C://Users/pchadha/Boosting_Kaggle_Practice/Prudential_Life_insurance/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.set_option('display.max_row', 1000)\npd.set_option('display.max_columns', 300)\npd.set_option('display.max_colwidth',5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc.Response.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# The 'Response' shows that potential customers have been classified into 8 categories\n# Most have been classified as level '8', followed by '6' and '7'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will first treat the columns (if any) with missing values "},{"metadata":{"trusted":false},"cell_type":"code","source":"pc.info(verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# It can be seen that there are 18 float type, 109 int type and 1 object type variables in the dataset. \n#Total columns are 128, where 'Id' and 'Response' will not be part of model learning as they are customer 'id' and 'Target' \n# values respectively ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cols4 = pc.select_dtypes(include=['int64']).columns.values\ncols5 = pc.select_dtypes(include = ['float64']).columns.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"round((pc.isnull().sum()/len(pc.index))*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# As we have lot of columns, will remove columns with above 50% missing values\ncol = []\nfor i in pc.columns:\n    if round((pc[i].isnull().sum()/len(pc.index))*100,2) >= 50:\n        col.append(i)\nprint(col)\nprint(len(col))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Removing these columns\npc = pc.drop(col, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"round((pc.isnull().sum()/len(pc.index))*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Focussing only on columns that have missing values\ncol = []\nfor i in pc.columns:\n    if round((pc[i].isnull().sum()/len(pc.index))*100,2) != 0:\n        col.append(i)\nprint(col)\nprint(len(col))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Analysing 'Employment_Info_1' feature\npc[\"Employment_Info_1\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Employment_Info_1\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Employment_Info_1\"].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# These are normalized values related employment history and since there's insignificant number of missing rows in this case,\n# will remove the missing rows rather than imputing them\npc = pc[~pd.isnull(pc[\"Employment_Info_1\"])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"round((pc[col].isnull().sum()/len(pc.index))*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Employment_Info_4\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Employment_Info_4\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see that '0' value dominates the distribution of values within this feature. Therefore, imputing value '0' \n# for missing values in this case\npc.loc[pd.isnull(pc[\"Employment_Info_4\"]),\"Employment_Info_4\"] = 0 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"round((pc[col].isnull().sum()/len(pc.index))*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc.index = pd.RangeIndex(1, len(pc.index) + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Employment_Info_6\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Employment_Info_6\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This feature has no dominant value so will use iterative imputer to fill the null values here\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Insurance_History_5\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Insurance_History_5\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, this feature too is normalized and has no dominant value. Therefore, here too will impute with use of Iterative imputer"},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Family_Hist_2\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Family_Hist_2\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again for similar reasons, will impute using Iterative Imputer"},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Family_Hist_4\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Family_Hist_4\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again for similar reasons, will impute using Iterative Imputer"},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Medical_History_1\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Medical_History_1\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, no one particular dominant value and even 'mean' is not suitable here as there's significant diference between the median and the mean. Will impute using Iterative Imputer function here as well"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"round((pc[col].isnull().sum()/len(pc.index))*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Before imputing the values, will first analyze the 'object' feature \npc[\"Product_Info_2\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that this is a categorical value that can be converted to numeric type using encoder. Will use 'labelencoder' to avoid creating multiple features using 'Dummy variables' as we already have lot of features."},{"metadata":{"trusted":false},"cell_type":"code","source":"lc = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Product_Info_2\"] = lc.fit_transform(pc[\"Product_Info_2\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Product_Info_2\"].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc[\"Product_Info_2\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Imputing values using Iterative Imputer\ncols2 = pc.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp = pd.DataFrame(IterativeImputer().fit_transform(pc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp.columns = cols2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"round((pc_imp[col].isnull().sum()/len(pc_imp.index))*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that all null values have been treated. However, side-effect of Iterative computer is that it converts all columns to float type while processing them. Will now convert the columns originally integer type to 'int' "},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp[cols4] = pc_imp[cols4].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp.info(verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp[\"Product_Info_2\"] = pc_imp[\"Product_Info_2\"].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing features and their relations using univariate and bivariate analysis"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Removing 'Id' variable as it will not be used for model learning\npc_imp = pc_imp.drop('Id', axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analysing few continuous features first\nContinuous variables are as follows:\nProduct_Info_4, Ins_Age, Ht, Wt, BMI, Employment_Info_1, Employment_Info_4, Employment_Info_6, Insurance_History_5, Family_Hist_2, Family_Hist_3, Family_Hist_4, Family_Hist_5"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Box plots for outlier analysis\ncol = [\"Product_Info_4\", \"Ins_Age\", \"Ht\", \"Wt\", \"BMI\"]\ncoln = [\"Employment_Info_1\", \"Employment_Info_4\", \"Employment_Info_6\", \"Insurance_History_5\"]\ncol2 = [\"Family_Hist_2\", \"Family_Hist_4\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(data = pc_imp[col], orient = 'v')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# We can see that there are few outliers in the case of features: 'Ht', 'Wt' and 'BMI'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (20,12))\nsns.boxplot(data = pc_imp[coln], orient = 'v')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above case, all features have outliers. Now all these variables are normalized so scaling is not required. \nHowever, will remove outliers using IQR for variables 'BMI', 'Employment_Info_6' to avoid data getting skewed for these variables "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Outlier removal for 'BMI' and 'Employment_Info_6'\nQ1= pc_imp['BMI'].quantile(0.5)\nQ3= pc_imp['BMI'].quantile(0.95)\nRange=Q3-Q1\nprint(Range)\npc_imp= pc_imp[(pc_imp['BMI'] >= Q1-1.5*Range) & (pc_imp['BMI'] <= Q3+1.5*Range) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Q1= pc_imp['Employment_Info_6'].quantile(0.5)\nQ3= pc_imp['Employment_Info_6'].quantile(0.95)\nRange=Q3-Q1\npc_imp= pc_imp[(pc_imp['Employment_Info_6'] >= Q1-1.5*Range) & (pc_imp['Employment_Info_6'] <= Q3+1.5*Range) ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (10,10))\nsns.boxplot(data = pc_imp[col2], orient = 'v')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Few outliers in the case of Family_Hist_2 and Family_Hist_4 features observed"},{"metadata":{"trusted":false},"cell_type":"code","source":"# pairplot analysis to understand correlation between continous variables\ncolc = [\"Product_Info_4\", \"Ins_Age\", \"Ht\", \"Wt\", \"BMI\", \"Employment_Info_1\", \"Employment_Info_4\", \"Employment_Info_6\", \"Insurance_History_5\", \"Family_Hist_2\",\"Family_Hist_4\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(pc_imp[colc])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the pair plots, we can see that few variables such as \"Family_Hist_2\",\"Ins_Age\" and \"Family_Hist_4\", \"BMI\", \"Wt\" and \"Ht\" are correlated to each other. \nWill now plot heatmap to understand correlation in details between these variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"colc = [\"Product_Info_4\", \"Ins_Age\", \"Ht\", \"Wt\", \"BMI\", \"Employment_Info_1\", \"Employment_Info_4\", \"Employment_Info_6\", \"Insurance_History_5\", \"Family_Hist_2\",\"Family_Hist_4\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (15, 15))\nsns.heatmap(pc_imp[colc].corr(), annot = True, square = True, cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen by the plots, correlation is observed between the same features. Since BMI is calculated using 'Wt' and 'Ht' details and so would be sufficient by itslef to help in identifying the risk level, will drop the 'Wt' and 'Ht' features. \n"},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp = pc_imp.drop([\"Ht\", \"Wt\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will now analyse 'Family_Hist_2', 'Ins_Age', 'Family_Hist_4' and 'BMI' w.r.t 'Response' variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x = \"Response\",y = \"Family_Hist_2\", data = pc_imp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x = \"Response\",y = \"Family_Hist_4\", data = pc_imp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x = \"Response\",y = \"Ins_Age\", data = pc_imp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.boxplot(x = \"Response\",y = \"BMI\", data = pc_imp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see from above plots that 'Family_Hist_2', 'Family_Hist_4' and 'Ins_Age' show similar distribution w.r.t 'Response' feature, with 'Family_Hist_2', 'Family_Hist_4' almost the same!. Therefore, can drop either of the features, will drop 'Family_Hist_4' for this iteration. \n'All' the features have least 'median' and 'max' (not considering outliers) values for 'Response' value -'8'\nThe distribution of the above analysed features is similar for 'Response' values -'3', '4' and '8' "},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp = pc_imp.drop(\"Family_Hist_4\", axis =1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Analyse now few categorical variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp[\"Product_Info_1\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp[\"Medical_History_8\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp[\"Medical_History_30\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp[\"Medical_History_1\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp[\"Medical_History_2\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_imp.info(verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Preparing the data in the 'test' dataset as well now","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# round((pc_test.isnull().sum()/len(pc_test.index))*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#First deleting the columns in 'test' dataset that have been deleted in the 'train' dataset\n#colt = ['Family_Hist_3', 'Family_Hist_5', 'Medical_History_10', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32','Ht','Wt','Family_Hist_4','Id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#pc_test_rev = pc_test.drop(colt, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#pc_test_rev.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# pc_test_rev.info(verbose = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model Building\nWill use Random Forest with default features and estimate performance parameters such as 'Precision', 'Accuracy', 'f-score', 'Recall' to judge the model performance\nThen will use GridSearch, along with Cross-validation, to tune few key parameters such as 'Max_depth', 'min_samples_leaf', 'n_estimators', 'max_features' and get the best possible result\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Splitting data between test and train data sets\npc_imp_train, pc_imp_test = train_test_split(pc_imp, train_size = 0.7, test_size = 0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pc_imp_train = pc_imp_train.pop(\"Response\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_pc_imp_train = pc_imp_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pc_imp_test = pc_imp_test.pop(\"Response\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_pc_imp_test = pc_imp_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_pc_imp_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_pc_imp_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will use PCA to see if dimensionality can be reduced to only the most important feature set"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Will now do PCA to reduce dimensionality\npca = PCA(svd_solver='randomized', random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pca.fit(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"colnames = list(X_pc_imp_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Dataframe with features and respective first two Prinicple components\npca_df = pd.DataFrame({'PC1':pca.components_[0], 'PC2':pca.components_[1], 'Features': colnames})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pca_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%matplotlib inline\nfig = plt.figure(figsize = (15,15))\nplt.scatter(pca_df.PC1, pca_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pca_df.Features):\n    plt.annotate(txt, (pca_df.PC1[i],pca_df.PC2[i]))\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that only two dimensions or features,  contrbute heavily in terms of variance along two principal components"},{"metadata":{"trusted":false},"cell_type":"code","source":"pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plotting the cummulative variance and number of PCs graph to identify the correct number of PCs required to explain 95% of variance\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Cummulative variance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen from the above plot too that PCA would reduce features to less than 10 as per the variance contribution. While this may be good in the ease of model calculation, we would loose certain important features that may not be important in terms of just considering variance, still would help in model being applicable over different set of data.\nTherefore, will not use PCA reduced feature/dimension set and instead, use the in-build feature filtering within RandomForest model to avoid multi-collinearity  "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Base Random Forest result\nrfc=RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rfc.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Tr_predict = rfc.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's check the report of our default model\nprint(classification_report(y_pc_imp_train,Tr_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Printing confusion matrix\nprint(confusion_matrix(y_pc_imp_train,Tr_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scores achieved are all quite high but also show strong signs of overfitting. Lets see the results on test data"},{"metadata":{"trusted":false},"cell_type":"code","source":"test_predict = rfc.predict(X_pc_imp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's check the report of default model on test dataset\nprint(classification_report(y_pc_imp_test,test_predict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As per the apprehension, model is overfitted as the scores on test dataset are quite poor. \nRandomforest usually do not overfit but we used default model without any tuninng so overfitting was not avoided.\nWe will now tune the model for following hyper parameters:\nMax_depth\nn_estimators\nMax_features\nmin_smaples_leaf\n\nWill use grid search to estimate appropriate hyper parameters to get to best possible model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Max depth tuning using CV an\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(10, 80, 5)}\n\n# instantiate the base model\nrf_m = RandomForestClassifier()\n\n\n# fit tree on training data\nrf_m = GridSearchCV(rf_m, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                    return_train_score=True)\nrf_m.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf_m.cv_results_\n# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best 'test' score is observed at 'max_depth' of 15. Lets now tune other hyper parameters"},{"metadata":{"trusted":false},"cell_type":"code","source":"# n_estimators tuning using CV and keeping tuned 'max_depth' of 15\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'n_estimators': range(300, 2400, 300)}\n\n# instantiate the base model\nrf_e = RandomForestClassifier(max_depth=15)\n\n\n# fit tree on training data\nrf_e = GridSearchCV(rf_e, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                    return_train_score=True)\nrf_e.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sc = pd.DataFrame(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf_e.cv_results_\n# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_n_estimators\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above graph, it can be observed that 'n_estimators' value of above '1250' seem to be most appropriate as the 'test' accuracy' is maximum at this value. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# tuning Max_features\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_features': [10,20,30,40,50]}\n\n# instantiate the model\nrf_mx = RandomForestClassifier(max_depth=15, n_estimators = 1250)\n\n\n# fit tree on training data\nrf_mx = GridSearchCV(rf_mx, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                    return_train_score=True)\nrf_mx.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scores = rf_mx.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sc = pd.DataFrame(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's observed that the 'target' accuracy becomes maximum at about 30 'max_features' and difference between 'training' and 'test' accuracy scores are minimum at this point too.  Therefore, will consider 'max_features' to be 30. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# tuning min_samples_leaf\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(50, 1000, 50)}\n\n# instantiate the model\nrf_sl = RandomForestClassifier(max_depth=15, max_features = 30, n_estimators = 1250)\n\n\n# fit tree on training data\nrf_sl = GridSearchCV(rf_sl, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                    return_train_score=True)\nrf_sl.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scores = rf_sl.cv_results_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sc = pd.DataFrame(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will first consider min_samples_leaf value of 800 as both of the accuracy scores are first closest at this point. However, it can be seen that test and train accuracy scores are best with 'min_sample_leaf' values being around 50. So will consider value to be 50 or lower in case model does not give optimum results with following parameters:"},{"metadata":{},"cell_type":"markdown","source":"We have the following ideal values of the hyper parameters that were selected for tuning:\n1. max_depth: 15\n2. n_estimators: 1250\n3. max_features: 30\n4. min_sample_leaf: 800\n\nWill now create the model with these parameters and analyse the result"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f = RandomForestClassifier(max_depth=15, n_estimators = 1250, max_features = 30, min_samples_leaf = 800)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred = rf_f.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_train,rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_train, rf_pred, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_train, rf_pred, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model with parameters selected is not giving good results on train data itself. Lets try now with lower values of 'min_sample_leaf'"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f = RandomForestClassifier(max_depth=15, n_estimators = 1600, max_features = 40, min_samples_leaf = 40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred = rf_f.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_train,rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_train, rf_pred, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_train, rf_pred, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance of this model is above 55% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred = rf_f.predict(X_pc_imp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_test,rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_test, rf_pred, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_test, rf_pred, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model performance for test data is quite better for this model as compared to original model. Still, will try few more models and see if the gap between test and train results can be reduced further"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_1 = RandomForestClassifier(max_depth=15, n_estimators = 1600, max_features = 50, min_samples_leaf = 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_1.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred = rf_f_1.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_train,rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_train, rf_pred, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_train, rf_pred, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance of this model is about 57% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_test = rf_f_1.predict(X_pc_imp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_test,rf_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Results are bit closer for train and test data. However, the precision score is less than 50% which needs to be better. Lets try and optimize model bit more"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_2 = RandomForestClassifier(max_depth=15, n_estimators = 1600, max_features = 50, min_samples_leaf = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_2.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_train_2 = rf_f_2.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_train,rf_pred_train_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_2, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_2, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance of this model is near 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_test_2 = rf_f_2.predict(X_pc_imp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_test,rf_pred_test_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_2, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_2, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, the scores of test and train are still not close enough for model to be considered an optimized one. Will try one last model with use of 'min_samples_leaf'. If model performance is not better, will not set the value of 'min_samples_leaf'"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_3 = RandomForestClassifier(max_depth=15, n_estimators = 1800, max_features = 20, min_samples_leaf = 25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_3.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_train_3 = rf_f_3.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_train,rf_pred_train_3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_3, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_3, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance of this model is again near 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_test_3 = rf_f_3.predict(X_pc_imp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_test,rf_pred_test_3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_3, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_3, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, there's not much improvement in the model. Will now run the model without setting the parameter 'min_samples_leaf'"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_4 = RandomForestClassifier(max_depth=15, n_estimators = 1800, max_features = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_4.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_train_4 = rf_f_4.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_train,rf_pred_train_4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_4, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_4, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance of this model is lot better with scores around 75% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_test_4 = rf_f_4.predict(X_pc_imp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_test,rf_pred_test_4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_4, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_4, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test scores are best of any model so far but the difference between the train and test scores is quite huge. Will now select larger value of 'min_samples_leaf' than the default '1' to reduce overfitting"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_5 = RandomForestClassifier(max_depth=15, n_estimators = 1800, max_features = 20, min_samples_leaf = 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_5.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_train_5 = rf_f_5.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_train,rf_pred_train_5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_5, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_5, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance of this model is decent with scores around 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data and see if test scores are around the 'train' data scores"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_test_5 = rf_f_5.predict(X_pc_imp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_test,rf_pred_test_5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_5, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_5, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The test and train scores are closer to each other, with test scores being above 50%. Lets try optimizing further"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_6 = RandomForestClassifier(max_depth=15, n_estimators = 1800, max_features = 20, min_samples_leaf = 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_6.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_train_6 = rf_f_6.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_train,rf_pred_train_6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_6, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_6, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance of this model is decent with scores around 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data and see if test scores are around the 'train' data scores"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_test_6 = rf_f_6.predict(X_pc_imp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_test,rf_pred_test_6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_6))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_6, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_6, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both train and test scores are closer with test scores above 50%. Will execute one more model with 'min_leaf_samples' less than 10, i.e., 8, and then select the best model out of the lot so far"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_7 = RandomForestClassifier(max_depth=15, n_estimators = 1800, max_features = 20, min_samples_leaf = 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_7.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_train_7 = rf_f_7.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_train,rf_pred_train_7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_7, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_7, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance of this model is decent with scores around 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data and see if test scores are around the 'train' data scores"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_test_7 = rf_f_7.predict(X_pc_imp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_test,rf_pred_test_7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_7, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_7, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far the last three models have given similar performances. Will now see if we can get better performance by optimizing 'Max_depth' now  "},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_8 = RandomForestClassifier(max_depth=20, n_estimators = 1800, max_features = 30, min_samples_leaf = 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_8.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_train_8 = rf_f_8.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_train,rf_pred_train_8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_8, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_8, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance of this model is decent with scores around 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data and see if test scores are around the 'train' data scores"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_test_8 = rf_f_8.predict(X_pc_imp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_test,rf_pred_test_8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_8, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_8, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Max depth tuning using CV an\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(10, 120, 5)}\n\n# instantiate the base model\nrf_m = RandomForestClassifier(n_estimators = 1600, max_features = 30, min_samples_leaf = 15)\n\n\n# fit tree on training data\nrf_m = GridSearchCV(rf_m, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\",\n                    return_train_score=True)\nrf_m.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# scores of GridSearch CV\nscores = rf_m.cv_results_\n# plotting accuracies with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the max_depth of around 18 results in best test score and difference between train and test at this point is also minimum. Will keep the max_depth as '18'"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_9 = RandomForestClassifier(max_depth=18, n_estimators = 1800, max_features = 25, min_samples_leaf = 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_f_9.fit(X_pc_imp_train, y_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_train_9 = rf_f_9.predict(X_pc_imp_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_train,rf_pred_train_9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_train, rf_pred_train_9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_train, rf_pred_train_9, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_train, rf_pred_train_9, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Performance of this model is decent with scores around 60% for all the key metrics such as 'Accuracy', 'Precision' and 'Recall'. Lets test this model on test data and see if test scores are around the 'train' data scores"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_test_9 = rf_f_9.predict(X_pc_imp_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(classification_report(y_pc_imp_test,rf_pred_test_9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.accuracy_score(y_pc_imp_test, rf_pred_test_9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.precision_score(y_pc_imp_test, rf_pred_test_9, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(metrics.recall_score(y_pc_imp_test, rf_pred_test_9, average = 'weighted'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even after performing multiple iterations, it can be seen that the test accuracy and recall scores are around 53%, while precision is at 50%. Will stick with this model then as this is giving best performance so far with train scores around 60% and test at 53%. \nI am submitting this project with modelling performed by using Randomforest model though we could get better performance using models such as XGBOOST. However, for this submission I am sticking randomforest based model: 'rf_f_9'"},{"metadata":{},"cell_type":"markdown","source":"Preparing the data in the 'test' dataset as well now so that can predict the risk values using the selected model. "},{"metadata":{"trusted":false},"cell_type":"code","source":"round((pc_test.isnull().sum()/len(pc_test.index))*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#First deleting the columns in 'test' dataset that have been deleted in the 'train' dataset\ncolt = ['Family_Hist_3', 'Family_Hist_5', 'Medical_History_10', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32','Ht','Wt','Family_Hist_4','Id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test_rev = pc_test.drop(colt, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test_rev.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"round((pc_test_rev.isnull().sum()/len(pc_test_rev.index))*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Focussing only on columns that have missing values\ncol = []\nfor i in pc_test_rev.columns:\n    if round((pc_test_rev[i].isnull().sum()/len(pc_test_rev.index))*100,2) != 0:\n        col.append(i)\nprint(col)\nprint(len(col))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Employment_Info_1 and Employment_Info_4 have quite low percentage of missing values so will just remove the rows\npc_test_rev = pc_test_rev[~pd.isnull(pc_test_rev[\"Employment_Info_1\"])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test_rev = pc_test_rev[~pd.isnull(pc_test_rev[\"Employment_Info_4\"])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"round((pc_test_rev[col].isnull().sum()/len(pc_test_rev[col].index))*100,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test_rev.index = pd.RangeIndex(1, len(pc_test_rev.index) + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Will treat missing values via iterative imputer for rest of columns. Will first encode the column 'Product_Info_2'","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"lc = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test_rev[\"Product_Info_2\"] = lc.fit_transform(pc_test_rev[\"Product_Info_2\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Imputing values using Iterative Imputer\ncolt2 = pc_test_rev.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test_imp = pd.DataFrame(IterativeImputer().fit_transform(pc_test_rev))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test_imp.columns = colt2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test_imp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"round((pc_test_imp[col].isnull().sum()/len(pc_test_imp.index))*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen that all null values have been treated. However, side-effect of Iterative computer is that it converts all columns to float type while processing them. Will now convert the columns originally integer type to 'int' "},{"metadata":{"trusted":false},"cell_type":"code","source":"colt4 = np.delete(cols4,0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(len(colt4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"colt4 = np.delete(colt4,107)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(colt4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test_imp[colt4] = pc_test_imp[colt4].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test_imp[\"Product_Info_2\"] = pc_test_imp[\"Product_Info_2\"].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test_imp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Will now predict the risk classifiers using the model 'rf_f_9' for the above test data set for submission"},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_sub = rf_f_9.predict(pc_test_imp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_pred_sub.reshape(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_rk = pd.DataFrame({'Risk': rf_pred_sub})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_rk.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_rk.index = pd.RangeIndex(1, len(pred_rk.index) + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(pred_rk)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating the required output file"},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test.index = pd.RangeIndex(1, len(pc_test.index) + 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pc_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fn_dt = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fn_dt['Id'] = pc_test['Id'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fn_dt['Response'] = pred_rk['Risk']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Final dataframe to be submitted is:\nfn_dt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Writing this to csv file for submission\nfn_dt.to_csv(\"C://Users/pchadha/Boosting_Kaggle_Practice/Prudential_Life_insurance/submission_file.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":1}