{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.metrics import log_loss\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pprint import pprint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix,r2_score\nimport warnings\nfrom mlxtend.classifier import StackingClassifier\nimport missingno as msno\nfrom sklearn.ensemble import VotingClassifier\nimport shap\nshap.initjs()\nimport lime\nfrom lime import lime_tabular\nwarnings.simplefilter('ignore')\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-13T22:13:18.303923Z","iopub.execute_input":"2022-01-13T22:13:18.30468Z","iopub.status.idle":"2022-01-13T22:13:21.431466Z","shell.execute_reply.started":"2022-01-13T22:13:18.304566Z","shell.execute_reply":"2022-01-13T22:13:21.430161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_df = pd.read_csv('../input/prudential-life-insurance-assessment/train.csv.zip', index_col='Id')\ninsurance_df.head()\n\n#Combining the Categores to 3 categories\ninsurance_df['Modified_Response']  = insurance_df['Response']\nsns.countplot(x= insurance_df['Modified_Response']);\n# Dropping old response columns\ninsurance_df.drop('Response',axis = 1, inplace=True)\n\n# Making lists with categorical and numerical features.\ncategorical =  [col for col in insurance_df.columns if insurance_df[col].dtype =='object']\n\nnumerical = categorical =  [col for col in insurance_df.columns if insurance_df[col].dtype !='object']","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:13:21.433484Z","iopub.execute_input":"2022-01-13T22:13:21.433876Z","iopub.status.idle":"2022-01-13T22:13:22.743866Z","shell.execute_reply.started":"2022-01-13T22:13:21.43382Z","shell.execute_reply":"2022-01-13T22:13:22.742949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_df_test = pd.read_csv('../input/prudential-life-insurance-assessment/test.csv.zip', index_col='Id')\n\n\ninsurance_df_test['Modified_Response'] = np.zeros(insurance_df_test.shape[0])\n\n\nwhole_df = pd.concat([insurance_df, insurance_df_test], ignore_index=True, sort=False)\n\nwhole_df.head()\nprint(insurance_df.shape)\nprint(insurance_df_test.shape)\nprint(whole_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:13:25.988606Z","iopub.execute_input":"2022-01-13T22:13:25.989029Z","iopub.status.idle":"2022-01-13T22:13:26.286782Z","shell.execute_reply.started":"2022-01-13T22:13:25.988978Z","shell.execute_reply":"2022-01-13T22:13:26.285942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking percentage of missing values in a column\nmissing_val_count_by_column = whole_df.isnull().sum()/len(whole_df)\n\nprint(missing_val_count_by_column[missing_val_count_by_column > 0.4].sort_values(ascending=False))\n\n# Dropping all columns in which greater than 40 percent null values\nwhole_df = whole_df.dropna(thresh=whole_df.shape[0]*0.4,how='all',axis=1)\n# Does not contain important information\nwhole_df.drop('Product_Info_2',axis=1,inplace=True)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:13:29.000665Z","iopub.execute_input":"2022-01-13T22:13:29.000971Z","iopub.status.idle":"2022-01-13T22:13:29.24297Z","shell.execute_reply.started":"2022-01-13T22:13:29.000934Z","shell.execute_reply":"2022-01-13T22:13:29.242114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insurance_df = whole_df.iloc[0:insurance_df.shape[0]]\ninsurance_df_test = whole_df.iloc[insurance_df.shape[0]:whole_df.shape[0]]\n\nprint(whole_df.shape)\nprint(insurance_df.shape)\nprint(insurance_df_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:13:32.141381Z","iopub.execute_input":"2022-01-13T22:13:32.142458Z","iopub.status.idle":"2022-01-13T22:13:32.154225Z","shell.execute_reply.started":"2022-01-13T22:13:32.142411Z","shell.execute_reply":"2022-01-13T22:13:32.15302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data for all the independent variables\nX = insurance_df.drop(labels='Modified_Response',axis=1)\n\n# Data for the dependent variable\nY = insurance_df['Modified_Response']\n\n# Filling remaining missing values with mean\nX = X.fillna(X.mean())\n\nX_TEST = insurance_df_test.drop(labels='Modified_Response',axis=1)\nX_TEST = X_TEST.fillna(X_TEST.mean())\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:13:34.586611Z","iopub.execute_input":"2022-01-13T22:13:34.586882Z","iopub.status.idle":"2022-01-13T22:13:34.709479Z","shell.execute_reply.started":"2022-01-13T22:13:34.586854Z","shell.execute_reply":"2022-01-13T22:13:34.708536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train-test split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.25, random_state=1)\n# Check the shape of train dataset\nprint(X_train.shape,Y_train.shape)\n\n# Check the shape of test dataset\nprint(X_test.shape, Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:13:38.700934Z","iopub.execute_input":"2022-01-13T22:13:38.701295Z","iopub.status.idle":"2022-01-13T22:13:38.769283Z","shell.execute_reply.started":"2022-01-13T22:13:38.701261Z","shell.execute_reply":"2022-01-13T22:13:38.768221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_scores(model, X_train, X_test ):\n  # Making predictions on train and test data\n\n  train_class_preds = model.predict(X_train)\n  test_class_preds = model.predict(X_test)\n\n\n  # Get the probabilities on train and test\n  train_preds = model.predict_proba(X_train)\n  test_preds = model.predict_proba(X_test)\n\n\n  # Calculating accuracy on train and test\n  train_accuracy = accuracy_score(Y_train,train_class_preds)\n  test_accuracy = accuracy_score(Y_test,test_class_preds)\n\n  print(\"The accuracy on train dataset is\", train_accuracy)\n  print(\"The accuracy on test dataset is\", test_accuracy)\n  print()\n  # Get the confusion matrices for train and test\n  train_cm = confusion_matrix(Y_train,train_class_preds)\n  test_cm = confusion_matrix(Y_test,test_class_preds )\n\n  print('Train confusion matrix:')\n  print( train_cm)\n  print()\n  print('Test confusion matrix:')\n  print(test_cm)\n  print()\n\n  # Get the roc_auc score for train and test dataset\n  train_auc = roc_auc_score(Y_train,train_preds, multi_class=\"ovr\")\n  test_auc = roc_auc_score(Y_test,test_preds,  multi_class=\"ovr\")\n\n  print('ROC on train data:', train_auc)\n  print('ROC on test data:', test_auc)\n  \n  \n  return model, train_auc, test_auc, train_accuracy, test_accuracy","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:13:44.70008Z","iopub.execute_input":"2022-01-13T22:13:44.700429Z","iopub.status.idle":"2022-01-13T22:13:44.710755Z","shell.execute_reply.started":"2022-01-13T22:13:44.700393Z","shell.execute_reply":"2022-01-13T22:13:44.709865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Logistic regression\n\nclf_log = LogisticRegression(random_state=0, solver='sag').fit(X_train, Y_train)\ncheck_scores(clf_log, X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:13:48.33708Z","iopub.execute_input":"2022-01-13T22:13:48.337398Z","iopub.status.idle":"2022-01-13T22:14:11.394501Z","shell.execute_reply.started":"2022-01-13T22:13:48.337363Z","shell.execute_reply":"2022-01-13T22:14:11.393487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CART\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nclf_cart =  DecisionTreeClassifier().fit(X_train, Y_train)\ncheck_scores(clf_cart, X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:14:17.45359Z","iopub.execute_input":"2022-01-13T22:14:17.453927Z","iopub.status.idle":"2022-01-13T22:14:19.884363Z","shell.execute_reply.started":"2022-01-13T22:14:17.453893Z","shell.execute_reply":"2022-01-13T22:14:19.883375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGBOOST\nimport xgboost as xgb\n\nclf_xgb = xgb.XGBClassifier(random_state=42, use_label_encoder=True, eval_metric='logloss').fit(X_train, Y_train)\ncheck_scores(clf_xgb, X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:14:23.975909Z","iopub.execute_input":"2022-01-13T22:14:23.976945Z","iopub.status.idle":"2022-01-13T22:15:39.041143Z","shell.execute_reply.started":"2022-01-13T22:14:23.976906Z","shell.execute_reply":"2022-01-13T22:15:39.039845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nimport random\n\n# Define the grid that we want to search over\nparam_grid = {'C': np.arange(0.001, 1, 0.15), \n              'penalty': ['l2', 'l1'], \n              'solver': ['liblinear']}\n\n# Define the parameters for the model \ngs_log = GridSearchCV(LogisticRegression(random_state=0, max_iter = 1000, solver='sag'),\n                  return_train_score=True, \n                  param_grid=param_grid, \n                  scoring='f1_macro',\n                  cv=5, verbose = 0)\n## Fit the model\nrandom.seed(42)\ngs_log.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:28:48.315781Z","iopub.execute_input":"2022-01-13T16:28:48.316314Z","iopub.status.idle":"2022-01-13T16:41:10.323867Z","shell.execute_reply.started":"2022-01-13T16:28:48.316265Z","shell.execute_reply":"2022-01-13T16:41:10.323078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clg_log_cv = gs_log.best_estimator_\ncheck_scores(clg_log_cv, X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:42:19.906685Z","iopub.execute_input":"2022-01-13T16:42:19.907326Z","iopub.status.idle":"2022-01-13T16:42:20.3359Z","shell.execute_reply.started":"2022-01-13T16:42:19.907266Z","shell.execute_reply":"2022-01-13T16:42:20.335356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the grid that we want to search over\nparam_grid = {\"max_depth\": np.arange(3,10,1), \"criterion\": ['gini', 'entropy']}\n\n# Define the parameters for the model \ngs_cart = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  return_train_score=True, \n                  param_grid=param_grid, \n                  scoring='f1_macro',\n                  cv=5, verbose = 0)\n\n## Fit the model\nrandom.seed(1)\ngs_cart.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:42:30.773747Z","iopub.execute_input":"2022-01-13T16:42:30.774418Z","iopub.status.idle":"2022-01-13T16:42:59.866746Z","shell.execute_reply.started":"2022-01-13T16:42:30.774371Z","shell.execute_reply":"2022-01-13T16:42:59.865974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clg_cart_cv = gs_cart.best_estimator_\ncheck_scores(clg_cart_cv, X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:43:57.780761Z","iopub.execute_input":"2022-01-13T16:43:57.781049Z","iopub.status.idle":"2022-01-13T16:43:58.07556Z","shell.execute_reply.started":"2022-01-13T16:43:57.781018Z","shell.execute_reply":"2022-01-13T16:43:58.074528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n    'n_estimators': [100,200],\n     'max_depth': np.arange(2,6,1),     \n}\n        \n# Define the parameters for the model \ngs_xgb = GridSearchCV(xgb.XGBClassifier(random_state=42, use_label_encoder=True, eval_metric='logloss'),\n                  return_train_score=True, \n                  param_grid=param_grid, \n                  scoring='f1_macro',\n                  cv=5, verbose = 0)\n\n## Fit the model\nrandom.seed(1)\ngs_xgb.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T16:47:23.940699Z","iopub.execute_input":"2022-01-13T16:47:23.940989Z","iopub.status.idle":"2022-01-13T17:04:54.395431Z","shell.execute_reply.started":"2022-01-13T16:47:23.940956Z","shell.execute_reply":"2022-01-13T17:04:54.394404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clg_xgb_cv = gs_xgb.best_estimator_\ncheck_scores(clg_xgb_cv, X_train, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T17:05:37.325713Z","iopub.execute_input":"2022-01-13T17:05:37.32679Z","iopub.status.idle":"2022-01-13T17:05:38.694063Z","shell.execute_reply.started":"2022-01-13T17:05:37.32674Z","shell.execute_reply":"2022-01-13T17:05:38.693263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We keep our train/test set from previously but we separate our train set into train and val.\nfrom sklearn import metrics\n\nX_train_ensemble, X_val_ensemble, y_train_ensemble, y_val_ensemble = train_test_split(X_train, Y_train, \n                                                    train_size = 0.75, random_state = 6,\n                                                   stratify = Y_train)\n\ndef create_prediction_data(model_list, X_train, y_train, X_val, y_val, X_test, y_test, verbose=False):\n    df_prob = np.zeros((X_test.shape[0],8))\n    selected = np.zeros(X_test.shape[0])\n    for key in model_list:\n        model_list[key].fit(X_train, y_train)\n        df_prob = df_prob  + model_list[key].predict_proba(X_test)/3\n        if verbose:\n            print(\"\\n#### \" + key +  \" ####\")\n            print(\"Test AUC: \", metrics.roc_auc_score(y_test, model_list[key].predict_proba(X_test),  multi_class=\"ovr\"))\n            print(\"Test Acc: \", metrics.accuracy_score(y_test, model_list[key].predict(X_test)))\n    print(\"Ensemble Test AUC: \", metrics.roc_auc_score(y_test, df_prob,  multi_class=\"ovr\"))\n    selected = np.argmax(df_prob, axis=1)+np.ones(selected.shape[0])\n    print(\"Ensemble Test Acc: \", accuracy_score(y_test,selected))\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:21:43.048423Z","iopub.execute_input":"2022-01-13T22:21:43.048942Z","iopub.status.idle":"2022-01-13T22:21:43.14254Z","shell.execute_reply.started":"2022-01-13T22:21:43.04889Z","shell.execute_reply":"2022-01-13T22:21:43.141726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We get our ensemble data\nd = {'logreg': clg_log_cv, 'cart': clg_cart_cv, 'xgb': clg_xgb_cv}\n# d = {'logreg': clf_log, 'cart': clf_cart, 'xgb': clf_xgb}\n\ncreate_prediction_data(d, X_train_ensemble, y_train_ensemble, X_val_ensemble, y_val_ensemble, X_test, Y_test, verbose = True)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T22:21:46.25741Z","iopub.execute_input":"2022-01-13T22:21:46.257921Z","iopub.status.idle":"2022-01-13T22:22:58.884698Z","shell.execute_reply.started":"2022-01-13T22:21:46.257867Z","shell.execute_reply":"2022-01-13T22:22:58.883376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_prediction_data_noY(model_list,  X_test):\n    df_test_prob = np.zeros((X_test.shape[0],8))\n    selected = np.zeros(X_test.shape[0])\n    for key in model_list:\n        df_test_prob = df_test_prob  + model_list[key].predict_proba(X_test)/3\n    selected = np.argmax(df_test_prob, axis=1)\n    return selected\n\n\nY_PRED = create_prediction_data_noY(d, X_TEST) \nY_PRED = Y_PRED + np.ones(len(Y_PRED))\nY_PRED = Y_PRED.astype(int)\n\n\n\n\n\n\nsubmission = pd.read_csv('../input/prudential-life-insurance-assessment/sample_submission.csv.zip', index_col='Id')\n\n\n\nsubmission['Response'][:] = Y_PRED\n\nsubmission.head()\n\nsubmission.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-13T17:30:45.986603Z","iopub.execute_input":"2022-01-13T17:30:45.987014Z","iopub.status.idle":"2022-01-13T17:30:46.306763Z","shell.execute_reply.started":"2022-01-13T17:30:45.986984Z","shell.execute_reply":"2022-01-13T17:30:46.306013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsubmission.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T17:30:54.488411Z","iopub.execute_input":"2022-01-13T17:30:54.488925Z","iopub.status.idle":"2022-01-13T17:30:54.499647Z","shell.execute_reply.started":"2022-01-13T17:30:54.488871Z","shell.execute_reply":"2022-01-13T17:30:54.498538Z"},"trusted":true},"execution_count":null,"outputs":[]}]}