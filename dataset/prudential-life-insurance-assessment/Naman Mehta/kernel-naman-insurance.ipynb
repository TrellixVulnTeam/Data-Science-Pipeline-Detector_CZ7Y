{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nIn this Notebook, we will use Prudential Life Insurance's data of its policyholders and build a classifier which will try to predict different classes of its policyholders depending on the underwriting and risk assessment."},{"metadata":{},"cell_type":"markdown","source":"## Loading Libraries and Data\n\nLet's first load few libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":false,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier  \nfrom xgboost.sklearn import XGBRegressor\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nfrom scipy.special import boxcox1p, inv_boxcox1p\nfrom scipy.stats import skew,norm\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Common Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def countOutlier(df_in, col_name):\n    if df_in[col_name].nunique() > 2:\n        orglength = len(df_in[col_name])\n        q1 = df_in[col_name].quantile(0.00)\n        q3 = df_in[col_name].quantile(0.95)\n        iqr = q3-q1 #Interquartile range \n        fence_low  = q1-1.5*iqr \n        fence_high = q3+1.5*iqr \n        df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n        newlength = len(df_out[col_name])\n        return round(100 - (newlength*100/orglength),2)  \n    else:\n        return 0\n\ndef drop_columns(dataframe, axis =1, percent=0.3):\n    '''\n    * drop_columns function will remove the rows and columns based on parameters provided.\n    * dataframe : Name of the dataframe  \n    * axis      : axis = 0 defines drop rows, axis =1(default) defines drop columns    \n    * percent   : percent of data where column/rows values are null,default is 0.3(30%)\n    '''\n    df = dataframe.copy()\n    ishape = df.shape\n    if axis == 0:\n        rownames = df.transpose().isnull().sum()\n        rownames = list(rownames[rownames.values > percent*len(df)].index)\n        df.drop(df.index[rownames],inplace=True) \n        print(\"\\nNumber of Rows dropped\\t: \",len(rownames))\n    else:\n        colnames = (df.isnull().sum()/len(df))\n        colnames = list(colnames[colnames.values>=percent].index)\n        df.drop(labels = colnames,axis =1,inplace=True)        \n        print(\"Number of Columns dropped\\t: \",len(colnames))\n        \n    print(\"\\nOld dataset rows,columns\",ishape,\"\\nNew dataset rows,columns\",df.shape)\n\n    return df\n\ndef correlation(df, dftest, threshold):\n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = df.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n                if colname in df.columns:\n                    del df[colname] # deleting the column from the train dataset\n                    del dftest[colname] # deleting the column from the test dataset\n\n    print(df.shape)\n    print(dftest.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data\n\nLet's see how does our data look like.\n\nWe will see first few entries, its shape and its statistical description"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = pd.read_csv('../input/prudential-life-insurance-assessment/train.csv')\ntest_all = pd.read_csv('../input/prudential-life-insurance-assessment/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are around 128 features and on a very broad level, these can be categorized into:\n\n1. Product Information (boundary conditions)\n2. Age\n3. Height\n4. Weight\n5. BMI\n6. Employment Information\n7. Other insured information\n8. Family History\n9. Medical History\n10. Medical Keywords"},{"metadata":{},"cell_type":"markdown","source":"\"Response\" is the target variable in the data. Let's see the value counts of the target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all['Response'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that Class 8 has the highest distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"#### Checking null"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking null values in each cols.\n(df_all.isnull().sum()*100/df_all.shape[0]).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking null values in each cols.\n(test_all.isnull().sum()*100/df_all.shape[0]).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting null values for first 13 columns\nisna_train = df_all.isnull().sum().sort_values(ascending=False)\nisna_train[:13].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting null values for first 13 columns\nisna_train = test_all.isnull().sum().sort_values(ascending=False)\nisna_train[:13].plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropping null values more than 30%"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = drop_columns(df_all, axis =1, percent=0.3)\ntest_all = drop_columns(test_all, axis =1, percent=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking null values in each cols.\n(df_all.isnull().sum()*100/df_all.shape[0]).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking null values in each cols.\n(test_all.isnull().sum()*100/test_all.shape[0]).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.Medical_History_1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.Employment_Info_4.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.Employment_Info_6.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.Employment_Info_1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Filling Null Values "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.Employment_Info_1.fillna(0, inplace=True)\ndf_all.Employment_Info_6.fillna(1, inplace=True)\ndf_all.Employment_Info_4.fillna(0, inplace=True)\ndf_all.Medical_History_1.fillna(1, inplace=True)\n\ntest_all.Employment_Info_1.fillna(0, inplace=True)\ntest_all.Employment_Info_6.fillna(1, inplace=True)\ntest_all.Employment_Info_4.fillna(0, inplace=True)\ntest_all.Medical_History_1.fillna(1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking null again"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking null values in each cols.\n(df_all.isnull().sum()*100/df_all.shape[0]).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking null values in each cols.\n(test_all.isnull().sum()*100/test_all.shape[0]).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_all.isnull().values.any())\nprint(test_all.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{},"cell_type":"markdown","source":"Let's plot few variables. These will be helpful in doing some very important feature engineering."},{"metadata":{},"cell_type":"markdown","source":"#### Response Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_color_codes()\nplt.figure(figsize=(8,8))\nsns.countplot(df_all.Response).set_title('Dist of Response variables')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that Class 8 has the highest distribution. We will assume this as clean and accepted policies on standard underwriting terms. Rest other classes can be considered as policies rejected or accepted at extra terms and conditions"},{"metadata":{},"cell_type":"markdown","source":"#### BMI Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'BMI', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['BMI'],  ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countOutlier(df_all,'BMI')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Age Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'Ins_Age', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['Ins_Age'],  ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countOutlier(df_all,'Ins_Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Height Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'Ht', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['Ht'],  ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countOutlier(df_all,'Ht')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Weight Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2, figsize=(10,5))\nsns.boxplot(x = 'Wt', data=df_all,  orient='v' , ax=axes[0])\nsns.distplot(df_all['Wt'],  ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"countOutlier(df_all,'Wt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"This is perhaps the most important part of this notebook.\n\nBased on industry knowledge, we know that these are high risk policies:\n\n1. Old Age\n2. Obese persons\n3. High BMI\n4. Extremely short or tall persons\n\nWe will therefore create few features such as:\n\n1. Person very old or very young or in middle\n2. Person very short or tall or in middle\n3. Person with very high BMI or low BMI or in middle\n4. Persons with obesity or are very thin or in middle\n\nWe will also create few more features such as:\n\n1. Multiplication of BMI and Age - higher the factor, higher the risk\n2. Multiplication of Weight and Age - higher the factor, higher the risk\n3. Multiplication of Height and Age - higher the factor, higher the risk\n4. Add all values of Medical Keywords Columns\n5. BMI Categorization\n6. Age Categorization\n7. Height Categorization\n8. Weight Categorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1 Multiplication of BMI and Age - higher the factor, higher the risk\ndf_all['bmi_age'] = df_all['BMI'] * df_all['Ins_Age']\n\n#2 Multiplication of Weight and Age - higher the factor, higher the risk\ndf_all['age_wt'] = df_all['Ins_Age'] * df_all['Wt']\n\n#3 Multiplication of Height and Age - higher the factor, higher the risk\ndf_all['age_ht'] = df_all['Ins_Age'] * df_all['Ht']\n\n#4 Add all values of Medical Keywords Columns\nmed_keyword_columns = df_all.columns[df_all.columns.str.startswith('Medical_Keyword_')]\ndf_all['med_keywords_count'] = df_all[med_keyword_columns].sum(axis=1)\ndf_all.drop(med_keyword_columns, axis=1, inplace=True)\n\n#5 BMI Categorization\nconditions = [\n    (df_all['BMI'] <= df_all['BMI'].quantile(0.25)),\n    (df_all['BMI'] > df_all['BMI'].quantile(0.25)) & (df_all['BMI'] <= df_all['BMI'].quantile(0.75)),\n    (df_all['BMI'] > df_all['BMI'].quantile(0.75))]\nchoices = ['under_weight', 'average', 'overweight']\ndf_all['bmi_wt'] = np.select(conditions, choices)\n\n#6 Age Categorization\nconditions = [\n    (df_all['Ins_Age'] <= df_all['Ins_Age'].quantile(0.25)),\n    (df_all['Ins_Age'] > df_all['Ins_Age'].quantile(0.25)) & (df_all['Ins_Age'] <= df_all['Ins_Age'].quantile(0.75)),\n    (df_all['Ins_Age'] > df_all['Ins_Age'].quantile(0.75))]\nchoices = ['young', 'average', 'old']\ndf_all['age_cat'] = np.select(conditions, choices)\n\n#7 Height Categorization\nconditions = [\n    (df_all['Ht'] <= df_all['Ht'].quantile(0.25)),\n    (df_all['Ht'] > df_all['Ht'].quantile(0.25)) & (df_all['Ht'] <= df_all['Ht'].quantile(0.75)),\n    (df_all['Ht'] > df_all['Ht'].quantile(0.75))]\nchoices = ['short', 'average', 'tall']\ndf_all['ht_cat'] = np.select(conditions, choices)\n\n#8 Weight Categorization\nconditions = [\n    (df_all['Wt'] <= df_all['Wt'].quantile(0.25)),\n    (df_all['Wt'] > df_all['Wt'].quantile(0.25)) & (df_all['Wt'] <= df_all['Wt'].quantile(0.75)),\n    (df_all['Wt'] > df_all['Wt'].quantile(0.75))]\nchoices = ['thin', 'average', 'fat']\ndf_all['wt_cat'] = np.select(conditions, choices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#1 Multiplication of BMI and Age - higher the factor, higher the risk\ntest_all['bmi_age'] = test_all['BMI'] * test_all['Ins_Age']\n\n#2 Multiplication of Weight and Age - higher the factor, higher the risk\ntest_all['age_wt'] = test_all['Ins_Age'] * test_all['Wt']\n\n#3 Multiplication of Height and Age - higher the factor, higher the risk\ntest_all['age_ht'] = test_all['Ins_Age'] * test_all['Ht']\n\n#4 Add all values of Medical Keywords Columns\nmed_keyword_columns = test_all.columns[test_all.columns.str.startswith('Medical_Keyword_')]\ntest_all['med_keywords_count'] = test_all[med_keyword_columns].sum(axis=1)\ntest_all.drop(med_keyword_columns, axis=1, inplace=True)\n\n#5 BMI Categorization\nconditions = [\n    (test_all['BMI'] <= test_all['BMI'].quantile(0.25)),\n    (test_all['BMI'] > test_all['BMI'].quantile(0.25)) & (test_all['BMI'] <= test_all['BMI'].quantile(0.75)),\n    (test_all['BMI'] > test_all['BMI'].quantile(0.75))]\nchoices = ['under_weight', 'average', 'overweight']\ntest_all['bmi_wt'] = np.select(conditions, choices)\n\n#6 Age Categorization\nconditions = [\n    (test_all['Ins_Age'] <= test_all['Ins_Age'].quantile(0.25)),\n    (test_all['Ins_Age'] > test_all['Ins_Age'].quantile(0.25)) & (test_all['Ins_Age'] <= test_all['Ins_Age'].quantile(0.75)),\n    (test_all['Ins_Age'] > test_all['Ins_Age'].quantile(0.75))]\nchoices = ['young', 'average', 'old']\ntest_all['age_cat'] = np.select(conditions, choices)\n\n#7 Height Categorization\nconditions = [\n    (test_all['Ht'] <= test_all['Ht'].quantile(0.25)),\n    (test_all['Ht'] > test_all['Ht'].quantile(0.25)) & (test_all['Ht'] <= test_all['Ht'].quantile(0.75)),\n    (test_all['Ht'] > test_all['Ht'].quantile(0.75))]\nchoices = ['short', 'average', 'tall']\ntest_all['ht_cat'] = np.select(conditions, choices)\n\n#8 Weight Categorization\nconditions = [\n    (test_all['Wt'] <= test_all['Wt'].quantile(0.25)),\n    (test_all['Wt'] > test_all['Wt'].quantile(0.25)) & (test_all['Wt'] <= test_all['Wt'].quantile(0.75)),\n    (test_all['Wt'] > test_all['Wt'].quantile(0.75))]\nchoices = ['thin', 'average', 'fat']\ntest_all['wt_cat'] = np.select(conditions, choices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_target(row):\n    if (row['bmi_wt']=='overweight') or (row['age_cat']=='old')  or (row['wt_cat']=='fat'):\n        val='extremely_risky'\n    else:\n        val='not_extremely_risky'\n    return val\n\ndf_all['extreme_risk'] = df_all.apply(new_target,axis=1)\ntest_all['extreme_risk'] = test_all.apply(new_target,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.extreme_risk.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Risk Categorization\nconditions1 = [\n    (df_all['bmi_wt'] == 'overweight') ,\n    (df_all['bmi_wt'] == 'average') ,\n    (df_all['bmi_wt'] == 'under_weight') ]\nconditions2 = [\n    (test_all['bmi_wt'] == 'overweight') ,\n    (test_all['bmi_wt'] == 'average') ,\n    (test_all['bmi_wt'] == 'under_weight') ]\nchoices = ['risk', 'non-risk', 'risk']\ndf_all['bmi_risk'] = np.select(conditions1, choices)\ntest_all['bmi_risk'] = np.select(conditions2, choices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.bmi_risk.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_target(row):\n    if (row['bmi_wt']=='average') or (row['age_cat']=='average')  or (row['wt_cat']=='average') or (row['ht_cat']=='average'):\n        val='average'\n    else:\n        val='non_average'\n    return val\n\ndf_all['average_risk'] = df_all.apply(new_target,axis=1)\ntest_all['average_risk'] = test_all.apply(new_target,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.average_risk.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_target(row):\n    if (row['bmi_wt']=='under_weight') or (row['age_cat']=='young')  or (row['wt_cat']=='thin') or (row['ht_cat']=='short'):\n        val='low_end'\n    else:\n        val='non_low_end'\n    return val\ndf_all['low_end_risk'] = df_all.apply(new_target,axis=1)\ntest_all['low_end_risk'] = test_all.apply(new_target,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.low_end_risk.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_target(row):\n    if (row['bmi_wt']=='overweight') or (row['age_cat']=='old')  or (row['wt_cat']=='fat') or (row['ht_cat']=='tall'):\n        val='high_end'\n    else:\n        val='non_high_end'\n    return val\ndf_all['high_end_risk'] = df_all.apply(new_target,axis=1)\ntest_all['high_end_risk'] = test_all.apply(new_target,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.high_end_risk.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_all.isnull().values.any())\nprint(test_all.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Delting all values of Medical History Columns having Value is biased more than 81%\nmed_keyword_columns = test_all.columns[test_all.columns.str.startswith('Medical_History_')]\nfor col in med_keyword_columns:\n    print(\"Dropping column: \" +col)\n    print(df_all[col].value_counts(normalize=True) * 100)\n    \ndrop_columns = ['Medical_History_3','Medical_History_5','Medical_History_6','Medical_History_7','Medical_History_9',\n               'Medical_History_11','Medical_History_12','Medical_History_13','Medical_History_14',\n               'Medical_History_16','Medical_History_17','Medical_History_18','Medical_History_19',\n               'Medical_History_20','Medical_History_22','Medical_History_27','Medical_History_30',\n               'Medical_History_31','Medical_History_33','Medical_History_34','Medical_History_35',\n               'Medical_History_36','Medical_History_37','Medical_History_38','Medical_History_39',]     \ndf_all.drop(drop_columns, axis=1, inplace=True)\ntest_all.drop(drop_columns, axis=1, inplace=True)\nprint(df_all.shape)\nprint(test_all.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if these feature engineering makes sense"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'extreme_risk', hue = 'Response', data = df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Under \"extreme risk\" category (overweight, old & fat), less polices are issued."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'average_risk', hue = 'Response', data = df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Under average body type ('bmi_wt'=='average' or 'age_cat'=='average'  or 'wt_cat'=='average' or 'ht_cat'=='average'), more polices are issued."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'low_end_risk', hue = 'Response', data = df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Under low-end risk category ('bmi_wt'=='under_weight' or 'age_cat'=='young' or 'wt_cat'=='thin' or 'ht_cat'=='short'), more policies are issued."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'high_end_risk', hue = 'Response', data = df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Under high-end-risk category ('bmi_wt'=='overweight' or 'age_cat'=='old' or 'wt_cat'=='fat' or 'ht_cat'=='tall'), less policies are issued."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'bmi_wt', hue = 'Response', data = df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Under overweight less policies are issued."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'age_cat', hue = 'Response', data = df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Under old age category less policies are issued"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'wt_cat', hue = 'Response', data = df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Under fat category less policies are issued"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'ht_cat', hue = 'Response', data = df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Under tall category less policies are issued"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x = 'bmi_risk', hue = 'Response', data = df_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Under risky category more often less policies are issued"},{"metadata":{},"cell_type":"markdown","source":"## Data preparation"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_all.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_all.isnull().values.any())\nprint(test_all.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df_train = df_all\ndf_test = test_all","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop ID from the data"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"df_train.drop(['Id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Categorical boolean mask\ncategorical_feature_mask = df_train.dtypes=='object'\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = df_train.columns[categorical_feature_mask].tolist()\ncategorical_cols","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder object\nle = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# apply le on categorical feature columns\ndf_train[categorical_cols] = df_train[categorical_cols].apply(lambda col: le.fit_transform(col))\ndf_train[categorical_cols].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[categorical_cols] = df_test[categorical_cols].apply(lambda col: le.fit_transform(col))\ndf_test[categorical_cols].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing highly correlated features"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation(df_train,df_test,0.90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_train.drop(['Response'], axis=1)\ny = df_train['Response']\n\n# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building 1 - Rain Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [8,12,16]\n    #'min_samples_leaf': range(30, 50),\n    #'min_samples_split': range(20, 40),\n    #'n_estimators': [300,500], \n    #'max_features': [24,48]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\nrf = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\", return_train_score = True)\n\nrf.fit(X_train, y_train)\nprint('We can get accuracy of ',rf.best_score_,' using ',rf.best_params_)\nscores = rf.cv_results_\n\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n       scores[\"mean_train_score\"], \n        label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    #'max_depth': [8,12,16],\n    'min_samples_leaf': range(20,100,10)\n    #'min_samples_split': range(20, 40),\n    #'n_estimators': [300,500], \n    #'max_features': [24,48]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\nrf = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\", return_train_score = True)\n\nrf.fit(X_train, y_train)\nprint('We can get accuracy of ',rf.best_score_,' using ',rf.best_params_)\nscores = rf.cv_results_\n\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n       scores[\"mean_train_score\"], \n        label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    #'max_depth': [8,12,16],\n    #'min_samples_leaf': range(20,100,10),\n    'min_samples_split': range(20,200,10)\n    #'n_estimators': [300,500], \n    #'max_features': [24,48]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\nrf = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\", return_train_score = True)\n\nrf.fit(X_train, y_train)\nprint('We can get accuracy of ',rf.best_score_,' using ',rf.best_params_)\nscores = rf.cv_results_\n\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n       scores[\"mean_train_score\"], \n        label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    #'max_depth': [8,12,16],\n    #'min_samples_leaf': range(20,100,10),\n    #'min_samples_split': range(20,100,10)\n    #'n_estimators': [200,300,400,500]\n    'max_features': [10,20,30,40,50]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\nrf = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\", return_train_score = True)\n\nrf.fit(X_train, y_train)\nprint('We can get accuracy of ',rf.best_score_,' using ',rf.best_params_)\nscores = rf.cv_results_\n\nplt.figure()\nplt.plot(scores[\"param_max_features\"], \n       scores[\"mean_train_score\"], \n        label=\"training accuracy\")\nplt.plot(scores[\"param_max_features\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_features\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=12,\n                             min_samples_leaf=30, \n                             min_samples_split=80,\n                             max_features=30,\n                             n_estimators=600)\nrfc.fit(X_train,y_train)\npredictions = rfc.predict(X_test)\nprint(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building - XgBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As labeles need to be from (0,7] instead of (1 to 8) else xgb.train will fail\ny_train_new = y_train - 1\ny_test_new = y_test - 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_new.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_classes = len(np.unique(y_train))\nn_classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params1 = {\n    'learning_rate': 0.02,\n    'max_depth': 16, \n    'n_estimators':500,\n    'subsample':0.9,\n    'objective':'multi:softmax',\n    'num_class': n_classes\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params2 = {\n    'silent': False, \n    'learning_rate': 0.6,  \n    'colsample_bytree': 0.4,\n    'subsample': 0.8,\n    'objective': 'multi:softmax', \n    'num_class': n_classes,\n    'n_estimators': 1000, \n    'reg_alpha': 0.3,\n    'max_depth': 32, \n    'gamma': 10\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain = xgb.DMatrix(data=X_train, label=y_train_new)\ndtest = xgb.DMatrix(data=X_test)\n\n\n# fit model on training data\nxgb_model = xgb.train(params1, dtrain)\n\npred = xgb_model.predict(dtest)\nprint(pred)\n\nprint(classification_report(y_test_new, pred))\nprint(confusion_matrix(y_test_new,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain = xgb.DMatrix(data=X_train, label=y_train_new)\ndtest = xgb.DMatrix(data=X_test)\n\n\n# fit model on training data\nxgb_model = xgb.train(params2, dtrain)\n\npred = xgb_model.predict(dtest)\nprint(pred)\n\nprint(classification_report(y_test_new, pred))\nprint(confusion_matrix(y_test_new,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}