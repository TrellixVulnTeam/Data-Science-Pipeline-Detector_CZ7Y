{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"**Overview**  \n\n![](https://www.ffwd.org/wp-content/uploads/CareerVillage-logo.png)\n\nCareerVillage.org is a nonprofit that crowdsources career advice for underserved youth. Founded in 2011 in four classrooms in New York City, the platform has now served career advice from 25,000 volunteer professionals to over 3.5M online learners. The platform uses a Q&A style similar to StackOverflow or Quora to provide students with answers to any question about any career.\n\nIn this Data Science for Good challenge, CareerVillage.org, in partnership with Google.org, is inviting fellow Data Scientist around the world to help recommend questions to appropriate volunteers. To support this challenge, CareerVillage.org has supplied five years of data.\n\n**Problem Statement**  \nThe U.S. has almost 500 students for every guidance counselor. Underserved youth lack the network to find their career role models, making CareerVillage.org the only option for millions of young people in America and around the globe with nowhere else to turn.\n\nTo date, 25,000 volunteers have created profiles and opted in to receive emails when a career question is a good fit for them. This is where your skills come in. To help students get the advice they need, the team at CareerVillage.org needs to be able to send the right questions to the right volunteers. The notifications sent to volunteers seem to have the greatest impact on how many questions are answered.\n\n**Our objective is to develop a method to recommend relevant questions to the professionals who are most likely to answer them.**\n\n**Approach**  \nThe one that never changes in Data Science related analysis - **Exploratory Data Analysis**. I will start with EDA and based on the insights will decided the recommendations."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Listing the files**   \n![](https://media.giphy.com/media/ig3CJoXHgcJ5m/giphy.gif)  \nLet's first list all the files that are given for analysis and know the data provided inside those files. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"bc6e429f87684a977d25789f196d19594de479c3"},"cell_type":"code","source":"!ls ../input/","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"079beeae4f286913c9c54742a8b09b06c846e32c"},"cell_type":"markdown","source":"**Details on the Files**  \n![](https://media.giphy.com/media/tQliIp3sn1T44/giphy.gif)    \nLet's look at the details of files that are provided for the analysis.  \n\n* answers.csv - Answers are what this is all about! Answers get posted in response to questions. Answers can only be posted by users who are registered as Professionals. However, if someone has changed their registration type after joining, they may show up as the author of an Answer even if they are no longer a Professional.  \n* comments.csv - Comments can be made on Answers or Questions. We refer to whichever the comment is posted to as the \"parent\" of that comment. Comments can be posted by any type of user. Our favorite comments tend to have \"Thank you\" in them :)  \n* emails.csv - Each email corresponds to one specific email to one specific recipient. The frequency_level refers to the type of email template which includes immediate emails sent right after a question is asked, daily digests, and weekly digests.  \n* group_memberships.csv - Any type of user can join any group. There are only a handful of groups so far.  \n* groups.csv - Each group has a \"type\". For privacy reasons we have to leave the group names off.  \n* matches.csv - Each row tells you which questions were included in emails. If an email contains only one question, that email's ID will show up here only once. If an email contains 10 questions, that email's ID would show up here 10 times.  \n* professionals.csv - We call our volunteers \"Professionals\", but we might as well call them Superheroes. They're the grown ups who volunteer their time to answer questions on the site.  \n* questions.csv - Questions get posted by students. Sometimes they're very advanced. Sometimes they're just getting started. It's all fair game, as long as it's relevant to the student's future professional success.  \n* school_memberships.csv - Just like group_memberships, but for schools instead.  \n* students.csv - Students are the most important people on CareerVillage.org. They tend to range in age from about 14 to 24. They're all over the world, and they're the reason we exist!  \n* tag_questions.csv - Every question can be hashtagged. We track the hashtag-to-question pairings, and put them into this file.  \n* tag_users.csv - Users of any type can follow a hashtag. This shows you which hashtags each user follows.  \n* tags.csv - Each tag gets a name."},{"metadata":{"trusted":true,"_uuid":"851cca33f80d2c51460661129335c4ddb230b646"},"cell_type":"markdown","source":"**Getting hands dirty with EDA**  \n\n![](https://media.giphy.com/media/3og0ITQOC5wlyk8ffy/giphy.gif)\n\nNow, Let's analyse each of the datasets that is provided one after another and then try to join all the dataset based on the analysis results to provide a valid recommendation."},{"metadata":{"_uuid":"d82a176dd39428a2bf93e9cd85c3a68edacca330"},"cell_type":"markdown","source":"**Fun with answers.csv dataset**  \nThe answers.csv dataset consists of answers for the questions that are posted. The dataset has the below list of columns,\n* answers_id\n* answers_author_id\n* answers_question_id\n* answers_date_added\n* answers_body"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2b9d08d4213028eb5d71e299f2a106ae175c9612"},"cell_type":"code","source":"import pandas as pd\nimport datetime as dt\npd.set_option('display.max_columns', None)  # or 1000\npd.set_option('display.max_rows', None)  # or 1000\npd.set_option('display.max_colwidth', -1)  # or 199\n# pd.describe_option('display') #Full list of useful options\nanswers = pd.read_csv(\"../input/answers.csv\")\nanswers['date_format'] = pd.to_datetime(answers['answers_date_added'], format='%d%b%Y:%H:%M:%S.%f', infer_datetime_format=True)\nanswers['year_month'] = answers.date_format.dt.to_period('M')\nanswers.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0741a7d691f6d29dbfaedb41d83a42386a799a64"},"cell_type":"markdown","source":"**Missing data check**  \nLet us check whether there is any missing data in the answers dataset."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"38f6b16d3d9ee13561e3c2008ad0ac76d1bd4430"},"cell_type":"code","source":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n    \nmissing_values_table(answers)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06e156bf46fd1b253daf772949d15f1ec543256f"},"cell_type":"markdown","source":"**Answers dataset with no missing values**  \nThe answers dataset has one row as missing value. So, let us drop the missing value row from the dataset."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a512eabf6886117eb512f6f3aac27cf13aacfa52"},"cell_type":"code","source":"#Selecting all the rows which has the data\nanswers = answers[pd.notnull(answers['answers_body'])]\nqts = answers['answers_question_id'].nunique()\naut = answers['answers_author_id'].nunique()\nprint('There are {0} unique questions that are answered by {1} unique responders.'.format(qts, aut))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85456ab4c018d8ec7c1961594df8fa69deb2c119"},"cell_type":"markdown","source":"**Top 10 authors who answered the questions by count**  \n![](https://media.giphy.com/media/d3g1lkA1srw1c3Di/giphy.gif)\nLet's, visualize the top 10 authors who answered by the number of questions that are posted."},{"metadata":{"trusted":true,"_uuid":"8a46b37f42ce0ecf94705215cdee5612af95e258","_kg_hide-input":true},"cell_type":"code","source":"ans_aut_df = answers['answers_author_id'].value_counts().reset_index()\nans_aut_df.columns = ['Author ID', 'Count']\nans_aut_10 = ans_aut_df.nlargest(10, 'Count')\n\n#Plotting top 10 authors\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.plotly as py\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n\ntrace = go.Bar(\n    x=ans_aut_10['Count'],\n    y=ans_aut_10['Author ID'],\n    orientation = 'h',\n    marker=dict(\n        color=ans_aut_10['Count'],\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Top 10 author ID''s who responded for the post', \n    margin=dict(\n        l=320,\n        r=10,\n        t=140,\n        b=80\n    )\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename=\"Unique_val\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"863b97013676dc0e841bc0860b0c80b33d580069"},"cell_type":"markdown","source":"**Average Length of Top 10 responded Vs rest**  "},{"metadata":{"trusted":true,"_uuid":"594cffef27493e75c3b3a19fab83d5384b4d5311","_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\ntop_10_res = ans_aut_10['Author ID']\ntop_10 = answers.loc[answers['answers_author_id'].isin(top_10_res)]\nrest_answers = answers.loc[~answers['answers_author_id'].isin(top_10_res)]\n\nprint('Average word length of response by Top 10 responders is {0:.0f}.'.format(np.mean(top_10['answers_body'].apply(lambda x: len(x.split())))))\nprint('Average word length of response by rest is {0:.0f}.'.format(np.mean(rest_answers['answers_body'].apply(lambda x: len(x.split())))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ecbda57649e35f49c6289626c876935fe5d15bc"},"cell_type":"markdown","source":"There is approximately 20 word difference between the average word length of response by top 10 responders and the average length of response by rest of the responders. "},{"metadata":{"_uuid":"be6dd4607fe7a9acfbf988f33d0cb43755a3f98d"},"cell_type":"markdown","source":"**Answer trend**  \n![](https://media.giphy.com/media/dfEYhn5LpEezu/giphy.gif)  \nLet us see if there is any pattern in answering the questions posted over a period of time."},{"metadata":{"trusted":true,"_uuid":"833d5b11bfbe359ae95d0dbd9a89848c26074352","_kg_hide-input":true},"cell_type":"code","source":"ans_tnd_df = pd.DataFrame({'Count' : answers.groupby([ \"year_month\"]).size()}).reset_index()\nans_tnd_df = ans_tnd_df.sort_values(by='year_month', ascending=True)\n\nimport matplotlib.pyplot as plt\nf,ax1=plt.subplots(figsize=(25,10))\nimport seaborn as sns\nsns.pointplot(x=ans_tnd_df['year_month'],\n              y=ans_tnd_df['Count'],color='lime',alpha=0.8)\nplt.xlabel('Period')\nplt.ylabel('Count of questions answered by responders')\nplt.title('Trend of questions answered by the responders')\nplt.xticks(rotation=90)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"457adc65f8d3dea610fc34b83994414796fec542"},"cell_type":"markdown","source":"The questions got answered in an increasing trend from August 2014. Till February 2016 the number of questions answered were below the count of 500. After February 2016 the number of questions answered were all above 500. Further, from April 2016 to December 2017 there seems a repeating trend in answering the questions."},{"metadata":{"trusted":true,"_uuid":"11416f7165ab2c92632f4c5b8b73e72f92768f2d"},"cell_type":"markdown","source":"**Meta Features on answers dataset**"},{"metadata":{"trusted":true,"_uuid":"5b82b64353c4bdcbd7e712d2305a2e2c3312a3bd","_kg_hide-input":true},"cell_type":"code","source":"import string\ndef add_metafeatures(dataframe):\n    mf_df = dataframe.copy()\n    ans_pts = answers['answers_body']\n    n_charac = pd.Series([len(t) for t in ans_pts])\n    n_punctuation = pd.Series([sum([1 for x in text if x in set(string.punctuation)]) for text in ans_pts])\n    n_upper = pd.Series([sum([1 for c in text if c.isupper()]) for text in ans_pts])\n    mf_df['n_charac'] = n_charac\n    mf_df['n_punctuation'] = n_punctuation\n    mf_df['n_upper'] = n_upper\n    return mf_df\n\nans_meta = add_metafeatures(answers)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f871de8efdbd75894b17ed90c9a13a4f978c7fb7"},"cell_type":"markdown","source":"**Plotting Meta features**  \nWe will plot the meta features 'n_charac', 'n_punctuation' and 'n_upper' with its average and see how its varied over the period of time."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"da8f36f54b49d373d463d21b84bd3164f827afab"},"cell_type":"code","source":"ans_meta_df = pd.DataFrame(ans_meta.groupby('date_format').\\\n                           mean()[['n_charac','n_punctuation','n_upper']])\n\nf,ax1=plt.subplots(figsize=(25,10))\nsns.lineplot(data=ans_meta_df, palette=\"tab10\", linewidth=2.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7aad3156aaf520a61c2e11bb05efa397acecfe7"},"cell_type":"markdown","source":"**Fun with response**  \nNow, its time to explore the responses provided by the responders for the questions that are posted."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"02f2de209c18551a7eacbc51ab6e629961660b87"},"cell_type":"code","source":"    # Tokenizing the response\n    import nltk\n    from nltk.tokenize import RegexpTokenizer\n    import re\n    resp_det = pd.DataFrame(answers['answers_body'])\n    resp_det.reset_index(drop=True, inplace=True)\n    res_lt = []\n    tokenizer = RegexpTokenizer(r'\\w+')\n    for rows in range(0, resp_det.shape[0]):\n        res_txt = \" \".join(re.findall(\"[a-zA-Z]+\", resp_det.answers_body[rows]))\n        res_txt = tokenizer.tokenize(res_txt)\n        res_lt.append(res_txt)\n\n    #Converting into single list of response\n    import itertools\n    res_list = list(itertools.chain(*res_lt))\n\n    #Removing Non sense words - I\n    words = set(nltk.corpus.words.words())\n    def clean_sent(sent):\n        return \" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n         if w.lower() in words or not w.isalpha())\n\n    res_list_cl = [clean_sent(item) for item in res_list]\n\n    #Removing Space, Tab, CR and Newline\n    res_list_cl = [re.sub(r'\\s+', '', item) for item in res_list_cl]\n\n    #import nltk\n    stopwords = nltk.corpus.stopwords.words('english')\n    res_list_cl = [word for word in res_list_cl if word.lower() not in stopwords]\n\n\n    #Removing non sense words - II\n    tp = pd.DataFrame(list(set(res_list_cl))).reset_index(drop=True)\n    tp.columns = ['uniq_lt']\n    tp['length'] = tp['uniq_lt'].apply(lambda x: len(x))\n    non_sense = list(tp['uniq_lt'][tp.length <= 2])\n    res_list_cl = [item for item in res_list_cl if item not in non_sense]\n\n    print(\"Length of original response list: {0} words\\n\"\n          \"Length of response list after stopwords removal: {1} words\"\n          .format(len(res_list), len(res_list_cl)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ae1209ce2768e333d1912608e3bae7f4389d68c"},"cell_type":"markdown","source":"**Top 30 Occuring words after removing Stopwords from response**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"4986f5724382bcf0f7d4d8a2c05476c151c7b0be"},"cell_type":"code","source":"#Data cleaning for getting top 30\nfrom collections import Counter\nresp_cnt = Counter(res_list_cl)\n\n#Dictonary to Dataframe\nresp_cnt_df = pd.DataFrame(list(resp_cnt.items()), columns = ['Words', 'Freq'])\nresp_cnt_df = resp_cnt_df.sort_values(by=['Freq'], ascending=False)\n\n#Top 30\nresp_cnt_df_l30 = resp_cnt_df.nlargest(30, 'Freq')\nresp_cnt_df_s30 = resp_cnt_df.nsmallest(30, 'Freq')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b55e9953d8c30de9482f1b08cc04b22e1fbc1368"},"cell_type":"code","source":"#Plotting the top 30 largest Vs smallest\nfrom plotly import tools\nlr_tr  = go.Bar(\n    x=resp_cnt_df_l30['Freq'],\n    y=resp_cnt_df_l30['Words'],\n    name='Most used',\n    marker=dict(\n        color='rgba(88, 214, 141, 0.6)',\n        line=dict(\n            color='rgba(88, 214, 141, 1.0)',\n            width=.3,\n        )\n    ),            \n           orientation='h',\n    opacity=0.6\n)\n\nsm_tr = go.Bar(\n    x=resp_cnt_df_s30['Freq'],\n    y=resp_cnt_df_s30['Words'],\n    name='Least used',\n    marker=dict(\n        color='rgba(155, 89, 182, 0.6)',\n        line=dict(\n            color='rgba(155, 89, 182, 1.0)',\n            width=.3,\n        )\n    ),\n    orientation='h',\n    opacity=0.6\n)\n\nfig = tools.make_subplots(rows=2, cols=1, subplot_titles=('Top 30 Most occuring words in response',\n                                                          'Top 30 Least occuring words in response'))\n\nfig.append_trace(lr_tr, 1, 1)\nfig.append_trace(sm_tr, 2, 1)\n\n\nfig['layout'].update(height=1200, width=800)\n\niplot(fig, filename='lr_vs_sm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f8092856086cbb7bb2eabd9cc885c6b1e7f1eec"},"cell_type":"markdown","source":"Both the Top 30 most occuring and least occuring have entirely different pattern of word list. In the top 30 most of the words represent the positive sentiment and the least 30 words represent the negative sentiment."},{"metadata":{"_uuid":"fae927c894497f670cf1c9e4f07b4ced0a518806"},"cell_type":"markdown","source":"**Fun with Comments.csv dataset**  \nThe comments dataset consists of comments posted in response to the answers answered by the responders to the questions that are posted. The comments consists of list of below list of columns,\n* comments_id\n* comments_author_id\n* comments_parent_content_id\n* comments_date_added\n* comments_body  "},{"metadata":{"trusted":true,"_uuid":"b40654cf7a6a7b87acf126cdb3bbffca4288dafe","_kg_hide-input":true},"cell_type":"code","source":"comments = pd.read_csv(\"../input/comments.csv\")\ncomments['date_format'] = pd.to_datetime(comments['comments_date_added'], format='%d%b%Y:%H:%M:%S.%f', infer_datetime_format=True)\ncomments['year_month'] = comments.date_format.dt.to_period('M')\ncomments.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91e33332f9ffd8c878d18384c911d2b4e202c070"},"cell_type":"markdown","source":"**Missing value check**  \nLet, us check the quality of data by checking whether the data has any missing value and its proportion."},{"metadata":{"trusted":true,"_uuid":"88f72cb485b029975124fd69c1b7108ad23f7964","_kg_hide-input":true},"cell_type":"code","source":"#Missing value check\nmissing_values_table(comments)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7263eb2733596fe0dbf11a96fab2fd19eb3d17b6"},"cell_type":"markdown","source":"As we can see only 4 rows of data is missing, we will drop all the 4 rows from the comments dataset."},{"metadata":{"_uuid":"1876902a46df26c0b4eec5c070688cc69fcb860b"},"cell_type":"markdown","source":"**Comments dataset with no missing values**  \nThe comments dataset has 4 rows as missing value. So, let us drop the missing value row from the dataset."},{"metadata":{"trusted":true,"_uuid":"6c62ea92e83e2d3fc9b90bc4355f749814d96631","_kg_hide-input":true},"cell_type":"code","source":"#Selecting all the rows which has the data\ncomments = comments[pd.notnull(comments['comments_body'])]\ncntid = comments['comments_parent_content_id'].nunique()\nautid = comments['comments_author_id'].nunique()\nprint('There are {0} unique contents that are commented by {1} unique authors.'.format(cntid, autid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc22010ea259f693b6ae9a71badc2b9504c1d763"},"cell_type":"markdown","source":"**Top 10 authors who commented on the post by count**  \n![](https://media.giphy.com/media/2tSodgDfwCjIMCBY8h/200w_d.gif)\nLet's, visualize the top 10 authors who commented by the number of questions that are posted."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9eabf128fbfaf14258e901e8601158112f442bf9"},"cell_type":"code","source":"comm_aut_df = comments['comments_author_id'].value_counts().reset_index()\ncomm_aut_df.columns = ['Author ID', 'Count']\ncomm_aut_10 = comm_aut_df.nlargest(10, 'Count')\n\n\ntrace = go.Bar(\n    x=comm_aut_10['Count'],\n    y=comm_aut_10['Author ID'],\n    orientation = 'h',\n    marker=dict(\n        color=comm_aut_10['Count'],\n        colorscale = 'Viridis',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Top 10 author ID''s who commented for the post', \n    margin=dict(\n        l=320,\n        r=10,\n        t=140,\n        b=80\n    )\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename=\"Unique_val_comm\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"596a87001a2343aac94e7b11ae9122f9b59fe302"},"cell_type":"markdown","source":"**Blogger Addict**  \n\n![](https://media.giphy.com/media/ql2lUYvISjpaE/giphy.gif)\n\nLet us see if any of the authors who responded to the questions which are posted have commented on the post as well."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"be09072e16d2704d492d19d74be08b214a5a3b51"},"cell_type":"code","source":"ans_10 = ans_aut_10['Author ID'].unique()\ncomm_10 = comm_aut_10['Author ID'].unique()\nans_comm_10 = [comm for comm in comm_10 if comm in ans_10]\nprint(\"There are 3 authors who responded most of the questions who were also present in the top 10 commenters list.\\n\" \\\n       \"The Author ID's are {0}, {1} and {2}.\".format(ans_comm_10[0], ans_comm_10[1], ans_comm_10[2]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"525a531a8d87d0921a11606d3b0e8bb1b81228ec"},"cell_type":"markdown","source":"There are 3 authors namely '36ff3b3666df400f956f8335cf53e09e',  '05444a2f42454327b2ac4b463c0adbe0' and '58fa5e95fe9e480a9349bbb1d7faaddb' who responded most of the questions were also present in the top 10 commented list."},{"metadata":{"_uuid":"2ea26e98876c6972453be342655d9c29e8a2d8cd"},"cell_type":"markdown","source":"**Comment Trend**  \n![](https://media.giphy.com/media/1vZfYD1L2lwhLC7I9t/200w_d.gif)  \nLet us see if there is any pattern in comments for the responses over a period of time."},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"127cbe9122f2f159160a8c0c11875f1d525ffb2c"},"cell_type":"code","source":"comm_tnd_df = pd.DataFrame({'Count' : comments.groupby([ \"year_month\"]).size()}).reset_index()\ncomm_tnd_df = comm_tnd_df.sort_values(by='year_month', ascending=True)\n\nimport matplotlib.pyplot as plt\nf,ax1=plt.subplots(figsize=(25,10))\nimport seaborn as sns\nsns.pointplot(x=comm_tnd_df['year_month'],\n              y=comm_tnd_df['Count'],color='blue',alpha=0.8)\nplt.xlabel('Period')\nplt.ylabel('Count of comments by responders')\nplt.title('Comments trend based on comments by the responders')\nplt.xticks(rotation=90)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4aefc9ab47757db12fda8ab17925b4a066c92fbe"},"cell_type":"markdown","source":"Both the response and the comments have the same peak on the month of May-2016. But, with respect to count the comments count is approximately half of the response count. Further, there was major spike on the month of April 2014 with respect to comments with a count of 800 and  430 in count of response which has the reverse trend in count.\nIn addition, after May - 2106 there are multiple spikes in both response and comments. "},{"metadata":{"_uuid":"032bff4ab98a0702160186e12c12931dbcee8890"},"cell_type":"markdown","source":"**Fun with comments**  \nNow, its time to explore the comments provided by the responders for the responses posted for the questions.."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5042b598e07a22f8aba4d8e07c6af4e2e37d2315"},"cell_type":"code","source":"# Tokenizing the response\ncomm_det = pd.DataFrame(comments['comments_body'])\ncomm_det.reset_index(drop=True, inplace=True)\ncomm_lt = []\ntokenizer = RegexpTokenizer(r'\\w+')\nfor rows in range(0, comm_det.shape[0]):\n    comm_txt = \" \".join(re.findall(\"[a-zA-Z]+\", comm_det.comments_body[rows]))\n    comm_txt = tokenizer.tokenize(comm_txt)\n    comm_lt.append(comm_txt)\n    \n#Converting into single list of comments\nimport itertools\ncomm_list = list(itertools.chain(*comm_lt))\n\n#Removing Non sense words - I\nwords = set(nltk.corpus.words.words())\ndef clean_sent(sent):\n    return \" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n     if w.lower() in words or not w.isalpha())\n\ncomm_list_cl = [clean_sent(item) for item in comm_list]\n\n#Removing Space, Tab, CR and Newline\ncomm_list_cl = [re.sub(r'\\s+', '', item) for item in comm_list_cl]\n\n#import nltk\nstopwords = nltk.corpus.stopwords.words('english')\ncomm_list_cl = [word for word in comm_list_cl if word.lower() not in stopwords]\n\n\n#Removing non sense words - II\ntp = pd.DataFrame(list(set(comm_list_cl))).reset_index(drop=True)\ntp.columns = ['uniq_lt']\ntp['length'] = tp['uniq_lt'].apply(lambda x: len(x))\nnon_sense = list(tp['uniq_lt'][tp.length <= 2])\ncomm_list_cl = [item for item in comm_list_cl if item not in non_sense]\n\nprint(\"Length of original comments list: {0} words\\n\"\n      \"Length of comments list after stopwords removal: {1} words\"\n      .format(len(comm_list), len(comm_list_cl)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6f8218a99ca2fe3e8c3ff055a5412f6f01e0633"},"cell_type":"markdown","source":"**Top 30 Occuring words after removing Stopwords from comments**"},{"metadata":{"trusted":true,"_uuid":"23cb307d3e9fadefe466b2225d55fc268ef5897c","_kg_hide-input":true},"cell_type":"code","source":"#Data cleaning for getting top 30\nfrom collections import Counter\ncomm_cnt = Counter(comm_list_cl)\n\n#Dictonary to Dataframe\ncomm_cnt_df = pd.DataFrame(list(comm_cnt.items()), columns = ['Words', 'Freq'])\ncomm_cnt_df = comm_cnt_df.sort_values(by=['Freq'], ascending=False)\n\n#Top 30\ncomm_cnt_df_l30 = comm_cnt_df.nlargest(30, 'Freq')\ncomm_cnt_df_s30 = comm_cnt_df.nsmallest(30, 'Freq')\n\n\n#Plotting the top 30 largest Vs smallest\nfrom plotly import tools\nlr_tr  = go.Bar(\n    x=comm_cnt_df_l30['Freq'],\n    y=comm_cnt_df_l30['Words'],\n    name='Most used',\n    marker=dict(\n        color='rgba(88, 214, 141, 0.6)',\n        line=dict(\n            color='rgba(88, 214, 141, 1.0)',\n            width=.3,\n        )\n    ),            \n           orientation='h',\n    opacity=0.6\n)\n\nsm_tr = go.Bar(\n    x=comm_cnt_df_s30['Freq'],\n    y=comm_cnt_df_s30['Words'],\n    name='Least used',\n    marker=dict(\n        color='rgba(155, 89, 182, 0.6)',\n        line=dict(\n            color='rgba(155, 89, 182, 1.0)',\n            width=.3,\n        )\n    ),\n    orientation='h',\n    opacity=0.6\n)\n\nfig = tools.make_subplots(rows=2, cols=1, subplot_titles=('Top 30 Most occuring words in Comments',\n                                                          'Top 30 Least occuring words in Comments'))\n\nfig.append_trace(lr_tr, 1, 1)\nfig.append_trace(sm_tr, 2, 1)\n\n\nfig['layout'].update(height=1200, width=800)\n\niplot(fig, filename='lr_vs_sm')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c880e72df544627f2a8601fe2672a0144f3d480c"},"cell_type":"markdown","source":"**Fun with emails.csv dataset**  \nThe emails dataset consists of email ID's of sender and receiver with email sent date and frequency of emails . The email dataset  consists of list of below list of columns,\n* emails_id\n* emails_recipient_id\n* emails_date_sent\n* emails_frequency_level"},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"82ff3a9f468f4ebe66b109943e005119daa399d4"},"cell_type":"code","source":"email = pd.read_csv(\"../input/emails.csv\")\nemail['date_format'] = pd.to_datetime(email['emails_date_sent'], format='%d%b%Y:%H:%M:%S.%f', infer_datetime_format=True)\nemail['year_month'] = email.date_format.dt.to_period('M')\nemail.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73369799c437a371734be82bb08859e883d94652"},"cell_type":"markdown","source":"**Missing value check**  \nLet, us check the quality of data by checking whether the data has any missing value and its proportion."},{"metadata":{"trusted":true,"_uuid":"ba9282f2fb26d305d08bda0a88bd988456be6452","_kg_hide-input":true},"cell_type":"code","source":"#Missing value check\nmissing_values_table(email)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4199b4f305ea60c749c2263ce6e09bb1f827b24"},"cell_type":"markdown","source":"There is no missing values in the emails dataset. So, we will move ahead with the analysis."},{"metadata":{},"cell_type":"markdown","source":"**Proportion of Email Frequency**  \nLet us see the how often people send mails with respect to email frequency which helps us to determine how fast people respond to the queries that are posted."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Summarising the email frequency\nemail_freq = email.groupby('emails_frequency_level', as_index=False).agg({\"emails_id\": \"count\"})\nemail_freq.columns = ['Email_Freq', 'Count_of_Freq']\n\ne_trace = go.Pie(labels=email_freq.Email_Freq, values=email_freq.Count_of_Freq)\n\ndata = [e_trace]\nlayout = go.Layout(title = \"Proportion of Email Frequency\")\n\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the pie chart its clearly seen that almost 18% replying immediately and 80% of the people respond on daily basis which seems reasonably good number. Its great as people help out other people in need of guidance. "},{"metadata":{},"cell_type":"markdown","source":"**Trend of response frequency**  \nNow, let us see as how the trend of response frequency varies with each other category of frequencies."},{"metadata":{"trusted":true},"cell_type":"code","source":"res_freq_trend = email.groupby(['year_month', 'emails_frequency_level']).size().reset_index(name=\"Count\")\nres_freq_trend.sort_values(['emails_frequency_level', 'year_month'], ascending=[True, True])\nres_freq_trend.head()","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"  year_month        emails_frequency_level  Count\n0 2013-11     email_notification_immediate  4    \n1 2013-12     email_notification_immediate  144  \n2 2014-01     email_notification_immediate  73   \n3 2014-01     email_notification_weekly     2    \n4 2014-02     email_notification_daily      8    ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year_month</th>\n      <th>emails_frequency_level</th>\n      <th>Count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2013-11</td>\n      <td>email_notification_immediate</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2013-12</td>\n      <td>email_notification_immediate</td>\n      <td>144</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2014-01</td>\n      <td>email_notification_immediate</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2014-01</td>\n      <td>email_notification_weekly</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2014-02</td>\n      <td>email_notification_daily</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Stay Tuned.....**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}