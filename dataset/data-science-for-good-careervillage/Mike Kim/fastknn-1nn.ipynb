{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install nmslib","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"print(\"Loading imports\")\nfrom collections import Counter\nimport os\nprint(os.listdir(\"../input\"))\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\nimport nmslib\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nnp.random.seed(0)\nprint(\"Finished loading imports\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load all CSVs here\nanswers_df = pd.read_csv(\"../input/answers.csv\", usecols=['answers_author_id', 'answers_question_id','answers_body'])\nquestions_df = pd.read_csv(\"../input/questions.csv\", usecols=['questions_id','questions_title', 'questions_body'])\nquestions_df['questions_title_and_body'] = questions_df['questions_title'] + ' ' + questions_df['questions_body']\nquestions_df.drop(['questions_title', 'questions_body'], inplace=True, axis=1)\n\nprint('loaded all CSVs here')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create base features and ground truth DF\nQA_df = questions_df.merge(answers_df, how=\"left\", left_on='questions_id', right_on='answers_question_id')\nQA_df.dropna(inplace=True)\nQA_df.drop(['answers_question_id'],inplace=True,axis=1)\n\n# Space out each because of later .sum step\nQA_df[\"questions_title_and_body\"] = QA_df[\"questions_title_and_body\"] + ' '\nQA_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data on question_id via hash\nQA_df['split'] = QA_df['questions_id'].apply(lambda x: hash(x) % 5)\nQA_df['split'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = QA_df[QA_df['split']!=0]\ntest_df = QA_df[QA_df['split']==0]\n\nprint(QA_df.shape)\nprint(train_df.shape)\nprint(test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make sure there's only one author_id per question\n# This will be knn with 1 neighbor\ntrain_df = train_df.groupby(\"answers_author_id\")[\"questions_title_and_body\"].sum().reset_index()\ntrain_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the vectorizer\nword_vectorizer = TfidfVectorizer(\n    encoding='utf8',\n    use_idf=True, # T much better than F\n    stop_words=None, #None is beter than 'english',\n    lowercase=True, #True better than False\n    min_df=2, #2 better than 1 or 3\n    max_df=0.99, #Doesn't seem to matter much better 0.8,1.0, \n    smooth_idf=False, #False better than True\n    sublinear_tf=False,  # keep sublinear_df as False, the alternative is much worse\n    norm='l1', #better than l2,None,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{3,}',  #vectorize 3-character words or more\n    ngram_range=(1, 2), #(1,3) worse\n    max_features=None) #None better than 90000)\n\n# fit and transform on it the training features\nword_vectorizer.fit(train_df['questions_title_and_body'])\ntrain_df_sparse = word_vectorizer.transform(train_df['questions_title_and_body'])\n\n#transform the test features to sparse matrix\ntest_df_sparse = word_vectorizer.transform(test_df['questions_title_and_body'])\n\nprint(test_df_sparse.shape)\nprint(train_df_sparse.shape)\nprint('TFIDF done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://github.com/nmslib/nmslib/blob/master/python_bindings/notebooks/search_sparse_cosine.ipynb\nKNN_VAL = 1 # Needs to be set at 1 given authors are aggregated into one. \nNUM_THREADS = 4\n# Set index parameters\nindex_time_params = {'M': 30, \n                     'indexThreadQty': NUM_THREADS, \n                     'efConstruction': 100,\n                     'post' : 0}\n\nindex = nmslib.init(method='hnsw', space='cosinesimil_sparse', data_type=nmslib.DataType.SPARSE_VECTOR)  #521\nindex.addDataPointBatch(train_df_sparse) \nindex.createIndex(index_time_params) #index_time_params)# {'post': 2}, print_progress=True) #'post': 0\nneighbours = index.knnQueryBatch(test_df_sparse, k=KNN_VAL, num_threads=NUM_THREADS)\nprint('knn done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = []\nfor val in neighbours:\n    mode_val = train_df.loc[val[0][0],'answers_author_id']\n    #row_list = []\n    #for j in range(KNN_VAL):\n    #    row_list.append(train_df.loc[val[0][j],'answers_author_id'])\n        \n    #row_list = Counter(row_list)\n    #mode_val = row_list.most_common(1)[0][0]\n    #mode_val = max(set(row_list), key=row_list.count) # slower\n    test_preds.append(mode_val)\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['preds'] = test_preds\ntest_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numer = np.sum(test_df['preds']==test_df['answers_author_id'])\nprint(numer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(numer/float(test_df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Using the most common author_id in test')\nmax_count = test_df['answers_author_id'].value_counts().max()\nprint(max_count)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}