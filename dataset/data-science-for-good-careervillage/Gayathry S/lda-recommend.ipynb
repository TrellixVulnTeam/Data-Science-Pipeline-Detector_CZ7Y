{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport math\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib.dates as mdates\nimport networkx as nx\n\nfrom wordcloud import WordCloud\nfrom datetime import datetime\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport spacy\nnlp = spacy.load('en')\nnlp.remove_pipe('parser')\nnlp.remove_pipe('ner')\n#nlp.remove_pipe('tagger')\n\nimport gensim\nimport pyLDAvis\nimport pyLDAvis.gensim\npyLDAvis.enable_notebook()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_dir = '../input/data-science-for-good-careervillage'\nprint(os.listdir(input_dir))\n\nprofessionals = pd.read_csv(os.path.join(input_dir, 'professionals.csv'))\ngroups = pd.read_csv(os.path.join(input_dir, 'groups.csv'))\ncomments = pd.read_csv(os.path.join(input_dir, 'comments.csv'))\nschool_memberships = pd.read_csv(os.path.join(input_dir, 'school_memberships.csv'))\ntags = pd.read_csv(os.path.join(input_dir, 'tags.csv'))\nemails = pd.read_csv(os.path.join(input_dir, 'emails.csv'))\ngroup_memberships = pd.read_csv(os.path.join(input_dir, 'group_memberships.csv'))\nanswers = pd.read_csv(os.path.join(input_dir, 'answers.csv'))\nstudents = pd.read_csv(os.path.join(input_dir, 'students.csv'))\nmatches = pd.read_csv(os.path.join(input_dir, 'matches.csv'))\nquestions = pd.read_csv(os.path.join(input_dir, 'questions.csv'))\ntag_users = pd.read_csv(os.path.join(input_dir, 'tag_users.csv'))\ntag_questions = pd.read_csv(os.path.join(input_dir, 'tag_questions.csv'))\nanswer_scores = pd.read_csv(os.path.join(input_dir, 'answer_scores.csv'))\nquestion_scores = pd.read_csv(os.path.join(input_dir, 'question_scores.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"warnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', -1)\n\nseed = 13\nrandom.seed(seed)\nnp.random.seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_pos = ['NOUN', 'VERB', 'PROPN', 'ADJ', 'INTJ', 'X']\nactual_date = datetime(2019, 2 ,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def nlp_preprocessing(data):    \n    def token_filter(token):   \n        return not token.is_stop and token.is_alpha and token.pos_ in token_pos\n    data = [re.compile(r'<[^>]+>').sub('', x) for x in data] \n    processed_tokens = []\n    data_pipe = nlp.pipe(data)\n    for doc in data_pipe:\n        filtered_tokens = [token.lemma_.lower() for token in doc if token_filter(token)]\n        processed_tokens.append(filtered_tokens)\n    return processed_tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform datetime datatypes\nquestions['questions_date_added'] = pd.to_datetime(questions['questions_date_added'], infer_datetime_format=True)\nanswers['answers_date_added'] = pd.to_datetime(answers['answers_date_added'], infer_datetime_format=True)\nprofessionals['professionals_date_joined'] = pd.to_datetime(professionals['professionals_date_joined'], infer_datetime_format=True)\nstudents['students_date_joined'] = pd.to_datetime(students['students_date_joined'], infer_datetime_format=True)\nemails['emails_date_sent'] = pd.to_datetime(emails['emails_date_sent'], infer_datetime_format=True)\ncomments['comments_date_added'] = pd.to_datetime(comments['comments_date_added'], infer_datetime_format=True)\n\n# Merge Question Title and Body\nquestions['questions_full_text'] = questions['questions_title'] +'\\r\\n\\r\\n'+ questions['questions_body']\n# Count of answers\ntemp = answers.groupby('answers_question_id').size()\nquestions['questions_answers_count'] = pd.merge(questions, pd.DataFrame(temp.rename('count')), left_on='questions_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n# First answer for questions\ntemp = answers[['answers_question_id', 'answers_date_added']].groupby('answers_question_id').min()\nquestions['questions_first_answers'] = pd.merge(questions, pd.DataFrame(temp), left_on='questions_id', right_index=True, how='left')['answers_date_added']\n# Last answer for questions\ntemp = answers[['answers_question_id', 'answers_date_added']].groupby('answers_question_id').max()\nquestions['questions_last_answers'] = pd.merge(questions, pd.DataFrame(temp), left_on='questions_id', right_index=True, how='left')['answers_date_added']\n# Hearts Score\ntemp = pd.merge(questions, question_scores, left_on='questions_id', right_on='id', how='left')\nquestions['questions_hearts'] = temp['score'].fillna(0).astype(int)\n# Questions Tags list\ntemp = pd.merge(questions, tag_questions, left_on='questions_id', right_on='tag_questions_question_id', how='inner')\ntemp = pd.merge(temp, tags, left_on='tag_questions_tag_id', right_on='tags_tag_id', how='inner')\ntemp = temp.groupby('questions_id')['tags_tag_name'].apply(list).rename('questions_tags')\nquestions['questions_tags'] = pd.merge(questions, temp.to_frame(), left_on='questions_id', right_index=True, how='left')['questions_tags']\n# Get NLP Tokens\nquestions['nlp_tokens'] = nlp_preprocessing(questions['questions_full_text'])\n\n# Days required to answer the question\ntemp = pd.merge(questions, answers, left_on='questions_id', right_on='answers_question_id')\nanswers['time_delta_answer'] = (temp['answers_date_added'] - temp['questions_date_added'])\n# Ranking for answers time\nanswers['answers_time_rank'] = answers.groupby('answers_question_id')['time_delta_answer'].rank(method='min').astype(int)\n# Hearts Score\ntemp = pd.merge(answers, answer_scores, left_on='answers_id', right_on='id', how='left')\nanswers['answers_hearts'] = temp['score'].fillna(0).astype(int)\n\n# Time since joining\nprofessionals['professionals_time_delta_joined'] = actual_date - professionals['professionals_date_joined']\n# Number of answers\ntemp = answers.groupby('answers_author_id').size()\nprofessionals['professionals_answers_count'] = pd.merge(professionals, pd.DataFrame(temp.rename('count')), left_on='professionals_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n# Number of comments\ntemp = comments.groupby('comments_author_id').size()\nprofessionals['professionals_comments_count'] = pd.merge(professionals, pd.DataFrame(temp.rename('count')), left_on='professionals_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n# Last activity (Answer)\ntemp = answers.groupby('answers_author_id')['answers_date_added'].max()\nprofessionals['date_last_answer'] = pd.merge(professionals, pd.DataFrame(temp.rename('last_answer')), left_on='professionals_id', right_index=True, how='left')['last_answer']\n# First activity (Answer)\ntemp = answers.groupby('answers_author_id')['answers_date_added'].min()\nprofessionals['date_first_answer'] = pd.merge(professionals, pd.DataFrame(temp.rename('first_answer')), left_on='professionals_id', right_index=True, how='left')['first_answer']\n# Last activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].max()\nprofessionals['date_last_comment'] = pd.merge(professionals, pd.DataFrame(temp.rename('last_comment')), left_on='professionals_id', right_index=True, how='left')['last_comment']\n# First activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].min()\nprofessionals['date_first_comment'] = pd.merge(professionals, pd.DataFrame(temp.rename('first_comment')), left_on='professionals_id', right_index=True, how='left')['first_comment']\n# Last activity (Total)\nprofessionals['date_last_activity'] = professionals[['date_last_answer', 'date_last_comment']].max(axis=1)\n# First activity (Total)\nprofessionals['date_first_activity'] = professionals[['date_first_answer', 'date_first_comment']].min(axis=1)\n# Total Hearts score\ntemp = answers.groupby('answers_author_id')['answers_hearts'].sum()\nprofessionals['professional_answers_hearts'] = pd.merge(professionals, pd.DataFrame(temp.rename('answers_hearts')), left_on='professionals_id', right_index=True, how='left')['answers_hearts'].fillna(0).astype(int)\n# Professionals Tags to List\ntemp = pd.merge(professionals, tag_users, left_on='professionals_id', right_on='tag_users_user_id', how='inner')\ntemp = pd.merge(temp, tags, left_on='tag_users_tag_id', right_on='tags_tag_id', how='inner')\ntemp = temp.groupby('professionals_id')['tags_tag_name'].apply(list).rename('professionals_tags')\nprofessionals['professionals_tags'] = pd.merge(professionals, temp.to_frame(), left_on='professionals_id', right_index=True, how='left')['professionals_tags']\n\n# Time since joining\nstudents['students_time_delta_joined'] = actual_date - students['students_date_joined']\n# Number of answers\ntemp = questions.groupby('questions_author_id').size()\nstudents['students_questions_count'] = pd.merge(students, pd.DataFrame(temp.rename('count')), left_on='students_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n# Number of comments\ntemp = comments.groupby('comments_author_id').size()\nstudents['students_comments_count'] = pd.merge(students, pd.DataFrame(temp.rename('count')), left_on='students_id', right_index=True, how='left')['count'].fillna(0).astype(int)\n# Last activity (Question)\ntemp = questions.groupby('questions_author_id')['questions_date_added'].max()\nstudents['date_last_question'] = pd.merge(students, pd.DataFrame(temp.rename('last_question')), left_on='students_id', right_index=True, how='left')['last_question']\n# First activity (Question)\ntemp = questions.groupby('questions_author_id')['questions_date_added'].min()\nstudents['date_first_question'] = pd.merge(students, pd.DataFrame(temp.rename('first_question')), left_on='students_id', right_index=True, how='left')['first_question']\n# Last activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].max()\nstudents['date_last_comment'] = pd.merge(students, pd.DataFrame(temp.rename('last_comment')), left_on='students_id', right_index=True, how='left')['last_comment']\n# First activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].min()\nstudents['date_first_comment'] = pd.merge(students, pd.DataFrame(temp.rename('first_comment')), left_on='students_id', right_index=True, how='left')['first_comment']\n# Last activity (Total)\nstudents['date_last_activity'] = students[['date_last_question', 'date_last_comment']].max(axis=1)\n# First activity (Total)\nstudents['date_first_activity'] = students[['date_first_question', 'date_first_comment']].min(axis=1)\n# Total Hearts score\ntemp = questions.groupby('questions_author_id')['questions_hearts'].sum()\nstudents['students_questions_hearts'] = pd.merge(students, pd.DataFrame(temp.rename('questions_hearts')), left_on='students_id', right_index=True, how='left')['questions_hearts'].fillna(0).astype(int)\n# Students Tags to List\ntemp = pd.merge(students, tag_users, left_on='students_id', right_on='tag_users_user_id', how='inner')\ntemp = pd.merge(temp, tags, left_on='tag_users_tag_id', right_on='tags_tag_id', how='inner')\ntemp = temp.groupby('students_id')['tags_tag_name'].apply(list).rename('students_tags')\nstudents['students_tags'] = pd.merge(students, temp.to_frame(), left_on='students_id', right_index=True, how='left')['students_tags']\n\nemails_response = pd.merge(emails, matches, left_on='emails_id', right_on='matches_email_id', how='inner')\nemails_response = pd.merge(emails_response, questions, left_on='matches_question_id', right_on='questions_id', how='inner')\nemails_response = pd.merge(emails_response, answers, left_on=['emails_recipient_id', 'matches_question_id'], right_on=['answers_author_id', 'answers_question_id'], how='left')\nemails_response = emails_response.drop(['matches_email_id', 'matches_question_id', 'answers_id', 'answers_author_id', 'answers_body', 'answers_question_id'], axis=1)\nemails_response = emails_response.drop(['questions_author_id', 'questions_title', 'questions_body', 'questions_full_text'], axis=1)\nemails_response['time_delta_email_answer'] = (emails_response['answers_date_added'] - emails_response['emails_date_sent'])\nemails_response['time_delta_question_email'] = (emails_response['emails_date_sent'] - emails_response['questions_date_added'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gensim Dictionary Filter\nextremes_no_below = 20\nextremes_no_above = 0.6\nextremes_keep_n = 8000\n\n# LDA\nnum_topics = 21\npasses = 15\nchunksize = 1000\nalpha = 1/50","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_results(ldamodel, corpus, dictionary): \n    vis = pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary, sort_topics=False)\n    transformed = ldamodel.get_document_topics(corpus)\n    df = pd.DataFrame.from_records([{v:k for v, k in row} for row in transformed])\n    return vis, df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model_wordcloud(ldamodel):\n    plot_cols = 3\n    plot_rows = math.ceil(num_topics / 3)\n    axisNum = 0\n    plt.figure(figsize=(5*plot_cols, 3*plot_rows))\n    for topicID in range(ldamodel.state.get_lambda().shape[0]):\n                #gather most relevant terms for the given topic\n        topics_terms = ldamodel.state.get_lambda()\n        tmpDict = {}\n        for i in range(1, len(topics_terms[0])):\n            tmpDict[ldamodel.id2word[i]]=topics_terms[topicID,i]\n        wordcloud = WordCloud( margin=0,max_words=20 ).generate_from_frequencies(tmpDict)\n        axisNum += 1\n        ax = plt.subplot(plot_rows, plot_cols, axisNum)\n\n        plt.imshow(wordcloud, interpolation='bilinear')\n        title = topicID\n        plt.title(title)\n        plt.axis(\"off\")\n        plt.margins(x=0, y=0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def topic_query(data, query):\n    result = data\n    result['sort'] = 0\n    for topic in query:\n        result = result[result[topic] >= query[topic]]\n        result['sort'] += result[topic]\n    result = result.sort_values(['sort'], ascending=False)\n    result = result.drop('sort', axis=1)\n    result = result.head(5)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_text_topics(text, top=20):    \n    def token_topic(token):\n        return topic_words.get(token, -1)    \n    colors = ['\\033[46m', '\\033[45m', '\\033[44m', '\\033[43m', '\\033[42m', '\\033[41m', '\\033[47m']    \n    nlp_tokens = nlp_preprocessing([text])\n    bow_text = [lda_dic.doc2bow(doc) for doc in nlp_tokens]\n    bow_text = lda_tfidf[bow_text]\n    topic_text = lda_model.get_document_topics(bow_text)\n    topic_text = pd.DataFrame.from_records([{v:k for v, k in row} for row in topic_text])    \n    print('Question:')\n    topic_words = []\n    topic_labeled = 0\n    for topic in topic_text.columns.values:\n        topic_terms = lda_model.get_topic_terms(topic, top)\n        topic_words = topic_words+[[topic_labeled, lda_dic[pair[0]], pair[1]] for pair in topic_terms]\n        topic_labeled += 1\n    topic_words = pd.DataFrame(topic_words, columns=['topic', 'word', 'value']).pivot(index='word', columns='topic', values='value').idxmax(axis=1)\n    nlp_doc = nlp(text)\n    text_highlight = ''.join([x.string if token_topic(x.lemma_.lower()) <0  else colors[token_topic(x.lemma_.lower()) % len(colors)] + x.string + '\\033[0m' for x in nlp_doc])\n    print(text_highlight) \n    \n    print('\\nTopics:')\n    topic_labeled = 0\n    for topic in topic_text:\n        print(colors[topic_labeled % len(colors)]+'Topic '+str(topic)+':', '{0:.2%}'.format(topic_text[topic].values[0])+'\\033[0m')\n        topic_labeled += 1\n    plt_data = topic_text\n    plt_data.columns = ['Topic '+str(c) for c in plt_data.columns]\n    plt_data['Others'] = 1-plt_data.sum(axis=1)\n    plt_data = plt_data.T\n    plt_data.plot(kind='pie', y=0, autopct='%.2f')\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.title('Topics Probabilities')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_tokens = questions['nlp_tokens']\nlda_dic = gensim.corpora.Dictionary(lda_tokens)\nlda_dic.filter_extremes(no_below=extremes_no_below, no_above=extremes_no_above, keep_n=extremes_keep_n)\nlda_corpus = [lda_dic.doc2bow(doc) for doc in lda_tokens]\n\nlda_tfidf = gensim.models.TfidfModel(lda_corpus)\nlda_corpus = lda_tfidf[lda_corpus]\nlda_model = gensim.models.ldamodel.LdaModel(lda_corpus, num_topics=num_topics, \n                                            id2word = lda_dic, passes=passes,\n                                            chunksize=chunksize,update_every=0,\n                                            alpha=alpha, random_state=seed)\nlda_vis, lda_result = get_model_results(lda_model, lda_corpus, lda_dic)\nlda_questions = questions[['questions_id', 'questions_title', 'questions_body']]\nlda_questions = pd.concat([lda_questions, lda_result.add_prefix('Topic_')], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_model_wordcloud(lda_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_questions.head(5).dropna(axis=1, how='all').T\nquery = {'Topic_3':0.4, 'Topic_18':0.4}\ntopic_query(lda_questions, query).dropna(axis=1, how='all').head(2).T\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}