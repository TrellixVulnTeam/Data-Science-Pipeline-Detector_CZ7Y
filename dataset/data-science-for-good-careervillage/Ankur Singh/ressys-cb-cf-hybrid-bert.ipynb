{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CV: Question-Professional Matching with RecSys\n\n## Problem Statement\n\nThe U.S. has almost 500 students for every guidance counselor. Underserved youth lack the network to find their career role models, making CareerVillage.org the only option for millions of young people in America and around the globe with nowhere else to turn.\n\nTo date, 25,000 volunteers have created profiles and opted in to receive emails when a career question is a good fit for them. This is where your skills come in. To help students get the advice they need, the team at CareerVillage.org needs to be able to send the right questions to the right volunteers. The notifications sent to volunteers seem to have the greatest impact on how many questions are answered.\n\n### Problem Statement Breakdown:\n\n1. The problem statement has two verticals - Questions and Professionals\n2. The target action among the two verticals is to find right connections betwen them\n3. Find the relevant Professional for any given new Question\n4. Send targeted emails to the recommended Professionals\n\n\n### Dataset Summary:\n\n1. There are **23,931 questions** from 12,329 students\n2. There are **51,123 answers**\n3. There are **50,106 answers** provided by **28,152 professionals**\n4. There are **6,679 answers** provided where a professional has answered more questions than asked\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The usual suspects for data processing and visualization\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib as plt\nimport datetime\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport scipy\nimport pandas as pd\nimport math\nimport random\nimport sklearn\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse.linalg import svds\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emails = pd.read_csv('../input/emails.csv')\nquestions = pd.read_csv('../input/questions.csv', parse_dates = ['questions_date_added'])\nprofessionals = pd.read_csv('../input/professionals.csv')\ncomments = pd.read_csv('../input/comments.csv')\ntag_users = pd.read_csv('../input/tag_users.csv')\ngroup_memberships = pd.read_csv('../input/group_memberships.csv')\ntags = pd.read_csv('../input/tags.csv')\nstudents = pd.read_csv('../input/students.csv')\ngroups = pd.read_csv('../input/groups.csv')\ntag_questions = pd.read_csv('../input/tag_questions.csv')\nmatches = pd.read_csv('../input/matches.csv')\nanswers = pd.read_csv('../input/answers.csv')\nschool_memberships = pd.read_csv('../input/school_memberships.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Question and Answer Analysis"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"min_q_date = min(questions['questions_date_added'])\nmax_q_date = max(questions['questions_date_added'])\nprint('There were {:,} questions asked between {} and {}'.format(questions.shape[0], min_q_date.strftime('%Y-%m-%d'), max_q_date.strftime('%Y-%m-%d')))\n\n# Plot count of questions accross years\nsns.set_style(\"white\")\nsns.countplot(x=questions['questions_date_added'].dt.year, data=questions, facecolor='darkorange').set_title('Volume of Questions per Year')\nsns.despine();","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"answers = pd.read_csv('../input/answers.csv', parse_dates=['answers_date_added'])\nmin_a_date = min(answers['answers_date_added'])\nmax_a_date = max(answers['answers_date_added'])\nprint('There were {:,} answers provided between {} and {}'.format(answers.shape[0], min_a_date.strftime('%Y-%m-%d'), max_a_date.strftime('%Y-%m-%d')))\n\n# Plot count of questions accross years\nsns.set_style(\"white\")\nsns.countplot(x=answers['answers_date_added'].dt.year, data=answers, facecolor='darkorange').set_title('Volume of Answers per Year')\nsns.despine();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_a = questions.merge(right=answers, how='inner', left_on='questions_id', right_on='answers_question_id')\nprint('There are {:,} questions that got answered, which is {:.0f}% of all questions.'.format(q_a['questions_id'].nunique(), 100*q_a['questions_id'].nunique()/questions.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Distribution of days taken to answer a question"},{"metadata":{"trusted":true},"cell_type":"code","source":"questions['questions_date_added'] = pd.to_datetime(questions['questions_date_added'])\nanswers['answers_date_added'] = pd.to_datetime(answers['answers_date_added'])\nqa = pd.merge(questions, answers, left_on='questions_id', right_on='answers_question_id', how='left')\nqa_grouped = qa.groupby('questions_id').agg({'questions_date_added':min, 'answers_date_added':min,\n                                                   'questions_body':min})\nqa_grouped['days_taken'] = (qa_grouped['answers_date_added'] - qa_grouped['questions_date_added']).dt.days\nqa_grouped['questions_body_length'] = qa_grouped['questions_body'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Numerical summary of days taken to answer a question')\ndisplay(qa_grouped['days_taken'].describe())\nplt.figure(figsize=(10,6))\nplt.title('Distribution of days taken to answer a question')\nplt.hist(qa_grouped['days_taken'], color='blue', edgecolor='black', bins=100)\nplt.xlabel('min days taken to answer a question')\nplt.ylabel('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Distribution of tags amongst the questions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"q_t = questions.merge(right=tag_questions, how='left', left_on='questions_id', right_on='tag_questions_question_id')\nq_tag = q_t.merge(right=tags, how='left', left_on='tag_questions_tag_id', right_on='tags_tag_id')\nq_a_tag = q_tag.merge(right=answers, how='left', left_on='questions_id', right_on='answers_question_id')\ntagnames = q_tag['tags_tag_name'].nunique()\nnotag = q_tag['questions_id'][q_tag['tags_tag_name'].isnull()]\nprint('Across the {:,} questions {} unique tags were used; {} questions had no tag'.format(questions.shape[0], tagnames, len(notag)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of tags \nq_tag.groupby(['questions_id'])['tags_tag_name'].nunique().describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style='whitegrid', palette='bright', color_codes=True)\n# Draw a violinplot of the number of tags per question\n# sns.swarmplot(x=q_tag.groupby(['questions_id'])['tags_tag_name'].nunique(),data=q_tag)\nax = sns.distplot(q_tag.groupby(['questions_id'])['tags_tag_name'].nunique(), hist=True, kde=False, rug=True, bins=40)\nax.set(xlabel='Number of Unique Tags', ylabel='Number of Questions', title='Distribution of Unique Tags')\nsns.despine(left=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Questions with the highest number of tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exploring the variety of tags for the question with the most tags\ntenlargest = q_tag.groupby(['questions_id'])['tags_tag_name'].nunique().nlargest(20)\ntwenlargest_1 = tenlargest.index\nfor i in twenlargest_1:\n    print(q_tag[['questions_title', 'questions_date_added']][q_tag['questions_id'] == i].iloc[1,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5. Some of the most popular tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To see the tags that every user follows \ntag_users_merged = pd.merge(tag_users, tags, left_on='tag_users_tag_id', right_on='tags_tag_id', how='inner')\n#To see the tags that are linked with every question\nquestions_merged = pd.merge(tag_questions, tags, left_on='tag_questions_tag_id', right_on='tags_tag_id', how='inner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.title('50 most popular tags wrt user following')\nsns.countplot(tag_users_merged[tag_users_merged['tags_tag_name'].isin(\n    tag_users_merged['tags_tag_name'].value_counts().index[:50])]['tags_tag_name'], color='maroon', order=tag_users_merged['tags_tag_name'].value_counts().index[:50])\nplt.ylabel('count')\nplt.xticks(rotation='vertical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.title('50 most popular tags wrt the number of questions they are linked to')\nsns.countplot(questions_merged[questions_merged['tags_tag_name'].isin(\n    questions_merged['tags_tag_name'].value_counts().index[:50])]['tags_tag_name'], color='maroon', order=questions_merged['tags_tag_name'].value_counts().index[:50])\nplt.ylabel('count')\nplt.xticks(rotation='vertical')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recomendation System\n\n*Wikipedia*\n\nA recommender system or a recommendation system (sometimes replacing \"system\" with a synonym such as platform or engine) is a subclass of information filtering system that seeks to predict the \"preference\" a user would give to an item.\n\nRecommender systems typically produce a list of recommendations in one of two ways – through collaborative filtering or through content-based filtering (also known as the personality-based approach). Collaborative filtering approaches build a model from a user's past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in. Content-based filtering approaches utilize a series of discrete characteristics of an item in order to recommend additional items with similar properties. These approaches are often combined (see Hybrid Recommender Systems).\n\nTherefore, I propose the following three approaches to solve this problem:\n\n* **Content-Based Filtering-** This method uses only information about the question title and question description of the questions, professional has previously answered to when modeling the professional's preferences. In other words, these algorithms try to recommend questions that are similar to those that a professional has answered to in the past. In particular, various student questions are compared with questions the professional has answered to, and the best-matching questions will be recommended.\n\n* **Collaborative Filtering-** This method makes automatic predictions (filtering) about the preference of a professionals by collecting preferences from many other professional (collaborating). It predicts what a particular professional will answer to based on what questions other similar professionals have answered to. The underlying assumption of the collaborative filtering approach is that if Alice and Bob have answer to the same question(s), Alice is more likely to share Bob's preference for a given question than that of a randomly chosen professional.\n\n* **Hybrid methods-** Most companies like Netflix and Hulu use a hybrid approach in their recommendation models, which provide recommendation based on the combination of what content a user like in the past as well as what other similar user like. Recent research has demonstrated that a hybrid approach that combines collaborative filtering and content-based filtering could be more effective than both approaches in some cases. These hybrid methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem.\n\n\n### Model Evaluation\n\nIn Recommender Systems, there are a set metrics commonly used for evaluation. We chose to work with Top-N accuracy metrics, which evaluates the accuracy of the top recommendations provided to a user, comparing to the items the user has actually interacted.\nThis evaluation method works as follows:\n\n* For each user\n    * For each item the user has interacted in test set\n    \n        * Sample 1000 other items the user has never interacted.\n        * Ask the recommender model to produce a ranked list of recommended items, from a set composed one interacted item and the 100 non-interacted (\"non-relevant!) items\n\n        * Compute the Top-N accuracy metrics for this user and interacted item from the recommendations ranked list\n* Aggregate the global Top-N accuracy metrics\n\nIn the next code block, we build the model evaluator"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Top-N accuracy metrics consts\nEVAL_RANDOM_SAMPLE_NON_INTERACTED_questions = 100\n\nclass ModelEvaluator:\n\n    def get_not_answered_questions_sample(self, professional_id, sample_size, seed=42):\n        answered_questions = get_questions_answered(professional_id, answers_full_indexed_df)\n        all_questions = set(questions['questions_id'])\n        non_answered_questions = all_questions - answered_questions\n\n        #random.seed(seed)\n        non_answered_questions_sample = random.sample(non_answered_questions, sample_size)\n        return set(non_answered_questions_sample)\n\n    def _verify_hit_top_n(self, project_id, recommended_questions, topn):        \n            try:\n                index = next(i for i, c in enumerate(recommended_questions) if c == project_id)\n            except:\n                index = -1\n            hit = int(index in range(0, topn))\n            return hit, index\n\n    def evaluate_model_for_professional(self, model, professional_id, processed_text_matrix = None):\n        #Getting the questions in test set\n        answered_values_testset = answers_test_indexed_df.loc[professional_id]\n        if type(answered_values_testset['questions_id']) == pd.Series:\n            professional_answered_questions_testset = set(answered_values_testset['questions_id'])\n        else:\n            professional_answered_questions_testset = set([answered_values_testset['questions_id']])  \n        answered_questions_count_testset = len(professional_answered_questions_testset) \n\n        #Getting a ranked recommendation list from a model for a given professional\n        if(processed_text_matrix==None): \n            professional_recs_df = model.recommend_questions(professional_id, \n                                                   questions_to_ignore=get_questions_answered(professional_id, \n                                                                                        answers_train_indexed_df), \n                                                   topn=100000000)\n        else:\n            professional_recs_df = model.recommend_questions(professional_id,processed_text_matrix, \n                                                   questions_to_ignore=get_questions_answered(professional_id, \n                                                                                        answers_train_indexed_df), \n                                                   topn=100000000)\n        hits_at_3_count = 0\n        hits_at_5_count = 0\n        hits_at_10_count = 0\n        #For each project the professional has answered in test set\n        for project_id in professional_answered_questions_testset:\n            #Getting a random sample (100) questions the professional has not answered \n            #(to represent questions that are assumed to be no relevant to the professional)\n            non_answered_questions_sample = self.get_not_answered_questions_sample(professional_id, \n                                                                          sample_size=EVAL_RANDOM_SAMPLE_NON_INTERACTED_questions, \n                                                                              seed=42)\n\n            #Combining the current answered project with the 100 random questions\n            questions_to_filter_recs = non_answered_questions_sample.union(set([project_id]))\n\n            #Filtering only recommendations that are either the answered project or from a random sample of 100 non-answered questions\n            valid_recs_df = professional_recs_df[professional_recs_df['questions_id'].isin(questions_to_filter_recs)]                    \n            valid_recs = valid_recs_df['questions_id'].values\n            #Verifying if the current answered project is among the Top-N recommended questions\n            hit_at_3, index_at_3 = self._verify_hit_top_n(project_id, valid_recs, 3)\n            hits_at_3_count += hit_at_3\n            hit_at_5, index_at_5 = self._verify_hit_top_n(project_id, valid_recs, 5)\n            hits_at_5_count += hit_at_5\n            hit_at_10, index_at_10 = self._verify_hit_top_n(project_id, valid_recs, 10)\n            hits_at_10_count += hit_at_10\n\n        #Recall is the rate of the answered questions that are ranked among the Top-N recommended questions, \n        #when mixed with a set of non-relevant questions\n        recall_at_3 = hits_at_3_count / float(answered_questions_count_testset)\n        recall_at_5 = hits_at_5_count / float(answered_questions_count_testset)\n        recall_at_10 = hits_at_10_count / float(answered_questions_count_testset)\n\n        professional_metrics = {'hits@3_count':hits_at_3_count, \n                         'hits@5_count':hits_at_5_count, \n                          'hits@10_count':hits_at_10_count, \n                          'answered_count': answered_questions_count_testset,\n                          'recall@3': recall_at_3,\n                          'recall@5': recall_at_5,\n                          'recall@10': recall_at_10}\n        return professional_metrics\n\n    def evaluate_model(self, model, processed_text_matrix):\n        #print('Running evaluation for professionals')\n        people_metrics = []\n        for idx, professional_id in enumerate(list(answers_test_indexed_df.index.unique().values)):\n            \n            professional_metrics = self.evaluate_model_for_professional(model, professional_id, processed_text_matrix)  \n            professional_metrics['_professional_id'] = professional_id\n            people_metrics.append(professional_metrics)\n        print('%d professionals processed' % idx)\n        print(pd.DataFrame(people_metrics).head())\n\n        detailed_results_df = pd.DataFrame(people_metrics) \\\n                            .sort_values('answered_count', ascending=False)\n        \n        global_recall_at_3 = detailed_results_df['hits@3_count'].sum() / float(detailed_results_df['answered_count'].sum())\n        global_recall_at_5 = detailed_results_df['hits@5_count'].sum() / float(detailed_results_df['answered_count'].sum())\n        global_recall_at_10 = detailed_results_df['hits@10_count'].sum() / float(detailed_results_df['answered_count'].sum())\n        \n        global_metrics = {'modelName': model.get_model_name(),\n                          'recall@3': global_recall_at_3,\n                          'recall@5': global_recall_at_5,\n                          'recall@10': global_recall_at_10}    \n        return global_metrics, detailed_results_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"Before getting into the recommender systems, we will load and preprocess our datasets. The code creates two datasets: \"questions\" contains all question-related information, while \"answers_df\" contains answers- and professionals-related information. To make sure the code runs smoothly in Kaggle kernel, I turned on the test mode to only keep 10000 answered events in the dataset.\n\nBefore modeling, we need to measure the relation strength between a professional and a question. Although most professionals only answers once in the dataset. To better measure this strength, we combine the times of answers, and create a new dataset containing unique answer relations between a professional, a question, and the relation strength. The hidden code block will output the number of questions and unique professional-question answer events:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_preperation(questions, answers, professionals):\n   \n    test_mode = True\n    # Merge datasets\n    answers = answers.merge(professionals, left_on='answers_author_id', right_on='professionals_id', how=\"left\")\n    df = answers.merge(questions,left_on='answers_question_id', right_on='questions_id', how=\"left\")\n    print(df.shape)\n    # only load a few lines in test mode\n    if test_mode:\n        df = df.head(10000)\n\n    answers_df = df\n\n    # Define event strength as the answered amount to a certain question\n    answers_df['eventStrength'] = 1\n\n    def smooth_professional_preference(x):\n        return x\n        \n    answers_full_df = answers_df \\\n                        .groupby(['professionals_id', 'questions_id'])['eventStrength'].sum() \\\n                        .apply(smooth_professional_preference).reset_index()\n            \n    # Update questions dataset\n    question_cols = questions.columns\n    questions = df[question_cols].drop_duplicates()\n\n    print('# of questions: %d' % len(questions))\n    print('# of unique user/question answers: %d' % len(answers_full_df))\n\n    return (questions, answers_full_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Split Data into Train & Test\n\nTo evaluate our models, we will split the donation dataset into training and validation sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_train_test_data(answers_full_df):\n    answers_train_df, answers_test_df = train_test_split(answers_full_df,\n                                    test_size=0.20,\n                                    random_state=42)\n\n    print('# answers on Train set: %d' % len(answers_train_df))\n    print('# answers on Test set: %d' % len(answers_test_df))\n    \n    answers_full_indexed_df = answers_full_df.set_index('professionals_id')\n    answers_test_indexed_df = answers_test_df.set_index('professionals_id')\n    answers_train_indexed_df = answers_train_df.set_index('professionals_id')\n    \n    return (answers_train_df, answers_test_df, answers_full_indexed_df, answers_test_indexed_df, answers_train_indexed_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Content Based RecSys"},{"metadata":{},"cell_type":"markdown","source":"## A) Text Processing"},{"metadata":{},"cell_type":"markdown","source":"#### Using tfidf\n\nWe will use a first TF-IDF, to extract information from question title and descriptions. TF-IDF converts unstructured text into a vector structure, where each word is represented by a position in the vector, and the value measures how relevant a given word is for a question title/descriptions. It is used bo compute similarity between questions based on question titles and descriptions."},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_text_using_tfidf(questions):\n    # Preprocessing of text data\n    textfeats = [\"questions_title\",\"questions_body\"]\n    for cols in textfeats:\n        questions[cols] = questions[cols].astype(str) \n        questions[cols] = questions[cols].astype(str).fillna('') # FILL NA\n        questions[cols] = questions[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n    \n    text = questions[\"questions_title\"] + ' ' + questions[\"questions_body\"]\n    vectorizer = TfidfVectorizer(strip_accents='unicode',\n                                analyzer='word',\n                                lowercase=True, # Convert all uppercase to lowercase\n                                stop_words='english', # Remove commonly found english words ('it', 'a', 'the') which do not typically contain much signal\n                                max_df = 0.9, # Only consider words that appear in fewer than max_df percent of all documents\n                                # max_features=5000 # Maximum features to be extracted                    \n                                )                        \n    question_ids = questions['questions_id'].tolist()\n    tfidf_matrix = vectorizer.fit_transform(text)\n    tfidf_feature_names = vectorizer.get_feature_names()\n    tfidf_matrix\n\n    return (tfidf_matrix, tfidf_feature_names, question_ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using BERT Embedding\n\nNow, let's use BERT to extract information from question title and descriptions."},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_setup():\n    # Install bert-as-service\n    !pip install bert-serving-server\n    !pip install bert-serving-client\n\n    # Download and unzip the pre-trained model\n    !wget http://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n    !unzip uncased_L-12_H-768_A-12.zip\n\n    # Start the BERT server\n    bert_command = 'bert-serving-start -model_dir /kaggle/working/uncased_L-12_H-768_A-12'\n    process = subprocess.Popen(bert_command.split(), stdout=subprocess.PIPE)\n   \ndef combine_two_list(x,y):\n    z = []\n\n    for i in range(len(x)):\n        z.append(x[i]+y[i])\n        \n    return z\ndef process_text_using_bert_embeddings(questions):\n\n    title_embeddings = bc.encode(questions[\"questions_title\"].tolist())\n    body_embeddings = bc.encode(questions[\"questions_body\"].tolist())\n\n    question_embeddings = np.asarray(combine_two_list(title_embeddings.tolist(),body_embeddings.tolist()))\n\n    return scipy.sparse.csr_matrix(question_embeddings)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## B) Build Professional profile\n\nTo build a professional's profile, we take all the questions the professional has answered to and average them. The average is weighted by the event strength based on answer times."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_question_profile(question_id, processed_text_matrix):\n    idx = question_ids.index(question_id)\n    question_profile = processed_text_matrix[idx:idx+1]\n    return question_profile\n\ndef get_question_profiles(ids, processed_text_matrix):\n    question_profiles_list = [get_question_profile(x, processed_text_matrix) for x in np.ravel([ids])]\n    question_profiles = scipy.sparse.vstack(question_profiles_list)\n    return question_profiles\n\ndef build_professionals_profile(professional_id, answers_indexed_df, processed_text_matrix):\n    answers_professional_df = answers_indexed_df.loc[professional_id]\n    professional_question_profiles = get_question_profiles(answers_professional_df['questions_id'], processed_text_matrix)\n    professional_question_strengths = np.array(answers_professional_df['eventStrength']).reshape(-1,1)\n    #Weighted average of question profiles by the answers strength\n    professional_question_strengths_weighted_avg = np.sum(professional_question_profiles.multiply(professional_question_strengths), axis=0) / (np.sum(professional_question_strengths)+1)\n    professional_profile_norm = sklearn.preprocessing.normalize(professional_question_strengths_weighted_avg)\n    return professional_profile_norm\n\nfrom tqdm import tqdm\n\ndef build_professionals_profiles(answers_full_df, processed_text_matrix): \n    answers_indexed_df = answers_full_df[answers_full_df['questions_id'].isin(questions['questions_id'])].set_index('professionals_id')\n    professional_profiles = {}\n    for professional_id in tqdm(answers_indexed_df.index.unique()):\n        professional_profiles[professional_id] = build_professionals_profile(professional_id, answers_indexed_df, processed_text_matrix)\n    print(\"# of professionals with profiles: %d\" % len(professional_profiles))\n    return professional_profiles","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## C) Build the Content-Based Recommender\n\nNow it's time to build our content-based recommender:"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ContentBasedRecommender:\n    \n    MODEL_NAME = 'Content-Based'\n    \n    def __init__(self, questions_df=None):\n        self.question_ids = question_ids\n        self.questions_df = questions_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n        \n    def _get_similar_questions_to_professional_profile(self, professional_id, processed_text_matrix, topn=1000):\n        #Computes the cosine similarity between the professional profile and all question profiles\n        cosine_similarities = cosine_similarity(professional_profiles[professional_id], processed_text_matrix)\n        #Gets the top similar questions\n        similar_indices = cosine_similarities.argsort().flatten()[-topn:]\n        #Sort the similar questions by similarity\n        similar_questions = sorted([(question_ids[i], cosine_similarities[0,i]) for i in similar_indices], key=lambda x: -x[1])\n        return similar_questions\n        \n    def recommend_questions(self, professional_id, processed_text_matrix, questions_to_ignore=[], topn=10, verbose=False):\n        similar_questions = self._get_similar_questions_to_professional_profile(professional_id, processed_text_matrix)\n        #Ignores questions the professional has already answered\n        similar_questions_filtered = list(filter(lambda x: x[0] not in questions_to_ignore, similar_questions))\n        \n        recommendations_df = pd.DataFrame(similar_questions_filtered, columns=['questions_id', 'recStrength']).head(topn)\n\n        recommendations_df = recommendations_df.merge(self.questions_df, how = 'left', \n                                                    left_on = 'questions_id', \n                                                    right_on = 'questions_id')[['recStrength', 'questions_id', 'questions_title', 'questions_body']]\n\n\n        return recommendations_df\n    \n\ndef get_questions_answered(professional_id, answers_df):\n    # Get the professional's data and merge in the movie information.\n    try:\n        answered_questions = answers_df.loc[professional_id]['Project ID']\n        return set(answered_questions if type(answered_questions) == pd.Series else [answered_questions])\n    except KeyError:\n        return set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### a) Run Content-based model withTFIDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"questions, answers_full_df = data_preperation(questions, answers, professionals)\nanswers_train_df, answers_test_df, answers_full_indexed_df, answers_test_indexed_df, answers_train_indexed_df = split_train_test_data(answers_full_df)\ntfidf_matrix, tfidf_feature_names, question_ids = process_text_using_tfidf(questions)\nprofessional_profiles = build_professionals_profiles(answers_full_df, tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myprofessional1 = \"000d4635e5da41e3bfd83677ee11dda4\"\nmyprofessional2 = \"00271cc10e0245fba4a35e76e669c281\"\ncbr_model_tfidf = ContentBasedRecommender(questions)\ncbr_model_tfidf.recommend_questions(myprofessional2, tfidf_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_evaluator = ModelEvaluator()\n\nprint('Evaluating Content-Based Filtering model...')\ncb_tfidf_global_metrics, cb_tfidf_detailed_results_df = model_evaluator.evaluate_model(cbr_model_tfidf, tfidf_matrix)\nprint('\\nGlobal metrics:\\n%s' % cb_tfidf_global_metrics)\ncb_tfidf_detailed_results_df = cb_tfidf_detailed_results_df[['_professional_id', 'answered_count', \"hits@3_count\", 'hits@5_count','hits@10_count', \n                                                'recall@3','recall@5','recall@10']]\ncb_tfidf_detailed_results_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### b) Run Content-based model with BERT"},{"metadata":{"trusted":true},"cell_type":"code","source":"import subprocess\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nbert_setup()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from bert_serving.client import BertClient\n# Start the BERT client\nbc = BertClient()\n\nquestions, answers_full_df = data_preperation(questions, answers, professionals)\nanswers_train_df, answers_test_df, answers_full_indexed_df, answers_test_indexed_df, answers_train_indexed_df = split_train_test_data(answers_full_df)\nprocessed_text_embedding = process_text_using_bert_embeddings(questions)\nprofessional_profiles = build_professionals_profiles(answers_full_df, processed_text_embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myprofessional1 = \"000d4635e5da41e3bfd83677ee11dda4\"\nmyprofessional2 = \"00271cc10e0245fba4a35e76e669c281\"\ncbr_model_bert = ContentBasedRecommender(questions)\ncbr_model_bert.recommend_questions(myprofessional2, processed_text_embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_evaluator = ModelEvaluator()\n\nprint('Evaluating Content-Based Filtering model...')\ncb_bert_global_metrics, cb_bert_detailed_results_df = model_evaluator.evaluate_model(cbr_model_bert, processed_text_embedding)\nprint('\\nGlobal metrics:\\n%s' % cb_bert_global_metrics)\ncb_bert_detailed_results_df = cb_bert_detailed_results_df[['_professional_id', 'answered_count', \"hits@3_count\", 'hits@5_count','hits@10_count', \n                                                'recall@3','recall@5','recall@10']]\ncb_bert_detailed_results_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Collaborative Filtering RecSys\n\nNext, we will build a model-based Collaborative Filtering (CF) Recommender. In this approach, models are developed using machine learning algorithms to recommend question to professionals. There are many model-based CF algorithms, here we adopt a latent factor model, which compresses professional-question matrix into a low-dimensional representation in terms of latent factors. A reduced presentation could be utilized for either professional-based or question-based neighborhood searching algorithms to find recommendations. Here we a use popular latent factor model named Singular Value Decomposition (SVD)."},{"metadata":{},"cell_type":"markdown","source":"### A) Create the professional-question matrix\n\nWe will first get the professional-question matrix and print the first five rows."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a sparse pivot table with professionals in rows and questions in columns\nprofessionals_questions_pivot_matrix_df = answers_full_df.pivot(index='professionals_id', \n                                                          columns='questions_id', \n                                                          values='eventStrength').fillna(0)\n\n# Transform the professional-question dataframe into a matrix\nprofessionals_questions_pivot_matrix = professionals_questions_pivot_matrix_df.as_matrix()\n\n# Get professional ids\nprofessionals_ids = list(professionals_questions_pivot_matrix_df.index)\n\n# Print the first 5 rows of the professional-question matrix\nprofessionals_questions_pivot_matrix[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### B) Singular Value Decomposition (SVD)\n\nNow we will use SVD to get latent factors. After the factorization, we will try to reconstruct the original matrix by multiplying its factors. The resulting matrix is not sparse any more. It is the generated predictions for questions the donor have not yet answer to, which we will exploit for recommendations."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Performs matrix factorization of the original professional-question matrix\n# Here we set k = 20, which is the number of factors we are going to get\n# In the definition of SVD, an original matrix A is approxmated as a product A ≈ UΣV \n# where U and V have orthonormal columns, and Σ is non-negative diagonal.\nU, sigma, Vt = svds(professionals_questions_pivot_matrix, k = 20)\nsigma = np.diag(sigma)\n\n# Reconstruct the matrix by multiplying its factors\nall_professional_predicted_ratings = np.dot(np.dot(U, sigma), Vt) \n\n#Converting the reconstructed matrix back to a Pandas dataframe\ncf_preds_df = pd.DataFrame(all_professional_predicted_ratings, \n                           columns = professionals_questions_pivot_matrix_df.columns, \n                           index=professionals_ids).transpose()\ncf_preds_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### C) Build the Collaborative Filtering Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CFRecommender:\n    \n    MODEL_NAME = 'Collaborative Filtering'\n    \n    def __init__(self, cf_predictions_df, questions_df=None):\n        self.cf_predictions_df = cf_predictions_df\n        self.questions_df = questions_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n        \n    def recommend_questions(self, professional_id, questions_to_ignore=[], topn=10):\n        # Get and sort the professional's predictions\n        sorted_professional_predictions = self.cf_predictions_df[professional_id].sort_values(ascending=False) \\\n                                    .reset_index().rename(columns={professional_id: 'recStrength'})\n\n        # Recommend the highest predicted questions that the professional hasn't donated to\n        recommendations_df = sorted_professional_predictions[~sorted_professional_predictions['questions_id'].isin(questions_to_ignore)] \\\n                               .sort_values('recStrength', ascending = False) \\\n                               .head(topn)\n\n \n        recommendations_df = recommendations_df.merge(self.questions_df, how = 'left', \n                                                          left_on = 'questions_id', \n                                                          right_on = 'questions_id')[['recStrength', 'questions_id', 'questions_title', 'questions_body']]\n\n\n        return recommendations_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfr_model = CFRecommender(cf_preds_df, questions)\ncfr_model.recommend_questions(myprofessional2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Evaluating Collaborative Filtering (SVD Matrix Factorization) model...')\ncf_global_metrics, cf_detailed_results_df = model_evaluator.evaluate_model(cfr_model, None)\nprint('\\nGlobal metrics:\\n%s' % cf_global_metrics)\ncf_detailed_results_df = cf_detailed_results_df[['_professional_id', 'answered_count', \"hits@3_count\", 'hits@5_count','hits@10_count', \n                                               'recall@3','recall@5','recall@10']]\ncf_detailed_results_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Hybrid RecSys\n\nThe third approach, the hybrid method, combines the first two approaches to try to give even better recommendations. Hybrid methods have performed better than individual approaches in many studies and have being extensively used by researchers and practioners.\n\nLet's build a very simple hybridization method, by only multiply the Content-Based score with the Collaborative-Filtering score , and ranking by the resulting hybrid score."},{"metadata":{"trusted":true},"cell_type":"code","source":"class HybridRecommender:\n    \n    MODEL_NAME = 'Hybrid'\n    \n    def __init__(self, cb_rec_model, cf_rec_model, questions_df):\n        self.cb_rec_model = cb_rec_model\n        self.cf_rec_model = cf_rec_model\n        self.questions_df = questions_df\n        \n    def get_model_name(self):\n        return self.MODEL_NAME\n        \n    def recommend_questions(self, professional_id,processed_text_matrix, questions_to_ignore=[], topn=10):\n        #Getting the top-1000 Content-based filtering recommendations\n        cb_recs_df = self.cb_rec_model.recommend_questions(professional_id,processed_text_matrix, questions_to_ignore=questions_to_ignore, \n                                                           topn=1000).rename(columns={'recStrength': 'recStrengthCB'})\n        \n        #Getting the top-1000 Collaborative filtering recommendations\n        cf_recs_df = self.cf_rec_model.recommend_questions(professional_id, questions_to_ignore=questions_to_ignore,  \n                                                           topn=1000).rename(columns={'recStrength': 'recStrengthCF'})\n        \n        #Combining the results by question ID\n        recs_df = cb_recs_df.merge(cf_recs_df,\n                                   how = 'inner', \n                                   left_on = 'questions_id', \n                                   right_on = 'questions_id')\n        \n        #Computing a hybrid recommendation score based on CF and CB scores\n        recs_df['recStrengthHybrid'] = recs_df['recStrengthCB'] * recs_df['recStrengthCF']\n        \n        #Sorting recommendations by hybrid score\n        recommendations_df = recs_df.sort_values('recStrengthHybrid', ascending=False).head(topn)\n\n        recommendations_df = recommendations_df.merge(self.questions_df, how = 'left', \n                                                    left_on = 'questions_id', \n                                                    right_on = 'questions_id')[['recStrengthHybrid', \n                                                                              'questions_id', 'questions_title', \n                                                                              'questions_body']]\n\n\n        return recommendations_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hybrid_model = HybridRecommender(cbr_model_bert, cfr_model, questions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hybrid_model.recommend_questions(myprofessional2, processed_text_embedding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Evaluating Hybrid model...')\nhybrid_global_metrics, hybrid_detailed_results_df = model_evaluator.evaluate_model(hybrid_model, processed_text_embedding)\nprint('\\nGlobal metrics:\\n%s' % hybrid_global_metrics)\nhybrid_detailed_results_df = hybrid_detailed_results_df[['_professional_id', 'answered_count', \"hits@3_count\", 'hits@5_count','hits@10_count', \n                                             'recall@3','recall@5','recall@10']]\nhybrid_detailed_results_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Comparision"},{"metadata":{"trusted":true},"cell_type":"code","source":"global_metrics_df = pd.DataFrame([cf_global_metrics, \n                                  cb_tfidf_global_metrics,\n                                  cb_bert_global_metrics,\n                                  hybrid_global_metrics]).set_index('modelName')\nglobal_metrics_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In my Observation: Content Based Model using TFIDF is best performing Model.**"},{"metadata":{},"cell_type":"markdown","source":"## References\n\n1. http://recommender-systems.org/\n2. https://en.wikipedia.org/wiki/Recommender_system\n3. https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/\n4. https://www.kaggle.com/gspmoreira/recommender-systems-in-python-101\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}