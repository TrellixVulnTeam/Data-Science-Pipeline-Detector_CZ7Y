{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfrom time import time\nimport datetime as dt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\nfrom sklearn.ensemble.partial_dependence import partial_dependence","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I Train and Test Data Preparation #"},{"metadata":{},"cell_type":"markdown","source":"## I.1. Loading the Supervised Machine Learning Data Set ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dir = \"../input\"\nexamples_dir = os.path.join(input_dir,'cv-data-augmentation-text-scores')\nexamples = pd.read_parquet(os.path.join(examples_dir,'positive_negative_examples.parquet.gzip'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examples.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examples.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"examples.dropna().shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## I.2. Splitting Data into Train and Test Data Sets ##"},{"metadata":{},"cell_type":"markdown","source":"### The test period is from July 2018 to January 2019 ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_period_start = dt.datetime(2018, 7, 1)\ntest_period_end = dt.datetime(2019, 1, 31)\n\n# the response or target column\ny_col = 'matched' \n\n# 'questions_id' is only used in the Conditional Logistic Regression model\n# 'answer_user_id' and 'emails_date_sent' are only used for comparison statistics\nfeature_cols = ['questions_id', 'answer_user_id', 'emails_date_sent',\n                'days_from_joined_dates', 'days_from_last_activities',\n                'professional_activities_sum_100000', 'professional_activities_sum_365', 'professional_activities_sum_30', \n                'questioner_answerer_shared_schools', 'questioner_answerer_shared_tags', 'questioner_answerer_shared_groups', \n                'questioners_answerers_paths', 'commenters_questioners_paths', 'commenters_answerers_paths',\n                'LSI_Score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_train_test(examples, feature_cols, y_col,\n                     test_period_start, test_period_end):\n    \n    train_data = examples[examples['questions_date_added'] < test_period_start]\n    train_x = train_data[feature_cols]\n    train_y = train_data[y_col]\n    print('Train Data Set Size: {}'.format(train_x.shape))\n    \n    test_data = examples[(examples['questions_date_added'] >= test_period_start) & (examples['questions_date_added'] <= test_period_end)]\n    test_x = test_data[feature_cols]\n    test_y = test_data[y_col]\n    print('Test Data Set Size: {}'.format(test_x.shape))\n    \n    return train_x, train_y, test_x, test_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Slipt the original data set \ntrain_x, train_y, test_x, test_y = split_train_test(examples, feature_cols, y_col, test_period_start, test_period_end)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save train and test data sets in CSV format\ntrain_x.to_csv('train_x.gz', compression='gzip')\ntrain_y.to_csv('train_y.gz', compression='gzip')\ntest_x.to_csv('test_x.gz', compression='gzip')\ntest_y.to_csv('test_y.gz', compression='gzip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop 'questions_id' column which is only used in the Conditional Logistic Regression model\n# Drop 'answer_user_id' and 'emails_date_sent' columns which are only used for comparison statistics\nfor dropped_col in ['questions_id', 'answer_user_id', 'emails_date_sent']:    \n    train_x = train_x.drop(dropped_col, axis=1)\n    test_x = test_x.drop(dropped_col, axis=1)\n    feature_cols.remove(dropped_col)\nprint(feature_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# II. Classification with Gradient Boosting Decision Trees #"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sample_size = None\nif train_sample_size is not None:    \n    train_sample_index = np.random.choice(train_x.index, size=train_sample_size, replace=False)\n    train_x = train_x.iloc[train_sample_index]\n    train_y = train_y.iloc[train_sample_index]\nprint('Label distribution: ')\nprint(train_y.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## II.1 Model Training ##"},{"metadata":{},"cell_type":"markdown","source":"### II.1.1. Search for Optimal Hyper Parameters ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"def report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This hyper parameter set is selected by the below randomized grid search in the previous runs of this notebook.\n# Therefore, we use this set of hyper parameters from now on to reduce the running time\n# In production, this step needs to be fine tuned further\nbest_params = {\"learning_rate\": 0.1,\n               \"n_estimators\": 100,\n               \"max_depth\": 3}\n\n# if we would like to search for best hyper parameters, uncomment the below line to set best_params to None #\n# best_params = None         \nif best_params is None:    \n    # a sample grid of hyper parameters\n    param_dist = {\"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n                  \"n_estimators\": [50, 100, 200],\n                  \"max_depth\": [3]}\n    # run randomized search\n    n_iter_search = 5\n    random_search = RandomizedSearchCV(ensemble.GradientBoostingClassifier(), param_distributions=param_dist,\n                                       n_iter=n_iter_search, cv=3)\n    start = time()\n    random_search.fit(train_x, train_y)\n    print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n          \" parameter settings.\" % ((time() - start), n_iter_search))\n    report(random_search.cv_results_)\n    best_params = random_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### II.1.2 Model Training with the Optimal Hyper Parameters ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngbc_model = ensemble.GradientBoostingClassifier(**best_params)\nprint(gbc_model)\ngbc_model.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## II.2 Model Interpretation: Feature Importance and Interpretation ##"},{"metadata":{},"cell_type":"markdown","source":"### II.2.1. Feature Importance ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = gbc_model.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.asarray(feature_cols)[np.asarray(sorted_idx)])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.savefig('feature_importance.jpg')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### II.2.2. Feature Intepretation with Partial Dependence Plots ###"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = pd.DataFrame({'Name':feature_cols, \n                              'ID': range(0, len(feature_cols))}).set_index('Name')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_partial_dependence(gbc_model, train_x, [feature_names.loc['days_from_joined_dates']],\n                        feature_names=feature_cols,\n                        n_jobs=3, grid_resolution=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_partial_dependence(gbc_model, train_x, [feature_names.loc['professional_activities_sum_30']],\n                        feature_names=feature_cols,\n                        n_jobs=3, grid_resolution=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_partial_dependence(gbc_model, train_x, [feature_names.loc['questioners_answerers_paths']],\n                        feature_names=feature_cols,\n                        n_jobs=3, grid_resolution=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_partial_dependence(gbc_model, train_x, [feature_names.loc['LSI_Score']],\n                        feature_names=feature_cols,\n                        n_jobs=3, grid_resolution=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature_name in feature_names.index:\n    plot_partial_dependence(gbc_model, train_x, [feature_names.loc[feature_name]],\n                            feature_names=feature_cols,\n                            n_jobs=3, grid_resolution=50)\n    plt.savefig('{}.jpg'.format(feature_name))\nos.listdir()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## II.3 Model Prediction on the Test Data Set ##"},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_test_y = pd.DataFrame(gbc_model.predict_proba(test_x)[:,1], index=test_y.index, columns=['Predicted_Match_Prob'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_test_y.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_test_y.to_csv('predicted_test_y.gz', compression='gzip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}