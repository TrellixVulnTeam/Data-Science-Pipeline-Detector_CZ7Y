{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tag Embeddings and Other Features Demo for CareerVillage.org"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime as dt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom keras import layers\nfrom keras import Input\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\n\nfrom  keras import regularizers\n\nimport pickle\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\ninput_dir = \"../input\"\nbase_data_dir = os.path.join(input_dir,'data-science-for-good-careervillage')\nfe_ts_dir = os.path.join(input_dir,'cv-feature-engineering-text-scores')\nag_data_dir = os.path.join(input_dir,'cv-data-augmentation-network-predictors-2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I. Loading Data"},{"metadata":{},"cell_type":"markdown","source":"## I.1. Tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"tags = pd.read_csv(os.path.join(base_data_dir, 'tags.csv'))\nprint(tags.shape)\nprint(tags.head(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## I.2. User Tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_users = pd.read_csv(os.path.join(base_data_dir, 'tag_users.csv'))\ntag_users.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count users per tag\ntags_by_users_counts = tag_users.groupby('tag_users_tag_id')['tag_users_user_id'].count().reset_index()\ntags_by_users_counts = tags_by_users_counts.rename(columns={'tag_users_user_id': 'users_count'})\nprint(tags_by_users_counts.shape)\ntags_by_users_counts.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags = tags.merge(tags_by_users_counts, \n                  left_on='tags_tag_id', right_on='tag_users_tag_id', how='left')\nprint(tags.shape)\nprint(tags.head(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## I.3. Question Tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_questions = pd.read_csv(os.path.join(base_data_dir, 'tag_questions.csv'))\nprint(tag_questions.shape)\nprint(tag_questions.sample(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count questions per tag\ntags_by_questions_counts = tag_questions.groupby('tag_questions_tag_id')['tag_questions_question_id'].count().reset_index()\ntags_by_questions_counts = tags_by_questions_counts.rename(columns={'tag_questions_question_id': 'questions_count'})\nprint(tags_by_questions_counts.shape)\ntags_by_questions_counts.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags = tags.merge(tags_by_questions_counts, \n                  left_on='tags_tag_id', right_on='tag_questions_tag_id', how='left')\nprint(tags.shape)\nprint(tags.head(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## I.4. Filter Rare Tags"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tags.shape)\ntags = tags.dropna(how='any', subset=['users_count', 'questions_count'])\nprint(tags.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tags.shape)\ntags = tags[(tags['users_count'] >= 5) | (tags['questions_count'] >= 5)]\nprint(tags.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags = tags.reset_index()\ndel tags['index']\ntags = tags.reset_index().set_index('tags_tag_id')\nprint(tags.head(5))\nprint(tags.tail(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tag_users.shape)\ntag_users = tag_users[tag_users['tag_users_tag_id'].isin(tags['tag_users_tag_id'])]\nprint(tag_users.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tag_questions.shape)\ntag_questions = tag_questions[tag_questions['tag_questions_tag_id'].isin(tags['tag_users_tag_id'])]\nprint(tag_questions.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags = tags[['index', 'tags_tag_name']]\nprint(tags.head(5))\nprint(tags.tail(5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## I.4. Supervised ML Data Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"examples = pd.read_parquet(os.path.join(ag_data_dir,'positive_negative_examples.parquet.gzip'))\nexamples = examples.sort_values(\n    by=['questions_id', 'questions_date_added', 'answer_user_id', 'emails_date_sent'])\nexamples.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(examples.groupby(['questions_id', 'answer_user_id'])['questions_date_added'].count().sort_values(\n    ascending=False).head(3))\nprint(examples[(examples['questions_id']=='9a42d4109ee141c0838fd966efcb5026') & \n               (examples['answer_user_id']=='6adc2bf866bd428892821b044eb8f0fe')].drop_duplicates())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** The supervised ML data set is divided into two train and validation sets. The validation period starts from July 1, 2018. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"val_period_start = dt.datetime(2018, 7, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_examples = examples[examples['questions_date_added'] < val_period_start]\nprint(train_examples.shape)\nprint(train_examples[train_examples['matched']==1].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_examples = examples[examples['questions_date_added'] >= val_period_start]\nprint(val_examples.shape)\nprint(val_examples[val_examples['matched']==1].shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# II. Functions"},{"metadata":{},"cell_type":"markdown","source":"# II.1. Functions for Dense Tag Vectors"},{"metadata":{},"cell_type":"markdown","source":"** Functions to obtain tag indicators for professionals and questions. The indicator for a tag is set to 1 if the professional or the question registers for that tag. Otherwise, it is set to 0. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_user_tags_vector(user_id, tags):\n    user_tags = tag_users[tag_users['tag_users_user_id']==user_id]['tag_users_tag_id']\n    user_tags_vector = np.zeros(tags.shape[0])\n    for tag_id in user_tags:\n        user_tags_vector[tags.loc[tag_id]['index']] = 1\n    return user_tags_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_user_tags_vector('', tags))\nsum(get_user_tags_vector('', tags))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_user_tags_vector('6adc2bf866bd428892821b044eb8f0fe', tags))\nsum(get_user_tags_vector('6adc2bf866bd428892821b044eb8f0fe', tags))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_question_tags_vector(question_id, tags):\n    question_tags = tag_questions[tag_questions['tag_questions_question_id']==question_id]['tag_questions_tag_id']\n    question_tags_vector = np.zeros(tags.shape[0])\n    for tag_id in question_tags:\n        question_tags_vector[tags.loc[tag_id]['index']] = 1\n    return question_tags_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_question_tags_vector('', tags))\nsum(get_question_tags_vector('', tags))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_question_tags_vector('9a42d4109ee141c0838fd966efcb5026', tags))\nsum(get_question_tags_vector('9a42d4109ee141c0838fd966efcb5026', tags))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## II.2. Data Generators"},{"metadata":{},"cell_type":"markdown","source":"** We generate training and validation data for deep learning models using random sampling. For each iteration, a random sample of questions is drawn from the full list of questions. We then generate a sample for each selected question by combining all matched instances with a random sample of unmatched instances. The sample size of unmatched instances is equal to * unmatched_matched_ratio * times the number of matched cases. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create case-control samples\ndef sample_question_professionals(question_id, examples, unmatched_matched_ratio):\n    question_instances = examples[examples['questions_id']==question_id]\n    \n    matched_professionals = question_instances[question_instances['matched']==1]['answer_user_id'].values\n    matched_professionals_targets = np.repeat(1, len(matched_professionals))\n    \n    unmatched_professionals = question_instances[question_instances['matched']==0]['answer_user_id'].values\n    sampled_unmatched_professionals = np.random.choice(\n        unmatched_professionals, size=unmatched_matched_ratio * len(matched_professionals), replace=False)\n    sampled_unmatched_professionals_targets = np.repeat(0, len(sampled_unmatched_professionals))\n\n    return (np.concatenate((matched_professionals, sampled_unmatched_professionals), axis=0), \n            np.concatenate((matched_professionals_targets, sampled_unmatched_professionals_targets), axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = ['days_from_joined_dates', 'days_from_last_activities',\n                     'professional_activities_sum_100000', 'professional_activities_sum_365',\n                     'professional_activities_sum_30', 'questioner_answerer_shared_tags', \n                     'question_user_shared_tags', 'questioners_answerers_paths']\ndef get_other_features(question_id, professional_id, examples):\n    rows = examples[\n        (examples['questions_id']==question_id) & \n        (examples['answer_user_id']==professional_id)][selected_features]\n    return rows.drop_duplicates(subset=selected_features).head(1).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sample_question_tags_vectors(question_id, examples, tags, unmatched_matched_ratio):\n    sampled_question_professionals, matched_targets = sample_question_professionals(\n        question_id, examples, unmatched_matched_ratio)\n\n    question_tags_vector = get_question_tags_vector(question_id, tags) \n    question_tags_vectors = np.broadcast_to(\n        question_tags_vector, (len(sampled_question_professionals), len(question_tags_vector)))\n\n    professionals_tags_vectors = []\n    question_professionals_features_vectors = []\n    for professional_id in sampled_question_professionals:\n        professionals_tags_vectors.append(get_user_tags_vector(professional_id, tags))\n        question_professionals_features_vectors.append(get_other_features(question_id, professional_id, examples))\n    professionals_tags_vectors = np.vstack(professionals_tags_vectors)\n    question_professionals_features_vectors = np.vstack(question_professionals_features_vectors)\n    \n    return question_tags_vectors, professionals_tags_vectors, question_professionals_features_vectors, matched_targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator(examples, tags, num_questions, unmatched_matched_ratio, is_shuffle=True):\n    \n    question_statistics = examples.groupby('questions_id').agg({'matched': 'sum', 'answer_user_id': 'count'})\n    question_statistics = question_statistics[\n        ((question_statistics['matched']>0) &\n         (question_statistics['answer_user_id']>((unmatched_matched_ratio + 1)*question_statistics['matched'])))]\n    question_ids = question_statistics.index.values\n    \n    num_question_ids = question_ids.shape[0]\n    if is_shuffle:\n        np.random.shuffle(question_ids)\n    \n    batch_id = 0\n    while True:\n\n        cum_question_tags_vectors = []\n        cum_professionals_tags_vectors = []\n        cum_question_professionals_features_vectors = []\n        cum_matched_targets = []\n        \n        if (batch_id + num_questions) >= num_question_ids:\n            batch_id = 0\n        selected_ids = np.arange(batch_id, min(batch_id + num_questions, num_question_ids))\n        batch_id += len(selected_ids)\n\n        for selected_id in selected_ids:\n            question_id = question_ids[selected_id]\n            question_tags_vectors, professionals_tags_vectors, question_professionals_features_vectors, matched_targets = sample_question_tags_vectors(\n                question_id, examples, tags, unmatched_matched_ratio)\n            cum_question_tags_vectors.append(question_tags_vectors)\n            cum_professionals_tags_vectors.append(professionals_tags_vectors)\n            cum_question_professionals_features_vectors.append(question_professionals_features_vectors)\n            cum_matched_targets.append(matched_targets)\n\n        cum_question_tags_vectors = np.concatenate(cum_question_tags_vectors, axis=0)\n        cum_professionals_tags_vectors = np.concatenate(cum_professionals_tags_vectors, axis=0)\n        cum_question_professionals_features_vectors = np.concatenate(cum_question_professionals_features_vectors, axis=0)\n        cum_matched_targets = np.concatenate(cum_matched_targets, axis=0)\n    \n        yield [cum_question_tags_vectors, \n               cum_professionals_tags_vectors, \n               cum_question_professionals_features_vectors], cum_matched_targets        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compared to the **Tag Embeddings** notetbook, a smaller case control sample is generated in each gradient descent training step to reduce the training time. The validation settting is kept the same so that results are comparable."},{"metadata":{"trusted":true},"cell_type":"code","source":" # the number of questions randomly drawn for each training round\ntraining_num_questions = 8\n# the ratio of the sample size of unmatched instances over matched ones for each training round\ntraining_unmatched_matched_ratio = 1\ntrain_gen = generator(train_examples, tags, training_num_questions, training_unmatched_matched_ratio)\n\n # the number of questions randomly drawn for each validation round\nval_num_questions = 8\n # the ratio of the sample size of unmatched instances over matched ones for each validation round\nval_unmatched_matched_ratio = 1\nval_gen = generator(val_examples, tags, val_num_questions, val_unmatched_matched_ratio)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# III. Deep Learning Models for Tag Semantics"},{"metadata":{},"cell_type":"markdown","source":"** The model here by all means is not the optimal one. We use a simple model just to demonstrate how the supervised ML data set that we have built can be used to learn tag semantics. **"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## III.1. Model Specification"},{"metadata":{"trusted":true},"cell_type":"code","source":"latent_dimensions = 64\n\n# Both questions and professionals share the same tag embedding layer\nembeddings = layers.Dense(latent_dimensions, activation='linear')\n\nquestion_tags_input = Input(shape=(tags.shape[0],), name='question_tags')\nquestion_tags_output = embeddings(question_tags_input)\n\nprofessional_tags_input = Input(shape=(tags.shape[0],), name='professional_tags')\nprofessional_tags_output = embeddings(professional_tags_input)\n\nquestion_professionals_features_input = Input(shape=(len(selected_features),), name='question_professionals_features')\n\n# There are many way to combine the embeddins of question tags and professional tags\n# One simple option is used here just for demonstration\ntags_multiplied = layers.multiply([question_tags_output, professional_tags_output])\n\n# Merge with other features\nmerged_with_others = layers.concatenate([question_tags_output, professional_tags_output, \n                                         tags_multiplied, question_professionals_features_input], axis=-1)\n\n# Another feed forward layer\nreduced_l1 = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(merged_with_others)\n\nreduced_l2 = layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01))(reduced_l1)\n\n# The binary predictions of matched (1) versus unmatched (0) are modeled by a sigmoid function\npredictions = layers.Dense(1, activation='sigmoid')(reduced_l2)\n\nmodel = Model([question_tags_input, professional_tags_input, question_professionals_features_input], predictions)\n\nmodel.compile(optimizer=RMSprop(lr=0.001), \n              loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** We can use the outputs of the embedding layer or the prediction layer as inputs in the GBDTs model. This model also demonstrates how deep learning models can replace the GBDTs one. We can have another embedding layer for matching texts between questions and professionals plus other inputs for activity and network statistics... **"},{"metadata":{},"cell_type":"markdown","source":"## III.2. Model Estimation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Test Run\n# history = model.fit_generator(train_gen, steps_per_epoch=5, epochs=5, \n#                               validation_data=val_gen, validation_steps=5)\n\n# Full Run\nhistory = model.fit_generator(train_gen, steps_per_epoch=50, epochs=10, \n                              validation_data=val_gen, validation_steps=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('tags_embeddings_with_other_features.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## III.3. Model Checking"},{"metadata":{},"cell_type":"markdown","source":"** Training and Validation Loss **"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Training and Validation Accuracy **"},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nepochs = range(1, len(loss) + 1)\nplt.figure()\nplt.plot(epochs, acc, 'bo', label='Training Accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('trainHistory_tags_embeddings_with_other_features', 'wb') as out_file:\n    pickle.dump(history.history, out_file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}