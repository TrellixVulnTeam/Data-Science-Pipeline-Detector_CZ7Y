{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Data Overview\nFor detailed description, refer to the competetion page https://www.kaggle.com/c/data-science-for-good-careervillage.\n\n![data overview](https://storage.googleapis.com/kagglesdsdata/datasets/308369/627264/data_overview.png?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1566501804&Signature=eSwTN5SXDbNeZzPrFTnGNxRIRluacFVt3HOAdJVPoFRNxwA%2FdswsTPcQGj5umyrbP4q%2F86eiDmE446qrQydcjvFE1Pk0%2FneKQZnTkYbajyWNu7TAU2mG2sRedXR59mBaOhJzI7rs8rIF8xaV8WSAcM2etIXM%2BvXg7%2Fmq%2BMIAuDRUnlfVCoxDwJNQoMBZ91ypJnVH4s1Exi335vAWjTGsXR2LNgSKvdw00%2BlVgFSibSnzvi7qvaV4WEPEoZMjDqB3y70lqDz3VrYsf9O%2BUJ4bdFRg5BFDsL%2B0t%2FhWf3oKG2z3L1j8NwoNkeUAitqnWHX3gAe13dkQAe0ovV3J4Cca8g%3D%3D)","metadata":{}},{"cell_type":"markdown","source":"## 2. Data import","metadata":{}},{"cell_type":"code","source":"import os\nimport copy\nimport datetime\nimport warnings\n\nimport random\nfrom datetime import datetime\nimport re\n\nimport numpy as np\nfrom scipy.stats import t\nimport pandas as pd\nimport keras\n\nfrom matplotlib import pyplot as plt\nimport matplotlib as mpl\n\nfrom wordcloud import WordCloud\n\nimport seaborn as sns\n","metadata":{"ExecuteTime":{"end_time":"2019-04-23T17:37:37.865058Z","start_time":"2019-04-23T17:37:34.718219Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\n\nDATA_PATH = '../input/data-science-for-good-careervillage/'\nSPLIT_DATE = '2019-01-01'","metadata":{"ExecuteTime":{"end_time":"2019-04-23T17:37:37.918822Z","start_time":"2019-04-23T17:37:37.865058Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read CSV\nanswers = pd.read_csv(DATA_PATH+'answers.csv')\nanswer_scores = pd.read_csv(DATA_PATH+'answer_scores.csv')\ncomments = pd.read_csv(DATA_PATH+'comments.csv')\nemails = pd.read_csv(DATA_PATH+'emails.csv')\ngroups = pd.read_csv(DATA_PATH+'groups.csv')\ngroup_memberships = pd.read_csv(DATA_PATH+ 'group_memberships.csv')\nmatches = pd.read_csv(DATA_PATH+ 'matches.csv')\nprofessionals = pd.read_csv(DATA_PATH +'professionals.csv')\nquestions = pd.read_csv(DATA_PATH+ 'questions.csv')\nquestion_scores = pd.read_csv(DATA_PATH+'question_scores.csv')\nschool_memberships = pd.read_csv(DATA_PATH +'school_memberships.csv')\nstudents = pd.read_csv(DATA_PATH+ 'students.csv')\ntags = pd.read_csv(DATA_PATH+ 'tags.csv')\ntag_questions = pd.read_csv(DATA_PATH +'tag_questions.csv')\ntag_users = pd.read_csv(DATA_PATH+ 'tag_users.csv')","metadata":{"ExecuteTime":{"end_time":"2019-04-23T17:37:42.863678Z","start_time":"2019-04-23T17:37:37.918822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Data processing","metadata":{}},{"cell_type":"code","source":"# Convert datetime format\nanswers['answers_date_added'] = pd.to_datetime(answers['answers_date_added'], infer_datetime_format=True)\ncomments['comments_date_added'] = pd.to_datetime(comments['comments_date_added'], infer_datetime_format=True)\nemails['emails_date_sent'] = pd.to_datetime(emails['emails_date_sent'], infer_datetime_format=True)\nprofessionals['professionals_date_joined'] = pd.to_datetime(professionals['professionals_date_joined'], infer_datetime_format=True)\nquestions['questions_date_added'] = pd.to_datetime(questions['questions_date_added'], infer_datetime_format=True)\nstudents['students_date_joined'] = pd.to_datetime(students['students_date_joined'], infer_datetime_format=True)","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the first and last date of activities for each professionals and students\n\n# Answer activity\ntemp = answers.groupby('answers_author_id').max()['answers_date_added']\nprofessionals = professionals.merge(pd.DataFrame(temp.rename('last_answer')),\\\n                                    left_on='professionals_id',right_index=True,how='left')\ntemp = answers.groupby('answers_author_id').min()['answers_date_added']\nprofessionals = professionals.merge(pd.DataFrame(temp.rename('first_answer')),\\\n                                    left_on='professionals_id',right_index=True,how='left')\n# Question activity\ntemp = questions.groupby('questions_author_id').max()['questions_date_added']\nstudents = students.merge(pd.DataFrame(temp.rename('last_question')),\\\n                                    left_on='students_id',right_index=True,how='left')\ntemp = questions.groupby('questions_author_id').min()['questions_date_added']\nstudents = students.merge(pd.DataFrame(temp.rename('first_question')),\\\n                                    left_on='students_id',right_index=True,how='left')\n# Comment activity\ntemp = comments.groupby('comments_author_id').max()['comments_date_added']\nstudents = students.merge(pd.DataFrame(temp.rename('last_comment')),\\\n                                    left_on='students_id',right_index=True,how='left')\nprofessionals = professionals.merge(pd.DataFrame(temp.rename('last_comment')),\\\n                                    left_on='professionals_id',right_index=True,how='left')\ntemp = comments.groupby('comments_author_id').min()['comments_date_added']\nstudents = students.merge(pd.DataFrame(temp.rename('first_comment')),\\\n                                    left_on='students_id',right_index=True,how='left')\nprofessionals = professionals.merge(pd.DataFrame(temp.rename('first_comment')),\\\n                                    left_on='professionals_id',right_index=True,how='left')\n","metadata":{"ExecuteTime":{"end_time":"2019-04-23T17:37:51.261083Z","start_time":"2019-04-23T17:37:51.073906Z"},"_kg_hide-input":false,"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. EDA","metadata":{}},{"cell_type":"markdown","source":"### 4.1 Data consumption overview\n* Professionals: Location, Industry, Headline, Tags, Groups, Schools, Answers, and Comments,\n* Students: Location, Tags, Groups, Schools, Questions, and Comments,","metadata":{}},{"cell_type":"code","source":"# Professionals\nxTick=['Location','Industry', 'Headline', 'Tags', 'Groups', 'Schools', 'Answers', 'Comments']\nxidx=range(len(xTick))\ntotal=professionals.shape[0]\nyNMiss = []\n# Location\nyNMiss.append(professionals['professionals_location'].count())\n# Industry\nyNMiss.append(professionals['professionals_industry'].count())\n# Headline\nyNMiss.append(professionals['professionals_headline'].count())\n# Tags\ntemp=tag_users.groupby('tag_users_user_id').min()['tag_users_tag_id']\nyNMiss.append(professionals.merge(pd.DataFrame(temp),left_on='professionals_id',\\\n                                  right_index=True,how='left')['tag_users_tag_id'].count())\n# Groups\nyNMiss.append(professionals.merge(group_memberships,left_on='professionals_id',\\\n                                  right_on='group_memberships_user_id',how='left')['group_memberships_group_id'].count())\n# Schools\nyNMiss.append(professionals.merge(school_memberships,left_on='professionals_id',\\\n                                  right_on='school_memberships_user_id',how='left')['school_memberships_school_id'].count())\n# Answers\nyNMiss.append(professionals['first_answer'].count())\n# Comments\nyNMiss.append(professionals['first_comment'].count())\n\n# Plot\nyMiss = [total-x for x in yNMiss]\np1=plt.bar(xidx,yNMiss)\np2=plt.bar(xidx,yMiss,bottom=yNMiss)\nplt.xticks(xidx, xTick, rotation='vertical')\nplt.legend((p1[0],p2[0]),('Existing','Missing'),bbox_to_anchor=(1,.5))\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Students\nxTick=['Location','Tags', 'Groups', 'Schools', 'Questions', 'Comments']\nxidx=range(len(xTick))\ntotal=students.shape[0]\nyNMiss = []\n# Location\nyNMiss.append(students['students_location'].count())\n# Tags\ntemp=tag_users.groupby('tag_users_user_id').min()['tag_users_tag_id']\nyNMiss.append(students.merge(pd.DataFrame(temp),left_on='students_id',\\\n                                  right_index=True,how='left')['tag_users_tag_id'].count())\n# Groups\nyNMiss.append(students.merge(group_memberships,left_on='students_id',\\\n                                  right_on='group_memberships_user_id',how='left')['group_memberships_group_id'].count())\n# Schools\nyNMiss.append(students.merge(school_memberships,left_on='students_id',\\\n                                  right_on='school_memberships_user_id',how='left')['school_memberships_school_id'].count())\n# Answers\nyNMiss.append(students['first_question'].count())\n# Comments\nyNMiss.append(students['first_comment'].count())\n\n# Plot\nyMiss = [total-x for x in yNMiss]\np1=plt.bar(xidx,yNMiss)\np2=plt.bar(xidx,yMiss,bottom=yNMiss)\nplt.xticks(xidx, xTick, rotation='vertical')\nplt.legend((p1[0],p2[0]),('Existing','Missing'),bbox_to_anchor=(1,.5))\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 Locations distribution\n","metadata":{}},{"cell_type":"code","source":"# Professionals\nprofessionals['professionals_location'].value_counts(ascending=True).tail(30).plot.barh()\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Students\nstudents['students_location'].value_counts(ascending=True).tail(30).plot.barh()\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3 Users registered per year\n","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({'Prof':professionals['professionals_date_joined'].dt.year.value_counts().sort_index(),\\\n              'Stu':students['students_date_joined'].dt.year.value_counts().sort_index()}).plot()\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.4 Questions & answers posted per year","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({'Q':questions['questions_date_added'].dt.year.value_counts().sort_index(),\\\n              'A':answers['answers_date_added'].dt.year.value_counts().sort_index()}).plot()\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.5 Student-Questions & Professional-Answers count distributions","metadata":{}},{"cell_type":"code","source":"# S-Q distribution\nquestions['questions_author_id'].value_counts().hist(bins=range(1,25),grid=False)\nplt.xlabel('Question #')\nplt.ylabel('Student #')\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# P-A distribution\nanswers['answers_author_id'].value_counts().hist(bins=range(1,50),grid=False)\nplt.xlabel('Answer #')\nplt.ylabel('Prof #')\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.6 Question-Answer cont distribution","metadata":{}},{"cell_type":"code","source":"answers['answers_question_id'].value_counts().hist(bins=range(0,20),grid=False)\nplt.xlabel('Answer #')\nplt.ylabel('Question #')\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.7 Top tags in questions","metadata":{}},{"cell_type":"code","source":"tag_questions.merge(tags,left_on='tag_questions_tag_id',right_on='tags_tag_id',how='left')['tags_tag_name']\\\n            .value_counts(ascending=True).tail(20).plot.barh()\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.8 Top tags in students","metadata":{}},{"cell_type":"code","source":"tag_users[tag_users['tag_users_user_id'].isin(students['students_id'])]\\\n        .merge(tags,left_on='tag_users_tag_id',right_on='tags_tag_id',how='left')['tags_tag_name']\\\n        .value_counts(ascending=True).tail(20).plot.barh()\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.9 Top tags in professionals","metadata":{}},{"cell_type":"code","source":"tag_users[tag_users['tag_users_user_id'].isin(professionals['professionals_id'])]\\\n        .merge(tags,left_on='tag_users_tag_id',right_on='tags_tag_id',how='left')['tags_tag_name']\\\n        .value_counts(ascending=True).tail(20).plot.barh()\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.10 Average question response days per year","metadata":{}},{"cell_type":"code","source":"afirst = answers.groupby('answers_question_id').min()['answers_date_added'].rename('firstA')\nalast = answers.groupby('answers_question_id').max()['answers_date_added'].rename('lastA')\ntemp = questions.merge(pd.DataFrame(afirst),left_on='questions_id',right_index=True,how='left')\\\n            .merge(pd.DataFrame(alast),left_on='questions_id',right_index=True,how='left')[['questions_date_added','firstA','lastA']]\ntemp['firstA']=(temp['firstA']-temp['questions_date_added']).dt.days\ntemp['lastA']=(temp['lastA']-temp['questions_date_added']).dt.days\ntemp['year']=temp['questions_date_added'].dt.year\ntemp.groupby('year').mean().plot()\nplt.ylabel('Days')\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.11 Emails per year","metadata":{}},{"cell_type":"code","source":"emails['emails_date_sent'].dt.year.value_counts().sort_index().plot()\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.12 Questions-email distribution","metadata":{}},{"cell_type":"code","source":"matches.groupby('matches_email_id').size().hist(bins=range(15),grid=False)\nplt.xlabel('Question #')\nplt.ylabel('Email #')\nplt.show()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. NLP - Content Similarity Filtering\n\nBuild a LDA model on the full question text (headline+body). Given a new question, find the most similar question and get the professionals who answered that question. ","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Process training data","metadata":{}},{"cell_type":"code","source":"import spacy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create full text for questions\ntestData = questions.head(100)\ntestData['full_txt'] = testData['questions_title']+' '+testData['questions_body']\ntestData['full_txt']= testData['full_txt'].str.replace('#',' ')\n\n# Apply spaCy part-of-speech to tokenize the text\ntoken_pos = ['NOUN', 'VERB', 'PROPN', 'ADJ', 'INTJ', 'X']\nnlp = spacy.load(\"en_core_web_sm\")\ndpipe = nlp.pipe(testData['full_txt'],disable=[\"parser\",\"ner\"])\ntokens=[]\nfor doc in dpipe:\n    tokens.append([t.lower_ for t in doc if (t.pos_ in token_pos and not t.is_stop and t.is_alpha)])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Train LDA model","metadata":{}},{"cell_type":"code","source":"import gensim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Gensim dic from the tokens\nno_below = 20\nno_above = 0.6\nkeep_n = 8000\nlda_dic = gensim.corpora.Dictionary(tokens)\n##lda_dic.filter_extremes(no_below=no_below, no_above=no_above, keep_n=keep_n)\n\n# Create corpus from the dic\nlda_corpus = [lda_dic.doc2bow(doc) for doc in tokens]\n\n# Create tf-idf from corpus\ntfidf = gensim.models.TfidfModel(lda_corpus)\ntfidf_corpus = tfidf[lda_corpus]\n\n# Train model\nnum_topics = 19\npasses = 15\nchunksize = 1000\nalpha = 1/50\nseed=13\nlda_model = gensim.models.ldamodel.LdaModel(tfidf_corpus, num_topics=num_topics, \\\n                                            id2word = lda_dic, passes=passes,\\\n                                            chunksize=chunksize,update_every=0,\\\n                                            alpha=alpha, random_state=seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Find similar questions and corresponding professionals","metadata":{}},{"cell_type":"code","source":"# Convert the query to LDA space\ndoc = \"I would love to be a civil engineer one day\"\nvec_bow = lda_dic.doc2bow(doc.lower().split())\nvec_corpus = tfidf[vec_bow]\nvec_lda = lda_model[vec_corpus] \n\n# Transform tfidf corpus to LDA space and index it\nindex = gensim.similarities.MatrixSimilarity(lda_model[tfidf_corpus])\n\n# Perform a similarity query against the tfidf corpus\nsims = index[vec_lda]  \nsims = sorted(enumerate(sims), key=lambda item: -item[1])\n\n# Get the most similar question\nq_id = testData['questions_id'][sims[0][0]]\nprint(testData.iloc[sims[0][0]])\nprint(testData['full_txt'][sims[0][0]])\n\n# Get the professionals\nprof_id = answers['answers_author_id'][answers['answers_question_id']==q_id]\nprint(prof_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Collaberative Filtering\n\nThe approach above can find most-likely professaionals who had answered similar questions. We can further find more professaionals who behave similarly as the most-likely ones, though they haven't answered any of the top similar questions. ","metadata":{}},{"cell_type":"markdown","source":"### 6.1 Create question-professional matrix","metadata":{}},{"cell_type":"code","source":"from scipy.sparse import csr_matrix\n\n# Reindex professionals and questions with integer\ntemp=answers.merge(professionals,how='inner',left_on='answers_author_id',right_on='professionals_id')\ntemp=temp.merge(questions,how='inner',left_on='answers_question_id',right_on='questions_id')\ntemp['question'] = temp['questions_id']\\\n                        .apply(lambda x : np.argwhere(questions['questions_id'] == x)[0][0])\ntemp['prof'] = temp['professionals_id']\\\n                        .apply(lambda x : np.argwhere(professionals['professionals_id'] == x)[0][0])\n\n\n# Create a sparse matrix for co-occurences\noccurences = csr_matrix((questions.shape[0], professionals.shape[0]), dtype='int8')\ndef set_occurences(q, p):\n    occurences[q, p] += 1\ntemp.apply(lambda row: set_occurences(row['question'], row['prof']), axis=1)\noccurences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2 Construct prof-prof matrix\nA co-occurrence matrix, the element of which indicates how many times two professionals answered the same question","metadata":{}},{"cell_type":"code","source":"cooc = occurences.transpose().dot(occurences)\ncooc.setdiag(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.3 Log-Likelihood Ratio (LLR)\nThe LLR function is computing the likelihood of two events, A and B appear together.\n\nk11, number of when both events appeared together\n\nk12, number of B appear without A\n\nk21, number of A appear without B\n\nk22, number of other things appeared without both of them\n\n","metadata":{}},{"cell_type":"code","source":"# Functions to compute LLR\ndef xLogX(x):\n    return x * np.log(x) if x != 0 else 0.0\ndef entropy(x1, x2=0, x3=0, x4=0):\n    return xLogX(x1 + x2 + x3 + x4) - xLogX(x1) - xLogX(x2) - xLogX(x3) - xLogX(x4)\ndef LLR(k11, k12, k21, k22):\n    rowEntropy = entropy(k11 + k12, k21 + k22)\n    columnEntropy = entropy(k11 + k21, k12 + k22)\n    matrixEntropy = entropy(k11, k12, k21, k22)\n    if rowEntropy + columnEntropy < matrixEntropy:\n        return 0.0\n    return 2.0 * (rowEntropy + columnEntropy - matrixEntropy)\ndef rootLLR(k11, k12, k21, k22):\n    llr = LLR(k11, k12, k21, k22)\n    sqrt = np.sqrt(llr)\n    if k11 * 1.0 / (k11 + k12) < k21 * 1.0 / (k21 + k22):\n        sqrt = -sqrt\n    return sqrt\n\n# Compute LLR\nrow_sum = np.sum(cooc, axis=0).A.flatten()\ncolumn_sum = np.sum(cooc, axis=1).A.flatten()\ntotal = np.sum(row_sum, axis=0)\npp_score = csr_matrix((cooc.shape[0], cooc.shape[1]), dtype='double')\ncx = cooc.tocoo()\nfor i,j,v in zip(cx.row, cx.col, cx.data):\n    if v != 0:\n        k11 = v\n        k12 = row_sum[i] - k11\n        k21 = column_sum[j] - k11\n        k22 = total - k11 - k12 - k21\n        pp_score[i,j] = rootLLR(k11, k12, k21, k22)\n        \nresult = np.flip(np.sort(pp_score.A, axis=1), axis=1)\nresult_indices = np.flip(np.argsort(pp_score.A, axis=1), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.4 Find similar professionals","metadata":{}},{"cell_type":"code","source":"# Use the professional id from 5.3\nx = prof_id.iloc[0]\nprof_num = np.argwhere(professionals['professionals_id'] == x)\n# Find similar professionals\nprint(result[prof_num][0][0])\nprint(result_indices[prof_num][0][0])\n# Display the most similar professional id\nprint(professionals.loc[result_indices[prof_num][0][0][0],'professionals_id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}