{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Summary\n## The problem\nThis Kernel is an entry into the CareerVillage.org (CV) Kaggle competition with the following objective: develop a method to recommend relevant questions to the professionals who are most likely to answer them.\n\nThe proposed solution should support the CV metric:\nA good recommender system should help us to reach a target of a high (95%?) percent of questions which get a 10-quality-point-answer within 24 hours, without churning the Pros, and within the bounds of fairness.\n\n## Proposed Solution\nA triage question handler is proposed together with a set of recommendations to handle the cold start problem and a method that provides a list of suggested questions for professionals that visit the CV site.\nThe triage question handler compares a new question with all the previously asked questions and puts the question into one of three categories:\n1.\tWhen there are many good matches with similar previously answered questions, students should be given immediate feedback by displaying this list of similar questions. They could be asked if their question was answered or if they still want professionals to be asked. This gives old questions/answers new life and the opportunity for professionals to get regular feedback to promote continued engagement.\n2.\tWhen there are fewer less good matches with similar previously answered questions, there is a group of questions where the matches are still good enough to find professionals who will find the student's question relevant.\n3.\tWhen there are very few or no good matches with similar previously answered questions the system can immediately flag this problem. Ideally CV could recruit a group of very engaged professionals who are willing to get involved in answering these difficult to answer questions. In some cases that answer may require the development of a dialogue which would require an extension to the current system.\n\nA number of technologies are considered to drive the model. Six of these are explored in detail:\n\n* tfidf questions bags of words similarity\n\n* Word2Vec\n\n* FastText\n\n* GloVe\n\n* Universal sentence encoder (USE)\n\n* tfidf professionals bag of words similarity \n\nFor the first 5 models, the 10 best matches are found for the last 100 questions in the data set that are answered. An extensive but subjective comparison shows the FastText system is the most suitable with Word2Vec almost as good.\n\nThe subjective analysis of the last 20 questions shows that the FastText method can deliver 4 relevant similar question out of 5 whereas the tfidf method only delivers 2 relevant similar question out of 3.\n\nA process is then developed to use the recommendations to tune the model to fulfil the problem requirements.\n\n## Support\nThe proposed solution is supported by an extensive Exploratory Data Analysis and recommendations for further improving the effectiveness of the system.\n"},{"metadata":{},"cell_type":"markdown","source":"# Kernel Contents\n## 1 [Jared's Checklist](#checklist)\n## 2 [Exploratory Data Analysis (EDA)](#eda)\n## 3 [Cold Start](#coldstart)\n## 4 [Recommender technology for Engaged Professionals](#recommender)\n## 5 [Triage Question Handler](#triage)\n## 6 [Frequency and  Relevance](#FandR)\n## 7 [Tuning the Question Handler](#tuning)\n## 8 [Recommendations for further improvement](#recommend)\n## 9 [Looking for similar questions](#simq)\n\n###     [Method 1: tfidf](#m1)\n###     [Method 2: Word2vec and sentence embedding](#m2)\n###     [Method 3: Using Fasttext](#m3)\n###     [Method 4: Using Global Vectors](#m4)\n###     [Method 5: universal-sentence-encoder](#m5)\n###     [Method 6: Using tfidf to compare question bow with prof bow](#m6)\n## 10 [Comparing recommender engines](#compare)\n## 11 [Finding a list of relevant questions for a professional](#proflist)\n## 12 [Find similar professionals](#simprof)\n\n## 13 [EDA Code](#edacode)\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Jared's Checklist<a id='checklist'></a>\n\nJared has provided a checklist in the Discussion area that is a useful way of summarising this Kernel:\n\n#### Did you decide to predict Pros given a Question, or predict Questions given a Pro? Why?\nBoth directions are handled:\n* When a student asks a question, similar questions are found and the professionals who answered these question are predicted. Comparison of professionals also allows for the option of finding more professionals.\n\n* When a professional visits the website a list of relevant questions are provided by comparing the professional's bag of words with all the asked questions.\n\n#### Does your model address the \"cold start\" problem for Professionals who just signed up but have not yet answered any questions? How'd you approach that challenge?\n\nYes [there is a separate section on this](#coldstart). Ideally the professional should be encouraged to answer a few questions on the website as part of the on-boarding.\n\nIn addition a professional defined question frequency variable is [recommended](#maxfreq). This is the frequency with which professionals would like to receive questions. If questions are not being allocated at the desired frequency, then the top most relevant questions are sent to the professional which are similar to the ones the professional would have received on visiting the website. \n    \n#### Did your model have any novel approaches to ensuring that \"no question gets left behind\"?\n\nYes, the triage system immediately identifies question that are likely to be difficult to answer. Ideally these are directed to particularly engaged professionals who have “signed up” to answer difficult questions.\n\nIf this is impractical then the alternative is for these questions to be directed to professionals who have not received a question in their requested timeframe. Other questions which had not received an answer within 24 hours should also be allocated to these [professionals](#maxfreq2).\n\nThese difficult to answer questions should be identified separately from the relevant questions that could be directed to the professional on hitting the time limit with a message like:\n\n\"We are having trouble finding somebody who might be able to provide an answer to these questions. Do you think you have any useful information for the student or do you know someone who does. This way, the dangers of delivering irrelevant questions is overcome.\"\n\nSome tuning of this method would be needed. For instance it should probably be avoided for new professionals and the allocation could be based on relevance but with a lower threshold than normal.\n    \n#### What types of models did you try, and why did you pick the model architecture that you picked?\n\n[There is a section on this](#recommender).\n\n    \n#### Did you engineer any new data features? Which ones worked well for your model?\n\nI think there are a number of factors that make the recommender work well using the FastText technology:\n\n* Bringing all the question information together in one unit as the students use the interface is different ways and so the information is spread.\n\n* Cleaning the text in a bespoke way designed for the particular recommender technology\n\n* Using the cosine similarity measure to triage the questions so that they can be treated differently.\n\n* Developing a bag of words for the professional which mean that the system is continually and automatically learning more about the professional.\n\n* Using the professional defined question frequency variable to manage the relationship with the professional. As a result, new professionals do not need to be treated differently from seasoned professionls. Also all the professionals whose time period is about to elapse can be considered when we see that a question is not getting answered within the 24 hours target. In these circumstances the thresholds could be lowered.\n\n#### Is there anything built into your approach to protect certain Professionals from being \"overburdened\" with a disproportionately high share of matches?\n\nYes, the professional defined question frequency mentioned above makes sure that CV can tailor the emails to the individual professional. Professional would not receive emails more frequently than requested. However to ensure flexibility, when professionals answer a question they  should be asked if they would be happy to receive another question within the time frame set by the frequency.\n\n#### What do you wish you could use in the future (that perhaps we could try out on our own as an extension of your work)?\n\n[There is a separate section on this](#recommend).\n\nIf I had to prioritise, I would pick exploring ways of:\n* building on the interaction students have with the system so they provide more feedback and develop a richer understanding of their opportunities.\n* building ownership of the mission and loyalty within the professionals\n\n"},{"metadata":{"_uuid":"1446641dbed26a9443c3d2a6e0e56a5aeec082a0"},"cell_type":"markdown","source":"\n# Exploratory Data Analysis<a id='eda'></a>\n\nThis competition is, essentially, about changing the behaviour of the professionals.\nBy better targeting of questions to professionals, the aim is to make it more likely that they will respond.\n\nMuch of this EDA has therefore been designed to provide information and insights about the following:\n\n•\tif we want to change behavior it would be good to know what happens now\n\n•\tif we want to change behavior we need some way of measuring impact...so what happens now?\n\n \n## Data Summary\n\n[23,931](#questions) questions from 12,329 students  \n[51,123](#questions)  answers   \n[50,106](#pd) answers provided by [28,152 professionals](#professionals)  \n[6,679](#pd) answers provided where a professional has answered more questions than asked\n        * *hence professionals have some other way of accessing questions than emails*  \n[4.3m questions are sent to professionals in 1.85m emails](#pd)  \n        * *hence only about 1 in 100 questions sent to professional results in an answer*  \n            \n## Plots Summary\n\n[This diagram](#log_questions_answered) which plots the log of the number of questions answered against the  log of the number of questions asked for each professional, provides some useful insights:  \n* if the only source of answers was from emails sent to professionals then there should not be any points above the red line. But there are and many of these points are in the early stages of the professional interacting with CV  \n* after receiving about 10 questions the number of answers drops off and from 40 questions onwards there is no noticable increase in answers  \n* there is a huge range in the ratio of questions answered to questions asked: one professional has answered 1,710 question but has only been asked about 10 and there are some professionals who have been asked thousands of questions and have never provided an answer.  \n\n[Interest in answering questions drops off quickly](#activity)  but  there is weak evidence that those that stay continue to contribute.\n\n[This diagram](#answers v professionals) shows \"who answers\". The [top 5 professionals](#whoanswers) provide nearly 10% of the answers. 18k professionals have provided no answers. 9k professionals who have answered between 1 and 10 questions provide nearly half the of answers. The professionals who have answered between 11 and 40 questions provide nearly a quarter of the answers.  \n"},{"metadata":{},"cell_type":"markdown","source":"# Insights\n\nThe analysis shows that: \n1.\tIn their interaction with CV, professionals are not limited to receiving questions and that this alternative is significant in the early stages of the relationship. This alternative is actually due to the fact that the professional can go to the CV website directly to  access relevant questions. This is an important source of answers and the solution should provide for it.\n2.\tInterest in answering questions drops off quickly and there is weak evidence that those that stay continue to contribute. . Over 25% of answers are provided in the first day of involvement in CV.\n3.\tThere are a few hero professionals who answer many questions but if we are going to make a significant improvement in the number of answers, we need to focus on the professionals that drop out early. \n4.\tCV have set the aim of the competition to better target questions to professionals so it more likely that they will respond. As we need to focus on the early stages of the relationship with the professional, we should not rely on using data from the answers of long-standing professionals to guide the targeting. They are non-typical.\n5.\tProfessionals can access questions from the CV website and so are not restricted to answering emails. Care is therefore needed when using inferred links between questions and professionals to build a recommender. By answering, the professional indicates that the question is relevant but the contact method, ie email, should not be assumed.\n6.\tTags are a working part of the existing CV system with students being familiar with the idea and are happy to use them. Any solution should therefore include the option of the currently available free-form tags.\n7.\tStudents are free to invent tags. This has the benefit of allowing the data set to evolve and automatically respond to requirement changes. This occurred recently with the tag data-science.\n8.\tThe hypen in data-science is useful for simple term based recommenders as it can be retained or collapsed to “datascience”. This is much more useful for targeting than “data” and “science”. Bi-grams can also be useful here. To give this problem some context: a data scientist can receive many irrelevant requests if his tags are “data” and “science” whereas datascience is very targeted.\n9.\tHowever there are problems with tags: not all students use them and many are so generic that they are not useful for targeting. Some form of augmentation is needed and that is the essence of the job.\n10.\tAlthough we shouldn't completely ignore the requirements of the professionals still answering questions after, say 30 days, initial focus should be on these first few days. In fact the CV metric is based on 24 hours and so professionals should be persuaded to respond within that time frame. Most membership organisations have \"on-boarding\" programmes and so it would be useful to think about how an automatic recommender system could help. \n11.\tThe analysis, which shows that student questions can be broadly categorised into three groups, can be used during the design of the recommender:\n\n•\tOnes that have many good matches with similar previously answered questions \n\n•\tOnes that have fewer less good matches with similar previously answered questions\n\n•\tOnes that have very few or no good matches with similar previously answered questions\n\n12.\tThese questions that have very few or no good matches with similar previously answered questions are a problem. In many cases it will not be clear what the student really wants to know or the question can appear to be completely off topic or so general that the professional doesn’t know where to start. \n13.\tWhen presented with a question, a professional’s choices are either to answer or ignore the questions. Ignoring a question leaves a bad feeling and too many of these difficult questions leads, possibly, to the professional moving away from CV. Ideally professionals new to CV should only get \"just right\" questions. Can we build a recommender that is that sophisticated or is some mix of human and machine more practical?\n14.\tAre professionals leaving because of inadequate feedback. We all need to know that our efforts, especially voluntary ones, are appreciated. There is a comment and “hearts” opportunity for students. They can either comment or like an answers. \n15.\tUnfortunately few students use the comments and “likes” opportunity. This has two drawbacks:\n\n•\tProfessionals do not get enough feedback.\n\n•\tThe comments system and “likes” are so little used that they should not currently be used in any scoring mechanism during the design of the recommender.\n\n16. Having something in common is important. The formations of groups then is a good idea. Currently there are only a few groups with members and so groups cannot currently be useful in directing questions to professionals.\n17. Membership of schools_membership is a mixture of students and professionals. The number are small and so cannot currently be a major part of the recommender. However as having something in common is important, it is possible that this group could be used in the future to help prioritise which professionals should be asked the question. However relevance is usually more important.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Cold Start<a id='coldstart'></a>\n\nThe data shows that many professionals, those that answer the questions, don’t answer many questions. This could be because questions directed to them are not relevant and their motivation to help fades. This can happen with recommender systems for new users where there is insufficient interaction with the users to understand their interests.\n\nIn a typical collaborative filtering application like a movie recommender, there are problems when a new movie is added to the catalogue or a new user joins the group. Pure collaborative filtering algorithm rely on the item’s and user’s interactions to make recommendations. If there are no interactions there can be no recommendations or at least recommendations are likely to be of poor quality.\n\nWith CV, each question is like a new item with no collaborative filtering possible. Our problem is more of a content-based filtering application where items are recommended on the basis of the features the item possesses. We have the words in the question and the tags and have to make the best use of them.\n\nStrategies to deal with the cold start problem are more interesting when thinking about the new professionals. There is an interesting article on Wikipedia: https://en.wikipedia.org/wiki/Cold_start_(computing).\n\nSo should we focus on these new professionals. There are a lot of them and just answering one or two more questions would make a big difference to the quantity of answers. Or should we work on making life for the committed professional easier so they can answer more questions. There are a fewer of these people but the quality of their answers may be higher? The answer probably is we should attempt to do both.\n\nIdeally the recommender design should be seamless so that there is no distinction in the way that the core engine works with new and seasoned professionals.\n\nWhen professionals join, we don’t know very much about them. This cold start problem can be addressed by engagement processes, user interface design and working with meta data.\n\nHere are some recommended strategies, most of which can be implemented with relatively little disruption:\n\n•\tIt would be good if we made it easier for professionals to add tags and perhaps categorise them: eg, sector, skills, experience, location, education. \n\n•\tWhen professionals join they should be encouraged to go onto the website and answer a few questions. We can use the information from the answers to vastly improve the recommender.\n\n•\tElsewhere in this Kernel it is suggested that professionals are given the option of determining how often they would like to receive emails. When this period is over without them receiving an email, they should receive the the most relevant questions which they would have seen if they had gone to the website. This approach would work well for new professionals who will not have answered many questions.\n\n•\tUsing the professionals' bags of words it is possible to find similar professionals. This process can be used to introduce new professionals into the system as follows. The student asks a question, the system finds a similar question and identifies the professionals who answered the similar questions. In addition to sending the student's question to these professionals, similar professionals are found by comparing professionals and if these similar professionals are within a tunable threshold they would also receive the email. It would be very useful to record metrics for this approach to allow tuning.\n\n•\tWhen a question is presented, the professional only has two options: answer or ignore. When the professional ignores the question, we are losing valuable information. The professional will have spent time reading the question and should be given the opportunity to provide feedback. Not only is this useful to CV, it engages the professional and is good for motivation. Questions are likely to be ignored because they are either off-topic, too generic or out of the professionals’ experience. Initially, we could have 2 new buttons: “off-topic” and “not for me”. “Off topic” questions could be re-directed for the moderator to review. When a question is marked “not for me”, a supplementary question could be asked: was the question “too generic” or “not in your field”. “Too generic” should go to the moderator for review. \n\n•\tIn all cases (off-topic, too generic and not in your field), the question should be marked so that the professional doesn’t see it again. This is really important as currently revisits to the CV site are discouraging because professionals are faced with a list of questions that they have previously ignored.\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Recommender technology for Engaged Professionals<a id='recommender'></a>\n\nQuote from the [FastText website](https://fasttext.cc/docs/en/supervised-tutorial.html):\n\n\"The goal of text classification is to assign documents (such as emails, posts, text messages, product reviews, etc...) to one or multiple categories. Such categories can be review scores, spam v.s. non-spam, or the language in which the document was typed. Nowadays, the dominant approach to build such classifiers is machine learning, that is learning classification rules from examples. In order to build such classifiers, we need labeled data, which consists of documents and their corresponding categories (or tags, or labels).\"\n\nTo build a deep learning, ie neural net, recommender we need labelled data, an algorithm and a classifier. For this problem we have data but it is not labelled. Some data is self-labelled, like the link between questions and professionals that answered, but there is not sufficient data like this to produce training, validation and data sets.\n\nCV has experience of the FastText library and has given some thought to labelling the data. But this is not currently available and so a solution must be based on some more traditional text classification methods.\n\nThere has been some research on improving the word2vec methodology to improve document similarity accuracy. For instance the Word Mover algorithm looks promising. [This paper](http://proceedings.mlr.press/v37/kusnerb15.pdf) includes an interesting figure which shows for small documents there is actually little benefit in using these more sophisticated methods compared with a traditional tfidf bag of words (bow).\n\nSome questions are long but many are short. The CV metric includes the phrase “within the bounds of fairness”. This refers to the desire to ensure that those with less advantage are catered for. It seems likely that these students will ask shorter questions and so the recommender should be tuned for short questions.\n\nFor this reason topic based methods like LDA and to some extent LSA are not explored. These very useful technniques find multiple topics in a document but in our case we have some questions with very few words and the words are the topic. Other techniques like SVA are not explored because the data set is small enough to not benefit from scaling down.\n\nFive recommender engines are tested in the dataset. Two are based on tfidf. In addition WordwVec, GloVe and the universal sentence embedder solutions are developed. Results for each method are presented in dataframes.\n\n### Accuracy Scores\nOne way of measuring the success of a recommender would be to see if the recommender identified the professional who answered the student's question in the previously answered set. This could be considered to be a good measure as it would then be compared against the current method. However we know that many questions are answered via the website and so it would not be a fair comparision. Also it could be that the professional's answer was the only time the professional answered that type of question. We would then have no evidence, apart from tags, that the professional should be identified. Tags are useful but are only part of the information available. This method is therefore not recommended.\n\nAs it is difficult to produce quantitative measures to develop accuracy scores,  the dataframes below hold the results from the last 100 questions asked in the provided data that were a answered. This allows for inspection of a range of questions and does not suffer from the problem of cherry picking questions to show the validity of models.\n\nIn addition for each method I have provided a grading based on inspection of the last 20 questions in the following table:\n\nsimilar questions to students\n\nThis is the number of times that the recommender has a least one similar question that is deemed good enough to be presented to the student in the hope that it might answer the students question.\n\nfalse positives\n\nThis is the number of times the best match question that would have been provided by the recommender was not deemed to be sufficiently relevant by me.\n\nfalse negatives\n\nThis is the number of times the best match question was not provided by the recommender but I deemed it to be sufficiently relevant.\n\nRelevance of Top10 Qs\n\nOut of the top 10 best matches, this is the number that I deemed to be relevant on the basis that the person who provided the answer to the question would find the student's questions to be answerable.\n\nOf course, all these judgements are subjective but they do give us the chance of comparing recommender engines.\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%javascript \nIPython.OutputArea.auto_scroll_threshold = 9999;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\ndf_recommender_results = pd.DataFrame([['','','','',''],[13,16,18,15,12],[4,1,2,4,3],[4,1,0,3,3],['','','','',''],['','','','',''],[10,10,10,10,10], [10,10,10,6,10], [0,6,9,1,7],[0,0,0,1,0],[6,10,10,6,10],\n                                          [0,0,0,0,0], [0,0,3,0,0], [0,7,6,4,6],[10,10,10,10,10],[10,10,10,10,10],\n                                          [5,10,10,3,10], [8,9,10,10,9], [7,6,6,5,1],[10,10,9,10,10],[10,10,9,9,9],\n                                          [10,10,9,10,10], [10,9,9,8,10], [10,10,10,10,9],[7,10,10,8,10],[8,8,10,8,8],[131,155, 160,129,149],\n                                       [66,78,80,65,75]],\n    \n                                      columns=['tfidf', 'Word2Vec','FastText','GloVec','UniSE'],\n                index=['Similar Q for last 20 queries','similar questions to students available','false positives','false negatives','','Relevance of Top10 Qs','query0', 'query1','query2', 'query3','query4', 'query5','query6', 'query7',\n                      'query8', 'query9','query10', 'query11','query12', 'query13','query14', 'query15','query16', 'query17',\n                      'query18', 'query19','Total','%'])\ndisplay (df_recommender_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe subjective analysis of the last 20 questions shows that FastText method can deliver good matches to the student 18 times with only one result being wrong. This represents 94% accuracy or 1 in 20 wrong. In comparison, the tfidf method can only deliver good matches to the student 13 times with four results being wrong. This represents 69% accuracy or 1 in 3 wrong.Overall the results show that the FastText approach with sentence embedding using averaging based on tfidf gives best results.\n\nThe \"Relevance of Top10 Qs\" data is a better proxy for determining each methods suitability to respond to the challenge set. FastText delivers 4 relevant similar question out 5 wheras tfidf delivers 2 relevant similar question out 3. \n\nFastText provides the most relevant similar questions and hence links to professionals most likely to answer a student's query. It also provides the best false positives and negatives results alongside Word2Vec\n\nIt is not suprising that FastText and Word2Vec outperform tfidf but they are also better than GloVe and USE.\n\nGloVe is better than tfidf but the other methods ae much better. GloVe is based on a fixed vocabulary that is probably not a good reflection on the terms used by students when asking career questions. The dimensions of its vectors are also smaller at 200 than FastText, Word2Vec and USE.\n\nUSE uses a pre-trained vocabulary like GloVe but the vectors have been trained at a sentence level. The USE code here does not work on the Kaggle GPU and so the CPU has been used. \n\nAlthough USE produces good results, it does not seem necessary to use the more sophisticated methods on this problem and sentence embedding with FastText does the job. In fact the USE module became unavailable on the April 21st and so the associated code has been commented out in this version of the Kernel. This is disappointing but not critical because the FastText method is preferred for this application. \n\nIt is worth noting that the tag system is particularly useful with FastText and Word2Vec. Normally a number of tags are defined and they are collected and stay together in the bag of words. Word2Vec and FastText will allocate significance to the proximity of tags to each other and develop vectors to reflect this.\n\nBy regularly updating and using the professionals bag of words (method 6) rather than the question bag of words, the system can continue to learn about a professional and respond to changes in demand and vocabulary. This approach can be used with any of the other five recommender engines but is only developed for tfidf here.\n\nCV is updating the professionals following tags. These are included in the professionals bags of words and so automatically benefit. The professonals bags of words can be used in three ways:\n* to find questions when a professional visit the site\n* to find similar professionals when it is necessary to augment the number of professionals to email\n* to find professionals who would find a student's question relevant.\n"},{"metadata":{},"cell_type":"markdown","source":"# Triage Question Handler<a id='triage'></a>\n\n\nThe method involves regularly pre-processing all the questions that have been previously answered. All the text associated with a questions is lumped together in one bow. The reason for lumping everything together (title, body and tags) is that students use the interface in different ways. Some ask the question in the title, some put the tags in the body and others don’t bother with tags.\n\nWhen a question is asked it is compared with every previously answered question using the FastText method to produce a cosine similarity array with one value between 0 and 1 for every previously answered questions.\n \nDuring the EDA it was noticed that student questions can be broadly categorised into three groups\n* Ones that have many good matches with similar previously answered questions \n* Ones that have fewer less good matches with similar previously answered questions\n* Ones that have very few or no good matches with similar previously answered questions\n\nTwo cosine thresholds are selected to separate questions into the three categories described above:\n\n* When there are many good matches with similar previously answered questions, students should be given immediate feedback by displaying this list of similar questions. They could be asked if their question was answered or if they still want professionals to be asked. This gives old questions new life and the opportunity for professionals to get regular feedback to promote continued engagement.\n \n* When there are fewer less good matches with similar previously answered questions, the DataFrame shows that the matches are still good enough to find professionals who will find the question relevant.\n\n* When very few or no good matches with similar previously answered questions the system can immediately flag this problem. Ideally CV could recruit a group of very engaged professionals who are willing to get involved in answering these difficult to answer questions. In some cases that answer may require the development of a dialogue which would require an extension to the current system. \n\n\nIf the approach of recruiting  super engaged professionals is impractical another approach is possible by identifying professionals that have not received an email withing their preferred frequency and asking them the question. The question should be flagged as one \"that is a little out of their area but as CV are having trouble placing it could they provide an answer or do they someone who might?\"\n\nFor FastText current suggested thresholds:\n\nhigh threshold:  0.94\nlow threshold:   0.90\n\nAll questions, to a maximum of say 10, with a cosine similarity greater that the high threshold can be directed to the student.\n\nThe low threshold is triggered when the cosine similarity of 10th best match is below this threshold."},{"metadata":{},"cell_type":"markdown","source":"# Frequency and  Relevance<a id='FandR'></a>\n\n*\"A good recommender system should help us to reach a target of a high (95%?) percent of questions which get a 10-quality-point-answer within 24 hours, without churning the Pros, and within the bounds of fairness.\"*\n\nHow do we balance getting 95%+ questions answered against professional churn. The more professionals that are asked a question, the more likely we are to get a timely answer.\nBut it will be more likely that professionals will find the question irrelevant and the frequency of being asked intolerable.\n\nTo be able to work towards a balance we need metrics and these are difficult to come by. The aspirational metric above is particularly difficult as there are qualitative items in it. As a first stage towards the aspiration, we could simplify the metric as follows:\n\n#### % of questions answered in 24 hours\n\neasy to measure\n\n#### 10 quality point answer\n\nThis is difficult and some questions are difficult to answer comprehensively anyway. If we work on the basis that any answer is useful, shows someone cares and could be the start of a conversation, we could drop this requirement initially.\n\n#### without churn\n\nWe know from the EDA that there is a churn problem. It is likely that it can be improved by addressing the cold start issues mentioned above. We do need a measure of the number of active professional that is reliable and responsive. We could use: % of professionals who have either visited the site or opened an email in the last 10 days say.\n\n#### within the bounds of fairness\n\nThis is about making the platform accessible and designed for the people with less adavantage. It also is about recruiting professionals from the full range of employment opportunities and should include more trades. On this basis we could exclude this element from the relevance v frequency metrics as it is handled by design. A separate metric could be developed which interrogates professionals tags etc to categorise the professionals and then report on defficiencies in professions.\n\nWe therefore have % questions answered v active professionals. \n\nThis measure of active professionals should be a measure of the underlying active professionals such that in any given period being measured new professionals are excluded from the data. \n"},{"metadata":{},"cell_type":"markdown","source":"# Tuning the Question Handler<a id='tuning'></a>\n\nWe know that many questions are answered by professionals visiting the website. This is good as professionals can choose themselves what to answer and their answers expands our knowledge of them. \n\nAs far as emails are concerned we have four variables:\n\nhow many emails to send per questions\n* Currently on average, 100 emails are sent for each answer\n\nwho to send them to:\n*  Professionals who have answered similar questions or whose bow shows that the query could be relevant to the professional\n\nwhen:<a id='maxfreq'></a>\n*  We could ask professionals as part of the on-boarding to choose the max frequency of questions being sent to them: say the options are 1 per week, 1 per day or any. Also assume that the majority select 1 per week\n\nwhen to send them:\n* emails can be sent immediately, daily or weekly. Consider dropping the weekly option as it does not support the performance metric\n        \n     \nWe know there is a huge range of responses from professionals, ranging from 0 to 1,710 answers provided. However in the analysis below I use mean/average to help develop a tuning method:\n\nIn the last 12 months of data, 5,844 questions were asked, or 112 per week on average. \nLet's say we are going to target more answers per question on average, by going from the current 2 to 3. \nTarget a move to 20 emails per answer from the current 100. This should be expected as we provide an improved recommender\n\nEmails sent per week = 112 x 3 x 20 = 6,720\n\nActive professionals required to service, based on 1 per week = 6,720\n\nThis seems to be in the right ball park. 6,720 x 100/20 = 33,600 professionals would be needed if the answer frequence did not improve. This is too many and so other things would have to compensate. \n\n#### Question Frequecy<a id='maxfreq2'></a>\nIt would be useful to ask a professional who has just answered a question if they would be willing to answer another question without waiting for their selected time period to end, ie if they select to receive answers once per week, would they be willing to receive another questions within a week immediately after answering a question.\n\nAt the other end of the spectrum, we know that some professsionals don't get asked questions. So at the end of the time period they have selected for their email frequency, if a question has not been asked then they should receive an email with the top 10 relevant questions calculated in the same way as the top 10 questions for professionals visiting the website.\n\n\nTo tune this process then we need the following data:\n\n% q answered in 24 hours, q24\n\nrolling average number of questions being asked per week, q\n\nrolling average number of emails sent to professionals to receive one answer, e\n\naverage professional question frequency request, number per week, f\n\ntarget answers per question, apq\n\nactive professionals, ap\n\nactive professionals required, apr\n\n\nunderlying active professionals, uap\n\nemails to send per question = apq x e\n\napr = apq x e x q / f\n\nif apr > ap then alert\n\nmonitor q24 and uap for trends, \n\nWe know that CV gets about 2 answers from professionals per questions. The top 10 most relevant questions shown in the results dataframe for each of the recommender technologies would therefore identify the 20 professionals required to service the targets above. We also know from the results that far more professionals could be identified for the majority of the questions asked and so even the 100 emails per answer could be serviced adequately. When this approach does not deliver sufficient professionals to ask then similar professionals can be found by [comparing the professionals' bags of words](#simprof).\n"},{"metadata":{},"cell_type":"markdown","source":"# Recommendations for further improvement<a id='recommend'></a>\n\nThe subjective analysis of the last 20 questions shows that FastText method can deliver good matches to the student 18 timess with only one result being wrong. This represents 94% accuracy. This is pretty good and while Machine learning could beat it, it is likely that resource could be better allocated elsewhere. In comparison, the tfidf method can only deliver good matches to the student 13 timess with four results being wrong. This represents 69% accuracy. Having said that there is room for tuning the model. Some work has been done here but further improvement is possible of the parameters used to build the FastText model and the thresholds set in the tirage model.\n\nHaving something in common between a student and an answering professional has been recognised as being important. This could be location, interest or skill. The development of groups and schools_membership should therefore be encouraged. Currently membership of schools_membership is a mixture of students and professionals but the numbers are small and so cannot currently be a major part of the recommender. Membership of groups is also low. \n\nIn some questions, location is important, for instance: how do I get a law internship in New York?. Generally though subject relevance is more important. In this case, would a lawyer at some distance from NY provide a better answer than a teacher in the student's schools_membership. As CV develops, group membership and locations could be used in the future to help prioritise which professionals should be asked the question. At this stage it is probably better to rely almost totally on question similiarity. The one exception could be to take into account previous interactions between professional and student. Even in this case though, question similarity will probably be enough. If the students is trying to decide between being an engineer and a doctor, the student will probably get better advice from an engineer and a doctor rather than getting an opinion only from one of them who was the first to engage.\n\nThe GloVec prep code shows that 1% of terms used by students are not recognised by the standard vocab. This is due to spelling and typo errors and also concatenating words to make tags. Some form of auto correcton would be useful. Also concatenation can be a problem for some recommender models because by allowing freeform concatenating, the chance of finding matches is reduced. A Word2Vec model does not need this concatenation and, on balance, it would be better if concatenation was discouraged. The current CV experiment on providing tag suggestions would be ideal. It both ensures correct spelling and promotes well used tags.\n\nProfessional are represented far more than trades in the professionals database. It would be useful if this database was matched against some standard business category system so that data about professions/trades coverage could be established and monitored. Initiatives could then be designed to overcome areas of weaker coverage.\n\nCV already has a system of rewards and feedback to professionals showing how they are doing. As students are not currently providing enough feedback, other forms of feedback are provided that are generated by the system. Although it is good to get this feedback it is not as rewarding for a professional as direct feedback from a a student that the professional has helped. Finding ways of encouraging students to provide feedback would be useful:\n* Consider initially withholding next step information and telling a student it is available on ticking a box. The tick could be assocated with a measure of the usefulness of the answer.\n* Provide students with good matches to their question immediately and asking them to tick relevant one allows us to collect \"views\" and \"likes\" data for questions.\n* Asking students if they think an answer would be useful to other students as suggested by Andrea Madunic.\n\nOn relaunch it would be good to contact all the professionals that have shown interest in the past and deliver some good matches to them and monitor if re-engagement is successful.\n\nRe-engagement strategies could be developed to alert professionals that have not answered for a while to a question where the cosine match is high.\n \nIt is clear that there is a wide range of questions being asked and some are so generic or unclear that an answer is difficult to provide. In these cases it would help to coach the student in available choices and options and so developing content or a career content platform would be useful. This could prove to be impractically expensive but volunteers could perhaps provide content. Ideally, given the demographic, the content needs to be video based and interactive.\n\nThe CV platform will benefit from the development in deep learning in the future. CV are currently developing a suggested tag system using Fasttext which will be very useful. Automation of metrics would also be useful. For instance using something like BERT on a labelled set of answers could provide a machine learning method to grade answers.\n\n "},{"metadata":{},"cell_type":"markdown","source":"# Set up"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"markdown","source":"\n"},{"metadata":{"trusted":true,"_uuid":"fc63c76b5ed038b7cd172a263e512c7a30d3b1f9"},"cell_type":"code","source":"\"\"\"Read in the data\"\"\"\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nglove_path = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\n\n\nprofessionals = pd.read_csv('../input/data-science-for-good-careervillage/professionals.csv')\nemails = pd.read_csv('../input/data-science-for-good-careervillage/emails.csv')\nmatches = pd.read_csv('../input/data-science-for-good-careervillage/matches.csv')\nquestions = pd.read_csv('../input/data-science-for-good-careervillage/questions.csv')\nanswers = pd.read_csv('../input/data-science-for-good-careervillage/answers.csv')\ntag_questions = pd.read_csv('../input/data-science-for-good-careervillage/tag_questions.csv')\ntags = pd.read_csv('../input/data-science-for-good-careervillage/tags.csv')\ntag_users = pd.read_csv('../input/data-science-for-good-careervillage/tag_users.csv')\ncomments = pd.read_csv('../input/data-science-for-good-careervillage/comments.csv')\n\nquestion_scores = pd.read_csv('../input/data-science-for-good-careervillage/question_scores.csv')\nanswer_scores = pd.read_csv('../input/data-science-for-good-careervillage/answer_scores.csv')\n\ngroup_memberships = pd.read_csv('../input/data-science-for-good-careervillage/group_memberships.csv')\ngroups = pd.read_csv('../input/data-science-for-good-careervillage/groups.csv')\nschool_memberships = pd.read_csv('../input/data-science-for-good-careervillage/school_memberships.csv')\nstudents = pd.read_csv('../input/data-science-for-good-careervillage/students.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport time\nimport os\nimport nltk, string\nimport random\nfrom nltk.corpus import stopwords \nfrom nltk.stem import WordNetLemmatizer\n\nrandom_state = 21\n\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\nstop_words = set(stopwords.words('english'))\nstop_words.update(['gives'])\n\n\"\"\"number of columns in the results from a recommender run\"\"\"\nsample_len = 100\n\n'''remove punctuation, lowercase, stem'''\nremove_punctuation_map = dict((ord(char), ' ') for char in string.punctuation)    \ndef normalize(text):\n    return nltk.word_tokenize(text.lower().translate(remove_punctuation_map))\n\n\ndef cosine_sim(text1, text2):\n    tfidf = vectorizer.fit_transform([text1, text2])\n    return ((tfidf * tfidf.T).A)[0,1]\n\ndef takeSecond(elem):\n    return elem[1]\n\ndef clean_text(text):\n    text = text.lower().translate(remove_punctuation_map)\n    \n    return ' '.join(lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text))\n\n\nprint(os.listdir(\"../input\"))\n\n\n#pd.options.display.max_colwidth = -1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recommender Exploration<a id='recommender_exploration'></a>"},{"metadata":{},"cell_type":"markdown","source":"## Questions Bags of Words\n\nStudents use the title, body and tags sections in different ways. To capture all the information in one place we produce a bag of words.\n\nFirst step is to combine the question title and text and all the tags for the questions into one cell...\n"},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"questions_tags = questions.merge(right=tag_questions, how = 'left',\n                                            left_on ='questions_id',\n                                            right_on ='tag_questions_question_id')\n\nquestions_tagwords = questions_tags.merge(right=tags, how = 'left',\n                                            left_on ='tag_questions_tag_id',\n                                            right_on ='tags_tag_id')\n\nquestions_tagwords =questions_tagwords.sort_values('questions_id')\nquestions_tagwords = questions_tagwords.drop (['tag_questions_tag_id','tag_questions_question_id','tags_tag_id','questions_author_id'], axis = 1)\n\nquestions_tagwords_tb = questions_tagwords.copy()\nquestions_tagwords_tb['q_tb'] = questions_tagwords['questions_title'] + \" \" + questions_tagwords['questions_body']\n\nquestions_tagwords_tb = questions_tagwords_tb.drop (['questions_title','questions_body'], axis = 1)\nquestions_tagwords_tb_str = questions_tagwords_tb.copy()\nquestions_tagwords_tb_str ['tags'] = questions_tagwords_tb ['tags_tag_name'].map (str)\nquestions_tagwords_tb_str = questions_tagwords_tb_str.drop (['tags_tag_name'], axis = 1)\n\nfoo =lambda x:', '.join(x)\nagg_f = {'questions_id':'first', 'questions_date_added' : 'first' ,'q_tb': 'first','tags' : foo}\n\nquestions_q_tb_tags  = questions_tagwords_tb_str.groupby(by='questions_id').agg(agg_f)\n\nquestions_q_tb_tags  = questions_q_tb_tags.drop(['questions_id'], axis = 1)\nquestions_q_tb_tags  =questions_q_tb_tags .sort_values ('questions_date_added', ascending = False).reset_index()\n\n\n\nquestions_q_tb_tags.head(1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clean Text\n\nHere a \"clean text\" version is produced by moving to lower case, removing punctuation and lemmatizing all in the function clean_text. \n* Lemmatizing was particularly useful for tfidf as it removed the distinction between singular and plural.\n* Punctuation is replaced with a space to reduce concatenation problems. Actually though FastText could probably handle the concatenation of tags. However with the current setting only tags that appear more than once would be considered.\n\nThen we remove stop words using a standard english stop word list with the option of updating as shown in the set up cell. This option was only used for a test word \"gives\" as results were good without further tuning.\n\nThe result of the cleaning can be seen by comparing the full text (bow_f) with the clean text (bow).\n\nFurther tuning by adding the stop word \"would\" would probably improve the accuracy for question4 below.\n\nGiven the experience gained here, it is recommended that the cleaning function is bespoke to the particular recommender chosen for the CV task."},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\"\"\"Tags are repeated in the body of the question and so stop \"\"\"\n\"\"\"questions_bow  = questions_q_tb_tags.copy()\nquestions_bow ['bow'] = questions_bow['q_tb'] + \" \" + questions_bow['tags']\nquestions_bow  = questions_bow.drop(['q_tb','tags','questions_id'], axis = 1)\nquestions_bow  =questions_bow .sort_values ('questions_date_added', ascending = False).reset_index()\npd.options.display.max_colwidth = -1\"\"\"\nstart = time.time()\nquestions_bow  = questions_q_tb_tags.copy()\nquestions_bow = questions_bow.rename(columns={'q_tb': 'bow_f'})\n\nquestions_bow['bow'] = questions_bow.bow_f.apply(clean_text)\nquestions_bow['bow'] = questions_bow['bow'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\nquestions_bow  = questions_bow.drop(['tags'], axis = 1)\n#questions_bow  =questions_bow .sort_values ('questions_date_added', ascending = False).reset_index()\n\n\nend = time.time()\nprint('run time',end - start)\npd.options.display.max_colwidth = 500\nquestions_bow.head(5)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop the questions without answers.\n\nTo make connections between students and professionals we concentrate on the questions with answers.\n\nSome questions have no answers because they were still new when the data set was collected."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nq_with_answers_bow = questions_bow.merge(right=answers, how = 'left',\n                                            left_on ='questions_id',\n                                            right_on ='answers_question_id')\nq_with_answers_bow = q_with_answers_bow.dropna(how='any')\nq_with_answers_bow = q_with_answers_bow.drop (['answers_id','answers_author_id','answers_question_id','answers_date_added','answers_body'], axis=1)\nq_with_answers_bow  =q_with_answers_bow .sort_values ('questions_date_added', ascending = False)\n\nq_with_answers_bow.drop_duplicates( inplace = True)\nq_with_answers_bow = q_with_answers_bow.reset_index()\nq_with_answers_bow = q_with_answers_bow.drop (['index'], axis=1)\n\nq_with_answers_bow.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Produce a subset of the questions data, using the most recent...\n\nThe last questions was asked on 31/1/19.\n\n5,844 questions asked in the previous 12 months."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nquestions_bow['questions_date'] = pd.to_datetime(questions_bow['questions_date_added'])\nq_bow_n = questions_bow.copy()\nq_bow_n['questions_date'] = questions_bow['questions_date'].dt.normalize()\nq_bow_n = q_bow_n.drop (['questions_date_added'], axis = 1)\n\"\"\"Change date to include more questions\"\"\"\n\"\"\"last month is 304\"\"\"\nlast_qbow_full = q_bow_n[questions_bow.questions_date > '2018-01-31']\nlast_qbow_full.describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Produce a subset of the questions data for performance testing..."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Produce a subset of the questions data for performance testing\"\"\"\n\nhalf_qbow_full = q_bow_n[questions_bow.questions_date > '2017-01-1']\nhalf_qbow_full.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Run this cell to get a smaller random sample of the smaller question data set\"\"\"\n\nlast_qbow = last_qbow_full.sample(n=10, random_state = 42).reset_index()\nlast_qbow = last_qbow.drop (['index'], axis = 1)\nlast_qbow.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Produce a subset of the questions data, using the x most recent\"\"\"\n\n#last_qbow = q_with_answers_bow[0:50]\n\nlast_qbow = questions_bow[0:sample_len]\nlast_qbow.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Professionals Bags of Words\n\nWe want to get to know more about the professionals so that we can direct more relevant questions to them. We will do this by putting everything they have told us about themselves into a bag of words **together with all the words from the questions they have answered**.\n\nWe could also look at including the the words of the answers they have provided but there is the danger that they use different vocabulary from the students and so I will not do this initially. This could we tried later to see if relevance improves.\n\nGet the tags associated with professionals..."},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"professionals_tags = professionals.merge(right=tag_users, how = 'left',\n                                            left_on ='professionals_id',\n                                            right_on ='tag_users_user_id')\n\nprofessionals_tagwords = professionals_tags.merge(right=tags, how = 'left',\n                                            left_on ='tag_users_tag_id',\n                                            right_on ='tags_tag_id')\n\nprofessionals_tagwords =professionals_tagwords.sort_values('professionals_id')\nprofessionals_tagwords = professionals_tagwords.drop (['professionals_location','professionals_date_joined','tag_users_tag_id','tag_users_user_id','tags_tag_id'], axis=1)\n\nprofessionals_tagwords.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\"\"\"Convert the columns to strings, even though they already look like strings? \n    This is to make concatenation possible later...\"\"\"\ndf_p_q_str = professionals_tagwords.copy()\ndf_p_q_str ['tag'] = df_p_q_str ['tags_tag_name'].map (str)\ndf_p_q_str = df_p_q_str.drop (['tags_tag_name'], axis = 1)\ndf_p_q_str ['industry'] = df_p_q_str ['professionals_industry'].map (str)\ndf_p_q_str = df_p_q_str.drop (['professionals_industry'], axis = 1)\ndf_p_q_str ['job'] = df_p_q_str ['professionals_headline'].map (str)\ndf_p_q_str = df_p_q_str.drop (['professionals_headline'], axis = 1)\n\ndf_p_q_str.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"merge the tags\"\"\"\n\nfoo =lambda x:', '.join(x)\nagg_f = {'professionals_id':'first', 'industry': 'first','job': 'first','tag' : foo}\n\ndf_p_q= df_p_q_str.groupby(by='professionals_id').agg(agg_f)\ndf_p_q = df_p_q.drop (['professionals_id'], axis = 1).reset_index()\n\n\ndf_p_q.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\"\"\"Merge the questons answered by the professional to the professionals dataframe\"\"\"\n\ndf_p_a = df_p_q.merge(right=answers, how = 'left',\n                                            left_on ='professionals_id',\n                                            right_on ='answers_author_id')\ndf_p_a = df_p_a.drop (['answers_author_id','answers_date_added','answers_body'], axis=1)\n\ndf_p = df_p_a.merge(right=questions_bow, how = 'left',\n                                            left_on ='answers_question_id',\n                                            right_on ='questions_id')\n\ndf_p = df_p.drop (['answers_id','answers_question_id','questions_id'], axis=1)\ndf_p ['qbow'] = df_p ['bow'].map (str)\ndf_p = df_p.drop (['bow'], axis = 1)\ndf_p ['qbow_f'] = df_p ['bow_f'].map (str)\ndf_p = df_p.drop (['bow_f'], axis = 1)\n\n\ndf_p.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\"\"\"merge the tags\"\"\"\n\nfoo =lambda x:', '.join(x)\nagg_f = {'professionals_id':'first', 'industry': 'first','job': 'first','tag' : foo}\n\ndf_p_q= df_p_q_str.groupby(by='professionals_id').agg(agg_f)\ndf_p_q = df_p_q.drop (['professionals_id'], axis = 1).reset_index()\n\n\ndf_p_q.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\"\"\"Merge the questions\"\"\"\n\nFoo =lambda x:', '.join(x)\nagg_f = {'professionals_id':'first',  'industry': 'first','job': 'first','tag' : 'first', 'qbow' : foo, 'qbow_f' : foo}\n\ndf_p_bow  = df_p.groupby(by='professionals_id').agg(agg_f)\ndf_p_bow = df_p_bow.drop (['professionals_id'], axis = 1).reset_index()\ndf_p_bow = df_p_bow.sort_values('professionals_id')\n\n\ndf_p_bow['bow_f'] = df_p_bow['industry'] + \" \" + df_p_bow['job']+ \" \" + df_p_bow['tag']+ \" \" + df_p_bow['qbow_f']\n\ndf_p_bow['bow'] = df_p_bow['industry'] + \" \" + df_p_bow['job']+ \" \" + df_p_bow['tag']+ \" \" + df_p_bow['qbow']\ndf_p_bow = df_p_bow.drop (['industry','job','tag','qbow','qbow_f'], axis = 1).reset_index()\n\ndf_p_bow.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"\"\"\"drop the professionals who have not answered a question\"\"\"\n\ndf_p_nonan = df_p.dropna(how='any')\n\ndf_p_bow_nonan  = df_p_nonan.groupby(by='professionals_id').agg(agg_f)\ndf_p_bow_nonan = df_p_bow_nonan.drop (['professionals_id'], axis = 1).reset_index()\ndf_p_bow_nonan = df_p_bow_nonan.sort_values('professionals_id')\n\n\n\ndf_p_bow_nonan['bow'] = df_p_bow_nonan['industry'] + \" \" + df_p_bow_nonan['job']+ \" \" + df_p_bow_nonan['tag']+ \" \" + df_p_bow_nonan['qbow']\ndf_p_bow_nonan = df_p_bow_nonan.drop (['industry','job','tag','qbow'], axis = 1).reset_index()\n\n\ndf_p_bow_nonan.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looking for similar questions<a id='simq'></a>"},{"metadata":{},"cell_type":"markdown","source":"# Method 1: tfidf<a id='m1'></a>\n\ntdidf or tf-idf stands for term frequency–inverse document frequency. It is a statistic which reflects the importance of a word in a document in a collection of documents. The term increases with the number of times the word appears in the document and decreases with the number of times the word appears in the collection.\n\ntfidf is simple to calculate but does not accunt for context or meaning.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Function to return array of cosine similiarities for two arrays of questions using tfidf\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\"\"\"Find the tfidf cos between one set of questions and another\"\"\"\n\"\"\"Allow for the first array will be the total sample and the second a smaller sample\"\"\"\n\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n   \ndef get_sim_q_array (q_total,q_query):\n    #vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n    vectorizer = TfidfVectorizer(tokenizer=normalize)\n\n    \"\"\"q_query could be passed 1 or more queries\"\"\"\n    vectorizer.fit(q_total)\n    q_total_tfidf = vectorizer.transform(q_total)\n    q_query_tfidf = vectorizer.transform(q_query)\n    q_sim_array = cosine_similarity(q_total_tfidf, q_query_tfidf)\n    \n    return (q_sim_array)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance problem with tfidf<a id='production'></a>\n\nThis cell shows the first stage of a production implementation. The array output (q_sim) can be used to indentify similiar questions that can either be immediately presented to the student or can be used to find suitable professionals to be asked the questions.\n\nFor this test we are finding a similarity array for the last question in the set."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Single query to get tfidf similiarities\"\"\"\nstart = time.time()\n\nq_total = [\"\".join(x) for x in (q_with_answers_bow['bow'])]\nq_queries = [last_qbow.loc [0]['bow']]\nq_sim = get_sim_q_array (q_total,q_queries)\n\nend = time.time()\nprint('run time',end - start)\n#print(q_sim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The last cell and the next cell show that the relationship between procesing time vs questions in the data set is linear. This makes the use of tfidf questionable<a id='performance'></a> as the bank of questions expands due to the continued success of CV."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Single query to get tfidf similiarities for half questions for performance test\"\"\"\nstart = time.time()\n\nq_total = [\"\".join(x) for x in (half_qbow_full['bow'])]\nq_queries = [last_qbow.loc [0]['bow']]\nq_sim = get_sim_q_array (q_total,q_queries)\n\nend = time.time()\nprint('run time',end - start)\n#print(q_sim)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get the similarity array for all questions with answers v all questions with answers"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Multiple queries to get tfidf similiarities\"\"\"\n\nstart = time.time()\n\nq_total = [\"\".join(x) for x in (q_with_answers_bow['bow'])]\nq_queries = [\"\".join(x) for x in (q_with_answers_bow['bow'])]\nq_sim_tfidf_array = get_sim_q_array (q_total,q_queries)\n\nend = time.time()\nprint('run time',end - start)\n#print(q_sim_m_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"function to produce dataframe of results of similarity tests\"\"\"\ndef get_sim_results_with_threshold (column_head,index,sim_array,questions,query,h_threshold,l_threshold):\n\n    col_h = column_head + str(index)\n    \n    df_sim_q = pd.DataFrame({'Cosine':sim_array[:,index], col_h:questions['bow_f']})\n\n    df_sim_q_sorted = df_sim_q.sort_values('Cosine',ascending = False )\n    if df_sim_q_sorted.iloc[0]['Cosine'] > .9999:\n        df_sim_q_sorted = df_sim_q_sorted.drop(df_sim_q_sorted.index[0])\n\n    h_num = 0\n    l_num = 0\n    worst_h_num = -1\n    i = 0\n    questions_len = len(questions)\n    while i< questions_len and df_sim_q_sorted.iloc[i]['Cosine'] > l_threshold:\n        #print ('i, df_sim_q_sorted.iloc[i]['Cosine']')\n        if df_sim_q_sorted.iloc[i]['Cosine'] > l_threshold:\n            l_num += 1\n            worst_match_to_profs= i\n        if df_sim_q_sorted.iloc[i]['Cosine'] > h_threshold:\n            worst_h_num = i\n            h_num += 1\n        i += 1\n    \n    df_sim_q_sample = df_sim_q_sorted[:10]\n        \n    best_cos_0 = df_sim_q_sample.iloc[0]['Cosine']\n    best_cos_9 = df_sim_q_sample.iloc[9]['Cosine']\n    \n    df_sim_q_sample = df_sim_q_sample.drop ('Cosine', axis=1).reset_index()\n    df_sim_q_sample = df_sim_q_sample.drop ( 'index', axis=1)\n\n    df_sim_q_sample_T = df_sim_q_sample.T\n    df_sim_q_sample_T.insert(loc=0, column='query_id', value=[query.iloc[index]['questions_id']] )\n    df_sim_q_sample_T.insert(loc=1, column='query_bow', value=[query.iloc[index]['bow_f']]  )\n    df_sim_q_sample_T.insert(loc=2, column='best_cos', value=best_cos_0)\n    df_sim_q_sample_T.insert(loc=3, column='10th_best_cos', value=best_cos_9)\n    df_sim_q_sample_T.insert(loc=4, column='similar Q to students', value= h_num)\n    df_sim_q_sample_T.insert(loc=5, column='Qs to profs', value=l_num)\n    df_sim_q_sample_T.insert(loc=6, column='best matches', value=' ')\n\n    if worst_h_num > -1:\n        df_sim_q_sample_T.insert(loc=17, column='worst match to students', value=df_sim_q_sorted.iloc[worst_h_num][col_h])\n    else:\n        df_sim_q_sample_T.insert(loc=17, column='worst match to students', value='not available')\n    \n    if l_num > 0:\n        df_sim_q_sample_T.insert(loc=18, column='worst match to profs', value=df_sim_q_sorted.iloc[worst_match_to_profs][col_h])\n    else:\n        df_sim_q_sample_T.insert(loc=18, column='worst match to profs', value='not available')\n    \n    \n    \n    return ( df_sim_q_sample_T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## tfidf similarity results<a id='tfidfsimilarityresults'></a>\n\nThe DataFrame below shows the  results from the tfidf bows method for the last 100 questions asked that have answers.\n\nquery_id\n\nThis is the id of the questions asked\n\nquery_bow\n\nThis is all the text from the question lumped together: title, body, tags\n\nbest_cos\n\nThis is the cosine similarity between the query and the best match. The range is 0-1, the higher the better. When the best_match_cos is < than the high threshold, the best_match text is not displayed and the h_ andl_thrshold_q text displays the best_match and next_best instead. In this case the studen would not be presented with any previously answered questions\n\n10th_best_cos\n\nThis is the text of a question that has been previously answered which the 10th best match to the query\n\nsimilar Q to students\n\nThis is the number of previously answered questions with a cosine similarity greater than the high threshold set by the user. Is this case, it is the number of questions which could be presented to the student as they may answer the student's questions\n\nQs to profs\n\nThis is the number of previously answered questions with a cosine similarity greater than the low threshold. When this number is low (say less than 100) then the question could be directed to the highly engaged professionals who have committed to answer difficult to answer questions\n\nbest_matches\n\nThese are the texts of a 10 questions that has been previously answered which best match the query\n\nworst match to students\n\nThis is the first previously answered question above the high threshold. Its relevance to the query is a good indication that the high threshold has been set high enough to ensure that highly relevant answers are provided to the student\n\nworst match to profs\n\nThis is the first previously answered question above the low threshold. Its relevance to the query is a good indication that the low threshold has been set high enough to ensure that reasonably relevant questions are being selected to identify professional that are likely to answer the query\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Compare  q with q using tfidf\"\"\"\nh_threshold =0.45\nl_threshold =0.225\n\n\nresults_T = get_sim_results_with_threshold ('query',0,q_sim_tfidf_array,q_with_answers_bow,q_with_answers_bow,h_threshold,l_threshold)\n\nfor i in range(1,sample_len):\n    next_result = get_sim_results_with_threshold ('query',i,q_sim_tfidf_array,q_with_answers_bow,q_with_answers_bow,h_threshold,l_threshold)\n    results_T = pd.concat([results_T,next_result])\nresults_tfidf = results_T.T\npd.options.display.max_colwidth = 500\ndisplay(results_tfidf) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Plots of number of similiar questions above the thresholds\n\nThe plots show the wide variety in the number of responses and the need for a system that can handle both difficult to answer and easy to answer questions"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nq_num =[]\nfor i in range (sample_len):\n    q_num.append(i)\n\narea = np.pi*3\nplt.figure(figsize=(10,10))\nplt.xlim(0, sample_len)\nplt.ylim(0,200)\n\n# Plot\nplt.scatter(q_num, results_T['similar Q to students'], s=25, c='red', alpha=0.5, label = \"> high threshold\")\nplt.scatter(q_num, results_T['Qs to profs'], s=25, c='blue', alpha=0.5, label = \"> low threshold\")\n\nplt.title('Scatter plot showing similiar questions found for each query ')\nplt.xlabel('query')\nplt.ylabel('similiar questions')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot showing number of questions above the high threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"area = np.pi*3\nplt.figure(figsize=(10,10))\nplt.xlim(0, sample_len)\nplt.ylim(0,20)\n\n# Plot\nplt.scatter(q_num, results_T['similar Q to students'], s=25, c='red', alpha=0.5, label = \"> high threshold\")\n\nplt.title('Scatter plot showing similiar questions found for each query ')\nplt.xlabel('query')\nplt.ylabel('similiar questions')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"### Function to call when finding professionals chosen by recommenders"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sim_questions_id (column_head,index,sim_array,questions,query):\n\n    col_h = column_head + str(index)\n    \n    df_sim_q = pd.DataFrame({'Cosine':sim_array[:,index], col_h:questions['questions_id']})\n    \n    \n    df_sim_q_sorted = df_sim_q.sort_values('Cosine',ascending = False )\n    if df_sim_q_sorted.iloc[0]['Cosine'] > .9999:\n        df_sim_q_sorted = df_sim_q_sorted.drop(df_sim_q_sorted.index[0])\n        \n    df_sim_q_sample = df_sim_q_sorted[:10]\n    \n    df_sim_q_sample = df_sim_q_sample.drop ('Cosine', axis=1).reset_index()\n    df_sim_q_sample = df_sim_q_sample.drop ( 'index', axis=1)\n\n    \n    df_sim_q_sample_T = df_sim_q_sample.T\n    df_sim_q_sample_T.insert(loc=0, column='id', value=[query.iloc[index]['questions_id']] )\n    df_sim_q_sample_T.insert(loc=1, column='bow', value=[query.iloc[index]['bow_f']]  )    \n    \n    \n    return ( df_sim_q_sample_T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_top_q_ids (q_sim_array):\n    q_id_results_T = get_sim_questions_id ('query',0,q_sim_array,q_with_answers_bow,q_with_answers_bow)\n\n    for i in range(1,sample_len):\n        next_result = get_sim_questions_id ('query',i,q_sim_array,q_with_answers_bow,q_with_answers_bow)\n        q_id_results_T = pd.concat([q_id_results_T,next_result])\n    return q_id_results_T.T\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def professionals_to_ask (sim_q_results, index):\n    col = 'query' + str(index)\n\n    df_prof_with_a = sim_q_results[[col]]\n    df_prof_with_a = df_prof_with_a.rename(columns={col: 'questions_id'})\n    df_prof_with_a = df_prof_with_a.drop(['id','bow'], axis = 0)   \n    df_prof_with_a = df_prof_with_a.merge(right=answers, how = 'left',\n                                            left_on ='questions_id',\n                                            right_on ='answers_question_id')\n    df_prof_with_a = df_prof_with_a.merge(right=professionals, how = 'left',\n                                            left_on ='answers_author_id',\n                                            right_on ='professionals_id')\n    df_prof_with_a =  df_prof_with_a.drop (['questions_id','answers_author_id','professionals_headline','professionals_date_joined',\n                                        'answers_id','answers_date_added','answers_body',\n                                    'answers_question_id','professionals_location','professionals_industry'], axis = 1)\n    tot = df_prof_with_a.shape[0]\n    \n     \n    df_prof_with_a_T = df_prof_with_a.T\n    #df_prof_with_a_T.insert(loc=0, column='total', value = tot)\n    df_prof_with_a = df_prof_with_a_T.T\n    #df_prof_with_a = df_prof_with_a.rename(columns={'professionals_id': col})\n    df_prof_with_a.insert(loc=1, column='total', value = 0)\n    \n    return df_prof_with_a\n#tfidfx.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find professionals chosen using tfidf"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Compare  q with q using tfidf to get q_ids then prof_ids\"\"\"\nq_id_results = find_top_q_ids (q_sim_tfidf_array)\npd.options.display.max_colwidth = 500\nq_id_results.head(20) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"check that we have right ids!\"\"\"\n\"\"\"x = q_with_answers_bow.count()\nprint (x.questions_id)\ni =0 \nwhile i<x.questions_id:\n    row = q_with_answers_bow.iloc[i]\n    \n    if  row.questions_id == 'c0c9260091b3443f9c712d5ff2d2c2e0':\n        print ('q_with_answers_bow.iloc[i][questions_id]',row.questions_id)\n        print ('q_with_answers_bow.iloc[i][bow_f]',row.bow_f)\n    i += 1\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tfifd_prof_with_a = professionals_to_ask (q_id_results, 1)\ndf_tfifd_prof_with_a.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tfifd_prof_with_a = professionals_to_ask (q_id_results, 0)\n\n\nfor i in range(1,sample_len):\n    next_p_to_a = professionals_to_ask (q_id_results, i)\n    df_tfifd_prof_with_a = pd.concat([df_tfifd_prof_with_a,next_p_to_a],axis=0, sort=True)\n    \npd.options.display.max_colwidth = 500\npd.options.display.max_seq_items = 2000\n\ndf_tfidf_p_grouped = df_tfifd_prof_with_a.groupby('professionals_id').count()\ndf_tfidf_p_grouped = df_tfidf_p_grouped.sort_values('total',ascending = False )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_p_to_a.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Professional identified by tfidf\n\nThe cells below show that the 1,000 most relevant questions, according to tfidf, to the 100 last questions asked in the dataset were provided by:\n\n1,108 professionals\nwho provided 2,180 answers\nwith the majority of professionals providing just one answer during that period but one professional providing 81 answers for the 100 questions. This is the same professonal who has answered 1,710 questions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tfidf_p_grouped.sum(axis = 0, skipna = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tfidf_p_grouped.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nq_num =[]\nfor i in range (df_tfidf_p_grouped.shape[0]):\n    q_num.append(i)\n\narea = np.pi*3\nplt.figure(figsize=(10,10))\nplt.xlim(0, df_tfidf_p_grouped.shape[0])\nplt.ylim(0,100)\n\n# Plot\nplt.scatter(q_num, df_tfidf_p_grouped['total'], s=25, c='red', alpha=1)\n\nplt.title('Scatter plot showing number of queries answered by professionals  ')\nplt.xlabel('professional')\nplt.ylabel('queries answered')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_sim_tfidf_array = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Method 2: Word2vec and sentence embedding<a id='m2'></a>\n\nThere is a Kaggle tutorial on Word2Vec here: https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial\n\nHere is an article which explains of the idea behind Word2Vec: http://cgi.cs.mcgill.ca/~enewel3/posts/implementing-word2vec/\n\n\"One of these assumptions is the distributional hypothesis, which is the idea that the meaning of a word can be understood from the words that tend to be near it. For example “bread” might tend to show up near “eat”, “bake”, “butter”, “toast”, etc., and this entourage gives a signal of what “bread” means.\"\n\nHere is the original paper: https://arxiv.org/pdf/1301.3781.pdf\n\nThe first step to finding the similarity between questions is to convert all the words in the vocabulary to vectors..."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstart = time.time()\n\ntotal_q_bow = [\"\".join(x) for x in (q_with_answers_bow['bow'])]\n#vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\nvectorizer = TfidfVectorizer(tokenizer=normalize)\ntfidf = vectorizer.fit_transform(total_q_bow)\n\ncachedStopWords = stopwords.words(\"english\")\n#print(total_q_bow)\ntotal_q_bow_l  = [x.lower() for x in total_q_bow]\n#print(total_q_bow_l)\nall_words = [nltk.word_tokenize(x.translate(remove_punctuation_map)) for x in total_q_bow_l]\n\nfor i in range(len(all_words)):  \n    all_words[i] = [w for w in all_words[i] if w not in cachedStopWords]\n\n#print(all_words)\n\nend = time.time()\nprint('run time',end - start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"w2v_model= Word2Vec(all_words, min_count=2)\n\"\"\"\nfrom gensim.models import Word2Vec\n\nembed_size = 300\n\n#the model is set up as with the Kaggle paper above with the noted exceptions:\n# min_count =2 because the corpus is realtively small\n# window = 5 to capture more context around a word\nw2v_model = Word2Vec(min_count=2,\n                     window=5,\n                     size=embed_size,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=1)\n\nstart = time.time()\n\nw2v_model.build_vocab(all_words, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time.time() - start) / 60, 2)))\n\nw2v_model.train(all_words, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to train the model: {} mins'.format(round((time.time() - start) / 60, 2)))\n\n\n\nprint('w2v_model.corpus_count',w2v_model.corpus_count)\nvocabulary = w2v_model.wv.vocab  \n#print(vocabulary)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How good is word2vec\n\nThis test shows that with the vocabulary built from the questions, the technique is finding good matches. In this case the most similar words to police are shown..."},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.wv.most_similar(positive=[\"police\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question embedding\n\nIn the following code the word2vec word vectors are combined to produce vectors for each question. This is done by finding the average of all the embeddings improved by taking into account the tfidf scores as described in this paper: http://www2.aueb.gr/users/ion/docs/BioNLP_2016.pdf.\n\nThese question vectors are then used to find similarity using cosine similarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"sentence embedding for questions usig Word2Vec\"\"\"\nstart = time.time()\n\nrows, cols = tfidf.nonzero()\nprint (rows)\nprint (cols)\nrows_l = len(rows)\n\ns_embed = []\ns_embeds = []\ndividend = []\natStart = True\noldr = -1\nw_cnt = 0\nvocab = vectorizer.get_feature_names()\n\nfor i in range (rows_l):\n    r = rows[i]\n    c = cols[i]\n    if (oldr != r):\n        if (atStart == False):\n            #calc embedding for questions\n            s_embed = np.divide(dividend, divisor)\n            s_embeds.append(s_embed.flatten())\n            \n        else: \n            atStart = False\n        oldr = r\n        w_cnt = 0\n        dividend = np.zeros((1, embed_size))\n        divisor = 0\n\n       \n    #print('r,c,w_cnt',r,c,w_cnt)\n    word = vocab[c]\n    if word in w2v_model.wv.vocab:\n        wt = tfidf[r,c]\n        #print (wt, word)\n        w_embed = w2v_model.wv[word]\n        #print(w_embed)\n        #print(w_embed * wt)\n        dividend = np.add(dividend, w_embed * wt)\n        divisor += wt\n        w_cnt +=1\n#    else:\n#        print (word, \" not in vocab\")\ns_embed = np.divide(dividend, divisor)\ns_embeds.append(s_embed.flatten())\n#print (s_embeds)\nend = time.time()\nprint('run time',end - start)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word2Vec Advantage\n\nYou would expect Word2Vec sentence embedding to provide better comparisons between questions compared to tfidf because Word2Vec accounts for context as well as term frequency.\n\nAnother big advantage of Word2Vec is that the vectors of the corpus of questions can be stored and do not have to be recalculated for each new question run. If the corpus could be rerun then it would be marginally better because all new words would be vectorised with relation to each other but the benefit is likely to be very small.\n\nBy re-running Word2Vec regularly it is possible to ensure that the vocabulary is a living record of how students ask questions and so relevance is improved."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Dataframe used to store embeddings\"\"\"\n\ndf_embed = pd.DataFrame({'col':s_embeds})\ndf_q_s_embed = pd.merge( questions_bow,df_embed, left_index=True, right_index=True)\ndf_q_s_embed.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Cosine similarity for all q v one q\"\"\"\n\nstart = time.time()\n\nq_embed_array = cosine_similarity(s_embeds, [s_embeds[0]])\nend = time.time()\nprint('run time',end - start)\n\n\"\"\"This should be compared against the tfidf method that takes about 12 secs to get the array\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Cosine similarity for all q v all q\"\"\"\n\nstart = time.time()\n\nq_embed_array = cosine_similarity(s_embeds, s_embeds)\nend = time.time()\nprint('run time',end - start)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Word2Vec Results\n\nThe key for the dataframe below can be found [here](#tfidfsimilarityresults).\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n\"\"\"Compare  q with q using sentence embedding\"\"\"\nh_threshold =0.84\nl_threshold =0.75\n\n\nresults_T = get_sim_results_with_threshold ('query',0,q_embed_array,q_with_answers_bow,q_with_answers_bow,h_threshold,l_threshold)\n\nfor i in range(1,sample_len):\n    next_result = get_sim_results_with_threshold ('query',i,q_embed_array,q_with_answers_bow,q_with_answers_bow,h_threshold,l_threshold)\n    results_T = pd.concat([results_T,next_result])\nresults_word2vec = results_T.T\npd.options.display.max_colwidth = 500\npd.options.display.max_seq_items = 2000\nend = time.time()\nprint('df run time',end - start)\ndisplay (results_word2vec) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Professionals chosen using Word2Vec"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Compare  q with q using Word2Vec to get q_ids then prof_ids\"\"\"\nq_id_results = find_top_q_ids (q_embed_array)\npd.options.display.max_colwidth = 500\nq_id_results.head(20) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"check that we have right ids!\"\"\"\nx = q_with_answers_bow.count()\nprint (x.questions_id)\ni =0 \nwhile i<x.questions_id:\n    row = q_with_answers_bow.iloc[i]\n    \n    if  row.questions_id == 'f46e757e38fa4243805534133ff9cb5b':\n        print ('q_with_answers_bow.iloc[i][questions_id]',row.questions_id)\n        print ('q_with_answers_bow.iloc[i][bow_f]',row.bow_f)\n    i += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Professionals identified by Word2Vec\n\nThe cells below show that the 1,000 most relevant questions, according to Word2Vec, to the 100 last questions asked in the dataset were provided by:\n\n1,198 professionals\nwho provided 2,2264 answers\nwith the majority of professionals providing just one answer during that period but one professional providing 79 answers for the 100 questions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_w2v_prof_with_a = professionals_to_ask (q_id_results, 0)\n\n\nfor i in range(1,sample_len):\n    next_p_to_a = professionals_to_ask (q_id_results, i)\n    df_w2v_prof_with_a = pd.concat([df_w2v_prof_with_a,next_p_to_a],axis=0, sort=True)\n    \npd.options.display.max_colwidth = 500\npd.options.display.max_seq_items = 2000\n\ndf_w2v_p_grouped = df_w2v_prof_with_a.groupby('professionals_id').count()\ndf_w2v_p_grouped = df_w2v_p_grouped.sort_values('total',ascending = False )\ndf_w2v_p_grouped.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_w2v_p_grouped.sum(axis = 0, skipna = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_embed_array.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_embed_array =[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Method 3: Using Fasttext with sentence embedding<a id='m3'></a>\n\nThis Kaggle blog alerted to me that FaxtText may produce better results than Word2Vec:\nhttps://www.kaggle.com/antonsruberts/sentence-embeddings-centorid-method-vs-doc2vec\n\nThe author Antons Rubert states:\n\n\"The main difference of FastText from Word2Vec is that it uses sub-word information (i.e character n-grams). While it brings additional utility to the embeddings, it also considerably slows down the process.\"\n\n\nThe method used here is identical to the one used for the Word2Vec model except that the vectors are calculated by FastText rather than Word2Vec.\n\nBy using FastText here, we retain all the advantages of the Word2Vec process and improve the accuracy.\n\n"},{"metadata":{},"cell_type":"markdown","source":"First build the FastText model and show that the vectors can produce word similarities in the question bow.."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"uncomment to see all_words\"\"\"\n#print (all_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import FastText\nstart = time.time()\nembed_size = 300\n\"\"\"all_words is a list of all the questions with the words separated and cleaned\"\"\"\nft_model = FastText(all_words, size=embed_size, window=5, min_count=2, workers=1\n                    ,sg=1)\nprint('Time to build FastText model: {} mins'.format(round((time.time() - start) / 60, 2)))\n\n\nft_model.wv.most_similar(positive=[\"police\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Question embedding\n\nIn the following code the FastText word vectors are combined to produce vectors for each question. This is done by finding the average of all the embeddings improved by taking into account the tfidf scores as described in this paper: http://www2.aueb.gr/users/ion/docs/BioNLP_2016.pdf.\n\nThese question vectors are then used to find similarity using cosine similarity."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Uncomment this to see how tdidfs are stored\"\"\"\n#print (tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"sentence embedding for questions usig FastText\"\"\"\nstart = time.time()\n\"\"\"tfidf is calculated in the Word2Vec section\"\"\"\n\"\"\"There a tfidf value for every word in all_words\"\"\"\nrows, cols = tfidf.nonzero()\nprint (rows)\nprint (cols)\nrows_l = len(rows)\n\ns_embed = []\ns_embeds = []\ndividend = []\natStart = True\noldr = -1\nw_cnt = 0\n\"\"\"using vectorization calculated in the Word2Vec section\"\"\"\nvocab = vectorizer.get_feature_names()\n\n#this method of calculating the embeddings is a bit ugly but takes advantage of how tfidfs are stored\n#for every question\nfor i in range (rows_l):\n    r = rows[i]\n    c = cols[i]\n    if (oldr != r):\n        #new questions and so store last embeddings\n        if (atStart == False):\n            #calc embedding for last questions\n            s_embed = np.divide(dividend, divisor)\n            s_embeds.append(s_embed.flatten())\n            \n        else: \n            atStart = False\n        oldr = r\n        w_cnt = 0\n        dividend = np.zeros((1, embed_size))\n        divisor = 0\n\n       \n    #find the next word\n    word = vocab[c]\n    if word in ft_model.wv.vocab:\n        #word is in the vocab and so calculate its contribution to the question vector\n        wt = tfidf[r,c]\n        #print (wt, word)\n        w_embed = ft_model.wv[word]\n        #print(w_embed)\n        #print(w_embed * wt)\n        dividend = np.add(dividend, w_embed * wt)\n        divisor += wt\n        w_cnt +=1\n#    else:\n#        print (word, \" not in vocab\")\ns_embed = np.divide(dividend, divisor)\ns_embeds.append(s_embed.flatten())\n#print (s_embeds)\nend = time.time()\nprint('Sentence embedding run time',end - start)\nstart = time.time()\n\nq_embed_array = cosine_similarity(s_embeds, s_embeds)\nend = time.time()\nprint('cosine sim time',end - start)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The key for the dataframe below can be found [here](#tfidfsimilarityresults)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Compare  q with q using sentence embedding\"\"\"\nstart = time.time()\n\nh_threshold =0.94\nl_threshold =0.9\n\nresults_T = get_sim_results_with_threshold ('query',0,q_embed_array,q_with_answers_bow,q_with_answers_bow,h_threshold,l_threshold)\nfor i in range(1,sample_len):\n#for i in range(1,20):\n\n    next_result = get_sim_results_with_threshold ('query',i,q_embed_array,q_with_answers_bow,q_with_answers_bow,h_threshold,l_threshold)\n    results_T = pd.concat([results_T,next_result])\nresults_FastText = results_T.T\npd.options.display.max_colwidth = 500\npd.options.display.max_seq_items = 2000\nend = time.time()\nprint('df time',end - start)\n\ndisplay (results_FastText) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Professionals identified by FastText\n\nThe cells below show that the 1,000 most relevant questions, according to Word2Vec, to the 100 last questions asked in the dataset were provided by:\n\n1,198 professionals\nwho provided 2,314 answers\nwith the majority of professionals providing just one answer during that period but one professional providing 78 answers for the 100 questions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Compare  q with q using FastText to get q_ids then prof_ids\"\"\"\nq_id_results = find_top_q_ids (q_embed_array)\npd.options.display.max_colwidth = 500\n#q_id_results.head(20) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_FT_prof_with_a = professionals_to_ask (q_id_results, 0)\n\n\nfor i in range(1,sample_len):\n    next_p_to_a = professionals_to_ask (q_id_results, i)\n    df_FT_prof_with_a = pd.concat([df_FT_prof_with_a,next_p_to_a],axis=0, sort=True)\n    \npd.options.display.max_colwidth = 500\npd.options.display.max_seq_items = 2000\n\ndf_FT_p_grouped = df_FT_prof_with_a.groupby('professionals_id').count()\ndf_FT_p_grouped = df_FT_p_grouped.sort_values('total',ascending = False )\ndf_FT_p_grouped.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_FT_p_grouped.sum(axis = 0, skipna = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_embed_array = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Method 4: Using Global Vectors<a id='m4'></a> \n\nFrom https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation:\n\n\"This dataset contains English word vectors pre-trained on the combined Wikipedia 2014 + Gigaword 5th Edition corpora (6B tokens, 400K vocab). All tokens are in lowercase. This dataset contains 50-dimensional, 100-dimensional and 200-dimensional pre trained word vectors. For 300-dimensional word vectors and additional information, please see the project website.\"\n\nFor these tests I have used the 200 dimension set of vectors.\n\nIn the following code the global word vectors are combined to produce vectors for each question. This is done by finding the average of all the embeddings improved by taking into account the tfidf scores in the same way as used for the Word2Vec test.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"start = time.time()\n\nglove_embed= dict()\nglove_data= open(glove_path)\nfor line in glove_data:\n    data = line.split(' ')\n    word = data[0]\n    vectors = np.asarray(data[1:], dtype='float32')\n    glove_embed[word] = vectors\n    \nglove_data.close()\nend = time.time()\nprint('run time',end - start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"sentence embedding for questions using Global Vectors\"\"\"\nstart = time.time()\n\nembed_size = 200\nrows, cols = tfidf.nonzero()\nprint (rows)\nprint (cols)\nrows_l = len(rows)\n\ns_embed = []\ns_embeds = []\ndividend = []\natStart = True\noldr = -1\nw_cnt = 0\nvocab = vectorizer.get_feature_names()\ntot_words = 0\nwords_not_in_gv = 0\nfor i in range (rows_l):\n    r = rows[i]\n    c = cols[i]\n    if (oldr != r):\n        if (atStart == False):\n            #calc embedding for questions\n            s_embed = np.divide(dividend, divisor)\n            s_embeds.append(s_embed.flatten())\n            \n        else: \n            atStart = False\n        oldr = r\n        w_cnt = 0\n        dividend = np.zeros((1, embed_size))\n        divisor = 0\n\n       \n    #print('r,c,w_cnt',r,c,w_cnt)\n    word = vocab[c]\n    #print (word)\n    wt = tfidf[r,c]\n    #print (wt, word)\n    if word in glove_embed:\n        w_embed = glove_embed[word]\n        dividend = np.add(dividend, w_embed * wt)\n        divisor += wt\n    else:\n        words_not_in_gv += 1\n    tot_words += 1    \n    w_cnt +=1\ns_embed = np.divide(dividend, divisor)\ns_embeds.append(s_embed.flatten())\nprint ('The following figures show the benefit of an auto correct or spell check on input')\nprint ('Number of words not in global vectors', words_not_in_gv,'total words', tot_words)\n#print (s_embeds)\nend = time.time()\nprint('run time',end - start)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Cosine similarity for all q v all q\"\"\"\n\nstart = time.time()\n\nq_embed_array = cosine_similarity(s_embeds, s_embeds)\nend = time.time()\nprint('run time',end - start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GloVe Results\n\nThe key for the dataframe below can be found [here](#tfidfsimilarityresults)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Compare  q with q using global vecs\"\"\"\nh_threshold =0.85\nl_threshold =0.75\n\n\nresults_T = get_sim_results_with_threshold ('query',0,q_embed_array,q_with_answers_bow,q_with_answers_bow,h_threshold,l_threshold)\n\nfor i in range(1,sample_len):\n    next_result = get_sim_results_with_threshold ('query',i,q_embed_array,q_with_answers_bow,q_with_answers_bow,h_threshold,l_threshold)\n    results_T = pd.concat([results_T,next_result])\nresults_glovec = results_T.T\npd.options.display.max_colwidth = 500\npd.options.display.max_seq_items = 2000\n\ndisplay (results_glovec) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Professionals chosen using GloVec"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Compare  q with q using GloVec to get q_ids then prof_ids\"\"\"\nq_id_results = find_top_q_ids (q_embed_array)\npd.options.display.max_colwidth = 500\nq_id_results.head(20) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_glo_prof_with_a = professionals_to_ask (q_id_results, 0)\n\n\nfor i in range(1,sample_len):\n    next_p_to_a = professionals_to_ask (q_id_results, i)\n    df_glo_prof_with_a = pd.concat([df_glo_prof_with_a,next_p_to_a],axis=0, sort=True)\n    \npd.options.display.max_colwidth = 500\npd.options.display.max_seq_items = 2000\n\ndf_glo_p_grouped = df_glo_prof_with_a.groupby('professionals_id').count()\ndf_glo_p_grouped = df_glo_p_grouped.sort_values('total',ascending = False )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Professional identified by GloVe\n\nThe cells below show that the 1,000 most relevant questions, according to GloVe, to the 100 last questions asked in the dataset were provided by:\n\n1,118 professionals\nwho provided 2,373 answers\nwith the majority of professionals providing just one answer during that period but one professional providing 87 answers for the 100 questions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_glo_p_grouped.sum(axis = 0, skipna = True) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_glo_p_grouped.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_embed_array =[]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Method 5: universal-sentence-encoder<a id='m5'></a>\n\n# The USE module became unavailable on the April 21st and so the associated code has been commented out in this version of the Kernel. This is disappointing but not critical because the FastText method is preferred for this application. \n\n\n\n\"Google’s Universal Sentence Encoder encodes text into high dimensional vectors.\nThe model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs.\n\nThe input is variable length English text and the output is a 512 dimensional vector.\"\n\nMore details can be found here:\nhttps://tfhub.dev/google/universal-sentence-encoder/2\n\nI found this blog useful:\nhttps://medium.com/@gaurav5430/universal-sentence-encoding-7d440fd3c7c7\n\nThe encoder provides a matrix defining the similarity between a set of questions. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Edit 1\"\"\"\n\n\"\"\"import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport os, sys\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# get cosine similairty matrix\ndef cos_sim(input_vectors):\n    similarity = cosine_similarity(input_vectors)\n    return similarity\n\n# get topN similar sentences\n\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n# Import the Universal Sentence Encoder's TF Hub module\nembed = hub.Module(module_url)\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Edit 2\"\"\"\n\n\"\"\"q_total = [\"\".join(x) for x in (q_with_answers_bow['bow'])]\n\n\nstart = time.time()\nwith tf.Session() as session:\n\n  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n  sentences_embeddings = session.run(embed(q_total))\n\nsimilarity_matrix = cos_sim(np.array(sentences_embeddings))\n\nprint (similarity_matrix)\nend = time.time()\nprint('run time',end - start)\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## USE Results\n\nThe key for the dataframe below can be found [here](#tfidfsimilarityresults)."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Edit 3\"\"\"\n\"\"\"h_threshold =0.75\nl_threshold =0.75\n\n\nuni_results_T = get_sim_results_with_threshold ('query',0,similarity_matrix,q_with_answers_bow,q_with_answers_bow,h_threshold,l_threshold)\n\nfor i in range(1,sample_len):\n    next_result = get_sim_results_with_threshold ('query',i,similarity_matrix,q_with_answers_bow,q_with_answers_bow,h_threshold,l_threshold)\n    uni_results_T = pd.concat([uni_results_T,next_result])\nuni_results = uni_results_T.T\npd.options.display.max_colwidth = 500\npd.options.display.max_seq_items = 2000\n\ndisplay (uni_results)\"\"\"\nuni_results = results_glovec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Professionals chosen using universal sentence embedder"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Edit 4\"\"\"\n\"\"\"Compare  q with q using USE to get q_ids then prof_ids\nq_id_results = find_top_q_ids (similarity_matrix)\npd.options.display.max_colwidth = 500\nq_id_results.head(20) \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_uni_prof_with_a = professionals_to_ask (q_id_results, 0)\n\n\nfor i in range(1,sample_len):\n    next_p_to_a = professionals_to_ask (q_id_results, i)\n    df_uni_prof_with_a = pd.concat([df_uni_prof_with_a,next_p_to_a],axis=0, sort=True)\n    \npd.options.display.max_colwidth = 500\npd.options.display.max_seq_items = 2000\n\ndf_uni_p_grouped = df_uni_prof_with_a.groupby('professionals_id').count()\ndf_uni_p_grouped = df_uni_p_grouped.sort_values('total',ascending = False )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Professionals identified by the universal sentence embedder\n\nThe cells below show that the 1,000 most relevant questions, according to USE, to the 100 last questions asked in the dataset were provided by:\n\n1,118 professionals\nwho provided 2,079 answers\nwith the majority of professionals providing just one answer during that period but one professional providing 87 answers for the 100 questions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_uni_p_grouped.sum(axis = 0, skipna = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_uni_p_grouped.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"similarity_matrix = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## Method 6: Using tfidf to compare question bow with professional bow<a id='m6'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_q_prof_with_threshold (column_head,index,sim_array,questions,query,h_threshold,l_threshold):\n\n    col_h = column_head + str(index)\n    \n    df_sim_q = pd.DataFrame({'Cosine':sim_array[:,index], col_h:questions['bow_f']})\n\n    df_sim_q_sorted = df_sim_q.sort_values('Cosine',ascending = False )\n    if df_sim_q_sorted.iloc[0]['Cosine'] > .9999:\n        df_sim_q_sorted = df_sim_q_sorted.drop(df_sim_q_sorted.index[0])\n\n    h_num = 0\n    l_num = 0\n    worst_h_num = -1\n    i = 0\n    questions_len = len(questions)\n    while i< questions_len and df_sim_q_sorted.iloc[i]['Cosine'] > l_threshold:\n        #print ('i, df_sim_q_sorted.iloc[i]['Cosine']')\n        if df_sim_q_sorted.iloc[i]['Cosine'] > l_threshold:\n            l_num += 1\n            worst_match_to_profs= i\n        if df_sim_q_sorted.iloc[i]['Cosine'] > h_threshold:\n            worst_h_num = i\n            h_num += 1\n        i += 1\n    \n        \n    df_sim_q_sample = df_sim_q_sorted[:10]\n    \n        \n    best_cos_0 = df_sim_q_sample.iloc[0]['Cosine']\n    best_cos_9 = df_sim_q_sample.iloc[9]['Cosine']\n    \n    df_sim_q_sample = df_sim_q_sample.drop ('Cosine', axis=1).reset_index()\n    df_sim_q_sample = df_sim_q_sample.drop ( 'index', axis=1)\n\n    df_sim_q_sample_T = df_sim_q_sample.T\n    df_sim_q_sample_T.insert(loc=0, column='id', value=[query.iloc[index]['questions_id']] )\n    df_sim_q_sample_T.insert(loc=1, column='bow', value=[query.iloc[index]['bow_f']]  )\n    df_sim_q_sample_T.insert(loc=2, column='best_cos', value=best_cos_0)\n    df_sim_q_sample_T.insert(loc=3, column='10th_best_cos', value=best_cos_9)\n    df_sim_q_sample_T.insert(loc=4, column='similar Q to students', value= h_num)\n    df_sim_q_sample_T.insert(loc=5, column='Qs to profs', value=l_num)\n    df_sim_q_sample_T.insert(loc=6, column='best matches', value=' ')\n\n    if worst_h_num > -1:\n        df_sim_q_sample_T.insert(loc=17, column='worst match to students', value=df_sim_q_sorted.iloc[worst_h_num][col_h])\n    else:\n        df_sim_q_sample_T.insert(loc=17, column='worst match to students', value='not available')\n    \n    if l_num > 0:\n        df_sim_q_sample_T.insert(loc=18, column='worst match to profs', value=df_sim_q_sorted.iloc[worst_match_to_profs][col_h])\n    else:\n        df_sim_q_sample_T.insert(loc=18, column='worst match to profs', value='not available')\n    \n    \n    \n    return ( df_sim_q_sample_T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\"\"\"Compare  q with prof using tfidf\"\"\"\nh_threshold =0.45\nl_threshold =0.225\n\n\nstart = time.time()\n\nq_total = [\"\".join(x) for x in (df_p_bow['bow'])]\nq_queries = [\"\".join(x) for x in (q_with_answers_bow['bow'])]\nq_sim_p_array = get_sim_q_array (q_total,q_queries)\n\nend = time.time()\nprint('run time',end - start)\n#print(q_sim_m_array)\n\nresults_prof_T = get_q_prof_with_threshold ('query',0,q_sim_p_array,df_p_bow,q_with_answers_bow,h_threshold,l_threshold)\n#for i in range(1,sample_len):\n#reduced to make ui managable\nfor i in range(1,20):\n    next_result = get_q_prof_with_threshold ('query',i,q_sim_p_array,df_p_bow,q_with_answers_bow,h_threshold,l_threshold)\n    results_prof_T = pd.concat([results_prof_T,next_result])\nresults_prof_tfidf = results_prof_T.T\npd.options.display.max_colwidth = 700\ndisplay(results_prof_tfidf) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sim_p_id (column_head,index,sim_array,questions,query):\n\n    col_h = column_head + str(index)\n    \n    df_sim_q = pd.DataFrame({'Cosine':sim_array[:,index], col_h:questions['professionals_id']})\n    \n    \n    df_sim_q_sorted = df_sim_q.sort_values('Cosine',ascending = False )\n    if df_sim_q_sorted.iloc[0]['Cosine'] > .9999:\n        df_sim_q_sorted = df_sim_q_sorted.drop(df_sim_q_sorted.index[0])\n        \n    df_sim_q_sample = df_sim_q_sorted[:20]\n    \n    df_sim_q_sample = df_sim_q_sample.drop ('Cosine', axis=1).reset_index()\n    df_sim_q_sample = df_sim_q_sample.drop ( 'index', axis=1)\n\n    \n    df_sim_q_sample_T = df_sim_q_sample.T\n    \"\"\"    df_sim_q_sample_T.insert(loc=0, column='id', value=[query.iloc[index]['questions_id']] )\n    df_sim_q_sample_T.insert(loc=1, column='bow', value=[query.iloc[index]['bow_f']]  )    \n    \"\"\"    \n    \n    return ( df_sim_q_sample_T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_top_p_ids (q_sim_array):\n    q_id_results_T = get_sim_p_id ('query',0,q_sim_array,df_p_bow,q_with_answers_bow)\n    for i in range(1,sample_len):\n        next_result = get_sim_p_id ('query',i,q_sim_array,df_p_bow,q_with_answers_bow)\n        q_id_results_T = pd.concat([q_id_results_T,next_result])\n    return q_id_results_T.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Compare  q with q using tfidf to get q_ids then prof_ids\"\"\"\np_id_results= find_top_p_ids (q_sim_p_array)\npd.options.display.max_colwidth = 500\np_id_results.head(2000) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"check that we have right ids!\"\"\"\n\"\"\"x = df_p_bow.count()\nprint (x.professionals_id)\ni =0 \nwhile i<x.professionals_id:\n    row = df_p_bow.iloc[i]\n    \n    if  row.professionals_id == '57a497a3dd214fe6880816c376211ddb':\n        print ('df_p_bow.iloc[i][professionals_id]',row.professionals_id)\n        print ('df_p_bow.iloc[i][bow_f]',row.bow_f)\n    i += 1\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Concatentate the professionals into one list"},{"metadata":{"trusted":true},"cell_type":"code","source":"p_id_results_T0 = get_sim_p_id ('query',0,q_sim_p_array,df_p_bow,q_with_answers_bow)\np_id_results =p_id_results_T0.T\nfor i in range(1,sample_len):\n    p_id_results_Tx = get_sim_p_id ('query',i,q_sim_p_array,df_p_bow,q_with_answers_bow)\n    p_id_resultsx = p_id_results_Tx.T\n    col_h = 'query' + str(i)\n    p_id_resultsx = p_id_resultsx.rename(columns={col_h: 'query0'})\n\n    p_id_results = pd.concat([p_id_results,p_id_resultsx],axis=0, sort=True)\n\np_id_results.describe()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_id_results = p_id_results.reset_index()\np_id_results = p_id_results.rename(columns={'query0': 'professionals_id'})\n\n#p_id_results_r = p_id_results_r.drop('index')\np_id_results.head(10)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_id_results_grouped = p_id_results.groupby('professionals_id').count()\np_id_results_grouped = p_id_results_grouped.rename(columns={'index': 'total'})\np_id_results_grouped = p_id_results_grouped.sort_values('total',ascending = False )\n\np_id_results_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_id_results_grouped.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (p_id_results.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q_sim_p_array = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing recommender engines<a id='compare'></a>"},{"metadata":{},"cell_type":"markdown","source":"This cell can be used to visuallycompare the results for one student question from the 5 question based methods.\n\nThree examples are chosen to show the similar questions found side by side.\n"},{"metadata":{},"cell_type":"markdown","source":"#### Example of a straight forward request about a job type\n\nHandled well by all methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Change the index number between 0 and sample_len \"\"\"\n\nindex = 0\n\ndef compare_methods(index):\n\n    col = 'query' + str(index)\n\n    tfidfx = results_tfidf[[col]]\n    tfidfx = tfidfx.rename(columns={col: 'tfidf'})\n    \n    senembedx = results_word2vec[[col]]\n    senembedx = senembedx.rename(columns={col: 'Word2Vec'})\n   \n    \n    senembedFTx = results_FastText[[col]]\n    senembedFTx = senembedFTx.rename(columns={col: 'FastText'})\n    \n\n    glovecx = results_glovec[[col]]\n    glovecx = glovecx.rename(columns={col: 'GloVe'})\n\n    univecx = uni_results[[col]]\n    univecx = univecx.rename(columns={col: 'USE'})\n\n    df_queryx = pd.concat([tfidfx,senembedx],axis=1, sort=False)\n    df_queryx = pd.concat([df_queryx,senembedFTx],axis=1, sort=False)\n    df_queryx = pd.concat([df_queryx,glovecx],axis=1, sort=False)\n    df_queryx = pd.concat([df_queryx,univecx],axis=1, sort=False)\n\n    \n    return df_queryx\n\ndisplay(compare_methods(index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Example of a general request\n\nhandled well by all methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 4\ncompare_methods(index)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Example of handling a spelling error\n\nFastText and Word2Vec to a lesser extent handle the incorrect spelling of pediatrician.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 2\ncompare_methods(index)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Professional who answered the questions\n\nIn the examples above, we can see that the four methods do provide similar results but the degree of overlap is difficult to analyse. The code below provides the statistics.\n\nNote these figure do  not include USE."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_tfidf_p_grouped.merge(right=df_w2v_p_grouped, how = 'outer',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\n\ndf_p_with_a = df_p_with_a.rename(columns={'total_x': 'tfidf'})\ndf_p_with_a = df_p_with_a.rename(columns={'total_y': 'Word2Vec'})\n\n\ndf_p_with_a = df_p_with_a.merge(right=df_FT_p_grouped, how = 'outer',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a = df_p_with_a.rename(columns={'total': 'FastText'})\n\n\n\ndf_p_with_a = df_p_with_a.merge(right=df_glo_p_grouped, how = 'outer',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\n\ndf_p_with_a = df_p_with_a.rename(columns={'total': 'GloVe'})\n\n\ndf_p_with_a = df_p_with_a.merge(right=df_uni_p_grouped, how = 'outer',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a = df_p_with_a.rename(columns={'total': 'USE'})\n\n\ndf_p_with_a.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following table shows that of the 2,440 professionals found only 394 are present in all 4 sets of results. The slight variation in these quoted numbers and the ones printed by running the code is due to some slight variation in the model outputs that is not significant."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_tfidf_p_grouped.merge(right=df_w2v_p_grouped, how = 'inner',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a = df_p_with_a.rename(columns={'total_x': 'tfidf'})\n\n\ndf_p_with_a = df_p_with_a.merge(right=df_FT_p_grouped, how = 'inner',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\n\n\n\ndf_p_with_a = df_p_with_a.merge(right=df_glo_p_grouped, how = 'inner',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\n\ndf_p_with_a = df_p_with_a.merge(right=df_uni_p_grouped, how = 'inner',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\n\n\ndf_p_with_a.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Comparing the overlap of professionals answering between 2 engines\n\nThe following cell outputs show that roughly each method shares half the professionals with each other method. Using two methods could therefore increase the number of identified professionals by 50%. However, if more professionals are required,  it is probably better to continue to use the best method and ask for the professionals who asked the next most relevant questions. The visual inspection of questions shows that the methods will find the same questions but they rank them differently."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_tfidf_p_grouped.merge(right=df_w2v_p_grouped, how = 'outer',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_tfidf_p_grouped.merge(right=df_w2v_p_grouped, how = 'inner',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_tfidf_p_grouped.merge(right=df_glo_p_grouped, how = 'outer',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_tfidf_p_grouped.merge(right=df_glo_p_grouped, how = 'inner',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_tfidf_p_grouped.merge(right=df_uni_p_grouped, how = 'outer',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_tfidf_p_grouped.merge(right=df_uni_p_grouped, how = 'inner',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_w2v_p_grouped.merge(right=df_glo_p_grouped, how = 'outer',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_w2v_p_grouped.merge(right=df_glo_p_grouped, how = 'inner',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_w2v_p_grouped.merge(right=df_uni_p_grouped, how = 'outer',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_w2v_p_grouped.merge(right=df_uni_p_grouped, how = 'inner',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_glo_p_grouped.merge(right=df_uni_p_grouped, how = 'outer',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_glo_p_grouped.merge(right=df_uni_p_grouped, how = 'inner',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_id_results_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tfidf_p_grouped.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_tfidf_p_grouped.merge(right=p_id_results_grouped, how = 'outer',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_p_with_a = df_tfidf_p_grouped.merge(right=p_id_results_grouped, how = 'inner',\n                                            left_on ='professionals_id',\n                                            right_on ='professionals_id')\ndf_p_with_a.shape\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding a list of relevant questions for a professional<a id='proflist'></a>\n\n This is used when a professional visits the website and needs to be presented with a set of questions that are relevant.\n \n In the production version, filtering would be required so that previously answered questions are not re-presented.\n \n In this code tfidf is used to provide the questions. Any of the other methods could also be used."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_prof_q_results (prof_index,dfs_p_bow,q_sim_p_array,q_with_answers_bow):\n\n    prof = 'prof ' + str(prof_index)\n    df_profs_q = pd.DataFrame({'Cosine':q_sim_p_array[:,prof_index], prof:q_with_answers_bow['bow_f']})\n\n    df_profs_q_sorted = df_profs_q.sort_values('Cosine',ascending = False )\n    if df_profs_q_sorted.iloc[0]['Cosine'] > .9999:\n        df_profs_q_sorted = df_profs_q_sorted.drop(df_profs_q_sorted.index[0])\n\n    \n    df_profs_q_sample = df_profs_q_sorted[:10]\n    best_cos_0 = df_profs_q_sample.iloc[0]['Cosine']\n    best_cos_9 = df_profs_q_sample.iloc[9]['Cosine']\n\n    df_profs_q_sample = df_profs_q_sample.drop ('Cosine', axis=1).reset_index()\n    df_profs_q_sample = df_profs_q_sample.drop ( 'index', axis=1)\n\n    df_profs_q_sample_T = df_profs_q_sample.T\n    df_profs_q_sample_T.insert(loc=0, column='professionals_id', value=[dfs_p_bow.iloc[prof_index]['professionals_id']] )\n    df_profs_q_sample_T.insert(loc=1, column='professionals_bow', value=[dfs_p_bow.iloc[prof_index]['bow']]  )\n    df_profs_q_sample_T.insert(loc=2, column='best_cos', value=best_cos_0)\n    df_profs_q_sample_T.insert(loc=3, column='10th_best_cos', value=best_cos_9)\n    df_profs_q_sample_T.insert(loc=4, column='Best Matches', value=' ')\n\n    return ( df_profs_q_sample_T)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Produce a subset of the professional data of all professionals\"\"\"\n\ndfs_p_bow = df_p_bow.sample(n=10, random_state = 21).reset_index()\ndfs_p_bow = dfs_p_bow.drop (['index','level_0'], axis = 1)\n\n\"\"\"Compare questions against the professionals bow\"\"\"\n\nq_total = [\"\".join(x) for x in (q_with_answers_bow['bow'])]\nq_queries = [\"\".join(x) for x in (dfs_p_bow['bow'])]\nq_sim_p_array = get_sim_q_array (q_total,q_queries)\n\n\"\"\"get results for all professionals sample\"\"\"\nresults_T = get_prof_q_results (0,dfs_p_bow,q_sim_p_array,q_with_answers_bow)\nprof_sample_len = 10\nfor i in range(1, prof_sample_len):\n    next_result = get_prof_q_results (i,dfs_p_bow,q_sim_p_array,q_with_answers_bow)\n    results_T = pd.concat([results_T,next_result])\nresults = results_T.T\npd.options.display.max_colwidth = 500\nresults.head(15)    \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find similar professionals<a id='simprof'></a>\n\n## This is used when we need to find more professionals to answer a particular question."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Compare nonan sample of professionals bow against the professionals bow\"\"\"\n\ndfs_p_bow_nonan = df_p_bow_nonan.sample(n=10, random_state = 21).reset_index()\ndfs_p_bow_nonan = dfs_p_bow_nonan.drop (['index','level_0'], axis = 1)\n\npd.options.display.max_colwidth = -1\n\nq_total = [\"\".join(x) for x in (df_p_bow['bow'])]\nq_queries = [\"\".join(x) for x in (dfs_p_bow_nonan['bow'])]\nq_sim_pp_array = get_sim_q_array (q_total,q_queries)\n\n\"\"\"get results for  p v p comparison\"\"\"\nresults_T = get_prof_q_results (0,dfs_p_bow_nonan,q_sim_pp_array,df_p_bow)\nprof_sample_len = 10\nfor i in range(1, prof_sample_len):\n    next_result = get_prof_q_results (i,dfs_p_bow_nonan,q_sim_pp_array,df_p_bow)\n    results_T = pd.concat([results_T,next_result])\nresults = results_T.T\npd.options.display.max_colwidth = 500\nresults.head(15)    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f34e5869ac40b57a67eeac2e0637e6899f981b8"},"cell_type":"markdown","source":"# EDA Code<a id='edacode'></a>\n\nThis competition is, essentially, about changing the behaviour of the professionals.\nBy better targeting of questions to professionals, the aim is to make it more likely that they will respond.\n\nMuch of this EDA has therefore been designed to provide information and insights about the following\n\n•\tif we want to change behavior it would be good to know what happens now\n\n•\tif we want to change behavior we need some way of measuring impact...so what happens now?\n"},{"metadata":{},"cell_type":"markdown","source":"## Setup"},{"metadata":{"trusted":true,"_uuid":"cee752a1ce564b35dbbbe1e782451c89c7f96adf"},"cell_type":"code","source":"\"\"\"Merge matches and emails to find total questions asked\"\"\"\n\"\"\"There are about 1.8m emails and 4.3m matches and so soe emails contain more than one question \"\"\"\n\nmatch_q_p = matches.merge(right=emails, how = 'left',\n                                            left_on ='matches_email_id',\n                                            right_on ='emails_id')\n\nmatch_q_p.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d438bf8dc0844d7cfebf0fb6538bbcf129b1a7ee"},"cell_type":"code","source":"match_q_p_simple = match_q_p.drop (['emails_id','emails_date_sent','emails_frequency_level'], axis=1)\nmatch_q_p_simple.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ed36503f72203fb320bcfdf34cb4a5815c8d7f7"},"cell_type":"code","source":"match_q_p_simple = match_q_p_simple.sort_values ('emails_recipient_id')\nmatch_recipents = match_q_p_simple.groupby('emails_recipient_id').count()\nmatch_recipents = match_recipents.sort_values ('emails_recipient_id')\nmatch_recipents = match_recipents.reset_index()\n\nmatch_recipents = match_recipents.drop ('matches_question_id', axis=1)\nmatch_recipents = match_recipents.rename(columns={'matches_email_id': 'questions_received'})\n\n\nmatch_recipents.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This section counts up the number of emails that each professional has been sent"},{"metadata":{"trusted":true,"_uuid":"47951ce2d6a54af192fc5d29db6e23aaf30e46d2"},"cell_type":"code","source":"\n\nemail_recipents = emails[['emails_id' ,'emails_recipient_id']]\nemail_recipents = email_recipents.groupby('emails_recipient_id').count()\nsorted_email_recipents = email_recipents.sort_values ('emails_recipient_id')\nsorted_email_recipents = sorted_email_recipents.reset_index()\n\ndf_profs_emails = professionals.copy()\n\ndf_profs_emails = df_profs_emails.sort_values('professionals_id')\ndf_profs_emails.reset_index(inplace =True, drop =True)\ndf_profs_emails = df_profs_emails.merge(right=sorted_email_recipents, how = 'left',\n                                            left_on ='professionals_id',\n                                            right_on ='emails_recipient_id')\n\ndf_profs_emails = df_profs_emails.drop ('emails_recipient_id', axis=1)\ndf_profs_emails = df_profs_emails.rename(columns={'emails_id': 'emails_received'})\ndf_profs_emails = df_profs_emails.fillna(0)\ndf_profs_emails.head() \n\n \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This cell shows that the people receiving the most emails have received over 3,000 in the last three years."},{"metadata":{"trusted":true,"_uuid":"8fbbd8804e619e69a8d396b3b6697afc633599f3"},"cell_type":"code","source":"df_profs_emails_q = df_profs_emails.merge(right=match_recipents, how = 'left',\n                                            left_on ='professionals_id',\n                                            right_on ='emails_recipient_id')\ndf_profs_emails_q = df_profs_emails_q.drop ('emails_recipient_id', axis = 1)\ndf_profs_emails_q_sorted = df_profs_emails_q.sort_values ('emails_received' ,ascending = False)\n\ndf_profs_emails_q_sorted.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This section counts the answers provided by each professional\n\nOne professional has answered 1,710 questions out of the 3,280 received."},{"metadata":{"trusted":true,"_uuid":"9f1231e8074add7af3402116425f3a937d9d70d4"},"cell_type":"code","source":"\n\"\"\"Using groupby to speed up data processing\"\"\"\n\nanswers_cut = answers[['answers_author_id' ,'answers_question_id']]\nanswer_count = answers_cut.groupby('answers_author_id').count()\nsorted_answer_count = answer_count.sort_values ('answers_author_id')\nsorted_answer_count = sorted_answer_count.reset_index()\n\nsorted_answer_count.head()\n\n\"\"\"Merge the info on answered questions to the professional df\"\"\"\n\ndf_profs_emails_answers = df_profs_emails_q.copy()\ndf_profs_emails_answers = df_profs_emails_answers.sort_values('professionals_id')\ndf_profs_emails_answers.reset_index(inplace =True, drop =True)\n\ndf_profs_emails_answers = df_profs_emails_answers.merge(right=sorted_answer_count, how = 'left',\n                                            left_on ='professionals_id',\n                                            right_on ='answers_author_id')\ndf_profs_emails_answers = df_profs_emails_answers.drop ('answers_author_id', axis=1)\ndf_profs_emails_answers = df_profs_emails_answers.rename(columns={'answers_question_id': 'questions_answered'})\ndf_profs_emails_answers = df_profs_emails_answers.fillna(0)\ndf_profs_emails_answers_sorted = df_profs_emails_answers.sort_values ('questions_answered' ,ascending = False)\n\ndf_profs_emails_answers_sorted.head() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6709c8a2f4583cb2227156eb3248e2c2195bebe"},"cell_type":"markdown","source":"## Comparing answers provided to questions asked\n**Scatter Plot**\nIn this plot, each dot represents the data of one professional and shows the relationship between questions asked and questions answered.\n\nIf you imagine a straight line going through 0,0 with a slop of 1 (45 degrees), then there should be no dots above the line.\n\n**Why?** because that would mean that more answers have been provided than questions. You can see that the majority of dots are below the line but there is a significant minority that are above it.\n\nThe reason for this is that professionals can go directly to the website to find questions. This is an important source of answers to CV."},{"metadata":{"trusted":true,"_uuid":"33e201e26673506de2f7ae79d8df4e6a01653dc1"},"cell_type":"code","source":"\"\"\"Scatter Plot for q answered v q asked\"\"\"\nimport matplotlib.pyplot as plt\nimport math\n\n\n\"\"\"Need to use log to get data spreading\"\"\"\ndf_profs_emails_answers ['log_questions_received'] = df_profs_emails_answers ['questions_received']\ndf_profs_emails_answers ['log_questions_answered'] = df_profs_emails_answers ['questions_answered']\n\ndef getlog (x):\n    if (x == 0):\n        x= 'NaN'\n    else:\n        x = math.log10(x)\n    return x\n   \n\ndf_profs_emails_answers['log_questions_received'] = df_profs_emails_answers['log_questions_received'].map(getlog)\ndf_profs_emails_answers['log_questions_answered'] = df_profs_emails_answers['log_questions_answered'].map(getlog)\ndf_profs_emails_truncatedanswers = df_profs_emails_answers.copy()\n\nplt.figure(figsize=(10,10))\nplt.scatter(df_profs_emails_answers['questions_received'],df_profs_emails_answers['questions_answered'],  color='k', s=25, alpha=0.2)\nplt.xlim(-5, 50)\nplt.ylim(-5,50)\nplt.plot([-5,50], [-5,50], 'k-', color = 'red')\n\nplt.xlabel('questions_received')\nplt.ylabel('questions_answered')\nplt.title('CareerVillage Q v A truncated at 50')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4d80eeeb1aa2555f39116997b0586f0bc46525d"},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.scatter(df_profs_emails_answers['log_questions_received'],df_profs_emails_answers['log_questions_answered'],  color='k', s=25, alpha=0.2)\nplt.plot([0,3], [0,3], 'k-', color = 'red'), plt.xlim(0, 3), plt.ylim(0,3)\nplt.xlabel('log_questions_received'),plt.ylabel('log_questions_answered')\nplt.title('CareerVillage Questions Chart')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8efdc8b7bafa87daf71da3f187a01a1ee231345"},"cell_type":"code","source":"df_profs_emails_answers ['a_q_not_asked'] =  df_profs_emails_answers['questions_answered'] - df_profs_emails_answers['questions_received']\ndf_profs_emails_answers ['a_q_not_asked'] = df_profs_emails_answers ['a_q_not_asked'].apply(lambda x: 0 if x < 0 else x)\nprint (df_profs_emails_answers ['a_q_not_asked'].sum())\ndf_profs_emails_answers.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# This section develops data to determine activity\n"},{"metadata":{"trusted":true,"_uuid":"d9f30a18abe2f636914667e3682f26cd8efe733f"},"cell_type":"code","source":"\ndf_profs_emails_answers['DateTime'] = pd.to_datetime(df_profs_emails_answers['professionals_date_joined'])\ndf_profs_emails_answers['date_joined'] = df_profs_emails_answers['DateTime'].dt.normalize()\ndf_profs_emails_answers = df_profs_emails_answers.drop(['professionals_date_joined','DateTime'],axis = 1)\n#df_profs_emails_answers['Day Joined'] = [2011-01-01]\ndf_profs_emails_answers.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f28b6e3f2b02bd8f434e173ad334c3dd0143adce"},"cell_type":"code","source":"\"\"\"Start processing emails to get first and last emails sent dates\"\"\"\n\"\"\"Perhaps don't need to sort but useful for manualchecking\"\"\"\nsorted_email_recipents = emails.sort_values  (['emails_recipient_id','emails_date_sent'])\nsorted_email_recipents.reset_index(inplace =True, drop =True)\n\nsorted_email_recipents.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72650e84925fcbc2544e27f3f733e40d8c02ea1d"},"cell_type":"code","source":"\"\"\"This section finds the days that professionals receive emails\"\"\"\n\"\"\"Very slow, working on 1.8m emails\"\"\"\nsorted_email_recipents_dates = sorted_email_recipents.copy()\nsorted_email_recipents_dates['email_date'] = pd.to_datetime(sorted_email_recipents['emails_date_sent'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45ae53d2eb94b84a5183bed8441e6c66b36b9664"},"cell_type":"code","source":"sorted_email_recipents_dates['email_date'] = sorted_email_recipents_dates['email_date'].dt.normalize()\nsorted_email_recipents_dates = sorted_email_recipents_dates.drop ('emails_date_sent', axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4c18051e69fdd63fce725d89692eae8392c65e5"},"cell_type":"code","source":"\nsorted_email_recipents_dates_min = sorted_email_recipents_dates.groupby('emails_recipient_id').min()\nsorted_email_recipents_dates_min = sorted_email_recipents_dates_min.rename(columns={'email_date':'first_email_date'})\nsorted_email_recipents_dates_min = sorted_email_recipents_dates_min.drop(['emails_id','emails_frequency_level'], axis = 1)\n\n\nsorted_email_recipents_dates_min.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17f081ff018335fc60b0076196973f235d9cfc83"},"cell_type":"code","source":"sorted_email_recipents_dates_max = sorted_email_recipents_dates.groupby('emails_recipient_id').max()\nsorted_email_recipents_dates_max = sorted_email_recipents_dates_max.rename(columns={'email_date':'last_email_date'})\nsorted_email_recipents_dates_max = sorted_email_recipents_dates_max.drop(['emails_id','emails_frequency_level'], axis = 1)\n\nsorted_email_recipents_dates_max.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"600eac6c7c37082dbc562a1183597dc1c98d51b4"},"cell_type":"code","source":"\nsorted_email_recipents_dates_min_max = sorted_email_recipents_dates_min.merge(right=sorted_email_recipents_dates_max, how = 'left',\n                                            left_on ='emails_recipient_id',\n                                            right_on ='emails_recipient_id')\n\n\nsorted_email_recipents_dates_min_max.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c060711fb6cfb3fe7906a0a213b8476e29acd2f7"},"cell_type":"code","source":"df_profs_emails_answers = df_profs_emails_answers.merge(right=sorted_email_recipents_dates_min_max, how = 'left',\n                                            left_on ='professionals_id',\n                                            right_on ='emails_recipient_id')\n\ndf_profs_emails_answers.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b68937da909ae160d7e0e654ba803dc770cf6818"},"cell_type":"code","source":"#df_profs_emails_answers['Days to 1st email'] = pd.to_datetime(df['date'])\ndf_profs_emails_answers['days_before_1st_email'] = df_profs_emails_answers['first_email_date'] - df_profs_emails_answers['date_joined']\ndf_profs_emails_answers['days_ns'] = df_profs_emails_answers['last_email_date'] - df_profs_emails_answers['first_email_date']\ndf_profs_emails_answers['days_emailed'] = df_profs_emails_answers['days_ns'].apply(lambda x: x.days)\ndf_profs_emails_answers = df_profs_emails_answers.drop('days_ns', axis = 1)\ndf_profs_emails_answers.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6e18610bcea35f75a73581fb260aa6d52e49cfd"},"cell_type":"code","source":"df_profs_emails_answers['emails_per_day'] = df_profs_emails_answers['emails_received'] / df_profs_emails_answers['days_emailed']\ndf_profs_emails_answers['answers_per_day'] = df_profs_emails_answers['questions_answered'] / df_profs_emails_answers['days_emailed']\n\ndf_profs_emails_answers['log_answers_per_day'] = df_profs_emails_answers['answers_per_day'] \ndf_profs_emails_answers['log_answers_per_day'] = df_profs_emails_answers['log_answers_per_day'].map(getlog)\n\ndf_profs_emails_answers['log_emails_per_day'] = df_profs_emails_answers['emails_per_day'] \ndf_profs_emails_answers['log_emails_per_day'] = df_profs_emails_answers['log_emails_per_day'].map(getlog)\n\ndf_profs_emails_answers.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Questions answered vs time being emailed\n\nIf professionals were staying with the program, we would expect to see a trend rising from left to right.\n\nWe do not see this trend which indicates that professional stop answering questions even though they continue to receive emails. It would be useful to know whether the professionals  open the emails or not. This information would lead to different re-engagement strategies.\n\nIt is also apparent that there a are a large number of professionals that do not answer even one question. A re-engagement strategy is also required for these people."},{"metadata":{"trusted":true,"_uuid":"e50cbab8015aba9275aee08346c9ce3ad552a07e"},"cell_type":"code","source":"\nplt.figure(figsize=(10,10))\nplt.scatter(df_profs_emails_answers['days_emailed'],df_profs_emails_answers['questions_answered'],  color='red', s=25, alpha=0.2)\n\nplt.xlim(-5, 250)\nplt.ylim(-5,50)\n\nplt.xlabel('days_emailed')\nplt.ylabel('questions_answered')\nplt.title('CareerVillage Questions Answered')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17842b2d5c91a71caf6d62108252a9080d0de7b8"},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.scatter(df_profs_emails_answers['days_emailed'],df_profs_emails_answers['log_questions_answered'],  color='red', s=25, alpha=0.2)\n\nplt.xlabel('days_emailed')\nplt.ylabel('log_questions_answered')\nplt.title('CareerVillage Questions Answered')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This section finds the days that professionals are active"},{"metadata":{"trusted":true,"_uuid":"32f5df1e086d290776825b3d57112615d58e2656"},"cell_type":"code","source":"\"\"\"This section finds the days that professionals are active\"\"\"\n\nanswers_author_date = answers[['answers_author_id' ,'answers_date_added']]\nanswers_author_date = answers_author_date.sort_values(['answers_author_id' ,'answers_date_added'])\nanswers_author_date.reset_index(inplace =True, drop =True)\nanswers_author_date['answer_date'] = pd.to_datetime(answers_author_date['answers_date_added'])\nanswers_author_date['answer_date'] = answers_author_date['answer_date'].dt.normalize()\nanswers_author_date = answers_author_date.drop ('answers_date_added', axis = 1)\n\nanswers_author_date_min = answers_author_date.groupby('answers_author_id').min()\nanswers_author_date_max = answers_author_date.groupby('answers_author_id').max()\nanswers_author_date_min_max = answers_author_date_min.merge(right=answers_author_date_max, how = 'left',\n                                            left_on ='answers_author_id',\n                                            right_on ='answers_author_id')\n\nanswers_author_date_min_max = answers_author_date_min_max.rename(columns={'answer_date_x':'first_answer'})\nanswers_author_date_min_max = answers_author_date_min_max.rename(columns={ 'answer_date_y' :'last_answer'})\n\nanswers_author_date_min_max.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot below shows the answers per day decay with time. This re-inforces the conculsion that professionals are not staying with the program. "},{"metadata":{"trusted":true,"_uuid":"4f0c573edec5f84529fbd1fbc8d91817befb4638"},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.scatter(df_profs_emails_answers['days_emailed'],df_profs_emails_answers['log_answers_per_day'],  color='red', s=25, alpha=0.2)\n\nplt.xlabel('days_emailed')\nplt.ylabel('log_answers_per_day')\nplt.title('CareerVillage Response ')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b3cd7cbb047a3766e8a364cb07734449aff2671"},"cell_type":"code","source":"df_profs_emails_answers_active = df_profs_emails_answers.merge(right=answers_author_date_min_max, how = 'left',\n                                            left_on ='professionals_id',\n                                            right_on ='answers_author_id')\ndf_profs_emails_answers_active.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa8e2811c6450aec7328751f3bec9320f5ade9cd"},"cell_type":"code","source":"df_profs_emails_answers_active['days_active_ns'] = df_profs_emails_answers_active['last_answer'] - df_profs_emails_answers_active['first_answer']\ndf_profs_emails_answers_active['days_active'] = df_profs_emails_answers_active['days_active_ns'].apply(lambda x: x.days)\ndf_profs_emails_answers_active = df_profs_emails_answers_active.drop('days_active_ns', axis = 1)\ndf_profs_emails_answers_active.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"349434f3d78436cfee188a686a87b3a4071a404d"},"cell_type":"markdown","source":"# Activity v Engagement<a id='activity'></a>\n\nThe following plot explores the relationship between engagement, measured as the number of days between the first and last quesion answered, and activity, measured as the number of questions answered.\n\nThe plot shows a weak upward trend for those professions that stay active. \nThe density of the plots shows the number of professionals and this indicates that many do not stay active\n\n[Go to summary](#summary)   "},{"metadata":{"trusted":true,"_uuid":"60198e661b40e0607a786763141d1b079f8b3984"},"cell_type":"code","source":"df_profs_emails_answers_active['log_questions_answered'] = df_profs_emails_answers_active['questions_answered'] \ndf_profs_emails_answers_active['log_questions_answered'] = df_profs_emails_answers_active['log_questions_answered'].map(getlog)\nplt.figure(figsize=(10,10))\nplt.scatter(df_profs_emails_answers_active['days_active'],df_profs_emails_answers_active['log_questions_answered'],  color='red', s=25, alpha=0.2)\nplt.xlabel('days_active')\nplt.ylabel('log_questions_answered')\nplt.title('CareerVillage Activity ')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcd4b15bc7e72ab79a0930dfdfc0b52ffb4d48c8"},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.scatter(df_profs_emails_answers_active['days_active'],df_profs_emails_answers_active['questions_answered'],  color='red', s=25, alpha=0.2)\n\nplt.xlim(-50, 500)\nplt.ylim(-5,50)\n\nplt.xlabel('days_active')\nplt.ylabel('questions_answered')\nplt.title('CareerVillage Activity ')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7d7292ade69bd17e92a3a187b094c3f46f180cd"},"cell_type":"markdown","source":"# Professionals Data Frame<a id='professionals'></a>\n\n[Go to summary](#summary) "},{"metadata":{"trusted":true,"_uuid":"35e74bc844f8f9b8ba48aa14e3076e7b8957aa21"},"cell_type":"code","source":"df_profs_emails_answers_active.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"184b724f1be75bb03db458d37b5d7f4da4fb8a07"},"cell_type":"markdown","source":"# Professionals Data<a id='pd'></a>\n\n[Go to summary](#summary) "},{"metadata":{"trusted":true,"_uuid":"56e8dfbc20d1e499f3b76cf0356ea248ad8f7a3a"},"cell_type":"code","source":"df_profs_emails_answers_active.sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32de361d6dc800246b1f1ce75fbcf708c5814f75"},"cell_type":"markdown","source":"# Questions and Answers<a id='questions'></a>\n\n[Go to summary](#summary) "},{"metadata":{"trusted":true,"_uuid":"8b606e4441ba0ae08ab51b783d962999197fdba8"},"cell_type":"code","source":"questions.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aba72be97be26141c3786da766e48e9cdd85b5bc"},"cell_type":"code","source":"questions_answers = questions.merge(right=answers, how = 'left',\n                                            left_on ='questions_id',\n                                            right_on ='answers_question_id')\nquestions_answers.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ce7f40c66d41f21e7bbb504e40d25ca8b55c08d"},"cell_type":"code","source":"questions_answers.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c0fe7fe6e4a02bdc0791cb488ca0fec31220bc3"},"cell_type":"code","source":"print(questions_answers['answers_id'].isna().sum() )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5528661f6eb45541e4a5a873409825243f2c55dd"},"cell_type":"markdown","source":"# Who Answers<a id='whoanswers'></a>\n\n[Go to summary](#summary) "},{"metadata":{"trusted":true,"_uuid":"9959b4e0fc29e84864ee47b44c2c7b33ef576fd5","scrolled":false},"cell_type":"code","source":"answers_v_professionals = df_profs_emails_answers_active.copy()\nanswers_v_professionals = answers_v_professionals[['questions_answered','professionals_id']]\nanswers_v_professionals = answers_v_professionals.groupby('questions_answered').count()\nmatch_recipents = answers_v_professionals.sort_values ('questions_answered')\nanswers_v_professionals = answers_v_professionals.reset_index()\nanswers_v_professionals = answers_v_professionals.rename(columns={'professionals_id': 'professionals'})\nprint (answers_v_professionals.sum())\nanswers_v_professionals.head(50)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"faeee965e447685c645a3d336c26305ec6cf8c12"},"cell_type":"markdown","source":"# Answers v Professionals Plot 1"},{"metadata":{"trusted":true,"_uuid":"bb234f5d0cdf05265a7bf3bf6acf42fe80084bfd"},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.scatter(answers_v_professionals['questions_answered'],answers_v_professionals['professionals'],  color='red', s=25, alpha=0.2)\n\nplt.xlabel('questions_answered')\nplt.ylabel('professionals')\nplt.title('Professional Activity ')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e6dbe638309ce733bddd821733f81501faf3b60"},"cell_type":"markdown","source":"# Answers v Professionals Plot 2 <a id='answers v professionals'></a>\n\n[Go to summary](#summary) "},{"metadata":{"trusted":true,"_uuid":"d8c674f4da08673c27e5a55e58df428552798d3e"},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.scatter(answers_v_professionals['questions_answered'],answers_v_professionals['professionals'],  color='red', s=25, alpha=0.2)\nplt.xlim(-10, 100),plt.ylim(-50,200)\nplt.xlabel('questions_answered'),plt.ylabel('professionals')\nplt.title('Professional Activity ')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a59ba5de14af7d6978452fc31eef4fae86b85799"},"cell_type":"markdown","source":"# What % of answers are provided in a professional's 1st day? <a id='answers_first_day'></a>\n\n[Go to summary](#summary) "},{"metadata":{"trusted":true,"_uuid":"70c1017bcb02d42f3d1b32bcdb13f49653727e8f"},"cell_type":"code","source":"\nanswers_profs = answers.merge(right=professionals, how = 'left',\n                                            left_on ='answers_author_id',\n                                            right_on ='professionals_id')\nanswers_profs['DateTime'] = pd.to_datetime(answers_profs['professionals_date_joined'])\nanswers_profs['date_joined'] = answers_profs['DateTime'].dt.normalize()\nanswers_profs['DateTime'] = pd.to_datetime(answers_profs['answers_date_added'])\nanswers_profs['answer_date'] = answers_profs['DateTime'].dt.normalize()\nanswers_profs = answers_profs.drop(['DateTime'],axis = 1)\n\nanswers_profs['days_ns'] = answers_profs['answer_date'] - answers_profs['date_joined']\nanswers_profs['days_to_answer'] = answers_profs['days_ns'].apply(lambda x: x.days)\nanswers_profs = answers_profs.drop('days_ns', axis = 1)\n\n\nanswers_profs.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answers_v_professionals = answers_v_professionals.groupby('questions_answered').count()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b58af426bbf9871fae6c7761a336b19f0c8a05ef"},"cell_type":"code","source":"sorted_answers_profs = answers_profs.sort_values ('days_to_answer')\nsorted_answers_profs = sorted_answers_profs.reset_index()\n#sorted_answers_profs.head()\nsorted_answers_profs_g = sorted_answers_profs.groupby('days_to_answer').count()\nsorted_answers_profs_g.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA on hearts and comments"},{"metadata":{},"cell_type":"markdown","source":"The following analysis shows that there are too few comments and hearts to be able to use the information to score answers when deciding which professionals are providing the best answers:\n\ntotal answers: 51,138\n\ntotal comment: 14,966\n\ntotal heart > 0: 51,138 - 37,301\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"comments.describe ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"answer_scores.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grp_answer_scores = answer_scores.groupby('score').count()\ngrp_answer_scores.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_answer_scores_comments= answer_scores.merge(right=comments, how = 'left',\n                                            left_on ='id',\n                                            right_on ='comments_parent_content_id')\ndf_answer_scores_comments.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_answer_scores_comments.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_qa= questions_bow.merge(right=answers, how = 'left',\n                                            left_on ='questions_id',\n                                            right_on ='answers_question_id')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_qahc= df_qa.merge(right=df_answer_scores_comments, how = 'left',\n                                            left_on ='answers_id',\n                                            right_on ='id')\n\ndf_qahc_s = df_qahc.drop (['answers_id','answers_author_id',\n                           'answers_question_id','answers_date_added','id','comments_id','comments_author_id',\n                           'comments_parent_content_id','comments_date_added'], axis=1)\ndf_qahc_s = df_qahc_s.sort_values ('score' ,ascending = False)\ndf_qahc_s.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Groups\n\nHaving something in common is important. The formations of groups then is a good idea. Currently there are only groups with members and so groups cannot currently be useful in directing questions to professionals."},{"metadata":{"trusted":true},"cell_type":"code","source":"group_memberships.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_memberships_group  = group_memberships.groupby(by='group_memberships_group_id').count()\n\ngroup_memberships_group  =group_memberships_group.sort_values ('group_memberships_user_id', ascending = False).reset_index()\ngroup_memberships_group.head(50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"groups.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Schools membership\n\nMembership is a mixture of students and professionals. The number are small and so cannot currently be a major part of the recommender. However as having something in common is important, it is possible that this group could be used in the future to help prioritise which professional should be asked the question. However relevance is usually more important."},{"metadata":{"trusted":true},"cell_type":"code","source":"school_memberships.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"school_memberships_group  = school_memberships.groupby(by='school_memberships_school_id').count()\n\nschool_memberships_group  =school_memberships_group.sort_values ('school_memberships_user_id', ascending = False).reset_index()\nschool_memberships_group.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"school_memberships_group.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"school_student = school_memberships.merge(right=students, how = 'outer',\n                                            left_on ='school_memberships_user_id',\n                                            right_on ='students_id')\nschool_student_prof = school_student.merge(right=professionals, how = 'outer',\n                                            left_on ='school_memberships_user_id',\n                                            right_on ='professionals_id')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"school_student_prof_group  = school_student_prof.groupby(by='school_memberships_school_id').count()\n\nschool_student_prof_group  =school_student_prof_group.sort_values ('school_memberships_user_id', ascending = False).reset_index()\nschool_student_prof_group.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}