{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction \n**Tổng quan về sổ tay:** Trong sổ tay này, tôi xây dựng một hệ thống đề xuất kết hợp để giới thiệu các câu hỏi của sinh viên cho các chuyên gia cho CareerVillage.org. Hệ thống giới thiệu hoạt động bằng cách đối sánh các chuyên gia với các câu hỏi theo thẻ họ theo dõi, thẻ câu hỏi của câu trả lời trước đó của họ và các thẻ tương tự. Ngoài ra, nó còn khắc phục một số vấn đề với hệ thống tư vấn CareerVillage như khởi động lạnh và những vấn đề khác.\n","metadata":{}},{"cell_type":"markdown","source":"# Hệ thống khuyến nghị\nTheo định nghĩa, Hệ thống đề xuất là một hệ thống xác định và cung cấp nội dung đề xuất hoặc các mặt hàng kỹ thuật số cho người dùng bằng cách sử dụng sở thích của người dùng. Hệ thống đề xuất đã trở thành một tính năng quan trọng trong các trang web hiện đại, ví dụ: trên Amazon, Netflix hoặc Flickr. Tỷ lệ nhấp chuột, doanh thu và các biện pháp thành công khác có thể được tăng lên bằng cách áp dụng các hệ thống khuyến nghị hiệu quả.\n\nNhiệm vụ khó khăn là xác định các mặt hàng có liên quan ngay cả khi chúng thường không phổ biến. Hệ thống đề xuất tận dụng ngữ cảnh có sẵn như thông tin người dùng, thời gian, vị trí, v.v. để lọc các mục có liên quan. Do đó, các mục từ đuôi của phân phối phổ biến cũng được đề xuất thành công. [1]\n\n\n**Các loại hệ thống khuyến nghị:**\n1. ** Lọc cộng tác: ** Lọc cộng tác là phương pháp đưa ra dự đoán tự động (lọc) về sở thích của người dùng bằng cách thu thập thông tin sở thích hoặc sở thích từ nhiều người dùng (cộng tác). Giả thiết cơ bản của phương pháp lọc cộng tác là nếu một người A có cùng quan điểm với người B về một vấn đề, thì A có nhiều khả năng có ý kiến ​​của B về một vấn đề khác hơn là ý kiến ​​của một người được chọn ngẫu nhiên. Dưới đây là hình ảnh về cách lọc cộng tác hoạt động [5]. \n\n    ![](https://i.imgur.com/FZli7DC.gif)\n    Mặc dù lọc cộng tác Một vấn đề chính của lọc cộng tác là \"khởi động lạnh\". Như chúng ta đã thấy, lọc cộng tác có thể là một cách hiệu quả để đề xuất các mục dựa trên lịch sử người dùng, nhưng nếu không có lịch sử người dùng thì sao? Đây được gọi là vấn đề \"khởi động lạnh\" và nó có thể áp dụng cho cả mặt hàng mới và người dùng mới. Các mục có nhiều lịch sử được đề xuất rất nhiều, trong khi những mục không có lịch sử không bao giờ được đưa vào công cụ đề xuất, dẫn đến một vòng phản hồi tích cực. Đồng thời, người dùng mới không có lịch sử và do đó hệ thống không có bất kỳ đề xuất tốt nào. Giải pháp tiềm năng: Các quy trình giới thiệu có thể tìm hiểu thông tin cơ bản để bắt đầu tùy chọn của người dùng, nhập danh bạ mạng xã hội [4].\n    \n\n\n\n2. **Lọc dựa trên nội dung:** Các phương pháp lọc này dựa trên mô tả của một mục và hồ sơ về các lựa chọn ưu tiên của người dùng. Trong hệ thống đề xuất dựa trên nội dung, các từ khóa được sử dụng để mô tả các mục; bên cạnh đó, một hồ sơ người dùng được xây dựng để nêu rõ loại mặt hàng mà người dùng này thích. Nói cách khác, các thuật toán cố gắng đề xuất các sản phẩm tương tự như những sản phẩm mà người dùng đã thích trong quá khứ. Ý tưởng của lọc dựa trên nội dung là nếu bạn thích một mặt hàng, bạn cũng sẽ thích một mặt hàng ‘tương tự’. Ví dụ: khi chúng tôi đề xuất cùng một loại mặt hàng như đề xuất phim hoặc bài hát. [2]\n\n    Một vấn đề chính của phương pháp này là sự đa dạng. Mức độ liên quan là quan trọng, nhưng nó không phải là tất cả. Nếu bạn đã xem và thích Chiến tranh giữa các vì sao, thì khả năng cao là bạn cũng sẽ thích The Empire Strikes Back, nhưng có thể bạn không cần công cụ đề xuất để cho bạn biết điều đó. Điều quan trọng nữa là công cụ đề xuất phải đưa ra kết quả mới lạ (tức là nội dung mà người dùng không mong đợi) và đa dạng (tức là nội dung đại diện cho nhiều lựa chọn về sở thích của họ). [3]\n\n \n3. **Hệ thống tư vấn lai:** Hệ thống tư vấn lai là một loại hệ thống tư vấn đặc biệt kết hợp cả nội dung và phương pháp lọc cộng tác. Kết hợp lọc cộng tác và lọc dựa trên nội dung có thể hiệu quả hơn trong một số trường hợp. Các phương pháp tiếp cận kết hợp có thể được thực hiện theo một số cách: bằng cách đưa ra các dự đoán dựa trên nội dung và dựa trên cộng tác một cách riêng biệt và sau đó kết hợp chúng; bằng cách thêm các khả năng dựa trên nội dung vào cách tiếp cận dựa trên cộng tác (và ngược lại). Một số nghiên cứu so sánh thực nghiệm hiệu suất của phương pháp lai với các phương pháp cộng tác và dựa trên nội dung thuần túy và chứng minh rằng phương pháp lai có thể cung cấp các khuyến nghị chính xác hơn so với các phương pháp thuần túy. Các phương pháp này cũng có thể được sử dụng để khắc phục một số vấn đề thường gặp trong hệ thống khuyến nghị như khởi động nguội và vấn đề thưa thớt.\n\n\n\n\n** Các loại dữ liệu để xây dựng hệ thống khuyến nghị: ** Có hai loại dữ liệu có sẵn để xây dựng hệ thống khuyến nghị. Có\n1. **Phản hồi rõ ràng:** Phản hồi rõ ràng là dữ liệu về phản hồi rõ ràng của người dùng (xếp hạng, v.v.) về một sản phẩm. Nó cho biết trực tiếp rằng người dùng thích một sản phẩm hay không.\n\n2. **Phản hồi ngầm:** Trong phản hồi ngầm, chúng tôi không có dữ liệu về cách người dùng đánh giá một sản phẩm. Ví dụ về phản hồi ngầm là nhấp chuột, phim đã xem, bài hát đã phát, mua hàng hoặc thẻ được chỉ định\n\nBây giờ chúng ta biết hệ thống được đề xuất là gì và đó là các loại khác nhau. Tiếp theo, chúng ta sẽ xem xét phương pháp nào phù hợp hơn để xây dựng hệ thống khuyến nghị cho CareerVillage.org. ","metadata":{}},{"cell_type":"markdown","source":"# Đề xuất kết hợp LightFm cho CareerVillage\nTư vấn kết hợp là một loại tư vấn đặc biệt sử dụng cả lọc cộng tác và dựa trên nội dung để đưa ra các khuyến nghị. Điều đó làm cho khuyến cáo lai trở thành một phương pháp rất hữu ích và hữu ích để xây dựng hệ thống khuyến nghị. Nhưng có một số kỹ thuật và phương pháp cho hệ thống khuyến cáo lai buidling. Nhưng trong dự án này, tôi thích sử dụng ** mô hình LightFM Hybrid **. Mô hình thừa số hóa ma trận lai của Lyst. \n\n1. **LightFM là gì?**: LightFM là mô hình phân tích nhân tử ma trận kết hợp đại diện cho người dùng và các mục dưới dạng kết hợp tuyến tính của các yếu tố tiềm ẩn của tính năng nội dung của họ. Mô hình hoạt động tốt hơn cả mô hình cộng tác và dựa trên nội dung trong các kịch bản dữ liệu tương tác thưa thớt hoặc khởi động lạnh (sử dụng cả siêu dữ liệu người dùng và mục) và hoạt động ít nhất cũng như mô hình phân tích nhân tử ma trận cộng tác thuần túy khi dữ liệu tương tác dồi dào.\n\n    Trong LightFM, giống như trong mô hình lọc cộng tác, người dùng và các mục được biểu thị dưới dạng vectơ tiềm ẩn (nhúng). Tuy nhiên, cũng giống như trong mô hình CB, chúng hoàn toàn được xác định bởi các hàm (trong trường hợp này là kết hợp tuyến tính) của việc nhúng các tính năng nội dung mô tả từng sản phẩm hoặc người dùng.. \n\n    Ví dụ: nếu bộ phim 'Wizard of Oz' được mô tả bằng các đặc điểm sau: \"âm nhạc giả tưởng\", \"Judy Garland\" và \"Wizard of Oz\", thì biểu diễn tiềm ẩn của nó sẽ được tính bằng tổng các đặc điểm \"tiềm ẩn này các đại diện. Khi làm như vậy, LightFM hợp nhất những lợi thế của người giới thiệu dựa trên nội dung và cộng tác. [6]\n\n\n2. **Cách LightFM hoạt động**:: The [LightFM paper](https://arxiv.org/pdf/1507.08439.pdf) mô tả tuyệt vời cách hoạt động của lightFM. Nói một cách đơn giản, mô hình lightFM học cách nhúng (biểu diễn tiềm ẩn trong không gian chiều cao) cho người dùng và các mục theo cách mã hóa tùy chọn của người dùng trên các mục. Khi được nhân với nhau, các đại diện này tạo ra điểm cho mọi mục cho một người dùng nhất định; các mục được điểm cao có nhiều khả năng sẽ thú vị đối với người dùng [5].\n\n    Các đại diện của người dùng và mục được thể hiện dưới dạng đại diện cho các tính năng của họ: một lần nhúng được ước tính cho mọi tính năng và sau đó các tính năng này được tổng hợp lại với nhau để tạo ra các đại diện cho người dùng và các mục [5].\n    \n    Biểu diễn tiềm ẩn của người dùng u được cung cấp bởi tổng các vectơ tiềm ẩn của các tính năng của nó:  \\\\(q_{u}= \\sum_{}^{j\\in f_{u}}\\\\).\n    \n    Và tương tự cho các mặt hàng:  \\\\(p_{i}= \\sum_{}^{j\\in f_{i}}\\\\)\n    \n    Dự đoán của mô hình cho người dùng u và mặt hàng i sau đó được đưa ra bởi sản phẩm chấm của các đại diện của người dùng và mặt hàng, được điều chỉnh bởi thành kiến ​​về tính năng của người dùng và mặt hàng:\n    \\\\(\\hat{r_{ui}}= f\\left ( q_{u}\\cdot p_{i}+b_{u}+b_{i}  \\right )\\\\)\n    \n    Đây chỉ là một ý tưởng chung của mô hình. Vui lòng đọc thêm kiến ​​thức chuyên sâu về mô hình lightFM. \n    \n    \n    \n    \n    \n\n3. **Why LightFM**: \n     Trong cả kịch bản khởi động lạnh và mật độ thấp, LightFM hoạt động ít nhất cũng như các mô hình dựa trên nội dung thuần túy, về cơ bản hoạt động tốt hơn chúng khi (1) thông tin cộng tác có sẵn trong bộ đào tạo hoặc (2) các tính năng người dùng được bao gồm trong mô hình. Điều này thực sự hữu ích cho hệ thống khuyến nghị CareerVillage của chúng tôi bởi vì chúng tôi sẽ có rất nhiều câu hỏi mới và sinh viên tạo ra một môi trường rất tốt cho vấn đề khởi động nguội.\n\n    * Khi dữ liệu cộng tác dồi dào (khởi động ấm, ma trận mục người dùng dày đặc), LightFM hoạt động ít nhất cũng như mô hình MF. \n\n    * Nhúng do LightFM tạo ra mã hóa thông tin ngữ nghĩa quan trọng về các tính năng và có thể được sử dụng cho các nhiệm vụ đề xuất liên quan, chẳng hạn như đề xuất thẻ. Điều này cũng rất quan trọng đối với vấn đề của chúng tôi. Vì rất hữu ích cho việc tìm kiếm các thẻ tương tự để mô hình đó có thể đề xuất các câu hỏi có thẻ tương tự với thẻ chuyên gia. \n\n","metadata":{}},{"cell_type":"markdown","source":"# LightFM Python Library \nThật may mắn, có một thư viện giúp dễ dàng xây dựng một mô hình lightFM. Mô hình LightFM được phát triển bởi Lyst. Họ cũng tạo ra một thư viện để xây dựng mô hình lightfm. Nó rất phổ biến trên Github với hơn 2400 sao và 226 sự cố đã đóng. Bởi vì nó được duy trì tốt bởi Lyst (một công ty thương mại điện tử có trụ sở tại London) và cộng đồng học tập của nó, thư viện python lightFM là một nguồn thực sự tốt để xây dựng mô hình lightFM.\n\n\n**Cách hoạt động của thư viện Python LightFM**: Thư viện này thực sự dễ dàng cho việc xây dựng mô hình lightFM. Có một số bước để xây dựng mô hình bằng thư viện LightFM.\n1. Xử lý dữ liệu của chúng tôi và tạo tập dữ liệu lightFM bằng cách sử dụng api của nó\n2. Xây dựng ma trận tương tác, tính năng người dùng / mục\n3. Làm mô hình và đào tạo mô hình\n4. Đánh giá mô hình\n5. Đưa ra dự đoán\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Tổng quan về xây dựng Model\n\n![](https://i.imgur.com/UyaNRZg.png)\n\n\n    ","metadata":{}},{"cell_type":"markdown","source":"# Thu thập dữ liệu \nCareerVillage cung cấp cho chúng ta một bộ dữ liệu rất phong phú cho cuộc thi này. Tập dữ liệu chứa thông tin về sinh viên, chuyên gia, câu hỏi, câu trả lời, nhận xét và các thẻ đặc biệt. Cuộc thi này đã chứa rất nhiều phân tích dữ liệu và EDA tuyệt vời về dữ liệu này. Hãy thoải mái nhìn vào những điều đó. Điều này sẽ cung cấp cho bạn ý tưởng rất phong phú về tập dữ liệu.","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\n# Default value of display.max_rows is 10 i.e. at max 10 rows will be printed.\n# Set it None to display all rows in the dataframe\npd.set_option('display.max_rows', None)\n\n# Công cụ để xây dựng ma trận tính năng và tương tác, xử lý ánh xạ giữa id người dùng / mục và tên đối tượng và chỉ số tính năng nội bộ.\nfrom lightfm.data import Dataset\n\n# \nfrom lightfm import LightFM\n# Randomly split interactions between training and testing.\nfrom lightfm import cross_validation\n\n# Measure the precision at k metric for a model: the fraction of known positives in the first k positions of the ranked list of results. A perfect score is 1.0.\nfrom lightfm.evaluation import precision_at_k\n\n# \nfrom lightfm.evaluation import auc_score\n\n# re cho việc xử lý text\nimport re\nfrom datetime import datetime, timedelta\n\n# Bỏ qua một số cảnh báo để output rõ ràng ngắn gọn hơn\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-11T00:37:38.092372Z","iopub.execute_input":"2022-05-11T00:37:38.092658Z","iopub.status.idle":"2022-05-11T00:37:38.828223Z","shell.execute_reply.started":"2022-05-11T00:37:38.092614Z","shell.execute_reply":"2022-05-11T00:37:38.827008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################################\n# đọc dữ liệu và lưu vào dataframe của pandas(là thư viện chuyên dùng cho xử lý kiểu dữ liệu)\n############################################\nbase_path = '../input/data-science-for-good-careervillage/'\ndf_answer_scores = pd.read_csv(\n    base_path + 'answer_scores.csv')\n\ndf_answers = pd.read_csv(\n    base_path + 'answers.csv',\n    parse_dates=['answers_date_added'])\n\ndf_comments = pd.read_csv(\n    base_path + 'comments.csv')\n\ndf_emails = pd.read_csv(\n    base_path + 'emails.csv')\n\ndf_group_memberships = pd.read_csv(\n    base_path + 'group_memberships.csv')\n\ndf_groups = pd.read_csv(\n    base_path + 'groups.csv')\n\ndf_matches = pd.read_csv(\n    base_path + 'matches.csv')\n\ndf_professionals = pd.read_csv(\n    base_path + 'professionals.csv',\n    parse_dates=['professionals_date_joined'])\n\ndf_question_scores = pd.read_csv(\n    base_path + 'question_scores.csv')\n\ndf_questions = pd.read_csv(\n    base_path + 'questions.csv',\n    parse_dates=['questions_date_added'])\n\ndf_school_memberships = pd.read_csv(\n    base_path + 'school_memberships.csv')\n\ndf_students = pd.read_csv(\n    base_path + 'students.csv',\n    parse_dates=['students_date_joined'])\n\ndf_tag_questions = pd.read_csv(\n    base_path + 'tag_questions.csv')\n\ndf_tag_users = pd.read_csv(\n    base_path + 'tag_users.csv')\n\ndf_tags = pd.read_csv(\n    base_path + 'tags.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:37:38.82976Z","iopub.execute_input":"2022-05-11T00:37:38.830071Z","iopub.status.idle":"2022-05-11T00:38:12.77625Z","shell.execute_reply.started":"2022-05-11T00:37:38.830006Z","shell.execute_reply":"2022-05-11T00:38:12.775445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_answer_scores[:3]","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:12.777274Z","iopub.execute_input":"2022-05-11T00:38:12.777552Z","iopub.status.idle":"2022-05-11T00:38:12.802753Z","shell.execute_reply.started":"2022-05-11T00:38:12.777507Z","shell.execute_reply":"2022-05-11T00:38:12.801474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Xây dựng các chức năng cần thiết","metadata":{}},{"cell_type":"code","source":"def generate_int_id(dataframe, id_col_name):\n    \"\"\"\n    Generate unique integer id for users, questions and answers\n\n    Parameters\n    ----------\n    dataframe: Dataframe\n        Pandas Dataframe for Users or Q&A. \n    id_col_name : String \n        New integer id's column name.\n        \n    Returns\n    -------\n    Dataframe\n        Updated dataframe containing new id column \n    \"\"\"\n    new_dataframe=dataframe.assign(\n        int_id_col_name=np.arange(len(dataframe))\n        ).reset_index(drop=True)\n    return new_dataframe.rename(columns={'int_id_col_name': id_col_name})\n\n\n\ndef create_features(dataframe, features_name, id_col_name):\n    \"\"\"\n    Generate features that will be ready for feeding into lightfm\n\n    Parameters\n    ----------\n    dataframe: Dataframe\n        Pandas Dataframe which contains features\n    features_name : List\n        List of feature columns name avaiable in dataframe\n    id_col_name: String\n        Column name which contains id of the question or\n        answer that the features will map to.\n        There are two possible values for this variable.\n        1. questions_id_num\n        2. professionals_id_num\n\n    Returns\n    -------\n    Pandas Series\n        A pandas series containing process features\n        that are ready for feed into lightfm.\n        The format of each value\n        will be (user_id, ['feature_1', 'feature_2', 'feature_3'])\n        Ex. -> (1, ['military', 'army', '5'])\n    \"\"\"\n\n    features = dataframe[features_name].apply(\n        lambda x: ','.join(x.map(str)), axis=1)\n    features = features.str.split(',')\n    features = list(zip(dataframe[id_col_name], features))\n    return features\n\n\n\ndef generate_feature_list(dataframe, features_name):\n    \"\"\"\n    Generate features list for mapping \n\n    Parameters\n    ----------\n    dataframe: Dataframe\n        Pandas Dataframe for Users or Q&A. \n    features_name : List\n        List of feature columns name avaiable in dataframe. \n        \n    Returns\n    -------\n    List of all features for mapping \n    \"\"\"\n    features = dataframe[features_name].apply(\n        lambda x: ','.join(x.map(str)), axis=1)\n    features = features.str.split(',')\n    features = features.apply(pd.Series).stack().reset_index(drop=True)\n    return features\n\n\ndef calculate_auc_score(lightfm_model, interactions_matrix, \n                        question_features, professional_features): \n    \"\"\"\n    Measure the ROC AUC metric for a model. \n    A perfect score is 1.0.\n\n    Parameters\n    ----------\n    lightfm_model: LightFM model \n        A fitted lightfm model \n    interactions_matrix : \n        A lightfm interactions matrix \n    question_features, professional_features: \n        Lightfm features \n        \n    Returns\n    -------\n    String containing AUC score \n    \"\"\"\n    score = auc_score( \n        lightfm_model, interactions_matrix, \n        item_features=question_features, \n        user_features=professional_features, \n        num_threads=4).mean()\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:12.804276Z","iopub.execute_input":"2022-05-11T00:38:12.804643Z","iopub.status.idle":"2022-05-11T00:38:12.821838Z","shell.execute_reply.started":"2022-05-11T00:38:12.804576Z","shell.execute_reply":"2022-05-11T00:38:12.820668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tiền xử lý dữ liệu và tạo tính năng \nXử lý trước dữ liệu là điều cần thiết cho mọi dự án khoa học dữ liệu. Chúng tôi cần làm sạch và sửa đổi dữ liệu của mình cho các cơ sở dữ liệu của riêng chúng tôi. Ngoài ra, việc tạo tính năng cũng rất quan trọng. Bởi vì đó là mô hình dự đoán tốt và đa dạng của chúng tôi.","metadata":{}},{"cell_type":"code","source":"# generating unique integer id for users and q&a\ndf_professionals = generate_int_id(df_professionals, 'professionals_id_num')\ndf_students = generate_int_id(df_students, 'students_id_num')\ndf_questions = generate_int_id(df_questions, 'questions_id_num')\ndf_answers = generate_int_id(df_answers, 'answers_id_num')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:12.822974Z","iopub.execute_input":"2022-05-11T00:38:12.823228Z","iopub.status.idle":"2022-05-11T00:38:12.862073Z","shell.execute_reply.started":"2022-05-11T00:38:12.82318Z","shell.execute_reply":"2022-05-11T00:38:12.860978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" df_answers.groupby(['answers_author_id'], sort=False).ngroup().head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-11T00:38:12.863316Z","iopub.execute_input":"2022-05-11T00:38:12.863563Z","iopub.status.idle":"2022-05-11T00:38:12.891028Z","shell.execute_reply.started":"2022-05-11T00:38:12.863527Z","shell.execute_reply":"2022-05-11T00:38:12.889751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_answers)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:12.894398Z","iopub.execute_input":"2022-05-11T00:38:12.894688Z","iopub.status.idle":"2022-05-11T00:38:12.900156Z","shell.execute_reply.started":"2022-05-11T00:38:12.894637Z","shell.execute_reply":"2022-05-11T00:38:12.899605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** **Hợp nhất tập dữ liệu** Đây là một trong những bước quan trọng nhất cho giải pháp của chúng tôi. Các chuyên gia, sinh viên, q & a và các thẻ của chúng tôi được lưu trữ trong các bộ dữ liệu riêng biệt. Đối với mục đích của mô hình, chúng tôi phải hợp nhất các tập dữ liệu của mình một cách cẩn thận để chúng hữu ích cho mô hình của chúng tôi.\n\n1. Tất cả các thẻ (q & a) được lưu trữ trong một tập dữ liệu riêng biệt. Vì vậy, trước hết chúng tôi hợp nhất các thẻ đó với bộ dữ liệu câu hỏi và câu trả lời.\n2. Sau đó, chúng tôi hợp nhất các câu trả lời với các câu trả lời vì một câu hỏi có thể có nhiều câu trả lời.. ","metadata":{}},{"cell_type":"code","source":"###########################\n# merging dataset\n###########################\n\n# just dropna from tags \ndf_tags = df_tags.dropna()\ndf_tags['tags_tag_name'] = df_tags['tags_tag_name'].str.replace('#', '')\n\n\n# merge tag_questions with tags name\n# then group all tags for each question into single rows\ndf_tags_question = df_tag_questions.merge(\n    df_tags, how='inner',\n    left_on='tag_questions_tag_id', right_on='tags_tag_id')\ndf_tags_question = df_tags_question.groupby(\n    ['tag_questions_question_id'])['tags_tag_name'].apply(\n        ','.join).reset_index()\ndf_tags_question = df_tags_question.rename(columns={'tags_tag_name': 'questions_tag_name'})\n\n# merge tag_users with tags name \n# then group all tags for each user into single rows \n# after that rename the tag column name \ndf_tags_pro = df_tag_users.merge(\n    df_tags, how='inner',\n    left_on='tag_users_tag_id', right_on='tags_tag_id')\ndf_tags_pro = df_tags_pro.groupby(\n    ['tag_users_user_id'])['tags_tag_name'].apply(\n        ','.join).reset_index()\ndf_tags_pro = df_tags_pro.rename(columns={'tags_tag_name': 'professionals_tag_name'})\n\n\n# merge professionals and questions tags with main merge_dataset \ndf_questions = df_questions.merge(\n    df_tags_question, how='left',\n    left_on='questions_id', right_on='tag_questions_question_id')\ndf_professionals = df_professionals.merge(\n    df_tags_pro, how='left',\n    left_on='professionals_id', right_on='tag_users_user_id')\n\n# merge questions with scores \ndf_questions = df_questions.merge(\n    df_question_scores, how='left',\n    left_on='questions_id', right_on='id')\n# merge questions with students \ndf_questions = df_questions.merge(\n    df_students, how='left',\n    left_on='questions_author_id', right_on='students_id')\n\n\n\n# merge answers with questions \n# then merge professionals and questions score with that \ndf_merge = df_answers.merge(\n    df_questions, how='inner',\n    left_on='answers_question_id', right_on='questions_id')\ndf_merge = df_merge.merge(\n    df_professionals, how='inner',\n    left_on='answers_author_id', right_on='professionals_id')\ndf_merge = df_merge.merge(\n    df_question_scores, how='inner',\n    left_on='questions_id', right_on='id')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:12.90119Z","iopub.execute_input":"2022-05-11T00:38:12.90142Z","iopub.status.idle":"2022-05-11T00:38:19.67378Z","shell.execute_reply.started":"2022-05-11T00:38:12.901373Z","shell.execute_reply":"2022-05-11T00:38:19.672723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merge.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:19.674912Z","iopub.execute_input":"2022-05-11T00:38:19.675146Z","iopub.status.idle":"2022-05-11T00:38:19.744722Z","shell.execute_reply.started":"2022-05-11T00:38:19.6751Z","shell.execute_reply":"2022-05-11T00:38:19.743789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#######################\n# Generate some features for calculates weights\n# that will use with interaction matrix \n#######################\n\ndf_merge['num_of_ans_by_professional'] = df_merge.groupby(['answers_author_id'])['questions_id'].transform('count')\ndf_merge['num_ans_per_ques'] = df_merge.groupby(['questions_id'])['answers_id'].transform('count')\ndf_merge['num_tags_professional'] = df_merge['professionals_tag_name'].str.split(\",\").str.len()\ndf_merge['num_tags_question'] = df_merge['questions_tag_name'].str.split(\",\").str.len()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:19.746021Z","iopub.execute_input":"2022-05-11T00:38:19.746296Z","iopub.status.idle":"2022-05-11T00:38:20.034612Z","shell.execute_reply.started":"2022-05-11T00:38:19.746232Z","shell.execute_reply":"2022-05-11T00:38:20.033711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merge[:3]","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:20.035729Z","iopub.execute_input":"2022-05-11T00:38:20.036009Z","iopub.status.idle":"2022-05-11T00:38:20.103791Z","shell.execute_reply.started":"2022-05-11T00:38:20.03595Z","shell.execute_reply":"2022-05-11T00:38:20.102737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Maximum number of answer per question : \" + str(df_merge['num_ans_per_ques'].max()))\nprint(\"Maximum number of tags per professional : \" + str(df_merge['num_tags_professional'].max()))\nprint(\"Maximum number of tags per question : \" + str(df_merge['num_tags_question'].max()))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:20.104977Z","iopub.execute_input":"2022-05-11T00:38:20.105187Z","iopub.status.idle":"2022-05-11T00:38:20.113507Z","shell.execute_reply.started":"2022-05-11T00:38:20.105146Z","shell.execute_reply":"2022-05-11T00:38:20.112498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Minimum number of answer per question : \" + str(df_merge['num_ans_per_ques'].min()))","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:20.114633Z","iopub.execute_input":"2022-05-11T00:38:20.11483Z","iopub.status.idle":"2022-05-11T00:38:20.12215Z","shell.execute_reply.started":"2022-05-11T00:38:20.114792Z","shell.execute_reply":"2022-05-11T00:38:20.121231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################\n# Merge professionals previous answered \n# questions tags into professionals tags \n########################\n\n# select professionals answered questions tags \n# and stored as a dataframe\nprofessionals_prev_ans_tags = df_merge[['professionals_id', 'questions_tag_name']]\n# drop null values from that \nprofessionals_prev_ans_tags = professionals_prev_ans_tags.dropna()\n# because professsionals answers multiple questions, \n# we group all of tags of each user into single row \nprofessionals_prev_ans_tags = professionals_prev_ans_tags.groupby(\n    ['professionals_id'])['questions_tag_name'].apply(\n        ','.join).reset_index()\n\n# drop duplicates tags from each professionals rows\nprofessionals_prev_ans_tags['questions_tag_name'] = (\n    professionals_prev_ans_tags['questions_tag_name'].str.split(',').apply(set).str.join(','))\n\n# finally merge the dataframe with professionals dataframe \ndf_professionals = df_professionals.merge(professionals_prev_ans_tags, how='left', on='professionals_id')\n\n# join professionals tags and their answered tags \n# we replace nan values with \"\"\ndf_professionals['professional_all_tags'] = (\n    df_professionals[['professionals_tag_name', 'questions_tag_name']].apply(\n        lambda x: ','.join(x.dropna()),\n        axis=1))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:20.123187Z","iopub.execute_input":"2022-05-11T00:38:20.123378Z","iopub.status.idle":"2022-05-11T00:38:28.360075Z","shell.execute_reply.started":"2022-05-11T00:38:20.123334Z","shell.execute_reply":"2022-05-11T00:38:28.359138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** **Xử lý các giá trị null và trùng lặp**","metadata":{}},{"cell_type":"code","source":"# handling null values \ndf_questions['score'] = df_questions['score'].fillna(0)\ndf_questions['score'] = df_questions['score'].astype(int)\ndf_questions['questions_tag_name'] = df_questions['questions_tag_name'].fillna('No Tag')\n# remove duplicates tags from each questions \ndf_questions['questions_tag_name'] = df_questions['questions_tag_name'].str.split(',').apply(set).str.join(',')\n\n\n# fill nan with 'No Tag' if any \ndf_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].fillna('No Tag')\n# replace \"\" with \"No Tag\", because previously we replace nan with \"\"\ndf_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].replace('', 'No Tag')\ndf_professionals['professionals_location'] = df_professionals['professionals_location'].fillna('No Location')\ndf_professionals['professionals_industry'] = df_professionals['professionals_industry'].fillna('No Industry')\n\n# remove duplicates tags from each professionals \ndf_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].str.split(',').apply(set).str.join(',')\n\n\n\n# remove some null values from df_merge\ndf_merge['num_ans_per_ques']  = df_merge['num_ans_per_ques'].fillna(0)\ndf_merge['num_tags_professional'] = df_merge['num_tags_professional'].fillna(0)\ndf_merge['num_tags_question'] = df_merge['num_tags_question'].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:28.361167Z","iopub.execute_input":"2022-05-11T00:38:28.361412Z","iopub.status.idle":"2022-05-11T00:38:28.75853Z","shell.execute_reply.started":"2022-05-11T00:38:28.361362Z","shell.execute_reply":"2022-05-11T00:38:28.757619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_questions.head()\nlist(df_questions.columns.values)\n# df_merge['score'].str.contains(\"0\")","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:28.759725Z","iopub.execute_input":"2022-05-11T00:38:28.759967Z","iopub.status.idle":"2022-05-11T00:38:28.767759Z","shell.execute_reply.started":"2022-05-11T00:38:28.759917Z","shell.execute_reply":"2022-05-11T00:38:28.766825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Xây dựng mô hình trong LightFM\nTrong bước này, chúng ta sẽ xây dựng mô hình lighFM của mình bằng cách sử dụng thư viện python lightFM. Đầu tiên, chúng ta phải tạo lightFM ```Dataset``` cho mô hình của chúng tôi. Lớp LightFM Datset giúp chúng tôi thực sự dễ dàng tạo ```interection matrix```, ```weights``` and ```user/item features```.\n* ```interection matrix```: Nó là một ma trận chứa các mối quan tâm của người dùng / mặt hàng hoặc các mối quan hệ chuyên nghiệp / quesiton.. \n* ```weights```: trọng số của ma trận liên quan. Trọng lượng ít hơn có nghĩa là ít quan trọng hơn đối với ma trận tương tác đó. \n* ```user/item features```: tính năng người dùng / mặt hàng được cung cấp như thế này ```(user_id, ['feature_1', 'feature_2', 'feature_3'])```\n","metadata":{}},{"cell_type":"code","source":"# generating features list for mapping \nquestion_feature_list = generate_feature_list(\n    df_questions,\n    ['questions_tag_name'])\n\nprofessional_feature_list = generate_feature_list(\n    df_professionals,\n    ['professional_all_tags'])","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:38:28.769093Z","iopub.execute_input":"2022-05-11T00:38:28.769339Z","iopub.status.idle":"2022-05-11T00:39:05.323062Z","shell.execute_reply.started":"2022-05-11T00:38:28.769283Z","shell.execute_reply":"2022-05-11T00:39:05.32242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate our weight value \ndf_merge['total_weights'] = 1 / (\n    df_merge['num_ans_per_ques'])\n\n\n# creating features for feeding into lightfm \ndf_questions['question_features'] = create_features(\n    df_questions, ['questions_tag_name'], \n    'questions_id_num')\n\ndf_professionals['professional_features'] = create_features(\n    df_professionals,\n    ['professional_all_tags'],\n    'professionals_id_num')","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:39:05.324259Z","iopub.execute_input":"2022-05-11T00:39:05.324637Z","iopub.status.idle":"2022-05-11T00:39:16.521357Z","shell.execute_reply.started":"2022-05-11T00:39:05.3246Z","shell.execute_reply":"2022-05-11T00:39:16.52031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LightFM Dataset**: Trong bước này, chúng ta sẽ xây dựng bộ dữ liệu lightfm. Và sau đó, chúng tôi sẽ xây dựng ma trận tương tác, trọng số và các tính năng chuyên nghiệp / câu hỏi của chúng tôi ","metadata":{}},{"cell_type":"code","source":"# https://making.lyst.com/lightfm/docs/lightfm.data.html?highlight=dataset#lightfm.data.Dataset\n########################\n# Dataset building for lightfm\n########################\n\n# define our dataset variable\n# then we feed unique professionals and questions ids\n# and item and professional feature list\n# this will create lightfm internel mapping\ndataset = Dataset()\ndataset.fit(\n    set(df_professionals['professionals_id_num']), \n    set(df_questions['questions_id_num']),\n    item_features=question_feature_list, \n    user_features=professional_feature_list)\n\n\n# now we are building interactions matrix between professionals and quesitons\n# we are passing professional and questions id as a tuple\n# e.g -> pd.Series((pro_id, question_id), (pro_id, questin_id))\n# then we use lightfm build in method for building interactions matrix\ndf_merge['author_question_id_tuple'] = list(zip(\n    df_merge.professionals_id_num, df_merge.questions_id_num, df_merge.total_weights))\n\ninteractions, weights = dataset.build_interactions(\n    df_merge['author_question_id_tuple'])\n\n\n\n# now we are building our questions and professionals features\n# in a way that lightfm understand.\n# we are using lightfm build in method for building\n# questions and professionals features \nquestions_features = dataset.build_item_features(\n    df_questions['question_features'])\n\nprofessional_features = dataset.build_user_features(\n    df_professionals['professional_features'])","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:39:16.522596Z","iopub.execute_input":"2022-05-11T00:39:16.522844Z","iopub.status.idle":"2022-05-11T00:39:18.138137Z","shell.execute_reply.started":"2022-05-11T00:39:16.522794Z","shell.execute_reply":"2022-05-11T00:39:18.137054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** **Xây dựng và đào tạo mô hình**: Trong các bước này, tôi sẽ xây dựng mô hình lightfm và sau đó đào tạo mô hình. Nếu bạn muốn học cách tạo mô hình lightfm bằng thư viện này, vui lòng đọc bài đăng này [recommender for the Movielens dataset](https://lyst.github.io/lightfm/docs/examples/movielens_implicit.html). ","metadata":{}},{"cell_type":"code","source":"################################\n# Model building part\n################################\n\n# define lightfm model by specifying hyper-parametre\n# then fit the model with ineteractions matrix, item and user features \nmodel = LightFM(\n    no_components=150,\n    learning_rate=0.05,\n    loss='warp',\n    random_state=2019)\n\nmodel.fit(\n    interactions,\n    item_features=questions_features,\n    user_features=professional_features, sample_weight=weights,\n    epochs=5, num_threads=4, verbose=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:39:18.139448Z","iopub.execute_input":"2022-05-11T00:39:18.139754Z","iopub.status.idle":"2022-05-11T00:39:45.41249Z","shell.execute_reply.started":"2022-05-11T00:39:18.139697Z","shell.execute_reply":"2022-05-11T00:39:45.411712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Đánh giá hiệu suất của mô hình \n\n** **Điểm AUC trong thư viện lightfm là gì?** : Nó đo lường số liệu ROC AUC cho một mô hình: xác suất để một ví dụ tích cực được chọn ngẫu nhiên có điểm cao hơn một ví dụ tiêu cực được chọn ngẫu nhiên. Điểm tuyệt đối là 1,0.\n","metadata":{}},{"cell_type":"code","source":"calculate_auc_score(model, interactions, questions_features, professional_features)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:39:45.413624Z","iopub.execute_input":"2022-05-11T00:39:45.413852Z","iopub.status.idle":"2022-05-11T00:40:10.929758Z","shell.execute_reply.started":"2022-05-11T00:39:45.41381Z","shell.execute_reply":"2022-05-11T00:40:10.92887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** **Đưa ra các đề xuất thực tế** :","metadata":{}},{"cell_type":"code","source":"from IPython.display import display_html\ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n\ndef recommend_questions(professional_ids):\n     \n    for professional in professional_ids:\n        # print their previous answered question title\n        previous_q_id_num = df_merge.loc[df_merge['professionals_id_num'] == professional][:3]['questions_id_num']\n        df_previous_questions = df_questions.loc[df_questions['questions_id_num'].isin(previous_q_id_num)]\n        print('Professional Id (' + str(professional) + \"): Previous Answered Questions\")\n        display_side_by_side(\n            df_previous_questions[['questions_title', 'question_features']],\n            df_professionals.loc[df_professionals.professionals_id_num == professional][['professionals_id_num','professionals_tag_name']])\n        \n        # predict\n        discard_qu_id = df_previous_questions['questions_id_num'].values.tolist()\n        df_use_for_prediction = df_questions.loc[~df_questions['questions_id_num'].isin(discard_qu_id)]\n        questions_id_for_predict = df_use_for_prediction['questions_id_num'].values.tolist()\n        \n        scores = model.predict(\n            professional,\n            questions_id_for_predict,\n            item_features=questions_features,\n            user_features=professional_features)\n        \n        df_use_for_prediction['scores'] = scores\n        df_use_for_prediction = df_use_for_prediction.sort_values(by='scores', ascending=False)[:8]\n        print('Professional Id (' + str(professional) + \"): Recommended Questions: \")\n        display(df_use_for_prediction[['questions_title', 'question_features']])\n    \n\n    \n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:10.936861Z","iopub.execute_input":"2022-05-11T00:40:10.937068Z","iopub.status.idle":"2022-05-11T00:40:10.947803Z","shell.execute_reply.started":"2022-05-11T00:40:10.937026Z","shell.execute_reply":"2022-05-11T00:40:10.946882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recommend_questions([1200 ,19897, 3])","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:10.949194Z","iopub.execute_input":"2022-05-11T00:40:10.949462Z","iopub.status.idle":"2022-05-11T00:40:11.712149Z","shell.execute_reply.started":"2022-05-11T00:40:10.949408Z","shell.execute_reply":"2022-05-11T00:40:11.711193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Analysis**: \n* Đối với nghề nghiệp đầu tiên (1200) vẫn chưa trả lời bất kỳ câu hỏi nào. Nhưng anh ấy / cô ấy theo sau một số thẻ. Mô hình của chúng tôi lấy các thẻ đó làm tính năng và dự đoán các câu hỏi có các thẻ tương tự.\n* Đối với các chuyên gia thứ hai (19897) đã trả lời một câu hỏi có thẻ chuyên ngành. Nhưng trong hồ sơ của anh ấy, anh ấy theo dõi các thẻ như tác phẩm sáng tạo như nghệ thuật, họa sĩ minh họa, v.v. Vì vậy, mô hình của chúng tôi đề xuất các câu hỏi có thẻ sáng tạo như nghệ thuật, họa sĩ minh họa vì anh ấy theo dõi nhiều thẻ hơn cho một tác phẩm sáng tạo.\n* Đối với các chuyên gia thứ ba (3): đã trả lời các câu hỏi có thẻ pháp y, hình sự, khoa học, công lý, thám tử. Từ các thẻ, chúng tôi có thể nhận được ý tưởng về mối quan tâm của các chuyên gia. Mô hình của chúng tôi cũng học được điều đó. Đó là lý do tại sao nó đề xuất các mục có thẻ như luật, hình sự, thám tử.\n","metadata":{}},{"cell_type":"markdown","source":"# Mô hình trong sản xuất\n![](https://i.imgur.com/Kh4rVcL.png)\n\n","metadata":{}},{"cell_type":"code","source":"############################################\n# Read all our datasets agian \n# and store them in pandas dataframe objects. \n############################################\nbase_path = '../input/data-science-for-good-careervillage/'\ndf_answer_scores = pd.read_csv(\n    base_path + 'answer_scores.csv')\n\ndf_answers = pd.read_csv(\n    base_path + 'answers.csv',\n    parse_dates=['answers_date_added'])\n\ndf_comments = pd.read_csv(\n    base_path + 'comments.csv')\n\ndf_emails = pd.read_csv(\n    base_path + 'emails.csv')\n\ndf_group_memberships = pd.read_csv(\n    base_path + 'group_memberships.csv')\n\ndf_groups = pd.read_csv(\n    base_path + 'groups.csv')\n\ndf_matches = pd.read_csv(\n    base_path + 'matches.csv')\n\ndf_professionals = pd.read_csv(\n    base_path + 'professionals.csv',\n    parse_dates=['professionals_date_joined'])\n\ndf_question_scores = pd.read_csv(\n    base_path + 'question_scores.csv')\n\ndf_questions = pd.read_csv(\n    base_path + 'questions.csv',\n    parse_dates=['questions_date_added'])\n\ndf_school_memberships = pd.read_csv(\n    base_path + 'school_memberships.csv')\n\ndf_students = pd.read_csv(\n    base_path + 'students.csv',\n    parse_dates=['students_date_joined'])\n\ndf_tag_questions = pd.read_csv(\n    base_path + 'tag_questions.csv')\n\ndf_tag_users = pd.read_csv(\n    base_path + 'tag_users.csv')\n\ndf_tags = pd.read_csv(\n    base_path + 'tags.csv')\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-11T00:41:45.498731Z","iopub.execute_input":"2022-05-11T00:41:45.499031Z","iopub.status.idle":"2022-05-11T00:42:10.190188Z","shell.execute_reply.started":"2022-05-11T00:41:45.498994Z","shell.execute_reply":"2022-05-11T00:42:10.189314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Processing Class**: Bây giờ chúng ta sẽ xây dựng một lớp sẽ được sử dụng để làm sạch và xử lý dữ liệu được thiết kế riêng cho CareerVillage Datasetes. Tôi đã cung cấp tài liệu chi tiết và nhận xét với từng phần của mã. Điều này sẽ giúp hiểu mã và ý định rất tốt.. ","metadata":{}},{"cell_type":"code","source":"class CareerVillageDataPreparation:\n    \"\"\"\n    Clean and process data CareerVillage Data. \n    \n    This class process data in a way that will be useful\n    for building lightFM dataset. \n    \"\"\"\n    \n    def __init__(self):\n        pass\n\n    def _assign_unique_id(self, data, id_col_name):\n        \"\"\"\n        Generate unique integer id for users, questions and answers\n\n        Parameters\n        ----------\n        data: Dataframe\n            Pandas Dataframe for Users or Q&A. \n        id_col_name : String \n            New integer id's column name.\n\n        Returns\n        -------\n        Dataframe\n            Updated dataframe containing new id column\n        \"\"\"\n        new_dataframe=data.assign(\n            int_id_col_name=np.arange(len(data))\n            ).reset_index(drop=True)\n        return new_dataframe.rename(columns={'int_id_col_name': id_col_name})\n\n    def _dropna(self, data, column, axis):\n        \"\"\"Drop null values from specific column\"\"\"\n        return data.dropna(column, axis=axis)\n\n    def _merge_data(self, left_data, left_key, right_data, right_key, how):\n        \"\"\"\n        This function is used for merging two dataframe.\n        \n        Parameters\n        -----------\n        left_data: Dataframe\n            Left side dataframe for merge\n        left_key: String\n            Left Dataframe merge key\n        right_data: Dataframe\n            Right side dataframe for merge\n        right_key: String\n            Right Dataframe merge key\n        how: String\n            Method of merge (inner, left, right, outer)\n            \n        \n        Returns\n        --------\n        Dataframe\n            A new dataframe merging left and right dataframe\n        \"\"\"\n        return left_data.merge(\n            right_data,\n            how=how,\n            left_on=left_key,\n            right_on=right_key)\n\n    def _group_tags(self, data, group_by, tag_column):\n        \"\"\"Grouop multiple tags into single rows sepearated by comma\"\"\"\n        return data.groupby(\n            [group_by])[tag_column].apply(\n            ','.join).reset_index()\n\n    def _merge_cv_datasets(\n        self,\n        professionals,students,\n        questions,answers,\n        tags,tag_questions,tag_users, questions_score):\n        \"\"\"\n        This function merges all the necessary \n        CareerVillage dataset in defined way. \n        \n        Parameters\n        ------------\n        professionals,students,\n        questions,answers,\n        tags,tag_questions,\n        tag_users,\n        questions_score: Dataframe\n            Pandas dataframe defined by it's name\n        \n        \n        Returns\n        ---------\n        questions, professionals: Dataframe\n            Updated dataframe after merge\n        merge: Dataframe\n            A new datframe after merging answers with questions\n        \"\"\"\n        \n        \n        # merge tag_questions with tags name\n        # then group all tags for each question into single rows\n        tag_question = self._merge_data(\n            left_data=tag_questions,\n            left_key='tag_questions_tag_id',\n            right_data=tags,\n            right_key='tags_tag_id',\n            how='inner')\n        tag_question = self._group_tags(\n            data=tag_question,\n            group_by='tag_questions_question_id',\n            tag_column='tags_tag_name')\n        \n        tag_question = tag_question.rename(\n            columns={'tags_tag_name': 'questions_tag_name'})\n        \n        # merge tag_users with tags name\n        # then group all tags for each user into single rows \n        # after that rename the tag column name\n        tags_pro = self._merge_data(\n            left_data=tag_users,\n            left_key='tag_users_tag_id',\n            right_data=tags,\n            right_key='tags_tag_id',\n            how='inner')\n        tags_pro = self._group_tags(\n            data=tags_pro,\n            group_by='tag_users_user_id',\n            tag_column='tags_tag_name')\n        tags_pro = tags_pro.rename(\n            columns={'tags_tag_name': 'professionals_tag_name'})\n        \n        # merge professionals and questions tags with main merge_dataset \n        questions = self._merge_data(\n            left_data=questions,\n            left_key='questions_id',\n            right_data=tag_question,\n            right_key='tag_questions_question_id',\n            how='left')\n        professionals = self._merge_data(\n            left_data=professionals,\n            left_key='professionals_id',\n            right_data=tags_pro,\n            right_key='tag_users_user_id',\n            how='left')\n        \n        # merge questions with scores \n        questions = self._merge_data(\n            left_data=questions,\n            left_key='questions_id',\n            right_data=questions_score,\n            right_key='id',\n            how='left')\n        \n        # merge questions with students\n        questions = self._merge_data(\n            left_data=questions,\n            left_key='questions_author_id',\n            right_data=students,\n            right_key='students_id',\n            how='left')\n        \n        # merge answers with questions\n        # then merge professionals and questions score with that\n        merge = self._merge_data(\n            left_data=answers,\n            left_key='answers_question_id',\n            right_data=questions,\n            right_key='questions_id',\n            how='inner')\n        \n        merge = self._merge_data(\n            left_data=merge,\n            left_key='answers_author_id',\n            right_data=professionals,\n            right_key='professionals_id',\n            how='inner')\n        \n        return questions, professionals, merge\n  \n    def _drop_duplicates_tags(self, data, col_name):\n        # drop duplicates tags from each row\n        return (\n            data[col_name].str.split(\n                ',').apply(set).str.join(','))\n\n\n    def _merge_pro_pre_ans_tags(self, professionals, merge):\n        ########################\n        # Merge professionals previous answered\n        # questions tags into professionals tags\n        ########################\n        \n        # select professionals answered questions tags\n        # and stored as a dataframe\n        professionals_prev_ans_tags = (\n            merge[['professionals_id', 'questions_tag_name']])\n        # drop null values from that\n        professionals_prev_ans_tags = professionals_prev_ans_tags.dropna()\n        \n        # because professsionals answers multiple questions,\n        # we group all of tags of each user into single row\n        professionals_prev_ans_tags = self._group_tags(\n            data=professionals_prev_ans_tags,\n            group_by='professionals_id',\n            tag_column='questions_tag_name')\n        \n        # drop duplicates tags from each professionals rows\n        professionals_prev_ans_tags['questions_tag_name'] = \\\n        self._drop_duplicates_tags(\n            professionals_prev_ans_tags, 'questions_tag_name')\n        \n        # finally merge the dataframe with professionals dataframe\n        professionals = self._merge_data(\n            left_data=professionals,\n            left_key='professionals_id',\n            right_data=professionals_prev_ans_tags,\n            right_key='professionals_id',\n            how='left')\n        \n        # join professionals tags and their answered tags \n        # we replace nan values with \"\"\n        professionals['professional_all_tags'] = (\n            professionals[['professionals_tag_name',\n                           'questions_tag_name']].apply(\n                lambda x: ','.join(x.dropna()),\n                axis=1))\n        return professionals\n\n    def prepare(\n        self,\n        professionals,students,\n        questions,answers,\n        tags,tag_questions,tag_users, questions_score):\n        \n        \"\"\"\n        This function clean and process \n        CareerVillage Data sets. \n        \"\"\"\n        \n        # assign unique integer id\n        professionals = self._assign_unique_id(\n            professionals, 'professionals_id_num')\n        students = self._assign_unique_id(\n            students, 'students_id_num')\n        questions = self._assign_unique_id(\n            questions, 'questions_id_num')\n        answers = self._assign_unique_id(\n            answers, 'answers_id_num')\n        \n        # just dropna from tags \n        tags = tags.dropna()\n        tags['tags_tag_name'] = tags['tags_tag_name'].str.replace(\n            '#', '')\n        \n        \n        # merge necessary datasets\n        df_questions, df_professionals, df_merge = self._merge_cv_datasets(\n            professionals,students,\n            questions,answers,\n            tags,tag_questions,tag_users,\n            questions_score)\n        \n        #######################\n        # Generate some features for calculates weights\n        # that will use with interaction matrix\n        #######################\n        df_merge['num_ans_per_ques'] = df_merge.groupby(\n            ['questions_id'])['answers_id'].transform('count')\n        \n        # merge pro previoius answered question tags with pro tags \n        df_professionals = self._merge_pro_pre_ans_tags(\n            df_professionals, df_merge)\n        \n        # some more pre-processing \n        # handling null values \n        df_questions['score'] = df_questions['score'].fillna(0)\n        df_questions['score'] = df_questions['score'].astype(int)\n        df_questions['questions_tag_name'] = \\\n        df_questions['questions_tag_name'].fillna('No Tag')\n        \n        # remove duplicates tags from each questions \n        df_questions['questions_tag_name'] = \\\n        df_questions['questions_tag_name'].str.split(\n            ',').apply(set).str.join(',')\n\n        # fill nan with 'No Tag' if any \n        df_professionals['professional_all_tags'] = \\\n        df_professionals['professional_all_tags'].fillna(\n            'No Tag')\n        # replace \"\" with \"No Tag\", because previously we replace nan with \"\"\n        df_professionals['professional_all_tags'] = \\\n        df_professionals['professional_all_tags'].replace(\n            '', 'No Tag')\n        \n        df_professionals['professionals_location'] = \\\n        df_professionals['professionals_location'].fillna(\n            'No Location')\n        \n        df_professionals['professionals_industry'] = \\\n        df_professionals['professionals_industry'].fillna(\n            'No Industry')\n\n        # remove duplicates tags from each professionals\n        df_professionals['professional_all_tags'] = \\\n        df_professionals['professional_all_tags'].str.split(\n            ',').apply(set).str.join(',')\n\n        # remove some null values from df_merge\n        df_merge['num_ans_per_ques']  = \\\n        df_merge['num_ans_per_ques'].fillna(0)\n        \n        return df_questions, df_professionals, df_merge\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:42:10.191217Z","iopub.execute_input":"2022-05-11T00:42:10.191414Z","iopub.status.idle":"2022-05-11T00:42:10.219383Z","shell.execute_reply.started":"2022-05-11T00:42:10.191375Z","shell.execute_reply":"2022-05-11T00:42:10.218248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** **Xây dựng dữ liệu cho lớp LightFM:** Từ bước 2, chúng ta đã biết rằng thư viện lightfm ngoại trừ dữ liệu theo một cách rất cụ thể và thận trọng. Định dạng dữ liệu LightFM đã được thảo luận ở bước 2. Vui lòng đọc phần đó. Bây giờ chúng tôi đang xây dựng một lớp sẽ đặt tất cả câu đố xây dựng tập dữ liệu vào một lớp cụ thể.","metadata":{}},{"cell_type":"code","source":"class LightFMDataPrep:\n    def __init__(self):\n        pass\n    def create_features(self, dataframe, features_name, id_col_name):\n        \"\"\"\n        Generate features that will be ready for feeding into lightfm\n\n        Parameters\n        ----------\n        dataframe: Dataframe\n            Pandas Dataframe which contains features\n        features_name : List\n            List of feature columns name avaiable in dataframe\n        id_col_name: String\n            Column name which contains id of the question or\n            answer that the features will map to.\n            There are two possible values for this variable.\n            1. questions_id_num\n            2. professionals_id_num\n\n        Returns\n        -------\n        Pandas Series\n            A pandas series containing process features\n            that are ready for feed into lightfm.\n            The format of each value\n            will be (user_id, ['feature_1', 'feature_2', 'feature_3'])\n            Ex. -> (1, ['military', 'army', '5'])\n        \"\"\"\n\n        features = dataframe[features_name].apply(\n            lambda x: ','.join(x.map(str)), axis=1)\n        features = features.str.split(',')\n        features = list(zip(dataframe[id_col_name], features))\n        return features\n\n\n\n    def generate_feature_list(self, dataframe, features_name):\n        \"\"\"\n        Generate features list for mapping \n\n        Parameters\n        ----------\n        dataframe: Dataframe\n            Pandas Dataframe for Users or Q&A. \n        features_name : List\n            List of feature columns name avaiable in dataframe. \n\n        Returns\n        -------\n        List of all features for mapping \n        \"\"\"\n        features = dataframe[features_name].apply(\n            lambda x: ','.join(x.map(str)), axis=1)\n        features = features.str.split(',')\n        features = features.apply(pd.Series).stack().reset_index(drop=True)\n        return features\n    \n    def create_data(self, questions, professionals, merge):\n        question_feature_list = self.generate_feature_list(\n            questions,\n            ['questions_tag_name'])\n\n        professional_feature_list = self.generate_feature_list(\n            professionals,\n            ['professional_all_tags'])\n        \n        merge['total_weights'] = 1 / (\n            merge['num_ans_per_ques'])\n        \n        # creating features for feeding into lightfm \n        questions['question_features'] = self.create_features(\n            questions, ['questions_tag_name'], \n            'questions_id_num')\n\n        professionals['professional_features'] = self.create_features(\n            professionals,\n            ['professional_all_tags'],\n            'professionals_id_num')\n        \n        return question_feature_list,\\\n    professional_feature_list,merge,questions,professionals\n        \n    def fit(self, questions, professionals, merge):\n        ########################\n        # Dataset building for lightfm\n        ########################\n        question_feature_list, \\\n        professional_feature_list,\\\n        merge,questions,professionals = \\\n        self.create_data(questions, professionals, merge)\n        \n        \n        # define our dataset variable\n        # then we feed unique professionals and questions ids\n        # and item and professional feature list\n        # this will create lightfm internel mapping\n        dataset = Dataset()\n        dataset.fit(\n            set(professionals['professionals_id_num']), \n            set(questions['questions_id_num']),\n            item_features=question_feature_list, \n            user_features=professional_feature_list)\n\n\n        # now we are building interactions\n        # matrix between professionals and quesitons\n        # we are passing professional and questions id as a tuple\n        # e.g -> pd.Series((pro_id, question_id), (pro_id, questin_id))\n        # then we use lightfm build in method for building interactions matrix\n        merge['author_question_id_tuple'] = list(zip(\n            merge.professionals_id_num,\n            merge.questions_id_num,\n            merge.total_weights))\n\n        interactions, weights = dataset.build_interactions(\n            merge['author_question_id_tuple'])\n\n\n\n        # now we are building our questions and\n        # professionals features\n        # in a way that lightfm understand.\n        # we are using lightfm build in method for building\n        # questions and professionals features \n        questions_features = dataset.build_item_features(\n            questions['question_features'])\n\n        professional_features = dataset.build_user_features(\n            professionals['professional_features'])\n        \n        return interactions,\\\n    weights,questions_features,professional_features\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:11.932542Z","iopub.status.idle":"2022-05-11T00:40:11.932915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train Model Class**: Trong bước 2, chúng tôi đã xem cách chúng tôi xây dựng và đào tạo mô hình của mình. Bây giờ chúng ta sẽ tập hợp tất cả những thứ đó lại với nhau trong lớp TrainLightFM ","metadata":{}},{"cell_type":"code","source":"class TrainLightFM:\n    def __init__(self):\n        pass\n        \n    def train_test_split(self, interactions, weights):\n        train_interactions, test_interactions = \\\n        cross_validation.random_train_test_split(\n            interactions, \n            random_state=np.random.RandomState(2019))\n        \n        train_weights, test_weights = \\\n        cross_validation.random_train_test_split(\n            weights, \n            random_state=np.random.RandomState(2019))\n        return train_interactions,\\\n    test_interactions, train_weights, test_weights\n    \n    def fit(self, interactions, weights,\n            questions_features, professional_features,\n            cross_validation=False,no_components=150,\n            learning_rate=0.05,\n            loss='warp',\n            random_state=2019,\n            verbose=True,\n            num_threads=4, epochs=5):\n        ################################\n        # Model building part\n        ################################\n\n        # define lightfm model by specifying hyper-parametre\n        # then fit the model with ineteractions matrix,\n        # item and user features\n        \n        model = LightFM(\n            no_components,\n            learning_rate,\n            loss=loss,\n            random_state=random_state)\n        model.fit(\n            interactions,\n            item_features=questions_features,\n            user_features=professional_features, sample_weight=weights,\n            epochs=epochs, num_threads=num_threads, verbose=verbose)\n        \n        return model\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:11.933611Z","iopub.status.idle":"2022-05-11T00:40:11.93406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"** **Đề xuất classs** : ","metadata":{}},{"cell_type":"code","source":"class LightFMRecommendations:\n    \"\"\"\n    Make prediction given model and professional ids\n    \"\"\"\n    def __init__(self, lightfm_model,\n                 professionals_features,\n                 questions_features,\n                 questions,professionals,merge):\n        self.model = lightfm_model\n        self.professionals_features = professionals_features\n        self.questions_features = questions_features\n        self.questions = questions\n        self.professionals = professionals\n        self.merge = merge\n        \n    def previous_answered_questions(self, professionals_id):\n        previous_q_id_num = (\n            self.merge.loc[\\\n                self.merge['professionals_id_num'] == \\\n                professionals_id]['questions_id_num'])\n        \n        previous_answered_questions = self.questions.loc[\\\n            self.questions['questions_id_num'].isin(\n            previous_q_id_num)]\n        return previous_answered_questions\n        \n    \n    def _filter_question_by_pro(self, professionals_id):\n        \"\"\"Drop questions that professional already answer\"\"\"\n        previous_answered_questions = \\\n        self.previous_answered_questions(professionals_id)\n        \n        discard_qu_id = \\\n        previous_answered_questions['questions_id_num'].values.tolist()\n        \n        questions_for_prediction = \\\n        self.questions.loc[~self.questions['questions_id_num'].isin(discard_qu_id)]\n        \n        return questions_for_prediction\n    \n    def _filter_question_by_date(self, questions, start_date, end_date):\n        mask = \\\n        (questions['questions_date_added'] > start_date) & \\\n        (questions['questions_date_added'] <= end_date)\n        \n        return questions.loc[mask]\n        \n    \n    def recommend_by_pro_id_general(self,\n                                    professional_id,\n                                    num_prediction=8):\n        questions_for_prediction = self._filter_question_by_pro(professional_id)\n        score = self.model.predict(\n            professional_id,\n            questions_for_prediction['questions_id_num'].values.tolist(), \n            item_features=self.questions_features,\n            user_features=self.professionals_features)\n        \n        questions_for_prediction['recommendation_score'] = score\n        questions_for_prediction = questions_for_prediction.sort_values(\n            by='recommendation_score', ascending=False)[:num_prediction]\n        return questions_for_prediction\n    \n    def recommend_by_pro_id_frequency_date_range(self,\n                                                 professional_id,\n                                                 start_date,\n                                                 end_date,\n                                                 num_prediction=8):\n        questions_for_prediction = \\\n        self._filter_question_by_pro(professional_id)\n        \n        start_date = datetime.strptime(start_date, '%Y-%m-%d')\n        end_date = datetime.strptime(end_date, '%Y-%m-%d')\n        \n        questions_for_prediction = self._filter_question_by_date(\n            questions_for_prediction, start_date, end_date)\n        \n        score = self.model.predict(\n            professional_id,\n            questions_for_prediction['questions_id_num'].values.tolist(), \n            item_features=self.questions_features,\n            user_features=self.professionals_features)\n        \n        questions_for_prediction['recommendation_score'] = score\n        questions_for_prediction = questions_for_prediction.sort_values(\n            by='recommendation_score', ascending=False)[:num_prediction]\n        return questions_for_prediction\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:11.934849Z","iopub.status.idle":"2022-05-11T00:40:11.935306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Đặt tất cả lại với nhau**:","metadata":{}},{"cell_type":"code","source":"# instiate all class instance\ncv_data_prep = CareerVillageDataPreparation()\nlight_fm_data_prep = LightFMDataPrep()\ntrain_lightfm = TrainLightFM()\n\n# process raw data\ndf_questions_p, df_professionals_p, df_merge_p = \\\ncv_data_prep.prepare(\n    df_professionals,df_students,\n    df_questions,df_answers,\n    df_tags,df_tag_questions,df_tag_users,\n    df_question_scores)\n\n\n# prepare data for lightfm \ninteractions, weights, \\\nquestions_features, professional_features = \\\nlight_fm_data_prep.fit(\n    df_questions_p, df_professionals_p, df_merge_p)\n\n\n# finally build and trian our model\nmodel = train_lightfm.fit(interactions,\n                          weights,\n                          questions_features,\n                          professional_features)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:11.936025Z","iopub.status.idle":"2022-05-11T00:40:11.936504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tuyệt vời! Bạn có thấy không, việc xây dựng mô hình của chúng tôi dễ dàng như thế nào. Chúng tôi chắc chắn có thể áp dụng ý tưởng này khi đưa mô hình vào sản xuất. Bây giờ chúng ta sẽ xem một số khuyến nghị thực tế. ","metadata":{}},{"cell_type":"code","source":"# define our recommender class\nlightfm_recommendations = LightFMRecommendations(\n    model,\n    professional_features,questions_features,\n    df_questions_p, df_professionals_p, df_merge_p)\n\n# let's what our model predict for user id 3\nprint(\"Recommendation for professional: \" + str(3))\ndisplay(lightfm_recommendations.recommend_by_pro_id_general(3)[:8])","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-05-11T00:40:11.937172Z","iopub.status.idle":"2022-05-11T00:40:11.937663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# also let's see what our model predicts for professional 3\n# given questions between two dates\nprint(\"Recommendations for professionals (question from 2016-1-1 to 2016-12-31): \" + str(3))\ndisplay(lightfm_recommendations.recommend_by_pro_id_frequency_date_range(3,\n                                                                 '2016-1-1','2016-12-31')[:8])","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:11.938544Z","iopub.status.idle":"2022-05-11T00:40:11.939001Z"},"trusted":true},"execution_count":null,"outputs":[]}]}