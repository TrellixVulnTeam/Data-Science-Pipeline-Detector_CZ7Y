{"cells":[{"metadata":{},"cell_type":"markdown","source":"Predictive models using:\n\nLogisitc Regression Accuracy: 0.766\nRandom Forest Accuracy: 0.73\nNaive Bayes Accuracy: 0.753\nNeural Network Accuracy: 0.759\n\n**Credits to Original Code by @jaisi8631**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport random\nimport pickle\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** First Step: Team Details**\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"collapsed":true},"cell_type":"code","source":"# -------------------------\n# DATA PREPROCESSING\n# -------------------------\n# load required files and allocate data  \nregseason = pd.read_csv('/kaggle/input/march-madness-analytics-2020/2020DataFiles/2020DataFiles/2020-Womens-Data/WDataFiles_Stage1/WRegularSeasonCompactResults.csv')\n#/kaggle/input/march-madness-analytics-2020/WDataFiles_Stage2/WRegularSeasonDetailedR/esults.csv\npostseason = pd.read_csv('/kaggle/input/march-madness-analytics-2020/2020DataFiles/2020DataFiles/2020-Womens-Data/WDataFiles_Stage1/WNCAATourneyDetailedResults.csv')\nframes = [regseason, postseason]\ngames = pd.concat(frames)\nteams = pd.read_csv('/kaggle/input/march-madness-analytics-2020/2020DataFiles/2020DataFiles/2020-Womens-Data/WDataFiles_Stage1/WTeamConferences.csv')\n\nstart = (teams.Season.values == 2003).argmax()\nteams = teams.iloc[start:]\n\n# set variables of necessary data\nseasonIndex = games.columns.get_loc('Season')\nWTeamIDIndex = games.columns.get_loc('WTeamID')\nLTeamIDIndex = games.columns.get_loc('LTeamID')\nWTeamMetrics = ['WScore', 'LScore', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM',\n                'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']\nWTeamIndexes = []\nLTeamMetrics = ['LScore', 'WScore', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM',\n                'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF']\nLTeamIndexes = []\n\n# store index of necessary data within dataset\nfor i in range(0, len(WTeamMetrics)):\n    index = games.columns.get_loc(WTeamMetrics[i])\n    WTeamIndexes.append(index)\n\nfor i in range(0, len(LTeamMetrics)):\n    index = games.columns.get_loc(LTeamMetrics[i])\n    LTeamIndexes.append(index)\n    \n# -----------------------------\n# DATASET SKELETON PREPARATION\n# -----------------------------\ncols = 5 + len(WTeamMetrics)\nrows = teams.shape[0]\n\ndataset = np.zeros(shape = (rows, cols))\ndataset = dataset.astype(float)\nteamDetails = teams.iloc[:, 0:2].values\ndataset[:, 0:2] = teamDetails\n\nrawData = games.iloc[:, :].values\nindex = games.columns.get_loc(\"WLoc\")\nfor i in range(0, rawData.shape[0]):\n    rawData[i][index] = 0\nrawData = rawData.astype(float)\n\n\n# -------------------------\n# DATA PROCESSING\n# -------------------------\nfor i in range(0, rawData.shape[0]):\n    \n    # output to console\n    print(\"Game #: \", i)\n    \n    # get descriptive data of game\n    thisWTeamID = rawData[i][WTeamIDIndex]\n    thisLTeamID = rawData[i][LTeamIDIndex]\n    thisSeason = rawData[i][seasonIndex]\n    \n    # initialize variables\n    datasetWTeam = -1\n    datasetLTeam = -1\n    \n    # locate rows of winning and losing teams\n    for j in range(0, dataset.shape[0]):\n        if(dataset[j][0] == thisSeason):\n            if(dataset[j][1] == thisWTeamID):\n                datasetWTeam = j\n            elif(dataset[j][1] == thisLTeamID):\n                datasetLTeam = j\n    \n    # add a win, a game played and the necessary data to winning team         \n    dataset[datasetWTeam][2] += 1\n    dataset[datasetWTeam][-1] += 1\n    k = 0\n    for j in WTeamIndexes:\n        dataset[datasetWTeam][k + 4] += rawData[i][j]\n        k += 1\n    \n    # add a loss, a game played and the necessary data to losing team  \n    dataset[datasetLTeam][3] += 1\n    dataset[datasetLTeam][-1] += 1\n    k = 0\n    for j in LTeamIndexes:\n        dataset[datasetWTeam][k + 4] += rawData[i][j]\n        k += 1\n\n# convert totals to averages for statistics       \nfor i in range(0, dataset.shape[0]):\n    games = dataset[i][dataset.shape[1] - 1]\n    for j in range(2, dataset.shape[1] - 1):\n        if(games != 0): \n            val = float(dataset[i][j]) / float(games)\n            val = round(val, 3)\n            dataset[i][j] = val\n\n\n# -------------------------\n# DATA STORAGE\n# -------------------------    \n# save entire dataset to disk\nnp.savetxt(\"/kaggle/working/all_teamData.csv\", dataset, delimiter = \",\")\n\n# output feedback\nprint()\nprint()\nprint(\"All Team Dataset creation complete.\")\nprint()\nprint()\n\n# locate index where 2019 data begins\nfound = -1\ni = 0\nwhile(found == -1):\n    if(dataset[i][0] == 2019):\n        found = i\n    i += 1\n   \n# store subset dataset containing only data from 2019 \ncurr_year = dataset[found:, :]\nnp.savetxt(\"/kaggle/working/2020_teamData.csv\", curr_year, delimiter = \",\")\n\n# output feedback\nprint(\"2020 Team Dataset creation complete.\")\nprint()\nprint()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Second Stage: Train Dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -------------------------\n# DATA PREPROCESSING\n# -------------------------\n# load required files and allocate data \nteamdetails = pd.read_csv('/kaggle/working/all_teamData.csv', header = None)\nrawTeamData = teamdetails.iloc[:, :].values\n\nregseason = pd.read_csv('/kaggle/input/march-madness-analytics-2020/WDataFiles_Stage2/WRegularSeasonDetailedResults.csv')\npostseason = pd.read_csv('/kaggle/input/march-madness-analytics-2020/WDataFiles_Stage2/WNCAATourneyDetailedResults.csv')\nframes = [regseason, postseason]\ngames = pd.concat(frames)\n\n\n# -----------------------------\n# DATASET SKELETON PREPARATION\n# -----------------------------\ncols = 23\nrows = games.shape[0]\ndataset = np.zeros(shape = (rows, cols))\ndataset = dataset.astype(float)\n\n\n# -------------------------\n# DATA PROCESSING\n# -------------------------\nfor i in range(0, games.shape[0]):\n    \n    # output to console\n    print(\"Game #: \", i)\n    \n    # get descriptive data of game\n    season = games.iloc[i]['Season']\n    WTeamID = games.iloc[i]['WTeamID']\n    LTeamID = games.iloc[i]['LTeamID']\n    \n    # identify location of game (home, away or neutral)\n    x = games.iloc[i]['WLoc']\n    loc = 0\n    if(x == 'N'):\n        loc = 0\n    elif(x == 'H'):\n        loc = 1\n    elif(x == 'A'):\n        loc = -1\n    \n    # convert array to float\n    values = np.array([i+1, season])\n    values = values.astype(float)\n          \n    # add raw team data of winning team\n    found = -1\n    j = 0\n    while(found == -1):\n        if(rawTeamData[j][1] == WTeamID):\n            if(rawTeamData[j][0] == season):\n                found = j\n                WTeamData = rawTeamData[j, 2:-1]\n        j += 1\n        \n    # add raw team data of losing team\n    found = -1\n    j = 0\n    while(found == -1):\n        if(rawTeamData[j][1] == LTeamID):\n            if(rawTeamData[j][0] == season):\n                found = j\n                LTeamData = rawTeamData[j, 2:-1]\n        j += 1\n    \n    # find difference between winning and losing team statistics\n    winner = -1\n    difference = np.subtract(WTeamData, LTeamData)\n    \n    # randomize for which will come first, and modify data accordingly\n    x = random.uniform(0, 1)\n    if(x > 0.5):\n        winner = 1\n        values = np.append(values, WTeamID)\n        values = np.append(values, LTeamID)\n        values = np.append(values, loc)\n    else:\n        winner = 0\n        loc = loc * -1\n        difference = difference * -1\n        values = np.append(values, LTeamID)\n        values = np.append(values, WTeamID)\n        values = np.append(values, loc)\n        \n    # add winnning team position to data\n    difference = np.append(difference, winner)\n    difference = np.round(difference, decimals = 3)\n    \n    # merge descriptive data with team data\n    instance = np.concatenate([values, difference])\n    \n    # add game statistics to dataset\n    dataset[i] = instance\n\n\n# -------------------------\n# DATA STORAGE\n# ------------------------- \nnp.savetxt(\"/kaggle/working/all_dataset.csv\", dataset, delimiter = \",\")\n\n# output feedback\nprint()\nprint()\nprint(\"Training Dataset creation complete.\")\nprint()\nprint()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Stage 3: Creating the prediction Model(with all_datasetR, a cleaned version without nan values)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -------------------------\n# DATA PREPROCESSING\n# -------------------------\n# load required files and allocate data \ndata = pd.read_csv('/kaggle/working/all_datasetR.csv', header = None)\nX = data.iloc[:, 4:-1].values\ny = data.iloc[:, -1].values\n\n# create a train-test split with ratio 15:85\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\n\n# scale the training and testing data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# save the scaler for future use\nscalerfile = '/kaggle/working/scaler.save'\npickle.dump(sc, open(scalerfile, 'wb'))\n\n\n# -------------------------\n# MODEL CREATION\n# -------------------------\n# 1 - logistic regression\nprint(\"Training Logistic Regression model...\")\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier_lr = LogisticRegression(solver = 'lbfgs')\nclassifier_lr.fit(X_train, y_train)\n\ny_pred = classifier_lr.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nscore_lr = accuracy_score(y_test, y_pred)\n\n# 2 - random forest\nprint(\"Training Random Forest model...\")\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_rf = RandomForestClassifier(n_estimators = 75, criterion = 'entropy')\nclassifier_rf.fit(X_train, y_train)\n\ny_pred = classifier_rf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nscore_rf = accuracy_score(y_test, y_pred)\n\n# 3 - naive bayes\nprint(\"Training Naive Bayes model...\")\nfrom sklearn.naive_bayes import GaussianNB\nclassifier_nb = GaussianNB()\nclassifier_nb.fit(X_train, y_train)\n\ny_pred = classifier_nb.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nscore_nb = accuracy_score(y_test, y_pred)\n\n# 4 - neural network\nprint(\"Training Neural Network model...\")\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras import optimizers\n\nclassifier_nn = Sequential()\nclassifier_nn.add(Dense(50, input_dim = X_train.shape[1], \n                kernel_initializer = 'random_uniform', \n                activation = 'sigmoid'))\nclassifier_nn.add(Dropout(0.2))\nclassifier_nn.add(Dense(100, activation = 'relu'))\nclassifier_nn.add(Dropout(0.5))\nclassifier_nn.add(Dense(100, activation = 'relu'))\nclassifier_nn.add(Dropout(0.5))\nclassifier_nn.add(Dense(25, activation = 'relu'))\nclassifier_nn.add(Dropout(0.2))\nclassifier_nn.add(Dense(1, kernel_initializer='normal', activation = 'sigmoid'))\n\nadam = optimizers.Adam(lr = 0.005)\nclassifier_nn.compile(loss = 'binary_crossentropy', optimizer = adam)\nclassifier_nn.fit(X_train, y_train, batch_size = 20, epochs = 5)\n\ny_pred = classifier_nn.predict(X_test)\ny_pred = np.where(y_pred > 0.5, 1, 0)\nfrom sklearn.metrics import accuracy_score\nscore_nn = accuracy_score(y_test, y_pred)\n\n\n# -------------------------\n# MODEL EVALUATION\n# -------------------------\n# print the accuracy score of each \nprint()\nprint()\nprint(\"Logisitc Regression Accuracy: \" + str(round(score_lr, 3)))\nprint(\"Random Forest Accuracy: \" + str(round(score_rf, 3)))\nprint(\"Naive Bayes Accuracy: \" + str(round(score_nb, 3)))\nprint(\"Neural Network Accuracy: \" + str(round(score_nn, 3)))\n\n\n# -------------------------\n# MODEL STORAGE\n# -------------------------\n# identify the model with the highest accuracy score\nclassifiers = [classifier_lr, classifier_rf, classifier_nb, classifier_nn]\nscores = [score_lr, score_rf, score_nb, score_nn]\nx = scores.index(max(scores))\n\n# store the model with the highest accuracy score to disk\nwith open('/kaggle/working/predictor.pkl', 'wb') as fid:\n    pickle.dump(classifiers[x], fid)\n\n# output feedback\nprint()\nprint()\nprint(\"Predictive Models creation complete.\")\nprint()\nprint()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}