{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\n\nimport torchvision\nfrom torchvision import transforms as T\nimport torch.nn as nn\nimport torch.optim as optim\nfrom skimage import io\nimport cv2\n\nfrom torch.utils.data import Dataset, DataLoader\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 28\nbatch_size = 16\nnum_epochs = 10\nim_size =  512\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = pd.read_csv(\"../input/human-protein-atlas-image-classification/train.csv\")\nimag_f = labels.to_numpy()[:, 0]\nimag_f[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def op_img( id ):\n    colors = ['red','green','blue','yellow']\n    flags = cv2.IMREAD_GRAYSCALE\n    img = [cv2.imread(\"../input/human-protein-atlas-image-classification/train/\"+ id+'_'+color+'.png', flags).astype(np.float32)/255 for color in colors]\n    return np.transpose( np.stack(img, axis=-1) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class hpa_ds(Dataset):\n    def __init__(self ):\n        self.labels = pd.read_csv(\"../input/human-protein-atlas-image-classification/train.csv\").to_numpy()[:,0]\n        self.vals = pd.read_csv(\"../input/human-protein-atlas-image-classification/train.csv\").to_numpy()[:,1]\n            \n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        imgs = op_img(self.labels[idx])\n        labs = np.eye(num_classes ,dtype=np.float)[np.array( self.vals[idx].split(' ') ,dtype=np.int )].sum(axis=0)\n        \n        return torch.from_numpy(imgs) , torch.from_numpy(labs)\n\nhpa = hpa_ds()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hpa[0][0].shape , hpa[0][1].shape , len(hpa)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train , val = torch.utils.data.random_split(hpa, (30500 , 572) )\ndataloaders = { 'train': DataLoader( train ,batch_size=batch_size , num_workers=16)\n                ,'val'  : DataLoader( val ,batch_size=8 ) }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,4 , figsize=(20, 5))\nfor i in range(4):\n    ax[i].imshow(hpa[0][0][i].cpu())\n    ax[i].axis('off')\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 98\n\nc = hpa[idx][0][0, : , :]\nm = hpa[idx][0][1, : , :]\ny = hpa[idx][0][2, : , :]\nk = hpa[idx][0][3, : , :]\n\nr = torch.unsqueeze( (c+m)/2 , 0 )\ng = torch.unsqueeze( (m+k)/2 , 0 )\nb = torch.unsqueeze( (y+k)/2 , 0 )\n\nimg = torch.cat([r,g,b])\n\nplt.figure(figsize = (10,10))\nplt.imshow( img.permute(2,1,0).cpu() )\nplt.axis('off')\nplt.show()\n\nplt.figure(figsize = (10,10))\nplt.imshow( hpa[98][0][:3].permute(2,1,0).cpu() , aspect ='auto')\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_labels = [\n\"Nucleoplasm\", \n\"Nuclear membrane\",   \n\"Nucleoli\",   \n\"Nucleoli fibrillar center\" ,  \n\"Nuclear speckles\",\n\"Nuclear bodies\",\n\"Endoplasmic reticulum\",   \n\"Golgi apparatus\",\n\"Peroxisomes\",\n\"Endosomes\",\n \"Lysosomes\",\n \"Intermediate filaments\",   \n \"Actin filaments\",\n \"Focal adhesion sites\",   \n \"Microtubules\",\n \"Microtubule ends\",   \n \"Cytokinetic bridge\",   \n \"Mitotic spindle\",\n \"Microtubule organizing center\",  \n \"Centrosome\",\n \"Lipid droplets\",  \n \"Plasma membrane\",   \n \"Cell junctions\", \n \"Mitochondria\",\n \"Aggresome\",\n \"Cytosol\",\n \"Cytoplasmic bodies\",   \n \"Rods & rings\" \n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unq_c = []\nfull_c = np.array( [0]*28 )\n\nfor i in range(28):\n    unq_c.append( len( labels[labels.Target == str(i) ] ) )\n\nfor row in labels['Target']:\n    full_c[ np.array( row.split(' ') , np.int ) ] +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style='darkgrid')\nplt.figure(figsize=(10, 10))\nsns.set_color_codes(\"muted\")\nsns.barplot(x = list(full_c) , y= text_labels, label=\"half count\",color='b', orient = 'h' )\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = unq_c , y= text_labels, label=\"full count\",color='b', orient = 'h' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = hpa[-1][0][:3, : , : ]\ntr=  tr.to(device)\ntr = torch.unsqueeze( tr , 0 )\n\nmodel = torchvision.models.resnet50(pretrained=True)\nmodel = model.to(device)\nmodel.eval()\n\nwith torch.no_grad():\n    otps = model(tr)\n\notps.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class res_34(nn.Module):\n    def __init__(self ):\n        super().__init__()\n        encoder = model\n        \n        self.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        w = encoder.conv1.weight\n        self.conv1.weight = nn.Parameter(torch.cat((w, 0.5*(w[:,:1,:,:]+w[:,2:,:,:])),dim=1))\n        \n        self.bn1 = encoder.bn1\n        self.relu = nn.ReLU(inplace=True) \n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.layer0 = nn.Sequential(self.conv1,self.relu,self.bn1,self.maxpool)\n        self.layer1 = encoder.layer1\n        self.layer2 = encoder.layer2\n        self.layer3 = encoder.layer3\n        self.layer4 = encoder.layer4\n        self.avgpool = encoder.avgpool\n        self.fc = nn.Linear(2048 , num_classes )\n        \n        \n    def forward(self, x):\n        x = self.layer0(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n    \nres_next = res_34()\nres_next","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class focaloss(nn.Module):\n    def __init__(self, gamma=2):\n        super().__init__()\n        self.gamma = gamma\n        \n    def forward(self, input, target):\n        max_val = (-input).clamp(min=0)\n        loss = input - input * target + max_val + ((-max_val).exp() + (-input - max_val).exp()).log()\n        \n        m = nn.LogSigmoid()\n        invprobs = m(-input * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        \n        return loss.sum(dim=1).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_to_update = []\nfor name,param in model.named_parameters():\n    param.requires_grad = True\n    if param.requires_grad == True:\n        params_to_update.append(param)\n\noptimizer = torch.optim.SGD( params_to_update , lr=10)\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.005, max_lr=0.05,step_size_up=5,mode=\"exp_range\",gamma=0.85)\nlrs = []\n\n\nfor i in range(100):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n\nplt.plot(lrs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport copy\ndef train_model(model, dataloaders, criterion, optimizer, num_epochs , scheduler = None):\n    model = model.to(device)\n    model.cuda()\n    \n    since = time.time()\n    val_acc_history = []\n    train_acc_history = []\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}  - '.format(epoch, num_epochs - 1) , end = \" \")\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for i, (inputs, labels) in enumerate( dataloaders[phase]) :\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                inputs.cuda()\n                labels.cuda()\n                optimizer.zero_grad()\n                \n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                preds = outputs > 0.9\n\n                if phase == 'train':\n                    loss.backward()\n                    optimizer.step()\n                \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n                if(i%100==0):\n                    print(\"-\" , end ='')\n\n            if scheduler:\n                scheduler.step()                        \n                \n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print(' {} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc) , end = \" \")\n            \n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            \n            if phase == 'val':\n                val_acc_history.append(epoch_acc.to('cpu'))\n            else:\n                train_acc_history.append(epoch_acc.to('cpu'))\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    model.load_state_dict(best_model_wts)\n    hist = {}\n    hist['train'] = train_acc_history\n    hist['val'] = val_acc_history\n    return model, hist","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterian = focaloss()\n\noptimizer_ft = torch.optim.SGD( params_to_update , lr=10)\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer_ft, base_lr=0.005, max_lr=0.05,step_size_up=5,mode=\"exp_range\",gamma=0.85)\n\nres_next , hist = train_model(res_next, dataloaders , criterian, optimizer_ft, num_epochs=num_epochs , scheduler = scheduler)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_acc = np.array( hist['train'] )\nval_acc = np.array( hist['val'] )\n\nsns.lineplot(x = range(1,11) ,y =train_acc , label = \"Train accuracy\")\nsns.lineplot(x = range(1,11) ,y =val_acc , label = \"val accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(res_next , './res_34.pt')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}