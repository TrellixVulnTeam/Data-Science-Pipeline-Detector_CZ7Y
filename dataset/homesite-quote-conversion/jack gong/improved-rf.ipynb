{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"outputs":[],"source":"%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Imports\n\n# pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n\n# numpy, matplotlib, seaborn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# get homesite & test csv files as a DataFrame\nhomesite_df = pd.read_csv(\"../input/train.csv\")\ntest_df     = pd.read_csv(\"../input/test.csv\")\n\n# preview the data\nhomesite_df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"homesite_df.info()\nprint(\"----------------------------\")\ntest_df.info()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# drop unnecessary columns, these columns won't be useful in analysis and prediction\nhomesite_df = homesite_df.drop(['QuoteNumber'], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# date\n\n# Convert Date to Year, Month, and Week\nhomesite_df['Year']  = homesite_df['Original_Quote_Date'].apply(lambda x: int(str(x)[:4]))\nhomesite_df['Month'] = homesite_df['Original_Quote_Date'].apply(lambda x: int(str(x)[5:7]))\nhomesite_df['Week']  = homesite_df['Original_Quote_Date'].apply(lambda x: int(str(x)[8:10]))\n\ntest_df['Year']  = test_df['Original_Quote_Date'].apply(lambda x: int(str(x)[:4]))\ntest_df['Month'] = test_df['Original_Quote_Date'].apply(lambda x: int(str(x)[5:7]))\ntest_df['Week']  = test_df['Original_Quote_Date'].apply(lambda x: int(str(x)[8:10]))\n\nhomesite_df.drop(['Original_Quote_Date'], axis=1,inplace=True)\ntest_df.drop(['Original_Quote_Date'], axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# customers purchased insurance plan\n\n# Plot\nsns.countplot(x=\"QuoteConversion_Flag\", data=homesite_df)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# year\n# Which year has higher number of customers purchased insurance plan\n\n# Plot\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,5))\n\nsns.countplot(x=\"QuoteConversion_Flag\",hue=\"Year\", data=homesite_df, ax=axis1)\nsns.countplot(x=homesite_df[\"Year\"].loc[homesite_df[\"QuoteConversion_Flag\"] == 1], \n              order=[2013,2014,2015], ax=axis2)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# month\n# Which month has higher number of customers purchased insurance plan\n\n# Plot\nsns.countplot(x=homesite_df[\"Month\"].loc[homesite_df[\"QuoteConversion_Flag\"] == 1], \n              order=[1,2,3,4,5,6,7,8,9,10,11,12])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# fill NaN values\n\nhomesite_df.fillna(-1, inplace=True)\ntest_df.fillna(-1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# There are some columns with non-numerical values(i.e. dtype='object'),\n# So, We will create a corresponding unique numerical value for each non-numerical value in a column of training and testing set.\n\nfrom sklearn import preprocessing\n\nfor f in homesite_df.columns:\n    if homesite_df[f].dtype=='object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(np.unique(list(homesite_df[f].values) + list(test_df[f].values)))\n        homesite_df[f] = lbl.transform(list(homesite_df[f].values))\n        test_df[f] = lbl.transform(list(test_df[f].values))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# define training and testing sets\n\nX_train = homesite_df.drop(\"QuoteConversion_Flag\",axis=1)\nY_train = homesite_df[\"QuoteConversion_Flag\"]\nX_test  = test_df.drop(\"QuoteNumber\",axis=1).copy()\n\ndef classify_RF(X_train, Y_train, X_test):\n    print(\"\\nFitting Training Data...\", flush=True)\n    rfc = RandomForestClassifier(n_estimators=200, max_features=5, min_samples_leaf=50, n_jobs=-1, class_weight='balanced', verbose=2)\n    calibrated_rfc = CalibratedClassifierCV(rfc, method=\"isotonic\", cv=5)\n    calibrated_rfc.fit(X_train, Y_train)\n    print(\"Final Prediction\", flush=True)\n    return calibrated_rfc.predict(X_test)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Xgboost \n\n#params = {\"objective\": \"binary:logistic\"}\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import maxabs_scale\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n#scale\nX_train = maxabs_scale(X_train)\nX_test = maxabs_scale(X_test)\n\n#T_train_xgb = xgb.DMatrix(X_train, Y_train)\n#X_test_xgb  = xgb.DMatrix(X_test)\n\n#gbm = xgb.train(params, T_train_xgb, 20)\n#Y_pred = gbm.predict(X_test_xgb)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"Y_test = classify_RF(X_train, Y_train, X_test)\nprint(\"Generating Submission\")\nsubmission = pd.read_csv(\"../input/sample_submission.csv\")\nsubmission.QuoteConversion_Flag = Y_test\nsubmission.to_csv(\"RF_grid.csv\", index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}