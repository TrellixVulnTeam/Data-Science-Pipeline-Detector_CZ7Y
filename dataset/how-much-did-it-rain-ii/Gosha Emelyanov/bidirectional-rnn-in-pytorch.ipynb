{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torch.nn.functional as F\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)  # thx NumPy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51bec9599bbd960602d3fbd482fc6b9a0b65c47b"},"cell_type":"markdown","source":"Define some constants (I am going to use them below)."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"epochs = 10\nfixed_length = 19\nval_proportion = 0.05\nprint_every = 50\nlearning_rate = 1e-4\nbatch_size = 1024","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e17a405764c2314cf9088424fc479e2a431a6e6e"},"cell_type":"markdown","source":"Read datasets."},{"metadata":{"trusted":true,"_uuid":"15ef4b563aead16911c56a273d0c37c8f005948d"},"cell_type":"code","source":"Z_train = pd.read_csv('../input/train.csv', index_col='Id', dtype=np.float32)\nZ_test = pd.read_csv('../input/test.csv', index_col='Id', dtype=np.float32)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a281891c51707a35e1f336b20c25291e602bec5"},"cell_type":"markdown","source":"Apply some of preprocessing. As you can see, I don't care much about preprocessing, let the neural network do its work."},{"metadata":{"trusted":true,"_uuid":"916ad9b5818e6e1698f9e14d29042d71b3cb8cf9"},"cell_type":"code","source":"# See https://www.kaggle.com/c/how-much-did-it-rain-ii/discussion/16622\nref_ = Z_train['Ref'].groupby(level='Id').mean()\nZ_train.drop(ref_[ref_.isna()].index, axis=0, inplace=True)\n\n# Just replace all NaN with zero ¯\\_(ツ)_/¯\nZ_train.fillna(0, inplace=True)\nZ_test.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53bd907a408922ef1b3570e34985761e549c50a0"},"cell_type":"markdown","source":"Split on training and validation datasets."},{"metadata":{"trusted":true,"_uuid":"811a643c787ae14160073e73a43711e0431ce663"},"cell_type":"code","source":"train_unique_ids = Z_train.index.unique()\ntrain_ids = Z_train.index.unique()[:int((1 - val_proportion) * len(train_unique_ids))]\nval_ids = Z_train.index.unique()[int((1 - val_proportion) * len(train_unique_ids)):]\n\nZ_val = Z_train.loc[val_ids]\nZ_train = Z_train.loc[train_ids]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dbfb66e2a9c0658db6e710fd902d59395965d09"},"cell_type":"markdown","source":"Because ```torch.utils.data.sampler.RandomSampler``` relies on properly aligned indexes during batch sampling."},{"metadata":{"trusted":true,"_uuid":"c02acdd4879a690d263d6be78f1dd11cefc78e05"},"cell_type":"code","source":"def align_ids(df):\n    new_ids = []\n    prev_id = df.index[0]\n\n    q = 0\n    for _id in df.index:\n        q += bool(prev_id - _id)\n        prev_id = _id\n\n        new_ids.append(q)\n    return new_ids\n\nnew_train_ids = align_ids(Z_train)\nnew_val_ids = align_ids(Z_val)\nnew_test_ids = align_ids(Z_test)\n\nZ_train.set_index(pd.Index(new_train_ids), inplace=True)\nZ_val.set_index(pd.Index(new_val_ids), inplace=True)\nZ_test.set_index(pd.Index(new_test_ids), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1663a5f9686be5c5a11ac0306ece02d73b4028c6"},"cell_type":"markdown","source":"Some utility functions "},{"metadata":{"trusted":true,"_uuid":"583ee93f3d613383f38b5c8f7112fa5965d6f1d6"},"cell_type":"code","source":"def train(model, device, train_loader, criterion, optimizer, epoch):\n    model.train()\n\n    avg_loss = 0\n    for batch_idx, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        if batch_idx % print_every == 0:\n            print('Train Epoch: {} [{:.0f}%\\tLoss: {:.6f})]'.format(epoch, 100. * batch_idx / len(train_loader), loss.item()))\n        avg_loss += loss.item()\n    avg_loss /= len(train_loader.dataset)\n\n    print('\\nTrain set: Avg. loss: {:.4f}\\n'.format(avg_loss))\n    \ndef val(model, device, val_loader, criterion):\n    model.eval()\n\n    avg_loss = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            avg_loss += criterion(outputs, labels).item()\n        avg_loss /= len(val_loader.dataset)\n\n    print('\\nTest set: Avg. loss: {:.4f}\\n'.format(avg_loss))\n\ndef test(model, device, test_loader):\n    model.eval()\n\n    sol = pd.DataFrame(columns=['Id', 'Expected'])\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n\n            partial_sol = pd.DataFrame({'Id': labels.int().numpy(), 'Expected': outputs.cpu().numpy().flatten()})\n            sol = sol.append(partial_sol, ignore_index = True)\n    return sol.sort_values(by='Id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80dd1dd500ef59fac2e2168a74b994986b0a97de"},"cell_type":"markdown","source":"Define my model. Actually, not my. Thanks for inspiration: [Estimating Rainfall From Weather Radar Readings](simaaron.github.io/Estimating-rainfall-from-weather-radar-readings-using-recurrent-neural-networks/)."},{"metadata":{"trusted":true,"_uuid":"97285d9ac905a8589280406bc9e39229d876b6b0"},"cell_type":"code","source":"class BidirectionalRNN(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim, \n                 n_layers=5, activation=F.leaky_relu):\n        super(BidirectionalRNN, self).__init__()\n\n        kwargs = {'nonlinearity': 'relu', 'batch_first': True, 'bidirectional': True}\n\n        self.n_layers = n_layers\n        self.activation = activation\n\n        self.rnn = nn.ModuleList()\n        self.forward_linear = nn.ModuleList()\n        self.backward_linear = nn.ModuleList()\n        self.hidden_linear = nn.ModuleList()\n\n        prev_hi_size = None\n        for in_size, hi_size in zip([input_dim] + hidden_dim[:-1], hidden_dim):\n            self.rnn.append(nn.RNN(in_size, hi_size, **kwargs))\n            self.forward_linear.append(nn.Linear(hi_size, hi_size))\n            self.backward_linear.append(nn.Linear(hi_size, hi_size))\n\n            if prev_hi_size is not None:\n                self.hidden_linear.append(nn.Linear(prev_hi_size, hi_size))\n            prev_hi_size = hi_size\n\n        self.output_linear = nn.Linear(hidden_dim[-1], output_dim)\n\n    def forward(self, x):\n        outputs, hidden = x, None\n\n        for idx in range(self.n_layers):\n            outputs, hidden = self.rnn[idx](outputs, hidden)\n\n            outputs, output_lengths = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n\n            outputs_copy = outputs.clone()\n            outputs_copy[:, :, :self.rnn[idx].hidden_size] = self.forward_linear[idx](outputs[:, :, self.rnn[idx].hidden_size:])\n            outputs_copy[:, :, :self.rnn[idx].hidden_size] = self.activation(outputs[:, :, :self.rnn[idx].hidden_size], negative_slope=0.15)\n\n            outputs_copy[:, :, self.rnn[idx].hidden_size:] = self.backward_linear[idx](outputs[:, :, self.rnn[idx].hidden_size:])\n            outputs_copy[:, :, self.rnn[idx].hidden_size:] = self.activation(outputs[:, :, self.rnn[idx].hidden_size:], negative_slope=0.15)\n\n            outputs = outputs_copy[:, :, :self.rnn[idx].hidden_size] + outputs_copy[:, :, self.rnn[idx].hidden_size:]\n\n            if idx < len(self.hidden_linear):\n                outputs = nn.utils.rnn.pack_padded_sequence(outputs, output_lengths, batch_first=True)\n                hidden = self.hidden_linear[idx](hidden)\n\n        outputs = self.activation(self.output_linear(outputs), negative_slope=0.15).mean(dim=1)\n        return outputs","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07af8fbf66ce2812ab2e1fb6edbdee834f9de4bb"},"cell_type":"markdown","source":"Define my dataset."},{"metadata":{"trusted":true,"_uuid":"f85fc5edda1473f448a8485f3256faeb5784ba82"},"cell_type":"code","source":"class RainDataset(data.Dataset):\n    def __init__(self, X, y):   \n        self.radar_measurements = X\n        self.expected = y\n\n    def __len__(self):\n        return len(self.radar_measurements.index.unique())\n\n    def __getitem__(self, idx):\n        if (type(self.expected.loc[idx]) == np.float32):\n            return self.radar_measurements.loc[idx].values.reshape(1, -1), self.expected.loc[idx]\n        else:\n            return self.radar_measurements.loc[idx].values, self.expected.loc[idx].iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed42fef073fecb00fffcb573bb5c841547de3368"},"cell_type":"markdown","source":"Because PyTorch doesn't know how to properly stack sequences to form mini-batch, I need to define own ```collate_fn```."},{"metadata":{"trusted":true,"_uuid":"727e48ca3f39726c3207bb029e3b45e4858f505d"},"cell_type":"code","source":"def collate_fn(batch):\n    _sorted = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n\n    observations, expectations, lengths = \\\n    zip(*[(torch.FloatTensor(a), b, a.shape[0]) for (a,b) in _sorted])\n\n    input_tensor = torch.zeros(len(observations), fixed_length, observations[0].size(1)).float()\n    for batch_idx in range(input_tensor.size(0)):\n        for obs_idx, obs in enumerate(observations[batch_idx]):\n            input_tensor[batch_idx, obs_idx, :] = obs\n\n    pack = nn.utils.rnn.pack_padded_sequence(input_tensor, lengths, batch_first=True)\n    return pack, torch.FloatTensor(expectations)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"190577d3a8f4862d21c1ddcda92f439e26078799"},"cell_type":"markdown","source":"Prepare data loaders."},{"metadata":{"trusted":true,"_uuid":"89401f61feaabf2d12a95053eb0d92be65c4cf3d"},"cell_type":"code","source":"Z_test['Expected'] = Z_test.index + 1  # Hacked\n\nX, y = Z_train.drop('Expected', axis=1), Z_train['Expected']\ntrain_dataset = RainDataset(X, y)\nX, y = Z_val.drop('Expected', axis=1), Z_val['Expected']\nval_dataset = RainDataset(X, y)\nX, y = Z_test.drop('Expected', axis=1), Z_test['Expected'].astype(np.float32)\ntest_dataset = RainDataset(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a2040ca72eec9c9ce0ee08c35bb032d4144fd1c"},"cell_type":"code","source":"train_loader = data.DataLoader(train_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\nval_loader = data.DataLoader(val_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=True)\ntest_loader = data.DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46136c7ca1842bf31cc62100fde15f93d9b9e8c3"},"cell_type":"markdown","source":"Train the model."},{"metadata":{"trusted":true,"_uuid":"939dac817275b3bd034ccf734653bbe47e69dceb"},"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = BidirectionalRNN(input_dim=22, output_dim=1, hidden_dim=[64, 128, 256, 128, 64]).to(device)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.L1Loss()\n\nfor epoch in range(epochs):\n    train(model, device, train_loader, criterion, optimizer, epoch)\n    val(model, device, val_loader, criterion)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"071d649aba37f55415f5c343c3a9f7fb09c13c94"},"cell_type":"markdown","source":"Make submission. **This is really bad in terms of implementation, so never do something like this**."},{"metadata":{"trusted":true,"_uuid":"2583aeaaaa604acab236383c1f9eb9f452c76caa"},"cell_type":"code","source":"submission = test(model, device, test_loader)\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}