{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Instantiation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('fivethirtyeight')\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data input and exploration"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/tabular-playground-series-jan-2021/test.csv\")\ntrain = pd.read_csv(\"../input/tabular-playground-series-jan-2021/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,2, figsize=(20,6))\nsns.distplot(train.target, ax=ax[0])\nsns.boxplot(train.target, ax=ax[1], color=\"maroon\", saturation=4.6)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outlier removal"},{"metadata":{},"cell_type":"markdown","source":"Removiing outliers which are more than 1.5* 3rd quantile and less than 1.5* 1st quartile"},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_band = (np.quantile(train.target,0.75) - np.quantile(train.target,0.25))*1.5\nlow, high = np.quantile(train.target,0.25) - outlier_band, np.quantile(train.target,0.75) + outlier_band \ntrain = train[ (train.target>low) & (train.target<high)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(1,2, figsize=(20,6))\nsns.distplot(train.target, ax=ax[0], color=\"green\")\nsns.boxplot(train.target, ax=ax[1], color=\"gold\", saturation=0.6)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation checking between features and target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nfig, ax = plt.subplots(figsize=(18,12)) \nsns.heatmap(corr,cmap=\"RdPu\", mask=mask, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the correlation matrix some of the features are correlated. We need to something about them :("},{"metadata":{},"cell_type":"markdown","source":"# Feature engg"},{"metadata":{},"cell_type":"markdown","source":"**Let's have a look under the hood of our ML predictors, shall we?**"},{"metadata":{},"cell_type":"markdown","source":"Using mutual_info_regression to capture even non linear-relation between target and predictor variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, mutual_info_regression\nfs = SelectKBest(score_func=mutual_info_regression, k=\"all\")\nfs.fit(train.drop(['target'],axis=1), train.target)\nX_n = fs.transform(train.drop(['target'],axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = pd.concat([pd.DataFrame(train.columns),pd.DataFrame(fs.scores_)],axis=1)\nscore.columns = [\"feature\",\"scores\"]\nscore = score.sort_values(\"scores\", ascending=False)\nscore = score[score.feature != \"target\"]\nsns.barplot(x=score.scores, y=score.feature)\nplt.title(\"Importance of features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's have a look at the variance of each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(5,3, figsize=(30,30))\nsns.boxplot(train.id, ax=axes[0,0], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont1, ax=axes[0,1], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont2, ax=axes[0,2], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont3, ax=axes[1,0], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont4, ax=axes[1,1], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont5, ax=axes[1,2], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont6, ax=axes[2,0], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont7, ax=axes[2,1], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont8, ax=axes[2,2], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont9, ax=axes[3,0], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont10, ax=axes[3,1], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont11, ax=axes[3,2], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont12, ax=axes[4,0], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont13, ax=axes[4,1], color=\"yellow\", saturation=0.75)\nsns.boxplot(train.cont14, ax=axes[4,2], color=\"yellow\", saturation=0.75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regression plots:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(5,3, figsize=(30,30))\nsns.regplot(train.id, train.target, ax=axes[0,0], color=\"purple\")\nsns.regplot(train.cont1, train.target,ax=axes[0,1], color=\"purple\")\nsns.regplot(train.cont2, train.target,ax=axes[0,2], color=\"purple\")\nsns.regplot(train.cont3, train.target,ax=axes[1,0], color=\"purple\")\nsns.regplot(train.cont4, train.target,ax=axes[1,1], color=\"purple\")\nsns.regplot(train.cont5, train.target,ax=axes[1,2], color=\"purple\")\nsns.regplot(train.cont6, train.target,ax=axes[2,0], color=\"purple\")\nsns.regplot(train.cont7, train.target,ax=axes[2,1], color=\"purple\")\nsns.regplot(train.cont8, train.target,ax=axes[2,2], color=\"purple\")\nsns.regplot(train.cont9, train.target,ax=axes[3,0], color=\"purple\")\nsns.regplot(train.cont10, train.target,ax=axes[3,1], color=\"purple\")\nsns.regplot(train.cont11, train.target,ax=axes[3,2], color=\"purple\")\nsns.regplot(train.cont12, train.target,ax=axes[4,0], color=\"purple\")\nsns.regplot(train.cont13, train.target,ax=axes[4,1], color=\"purple\")\nsns.regplot(train.cont14, train.target,ax=axes[4,2], color=\"purple\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's have a closer look at more interesting features\n1. cont2\n2. cont3\n3. cony9\n4. cont14"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(23,8))\nsns.scatterplot(train.cont2, train.target, color=\"black\")\nplt.axvline(0.22, color='purple')\nplt.axvline(0.35, color='violet')\nplt.axvline(0.415, color='blue')\nplt.axvline(0.481, color='green')\nplt.axvline(0.549, color='yellow')\nplt.axvline(0.612, color='orange')\nplt.axvline(0.673, color='red')\nplt.axvline(0.727, color='pink')\nplt.axvline(0.747, color='black')\nplt.title(\"Binning cont2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(23,8))\nsns.scatterplot(train.cont3, train.target)\nplt.axvline(0.386, color=\"black\")\nplt.axvline(0.78, color=\"white\")\nplt.title(\"Binning cont3\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(23,8))\nsns.scatterplot(train.cont9, train.target, color=\"green\")\nplt.axvline(0.108, color=\"red\")\nplt.axvline(0.1165, color=\"pink\")\nplt.axvline(0.559, color=\"magenta\")\nplt.axvline(0.84, color=\"orange\")\nplt.title(\"Binning cont9\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(23,8))\nsns.scatterplot(train.cont14, train.target, color=\"blue\")\nplt.axvline(0.34, color=\"yellow\")\nplt.axvline(0.525, color=\"orange\")\nplt.axvline(0.66, color=\"red\")\nplt.axvline(0.78, color=\"pink\")\nplt.title(\"Binning cont14\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Binning the data for features:\n1. cont2 - [0.22, 0.35, 0.415, 0.489, 0.549, 0.612, 0.673, 0.727, 0.747]\n2. cont3 - [0.386, 0.78]\n3. cont9 - [0.108, 0.1165, 0.559, 0.84]\n4. cont14 - [0.34, 0.525, 0.66, 0.78]"},{"metadata":{"trusted":true},"cell_type":"code","source":"lims2 = [-0.1,0.22, 0.35, 0.415, 0.489, 0.549, 0.612, 0.673, 0.727, 0.747,0.9]\nlims3 = [0,0.386, 0.78,1.5]\nlims9 =  [-0.2,0.108, 0.1165, 0.559, 0.84,5]\nlims14 = [0,0.34, 0.525, 0.66, 0.78,0.9]\n\ntrain[\"c14\"] = pd.cut(train.cont14, bins=lims14, labels=np.arange(0,5), include_lowest=True)\ntrain[\"c2\"] = pd.cut(train.cont2, bins=lims2, labels=np.arange(0,10),  include_lowest=True)\ntrain[\"c3\"] = pd.cut(train.cont3, bins=lims3, labels=np.arange(0,3),include_lowest=True)\ntrain[\"c9\"] = pd.cut(train.cont9, bins=lims9, labels=np.arange(0,5), include_lowest=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now converting this categorical data into binomial distribution for each feature state"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[[\"c14-0\",\"c14-1\",\"c14-2\",\"c14-3\",\"c14-4\"]] = pd.DataFrame(pd.get_dummies(train.c14))\ntrain[[\"c2-0\",\"c2-1\",\"c2-2\",\"c2-3\",\"c2-4\",\"c2-5\",\"c2-6\",\"c2-7\",\"c2-8\",\"c2-9\"]] = pd.DataFrame(pd.get_dummies(train.c2))\ntrain[[\"c3-0\",\"c3-1\",\"c3-2\"]] = pd.DataFrame(pd.get_dummies(train.c3))\ntrain[[\"c9-0\",\"c9-1\",\"c9-2\",\"c9-3\",\"c9-4\"]] = pd.DataFrame(pd.get_dummies(train.c9))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping the categorical bins which we had created"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop([\"c2\",\"c3\",\"c9\",\"c14\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation of all features with target"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop([\"target\"], axis=1).corrwith(train.target).to_frame().sort_values(0, ascending=False).style.background_gradient(cmap=\"RdPu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trial models"},{"metadata":{},"cell_type":"markdown","source":"### Creating the predictor and target sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train.drop([\"target\",\"id\"],axis=1)\ny=train.target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data into train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_tr, x_te, y_tr, y_te = train_test_split(x,y, test_size=0.33)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will try with the following algorithms and use GridSearchCV to choose the parameters and the model. Please note that I have generally only mentioned the hyperparameter that I finally used for the model because my laptop is slow :(\n1. XGBoost\n2. CatBoost\n3. KNeighborsRegressor\n4. DecisionTreesRegressor\n5. ExtraTreesRegressor\n6. LightGBM\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxg_reg = xgb.XGBRegressor(objective='reg:squarederror', verbose=0)\nXG = GridSearchCV(xg_reg, scoring='neg_mean_squared_error', cv=5,param_grid={'colsample_bytree':[0.1], 'learning_rate':[0.1, 0.01], 'max_depth':[16], 'alpha':[5], 'n_estimators':[50]})\nXG.fit(x_tr,y_tr)\nprint(XG.score(x_te,y_te))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as ctb\nctb_reg = ctb.CatBoostRegressor(verbose=0)\nCB = GridSearchCV(ctb_reg, scoring='neg_mean_squared_error', cv=5,param_grid={'learning_rate':[0.1], 'max_depth':[12]})\nCB.fit(x_tr,y_tr)\nprint(CB.score(x_te,y_te))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNeighboursRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.neighbors as knn\nknn_reg = knn.KNeighborsRegressor(n_neighbors=5, algorithm='kd_tree')\nKNN = GridSearchCV(knn_reg, scoring='neg_mean_squared_error', cv=5,param_grid={'n_neighbors':[5], 'weights':['uniform','distance']})\nKNN.fit(x_tr,y_tr)\nprint(KNN.score(x_te,y_te))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.tree as t\ndt_reg = t.DecisionTreeRegressor(criterion='mse')\nDT = GridSearchCV(dt_reg, scoring='neg_mean_squared_error', cv=5, param_grid={'max_depth': [16]})\nDT.fit(x_tr,y_tr)\nprint(DT.score(x_te,y_te))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ExtratreesRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.tree as t\next_reg = t.ExtraTreeRegressor(criterion='mse')\nET = GridSearchCV(ext_reg, scoring='neg_mean_squared_error', cv=5, param_grid={'max_depth': [16]})\nET.fit(x_tr, y_tr)\nprint(ET.score(x_te,y_te))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nlgb = lgb.LGBMRegressor()\nL = GridSearchCV(lgb, scoring='neg_mean_squared_error', cv=5, param_grid={'max_depth': [8,9,10,11,12,13,14,15,16], 'n_estimators':[1000,5000], 'learning_rate':[0.01]})\nL.fit(x_tr,y_tr)\nL.score(x_te,y_te)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM offers the best result"},{"metadata":{},"cell_type":"markdown","source":"# Final model"},{"metadata":{},"cell_type":"markdown","source":"Applying all the transformations to test set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lims2 = [-0.1,0.22, 0.35, 0.415, 0.489, 0.549, 0.612, 0.673, 0.727, 0.747,0.9]\nlims3 = [0,0.386, 0.78,1.5]\nlims9 =  [-0.2,0.108, 0.1165, 0.559, 0.84,5]\nlims14 = [0,0.34, 0.525, 0.66, 0.78,0.9]\n\n\ntest[\"c14\"] = pd.cut(test.cont14, bins=lims14, labels=np.arange(0,5), include_lowest=True)\ntest[\"c2\"] = pd.cut(test.cont2, bins=lims2, labels=np.arange(0,10),  include_lowest=True)\ntest[\"c3\"] = pd.cut(test.cont3, bins=lims3, labels=np.arange(0,3),include_lowest=True)\ntest[\"c9\"] = pd.cut(test.cont9, bins=lims9, labels=np.arange(0,5), include_lowest=True)\n\ntest[[\"c14-0\",\"c14-1\",\"c14-2\",\"c14-3\",\"c14-4\"]] = pd.DataFrame(pd.get_dummies(test.c14))\ntest[[\"c2-0\",\"c2-1\",\"c2-2\",\"c2-3\",\"c2-4\",\"c2-5\",\"c2-6\",\"c2-7\",\"c2-8\",\"c2-9\"]] = pd.DataFrame(pd.get_dummies(test.c2))\ntest[[\"c3-0\",\"c3-1\",\"c3-2\"]] = pd.DataFrame(pd.get_dummies(test.c3))\ntest[[\"c9-0\",\"c9-1\",\"c9-2\",\"c9-3\",\"c9-4\"]] = pd.DataFrame(pd.get_dummies(test.c9))\nidzz=test.id\ntest.drop([\"c2\",\"c3\",\"c9\",\"c14\",\"id\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = L\nmodel.fit(x,y)\nyhat = model.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({\"Id\":idzz, \"target\":yhat})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}