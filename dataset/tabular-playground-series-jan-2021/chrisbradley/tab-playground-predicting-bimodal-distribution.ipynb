{"cells":[{"metadata":{},"cell_type":"markdown","source":"The competition is about fine margins, which i have had reasonable success by blending models and averaging many outputs. I havent had much success by removing outliers, transforming the data and attempting to normalize the target... all which seems counter intuitive.<br>\n\nSo is there another method we could use to gain a competative advantage, since a fractional improvement could mean a decent place on the leaderboard?\n\nThis kernel is inspired by the notebook: [handling-multimodal-distributions-fe-techniques](https://www.kaggle.com/iamleonie/handling-multimodal-distributions-fe-techniques/comments). <br>\nThe below shows there is potential to get a great score if we can split target into 2 distributions.<br>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# libs\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import confusion_matrix\n\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBRegressor\n\nimport optuna \nfrom optuna import Trial\nfrom optuna.samplers import TPESampler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load\ndf_train = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/test.csv')\ndf_train.info(verbose=False, memory_usage='deep')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# config\ntrain_mode = False\ntarget = 'target'\nrandom_state = 42\nx_cols = [c for c in df_train.columns if 'cont' in c] # all training columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation.\nSimple kfold prediction, returns oof and an average prediction on the test set (which i disregard when tuning)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluation function\ndef _evaluate(model, x_cols, df, target=target, n_folds=5):\n    \n    oof = np.zeros(len(df[target])) # means 'out of fold' - basically where we are going to store our test fold predictions\n    preds_test = np.zeros(len(df_test)) # test set predictions\n\n    # enum folds\n    kf = KFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n    for idx_train, idx_test in kf.split(df):\n\n        # setup test / train data\n        x_train = df.loc[idx_train, x_cols].values\n        y_train = df.loc[idx_train, target].values\n        x_test = df.loc[idx_test, x_cols].values\n        y_test = df.loc[idx_test, target].values\n    \n        # fit / predict\n        model.fit(x_train, y_train)#, eval_set = [(x_test, y_test)], early_stopping_rounds=100, verbose=False)\n        preds_train = model.predict(x_test) # train set predictions (used for hypertuning)\n        preds_test += model.predict(df_test[x_cols].values) / n_folds\n        \n        # append train predictions\n        oof[idx_test] = preds_train\n    \n    return oof, preds_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline.\nLets get a baseline score to try and beat..."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# get baseline score\nlgbm = LGBMRegressor(seed=random_state)\noof, preds_test = _evaluate(lgbm, x_cols, df_train)\n\n# plot\nfig, ax = plt.subplots(figsize=(12, 4))\nsns.kdeplot(df_train[target], color='b', label='actual')\nsns.kdeplot(oof, color='r', label='prediction')\nax.set_title('mse: ' + str(round(mean_squared_error(df_train[target], oof, squared=False), 5)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mse of 0.702, you can also see above that the model hasnt quite handled the biomodal distribution. <br>\n# Eda.\n## Correlations."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# correlations / mask\ncorr = df_train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# plot\nfig, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .6})\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems too convenient that the target doesnt correlate with anything. Initial observations are that fields 1 and 6-13 correlate well and 2-5 not so well. <br>\nIt does lead you to think that this is 2 datasets merged together and the challenge is to unpick it.\n## Boxplots (vs test data)."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# setup plot\nplot_cols = 5 # no of columns / change to preference\nlist_feats = [a for a in df_train.columns if 'cont' in a] # all features\ntotal_rows = int(np.ceil(len(list_feats) / plot_cols))\nfig, ax = plt.subplots(nrows=total_rows, ncols=plot_cols, figsize=(20, 4 * total_rows)) # setup subplots\n\n# loop each feature / plot\nj = 0 # keeps track of rows\nfor i in range(0, len(list_feats)): # loop index of each feature\n    \n    # create plot\n    sns.boxplot(data=[df_train[list_feats[i]], df_test[list_feats[i]]], palette='vlag', ax=ax[j,i % plot_cols])\n    ax[j,i % plot_cols].set_title(list_feats[i])\n    ax[j,i % plot_cols].set_ylabel('')\n    ax[j,i % plot_cols].set_xlabel('')\n    \n    # increment row\n    if i % plot_cols == (plot_cols - 1): j += 1 # basically says at end of each column start a new row\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Straight away you can see cont2 has a different distribution and cont7 / cont10 have extra outliers - we can trial removing them.\n## Distributions."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# setup plot\nplot_cols = 5 # no of columns / change to preference\nlist_feats = [a for a in df_train.columns if 'cont' in a] # all features\ntotal_rows = int(np.ceil(len(list_feats) / plot_cols))\nfig, ax = plt.subplots(nrows=total_rows, ncols=plot_cols, figsize=(20, 4 * total_rows)) # setup subplots\n\n# loop each feature / plot\nj = 0 # keeps track of rows\nfor i in range(0, len(list_feats)): # loop index of each feature\n    \n    # create plot\n    sns.distplot(df_train[list_feats[i]], ax=ax[j,i % plot_cols])\n    ax[j,i % plot_cols].set_title(list_feats[i])\n    ax[j,i % plot_cols].set_ylabel('')\n    ax[j,i % plot_cols].set_xlabel('')\n    \n    # increment row\n    if i % plot_cols == (plot_cols - 1): j += 1 # basically says at end of each column start a new row\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset is riddled with multimodal distributions, we may get a better result from binning... something to test."},{"metadata":{},"cell_type":"markdown","source":"## Iqr.\nUsing the Interquartile Range (Iqr) to remove outliers is fairly common practice, I wonder why sklearn doesnt have its own Iqr function?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# mask of rows outside iqr\ndef _iqr_mask(feature):\n    \n    # compute quantiles\n    q1 = df_train[feature].quantile(0.25)\n    q3 = df_train[feature].quantile(0.75)\n    iqr = q3 - q1\n    min_quartile = q1 - 1.5 * iqr\n    max_quartile = q3 + 1.5 * iqr\n\n    # create bool mask\n    mask_iqr = (df_train[feature] >= min_quartile) & (df_train[feature] <= max_quartile)\n    \n    return mask_iqr\n\n# get iqr mask for target\nmask_iqr_target = _iqr_mask(target)\n\n# plot\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20,5))\nsns.boxplot(data=df_train[target], ax=ax[0,0])\nsns.kdeplot(df_train[target], ax=ax[1,0])\nsns.boxplot(data=df_train.loc[mask_iqr_target, target], ax=ax[0,1])\nsns.kdeplot(df_train.loc[mask_iqr_target, target], ax=ax[1,1])\nax[0,0].set_title('pre iqr')\nax[0,1].set_title('post iqr')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visually the distribution looks much better and yeilds a much better training score, however the same cannot be said about the LB score."},{"metadata":{},"cell_type":"markdown","source":"# Cont 7 / Cont 10\nAgain i get a better oof score when the following are removed, this isnt reflected in the LB score."},{"metadata":{"trusted":true},"cell_type":"code","source":"# # remove outliers / reset index\n# df_train = df_train[df_train['cont7'] > -0.03]\n# df_train = df_train.loc[_iqr_mask('cont10')]\n# df_train.reset_index(drop=True, inplace=True)\n\n# plot\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,5))\nsns.boxplot(df_train['cont7'], ax=ax[0])\nsns.boxplot(df_train['cont10'], ax=ax[1])\nax[0].set_title('cont7')\nax[1].set_title('cont10')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optuna.\nAlways been a fan of GridSearch, until i found Optuna (its much more flexible, uses ranges rather that brute forcing lists and works better on larger datasets)..."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# lgbm / objective function\ndef objective(trial):\n    \n    # hyperparameters\n    param = {\n        'boosting_type':'gbdt',\n        'num_leaves':trial.suggest_int('num_leaves', 3, 150),\n        'max_depth':trial.suggest_int('max_depth', -1, 20),\n        'learning_rate':trial.suggest_float('learning_rate', 0.001, 0.6),\n        'n_estimators':trial.suggest_int('n_estimators', 50, 500),\n        'min_child_weight':trial.suggest_float('min_child_weight', 0.2, 0.6),\n        'min_child_samples':trial.suggest_int('min_child_samples', 15, 30),\n        'subsample':trial.suggest_float('subsample', 0.5, 1.0),\n        'subsample_freq':trial.suggest_int('subsample_freq', 3, 150),\n        'random_state':random_state,\n        'lambda_l1':trial.suggest_float('lambda_l1', 0.0, 5.0)\n    }\n    \n    # model / evaluate\n    lgbm = LGBMRegressor(**param) # model\n    x_cols = [c for c in df_train.columns if 'cont' in c] # all columns\n    oof, preds_test = _evaluate(lgbm, x_cols, df_train.reset_index(drop=True)) # evaluate\n    \n    return mean_squared_error(df_train[target].reset_index(drop=True), oof, squared=False)\n\nif train_mode:\n\n    # run study\n    study = optuna.create_study(direction='minimize', sampler=TPESampler())\n    study.optimize(objective, n_trials=200)\n\n    # output study\n    print (study.best_value)\n    print (study.best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# cat / objective function\ndef objective(trial):\n    \n    # hyperparameters\n    param = {\n        'iterations':1000,\n        'verbose':False,\n        'random_state':random_state,\n        'loss_function':'RMSE',\n        'bootstrap_type':'Bernoulli',\n        'learning_rate':trial.suggest_float('learning_rate', 0.0001, 0.31),\n        'max_depth':trial.suggest_int('max_depth', 3, 10),\n        'colsample_bylevel':trial.suggest_float('colsample_bylevel', 0.3, 0.8),\n    }\n    \n    # model / evaluate\n    cat = CatBoostRegressor(**param) # model\n    x_cols = [c for c in df_train.columns if 'cont' in c] # all columns\n    oof, preds_test = _evaluate(cat, x_cols, df_train, target=target) # evaluate\n    \n    return mean_squared_error(df_train[target], oof, squared=False)\n\nif train_mode:\n    \n    # run study\n    study = optuna.create_study(direction='minimize', sampler=TPESampler())\n    study.optimize(objective, n_trials=100)\n\n    # output study\n    print (study.best_value)\n    print (study.best_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb / objective function\ndef objective(trial):\n    \n    # hyperparameters\n    param = {\n        'random_state':random_state,\n        'objective':'reg:squarederror',\n        'booster':'gbtree',\n        'learning_rate':trial.suggest_float('learning_rate', 0.001, 0.1),\n        'alpha':trial.suggest_float('alpha', 0.001, 0.1),\n        'colsample_bylevel':trial.suggest_float('colsample_bylevel', 0.05, 0.1),\n        'colsample_bytree':trial.suggest_float('colsample_bytree', 0.1, 0.9),\n        'gamma':trial.suggest_float('gamma', 0, 0.5),\n        'max_depth':trial.suggest_int('max_depth', 3, 18),\n        'min_child_weight': trial.suggest_float ('min_child_weight', 1, 20),\n        #'reg_lambda':trial.suggest_int('reg_lambda', 0, 10),\n        #'reg_alpha':trial.suggest_int('reg_alpha', 10, 50),\n        'subsample':trial.suggest_float('subsample', 0.3, 0.7),\n    }\n    \n    # model / evaluate\n    xgb = XGBRegressor(**param) # model\n    x_cols = [c for c in df_train.columns if 'cont' in c] # all columns\n    oof, preds_test = _evaluate(xgb, x_cols, df_train, target=target) # evaluate\n    \n    return mean_squared_error(df_train[target], oof, squared=False)\n\nif train_mode:\n    \n    # run study\n    study = optuna.create_study(direction='minimize', sampler=TPESampler())\n    study.optimize(objective, n_trials=100)\n\n    # output study\n    print (study.best_value)\n    print (study.best_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Run each of our hyper optimized models, but this time keep hold of the test predictions..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyper params per each model\nparam_lgbm = {'num_leaves': 148, 'max_depth': 19, 'learning_rate': 0.04168752594808129, 'n_estimators': 468, 'min_child_weight': 0.35392113973764505, 'min_child_samples': 29, 'subsample': 0.9348697769228501, 'subsample_freq': 6, 'lambda_l1': 4.639129744838143}\nparam_cat = {'learning_rate': 0.07468089271003528, 'max_depth': 8, 'colsample_bylevel': 0.7338241468797853}\nparam_xgb = param_xgb = {'learning_rate': 0.0652222304334701, 'alpha': 0.0036866921576056855, 'colsample_bylevel': 0.09959606060270643, 'colsample_bytree': 0.863554381598069, 'gamma': 0.3959383978062547, 'max_depth': 15, 'min_child_weight': 19.357558021086128, 'subsample': 0.6991638855748524}\n\n# lgbm hyperopt score\nlgbm = LGBMRegressor(seed=random_state, **param_lgbm)\noof_lgbm, preds_lgbm = _evaluate(lgbm, x_cols, df_train, n_folds=5)\nprint ('hyper lgbm:', mean_squared_error(df_train[target], oof_lgbm, squared=False))\n\n# catboost hyperopt score\ncat = CatBoostRegressor(iterations=1000, verbose=False, random_state=random_state, loss_function='RMSE', bootstrap_type='Bernoulli', **param_cat)\noof_cat, preds_cat = _evaluate(cat, x_cols, df_train, n_folds=5)\nprint ('hyper cat:', mean_squared_error(df_train[target], oof_cat, squared=False))\n\n# xgb score\nxgb = XGBRegressor(random_state=random_state, **param_xgb)\noof_xgb, preds_xgb = _evaluate(xgb, x_cols, df_train, n_folds=2) # scaled back so it runs quicker for public.\nprint ('hyper xgb:', mean_squared_error(df_train[target], oof, squared=False))\n\n# blend score\nprint ('hyper blend:', mean_squared_error(df_train[target], ((oof_lgbm * 0.4) + (oof_cat * 0.4) + (oof_xgb * 0.2)), squared=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though we have only gone from 0.702 to 0.696 - its a huge improvement!"},{"metadata":{},"cell_type":"markdown","source":"# Submission (pt1).\nLets blend our model prediction to get a more robust average..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge model outputs to test\ndf_test['hyper_lgbm'] = preds_lgbm\ndf_test['hyper_cat'] = preds_cat\ndf_test['hyper_xgb'] = preds_xgb\ndf_test['hyper_blend'] = (preds_lgbm * 0.4) + (preds_cat * 0.4) + (preds_xgb * 0.2)\n\n# assign target\ndf_test['target'] = df_test['hyper_blend']\ndf_test[['id','target']].to_csv('submission_hyper_blend.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This produces a LB score of: <br>\n0.69851 - no records removed. <br>\n0.69858 - cont7 / cont 10 outliers removed. <br>\n0.69762 - xgb blend."},{"metadata":{},"cell_type":"markdown","source":"# Seeds.\nIf at first you dont succeed, pick another seed and try again (or blend a few)... <br>\nThe following is commented out because of the time it takes to run."},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 3 # controls no of times model is run and no of folds within evaluation\n\n# arrays to store oof and predictions\noof_lgbm_seeds = np.zeros(len(df_train)) \noof_car_seeds = np.zeros(len(df_train))\npreds_lgbm_seeds = np.zeros(len(df_test)) \npreds_car_seeds = np.zeros(len(df_test)) \n\n# enum each fold / generate random num (used for seed)\nfor g in range(n_folds):\n    rand_num = random.randint(1, 5000)\n    \n    # lgbm predictions\n    lgbm = LGBMRegressor(seed=rand_num, **param_lgbm)\n    oof, preds_lgbm = _evaluate(lgbm, x_cols, df_train, n_folds=n_folds)\n    oof_lgbm_seeds += (oof / n_folds) # merge with seed results\n    preds_lgbm_seeds += (preds_lgbm / n_folds) # merge with seed results\n    \n    # cat boost predictions\n    cat = CatBoostRegressor(iterations=1000, verbose=False, random_state=rand_num, loss_function='RMSE', bootstrap_type='Bernoulli', **param_cat)\n    oof, preds_cat = _evaluate(cat, x_cols, df_train, n_folds=n_folds)\n    oof_car_seeds += (oof / n_folds) # merge with seed results\n    preds_car_seeds += (preds_cat / n_folds) # merge with seed results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('hyper lgbm:', mean_squared_error(df_train[target], oof_lgbm_seeds, squared=False))\nprint ('hyper cat:', mean_squared_error(df_train[target], oof_car_seeds, squared=False))\n\n# hyper lgbm: 0.6959915123976514\n# hyper cat: 0.697341630050542","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission (pt2)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge model outputs to test\ndf_test['seed_lgbm'] = preds_lgbm_seeds\ndf_test['seed_cat'] = preds_car_seeds\ndf_test['seed_blend'] = (preds_lgbm_seeds * 0.4) + (preds_car_seeds * 0.6)\n\n# assign target\ndf_test['target'] = df_test['seed_blend']\ndf_test[['id','target']].to_csv('submission_seed_blend.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lb: 0.69847 - barely any difference."},{"metadata":{},"cell_type":"markdown","source":"# Gaussian Mixture (Gmm).\nThis was inspired by [https://www.kaggle.com/iamleonie/handling-multimodal-distributions-fe-techniques/comments](https://www.kaggle.com/iamleonie/handling-multimodal-distributions-fe-techniques/comments).<br>\nThe basics of the following process are:\n* Split the data into 2 based on the Gmm distribution output.\n* Train a classification model to predict which distribution it belongs to.\n* Train 2 regression models on each of the distributions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply gaussian mix / assign bins back to training\ngmm = GaussianMixture(n_components=2, random_state=42)\ngmm.fit(df_train[target].values.reshape(-1, 1))\ndf_train['target_gmm'] = gmm.predict(df_train[target].values.reshape(-1, 1))\n\n# masks for each gmm bin\nmask_gmm_0 = df_train['target_gmm'] == 0\nmask_gmm_1 = df_train['target_gmm'] == 1\n\n# plot\nfig, ax = plt.subplots(figsize=(12, 4))\nsns.kdeplot(data=df_train.loc[mask_gmm_0, target], label='gmm bin 0')\nsns.kdeplot(data=df_train.loc[mask_gmm_1, target], label='gmm bin 1')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof masks for dataframe\noof_gmm_0, preds = _evaluate(lgbm, x_cols, df_train.loc[mask_gmm_0].reset_index(drop=True))\noof_gmm_1, preds = _evaluate(lgbm, x_cols, df_train.loc[mask_gmm_1].reset_index(drop=True))\n\n# plot\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 4))\nsns.kdeplot(data=df_train.loc[mask_gmm_0, target], ax=ax[0], label='actual')\nsns.kdeplot(data=oof_gmm_0, ax=ax[0], label='prediction')\nsns.kdeplot(data=df_train.loc[mask_gmm_1, target], ax=ax[1], label='actual')\nsns.kdeplot(data=oof_gmm_1, ax=ax[1], label='prediction')\n\n# decorate\nax[0].set_title('gmm_0: ' + str(round(mean_squared_error(df_train.loc[mask_gmm_0, target], oof_gmm_0, squared=False), 5)))\nax[1].set_title('gmm_1: ' + str(round(mean_squared_error(df_train.loc[mask_gmm_1, target], oof_gmm_1, squared=False), 5)))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting results, we have gone from 0.71 to 0.38/0.36. If we could only split the test set accurately we could be onto a winning prediction...\n# Classification.\nTo make this easier to test we will reserve a random 3rd of the training data for a final test."},{"metadata":{"trusted":true},"cell_type":"code","source":"# split training data / reset indexes\ndf_gmm_test = df_train.sample(100000).copy()\ndf_gmm_train = df_train[(~df_train['id'].isin(df_gmm_test['id'].to_list()))].copy()\ndf_gmm_test.reset_index(drop=True, inplace=True)\ndf_gmm_train.reset_index(drop=True, inplace=True)\nprint (df_gmm_train.shape, df_gmm_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# baseline score for gmm classifier\nlgbm_gmm_class = LGBMClassifier(random_state=random_state)\noof, preds = _evaluate(lgbm_gmm_class, x_cols, df_gmm_train, target='target_gmm')\nprint(accuracy_score(df_gmm_train['target_gmm'], oof))\nprint(confusion_matrix(df_gmm_train['target_gmm'], oof))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unfortunatley not that accurately, can we boost a better value..."},{"metadata":{},"cell_type":"markdown","source":"# Optuna."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# # lgbm / objective function\n# def objective(trial):\n    \n#     # hyperparameters\n#     param = {\n#         'num_leaves':trial.suggest_int('num_leaves', 3, 150),\n#         'max_depth':trial.suggest_int('max_depth', -1, 20),\n#         'learning_rate':trial.suggest_float('learning_rate', 0.001, 0.6),\n#         'n_estimators':trial.suggest_int('n_estimators', 50, 500),\n#         'min_child_weight':trial.suggest_float('min_child_weight', 0.2, 0.6),\n#         'min_child_samples':trial.suggest_int('min_child_samples', 15, 30),\n#         'subsample':trial.suggest_float('subsample', 0.5, 1.0),\n#         'subsample_freq':trial.suggest_int('subsample_freq', 3, 150),\n#         'random_state':random_state,\n#         'lambda_l1':trial.suggest_float('lambda_l1', 0.0, 5.0)\n#     }\n    \n#     # model / evaluate\n#     lgbm_gmm_class = LGBMClassifier(**param) # model\n#     oof, preds = _evaluate(lgbm_gmm_class, x_cols, df_gmm_train, target='target_gmm') # evaluate\n    \n#     return accuracy_score(df_gmm_train['target_gmm'], oof)\n\n# # run study\n# study = optuna.create_study(direction='maximize',sampler=TPESampler())\n# study.optimize(objective, n_trials=30) # scaled down for public (i maxed out at 0.6)\n\n# # output study\n# print (study.best_value)\n# print (study.best_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To summarise the below:\n* Fit a classification tree that can predict which part of the Gmm it belongs to.\n* Fit 2 regression trees on each Gmm. (roughly about 0.38 mse).\n* Re fit the Lgbm and Catboost models on the new training set.\n* Perform predictions on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"# optuna hyper params\nparam_lgbm_gmm_class = {'num_leaves': 144, 'max_depth': 10, 'learning_rate': 0.0934759313797003, 'n_estimators': 107, 'min_child_weight': 0.30363944085393396, 'min_child_samples': 17, 'subsample': 0.9463879414095, 'subsample_freq': 80, 'lambda_l1': 4.67131782429971}\n\n# init classifiers / regressors (re-fit other ones on new partition)\nlgbm_gmm_class = LGBMClassifier(random_state=random_state, **param_lgbm_gmm_class)\nlgbm_gmm_reg_0 = LGBMRegressor(seed=random_state)\nlgbm_gmm_reg_1 = LGBMRegressor(seed=random_state)\nlgbm_gmm_reg = LGBMRegressor(seed=random_state, **param_lgbm)\nlgbm_cat_reg = CatBoostRegressor(iterations=1000, verbose=False, random_state=random_state, loss_function='RMSE', bootstrap_type='Bernoulli', **param_cat)\n\n# masks for each gmm bin\nmask_gmm_0 = df_gmm_train['target_gmm'] == 0\nmask_gmm_1 = df_gmm_train['target_gmm'] == 1\n\n# fit\nlgbm_gmm_class.fit(df_gmm_train[x_cols], df_gmm_train['target_gmm'])\nlgbm_gmm_reg_0.fit(df_gmm_train.loc[mask_gmm_0, x_cols].reset_index(drop=True), df_gmm_train.loc[mask_gmm_0, target].reset_index(drop=True))\nlgbm_gmm_reg_1.fit(df_gmm_train.loc[mask_gmm_1, x_cols].reset_index(drop=True), df_gmm_train.loc[mask_gmm_1, target].reset_index(drop=True))\nlgbm_gmm_reg.fit(df_gmm_train[x_cols], df_gmm_train[target])\nlgbm_cat_reg.fit(df_gmm_train[x_cols], df_gmm_train[target])\n\n# class predictions (obtain probabilities rather than class)\ndf_gmm_test[['gmm_class_' + str(a) + '_pred' for a in lgbm_gmm_class.classes_]] = lgbm_gmm_class.predict_proba(df_gmm_test[x_cols])\n\n# regression predictions (on all data)\ndf_gmm_test['lgbm'] = lgbm_gmm_reg.predict(df_gmm_test[x_cols])\ndf_gmm_test['cat'] = lgbm_cat_reg.predict(df_gmm_test[x_cols])\ndf_gmm_test['hyper_blend'] = (df_gmm_test['lgbm'] * 0.5) + (df_gmm_test['cat'] * 0.5)\ndf_gmm_test['gmm_reg_0'] = lgbm_gmm_reg_0.predict(df_gmm_test[x_cols])\ndf_gmm_test['gmm_reg_1'] = lgbm_gmm_reg_1.predict(df_gmm_test[x_cols])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# plot\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 4))\nsns.kdeplot(df_gmm_test['lgbm'], ax=ax[0], label='lgbm')\nsns.kdeplot(df_gmm_test['cat'], ax=ax[0], label='cat boost')\nsns.kdeplot(df_gmm_test['hyper_blend'], ax=ax[0], label='blend')\nsns.kdeplot(df_gmm_test['target'], ax=ax[0], label='target')\nsns.kdeplot(df_gmm_test['gmm_reg_0'], ax=ax[1], label='regression 0')\nsns.kdeplot(df_gmm_test['gmm_reg_1'], ax=ax[1], label='regression 1')\nsns.kdeplot(df_gmm_test['target'], ax=ax[1], label='target')\n\n# decorate\nax[0].set_title('full regressors')\nax[1].set_title('gmm regressors')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making a prediction using the classifier and the 2 Gmm regressors doesnt actually yeild a good prediction (its easy to see why). <br>\nSo instead, can we gain a competative advantage using our blended model then use our Gmm classifier (whereby the prediction is 90%+) to use the 2 Gmm regressors, lets call this 'super target'..."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# create super target\ndef _super_target(x):\n    \n    # certain class\n    if x['gmm_class_0_pred'] >= 0.7: return x['gmm_reg_0']\n    if x['gmm_class_1_pred'] >= 0.7: return x['gmm_reg_1']\n    \n    return x['hyper_blend']\n\n# apply super target\ndf_gmm_test['super_target'] = df_gmm_test.apply(lambda x: _super_target(x), axis=1)\n\n# plot\nfig, ax = plt.subplots(figsize=(18, 4))\nsns.kdeplot(df_gmm_test[target])\nsns.kdeplot(df_gmm_test['super_target'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visually this looks promising, but the LB tells me otherwise..."},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('blend:', mean_squared_error(df_gmm_test[target], df_gmm_test['hyper_blend'], squared=False))\nprint ('super:', mean_squared_error(df_gmm_test[target], df_gmm_test['super_target'], squared=False))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}