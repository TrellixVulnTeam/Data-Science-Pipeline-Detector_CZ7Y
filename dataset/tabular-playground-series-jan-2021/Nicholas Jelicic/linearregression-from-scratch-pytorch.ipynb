{"cells":[{"metadata":{},"cell_type":"markdown","source":"# PyTorch Regularized Regression\n\nIn my previous [notebook](https://www.kaggle.com/njelicic/pure-numpy-ols) you learned how we can use just NumPy to fit a linear regression model by using linear algebra. \n\nIn this notebook I demonstrate how to fit a  regression model with 'base' PyTorch (so no torch.nn). Here, I use Gradient Descent to find the parameters for the model. \n\nThe LinearRegression class supports: OLS (penalty=None), LASSO (penalty='l1') and  Ridge (penalty='l2'). \n\nAgain, I try to follow the Sklearn API (model.fit() -> model.predict()) in my implementation. \n\n**Note: For learing purposes. If you want to score high in this comp I suggest stacking as much boosted trees as possible ðŸ˜…** "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport torch\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(df):\n    df.drop('id',inplace=True,axis=1) #drop id columns\n    \n    if 'target' in df.columns: #if training, store targets in y\n        y = df['target'].values\n        df.drop('target',inplace=True,axis=1)\n\n    else:\n        y = None\n    \n    X = df.values \n\n    \n    return X,y","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/test.csv')\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv')\n\n#sanity check\nassert np.all(test_df['id'].values ==  sub['id'].values)\n\nX_train, y_train = preprocess(train_df)\nX_test, y_test = preprocess(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The RegularizedRegression Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LinearRegression():\n    \"\"\"\"My custom regression class\"\"\"   \n    def __init__(self, C=0.1,lr=1e-3, penalty=None, n_iter=10000):\n        self.C = C\n        self.lr = lr\n        self.history = []\n        self.penalty = penalty \n        self.n_iter = n_iter\n        return\n    \n    def linreg(self, x):\n        \"\"\"\"Linear regression function\"\"\"  \n        return x @ self.w.t() + self.b            # matrix multiply inputs (x) with the transposed weights (w) and add the intercept (b)\n    \n    def loss(self, y, y_hat):\n        \"\"\"\"Calculate loss\"\"\"  \n        mse = torch.mean((y-y_hat)**2)            # mean squared error\n        \n        if self.penalty == 'l2':\n            penalty = self.C*torch.sum(self.w**2) # lambda multiplied by the sum of squared weights \n        \n        if self.penalty == 'l1':\n            penalty = self.C*torch.sum(torch.abs(self.w))    # lambda multiplied by the sum of weights \n        \n        if self.penalty == None:\n            penalty = 0 \n        \n        return  mse + penalty \n    \n    def cast_to_tensor(self, x):\n        return torch.tensor(x).float()\n    \n        \n    def fit(self,x,y):\n        \"\"\"\"Fit model\"\"\"  \n        x = self.cast_to_tensor(x)\n        y = self.cast_to_tensor(y)\n        \n        self.w = torch.randn(x.size()[1], requires_grad=True) #instantiate weights\n        self.b = torch.randn(1, requires_grad=True)           #instantiate bias\n        \n        for i in range(self.n_iter):\n            y_hat = self.linreg(x)    # make predictions\n            loss = self.loss(y,y_hat) # calculate loss function\n            loss.backward()           # backprop\n            \n            with torch.no_grad(): \n                self.w -= self.w.grad * self.lr #update weights\n                self.b -= self.b.grad * self.lr #update bias\n                self.w.grad.zero_()\n                self.b.grad.zero_()\n            \n            self.history.append(loss.item())\n            \n    def predict(self, x):\n        \"\"\"\"Predict\"\"\"  \n        x = self.cast_to_tensor(x)\n        return self.linreg(x).detach().numpy()\n    \n\n    \n    def plot_history(self):\n        \"\"\"\"Plot loss function over time\"\"\"  \n        return sns.lineplot(x=[i+1 for i in range(len(self.history))],y=self.history).set(xlabel='Iteration', ylabel='Loss',title='History')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LinearRegression(penalty='l1')\nclf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot loss over time"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.plot_history()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'] = clf.predict(X_test)\n\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}