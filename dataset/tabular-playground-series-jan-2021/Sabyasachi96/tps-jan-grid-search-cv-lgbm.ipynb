{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Another TPS, weird set, but better than TPS-August. My method is simple and straight forward, something that will help any beginner. \n\nP.S - Would appreciate constructive criticism","metadata":{}},{"cell_type":"markdown","source":"# Importing Data and Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfont1 = {'family':'serif','color':'brown','size':15}","metadata":{"execution":{"iopub.status.busy":"2021-08-25T06:52:28.226014Z","iopub.execute_input":"2021-08-25T06:52:28.226436Z","iopub.status.idle":"2021-08-25T06:52:30.800419Z","shell.execute_reply.started":"2021-08-25T06:52:28.226355Z","shell.execute_reply":"2021-08-25T06:52:30.799419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"../input/tabular-playground-series-jan-2021/train.csv","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')\nsample = pd.read_csv('../input/tabular-playground-series-jan-2021/sample_submission.csv')\n\n# ID is of no use to me \ntrain.drop(columns = 'id', inplace = True)\ntest.drop(columns = 'id', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T06:52:32.419531Z","iopub.execute_input":"2021-08-25T06:52:32.419899Z","iopub.status.idle":"2021-08-25T06:52:35.83671Z","shell.execute_reply.started":"2021-08-25T06:52:32.419868Z","shell.execute_reply":"2021-08-25T06:52:35.835562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train Shape', train.shape, '\\n', 'Test Shape', test.shape)\ntrain.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:31:43.142182Z","iopub.execute_input":"2021-08-24T17:31:43.142565Z","iopub.status.idle":"2021-08-24T17:31:43.377692Z","shell.execute_reply.started":"2021-08-24T17:31:43.142528Z","shell.execute_reply":"2021-08-24T17:31:43.37672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"# Simple exploration\nprint('Null in Train', train.isnull().sum().sum())\nprint('Null in Test', test.isnull().sum().sum())\nprint('Train Duplicate data', train.duplicated().sum())\nprint('Test Duplicate data', test.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:34:43.030239Z","iopub.execute_input":"2021-08-24T17:34:43.030635Z","iopub.status.idle":"2021-08-24T17:34:43.753092Z","shell.execute_reply.started":"2021-08-24T17:34:43.0306Z","shell.execute_reply":"2021-08-24T17:34:43.75204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Univarite analysis\n Box plot and Histogram","metadata":{}},{"cell_type":"code","source":"hist = train.hist(figsize = (25, 12), bins=50, grid = False, \n                  xlabelsize=8, ylabelsize=8, layout = (3,5))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:35:35.951926Z","iopub.execute_input":"2021-08-24T17:35:35.952292Z","iopub.status.idle":"2021-08-24T17:35:39.87829Z","shell.execute_reply.started":"2021-08-24T17:35:35.952261Z","shell.execute_reply":"2021-08-24T17:35:39.877334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box = train.boxplot(figsize = (18,8), rot = 20 )","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:35:55.732545Z","iopub.execute_input":"2021-08-24T17:35:55.732894Z","iopub.status.idle":"2021-08-24T17:36:04.241026Z","shell.execute_reply.started":"2021-08-24T17:35:55.732864Z","shell.execute_reply":"2021-08-24T17:36:04.240338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well I will plot this again without Target, as the scales are clearly different","metadata":{}},{"cell_type":"code","source":"box = train.drop(columns = ['target']).boxplot(figsize = (18,8), rot = 20 )","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:36:17.201343Z","iopub.execute_input":"2021-08-24T17:36:17.201976Z","iopub.status.idle":"2021-08-24T17:36:25.203137Z","shell.execute_reply.started":"2021-08-24T17:36:17.201932Z","shell.execute_reply":"2021-08-24T17:36:25.202388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference** : \n\nNot Many outliers. The Scale of the variables is pretty same, still will check if the model accuracy increases with scaling.","metadata":{}},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:39:30.289845Z","iopub.execute_input":"2021-08-24T17:39:30.290301Z","iopub.status.idle":"2021-08-24T17:39:30.297678Z","shell.execute_reply.started":"2021-08-24T17:39:30.290258Z","shell.execute_reply":"2021-08-24T17:39:30.296857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14', 'target']\nsns.pairplot(train[cols], size = 4)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:40:23.675667Z","iopub.execute_input":"2021-08-24T17:40:23.676019Z","iopub.status.idle":"2021-08-24T17:45:04.29635Z","shell.execute_reply.started":"2021-08-24T17:40:23.675987Z","shell.execute_reply":"2021-08-24T17:45:04.2955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18,12))\nplt.title(\"Correlation Plot\", fontdict = font1)\nsns.heatmap(train.corr(), annot=True, cmap=\"RdBu\", fmt='.2f', \n            center = 0, linewidths=0.1, cbar_kws={\"shrink\": .8}) ","metadata":{"execution":{"iopub.status.busy":"2021-08-24T17:45:04.29764Z","iopub.execute_input":"2021-08-24T17:45:04.298055Z","iopub.status.idle":"2021-08-24T17:45:06.118204Z","shell.execute_reply.started":"2021-08-24T17:45:04.298025Z","shell.execute_reply":"2021-08-24T17:45:06.116997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inference** :\n\nNot a bad correlation but the data points are all over the place. I'll perform PCA to reduce the feature number. Lets see how PCA will work out with my model(s).","metadata":{}},{"cell_type":"markdown","source":"# PCA","metadata":{}},{"cell_type":"markdown","source":"PCA is scale sensitive and thus need scaling before","metadata":{}},{"cell_type":"code","source":"# Splitting\nx = train.iloc[:,:-1]\ny = train.iloc[:,-1]\n\n\n# PCA is scale sensitive and thus need scaling before\n# Standard scaling\nfrom sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nx_train = pd.DataFrame(ss.fit_transform(x))","metadata":{"execution":{"iopub.status.busy":"2021-08-25T06:56:08.423951Z","iopub.execute_input":"2021-08-25T06:56:08.424598Z","iopub.status.idle":"2021-08-25T06:56:08.523333Z","shell.execute_reply.started":"2021-08-25T06:56:08.424562Z","shell.execute_reply":"2021-08-25T06:56:08.5224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA\nfrom sklearn.decomposition import PCA\npca = PCA()\npca_train = pca.fit_transform(x_train)\n\n# Converting PC Datasets for model training\npca_train = pd.DataFrame(pca_train)\n\nlist_name = [f'pc{i}' for i in range(14)]\n\npca_train.columns = list_name","metadata":{"execution":{"iopub.status.busy":"2021-08-25T06:56:10.376168Z","iopub.execute_input":"2021-08-25T06:56:10.37674Z","iopub.status.idle":"2021-08-25T06:56:10.717683Z","shell.execute_reply.started":"2021-08-25T06:56:10.376694Z","shell.execute_reply":"2021-08-25T06:56:10.71661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NO of PC's\nexplained_variance = pca.explained_variance_ratio_\n\nplt.plot(explained_variance)\nplt.grid()\nplt.xlabel('Number of components')\nplt.ylabel('Explained variance')\nplt.title(\"Scree Plot\");","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:09:25.003307Z","iopub.execute_input":"2021-08-24T18:09:25.0037Z","iopub.status.idle":"2021-08-24T18:09:25.218699Z","shell.execute_reply.started":"2021-08-24T18:09:25.003668Z","shell.execute_reply":"2021-08-24T18:09:25.217652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well the PCA looks promising and PC's 6-7 looks good, that can explain >80% of variance","metadata":{}},{"cell_type":"code","source":"hist = pca_train.hist(figsize = (25, 12), bins=50, \n                      grid = False, xlabelsize=8, ylabelsize=8, \n                      layout = (3,5))\nplt.suptitle('PCA-train set', fontsize = 20)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:10:08.748927Z","iopub.execute_input":"2021-08-24T18:10:08.749276Z","iopub.status.idle":"2021-08-24T18:10:12.350821Z","shell.execute_reply.started":"2021-08-24T18:10:08.749247Z","shell.execute_reply":"2021-08-24T18:10:12.349981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Not much of change in the data distribution, now becoming bit skeptic of PCA in model building.\n\nWhat I can do is to run a baseline model on PCA and Non PCA data and check accuracy with CV","metadata":{}},{"cell_type":"code","source":"# x = pca_train\n# y ","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:10:54.810127Z","iopub.execute_input":"2021-08-24T18:10:54.810514Z","iopub.status.idle":"2021-08-24T18:10:54.814249Z","shell.execute_reply.started":"2021-08-24T18:10:54.810472Z","shell.execute_reply":"2021-08-24T18:10:54.813323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline models - PCA vs Normal Dataset\n\nHere I am evaluating the modle type and the dataset to work on for the final model building. \n\nSince its a regression problem with not that good correlation a random forest vs LGBM makes sense for now. \n\nP.S - Any suggestions what other models can be applied, comment down please\n","metadata":{}},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"# prepare the cross-validation procedure\ncv = KFold(n_splits = 3, random_state = 2, shuffle=True)\n\n# Import the models we are using\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T01:07:16.409438Z","iopub.execute_input":"2021-08-25T01:07:16.409814Z","iopub.status.idle":"2021-08-25T01:07:16.629214Z","shell.execute_reply.started":"2021-08-25T01:07:16.409783Z","shell.execute_reply":"2021-08-25T01:07:16.628349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model for Random Forest with original Dataset\nscores = cross_val_score(model, x, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('Random Forest baseline score without PCA' , -scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:30:38.572646Z","iopub.execute_input":"2021-08-24T18:30:38.572995Z","iopub.status.idle":"2021-08-24T18:55:24.795618Z","shell.execute_reply.started":"2021-08-24T18:30:38.572965Z","shell.execute_reply":"2021-08-24T18:55:24.794575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model For Random Forest with PCA Dataset\nscores = cross_val_score(model, pca_train, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('Random Forest baseline score with PCA' , -scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-25T01:07:59.227683Z","iopub.execute_input":"2021-08-25T01:07:59.228079Z","iopub.status.idle":"2021-08-25T01:34:55.424234Z","shell.execute_reply.started":"2021-08-25T01:07:59.228046Z","shell.execute_reply":"2021-08-25T01:34:55.422778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM","metadata":{}},{"cell_type":"code","source":"x = train.iloc[:,:-1]\ny = train.iloc[:,-1]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T06:53:25.211623Z","iopub.execute_input":"2021-08-25T06:53:25.21199Z","iopub.status.idle":"2021-08-25T06:53:25.217747Z","shell.execute_reply.started":"2021-08-25T06:53:25.211956Z","shell.execute_reply":"2021-08-25T06:53:25.216454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare the cross-validation procedure\ncv = KFold(n_splits=3, random_state=2, shuffle=True)\n\n# Import the models we are using\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = lgb.LGBMRegressor()","metadata":{"execution":{"iopub.status.busy":"2021-08-25T06:55:34.018953Z","iopub.execute_input":"2021-08-25T06:55:34.019573Z","iopub.status.idle":"2021-08-25T06:55:34.024208Z","shell.execute_reply.started":"2021-08-25T06:55:34.019535Z","shell.execute_reply":"2021-08-25T06:55:34.023315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model\nscores = cross_val_score(model, x, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM baseline score without PCA' , -scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-25T06:55:37.719116Z","iopub.execute_input":"2021-08-25T06:55:37.719765Z","iopub.status.idle":"2021-08-25T06:55:42.491355Z","shell.execute_reply.started":"2021-08-25T06:55:37.719715Z","shell.execute_reply":"2021-08-25T06:55:42.490484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate model\nscores = cross_val_score(model, pca_train, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM baseline score with PCA' , -scores.mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-25T06:57:19.5847Z","iopub.execute_input":"2021-08-25T06:57:19.585127Z","iopub.status.idle":"2021-08-25T06:57:23.776596Z","shell.execute_reply.started":"2021-08-25T06:57:19.585095Z","shell.execute_reply":"2021-08-25T06:57:23.775713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Base Summary**\n\n* Error - Random forest with PCA -> 0.6005048129803532\n* Error - Random forest without PCA -> 0.5926412679654879\n\n* Error - LGBM with PCA -> 0.6022344194486661\n* Error - LGBM with PCA -> 0.5900531147333651\n\nWell few things that are clear that, modelling with PCA was futile, since the error in the base model was higher in case of PCA.\n\nAlso I am finalising LGBM over Random forest, since the runtime was too much for random forest compared to LGBM. Next step will be hypertuning the LGBM Parameters.\n","metadata":{}},{"cell_type":"markdown","source":"# Final Model","metadata":{}},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"# Parameters\n\nn_estimators = [100, 300, 500, 700, 900]\nnum_leaves = [30, 50, 80]\nrandom_state = [2, 5]\n# max_depth = [-1, 30, 50, 100]\n# learning_rate = [0.1, 0.05, 0.01]","metadata":{"execution":{"iopub.status.busy":"2021-08-25T07:17:44.752165Z","iopub.execute_input":"2021-08-25T07:17:44.75259Z","iopub.status.idle":"2021-08-25T07:17:44.757457Z","shell.execute_reply.started":"2021-08-25T07:17:44.75256Z","shell.execute_reply":"2021-08-25T07:17:44.756361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*I have commented out some parameters, since the run time was getting too high exceeding the RAM capacity. \nIf anyone has a suggestion for a better way to compute multiple parameters without running out of RAM, please do comment out.*","metadata":{}},{"cell_type":"code","source":"# Dictionary of parameters\nparams = {'n_estimators': n_estimators,\n          'num_leaves': num_leaves,\n          'random_state': random_state\n          } ","metadata":{"execution":{"iopub.status.busy":"2021-08-25T07:17:47.182338Z","iopub.execute_input":"2021-08-25T07:17:47.182691Z","iopub.status.idle":"2021-08-25T07:17:47.188301Z","shell.execute_reply.started":"2021-08-25T07:17:47.182661Z","shell.execute_reply":"2021-08-25T07:17:47.186911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create model\nmodel = lgb.LGBMRegressor()\n\nr_search = RandomizedSearchCV(estimator = model, param_distributions = params, \n                              cv = 3, scoring ='neg_mean_absolute_error', \n                              n_iter = 10, verbose=1, return_train_score=True)\n\nr_search.fit(x,y)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T07:17:48.130974Z","iopub.execute_input":"2021-08-25T07:17:48.131358Z","iopub.status.idle":"2021-08-25T07:21:44.344995Z","shell.execute_reply.started":"2021-08-25T07:17:48.131327Z","shell.execute_reply":"2021-08-25T07:21:44.344013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Best parameters', r_search.best_params_)\nprint('Best score', -r_search.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T07:21:44.34752Z","iopub.execute_input":"2021-08-25T07:21:44.347981Z","iopub.status.idle":"2021-08-25T07:21:44.354052Z","shell.execute_reply.started":"2021-08-25T07:21:44.347934Z","shell.execute_reply":"2021-08-25T07:21:44.35293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcv_results = pd.DataFrame(r_search.cv_results_)\nrcv_results","metadata":{"execution":{"iopub.status.busy":"2021-08-25T07:21:44.35593Z","iopub.execute_input":"2021-08-25T07:21:44.356266Z","iopub.status.idle":"2021-08-25T07:21:44.399782Z","shell.execute_reply.started":"2021-08-25T07:21:44.356233Z","shell.execute_reply":"2021-08-25T07:21:44.398576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Final Showdown","metadata":{}},{"cell_type":"code","source":"model = lgb.LGBMRegressor(n_estimator = 900, num_leaves = 30,  )\nmodel.fit(x, y)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T07:25:19.870867Z","iopub.execute_input":"2021-08-25T07:25:19.871271Z","iopub.status.idle":"2021-08-25T07:25:21.733382Z","shell.execute_reply.started":"2021-08-25T07:25:19.871221Z","shell.execute_reply":"2021-08-25T07:25:21.732315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample['target'] = model.predict(test)\nsample.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-25T07:26:02.648623Z","iopub.execute_input":"2021-08-25T07:26:02.648978Z","iopub.status.idle":"2021-08-25T07:26:03.966914Z","shell.execute_reply.started":"2021-08-25T07:26:02.648949Z","shell.execute_reply":"2021-08-25T07:26:03.96592Z"},"trusted":true},"execution_count":null,"outputs":[]}]}