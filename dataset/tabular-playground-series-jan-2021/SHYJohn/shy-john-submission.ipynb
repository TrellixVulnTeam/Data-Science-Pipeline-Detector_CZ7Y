{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nfrom sklearn.linear_model import Ridge, Lasso, LinearRegression\nimport lightgbm as lgb\nfrom sklearn.ensemble import AdaBoostRegressor\nimport catboost as cgb\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom sklearn.ensemble import StackingRegressor\n\nfrom sklearn.metrics import mean_absolute_error\n\nfrom matplotlib import pyplot as plt \n%matplotlib inline\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\ntrain_df = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/test.csv') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(train_df.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.linalg.det(train_df.corr())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_cols = [x for x in train_df.columns.tolist() if 'cont' in x]\nplt.figure(figsize=(20,10))\nsubplot_count = 1\nfor i in range(7): \n    for j in range(2): \n        plt.subplot(2, 7, subplot_count)\n        train_df[X_cols[subplot_count-1]].plot.box()\n        subplot_count += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(test_df.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.linalg.det(test_df.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Processing for Lasso and Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code from https://www.kaggle.com/tosinabase/jan-21-regularized-regression-ridge-and-lasso\n\ny = train_df['target']\nX_lr = train_df.drop(['id', 'target'], axis=1)\n\nscaler = StandardScaler()\nscaler.fit(X_lr)\n\nX_lr = scaler.transform(X_lr)\nX_lr_test = scaler.transform(test_df.drop('id', axis=1).values)\n\nX_train_lr, X_val_lr, y_train_lr, y_val_lr = train_test_split(X_lr, y, test_size=0.3, random_state=17, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['target']\nX = train_df.drop(['target'], axis=1)\n\nX_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X, y, test_size =0.3, shuffle=False)\n\ndel train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"m1 = Lasso(alpha=0.001, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m1_fit = m1.fit(X_train_lr, y_train_lr)\nprint('Score reached: {} '.format(m1.score(X_train_lr, y_train_lr)))\n\n# Score without scaling: 0.0176265428750495 \n# Score with scaling: 0.01859676573900082 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_lr = test_df.drop(['id'], axis=1)\ny_test_lasso = m1.predict(X_test_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.bar(height=m1_fit.coef_, x=X.columns.values[1:])\nplt.title(\"Feature importances via coefficients\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter from https://www.kaggle.com/tosinabase/jan-21-regularized-regression-ridge-and-lasso\nm2 = Ridge(alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m2_fit = m2.fit(X_train_lr, y_train_lr)\nprint('Score reached: {} '.format(m2.score(X_train_lr, y_train_lr)))\n# Score 0.01865954277402282 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_lr = test_df.drop(['id'], axis=1)\ny_test_ridge = m2.predict(X_test_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.bar(height=m2_fit.coef_, x=X.columns.values[1:])\nplt.title(\"Feature importances via coefficients\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train_df, y_train_df, free_raw_data=False)\nlgb_eval = lgb.Dataset(X_val_df, y_val_df, free_raw_data=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # param values c.f. https://www.kaggle.com/zephyrwang666/riiid-lgbm-bagging2\n# param = {'num_leaves': sp_randint(10, 500), 'n_estimators': sp_randint(10, 6000), 'max_bin':sp_randint(100, 800), 'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4], \n#          'feature_fraction': sp_uniform(0, 1), 'bagging_fraction': sp_uniform(0, 1), \"bagging_seed\": [47], \n#          'objective': ['regression'], 'max_depth': [-1], \n#          'learning_rate': sp_uniform(0, 1), \"boosting_type\": [\"gbdt\"], \n#          'metric': ['rmse'], \"verbosity\": [-1], \n#          'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100], 'reg_lambda': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100], \n#          'random_state': [47]}\n\n# m3 = lgb.LGBMRegressor(verbose_eval = 30, num_boost_round = 10000, early_stopping_rounds = 10, n_estimators=3000)\n\n# '''\n# Hyperparameter optimisation\n# '''\n# # Code from https://www.kaggle.com/rtatman/lightgbm-hyperparameter-optimisation-lb-0-761#Model-fitting-with-HyperParameter-optimisation\n# #This parameter defines the number of hyperparameter points to be tested\n# n_HP_points_to_test = 150\n\n# gsLGBM = RandomizedSearchCV(\n#     estimator=m3, param_distributions=param, \n#     n_iter=n_HP_points_to_test,\n#     cv=5,\n#     refit=True,\n#     random_state=47,\n#     verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gsLGBM.fit(X_train_df, y_train_df, eval_set = (X_val_df, y_val_df), eval_metric = 'rmse')\n# print('Best score reached: {} with params: {} '.format(gsLGBM.best_score_, gsLGBM.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsLGBM.best_params_\n# Best score: 0.0937872506205801\n# opt_parameters_LGBM = {'bagging_fraction': 0.7997942505658034,\n#  'bagging_seed': 47, 'boosting_type': 'gbdt',\n#  'feature_fraction': 0.31477581669804067, 'learning_rate': 0.03875307567633712,\n#  'max_bin': 491, 'max_depth': -1,\n#  'metric': 'rmse', 'min_child_weight': 100.0,\n#  'n_estimators': 2559, 'num_leaves': 272,\n#  'objective': 'regression',\n#  'random_state': 47, 'reg_alpha': 10,\n#  'reg_lambda': 1, 'verbosity': -1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m3 = lgb.LGBMRegressor(valid_sets = [lgb_train, lgb_eval], verbose_eval = 30, num_boost_round = 10000, early_stopping_rounds = 10, n_jobs=4, **opt_parameters_LGBM)\nm3.fit(X_train_df, y_train_df, eval_set = (X_val_df, y_val_df), eval_metric = 'rmse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_df\ny_test_lgbm = m3.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nlightgbm.plot_importance(m3)\nplt.title(\"Feature importances\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ADABoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# m4 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=3, min_samples_leaf=1, min_impurity_decrease=10, random_state=47), random_state=47)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# param = {'learning_rate': sp_uniform(0, 1), 'n_estimators': sp_randint(5, 100)}\n\n# '''\n# Hyperparameter optimisation\n# '''\n# # Code from https://www.kaggle.com/rtatman/lightgbm-hyperparameter-optimisation-lb-0-761#Model-fitting-with-HyperParameter-optimisation\n# # This parameter defines the number of HP points to be tested\n# n_HP_points_to_test = 50\n\n# gsADA = RandomizedSearchCV(\n#     estimator=m4, param_distributions=param, \n#     n_iter=n_HP_points_to_test,\n#     cv=3,\n#     refit=True,\n#     random_state=47,\n#     verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gsADA.fit(X_train_df, y_train_df)\n# print('Best score reached: {} with params: {} '.format(gsADA.best_score_, gsADA.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Just in case, the parameters should be printed in here. \n# Score: -3.3217493390580444e-05\nopt_parameters_ADA = {'learning_rate': 0.028555288989857153, 'n_estimators': 36} ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m4 = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=3, min_samples_leaf=1, min_impurity_decrease=10, random_state=47), random_state=47, **opt_parameters_ADA)\nm4.fit(X_train_df, y_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_df\ny_test_ada = m4.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"m5 = CatBoostRegressor(random_seed=47)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param = {'learning_rate': sp_uniform(0, 1), 'n_estimators': sp_randint(5, 100), eta=sp_uniform(0, 1), num_trees=sp_randint(5, 100)}\n\n'''\nHyperparameter optimisation\n'''\n# Code from https://www.kaggle.com/rtatman/lightgbm-hyperparameter-optimisation-lb-0-761#Model-fitting-with-HyperParameter-optimisation\n# This parameter defines the number of HP points to be tested\nn_HP_points_to_test = 50\n\ngsCB = RandomizedSearchCV(\n    estimator=m5, param_distributions=param, \n    n_iter=n_HP_points_to_test,\n    cv=3,\n    refit=True,\n    random_state=47,\n    verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gsCB.fit(X_train_df, y_train_df)\nprint('Best score reached: {} with params: {} '.format(gsCB.best_score_, gsCB.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_df\ny_test_cb = m5.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensembling the Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code from https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python#Second-Level-Predictions-from-the-First-level-Output\ngbm = xgb.XGBRegressor(\n learning_rate = 0.01,\n n_estimators= 100,\n max_depth= 4,\n min_child_weight= 2,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'reg:squaredlogerror',\n nthread= -1,\n verbosity=3,\n random_state=20)\n\n# Code from https://stackoverflow.com/questions/65713104/sklearn-asking-for-eval-dataset-when-there-is-one/65714374?noredirect=1#comment116194594_65714374\nlgbm_params = m1.get_params()\n\n# remove early_stopping_rounds as your model is already fitted the data\nlgbm_params[\"early_stopping_rounds\"] = None\nm1.set_params(**lgbm_params)\n\nestimators = [('lgbm', m1), ('ada', m2), ('lasso', m3), ('ridge', m4)]\n\ngbm = StackingRegressor(estimators=estimators, final_estimator=gbm, cv=5, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del m1\n# del m2\n# del gsLGBM\n# del gsADA\n# del opt_parameters_LGBM\n# del opt_parameters_ADA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm.fit(X_train_df, y_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gbm.score(X_train_df, y_train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_df\ny_test_gbm = gbm.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_submission = pd.DataFrame({'id': test_df['id'], 'target': y_test_lasso})\nlasso_submission.to_csv('lasso_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_submission = pd.DataFrame({'id': test_df['id'], 'target': y_test_ridge})\nridge_submission.to_csv('ridge_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_submission = pd.DataFrame({'id': test_df['id'], 'target': y_test_lgbm})\nlgbm_submission.to_csv('lgbm_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_submission = pd.DataFrame({'id': test_df['id'], 'target': y_test_ada})\nada_submission.to_csv('ada_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm_submission = pd.DataFrame({'id': test_df['id'], 'target': y_test_gbm})\ngbm_submission.to_csv('gbm_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}