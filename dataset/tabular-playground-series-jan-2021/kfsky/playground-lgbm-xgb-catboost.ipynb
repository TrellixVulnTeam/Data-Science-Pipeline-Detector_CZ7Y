{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib_venn import venn2\n\nimport shap\nfrom optuna.integration import _lightgbm_tuner as lgb_tuner\nimport optuna\n\nfrom catboost import CatBoost\nfrom catboost import Pool\nfrom catboost import cv\nimport category_encoders as ce\nfrom tqdm import tqdm\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport joblib\n\nimport os\nimport logging\nimport datetime\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.info())\nprint(\"-\"*30)\nprint(df_test.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,20))\n\nfor i in range(1,15):\n    plt.subplot(4,4,i)\n    sns.distplot(df_train[\"cont\"+str(i)], label='train')\n    sns.distplot(df_test[\"cont\"+str(i)], label='test')\n    plt.legend()\n    plt.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colname = list()\n\nfig = plt.figure(figsize=(20,20))\n\nfor i in range(1,15):\n    colname.append('cont'+str(i))\n\ndf_train[colname].plot(figsize=(10,5), kind='box', rot=90, fontsize=12, title='train', grid='gainsboro')\ndf_test[colname].plot(figsize=(10,5), kind='box', rot=90, fontsize=12, title='test', grid='gainsboro')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 10))\nax.set_title(\"Correlation Matrix\", fontsize=16)\nsns.heatmap(df_train[df_train.columns[df_train.columns != 'id']].corr(), vmin=-1, vmax=1, cmap='jet', annot=True)\n\nfor tick in ax.xaxis.get_major_ticks():\n    tick.label.set_fontsize(14) \n    tick.label.set_rotation(90) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\n    tick.label.set_rotation(0) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_train[\"target\"]\nX = df_train.drop([\"target\",\"id\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM_Optuna"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\nfrom contextlib import contextmanager\nfrom time import time\n\nfold_num = 10\nEARLY_STOPPING_ROUNDS = 10\nVERBOSE_EVAL = 10000\nLGB_ROUND_NUM = 10000\nobjective = 'regression'\nmetric = 'rmse'\n\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': objective,\n    'metric': metric,\n    'verbosity': -1,\n    \"seed\": 42,\n}\n\n\n@contextmanager\ndef timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n    if prefix: format_str = str(prefix) + format_str\n    if suffix: format_str = format_str + str(suffix)\n    start = time()\n    yield\n    d = time() - start\n    out_str = format_str.format(d)\n    if logger:\n        logger.info(out_str)\n    else:\n        print(out_str)\n\ndef fit_lgbm(X, y, cv, params: dict=None, verbose: int=50):\n    metric_func = mean_squared_error\n\n    if params is None:\n        params = {}\n\n    models = []\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n        \n        lgb_train = lgb.Dataset(x_train, y_train)\n        lgb_valid = lgb.Dataset(x_valid, y_valid)\n\n        with timer(prefix='fit fold={} '.format(i + 1)):\n            lgb_model = lgb_tuner.train(params, lgb_train,\n                          num_boost_round=LGB_ROUND_NUM,\n                          valid_names=[\"train\", \"valid\"],\n                          valid_sets=[lgb_train, lgb_valid],\n                          early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                          verbose_eval=VERBOSE_EVAL)\n\n        pred_i = lgb_model.predict(x_valid)\n\n        oof_pred[idx_valid] = pred_i\n        models.append(lgb_model)\n\n        print(f'Fold {i} RMSE: {metric_func(y_valid, pred_i) ** .5:.4f}')\n\n    score = metric_func(y, oof_pred) ** .5\n    print('FINISHED | Whole RMSE: {:.4f}'.format(score))\n    return oof_pred, models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, GroupKFold\n\n\n\nfold = KFold(n_splits=5, shuffle=True, random_state=71)\ncv = list(fold.split(X, y)) # もともとが generator なため明示的に list に変換する\n\noof, models = fit_lgbm(X.values, y, cv, params=params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_importance(models, feat_train_df):\n    \"\"\"\n    args:\n        models:\n            List of lightGBM models\n        feat_train_df:\n            DataFrame\n    \"\"\"\n    feature_importance_df = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.feature_importance()\n        _df['column'] = feat_train_df.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column')\\\n        .sum()[['feature_importance']]\\\n        .sort_values('feature_importance', ascending=False).index[:50]\n\n    fig, ax = plt.subplots(figsize=(max(6, len(order) * .4), 7))\n    sns.boxenplot(data=feature_importance_df, x='column', y='feature_importance', order=order, ax=ax, palette='viridis')\n    ax.tick_params(axis='x', rotation=90)\n    ax.grid()\n    fig.tight_layout()\n    return fig, ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = visualize_importance(models, X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n\ndef opt(trial):\n    # params\n    n_estimators = trial.suggest_int('n_estimators', 0, 1000)\n    max_depth = trial.suggest_int('max_depth', 1, 20)\n    learning_rate = trial.suggest_discrete_uniform('learning_rate', 0.01,0.1,0.01)\n    min_child_weight = trial.suggest_int('min_child_weight', 1, 20)\n    subsample = trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.1)\n    colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree', 0.5, 0.9, 0.1)\n    # XGBostのインスタンス生成\n    xgboost_tuna = xgb.XGBRegressor(\n        random_state=42,\n        n_estimators = n_estimators,\n        max_depth = max_depth,\n        min_child_weight = min_child_weight,\n        subsample = subsample,\n        colsample_bytree = colsample_bytree,\n    )\n    # train\n    xgboost_tuna.fit(X_train,y_train)\n    # pred\n    tuna_pred_test = xgboost_tuna.predict(X_test)\n    # RMSE\n    return mean_squared_error(y_test, tuna_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#study = optuna.create_study()\n#study.optimize(opt, n_trials=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(study.best_trial)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_xgb(X, y, cv, params: dict=None, verbose: int=50):\n    metric_func = mean_squared_error\n    if params is None:\n        params = {}\n\n    models = []\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n        \n        model_xgb = xgb.XGBRegressor(**params)\n\n        with timer(prefix='fit fold={} '.format(i + 1)):\n            model_xgb.fit(x_train, y_train, eval_set=[(x_valid, y_valid)],verbose=-1)\n            \n        #print(model_xgb.best_score())\n        \n        pred_i = model_xgb.predict(x_valid)\n\n        oof_pred[idx_valid] = pred_i\n        models.append(model_xgb)\n\n        print(f'Fold {i} RMSE: {metric_func(y_valid, pred_i) ** .5:.4f}')\n\n    score = metric_func(y, oof_pred) ** .5\n    print('FINISHED | Whole RMSE: {:.4f}'.format(score))\n    return oof_pred, models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nFrozenTrial(number=45, value=0.4896166448243632, datetime_start=datetime.datetime(2021, 1, 12, 15, 44, 13, 759937),\ndatetime_complete=datetime.datetime(2021, 1, 12, 15, 44, 48, 234726), params={'n_estimators': 208, 'max_depth': 4,\n'min_child_weight': 13, 'subsample': 0.8, 'colsample_bytree': 0.8}, \ndistributions={'n_estimators': IntUniformDistribution(high=1000, low=0, step=1),\n'max_depth': IntUniformDistribution(high=20, low=1, step=1), 'min_child_weight':\nIntUniformDistribution(high=20, low=1, step=1),\n'subsample': DiscreteUniformDistribution(high=0.9, low=0.5, q=0.1), \n'colsample_bytree': DiscreteUniformDistribution(high=0.9, low=0.5, q=0.1)},\nuser_attrs={}, system_attrs={}, intermediate_values={}, trial_id=45, state=TrialState.COMPLETE)\n\"\"\"\n\nparams_xgb = {'n_estimators': 208, \n              'max_depth': 4, \n              'learning_rate':0.08,\n              'min_child_weight': 13,\n              'subsample': 0.8, \n              'colsample_bytree': 0.8}\n\noof_xgb, models_xgb = fit_xgb(X.values, y, cv, params=params_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# optuna\n\ndef opt_cb(trial):\n    # params\n    params = {\n        'iterations' : trial.suggest_int('iterations', 50, 300),                         \n        'depth' : trial.suggest_int('depth', 4, 10),                                       \n        'learning_rate' : trial.suggest_loguniform('learning_rate', 0.01, 0.3),               \n        'random_strength' :trial.suggest_int('random_strength', 0, 100),                       \n        'bagging_temperature' :trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), \n        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n        'od_wait' :trial.suggest_int('od_wait', 10, 50)\n    }\n    \n    train_pool = Pool(X_train, y_train)\n    test_pool = Pool(X_test, y_test)\n    \n    catboost_tuna = CatBoostRegressor(**params)\n    # train\n    catboost_tuna.fit(train_pool)\n    # pred\n    tuna_pred_test = catboost_tuna.predict(test_pool)\n    pred_labels = np.rint(tuna_pred_test)\n    # RMSE\n    return mean_squared_error(y_test, pred_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#study2 = optuna.create_study()\n#study2.optimize(opt_cb, n_trials=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(study.best_trial)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_cb(X, y, cv, params: dict=None, verbose: int=50):\n    metric_func = mean_squared_error\n    if params is None:\n        params = {}\n\n    models = []\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n        \n        train_pool = Pool(x_train, label = y_train)\n        valid_pool = Pool(x_valid, label = y_valid)\n        \n        model_cb = CatBoost(params)\n\n        with timer(prefix='fit fold={} '.format(i + 1)):\n            model_cb.fit(train_pool,\n              # valid_data\n              eval_set = valid_pool,\n              use_best_model = True,\n              silent = True,\n              plot = False)\n            \n        print(model_cb.get_best_score())\n        \n        pred_i = model_cb.predict(x_valid)\n\n        oof_pred[idx_valid] = pred_i\n        models.append(model_cb)\n\n        print(f'Fold {i} RMSE: {metric_func(y_valid, pred_i) ** .5:.4f}')\n\n    score = metric_func(y, oof_pred) ** .5\n    print('FINISHED | Whole RMSE: {:.4f}'.format(score))\n    return oof_pred, models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_cb = {\n    'loss_function': 'RMSE',\n    'max_depth': 3, \n    'learning_rate': 0.08, \n    'subsample': 0.8, \n    #'colsample_bytree': 0.7,\n    'num_boost_round': 1000,\n    'early_stopping_rounds': 100,\n}\n\noof_cb, models_cb = fit_cb(X.values, y, cv, params=params_cb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = df_test.drop(\"id\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgb = np.array([model.predict(df_test.values) for model in models])\npred_lgb = np.mean(pred_lgb, axis=0)\npred_lgb = np.where(pred_lgb < 0, 0, pred_lgb)\n\npred_xgb = np.array([model.predict(df_test.values) for model in models_xgb])\npred_xgb = np.mean(pred_xgb, axis=0)\npred_xgb = np.where(pred_xgb < 0, 0, pred_xgb)\n\npred_cb = np.array([model.predict(df_test.values) for model in models_cb])\npred_cb = np.mean(pred_cb, axis=0)\npred_cb = np.where(pred_cb < 0, 0, pred_cb)\n\ntmp_sub = pd.DataFrame({\"lgb\":pred_lgb,\n                        \"xgb\":pred_xgb,\n                         \"cb\":pred_cb})\ntmp_sub[\"pred\"] = tmp_sub.mean(axis=\"columns\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[\"target\"] = tmp_sub[\"pred\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_df = pd.DataFrame({\"lgb\":oof,\n                        \"xgb\":oof_xgb,\n                         \"cb\":oof_cb})\noof_df[\"pred\"] = oof_df.mean(axis=\"columns\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8, 8))\nsns.distplot(oof_df[\"pred\"], label='Test Predict')\nsns.distplot(submission[\"target\"], label='Out Of Fold')\nax.legend()\nax.grid()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}