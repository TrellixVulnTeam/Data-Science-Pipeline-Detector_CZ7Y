{"cells":[{"metadata":{},"cell_type":"markdown","source":"The dataset contains 14 continuous features and a continuous target from 300000 samples. The target has a bimodal distribution. In this notebook, I will show how to obtain stratified cross validation splits from the continuous targets."},{"metadata":{},"cell_type":"markdown","source":"### Setup"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom scipy.stats import ks_2samp\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read the data\ndf = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stratifying Continuous Target\n\nThe solution is to split the continous target distribution into N bins, and use these bins as classification targets in the standard StratifiedKFold cross-validator of scikit-learn. The binning can be easily done with the `pd.cut` in pandas. The python function to do the splitting is given below."},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_folds(df, n_s=5, n_grp=None):\n    df['Fold'] = -1\n    \n    if n_grp is None:\n        skf = KFold(n_splits=n_s)\n        target = df.target\n    else:\n        skf = StratifiedKFold(n_splits=n_s)\n        df['grp'] = pd.cut(df.target, n_grp, labels=False)\n        target = df.grp\n    \n    for fold_no, (t, v) in enumerate(skf.split(target, target)):\n        df.loc[v, 'Fold'] = fold_no\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train set has the following continuous target distribution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['target'], bins=100, density=True)\nplt.xlabel('Target')\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's split the train set into 5 folds with stratification and visualize the target distribution in each fold."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = create_folds(df, n_s=5, n_grp=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 5, sharex=True, sharey=True, figsize=(10,4))\nfor i, ax in enumerate(axs):\n    ax.hist(df[df.Fold == i]['target'], bins=100, density=True, label=f'Fold-{i}')\n    if i == 0:\n        ax.set_ylabel('Frequency')\n    if i == 2:\n        ax.set_xlabel(\"Target\")\n    ax.legend(frameon=False, handlelength=0)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can compare any two folds with the **Kolmogorov-Smirnov** test to examine if the folds come from the same distribution. Let's compare all folds with the 1st fold for simplicity. The test results are given below. Indeed, the low KS (~0.0008) and high probability (1.0) values confirm that all folds come from the same distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold in np.sort(df.Fold.unique())[1:]:\n    print(f'Fold 0 vs {fold}:', ks_2samp(df.loc[df.Fold==0,'target'], df.loc[df.Fold==fold,'target']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What would be the target distribution in each fold without stratification?\n\nTo answer this let's split the train data into 5 folds again but this time without stratification. Note that setting n_grp=None will assign the folds without stratification.\n\nThe figure below shows the distributions in each folds without stratification strategy. Note that the distributions generally looks alike, but the fine structures at the peaks are quite different."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = create_folds(df, n_s=5, n_grp=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 5, sharex=True, sharey=True, figsize=(10,4))\nfor i, ax in enumerate(axs):\n    ax.hist(df[df.Fold == i]['target'], bins=100, density=True, label=f'Fold-{i}')\n    if i == 0:\n        ax.set_ylabel('Frequency')\n    if i == 2:\n        ax.set_xlabel(\"Target\")\n    ax.legend(frameon=False, handlelength=0)\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To quantify the differences in the folds, let's run the KS test again. The test results are as anticipated - the KS statistic values are low for all folds but probability values are not small enough to reject the null hypothesis that the all folds come from the same distribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold in np.sort(df.Fold.unique())[1:]:\n    print(f'Fold 0 vs {fold}:', ks_2samp(df.loc[df.Fold==0,'target'], df.loc[df.Fold==fold,'target']))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}