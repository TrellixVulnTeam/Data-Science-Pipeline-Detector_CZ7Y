{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hello everyone!\n\n**This notebook presents a straightforward code to tune hyperparameter of LGBM, CAT, and XGB with Bayesian Optimization. It is like GridSearchCV and RandomizedSearchCV.**\n\nGridSearchCV searches for all combinations of parameters, and it could take a very long time. Not very efficient. RandomizedSearchCV searches the combination randomly. Somehow the algorithm can skip the optimal parameter, especially if the search grid is enormous. Bayesian Optimization is a smarter method to tune the hyperparameter. I won't discuss the theory behind it in this notebook as it is straightforward.\n\nIf you have any questions regarding the code, please comment below. I will update the notebook accordingly.\n\n**Please do upvote the notebook if this notebook helps you as it will be a benchmark for me to do more work in the future. Thank you :)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mlxtend.regressor import StackingCVRegressor\nfrom sklearn.datasets import load_boston\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.decomposition import PCA\n\nfrom bayes_opt import BayesianOptimization\nimport warnings\nwarnings.filterwarnings('ignore')\n\nRANDOM_SEED = 123","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/test.csv\")\nsample = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['magic1'] = train['cont10']/train['cont11']\ntrain['magic2'] = train['cont11']/train['cont10']\ntrain['magic3'] = train['cont1']/train['cont7']\ntrain['magic4'] = train['cont7']/train['cont1']\ntrain['magic5'] = train['cont4']/train['cont6']\n\ntest['magic1'] = test['cont10']/test['cont11']\ntest['magic2'] = test['cont11']/test['cont10']\ntest['magic3'] = test['cont1']/test['cont7']\ntest['magic4'] = test['cont7']/test['cont1']\ntest['magic5'] = test['cont4']/test['cont6']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\nX = train.drop('target', axis=1)\ny = train.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\ncat = CatBoostRegressor(iterations=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = [cat]\nfor mod in model:\n    score = cross_val_score(mod, X, y, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    print(\"CAT RMSE Mean Score: \", np.mean(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = [cat]\nfor mod in model:\n    score = cross_val_score(mod, X, y, cv=10, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    print(\"CAT RMSE Mean Score: \", np.mean(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm\nlgbm = lightgbm.LGBMRegressor(random_state=RANDOM_SEED, n_jobs=-1, metric= 'rmse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = [lgbm]\nfor mod in model:\n    score = cross_val_score(mod, X, y, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    print(\"LGBM RMSE Mean Score: \", np.mean(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = [lgbm]\nfor mod in model:\n    score = cross_val_score(mod, X, y, cv=10, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    print(\"LGBM RMSE Mean Score: \", np.mean(score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nxgbr = XGBRegressor(random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = [xgbr]\nfor mod in model:\n    score = cross_val_score(mod, X, y, cv=3, scoring=\"neg_root_mean_squared_error\", n_jobs=-1)\n    print(\"XGB RMSE Mean Score: \", np.mean(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will use Bayesian Optimization to tune the hyperparameter. Our goal is to minimize RMSE, but Bayesian Optimization here only support maximizing, so that's why we add a minus sign in the RMSE, so maximizing the minus RMSE is equal to minimizing the RMSE. Just a matter of sign.\n\nYou can also adjust what parameter you want to tune and the range of hyperparameter. You can also how many point or how many try during the optimization. "},{"metadata":{},"cell_type":"markdown","source":"## LGBM Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain = lightgbm.Dataset(data=X, label=y)\n\ndef hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, max_depth, min_split_gain, min_child_weight, learning_rate):\n      \n        params = {'application':'regression','num_iterations': 5000,\n                  'early_stopping_round':100, 'metric':'rmse'}\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        params['learning_rate'] = learning_rate\n        cv_result = lightgbm.cv(params, dtrain, nfold=3, \n                                seed=RANDOM_SEED, stratified=False, \n                                verbose_eval =None, metrics=['rmse'])\n        \n        return -np.min(cv_result['rmse-mean'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pds = {\n    'num_leaves': (5, 50),\n    'feature_fraction': (0.2, 1),\n    'bagging_fraction': (0.2, 1),\n    'max_depth': (2, 20),\n    'min_split_gain': (0.001, 0.1),\n    'min_child_weight': (10, 50),\n    'learning_rate': (0.01, 0.5),\n      }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer = BayesianOptimization(hyp_lgbm,pds,random_state=RANDOM_SEED)\n# optimizer.maximize(init_points=10, n_iter=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer.max['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CAT Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cgb\n\ndef cat_hyp(depth, bagging_temperature, l2_leaf_reg, learning_rate):\n  params = {\"iterations\": 100,\n            \"loss_function\": \"RMSE\",\n            \"verbose\": False} \n  params[\"depth\"] = int(round(depth)) \n  params[\"bagging_temperature\"] = bagging_temperature\n  params[\"learning_rate\"] = learning_rate\n  params[\"l2_leaf_reg\"] = l2_leaf_reg\n  \n  cat_feat = [] # Categorical features list, we have nothing in this dataset\n  cv_dataset = cgb.Pool(data=X,\n                        label=y,\n                        cat_features=cat_feat)\n\n  scores = cgb.cv(cv_dataset,\n              params,\n              fold_count=3)\n  return -np.min(scores['test-RMSE-mean']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Search space\npds = {'depth': (4, 10),\n       'bagging_temperature': (0.1,10),\n       'l2_leaf_reg': (0.1, 10),\n       'learning_rate': (0.1, 0.2)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer = BayesianOptimization(cat_hyp, pds, random_state=RANDOM_SEED)\n# optimizer.maximize(init_points=10, n_iter=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer.max['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGB Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(X, y, feature_names=X.columns.values)\n\ndef hyp_xgb(max_depth, subsample, colsample_bytree,min_child_weight, gamma, learning_rate):\n    params = {\n    'objective': 'reg:squarederror',\n    'eval_metric':'rmse',\n    'nthread':-1\n     }\n    \n    params['max_depth'] = int(round(max_depth))\n    params['subsample'] = max(min(subsample, 1), 0)\n    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n    params['min_child_weight'] = int(min_child_weight)\n    params['gamma'] = max(gamma, 0)\n    params['learning_rate'] = learning_rate\n    scores = xgb.cv(params, dtrain, num_boost_round=500,verbose_eval=False, \n                    early_stopping_rounds=10, nfold=3)\n    return -scores['test-rmse-mean'].iloc[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pds ={\n  'min_child_weight':(3, 20),\n  'gamma':(0, 5),\n  'subsample':(0.7, 1),\n  'colsample_bytree':(0.1, 1),\n  'max_depth': (3, 10),\n  'learning_rate': (0.01, 0.5)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer = BayesianOptimization(hyp_xgb, pds, random_state=RANDOM_SEED)\n# optimizer.maximize(init_points=4, n_iter=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer.max['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking\n\nWe will use the best parameter as a learner and use Linear Regression as the meta-learner. You can also tune the meta-learner parameter. Also, make sure to convert some parameters into an integer."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"param_lgbm = {\n     'bagging_fraction': 0.973905385549851,\n     'feature_fraction': 0.2945585590881137,\n     'learning_rate': 0.03750332268701348,\n     'max_depth': int(7.66),\n     'min_child_weight': int(41.36),\n     'min_split_gain': 0.04033836353603582,\n     'num_leaves': int(46.42),\n     'application':'regression',\n     'num_iterations': 5000,\n     'metric': 'rmse'\n}\n\nparam_cat = {\n     'bagging_temperature': 0.31768713094131684,\n     'depth': int(8.03),\n     'l2_leaf_reg': 1.3525686450404295,\n     'learning_rate': 0.2,\n     'iterations': 100,\n     'loss_function': 'RMSE',\n     'verbose': False\n}\n\nparam_xgb = {\n     'colsample_bytree': 0.8119098377889549,\n     'gamma': 2.244423418642122,\n     'learning_rate': 0.015800631696721114,\n     'max_depth': int(9.846),\n     'min_child_weight': int(15.664),\n     'subsample': 0.82345,\n     'objective': 'reg:squarederror',\n     'eval_metric':'rmse',\n     'num_boost_roun' : 500\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn import svm\nimport lightgbm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [\n        ('lgbm', lightgbm.LGBMRegressor(**param_lgbm, random_state=RANDOM_SEED, n_jobs=-1)),\n        ('xgbr', XGBRegressor(**param_xgb, random_state=RANDOM_SEED, nthread=-1)),\n        ('cat', CatBoostRegressor(**param_cat)),\n        ('mlp', MLPRegressor()) # without tuning\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = StackingRegressor(\n    estimators=estimators,\n    final_estimator=LinearRegression(),\n    n_jobs=-1,\n    cv=5\n)\n\nreg.fit(X, y)\n\ny_pred = reg.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample['target'] = y_pred\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}