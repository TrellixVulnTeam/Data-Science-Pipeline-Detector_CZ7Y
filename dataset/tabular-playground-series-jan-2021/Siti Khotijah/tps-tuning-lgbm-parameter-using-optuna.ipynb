{"cells":[{"metadata":{},"cell_type":"markdown","source":" # Importing Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nimport lightgbm as lgb\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport gc\nimport optuna\npd.set_option('display.max_columns', 1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ntest  = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')\nsample_submission  = pd.read_csv('../input/tabular-playground-series-jan-2021/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation in Data\n\nIn this work, I try to measure correlation in data using Correlation coefficients.\n\nCorrelation coefficientsare used to measure how strong a relationship is between two variables.Correlation coefficient formulas are used to find how strong a relationship is between data. The formulas return a value between -1 and 1, where:\n\n1 indicates a strong positive relationship.\n-1 indicates a strong negative relationship.\n\nA result of zero indicates no relationship at all.\n\n<img src=\"https://www.statisticshowto.com/wp-content/uploads/2012/10/pearson-2-small.png\" width=\"500\">\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=train.corr()[\"target\"]\ncorr[np.argsort(corr, axis=0)[:-1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting correlations"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting correlations\nnum_feat=train.columns[train.dtypes!=object]\nnum_feat=num_feat [:-1]\nlabels = []\nvalues = []\nfor col in num_feat:\n    labels.append(col)\n    values.append(np.corrcoef(train[col].values, train.target.values)[0,1])\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(8,15))\nrects = ax.barh(ind, np.array(values), color='red')\nax.set_yticks(ind+((width)/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation Coefficients each feature with target\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading & Preparing the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"features=['cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cont11','cont12','cont13','cont14']\ntarget = 'target'\nxTrain, xTest = train[features],test[features]\nyTrain= train[target]\n\nprint('******** Finished Reading & Preparing the Data**********')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# OPTUNA\n\nOptuna uses Bayesian methods to figure out an optimal set of hyperparameters. For more information on Bayesian methods for searching optimal parameters, check out this wonderful article : https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f ."},{"metadata":{},"cell_type":"markdown","source":"## Objective Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"from optuna import Trial\n\ndef objective(trial:Trial,fastCheck=True,targetMeter=0,returnInfo=False):\n    folds = 10\n    seed  = 0\n    shuffle = False\n    kf = KFold(n_splits=folds,shuffle=False,random_state=seed)\n    yValidPredTotal = np.zeros(xTrain.shape[0])\n    gc.collect()\n    models=[]\n    validScore=0\n    for trainIdx,validIdx in kf.split(xTrain,yTrain):\n        trainData=xTrain.iloc[trainIdx,:],yTrain[trainIdx]\n        validData=xTrain.iloc[validIdx,:],yTrain[validIdx]\n        model,yPredValid,log = fitLGBM(trial,trainData,validData,numRounds=5000)\n        yValidPredTotal[validIdx]=yPredValid\n        models.append(model)\n        gc.collect()\n        validScore+=log[\"validRMSE\"]\n    validScore/=len(models)\n    return validScore","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining Parameter Space for OPTUNA"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fitLGBM(trial,train,val,numRounds=5000): \n    xTrainLGBM,yTrainLGBM = train\n    xValidLGBM,yValidLGBM = val\n    boosting_list = ['gbdt','goss']\n    objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n    objective_list_class = ['regression','binary', 'cross_entropy']\n    params={\n      'boosting':trial.suggest_categorical('boosting',boosting_list),\n      'num_leaves':trial.suggest_int('num_leaves', 2, 2**11),\n      'max_depth':trial.suggest_int('max_depth', 2, 25),\n      'max_bin': trial.suggest_int('max_bin', 32, 450,550),      \n      'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 150,256),\n      'min_data_in_bin': trial.suggest_int('min_data_in_bin', 1, 150,256),\n      'min_gain_to_split' : trial.suggest_discrete_uniform('min_gain_to_split', 0.1, 5, 0.01),      \n      'lambda_l1':trial.suggest_loguniform('lamda_l1',1e-8,10),\n      'lambda_l2':trial.suggest_loguniform('lamda_l2',1e-8,10),\n      'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n      'metric':trial.suggest_categorical('metric', ['RMSE']),\n      'objective':trial.suggest_categorical('objective',objective_list_reg),\n      'bagging_fraction':trial.suggest_discrete_uniform('bagging_fraction',0.4, 1, 0.01),\n      'feature_fraction':trial.suggest_discrete_uniform('feature_fraction',0.4, 1, 0.01),\n    }\n    earlyStop=1000\n    verboseEval=100\n    dTrain = lgb.Dataset(xTrainLGBM,label=yTrainLGBM)\n    dValid = lgb.Dataset(xValidLGBM,label=yValidLGBM)\n    watchlist = [dTrain,dValid]\n\n    # Callback for pruning.\n    lgbmPruningCallback = optuna.integration.LightGBMPruningCallback(trial, 'rmse', valid_name='valid_1')\n\n    model = lgb.train(params,train_set=dTrain,num_boost_round=numRounds,valid_sets=watchlist,verbose_eval=verboseEval,early_stopping_rounds=earlyStop,callbacks=[lgbmPruningCallback])\n\n    #predictions\n    pred_val=model.predict(xValidLGBM,num_iteration=model.best_iteration)\n    oofPred = pred_val.astype(int)        \n    log={'trainRMSE':model.best_score['training']['rmse'],\n       'validRMSE':model.best_score['valid_1']['rmse']}\n    return model,pred_val,log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optimization & STUDY Object"},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\nstudy.optimize(objective,n_trials=200)#For the sake of simplicity, I have kept n_trials as less, but this can be altered for better results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## OPTUNA Study History : Analysis & Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## optuna.visualization.plot_optimization_history(study) : This function plots optimization history of all trials in a study"},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## optuna.visualization.plot_slice(study, params=None) : This function plots the parameter relationship as slice plot in a study"},{"metadata":{"trusted":true},"cell_type":"code","source":"optuna.visualization.plot_slice(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reference : \n\n* https://www.kaggle.com/kst6690/dsb2019-tuning-lightgbm-parameter-using-optuna\n* https://www.statisticshowto.com/probability-and-statistics/correlation-coefficient-formula/#Pearson"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}