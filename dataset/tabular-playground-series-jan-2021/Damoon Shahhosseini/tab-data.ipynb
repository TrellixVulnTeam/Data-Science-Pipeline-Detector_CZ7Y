{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!/opt/conda/bin/python3.7 -m pip install --upgrade pip -q\n!pip install --upgrade xgboost -q","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport xgboost as xgb\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.regularizers import l1, l2, l1_l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n\ntrain = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/test.csv')\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv')\n\nX, y, X_test = train.iloc[:,1:-1], train.iloc[:,-1], test.iloc[:,1:]\n\nfrom sklearn.preprocessing import StandardScaler\n\nfeature_data = pd.concat([train.iloc[:,1:-1], test.iloc[:,1:]])\nfeature_data_norm = StandardScaler().fit_transform(feature_data)\n\ntrain.iloc[:,1:-1] = feature_data_norm[:train.shape[0]]\ntest.iloc[:,1:] = feature_data_norm[:test.shape[0]]\n\nX_norm, X_test_norm = train.iloc[:,1:-1], test.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=5)\nrX = pca.fit_transform(X_norm)\nrX_test = pca.transform(X_test_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = train.iloc[:,1:].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  param = {\n#      'booster': 'dart',\n#      'subsample': 0.8,\n#      'rate_drop': 0.6,\n#      'one_drop': 1,\n#         'max_depth': 10,\n#         'min_child_weight': 50,  \n#         'tree_method': 'gpu_hist',\n#         'objective':'reg:squarederror',\n#      'normalize_type ':'forest'\n#      'colsample_bytree': 0.8,\n#      'grow_policy ': 'lossguide'\n#          'max_delta_step': 10,\n#         'n_jobs':-1,\n#         'reg_lambda':1e-3,\n#         'eta':0.1,  \n#         'eval_metric': ['mae','rmse'],\n#         'verbosity': 1,\n#         'predictor': 'gpu_predictor',\n#          'seed': 11647\n#     }\n    \n# param = {\n#         'booster': 'gbtree',\n#         'max_depth': 20,\n#         'subsample': 0.51,\n#         'min_child_weight': 3,  \n#         'tree_method': 'gpu_hist',\n#         'n_jobs':-1,\n#         'eta':0.09,  \n#         'eval_metric': 'mae',\n#         'verbosity': 1,\n#         'predictor': 'gpu_predictor'\n#     }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def kfold_validation(X, y, X_test,n_fold=10, seeds=[0, 1, 2, 3, 4, 5, 6]):\n    \"\"\"\n        Runs a repeated KFold on a given model and data\n        \n        input:\n            model: model to traing the data\n            X, y, X_test: datasets needed for training and prediction\n            n_rep, n_fold: parameters of Repeated KFold\n        \n        return:\n            model: trained model\n            preds: predictions at each fold\n            avg_preds: average of all predictions\n    \"\"\"\n    index = 0  # Keep track of the loss and val_loss (history object)\n    prediction = np.zeros((X_test.shape[0])) # For every single prediction\n    preds = np.empty((len(seeds) * n_fold, X_test.shape[0])) # Saving all the predictions\n      \n        \n#     param = {\n#         'booster': 'gbtree',\n#         'max_depth': 20,\n#         'subsample': 0.8,\n#         'min_child_weight': 3,  \n#         'tree_method': 'gpu_hist',\n#         'n_jobs':-1,\n#         'eta':0.09,  \n#         'eval_metric': 'mae',\n#         'verbosity': 1,\n#         'predictor': 'gpu_predictor'\n#     }\n    param = {\n     'booster': 'gblinear',\n        'objective':'reg:squarederror',\n        'n_jobs':-1,\n        'top_k': 10,\n        'updater': 'coord_descent',\n        'reg_lambda': 9e-3,\n        'reg_alpha': 5e-3,\n        'feature_selector': 'greedy',\n        'eta':0.3,  \n        'eval_metric': ['mae','rmse'],\n        'verbosity': 1,\n    }\n    \n    dtest = xgb.DMatrix(X_test)\n    \n    eary_stopping = xgb.callback.EarlyStopping(rounds=30, \n                                               metric_name='mae', \n                                               data_name='eval', \n                                               maximize=False, \n                                               save_best=True)\n    \n    bst = None\n    for seed in seeds:\n        kf = KFold(n_splits=n_fold, random_state=seed,shuffle=True)\n        LOAD_MODEL = False\n        \n        for train_indices, val_indices in kf.split(X, y):\n            # Data divided into Train and Validation splits\n            if type(X) == np.ndarray:\n                X_train, X_val = X[train_indices,:], X[val_indices,:]\n                y_train, y_val = y[train_indices], y[val_indices]\n                \n            else:\n                X_train, X_val = X.iloc[train_indices, :], X.iloc[val_indices,: ]\n                y_train, y_val = y.iloc[train_indices], y.iloc[val_indices]\n            \n            dtrain = xgb.DMatrix(X_train, label=y_train)\n            dval = xgb.DMatrix(X_val, label=y_val)\n\n            print(f'{seed}-{(index + 1)%n_fold}th fold')\n            \n            bst = xgb.train(param, \n                    dtrain, \n                    num_boost_round=500, \n                    evals=[(dtrain, 'train'), (dval, 'eval')],\n                    verbose_eval =True,\n                    callbacks=[xgb.callback.EarlyStopping(rounds=20, \n                                               metric_name='mae', \n                                               data_name='eval', \n                                               maximize=False)],\n                    xgb_model=f'/kaggle/working/xg00_{seed}' if LOAD_MODEL else None\n                   )\n            \n            LOAD_MODEL = True\n            bst.save_model(f'/kaggle/working/xg00_{seed}')\n    \n\n            #------------------ Predictions -------------------\n            model_prediction = bst.predict(dtest)\n\n            # Saving the predictions for each fold\n            preds[index] = model_prediction\n            index += 1\n\n            # Starting different fold or end of folding\n            print('#----------------#----------------#----------------#----------------#----------------#')\n        \n    # Averaging the predictions\n    p = pd.DataFrame(preds)\n    p = p.sum() / (n_fold * len(seeds))\n    \n    sub['target'] = p\n    sub.to_csv('subX.csv', index=False)\n    \n    return preds, p,bst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, avg_pred, bst = kfold_validation(X, y, X_test, 10, seeds=[991, 21])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig, ax = plt.subplots(figsize=(30, 30))\n# xgb.plot_tree(bst, num_trees=9, ax=ax)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig, ax = plt.subplots(figsize=(30, 30))\n# xgb.plot_tree(bst, num_trees=5, ax=ax)\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig, ax = plt.subplots(3,1,figsize=(10, 10))\n# xgb.plot_importance(bst, ax=ax[0])\n# xgb.plot_importance(bst, ax=ax[1], importance_type='gain', title='gain')\n# xgb.plot_importance(bst, ax=ax[2], importance_type='cover', title='cover')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = Sequential(layers = [\n    Dense(16,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01)),\n    Dense(16,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01),\n          activation='softmax'),\n    BatchNormalization(),\n    Dense(32,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01)),\n    Dense(32,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01), \n          activation='softmax'),\n    BatchNormalization(),\n    Dense(32,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01)),\n    Dense(32,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01), \n          activation='softmax'),\n    BatchNormalization(),\n    Dense(64,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01)),\n    Dense(64,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01), \n          activation='softmax'),\n    BatchNormalization(),\n    Dense(128,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01)),\n    Dense(128,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01), \n          activation='softmax'),\n    BatchNormalization(),\n    Dense(256,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01)),\n    Dense(256,\n         kernel_initializer=TruncatedNormal(0,2,111),\n         kernel_regularizer=l1(0.001),\n         bias_initializer=TruncatedNormal(0,0.1,121),\n         bias_regularizer=l2(0.01), \n          activation='softmax'),\n    BatchNormalization(),\n    Dense(1)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=Adam(0.0015), loss='mae')\nhist = model.fit(X,y,\n          validation_split=0.1, verbose=1, \n          batch_size=1024, epochs=1000,\n          callbacks=[\n              EarlyStopping(monitor='val_loss', \n                            min_delta=0, \n                            patience=20, \n                            verbose=1, \n                            mode='min', \n                            restore_best_weights=True),\n              ReduceLROnPlateau(monitor='val_loss', \n                                factor=0.9, \n                                patience=5, \n                                verbose=1,\n                                min_delta=0, \n                                cooldown=0, \n                                min_lr=1e-12)\n          ]\n         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(hist.history).plot(y=['loss','val_loss'], title='loss', figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'] = model.predict(X_test).reshape(X_test.shape[0])\nsub.to_csv('subN.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('nn01')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}