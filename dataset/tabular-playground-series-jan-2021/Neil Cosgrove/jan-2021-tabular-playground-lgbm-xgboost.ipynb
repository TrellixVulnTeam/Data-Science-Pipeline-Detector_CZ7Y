{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tabular Playground Series - Jan 2021\n\nThe Kaggle Tabular Playground for January offer us a Blackbox Challenge: an array of readings labeled cont1-cont14 which translate to a target value.  The goal is to develop a model based off of training data which provides the readings and the resulting target to be able to accurately predict the target value for a test dataset where we are supplied readings but must predict the target.   "},{"metadata":{},"cell_type":"markdown","source":"### The Prelimnaries, import the usual basic libraries. "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ninput_path = Path('/kaggle/input/tabular-playground-series-jan-2021/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read in the data files"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(input_path / 'test.csv', index_col='id')\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let us look at the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values to worry about, and the dataset look very similar, ;ets plot the two datasets as Boxplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot = train.boxplot(column=['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14'],\n                       figsize=(12,9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot = test.boxplot(column=['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14'],\n                       figsize=(12,9))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even the distributions of outliers appears the same between datasets. Just to round off lets look at the correlation matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 12))\n\nsns.heatmap(train.corr(), annot = True,fmt='.1g', vmin=-1, vmax=1, center= 0,cmap= 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not seeing any obvious clues to feature engineering, so lets break our training data into a training and training validation subset and see what the models tells us"},{"metadata":{},"cell_type":"markdown","source":"### Pull out the target, and make a validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train.pop('target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One thing I noted in the example notebook, they used a rather low (60%) of the training data for training.  I changed this to a perhaps more standard 80%"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Determination of Best Model "},{"metadata":{},"cell_type":"markdown","source":"We now do a preliminary run of some of the common regression models to see which one(s) perform well and are worth pursuing further."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.dummy import DummyRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import LassoLars\nfrom sklearn.linear_model import ARDRegression\nfrom sklearn.linear_model import PassiveAggressiveRegressor\nfrom sklearn.linear_model import TheilSenRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def FitAndScoreModel(df,name, model,X_tr,y_tr,X_tst,y_tst):\n    model.fit(X_tr,y_tr)\n    Y_pred = model.predict(X_tst)\n    score=mean_squared_error(y_tst, Y_pred, squared=False)\n    df = df.append({'Model':name, 'MSE': score},ignore_index = True) \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a Blank Dataframe, as we run each model we will note it and its score "},{"metadata":{"trusted":true},"cell_type":"code","source":"dResults = pd.DataFrame(columns = ['Model', 'MSE'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [\n    DummyRegressor(strategy='median'),\n   # SVR(),\n    SGDRegressor(),\n    BayesianRidge(),\n    LassoLars(),\n    ARDRegression(),\n    PassiveAggressiveRegressor(),\n    LinearRegression(),\n    LGBMRegressor(),\n    RandomForestRegressor(),\n    XGBRegressor()]\n\n \nfor item in classifiers:\n    print(item)\n    clf = item\n    dResults=FitAndScoreModel(dResults,item,item,X_train,y_train,X_test,y_test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Score Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"dResults.sort_values(by='MSE', ascending=True,inplace=True)\ndResults.set_index('MSE',inplace=True)\ndResults.head(dResults.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So LGBm Regressor, Random Forrest and XGBRegressor are the top 3. "},{"metadata":{},"cell_type":"markdown","source":"Note:  As tuning takes a verrrrry long time, I am leaving my tuning actions in for reference but commented out and just referencing the resulting parameters that were generated"},{"metadata":{},"cell_type":"markdown","source":"## Tuning LGBM Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna.integration.lightgbm as lgbTune\n\n#dtrain = lgbTune.Dataset(X_train, label=y_train)\n#dval = lgbTune.Dataset(X_test, label=y_test)\n#params = {\"objective\": \"regression\",\n#          \"metric\": \"rmse\",\n#          'num_leaves':2 ** 8,\n#          \"verbosity\": -1,\n#          \"boosting_type\": \"gbdt\",\n#          \"n_estimators\":20000, \n#          \"early_stopping_round\":400,\n#          'n_jobs': -1,\n#          'learning_rate': 0.005,\n#          'max_depth': 8,\n#          'tree_learner': 'serial',\n#          'colsample_bytree': 0.8,\n#          'subsample_freq': 1,\n#          'subsample': 0.8,\n#          'max_bin': 255}\n\n\n#model = lgbTune.train(params, dtrain, valid_sets=[dval], verbose_eval=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#params = model.params\n#params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params={'objective': 'regression',\n 'metric': 'rmse',\n 'num_leaves': 234,\n 'verbosity': -1,\n 'boosting_type': 'gbdt',\n 'n_jobs': -1,\n 'learning_rate': 0.005,\n 'max_depth': 8,\n 'tree_learner': 'serial',\n 'max_bin': 255,\n 'feature_pre_filter': False,\n 'bagging_fraction': 0.4134640813947842,\n 'bagging_freq': 1,\n 'feature_fraction': 0.4,\n 'lambda_l1': 9.511141306606756,\n 'lambda_l2': 1.3196758411622028e-08,\n 'min_child_samples': 20,\n 'num_iterations': 20000,\n 'early_stopping_round': 400}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now fit over 10 folds and arrive at LGBM predictions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import KFold\n\nn_fold = 10\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\ntrain_columns = train.columns.values\n\noof = np.zeros(len(train))\nLGBMpredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train, target.values)):\n    \n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    \n    X_tr, X_val = train.iloc[trn_idx], train.iloc[val_idx]\n    y_tr, y_val = target.iloc[trn_idx], target.iloc[val_idx]\n\n    model = LGBMRegressor(**params, n_estimators = 20000)\n   \n    model.fit(X_tr, y_tr, \n              eval_set=[(X_tr, y_tr), (X_val, y_val)], eval_metric='rmse',\n              verbose=1000, early_stopping_rounds=400)\n    \n    \n    oof[val_idx] = model.predict(X_val, num_iteration=model.best_iteration_)\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_columns\n    fold_importance_df[\"importance\"] = model.feature_importances_[:len(train_columns)]\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    #predictions\n    LGBMpredictions += model.predict(test, num_iteration=model.best_iteration_) / folds.n_splits\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look at the feature importance for LGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:3014].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure()\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using XGBoost"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Let's repeat the process for XGBoot, again tuning is commented out and the parameters that resulted from tuning used."},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import GridSearchCV\n\n#xgb = XGBRegressor()\n#parameters = {'nthread':[4], \n#              'objective':['reg:squarederror'],\n#              'learning_rate': [.01,.03, 0.05, .07], \n#              'max_depth': [5, 6, 7],\n#              'min_child_weight':range(1,6,2),\n#              'silent': [1],\n#              'subsample': [0.7],\n#              'colsample_bytree': [0.7],\n#              'n_estimators': [500,1000,2000,4000]}\n\n \n  \n\n#xgb_grid = GridSearchCV(xgb,\n#                        parameters,\n#                        cv = 2,\n#                        n_jobs = 5,\n#                        verbose=True)\n\n#xgb_grid.fit(train,\n#         target)\n\n#print(xgb_grid.best_score_)\n#print(xgb_grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGparams = xgb_grid.best_params_\n#XGparams","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGparams={'colsample_bytree': 0.7,\n 'learning_rate': 0.01,\n 'max_depth': 7,\n 'min_child_weight': 1,\n 'n_estimators': 4000,\n 'nthread': 4,\n 'objective': 'reg:squarederror',\n# 'silent': 1,\n 'subsample': 0.7}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As before we fit over 10 folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_fold = 10\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)\ntrain_columns = train.columns.values\n\noof = np.zeros(len(train))\nXGpredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train, target.values)):\n    \n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    \n    X_tr, X_val = train.iloc[trn_idx], train.iloc[val_idx]\n    y_tr, y_val = target.iloc[trn_idx], target.iloc[val_idx]\n\n    model = XGBRegressor(**XGparams)\n   \n    model.fit(X_tr, y_tr, \n              eval_set=[(X_tr, y_tr), (X_val, y_val)], verbose=1000, early_stopping_rounds=400)\n    \n    \n    oof[val_idx] = model.predict(X_val, ntree_limit=model.best_iteration)\n    preds = model.predict(test, ntree_limit=model.best_iteration)\n\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_columns\n    fold_importance_df[\"importance\"] = model.feature_importances_[:len(train_columns)]\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    #predictions\n    XGpredictions += model.predict(test, ntree_limit=model.best_iteration)/ folds.n_splits\n   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again let us look at feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")  \n        .mean()  \n        .sort_values(by=\"importance\", ascending=False)[:3014].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure()\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False)) \nplt.title('XGBoost Features (averaged over folds)')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission  = pd.read_csv(input_path / 'sample_submission.csv', index_col='id')\nsubmission.reset_index(inplace=True)\nsubmission = submission.rename(columns = {'index':'id'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create an LGBM Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"LGBMsubmission=submission.copy()\nLGBMsubmission['target'] = LGBMpredictions\nLGBMsubmission.to_csv('submission_LGBM.csv', header=True, index=False)\nLGBMsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score  0.69780"},{"metadata":{},"cell_type":"markdown","source":"#### Create an XGBoost Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBoostsubmission=submission.copy()\nXGBoostsubmission['target'] = XGpredictions\nXGBoostsubmission.to_csv('submission_XGBoost.csv', header=True, index=False)\nXGBoostsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Score 0.69945"},{"metadata":{},"cell_type":"markdown","source":"### Ensemble Solution of LGBM and XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"EnsembledSubmission=submission.copy()\n#EnsembledSubmission['target'] = (0.5*XGpredictions)+(0.5*LGBMpredictions)\nEnsembledSubmission['target'] = (LGBMpredictions*0.72 + XGpredictions*0.28)\nEnsembledSubmission.to_csv('ensembled_submission.csv', header=True, index=False)\nEnsembledSubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Score 0.69819"},{"metadata":{},"cell_type":"markdown","source":"Disappointed that the Ensemble did not produce an improvement, especially since each model weighed different features differently.  Will look into pulling the third place algorithm, Random Forest, into the mix and see if that doesn't make a change for the better."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}