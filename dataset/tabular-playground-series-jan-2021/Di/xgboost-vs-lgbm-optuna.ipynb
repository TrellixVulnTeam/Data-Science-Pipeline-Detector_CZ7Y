{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgbm\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.ensemble import StackingRegressor,GradientBoostingRegressor,RandomForestRegressor\nimport optuna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ndata.drop(columns='id',axis=1,inplace=True)\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization and Cleaning"},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at the boxplot of each feature and target."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_col_data = [data[col] for col in data.columns]\nall_col_data.pop(-1)\nfig,ax = plt.subplots(figsize=(14,10))\nax.boxplot(all_col_data)\nax.set_xticklabels(data.columns[:-1])\nax.set_title('boxplot before cleaning')\noriginal_len = len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some outliers in features 'cont7', 'cont9', 'cont10'"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(8,5))\nax.boxplot(data['target'])\nax.set_title('boxplot of target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"target=0 seems to be an extreme outlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data.target!=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Attention: This cleaning method will lead to bad result.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"IQR_dict = defaultdict(list)\nfor cont in ['cont7','cont9','cont10']:\n    Q1 = data[cont].quantile(0.25)\n    Q3 = data[cont].quantile(0.75)\n    IQR = Q3 - Q1  \n    IQR_dict[cont] = [Q1,Q3,IQR]\n    \nfor key,value in IQR_dict.items():\n    myfilter = (data[key] >= value[0] - 1.5 * value[2]) & (data[key] <= value[1] + 1.5 *value[2])\n    data = data.loc[myfilter] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_col_data = [data[col] for col in data.columns]\nall_col_data.pop(-1)\nfig,ax = plt.subplots(figsize=(14,10))\nax.boxplot(all_col_data)\nax.set_xticklabels(data.columns[:-1])\nax.set_title('boxplot after cleaning')\ncleaned_len = len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clean data under the rule of 1.5*interquantile range"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('%s rows of data is dropped'% (original_len-cleaned_len))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's also take a look at the box plot of target after cleaning."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize=(8,5))\nax.boxplot(data['target'])\nax.set_title('boxplot of target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the correlation matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"#x = data.drop(columns='target',axis=1)\ncorr = data.corr()\nfig,ax = plt.subplots(figsize=(12,10))\nsns.heatmap(corr,ax=ax,vmin=-1, vmax=1, cmap='coolwarm', annot=True)\nplt.yticks(rotation=0,fontsize=13)\nplt.xticks(rotation=90,fontsize=13)\nax.set_title('correlation heatmap',fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features 'cont1', 'cont6'~'cont13' have relatively high correlation with each other. "},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = np.full(shape=(corr.shape[0],), fill_value=True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if abs(corr.iloc[i,j]) >= 0.9:\n            if columns[j]:\n                columns[j] = False\nsel_columns = data.columns[columns]\ndata = data[sel_columns]\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No column are dropped due to high absoulte correlation > 0.9"},{"metadata":{},"cell_type":"markdown","source":"# Baseline Model"},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_model_pipeline(data):\n    #train_x,test_x,train_y,test_y = train_test_split(data,test_size=0.2,random_state=42)\n    X = data.drop(columns='target',axis=1)\n    y = data['target']\n\n    ##Caculate E_cv (cross validation error)\n    kf = KFold(n_splits=5,shuffle=True,random_state=42)\n    fold_error = []\n    for fold,(train_idx,val_idx) in enumerate(kf.split(X)):\n        train_X,val_X = X.iloc[train_idx,:],X.iloc[val_idx,:]\n        train_y,val_y = y.iloc[train_idx],y.iloc[val_idx]\n        \n        print('Fold %s:'% (fold))\n        xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.1, max_depth=8,seed=42,verbosity=1)\n        xgb_model.fit(train_X,train_y, eval_metric= 'rmse',\n              eval_set=[(train_X,train_y),(val_X,val_y)], early_stopping_rounds=5,verbose=False)\n        pred_y = xgb_model.predict(val_X)\n        in_fold_rmse = np.sqrt(np.mean((val_y-pred_y)**2))\n        print('Fold %s rmse: %s\\n' % (fold,in_fold_rmse))\n        fold_error.append(in_fold_rmse)\n        \n    oof_rmse = np.sum(fold_error)/len(fold_error)\n    print('E_cv: %s' % (oof_rmse))\n\n    ##Train on the whole training data\n    xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.1, max_depth=8,seed=42,verbosity=1)\n    xgb_model.fit(X,y,verbose=False)\n    \n    ##Plot feature importance\n    fig,ax = plt.subplots(figsize=(10,8))\n    xgb.plot_importance(xgb_model, ax=ax, importance_type='gain')\n    plt.yticks(fontsize=14)\n    \n    return xgb_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = xgb_model_pipeline(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lightgbm_pipeline(data):\n    X = data.drop(columns='target',axis=1)\n    y = data['target']\n\n    ##Caculate E_cv (cross validation error)\n    kf = KFold(n_splits=5,shuffle=True,random_state=42)\n    fold_error = []\n    for fold,(train_idx,val_idx) in enumerate(kf.split(X)):\n        train_X,val_X = X.iloc[train_idx,:],X.iloc[val_idx,:]\n        train_y,val_y = y.iloc[train_idx],y.iloc[val_idx]\n        \n        print('Fold %s:'% (fold))\n        lgbm_model = LGBMRegressor(n_estimators=1000, learning_rate=0.1, \n                                   max_depth=8,random_state=42,verbosity=-1)\n        lgbm_model.fit(train_X,train_y, eval_metric= 'rmse',\n              eval_set=[(val_X,val_y)], early_stopping_rounds=5,verbose=0)\n        pred_y = lgbm_model.predict(val_X)\n        in_fold_rmse = np.sqrt(np.mean((val_y-pred_y)**2))\n        print('Fold %s rmse: %s\\n' % (fold,in_fold_rmse))\n        fold_error.append(in_fold_rmse)\n        \n    oof_rmse = np.sum(fold_error)/len(fold_error)\n    print('E_cv: %s' % (oof_rmse))\n\n    ##Train on the whole training data\n    lgbm_model = LGBMRegressor(n_estimators=1000, learning_rate=0.1, \n                               max_depth=8, random_state=42,verbosity=-1)\n    lgbm_model.fit(X,y,verbose=0)\n    \n    ##Plot feature importance\n    fig,ax = plt.subplots(figsize=(10,8))\n    lgbm.plot_importance(lgbm_model, ax=ax, importance_type='gain')\n    plt.yticks(fontsize=14)\n    \n    return lgbm_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_model = lightgbm_pipeline(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LightGBM baseline model performs better than XGBoost baseline model. On public leaderboard(0.70140 vs 0.70587)."},{"metadata":{},"cell_type":"markdown","source":"# Optuna"},{"metadata":{},"cell_type":"markdown","source":"## Tune XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"def xgb_oof(trial,data):\n    params = {\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [250, 300, 350, 400, 450]),\n        \"eta\": trial.suggest_loguniform(\"eta\",1e-2,1e-1),\n        \"max_depth\": trial.suggest_categorical(\"max_depth\",[6,8,10,12]),\n        \"subsample\": trial.suggest_discrete_uniform(\"subsample\", 0.6,1,0.1),\n        \"colsample_bytree\": trial.suggest_discrete_uniform(\"colsample_bytree\", 0.6,1,0.1),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\",5,11),\n        \"random_state\": 42\n    }\n        \n    X = data.drop(columns='target',axis=1)\n    y = data['target']\n\n    kf = KFold(n_splits=5,shuffle=True,random_state=42)\n    fold_error = []\n    for fold,(train_idx,val_idx) in enumerate(kf.split(X)):\n        train_X,val_X = X.iloc[train_idx,:],X.iloc[val_idx,:]\n        train_y,val_y = y.iloc[train_idx],y.iloc[val_idx]\n        \n        xgb_model = XGBRegressor(**params)\n        xgb_model.fit(train_X,train_y)\n        pred_y = xgb_model.predict(val_X)\n        in_fold_rmse = np.sqrt(np.mean((val_y-pred_y)**2))\n\n        fold_error.append(in_fold_rmse)\n        \n    oof_rmse = np.sum(fold_error)/len(fold_error)\n    return oof_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    return xgb_oof(trial,data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"study = optuna.create_study(direction='minimize',study_name='XGBoost optimization')\nstudy.optimize(objective, n_trials=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_xgb = XGBRegressor(**(study.best_params))\nbest_xgb.fit(data.drop(columns='target',axis=1),data['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tune LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lgbm_oof(trial,data):\n    params = {\n        'num_leaves':trial.suggest_int('num_leaves',31,100),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [250, 300, 350, 400, 450]),\n        \"eta\": trial.suggest_loguniform(\"eta\",1e-2,1e-1),\n        \"max_depth\": trial.suggest_categorical(\"max_depth\",[6,8,10,12]),\n        \"subsample\": trial.suggest_discrete_uniform(\"subsample\", 0.6,1,0.1),\n        \"colsample_bytree\": trial.suggest_discrete_uniform(\"colsample_bytree\", 0.6,1,0.1),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\",5,11),\n        'min_child_sample':trial.suggest_int('min_child_sample',20,50),\n        \"random_state\": 42\n    }\n        \n    X = data.drop(columns='target',axis=1)\n    y = data['target']\n\n    kf = KFold(n_splits=5,shuffle=True,random_state=42)\n    fold_error = []\n    for fold,(train_idx,val_idx) in enumerate(kf.split(X)):\n        train_X,val_X = X.iloc[train_idx,:],X.iloc[val_idx,:]\n        train_y,val_y = y.iloc[train_idx],y.iloc[val_idx]\n        \n        lgbm_model = LGBMRegressor(**params)\n        lgbm_model.fit(train_X,train_y)\n        pred_y = lgbm_model.predict(val_X)\n        in_fold_rmse = np.sqrt(np.mean((val_y-pred_y)**2))\n\n        fold_error.append(in_fold_rmse)\n        \n    oof_rmse = np.sum(fold_error)/len(fold_error)\n    return oof_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trail):\n    return lgbm_oof(trail,data)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction='minimize',study_name='LGBM optimization')\nstudy.optimize(objective, n_trials=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lgbm = LGBMRegressor(**(study.best_params))\nbest_lgbm.fit(data.drop(columns='target',axis=1),data['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble two best models"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_xgb_param = {'n_estimators': 450,\n                  'eta': 0.025241948026570656,\n                  'max_depth': 10,\n                  'subsample': 0.6,\n                  'colsample_bytree': 0.6,\n                  'min_child_weight': 7}\n\nbest_lgbm_param = {'num_leaves': 42,\n                   'n_estimators': 400,\n                   'eta': 0.07349402647118564,\n                   'max_depth': 6,\n                   'subsample': 1.0,\n                   'colsample_bytree': 0.6,\n                   'min_child_weight': 9,\n                   'min_child_sample': 32}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ensemble_pipeline(data,best_xgb,best_lgbm):\n    X = data.drop(columns='target',axis=1)\n    y = data['target']\n\n    ##Caculate E_cv (cross validation error)\n    kf = KFold(n_splits=5,shuffle=True,random_state=42)\n    fold_error = []\n    for fold,(train_idx,val_idx) in enumerate(kf.split(X)):\n        train_X,val_X = X.iloc[train_idx,:],X.iloc[val_idx,:]\n        train_y,val_y = y.iloc[train_idx],y.iloc[val_idx]\n        \n        print('Fold %s:'% (fold))\n        ensemble_model = StackingRegressor(estimators = \n                                           [('best_xgb',XGBRegressor(**best_xgb_param)),\n                                            ('best_lgbm',LGBMRegressor(**best_lgbm_param))],\n                                           final_estimator = \n                                           GradientBoostingRegressor(n_estimators=200,\n                                                                 random_state=42))\n        ensemble_model.fit(train_X,train_y)\n        pred_y = ensemble_model.predict(val_X)\n        in_fold_rmse = np.sqrt(np.mean((val_y-pred_y)**2))\n        print('Fold %s rmse: %s\\n' % (fold,in_fold_rmse))\n        fold_error.append(in_fold_rmse)\n        \n    oof_rmse = np.sum(fold_error)/len(fold_error)\n    print('E_cv: %s' % (oof_rmse))\n\n    ##Train on the whole training data\n    ensemble_model = StackingRegressor(estimators = \n                                        [('best_xgb',best_xgb),\n                                        ('best_lgbm',best_lgbm)],\n                                        final_estimator = \n                                        GradientBoostingRegressor(n_estimators=200,\n                                                                 random_state=42))\n    ensemble_model.fit(X,y)\n    \n    return ensemble_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_xgb = XGBRegressor(**best_xgb_param)\nbest_lgbm = LGBMRegressor(**best_lgbm_param)\nensemble_model = ensemble_pipeline(data,best_xgb,best_lgbm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit Result"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')\n#pred_res = xgb_model.predict(test.loc[:,'cont1':])\n#pred_res = best_xgb.predict(test.loc[:,'cont1':])\n#pred_res = lgbm_model.predict(test.loc[:,'cont1':])\n#pred_res = best_lgbm.predict(test.loc[:,'cont1':])\npred_res = ensemble_model.predict(test.loc[:,'cont1':])\nsubmission = pd.DataFrame({'id':test.id,'target':pred_res})\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}