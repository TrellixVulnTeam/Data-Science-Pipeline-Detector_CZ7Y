{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport datetime\nimport gc\nimport os\nimport random\nimport time\nimport warnings\n\n\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport xgboost\nimport catboost\nimport seaborn as sns\nfrom pandas import DataFrame\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, auc\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, KFold\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder\nfrom statsmodels.gam.tests.test_gam import sigmoid\nfrom tqdm import tqdm\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/test.csv')\ntrain.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##vars\nimport random\nlabel = 'target'\nseed = 0\nlocal_test = True\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nseed_everything(seed)\nparams = {\n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        \n        'n_jobs': -1,\n        'learning_rate': 0.006,\n        'num_leaves': 2 ** 8,\n        'max_depth': 8,\n        'tree_learner': 'serial',\n        'colsample_bytree': 0.8,\n        'subsample_freq': 1,\n        'subsample': 0.8,\n\n        'max_bin': 255,\n        'verbose': -1,\n        'seed': seed,\n\n    }\nbase_features = [x for x in train.columns if 'cont' in x]\nremove_features = [label,'id']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## for reg \nsns.distplot(train['target'],bins=100,)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.sort_values(ascending=True).head()\n# train = train[train['target']!=0].reset_index(drop=True)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from lightgbm import LGBMClassifier\nfrom lightgbm import LGBMRegressor\nimport gc\n# from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.model_selection import KFold\n\ndef make_test(new_features):\n    features = base_features + new_features\n    oof_predictions = np.zeros(len(train))\n    final_predictions = np.zeros(len(test))\n    \n    cv = KFold(n_splits=10,shuffle=True,random_state=seed)\n    if local_test:\n        n_estimators=1000\n    else:\n        n_estimators = 10000\n    \n    lgb = LGBMRegressor(**params,n_estimators=n_estimators,device='GPU')\n\n    for n,(trn_id,val_id) in enumerate(cv.split(train[features],train[label])):\n        print(f\"===== training fold {n+1} =====\")\n        trn_x,trn_y = train.loc[trn_id,features],train.loc[trn_id,label]\n        val_x,val_y = train.loc[val_id,features],train.loc[val_id,label]\n        \n        lgb.fit(trn_x,trn_y,eval_set=[(val_x,val_y)],early_stopping_rounds=100,verbose=-1)\n        \n        oof_predictions[val_id] = lgb.predict(val_x)\n        final_predictions += lgb.predict(test[features]) / cv.n_splits\n        \n\n        mse_score = np.sqrt(mean_squared_error(y_true=val_y,y_pred=oof_predictions[val_id]))\n                                       \n        \n        del trn_x,trn_y,val_x,val_y\n        gc.collect()\n    \n    \n    cur_mse_score = np.sqrt(mean_squared_error(y_true=train[label],y_pred=oof_predictions))\n    cur_mae_score = mean_absolute_error(y_true=train[label],y_pred=oof_predictions)                                   \n    print(f\"global mse score {cur_mse_score}\")\n    print(f\"global mae score {cur_mae_score}\")\n    print(f\"diff with previous version {score[0] - cur_mse_score}\")\n    print(f\"diff with previous version {score[1] - cur_mae_score}\")\n    if not local_test:\n        test[label] = final_predictions\n        test[['id',label]].to_csv(f'sub_{np.round(cur_mse_score,4)}.csv',index=False)\n    \n    return [cur_mse_score,cur_mae_score]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#baseline\nlocal_test=False\nscore = [0.6970820000536615, 0.5829603998473519]\nmake_test([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## tuning WITH OPTUNA\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}