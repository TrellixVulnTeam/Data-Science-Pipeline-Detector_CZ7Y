{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install python-vivid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom vivid.estimators.base import MetaBlock\nfrom vivid.estimators.boosting.mixins import BoostingEarlyStoppingMixin\n\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vivid.features.base import BinningCountBlock\nfrom vivid.features.base import CountEncodingBlock\nfrom vivid.features.base import FilterBlock\n\nfrom vivid.estimators.boosting import XGBRegressorBlock\nfrom vivid.estimators.boosting import LGBMRegressorBlock\nfrom vivid.estimators.boosting.block import create_boosting_seed_blocks\n\nfrom vivid.estimators.linear import TunedRidgeBlock\nfrom vivid.estimators.svm import SVRBlock\nfrom vivid.estimators.ensumble import RFRegressorBlock\nfrom vivid.estimators.base import EnsembleBlock, BaseBlock","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/test.csv')\nsample_submission_df = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv')\n\nfeature_columns = [\n 'cont1',\n 'cont2',\n 'cont3',\n 'cont4',\n 'cont5',\n 'cont6',\n 'cont7',\n 'cont8',\n 'cont9',\n 'cont10',\n 'cont11',\n 'cont12',\n 'cont13',\n 'cont14',    \n]\n\ny = train_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture\n\nfrom vivid.core import BaseBlock\n\n\nclass PCABlock(BaseBlock):\n    def __init__(self, n_components=3, columns=None, *args, **kwrgs):\n        self.n_components = n_components\n        \n        if columns is None: columns = feature_columns\n        self.columns = columns\n        super().__init__(name='pca_n={}'.format(n_components), *args, **kwrgs)\n    \n    def fit(self, source_df, y, experiment=None) -> pd.DataFrame:\n        clf = PCA(n_components=self.n_components)\n        clf.fit(source_df[self.columns].values)\n        self.clf_ = clf\n        return self.transform(source_df)\n    \n    def transform(self, source_df):\n        z = self.clf_.transform(source_df[self.columns])\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix('PCA_')\n    \n\nclass GaussianMixtureBlock(BaseBlock):\n    def __init__(self, n_components=3, columns=None, *args, **kwrgs):\n        self.n_components = n_components\n        \n        if columns is None: columns = feature_columns\n        self.columns = columns\n        super().__init__(name='GMM_n={}'.format(n_components), *args, **kwrgs)\n    \n    def fit(self, source_df, y, experiment=None) -> pd.DataFrame:\n        clf = GaussianMixture(n_components=self.n_components)\n        clf.fit(source_df[self.columns].values)\n        self.clf_ = clf\n        return self.transform(source_df)\n    \n    def transform(self, source_df):\n        z = self.clf_.predict_proba(source_df[self.columns])\n        z = np.clip(z, 1e-6,1 - 1e-6)\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix('GMM_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vivid.estimators.boosting.mixins import TunedBoostingBlock\nfrom vivid.estimators.boosting.helpers import get_boosting_parameter_suggestions\nfrom vivid.estimators.boosting.lgbm import LGBMRegressorBlock\nimport lightgbm as lgbm\nfrom vivid.runner import create_runner","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TunedLightGBMRegressorBlock(TunedBoostingBlock):\n    model_class = lgbm.LGBMRegressor\n    default_eval_metric = 'rmse'\n    initial_params = LGBMRegressorBlock.initial_params\n    \n    def generate_model_class_try_params(self, trial):\n        param = get_boosting_parameter_suggestions(trial)\n        param['n_jobs'] = -1\n        return param","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_blocks = [\n    BinningCountBlock(name='BINS', column=feature_columns),\n    CountEncodingBlock(name='CE', column=feature_columns),\n    FilterBlock(name='F', column=feature_columns),\n    PCABlock(n_components=3),\n    GaussianMixtureBlock(n_components=3)\n]\n\n\nrunner = create_runner(blocks=[\n    # normal lightGBM\n    LGBMRegressorBlock(name='normal_lgbm', parent=feature_blocks),\n    \n    # tuned by optuna. 50 rounds.\n    TunedLightGBMRegressorBlock(name='tuned_lgbm', parent=feature_blocks, n_trials=50)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Run Tuning \n\n* only fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_results = runner.fit(train_df[feature_columns], y=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict\ntest_results = runner.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create out-of-fold overview\noof_df = pd.DataFrame()\n\nfor result in oof_results:\n    oof_df[result.block.name] = result.out_df.values[:, 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize Models\n\n* Model Output Correation\n* OOf Distribution\n* sort by RMSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(oof_df.corr(), cmap='Blues', annot=True, fmt='.2f', ax=ax)\nax.set_title('Out of Fold Correlation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\n\nfor result in oof_results:\n    sns.distplot(result.out_df.values[:, 0], ax=ax, label=str(result.block.name))\n\nax.legend()\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vivid.metrics import regression_metrics\n\nscore_df = pd.DataFrame()\n\nfor name, pred in oof_df.T.iterrows():\n    score_i = regression_metrics(y, pred)\n    score_df = score_df.append(pd.Series(score_i, name=name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_df.sort_values('rmse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for result in test_results:\n    out_df = result.out_df\n    \n    sub_df = sample_submission_df.copy()\n    sub_df['target'] = result.out_df.values[:, 0]\n    to = f'/kaggle/working/{str(result.block.name)}_submission.csv'\n    print('save to {}'.format(to))\n    sub_df.to_csv(to, index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}