{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install python-vivid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom vivid.estimators.base import MetaBlock\nfrom vivid.estimators.boosting.mixins import BoostingEarlyStoppingMixin\n\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vivid.features.base import BinningCountBlock\nfrom vivid.features.base import CountEncodingBlock\nfrom vivid.features.base import FilterBlock\n\nfrom vivid.estimators.boosting import XGBRegressorBlock\nfrom vivid.estimators.boosting import LGBMRegressorBlock\nfrom vivid.estimators.boosting.block import create_boosting_seed_blocks\n\nfrom vivid.estimators.linear import TunedRidgeBlock\nfrom vivid.estimators.svm import SVRBlock\nfrom vivid.estimators.ensumble import RFRegressorBlock\nfrom vivid.estimators.base import EnsembleBlock, BaseBlock","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/test.csv')\nsample_submission_df = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv')\n\nfeature_columns = [\n 'cont1',\n 'cont2',\n 'cont3',\n 'cont4',\n 'cont5',\n 'cont6',\n 'cont7',\n 'cont8',\n 'cont9',\n 'cont10',\n 'cont11',\n 'cont12',\n 'cont13',\n 'cont14',    \n]\n\ny = train_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ptitprince","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip freeze | grep seaborn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U seaborn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from ptitprince import RainCloud","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RainCloud(y, orient='h')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.percentile(y, [.5, 1, 10, 20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(10 ** y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = PCA(n_components=2)\nmodel.fit(train_df[feature_columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z = model.transform(train_df[feature_columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\nax.scatter(*z.T, c=y, alpha=.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = GaussianMixture(n_components=3)\nx = train_df[feature_columns].values\nclf.fit(x)\nz = clf.predict_proba(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(pd.DataFrame(np.vstack([z.T, y]).T).sample(400))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vivid.core import BaseBlock","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PCABlock(BaseBlock):\n    def __init__(self, n_components=3, columns=None, *args, **kwrgs):\n        self.n_components = n_components\n        \n        if columns is None: columns = feature_columns\n        self.columns = columns\n        super().__init__(name='pca_n={}'.format(n_components), *args, **kwrgs)\n    \n    def fit(self, source_df, y, experiment=None) -> pd.DataFrame:\n        clf = PCA(n_components=self.n_components)\n        clf.fit(source_df[self.columns].values)\n        self.clf_ = clf\n        return self.transform(source_df)\n    \n    def transform(self, source_df):\n        z = self.clf_.transform(source_df[self.columns])\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix('PCA_')\n    \n\nclass GaussianMixtureBlock(BaseBlock):\n    def __init__(self, n_components=3, columns=None, *args, **kwrgs):\n        self.n_components = n_components\n        \n        if columns is None: columns = feature_columns\n        self.columns = columns\n        super().__init__(name='GMM_n={}'.format(n_components), *args, **kwrgs)\n    \n    def fit(self, source_df, y, experiment=None) -> pd.DataFrame:\n        clf = GaussianMixture(n_components=self.n_components)\n        clf.fit(source_df[self.columns].values)\n        self.clf_ = clf\n        return self.transform(source_df)\n    \n    def transform(self, source_df):\n        z = self.clf_.predict_proba(source_df[self.columns])\n        z = np.clip(z, 1e-6,1 - 1e-6)\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix('GMM_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PCABlock().fit(train_df, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GaussianMixtureBlock().fit(train_df, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vivid.estimators.boosting import LGBMRegressorBlock\nfrom vivid.runner import create_runner","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CatRegressorBlock(BoostingEarlyStoppingMixin, MetaBlock):\n    \"\"\"cat-boost regressor blocks. \"\"\"\n    \n    # use cat-boost regressor \n    model_class = CatBoostRegressor\n    \n    fit_verbose = 100\n    early_stopping_rounds = 200\n    \n    # pass to __init__ \n    initial_params = {\n        'learning_rate': .05,\n        'verbose': 100,\n        'num_boost_round': 20000\n    }\n    \n    def get_fit_params_on_each_fold(self, *args, **kwrgs):\n        \"\"\"create parameters pass to `model.fit` method.\n        > see: https://github.com/nyk510/vivid/blob/master/vivid/estimators/boosting/mixins.py#L15\n        \"\"\"\n        params = super().get_fit_params_on_each_fold(*args, **kwrgs)\n        params['verbose'] = self.fit_verbose\n        \n        # delete keys for lightGBM / XGBoost.\n        remove_keys = [\n            'eval_metric',\n            'callbacks'\n        ]\n        \n        for k in remove_keys:\n            if k in params:\n                del params[k]\n        return params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_blocks = [\n    BinningCountBlock(name='BINS', column=feature_columns),\n    CountEncodingBlock(name='CE', column=feature_columns),\n    FilterBlock(name='F', column=feature_columns),\n    PCABlock(n_components=3),\n    GaussianMixtureBlock(n_components=3)\n]\n\n\nrunner = create_runner(blocks=[\n    CatRegressorBlock(name='cat', parent=feature_blocks)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cutted = np.clip(y, 5, np.inf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_results = runner.fit(train_df[feature_columns], y=y_cutted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls -lat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* cont13 / 3 / 4 is very importance feature. \n* ground truth vs out-of-fold predict is not similar ;(\n    * y is long tail dist but the objective and metric is `RMSE`, it assumes the noise is normal distribution."},{"metadata":{},"cell_type":"markdown","source":"### Modeling\n\ncreate more complex models"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FillnaBlock(BaseBlock):\n    def fit(self, source_df, y, experiment) -> pd.DataFrame:\n        self.fill_values_ = source_df.dropna().median()\n        return self.transform(source_df)\n    \n    def transform(self, source_df):\n        return source_df.fillna(self.fill_values_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_blocks = [\n    BinningCountBlock(name='BINS', column=feature_columns),\n    CountEncodingBlock(name='CE', column=feature_columns),\n    FilterBlock(name='F', column=feature_columns),\n    PCABlock(n_components=3),\n    GaussianMixtureBlock(n_components=3)\n]\n\n\nfilled_feature_block = FillnaBlock(name='FNA', parent=feature_blocks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"single_models = [\n    create_boosting_seed_blocks(feature_class=XGBRegressorBlock, \n                                prefix='xgb_', \n                                parent=feature_blocks),\n    create_boosting_seed_blocks(feature_class=LGBMRegressorBlock, \n                                prefix='lgb_', \n                                parent=feature_blocks),\n    CatRegressorBlock(name='cat', parent=feature_blocks),\n    RFRegressorBlock(name='rf', parent=filled_feature_block),\n    TunedRidgeBlock(name='ridge', \n                    add_init_param={ 'target_scaling': 'standard' },\n                    n_trials=30, \n                    parent=filled_feature_block)\n]\n\nstacking_models = [\n    EnsembleBlock(prefix='ens', parent=single_models),\n    TunedRidgeBlock(name='stacking_ridge', n_trials=30, parent=single_models, \n                    add_init_param={ 'target_scaling': 'standard' }),\n    LGBMRegressorBlock(name='stacked_lgb', parent=[*single_models, *feature_blocks]),\n    CatRegressorBlock(name='stacked_cat', parent=[*single_models, *feature_blocks]),\n    XGBRegressorBlock(name='stacked_xgb', parent=[*single_models, *feature_blocks])\n]\n\ntwo_stage_stacking_models = [\n    TunedRidgeBlock(name='stage-2_ridge', n_trials=30, parent=stacking_models, \n                    add_init_param={ 'target_scaling': 'standard' },)\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vivid.backends.experiments import LocalExperimentBackend\n\nrunner = create_runner(two_stage_stacking_models, \n                       experiment=LocalExperimentBackend(to='/kaggle/working/'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train models\noof_results = runner.fit(train_df, y=y_cutted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict\ntest_results = runner.predict(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create out-of-fold overview\noof_df = pd.DataFrame()\n\nfor result in oof_results:\n    oof_df[result.block.name] = result.out_df.values[:, 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize Models\n\n* Model Output Correation\n* OOf Distribution\n* sort by RMSE"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(oof_df.corr(), cmap='Blues', annot=True, fmt='.2f', ax=ax)\nax.set_title('Out of Fold Correlation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 6))\n\nfor result in oof_results:\n    sns.distplot(result.out_df.values[:, 0], ax=ax, label=str(result.block.name))\n\nax.legend()\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vivid.metrics import regression_metrics\n\nscore_df = pd.DataFrame()\n\nfor name, pred in oof_df.T.iterrows():\n    score_i = regression_metrics(y, pred)\n    score_df = score_df.append(pd.Series(score_i, name=name))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_df.sort_values('rmse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for result in test_results:\n    out_df = result.out_df\n    \n    sub_df = sample_submission_df.copy()\n    sub_df['target'] = result.out_df.values[:, 0]\n    to = f'/kaggle/working/{str(result.block.name)}_submission.csv'\n    print('save to {}'.format(to))\n    sub_df.to_csv(to, index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}