{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import librareis & data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import StackingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import VotingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv')\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train.isnull(), cbar = False);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.hist(bins = 30, figsize = (20,20), color = 'b')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = train.corr()\nf, ax = plt.subplots(figsize = (20,20))\nsns.heatmap(correlations, annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlations = train.corr()['target'].sort_values()\ncorrelations","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(['id','target'], axis = 1)\ny = train['target']\ntest = test.drop(['id'], axis= 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Paramenters XGBoost, LGBM, CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/hamzaghanmi/xgboost-hyperparameter-tuning-using-optuna\n\nparam_xgb = {'lambda': 0.0030282073258141168, \n             'alpha': 0.01563845128469084, \n             'colsample_bytree': 0.5,\n             'subsample': 0.7,\n             'n_estimators': 1000, \n             'learning_rate': 0.01,\n             'max_depth': 15,\n             'random_state': 2020, \n             'min_child_weight': 257,\n             'tree_method':'gpu_hist',\n             'predictor': 'gpu_predictor'}\n\n# parameters from this awesome kernal \n# https://www.kaggle.com/hamditarek/tabular-playground-series-xgboost-lightgbm\nparam_lgbm ={'random_state': 33,'n_estimators':1000,\n 'min_data_per_group': 5,\n 'boosting_type': 'gbdt',\n 'device_type' : 'gpu',\n 'num_leaves': 256,\n 'num_iterations' : 1000,\n 'max_dept': -1,\n 'learning_rate': 0.005,\n 'subsample_for_bin': 200000,\n 'lambda_l1': 1.074622455507616e-05,\n 'lambda_l2': 2.0521330798729704e-06,\n 'n_jobs': -1,\n 'cat_smooth': 1.0,\n 'silent': True,\n 'importance_type': 'split',\n 'metric': 'rmse',\n 'feature_pre_filter': False,\n 'bagging_fraction': 0.8206341150202605,\n 'min_data_in_leaf': 100,\n 'min_sum_hessian_in_leaf': 0.001,\n 'bagging_freq': 6,\n 'feature_fraction': 0.5,\n 'min_gain_to_split': 0.0,\n 'min_child_samples': 20}\n\n#https://www.kaggle.com/bhavikjain/lgbm-xgboost-catboost-tabular-playground-series\nparam_cat = {'iterations': 1000,\n                             'learning_rate': 0.05,\n                             'depth': 10,\n                             'reg_lambda': 10,\n                             'eval_metric': 'RMSE',\n                             'bagging_temperature': 0.2,\n                             'od_type': 'Iter',\n                             'metric_period': 50,\n                             'od_wait': 20\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nkf = KFold(n_splits=5, shuffle=True, random_state=7)\n\noof_preds = np.zeros(X.shape[0])\nsub_preds = np.zeros(test.shape[0])\n\nfor n_fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n    X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n    level10 = list()\n    level10.append(('cbr', CatBoostRegressor(**param_cat, task_type='GPU')))\n    level10.append(('lgr', LGBMRegressor(**param_lgbm)))\n    level10.append(('xgb', XGBRegressor(**param_xgb)))\n    level10.append(('mlp', MLPRegressor()))\n    \n    level1 = VotingRegressor(estimators = level10, n_jobs = -1)\n\n    model = StackingRegressor(estimators=level10, final_estimator=level1, cv=5)\n    model.fit(X_train, y_train)\n    oof_preds[val_idx] = model.predict(X_valid)\n    pred_xgb = model.predict(test)\n    sub_preds += pred_xgb / kf.n_splits\n\nprint(np.sqrt(mean_squared_error(y, oof_preds)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make submission\nsample_submission['target'] = sub_preds\nsample_submission.to_csv(\"stacking_submission.csv\", index=False)\nsample_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}