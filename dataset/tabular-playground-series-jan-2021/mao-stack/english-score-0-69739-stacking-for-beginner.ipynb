{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 超初心者向けにスタッキングのやり方を実装したNotebookです。\n私自身が最近Kaggleを始めたばかりなので、コードをきれいに書くことができません。玄人の方からしたら、いかにも初心者な書き方をしていると思います。しかし、逆にその分、初心者の方には読みやすいコードになっていると思います。"},{"metadata":{},"cell_type":"markdown","source":"# This is a Notebook that implements how to do stacking for serious beginners.\nI've only recently started Kaggle, so I can't write code well. For an expert user, I think the code is written in a very beginner-like way. But on the other hand, I think it makes the code easy to understand for beginners."},{"metadata":{},"cell_type":"markdown","source":"### If you find it helpful, I would appreciate it if you could upvote it...  \n### 参考になりましたら、Upvoteしていただけると幸いです、、、"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-jan-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-jan-2021/test.csv\")\n\ntrain.drop([170514], axis=0, inplace=True)\n\nX = np.array(train.drop([\"id\", \"target\"], axis=1))\nX_test = np.array(test.drop(\"id\", axis=1))\ny = np.array(train[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n2層のスタッキングを行う。two-layer stacking."},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error as mse\n\ndef RMSE(pred, true):\n    return np.sqrt(mse(pred, true))\n\nSEED = 2039","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# このセルでスタッキングに使う関数を定義している。\n# Define the function to be used for stacking in this cell.\n\n# stack_xgb, stack_lgbは次の層に入力する学習データとテストデータを返す関数。\n# stack_xgb and stack_lgb are functions that return the training and test data to be input to the next layer.\ndef stack_xgb(X, X_test, y, params):\n    \n    SEED = random.randint(0, 100)\n    kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n    \n    # 次の層の学習データとなるものを格納するリスト\n    # A list to store what will be the training data for the next layer\n    next_train = []\n    \n    # 次の層のテストデータとなるものを格納するリスト\n    # A list to store what will be the test data for the next layer\n    next_test = []\n    \n    # validationデータに指定されたデータの行番号を格納するリスト\n    # A list that stores the indices of the data specified in the validation data.\n    vl_ids = []\n    \n    d_test = xgb.DMatrix(X_test)\n    \n    for tr_id, vl_id in kf.split(X, y):\n        \n        X_train, X_val = X[tr_id, :], X[vl_id, :]\n        y_train, y_val = y[tr_id], y[vl_id]\n        \n        d_train = xgb.DMatrix(X_train, label=y_train)\n        d_val = xgb.DMatrix(X_val, label=y_val)\n\n        model = xgb.train(params=params,\n                          dtrain=d_train,\n                          num_boost_round=10000,\n                          early_stopping_rounds=100,\n                          verbose_eval=0,\n                          evals=[(d_val, \"val\")])\n        \n        # 検証用データに対して予測を行う。これが次の層の入力データ（次の層にとっての学習データ）となる\n        # Make predictions on the validation data. This will be the input data for the next layer (training data for the next layer).\n        pred_train = model.predict(d_val, ntree_limit=model.best_ntree_limit)\n        \n        # 予測したものをnext_trainに追加していく\n        # Add the predictions to the next_train\n        next_train.append(pred_train)\n        \n        # 検証用データに選ばれたデータのindex番号をリストに追加していく\n        # Add the index number of the data selected for verification to the list.\n        vl_ids.append(vl_id)\n        \n        # テストデータに対して予測を行う。これが次の層の入力データ（次の層にとってのテストデータ）となる\n        # Make predictions on the test data. This will be the input data for the next layer (the test data for the next layer).\n        pred_test = model.predict(d_test, ntree_limit=model.best_ntree_limit)\n        \n        # 予測したものをnext_testに追加していく。\n        # Add the predictions to the next_test.\n        next_test.append(pred_test)\n        \n    # とりあえずこの操作をしておく。\n    # I'll do this operation for now.\n    vl_ids = np.concatenate(vl_ids)\n    next_train = np.concatenate(next_train, axis=0)\n    \n    # KFoldのshuffle=Trueによって行番号がバラバラになっているので、それをここで修正する。\n    # KFold's shuffle=True is causing the line numbers to be broken up, so we'll fix that here.\n    order = np.argsort(vl_ids)\n    next_train = next_train[order]\n    \n    # KFoldを使ったことによって、テストデータに対する予測がn_splits=10回分行われているので、平均をとって1つにする。\n    # By using KFold, we have made \"n_splits=10\" predictions for the test data, so we will average them into one.\n    next_test = np.mean(next_test, axis=0)\n    \n    # pd.Series型で返す\n    # Return as pd.Series type\n    return pd.Series(next_train), pd.Series(next_test)\n\n\n# 上記のstack_xgbと同様\n# Same as stack_xgb above\ndef stack_lgb(X, X_test, y, params):\n    \n    SEED = random.randint(0, 100)\n    kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n    \n    next_train = []\n    next_test = []\n    vl_ids = []\n    \n    for tr_id, vl_id in kf.split(X, y):\n        \n        X_train, X_val = X[tr_id, :], X[vl_id, :]\n        y_train, y_val = y[tr_id], y[vl_id]\n\n        lgb_train = lgb.Dataset(X_train, label=y_train)\n        lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n\n        model = lgb.train(params=params,\n                          train_set=lgb_train,\n                          valid_sets=(lgb_train, lgb_val),\n                          num_boost_round=10000,\n                          early_stopping_rounds=100,\n                          verbose_eval=0)\n        \n        pred_train = model.predict(X_val, num_iteration=model.best_iteration)\n        next_train.append(pred_train)\n        \n        pred_test = model.predict(X_test, num_iteration=model.best_iteration)\n        next_test.append(pred_test)\n        \n        vl_ids.append(vl_id)\n        \n    vl_ids = np.concatenate(vl_ids)\n    next_train = np.concatenate(next_train, axis=0)\n    order = np.argsort(vl_ids)\n    next_train = next_train[order]\n    \n    next_test = np.mean(next_test, axis=0)\n    \n    return pd.Series(next_train), pd.Series(next_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Layer 1 / 1層目\nThe input for the first layer is the original training data X, y, X_test.  \n1層目の入力はもともとの学習データのX, y, X_test。"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_xgb11 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 15,\n    \"eta\": 0.02,\n    \"gamma\": 0.005346636874993822,\n    \"colsample_bytree\": 0.5,\n    \"subsample\": 0.7,\n    \"min_child_weight\": 257,\n    \"alpha\": 0.01563,\n    \"lambda\": 0.003,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_xgb12 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 20,\n    \"eta\": 0.02,\n    \"colsample_bytree\": 1.0,\n    \"subsample\": 0.5,\n    \"min_child_weight\": 100,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_xgb13 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 10,\n    \"eta\": 0.02,\n    \"colsample_bytree\": 0.5,\n    \"subsample\": 0.8,\n    \"min_child_weight\": 20,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_xgb14 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 6,\n    \"eta\": 0.01,\n    \"colsample_bytree\": 1.0,\n    \"subsample\": 0.8,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_lgb11 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 256,\n    \"bagging_fraction\": 0.8206341150202605,\n    \"feature_fraction\": 0.5,\n    \"min_data_in_leaf\": 100,\n    \"lambda_l1\": 1.074622455507616e-05,\n    \"lambda_l2\": 2.0521330798729704e-06,\n    \"min_data_per_group\": 5,\n    \"max_depth\": -1,\n    \"subsample_for_bin\": 200000,\n    \"cat_smooth\": 1.0,\n    \"importance_type\": \"split\",\n    \"min_sum_hessian_in_leaf\": 0.001,\n    \"bagging_freq\": 6,\n    \"min_gain_to_split\": 0.0,\n    \"random_state\": SEED\n}\n\nparams_lgb12 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 256,\n    \"max_depth\": 15,\n    \"bagging_fraction\": 0.8206341150202605,\n    \"feature_fraction\": 0.5,\n    \"min_data_in_leaf\": 100,\n    \"lambda_l1\": 1.074622455507616e-05,\n    \"lambda_l2\": 2.0521330798729704e-06,\n    \"min_data_per_group\": 5,\n    \"max_depth\": -1,\n    \"subsample_for_bin\": 200000,\n    \"cat_smooth\": 1.0,\n    \"importance_type\": \"split\",\n    \"min_sum_hessian_in_leaf\": 0.001,\n    \"bagging_freq\": 6,\n    \"min_gain_to_split\": 0.0,\n    \"random_state\": SEED\n}\n\nparams_lgb13 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 50,\n    \"bagging_fraction\": 0.5,\n    \"feature_fraction\": 0.8,\n    \"random_state\": SEED\n}\n\nparams_lgb14 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 100,\n    \"max_depth\": 7,\n    \"bagging_fraction\": 0.8,\n    \"feature_fraction\": 1.0,\n    \"random_state\": SEED\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_train_11, next_test_11 = stack_xgb(X, X_test, y, params_xgb11)\nnext_train_12, next_test_12 = stack_xgb(X, X_test, y, params_xgb12)\nnext_train_13, next_test_13 = stack_xgb(X, X_test, y, params_xgb13)\nnext_train_14, next_test_14 = stack_xgb(X, X_test, y, params_xgb14)\nnext_train_15, next_test_15 = stack_lgb(X, X_test, y, params_lgb11)\nnext_train_16, next_test_16 = stack_lgb(X, X_test, y, params_lgb12)\nnext_train_17, next_test_17 = stack_lgb(X, X_test, y, params_lgb13)\nnext_train_18, next_test_18 = stack_lgb(X, X_test, y, params_lgb14)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_train_1 = pd.concat([next_train_11, next_train_12,\n                          next_train_13, next_train_14,\n                          next_train_15, next_train_16,\n                          next_train_17, next_train_18], axis=1)\n\nnext_test_1 = pd.concat([next_test_11, next_test_12,\n                         next_test_13, next_test_14,\n                         next_test_15, next_test_16,\n                         next_test_17, next_test_18], axis=1)\n\nnext_train_1 = np.array(next_train_1)\nnext_test_1 = np.array(next_test_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Layer 2 / 2層目\nThe input of the second layer is the output of the first layer, next_train_1, next_test_1, and so on.  \n2層目の入力は1層目の出力であるnext_train_1, next_test_1たち。"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_xgb21 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 15,\n    \"eta\": 0.01,\n    \"gamma\": 0.004,\n    \"colsample_bytree\": 0.9,\n    \"subsample\": 0.7,\n    \"min_child_weight\": 200,\n    \"alpha\": 0.005,\n    \"lambda\": 0.001,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_xgb22 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 8,\n    \"eta\": 0.01,\n    \"gamma\": 0.0006,\n    \"colsample_bytree\": 0.9,\n    \"subsample\": 0.5,\n    \"min_child_weight\": 50,\n    \"alpha\": 0.0004,\n    \"lambda\": 0.00002,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_xgb23 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 6,\n    \"eta\": 0.01,\n    \"colsample_bytree\": 1.0,\n    \"subsample\": 0.95,\n    \"min_child_weight\": 10,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_lgb21 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 256,\n    \"max_depth\": 15,\n    \"bagging_fraction\": 0.8,\n    \"feature_fraction\": 0.8,\n    \"min_data_in_leaf\": 10,\n    \"lambda_l1\": 0.00003,\n    \"lambda_l2\": 0.000035,\n    \"random_state\": SEED\n}\n\nparams_lgb22 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.01,\n    \"num_leaves\": 50,\n    \"max_depth\": 20,\n    \"bagging_fraction\": 0.8,\n    \"feature_fraction\": 1.0,\n    \"random_state\": SEED\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_train_21, next_test_21 = stack_xgb(next_train_1, next_test_1, y, params_xgb21)\nnext_train_22, next_test_22 = stack_xgb(next_train_1, next_test_1, y, params_xgb22)\nnext_train_23, next_test_23 = stack_xgb(next_train_1, next_test_1, y, params_xgb23)\nnext_train_24, next_test_24 = stack_lgb(next_train_1, next_test_1, y, params_lgb21)\nnext_train_25, next_test_25 = stack_lgb(next_train_1, next_test_1, y, params_lgb22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_train_2 = pd.concat([next_train_21, next_train_22, next_train_23,\n                          next_train_24, next_train_25], axis=1)\nnext_test_2 = pd.concat([next_test_21, next_test_22, next_test_23,\n                         next_test_24, next_test_25], axis=1)\n\nnext_train_2 = np.array(next_train_2)\nnext_test_2 = np.array(next_test_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Last Layer / 最後の層\nThe inputs of the last layer are the outputs of the second layer, next_train_2 and next_test_2. In the last layer, we use KFold to predict as usual.  \n最後の層の入力は2層目の出力であるnext_train_2, next_test_2たち。最後の層ではKFoldを使って普通に予測する。"},{"metadata":{"trusted":true},"cell_type":"code","source":"params_xgb = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"eta\": 0.01,\n    \"max_depth\": 4,\n    \"gamma\": 0.09192773718666183, \n    \"colsample_bytree\": 0.6961218462820887,\n    \"subsample\": 0.9987425774067743,\n    \"min_child_weight\": 0.1137086502328514,\n    \"alpha\": 0.9245596765233609,\n    \"lambda\": 6.230027264411933e-06,\n    \"seed\": SEED\n}\n\nparams_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.01,\n    \"num_leaves\": 164,\n    \"max_depth\": 4,\n    \"bagging_fraction\": 0.8498649731960014,\n    \"feature_fraction\": 0.5016568140931941,\n    \"min_data_in_leaf\": 200,\n    \"lambda_l1\": 0.00023282363705273031,\n    \"lambda_l2\": 4.693349803533469e-08,\n    \"random_state\": SEED\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=10, shuffle=True, random_state=SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_xgb = pd.DataFrame()\nd_test = xgb.DMatrix(next_test_2)\n\nfor tr_id, vl_id in kf.split(next_train_2, y):\n    \n    X_train, X_val = next_train_2[tr_id, :], next_train_2[vl_id, :]\n    y_train, y_val = y[tr_id], y[vl_id]\n    \n    d_train = xgb.DMatrix(X_train, label=y_train)\n    d_val = xgb.DMatrix(X_val, label=y_val)\n    \n    model = xgb.train(params=params_xgb,\n                      dtrain=d_train,\n                      num_boost_round=10000,\n                      early_stopping_rounds=100,\n                      verbose_eval=0,\n                      evals=[(d_val, \"val\")])\n    \n    pred = model.predict(d_test, ntree_limit=model.best_ntree_limit)\n    pred = pd.Series(pred)\n    pred_xgb = pd.concat([pred_xgb, pred], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_xgb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgb = pd.DataFrame()\n\nfor tr_id, vl_id in kf.split(next_train_2, y):\n    \n    X_train, X_val = next_train_2[tr_id, :], next_train_2[vl_id, :]\n    y_train, y_val = y[tr_id], y[vl_id]\n    \n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n\n    model = lgb.train(params=params_lgb,\n                      train_set=lgb_train,\n                      valid_sets=(lgb_train, lgb_val),\n                      num_boost_round=10000,\n                      early_stopping_rounds=100,\n                      verbose_eval=0)\n    \n    pred = model.predict(next_test_2, num_iteration=model.best_iteration)\n    pred = pd.Series(pred)\n    pred_lgb = pd.concat([pred_lgb, pred], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_lgb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pd.concat([pred_xgb, pred_lgb], axis=1)\npred = pred.mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv(\"../input/tabular-playground-series-jan-2021/sample_submission.csv\")\nsub = sample_sub.copy()\n\nsub[\"target\"] = pred\nsub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission_stackingGBDTs.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}