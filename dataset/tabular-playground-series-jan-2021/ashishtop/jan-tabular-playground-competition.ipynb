{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, you will learn how to make your first submission to the **[Tabular Playground Series - Jan 2021](https://admin.kaggle.com/c/tabular-playground-series-jan-2021/overview)** competition. \n\nThis notebook will help get into top 30% solutions."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n        \ninput_path = Path('/kaggle/input/tabular-playground-series-jan-2021/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read in the data files"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv')\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(input_path / 'test.csv')\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(input_path / 'sample_submission.csv')\ndisplay(submission.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nfrom  sklearn.model_selection import train_test_split , StratifiedKFold\n\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV , StratifiedKFold , KFold\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\n\n\n# the scaler - for standardisation\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Get Basic Details of the data files\nprint('Train data Shape : ' , train.shape)\nprint('Test data Shape : ' , test.shape)\nprint('Submission data Shape : ' , submission.shape)\n\nprint('#########################################')\nprint('Null Data Details - Train data')\nprint(train.isnull().sum())\n\nprint('#########################################')\nprint('Null Data Details - Test data')\nprint(test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observations\n1. There is no null data in train or test dataset\n2. All features are numerical"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train Data Set -- >')\nprint(train.info())\nprint('######################################')\nprint('Test Dataset ====> ')\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Descriptive Statistics\ntrain.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Observation \n1. Some features have negative values\n2. Most features have values ranging from 0 to 1 \n3. Target feature has some records with 0 value. "},{"metadata":{"trusted":true},"cell_type":"code","source":"### Lets check the distribution of features\nfor i in train.columns :\n    sns.distplot(train[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'target'\nId_Cols = ['id' ]\nfeatures = [x for x in train.columns if x not in [target]+Id_Cols]\nX = train[features]\ny = train['target']\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Function to run alogorithm with Cross validation\ndef RunForAll(algo , k , train , test , features , params):\n    \n    ## Create matrix of zeros\n    val_set_pred = np.zeros(len(train))\n    test_set_pred = np.zeros(len(test))\n    #y_val = np.zeros(len(train))\n    \n    X = train[features]\n    y = train['target']\n    \n    kf = KFold(n_splits=k)\n    \n    for fold_, (train_index, val_index) in enumerate(kf.split(train , train['target'])):\n        print(f'\\n ---------------------Fold {fold_ + 1}-----------------')\n        \n        target = train['target']\n        X_train , y_train = train[features].iloc[train_index] , target.iloc[train_index]\n        X_val , y_val = train[features].iloc[val_index] , target.iloc[val_index]\n        \n        #New\n        X_train = X_train.abs()\n        X_test = test[features].abs()\n        \n        _ = algo.fit(X_train , y_train , eval_set = [(X_val , y_val)] , **params)\n        \n        prediction_val = algo.predict(X_val)\n        \n        kf_score = np.sqrt(mean_squared_error(y_val , prediction_val))\n        print(f'\\n Score For Validation Sample is {kf_score}')\n        \n        val_set_pred[val_index] = prediction_val\n        y_val = target.iloc[val_index]\n        #Predict for test \n        prediction_test = algo.predict(X_test)\n        test_set_pred += prediction_test / k\n        \n    val_score = np.sqrt(mean_squared_error(target,val_set_pred  ))\n    print(f'\\n Score for Validation set is {val_score}')\n    \n    return val_set_pred , test_set_pred , target        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb=XGBRegressor(n_estimators = 4000 , learning_rate = 0.011  )\nparams = {'verbose' : False , 'early_stopping_rounds' : 100}\ntarget = 'target'\nId_Cols = ['id' ]\n\nfeatures = [x for x in train.columns if x not in [target]+Id_Cols]\n\nxgb_val_pred , xgb_test_preds , y_val_xgb = RunForAll(xgb,5, train , test , features , params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef1 = pd.Series(xgb.feature_importances_, features).sort_values()\ncoef1.plot(kind='bar' , title = 'Model Coefficient')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_val_pred , xgb_test_preds , y_val_xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\n#lgb = LGBMRegressor(n_estimators = 8000 , importance_type = 'gain' ,  learning_rate = 0.001 , tree_method= gpu_hist ,\n#             predictor= gpu_predictor)\n\nlgb = LGBMRegressor(n_estimators = 100000, metric = 'rmse' ,  learning_rate = 0.01 , boosting_type = 'gbdt' ,\n             num_leaves = 200 , feature_fraction = 0.6 , lambda_l1 = 2 , lambda_l2 = 2 , min_child_samples = 50 , bagging_fraction = 0.4 ,\n                   bagging_freq = 1 , verbosity=-1 , max_depth = 12 , max_bin = 200 ,\n                   objective = 'regression')\n\nparams = {'verbose' : 100 ,  'early_stopping_rounds' : 1000 }\ntarget = 'target'\nId_Cols = ['id' ]\n\nfeatures = [x for x in train.columns if x not in [target]+Id_Cols]\n\nlgb_val_pred , lgb_test_preds , y_val_lgb = RunForAll(lgb,5, train , test , features , params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef1 = pd.Series(lgb.feature_importances_, features).sort_values()\ncoef1.plot(kind='bar' , title = 'Model Coefficient')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\n\ncb = CatBoostRegressor(n_estimators = 5000 ,  learning_rate = 0.005 )\n\nparams = {'verbose' : False , 'early_stopping_rounds' : 100}\ntarget = 'target'\nId_Cols = ['id' ]\n\nfeatures = [x for x in train.columns if x not in [target]+Id_Cols]\n\ncb_val_pred , cb_test_preds , y_val_cb = RunForAll(cb,5, train , test , features , params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = cb_test_preds\noutput = pd.DataFrame({'id' : test.id , 'target' : test.target})\noutput.to_csv('submission_cb.csv' , index=False)\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Lets Try to use Ensemble technique"},{"metadata":{"trusted":true},"cell_type":"code","source":"#target = train['target']\nensemble_df = pd.DataFrame(xgb_val_pred , columns=['xgboost'])\n\nensemble_df['lgboost'] = lgb_val_pred\nensemble_df['cbboost'] = cb_val_pred\n#ensemble_df['label'] = y_val_xgb\nensemble_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_test_df = pd.DataFrame(xgb_test_preds , columns=['xgboost'])\nensemble_test_df['lgboost'] = lgb_test_preds\nensemble_test_df['cbboost'] = cb_test_preds\nensemble_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = ensemble_df\ny = y_val_xgb\nreg = LinearRegression().fit(X, y)\nprint(reg.score(X, y))\nprint(reg.coef_)\nprint(reg.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_preds = reg.predict(ensemble_test_df)\nensemble_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = ensemble_preds\noutput = pd.DataFrame({'id' : test.id , 'target' : test.target})\noutput.to_csv('submission.csv' , index=False)\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = cb_test_preds\noutput = pd.DataFrame({'id' : test.id , 'target' : test.target})\noutput.to_csv('submission_1.csv' , index=False)\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nX = ensemble_df\ny = y_val_xgb\ndregr = DecisionTreeRegressor(max_depth=5)\ndregr.fit(X, y)\nensemble_preds_dr = dregr.predict(ensemble_test_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = ensemble_preds_dr\noutput = pd.DataFrame({'id' : test.id , 'target' : test.target})\noutput.to_csv('submission_2.csv' , index=False)\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nX = ensemble_df\ny = y_val_xgb\nrfregr = RandomForestRegressor(n_estimators = 200 , max_depth=5 , min_samples_leaf=100 , n_jobs=4)\nrfregr.fit(X, y)\nensemble_preds_rf = rfregr.predict(ensemble_test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['target'] = ensemble_preds_rf\noutput = pd.DataFrame({'id' : test.id , 'target' : test.target})\noutput.to_csv('submission_3.csv' , index=False)\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}