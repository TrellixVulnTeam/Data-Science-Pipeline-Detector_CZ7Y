{"cells":[{"metadata":{},"cell_type":"markdown","source":"To see part I go [here](https://www.kaggle.com/jth359/deep-dive-into-ml-interpretability-part-i/)\n\n\n## Model Agnostic Interpretability Techniques\n\nAll of the previous examples were dependent on the specific model (for example we could not do a feature importance on KNN). Model agnostic interpretability techniques can be used on any model.\n\n### Partial Dependency Plots\n\nPartial Dependency Plots show the impact of changing a single feature on the predicted value \n\n\nTo start I am going to look at the partial dependency plot for the random forest on the feature `cont2` (since it appeared to be the important feature) "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load in dataset \ndf = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\n\n# set id to be the index \ndf.set_index('id', inplace = True)\n\n# begin with a train test split \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\n\nX = df.drop('target', axis = 1)\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11)\n\n# make instance of model\nrf = RandomForestRegressor()\n\n# fit model\nrf.fit(X_train, y_train)\n\n# check root mean squared error \nnp.sqrt(mean_squared_error(y_test, rf.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import plot_partial_dependence\n\n# look at the partial \nplot_partial_dependence(rf, X_train, [1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that at around 3.5 the prediction jumps from 7.84 to 7.94.  If this was a feature that we were familiar with we could validate that this jump in predicted value makes sense or not.  \n\nDue to partial dependency plots being model agnostic, we can also use them on neural networks.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasRegressor\nimport keras\n\n# make Neural net \ndef neural_net():\n    model = keras.Sequential([\n          keras.layers.Dense(64, activation='relu', input_shape=[14]),\n          keras.layers.Dense(32, activation='relu'),\n          keras.layers.Dense(1, activation = 'linear')\n          ])\n\n    model.compile(loss='mse',\n          optimizer='adam')\n    \n    return model\n\n# make neural net with keras wrapper (so I can use partial dependency plotter)\nkr = KerasRegressor(build_fn=neural_net ,verbose=0)\nkr._estimator_type = \"regressor\"\nkr.fit(X_train,y_train, epochs=10, validation_data = (X_test, y_test))\n# needed for partial dependency plotter (I have no dummy variables)\nkr.dummy_ = None\n\n# check RMSE \nnp.sqrt(mean_squared_error(y_test, kr.predict(X_test))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot partial dependence plot for the first feature\nplot_partial_dependence(kr,X_train,[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see the neural network PDP is very different than the random forest PDP.  The neural network shows `cont2` having a positive lienar relationship until about 0.55 and then levels off.  This shows how the different models think the relationship between `cont2` and the target variable is.  \n\n### Individual Conditional Expectation (ICE) Plot\n\nAn ICE plot is very similar to a PDP.  A PDP plot takes the average of each sample, while an ICE plot shows the impact on each sample.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pdpbox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pdpbox import pdp\n\npdp_weekofyear = pdp.pdp_isolate(\nmodel=rf, dataset=X_train, model_features=X_train.columns, feature='cont2'\n)\nfig, axes = pdp.pdp_plot(pdp_weekofyear, 'cont2', plot_lines = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset is rather large, so a PDP would probably be a better use case.  But the ICE plot does show the variance in change as the `cont2` feature increases.  "},{"metadata":{},"cell_type":"markdown","source":"### Permutation Importance \n\nPermutation feature importance measures the increase in the prediction error of the model after we permuted the feature's values, which breaks the relationship between the feature and the true outcome.<br> \n[Source](https://christophm.github.io/interpretable-ml-book/feature-importance.html)\n\nSince I was not able to do feature importance earlier on a neural network, I am going to use permutation importance on my neural network.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.inspection import permutation_importance\nimport matplotlib.pyplot as plt \n\nresult = permutation_importance(kr, X_test, y_test, n_repeats=10,\n                                random_state=11)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=X_test.columns[sorted_idx])\nax.set_title(\"Permutation Importances (test set)\")\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}