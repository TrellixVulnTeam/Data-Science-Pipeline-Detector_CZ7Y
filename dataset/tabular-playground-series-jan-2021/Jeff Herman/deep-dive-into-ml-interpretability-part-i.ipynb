{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Interpretability \n\nHaving a model that you understand and trust is very important.  There is typically a tradeoff between performance of the model and interpretability \n![](https://miro.medium.com/max/1950/1*shNOspLyVn_2mvwves9MMA.png)\n[Image Source](https://medium.com/ansaro-blog/interpreting-machine-learning-models-1234d735d6c9)\n\nI am going to go through some techniques for interpretting your model - starting with linear regression.  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load in dataset \ndf = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# set id to be the index \ndf.set_index('id', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# begin with a train test split \nfrom sklearn.model_selection import train_test_split\n\nX = df.drop('target', axis = 1)\ny = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression\n\nI am going to use the [statsmodels](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.html) library to run my regression.  I like to use statsmodels because it gives you access to a lot of extra information that sklearn does not provide natively.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n\n# ad y intercept\nX_train = sm.add_constant(X_train)\n\nmodel = sm.OLS(y_train, X_train).fit()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I see that I have a $R^2$ value of 0.2 which indicates that I do not have a good fit, I have one feature that is not statistically significant at an alpha level 0f 0.05 `cont14`, and I have a Jarque-Bera value that is large indicating that my residuals are not normally distributed.  \n\n> For the purposes of this notebook, I am just going to focus on interpreting the current model and not addresses any of the above issues.  \n\nNow I want to see what the Root Mean Squarred Error is for this model "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# add y_intercept to X_test\nX_test = sm.add_constant(X_test)\n\nnp.sqrt(mean_squared_error(y_test, model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Global Interpretability \n\nTo start I am going to look at some global interpretability techniques.  Global interpretability looks at how the model makes decisions in general.  \n\n### Feature Importance \n\nThe importance of a feature in a linear regression model can be measured by the absolute value of its t-statistic.\n[Source](https://christophm.github.io/interpretable-ml-book/limo.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport seaborn as sns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('fivethirtyeight'):\n    abs(model.tvalues)[1:].sort_values().plot(kind = 'barh', figsize = (8, 6))\n    plt.title('Most Important Features According to Linear Regression')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that `cont10` is the feature that our model thinks is the most important feature \n\n### Weight Plot \n\nA weight plot shows the coefficients with the 95% confidence intervals for each coefficient"},{"metadata":{"trusted":true},"cell_type":"code","source":"# first make a dataframe with coefficents and 95% confidence interval \n\nwp = model.conf_int()\nwp.columns = ['lower_bound', 'upper_bound']\nwp['coefficient'] = model.params\nwp.drop('const', inplace = True)\nwp['error'] = np.abs(wp['coefficient'] - wp['upper_bound'])\n\nwp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (8, 8))\nplt.errorbar(wp['coefficient'], range(len(wp)), fmt='o', color='black', xerr = wp['error'], \n            ecolor='lightgray', elinewidth=3, capsize=0)\nplt.axvline(0, color = 'black', linestyle = '--', alpha = 0.8)\nplt.title('Weight Plot')\nplt.yticks(range(len(wp)), wp.index);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows which direction each feature impacts the model.  We see that `cont10` has a strong negative impact on the model and `cont6` has a strong positive impact on the model.  Something else of note - we see that `cont14` crosses over the 0 line, this is because the feature is not statistically significant.  \n\n### Effect Plot\n\nThe weights of the linear regression model can be more meaningfully analyzed when they are multiplied by the actual feature values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# make new dataframe that multiples the coefficient by the raw values \neffect_plot_df = pd.DataFrame()\n\nfor i in wp.index:\n    effect_plot_df[i] = wp.loc[i, 'coefficient'] * X_train[i]\neffect_plot_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('fivethirtyeight'):\n    fig, ax = plt.subplots(figsize = (12, 6))\n    sns.boxplot(data=effect_plot_df, width=0.5, ax = ax)\n    plt.xticks(rotation = 90)\n    plt.axhline(0, linestyle = '--', color = 'red')\n    plt.title('Effect Plot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that `cont10`, `cont6`, and `cont1` all have the biggest range of possible effects.  This makes sense, when we looked at our feature importance plot these were our most important features. \n\nIf we knew what these feature represented, we could use our understanding of the problem as a sanity check that our model is moving in the correct direction. "},{"metadata":{},"cell_type":"markdown","source":"## Local Interpretability\n\nLocal interpretability looks at why the model made a prediction for a specific instance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at the prediction for this instance \nX_test.iloc[0][1:] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('fivethirtyeight'):\n    predicted_value = np.round(model.predict(X_test.iloc[0].values)[0], 2)\n    fig, ax = plt.subplots(figsize = (12, 6))\n    sns.boxplot(data=effect_plot_df, width=0.5, ax = ax, zorder = 1)\n    plt.xticks(rotation = 90)\n    plt.title(f'Predicted: {predicted_value} - Actual Value: {np.round(y_test.iloc[0], 2)}')\n    plt.scatter(range(len(X_test.iloc[0][1:])), y = wp['coefficient'] * X_test.iloc[0][1:],\n                marker = 'x', color = 'red', zorder = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows were our data point that we made a prediction on, where it falls in the distribution of data that we trained on.  It can be easier to put this into context if we take the mean prediciton. "},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(X_test).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data point that we predicted on is very similar to the mean.  This makes sense because our data point looks to be pretty close to the median point for most of the features.  We see that our it is a little higher than the median on `cont10`, but is lower than the median on `cont6`.  These are the two most important features and balance each other out.  \n\n\nNext we will look at decision trees. \n\n## Decision Trees \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\n# make a model \n# setting max_depth to be 10 to prevent overfitting\ndt = DecisionTreeRegressor(max_depth = 10)\n\n# remove constant variable used for regression from X_train and X_test\nX_train.drop(columns='const', inplace = True)\nX_test.drop(columns='const', inplace = True)\n\n# fit model \ndt.fit(X_train, y_train)\n\n# check root mean squared error \nnp.sqrt(mean_squared_error(y_test, dt.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar performance to the linear regression model \n\n## Global Interpretability \n\n### Feature Importance "},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('fivethirtyeight'):\n    features=df.columns\n    importances = dt.feature_importances_\n    indices = np.argsort(importances)\n\n    fig, ax = plt.subplots(figsize = (12, 6))\n    plt.title('Most Important Features According to Decision Tree')\n    plt.barh(range(len(indices)), importances[indices], align='center')\n    plt.yticks(range(len(indices)), features[indices])\n    plt.xlabel('Relative Importance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the most important features differ from the linear regression "},{"metadata":{},"cell_type":"markdown","source":"### Visualize Decision Tree\n\nVisualizing the decision tree can give you both a global perspective and a local perspective of our model.  I am only going to look at the first 3 layers of splits, otherwise it is too large to see"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\n\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(dt, \n                   feature_names=X_train.columns,\n                   filled=True, max_depth = 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next up I'll use K-Nearest Neighbors \n\n## K-Nearest Neighbors \n\nWith K-Nearest Neighbors there is not a native way to get global interpretability, so we'll look at local interpretabilty \n\n### Local Interpretability \n\nWe'll look at a single prediction and look at the 5 nearest neighbors "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nknn = KNeighborsRegressor()\n\nknn.fit(X_train, y_train)\n\nnp.sqrt(mean_squared_error(y_test, knn.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# point to explore \nX_test.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataframe with nearest neighbors \nnn_df = X_train.iloc[knn.kneighbors(X_test.iloc[0].values.reshape(1, -1))[1][0]]\nnn_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('fivethirtyeight'):\n    prediction = np.round(knn.predict(X_test.iloc[0].values.reshape(1, -1))[0], 2)\n    fig, ax = plt.subplots(figsize = (8, 6))\n    sns.boxplot(data = nn_df)\n    plt.xticks(rotation = 90)\n    plt.title(f'Predicted: {prediction} - Actual Value: {np.round(y_test.iloc[0], 2)}')\n    plt.scatter(range(len(X_test.iloc[0])), y = X_test.iloc[1],\n                    marker = 'x', color = 'red', zorder = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see how similar the point we are making a prediction on to the 5 nearest neighbors.  We see that a lot of the features have very different values than the nearest neighbors.  \n\nAs a summary on the model performance so far:\n\n| Model  | RMSE  |  \n|---|---|\n| Linear Regression  | 0.73  |    \n| Decision Tree  | 0.72  |    \n|  K-Neareset Neighbors | 0.77  |   \n\n## Random Forest \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# make instance of model\nrf = RandomForestRegressor()\n\n# fit model\nrf.fit(X_train, y_train)\n\n# check root mean squared error \nnp.sqrt(mean_squared_error(y_test, rf.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Global Interpretability \n\nLets look at the most important features "},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('fivethirtyeight'):\n    features=df.columns\n    importances = rf.feature_importances_\n    indices = np.argsort(importances)\n\n    fig, ax = plt.subplots(figsize = (12, 6))\n    plt.title('Most Important Features According to Random Forest')\n    plt.barh(range(len(indices)), importances[indices], align='center')\n    plt.yticks(range(len(indices)), features[indices])\n    plt.xlabel('Relative Importance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost "},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xg \n\n# Make instance of model\nxgb_model = xg.XGBRegressor(objective ='reg:squarederror', \n                  n_estimators = 10, seed = 11) \n  \n# Fit model \nxgb_model.fit(X_train, y_train) \n\n# Check RMSSE\nnp.sqrt(mean_squared_error(y_test, xgb_model.predict(X_test))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Global Interpretability with Feature Importance "},{"metadata":{"trusted":true},"cell_type":"code","source":"with plt.style.context('fivethirtyeight'):\n    features=df.columns\n    importances = xgb_model.feature_importances_\n    indices = np.argsort(importances)\n\n    fig, ax = plt.subplots(figsize = (12, 6))\n    plt.title('Most Important Features According to XGBoost')\n    plt.barh(range(len(indices)), importances[indices], align='center')\n    plt.yticks(range(len(indices)), features[indices])\n    plt.xlabel('Relative Importance')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I am going to compare the feature importance from each of the algorithms "},{"metadata":{"trusted":true},"cell_type":"code","source":"# make dataframe with the feature importances \n\nrf_indices = np.argsort(rf.feature_importances_)\ndt_indices = np.argsort(dt.feature_importances_)\nxgb_indices = np.argsort(xgb_model.feature_importances_)\n\ndf_fi = pd.DataFrame(range(1, (len(features))), index = features[rf_indices[::-1]])\ndf_fi.columns = ['rf']\ndf_fi['dt'] = pd.DataFrame(range(1, (len(features))), index = features[dt_indices[::-1]])\ndf_fi['xgb'] = pd.DataFrame(range(1, (len(features))), index = features[xgb_indices[::-1]])\ndf_fi['lr'] = pd.DataFrame(range(1, (len(features))), \n             index = features[np.argsort(np.abs(model.tvalues[1:]).values)][::-1])\ndf_fi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot most important features \nwith plt.style.context('fivethirtyeight'):\n    df_fi.loc[df.drop(columns = 'target').columns].plot(kind = 'bar', figsize = (12, 6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that all the models think that `cont2` is an important feature.  All the models besides linear regression thinks that `cont3` is an important feature.  We also see that all the models think that `cont5` is not an important feature.  \n\n## Next Steps \n\nWe only used interpretability techniques that are native to the models.  We will next explore some interpretability techniques that are model agnostic.  \n\nPart II can be seen [here](https://www.kaggle.com/jth359/deep-dive-into-ml-interpretability-part-ii)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}