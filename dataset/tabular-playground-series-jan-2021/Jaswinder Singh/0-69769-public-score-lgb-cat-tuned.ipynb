{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Competition"},{"metadata":{},"cell_type":"markdown","source":"# Competition- [Tabular Playground Series - Jan 2021](https://www.kaggle.com/c/tabular-playground-series-jan-2021/overview)"},{"metadata":{},"cell_type":"markdown","source":"### Approach(TOP-19%)\n* I have use Models - Extreme Gradient Boosting,Light Gradient Boosting and Catboost Regressor.\n* There is only early stopping criteria is met with no other hyperparameter tuning, and 5 fold Cross-validation is used for making model robust.\n* My first approach(Ensembeling) is submitting average prediction of all these models- Out of fold cross validation score-0.6982 , Public leaderboard Score-0.69971\n* Second approach(Stacking) I have added output of these 3 models as 3 more features in the model and train Light Gradient Boosting model, Out of fold cross validation score-0.6978 , Public leaderboard Score-0.69908"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib as pyplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to see all the comands result in a single kernal \nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n# to increase no. of rows and column visibility in outputs\npd.set_option('display.max_rows', 2000)\npd.set_option('display.max_columns', 2000)\npd.set_option('display.width', 2000)\n\n#To ignore warnings\nimport warnings\nwarnings.simplefilter('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Import data\ntrain = pd.read_csv(r'../input/tabular-playground-series-jan-2021/train.csv')\ntest = pd.read_csv(r'../input/tabular-playground-series-jan-2021/test.csv')\nsample = pd.read_csv(r'../input/tabular-playground-series-jan-2021/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic Visualization"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Having a look at data and its shape \ntrain.head()\ntest.head()\ntrain.shape ,test.shape ,sample.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing value check \ntrain.isna().sum().sum()\ntest.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analysing Distribution of Variables\ntrain.describe().T\ntest.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Target Distribution check\ntrain['target'].describe([0,0.01,0.03,0.05,0.1,0.25,0.5,0.75,0.95,0.97,0.99,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# found out that there is no. missing value and only one address as object type variable\n# Target varibale distribution \ntrain['target'].plot(kind = 'density', title = 'target Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking correlation\nplt.figure(figsize=(15, 8))\nsns.heatmap(train.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing Packages\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV,KFold\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code for cross validation\ndef run_gradient_boosting(clf,k, fit_params, train, test, features):\n  N_SPLITS = k\n  oofs = np.zeros(len(train))\n  preds = np.zeros((len(test)))\n\n  folds = KFold(n_splits = N_SPLITS,random_state=2021)\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['target'])):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n    target=train['target']\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    X_test = test[features]\n    \n    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n    fold_score =  np.sqrt(mean_squared_error(y_val,preds_val)) \n    print(f'\\n RMLSE score for validation set is {fold_score}')\n    oofs[val_idx] = ((preds_val))\n    preds += preds_test / N_SPLITS\n\n\n  oofs_score = np.sqrt(mean_squared_error(target, oofs))\n  print(f'\\n\\n rmlse score for oofs is {oofs_score}')\n\n  return oofs, preds\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_col=['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']\nmodel_col_1=['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14','xgb','lgb','cb']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV,StratifiedKFold,KFold\n\nxgb = XGBRegressor(n_estimators = 8000,\n#                        learning_rate = 0.05,\n#                       colsample_bytree = 0.76,\n                        )\nfit_params = {'verbose': False, 'early_stopping_rounds': 100}\nTARGET_COL= 'target'\nfeatures =model_col\nxgb_oofs, xgb_preds = run_gradient_boosting(xgb,5, fit_params, train, test, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb = LGBMRegressor(n_estimators=5000, importance_type='gain',\n#                          learning_rate = 0.05,\n#                          colsample_bytree = 0.76,\n                        )\nfit_params = {'verbose': False, 'early_stopping_rounds': 100}\nTARGET_COL= 'target'\nfeatures =model_col\nlgb_oofs, lgb_preds = run_gradient_boosting(lgb,5, fit_params, train, test, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\ncb = CatBoostRegressor(iterations=10000,\n                       learning_rate = 0.1,\n#                         colsample_bytree = 0.76,\n                        )\nfit_params = {'verbose': False , 'early_stopping_rounds': 100}\nTARGET_COL= 'target'\nfeatures =model_col\ncb_oofs, cb_preds = run_gradient_boosting(cb,5, fit_params, train, test, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance \n# you can also change cb to xgb or lgb to see their imporatance of variable\nfeat_importances = pd.Series(cb.feature_importances_, index=model_col)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['xgb']=(xgb_oofs) \ntrain['lgb']=(lgb_oofs)\ntrain['cb']=(cb_oofs)\ntest['xgb']=(xgb_preds)\ntest['lgb']=(lgb_preds)\ntest['cb']=(cb_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sqrt(mean_squared_error(train['target'],(xgb_oofs)))\nnp.sqrt(mean_squared_error(train['target'],(lgb_oofs)))\nnp.sqrt(mean_squared_error(train['target'],(cb_oofs)))\nt=0.33*train['xgb']+0.33*train['lgb']+0.34*train['cb']\nnp.sqrt(mean_squared_error(train['target'],t))\ntest['target']=0.33*test['xgb']+0.33*test['lgb']+0.34*test['cb']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({\"id\":test.id, \"target\":test.target})\noutput.to_csv('Ensembeling.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb2 = LGBMRegressor(n_estimators=5000, importance_type='gain',\n#                          learning_rate = 0.05,\n#                          colsample_bytree = 0.76,\n                        )\nfit_params = {'verbose': False, 'early_stopping_rounds': 100}\nTARGET_COL= 'target'\nfeatures =model_col_1\nlgb2_oofs, lgb2_preds = run_gradient_boosting(lgb2,5, fit_params, train, test, features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['lgb2']=(lgb2_oofs)\ntest['lgb2']=(lgb2_preds)\noutput = pd.DataFrame({\"id\":test.id, \"target\":test.target})\noutput.to_csv('Stacking.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}