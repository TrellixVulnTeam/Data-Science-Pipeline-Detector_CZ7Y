{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# this is a simple example of XGBoost which i pulled together for a Kaggle competition entry\n# i really like how concise the code is, to achieve so much computation !\n# i feel this would be easily portable to another tabular dataset problem, requiring a random forest !!\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import training and test data into arrays\ntrain_data = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ntest_data  = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')\n\nprint('Train: ', train_data.shape)\nprint('Test: ', test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set up training data. y - output is the 'target' field. x - feature columns is everything except 'target' and 'id'\ny = train_data['target']\nX = train_data.drop(columns=['target', 'id'])\nX_test = test_data.drop(columns='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create training and validation sets using sklearn 'train_test_split'\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.05, random_state=22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n#create the xgb model\nxgb = XGBRegressor(random_state=22\n                  , n_estimators=100 #I found 1672 to work well. Changed to a lower value for testing.\n                  , early_stopping_rounds=10\n                  , learning_rate=0.05\n                  , subsample=0.9\n                  , colsample_bytree=0.9\n                  , n_jobs=-1)\n\n#train the model with '.fit'\n\nxgb.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create predictions against X_test data set\npredictions = xgb.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#output pandas dataframe with test data id & target from predictions\noutput = pd.DataFrame({\"id\":test_data.id, \"target\":predictions})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}