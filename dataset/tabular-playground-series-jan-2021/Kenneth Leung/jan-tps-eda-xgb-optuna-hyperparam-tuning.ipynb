{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport optuna\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\n!pip install --upgrade xgboost\nimport xgboost as xgb\nxgb.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\n\nThis notebook will summarize my process in moving from being ranked 67th percentile to 21st percentile.\n\n\n1) Simple EDA\n\n2) Initial Gradient Boosted Tree Model\n\n3) Hyperparameter Tuning on GPU\n\n4) Cross Validation\n\n5) Repeat"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv\")\ndata = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/train.csv\")\nfinal_test = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Simple EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that all the value are numeric. No encoding or dummies are required."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training Data')\nprint(data.isnull().sum())\n\nprint()\nprint()\n\nprint('Testing Data')\nprint(final_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no missing values in the training or testing data.\n\n\nNext we'll take a look at the distributions of the features and target variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = final_test.columns[1:]\n\ntrain = data[columns]\ntarget = data['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before plotting, lets note we do not need to normalize our data since:\n\n1) Decision trees don't require normalization\n\n2) We will be using a decision tree ensemble algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(target)\nplt.title('Target')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the data is composd of two normal distributions. I'm not sure how to handle gussian mixture models yet but this is definitely something to look into for the future."},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation matrix\ncorr_mat = train.corr()\n\n# make the triangular upper of matrix, to be all ones\nmask = np.triu(np.ones_like(corr_mat, dtype=bool))\n\n# customer colourmap \ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr_mat, mask=mask,cmap=cmap)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see variable `cont13` has strong correlation with the most amount of avaiables, `cont6` has very strong correlation with `cont13`, `cont12`, `cont11`, `cont10`, `cont9`."},{"metadata":{},"cell_type":"markdown","source":"# Initial Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data\nx_train, x_test, y_train, y_test =train_test_split(\n    train, target, random_state= 2021, test_size = 0.20)\n\n# initialize\nxgb_initial = xgb.XGBRegressor()\n\n# train\nxgb_initial.fit(x_train, y_train)\n\n# predict\ninitial_preds = xgb_initial.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# want sq_root MSE\nmean_squared_error(y_test, initial_preds, squared=False)\n\n#0.7044111055876526","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The current RMSE is approximately 0.7044. We will use Optuna to help use find the best parameters for our ensemble tree model. The GPU accelerator option must be turned on.\n\n\nIn this next part, I couldn't find what the paramters I had used so I chose new random values and ran objective function to find our best parameters. The outcome was slightly better than my final submission for this competition. \n\nYou can read more about the parameters in the [xgboost documentation.](https://xgboost.readthedocs.io/en/stable/parameter.html)"},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The objective function defines what we want to optimize\n\n\ndef objective(trial, X_data = train, Y_data = target):\n    \n    x_train, x_test, y_train, y_test = train_test_split(\n        X_data, Y_data, random_state= 2021, test_size = 0.20)\n    \n\n    param = {\n    'tree_method':'gpu_hist', # use GPU for train\n    'predictor': 'gpu_predictor', # use GPU for predict\n    'learning_rate': trial.suggest_discrete_uniform('learning_rate',0.01,0.50,0.05),\n    'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel',0.01,0.91, 0.1),\n    'colsample_bynode': trial.suggest_discrete_uniform('colsample_bytree',0.01,0.91, 0.1),\n    'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.01,0.91, 0.1),\n    'max_depth': trial.suggest_int('max_depth', 1,10),\n    'subsample': trial.suggest_discrete_uniform('subsample', 0.20,1, 0.05),\n    'n_estimators': trial.suggest_categorical('n_estimators',[4000,5000,6000,7000]),\n    'min_child_weight': trial.suggest_int('min_child_weight', 1,401,step=2),\n    'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n    'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n    'random_state': trial.suggest_categorical('random_state',[2000,3000,4000]),\n    'gamma': trial.suggest_discrete_uniform('gamma',0.01,2.01, 0.1)\n    }\n    \n    model = xgb.XGBRegressor(**param)  \n    \n    model.fit(x_train,y_train,eval_set=[(x_train,y_train)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(x_test)\n    \n    rmse = mean_squared_error(y_test, preds,squared=False)\n    \n    return rmse\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# create a study and specify we want to minimize the objective\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials= 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best objective value:', study.best_value)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params of the best trial\nbest_trial = study.best_trial.params\nbest_trial['tree_method'] = 'gpu_hist'\nbest_trial['predictor'] = 'gpu_predictor'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# best_trial= {'learning_rate': 0.01,\n#  'colsample_bylevel': 0.51,\n#  'colsample_bytree': 0.91,\n#  'max_depth': 10,\n#  'subsample': 0.7,\n#  'n_estimators': 4000,\n#  'min_child_weight': 245,\n#  'lambda': 6.089455795135218e-05,\n#  'alpha': 0.0019736061458465663,\n#  'random_state': 2000,\n#  'gamma': 1,\n#  'tree_method': 'gpu_hist',\n#  'predictor': 'gpu_predictor'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at how rmse changed over the trials\noptuna.visualization.plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # what were the most important params that impacted this study's results\noptuna.visualization.plot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Learning rate was he most importnat parameter. If we fix the learning rate, the model may be able to capture the importance of other parameters. But first let's see how this current model performs with cross validation."},{"metadata":{},"cell_type":"markdown","source":"# 1st Cross Valiation"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_trial= {'learning_rate': 0.01,\n 'colsample_bylevel': 0.6100000000000001,\n 'colsample_bytree': 0.91,\n 'max_depth': 10,\n 'subsample': 0.8,\n#  'n_estimators': 5000,\n 'min_child_weight': 67,\n 'lambda': 0.012157425362490908,\n 'alpha': 7.278941365308569e-08,\n 'random_state': 3000,\n 'gamma': 1,\n 'tree_method': 'gpu_hist',\n 'predictor': 'gpu_predictor'}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test = xgb.DMatrix(final_test[columns])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_oof = np.zeros((300000,))\ntest_preds = 0\ntrain_oof.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use cross validation here to get a more accurate RMSE metric for our model and we will also use the cross validation outcomes as our testing results. This was an interesting method to of using cross validation to reduce overfitting I learned from this [Notebook](http://www.kaggle.com/tunguz/tps-01-21-feature-importance-with-xgboost-and-shap)."},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FOLDS=10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\nfold_rmse =[]\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        #print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        \n        train_df = xgb.DMatrix(train_df, label=train_target)\n        val_df = xgb.DMatrix(val_df, label=val_target)\n        \n        model =  xgb.train(best_trial, train_df, 2000)\n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(final_test)\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test/NUM_FOLDS\n        \n        rmse_iter = mean_squared_error(temp_oof, val_target, squared=False)\n        print(rmse_iter)\n        fold_rmse.append(rmse_iter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average rmse of all 10 folds\nsum(fold_rmse)/10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can expect our predicts to have an RMSE of 0.696118 from our 10-fold cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'] = test_preds\nsub.to_csv('submission2_post_competition.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Repeat: Fix Learning Rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# the objective function defines what we want to optimize\ndef objective_2(trial, X_data = train, Y_data = target):\n    \n    x_train, x_test, y_train, y_test = train_test_split(\n        X_data, Y_data, random_state= 2021, test_size = 0.20)\n    \n\n    param = {\n    'tree_method':'gpu_hist', # use GPU for train\n    'predictor': 'gpu_predictor', # use GPU for predict\n    'learning_rate': 0.01,\n    'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel',0.01,0.91, 0.1),\n    'colsample_bynode': trial.suggest_discrete_uniform('colsample_bytree',0.01,0.91, 0.1),\n    'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.01,0.91, 0.1),\n    'max_depth': trial.suggest_int('max_depth', 1,10),\n    'subsample': trial.suggest_discrete_uniform('subsample', 0.20,1, 0.05),\n    'n_estimators': trial.suggest_categorical('n_estimators',[4000,5000,6000,7000]),\n    'min_child_weight': trial.suggest_int('min_child_weight', 1,401,step=2),\n    'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n    'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n    'random_state': trial.suggest_categorical('random_state',[2000,3000,4000]),\n    'gamma': trial.suggest_discrete_uniform('gamma',0.01,2.01, 0.1)\n    }\n    \n    model = xgb.XGBRegressor(**param)  \n    \n    model.fit(x_train,y_train,eval_set=[(x_train,y_train)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(x_test)\n    \n    rmse = mean_squared_error(y_test, preds,squared=False)\n    \n    return rmse\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# create a study and specify we want to minimize the objective\nstudy_2 = optuna.create_study(direction='minimize')\nstudy_2.optimize(objective_2, n_trials= 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of finished trials:', len(study_2.trials))\nprint('Best trial:', study_2.best_trial.params)\nprint('Best objective value:', study_2.best_value)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params of the best trial\nbest_trial_2 = study_2.best_trial.params\nbest_trial_2['tree_method'] = 'gpu_hist'\nbest_trial_2['predictor'] = 'gpu_predictor'\nbest_trial_2['learning_rate'] = 0.01","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at how rmse changed over the trials\noptuna.visualization.plot_optimization_history(study_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # what were the most important params that impacted this study's results\noptuna.visualization.plot_param_importances(study_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2nd Cross Validation "},{"metadata":{},"cell_type":"markdown","source":"Note, we have already converted the final_test data to a DMatrix in the first cross validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_oof = np.zeros((300000,))\ntest_preds_2 = 0\ntrain_oof.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"best_trial_2 = {'colsample_bylevel': 0.91,\n                'colsample_bytree': 0.6100000000000001,\n                'max_depth': 10,\n                'subsample': 0.5,\n#                 'n_estimators': 6000,\n                'min_child_weight': 21,\n                'lambda': 2.4118345076896113e-05,\n                'alpha': 3.234942680594196e-08,\n                'random_state': 3000,\n                'gamma': 1.51,\n                'tree_method': 'gpu_hist',\n                'predictor': 'gpu_predictor',\n                'learning_rate': 0.01}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_FOLDS=10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\nfold_rmse_2 =[]\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n        #print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_target, val_target = target[train_ind], target[val_ind]\n        \n        train_df = xgb.DMatrix(train_df, label=train_target)\n        val_df = xgb.DMatrix(val_df, label=val_target)\n        \n        model =  xgb.train(best_trial_2, train_df, 2000)\n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(final_test)\n\n        train_oof[val_ind] = temp_oof\n        test_preds_2 += temp_test/NUM_FOLDS\n        \n        rmse_iter = mean_squared_error(temp_oof, val_target, squared=False)\n        print(rmse_iter)\n        fold_rmse_2.append(rmse_iter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_rmse_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average rmse of all 10 folds\nsum(fold_rmse_2)/10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can expect our model to have an rmse of 0.696246 from our cross validation. Even though this rmse is slightly more than the previous rmse of 0.696118 I'll make the submission to see if the model has improved overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"# is the new rmse better than the previous one?\nsub['target'] = test_preds_2\nsub.to_csv('submission3_post_competition.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(fold_rmse)/10)\nprint(sum(fold_rmse_2)/10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The initial submission from this notebook is shown below.\n![image.png](attachment:image.png)","attachments":{"image.png":{"image/png":"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"}}},{"metadata":{},"cell_type":"markdown","source":"By simply fixing the learning rate and then finding other optimal paramter, we see that both the private( left ) and public ( right ) score has improved.\n![image.png](attachment:image.png)","attachments":{"image.png":{"image/png":"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"}}},{"metadata":{},"cell_type":"markdown","source":"By simply fixing the most important parameters during optimization, this will help give parameters that previously were unimportant, greater importance and influence for parameter tuning. This helped to find a better model and reduced overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}