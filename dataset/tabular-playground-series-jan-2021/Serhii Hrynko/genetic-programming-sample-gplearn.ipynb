{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# GPLearn - Genetic Programming for regression problems\n\n> Symbolic regression is a machine learning technique that aims to identify an underlying mathematical expression that best describes a relationship. It begins by building a population of naive random formulas to represent a relationship between known independent variables and their dependent variable targets in order to predict new data. Each successive generation of programs is then evolved from the one that came before it by selecting the fittest individuals from the population to undergo genetic operations.\n[Introduction to GP](https://gplearn.readthedocs.io/en/stable/intro.html#introduction-to-gp)\n\nThis Kernel is an adoption of my previous work [LANL GP Regression](https://www.kaggle.com/elvenmonk/lanl-gp-regression) for [LANL Earthquake Prediction](https://www.kaggle.com/c/LANL-Earthquake-Prediction) competition, applied to this playground data.\n\nInitial kernel was built based solely on my personal domain knowledge, and didn't include proper Feature Analysis and Engineering, that could help improve final result.\n\nI'm open to any suggestions and ideas on how I can improve feature or fuction set, what analysis and visualization tools and approaches can be used to get better understanding of features, their relations, etc. \n\nGenetic Programming approach is very sensitive to selection of functions, that will be used to build the result.\nGood understanding of feature value distributions and their possible relations to target can be crusial not only to accurasy of the final result but also to overall performance of computations.\n\nGPLearn I think is one of the great libraries for modeling of Genetic Programming solution.\nYou can read more about it in the official [GPLearn Docs](https://gplearn.readthedocs.io/en/stable/examples.html#example-1-symbolic-regressor)"},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nstart_time = time.time()\n\ntrain_df = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv', index_col='id')\ntrain_df = train_df[train_df['target'] != 0]\nX_test = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv', index_col='id')\nX_train = train_df[train_df.columns[:-1]]\nY_train = train_df[train_df.columns[-1:]]\nprint(X_test.shape)\nprint(X_train.shape)\nprint(Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Distribution\n\nBelow code is borrowed from [Handling Multimodal Distributions & FE Techniques](https://www.kaggle.com/iamleonie/handling-multimodal-distributions-fe-techniques). Please check out this great Kernel, if have not done so yet.\n\nDiagrams below shows comparison of value distributions for same feature in train (blue) and test (orange) datasets.  \n\nThese visualizations are very useful to make sure that all features have similar distribution in train and test datasets. Otherwise we could not rely on appropriate feature to help predict the test target when trained on data with different distribution.\n\nAlso it gives us some clues later when we choose set of functions to be used for genetic programming."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.distplot(Y_train)\n\nf, ax = plt.subplots(nrows=2, ncols=7, figsize=(21, 6))\n\nfor r in range(2):\n    for c in range(7):\n        n = 7*r+c\n        ax[r, c].set(ylim=(0, 4))\n        sns.distplot(X_train[f'cont{n+1}'], ax=ax[r, c])\n        sns.distplot(X_test[f'cont{n+1}'], ax=ax[r, c])\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Add GMM class features\n\nAs done in [Handling Multimodal Distributions & FE Techniques](https://www.kaggle.com/iamleonie/handling-multimodal-distributions-fe-techniques), we can use Gausian mixture to classify each feature and add class information as separate features.\n\nFor Genetic Programming it might be better to have multiple boolean features (yes/no for each class) then deside what value to assign to every class."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\nfrom tqdm.auto import tqdm\n\ndef get_gmm_class_feature(name, n):\n    gmm = GaussianMixture(n_components=n, random_state=42)\n    gmm.fit(X_train[name].values.reshape(-1, 1))\n    X_train_class = gmm.predict(X_train[name].values.reshape(-1, 1))\n    X_test_class = gmm.predict(X_test[name].values.reshape(-1, 1))\n\n    for i in range(n):\n        X_train[f'{name}_{i}'] = 1 * (X_train_class == i)\n        X_test[f'{name}_{i}'] = 1 * (X_test_class == i)\n\nn_classes = [4, 11, 3, 3, 9, 5, 1, 3, 5, 2, 2, 2, 5, 6]\nfor n in tqdm(range(14), total=14):\n    get_gmm_class_feature(f'cont{n+1}', 12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, ax = plt.subplots(nrows=2, ncols=7, figsize=(21, 6))\n\nfor r in range(2):\n    for c in range(7):\n        n = 7*r+c\n        ax[r, c].set(ylim=(0, 4+2*12))\n        X = X_train[f'cont{n+1}']\n        for i in range(12):\n            sns.distplot(X[X_train[f'cont{n+1}_{i}'] == 1], ax=ax[r, c])\n        #sns.distplot(X_test[f'cont{n}'], ax=ax[r, c])\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optional feature normalization\n\nSometimes we need to normalize (scale to fit or center within `[0-1)` interval) source feature data for some of the regression algorithms in order to get meaningful results.\n\nFor GPLearn regressor, this is not necessary as it can decide to multiply and shift any feature by specific constant during evolution.\nStill it can be very useful because it saves a lot of time for parameter optimization for each feature and reduces chances for good formula to be rejected or not selected because of [Bloat fight techniques](https://gplearn.readthedocs.io/en/stable/intro.html#bloat) used.\n\nThis step doesn't seem to be necessary for this competition, because all of the data seems to be normalized already, but I left this step as a guidance for someone adopting this tecnique for some other data."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nX_scaler = MinMaxScaler()\nX_scaler.fit(X_train)\nX_train_scaled = pd.DataFrame(X_scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\nX_test_scaled = pd.DataFrame(X_scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n\nY_scaler = MinMaxScaler()\nY_scaler.fit(Y_train[Y_train!=0])\nY_train_scaled = pd.DataFrame(Y_scaler.transform(Y_train), columns=Y_train.columns, index=Y_train.index)\n\nsns.distplot(Y_train_scaled)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define GPLearn functions\n\nTaking the multimodal nature of the features it might be a good idea to somehow split the features into unimodal components.\nApart from basic ariphmetic operations provided by GPLearn, following additional functions can be useful:\n* Compare one feature to another and return 0 or 1\n\nThis allows to produce some more complex relations, like crop feature from one or both sides, select one or other feature based on condition, which otherwise would require introducing ternary or 4+-nary operations.\nTernary operations may be supported by engine, but would make expression tree very complex and slow to evolve.\n\n*Note: x10 and x0.1 multipliers are given for example and will not be used for the solution*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gplearn.functions import make_function\nfrom joblib import wrap_non_picklable_objects\n\n@wrap_non_picklable_objects\ndef _less(x1, x2):\n    return 1*(x1 < x2)\n\n@wrap_non_picklable_objects\ndef _crop_less(x1, x2):\n    return x1*(x1 < x2)\n\n@wrap_non_picklable_objects\ndef _crop_more(x1, x2):\n    return x1*(x1 > x2)\n\n@wrap_non_picklable_objects\ndef _x10(x1):\n    return 10*x1\n\n@wrap_non_picklable_objects\ndef _x01(x1):\n    return 0.1*x1\n\nless = make_function(function=_less, name='less', arity=2)\ncrop_less = make_function(function=_crop_less, name='crop_less', arity=2)\ncrop_more = make_function(function=_crop_more, name='crop_more', arity=2)\ntanh = make_function(function=np.tanh, name='tanh', arity=1)\nsqr = make_function(function=np.square, name='sqr', arity=1)\nx10 = make_function(function=_x10, name='x10', arity=1)\nx01 = make_function(function=_x01, name='x01', arity=1)\nfunction_set=('add', 'mul', 'neg', 'inv', less, crop_less, crop_more, 'sqrt', sqr, 'log', 'cos', 'sin', tanh) #'sub', 'div', 'min', 'max', 'abs', 'tan')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Function visualization\n\nLet's see how our functions change initial feature."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(nrows=2, ncols=3, figsize=(18, 6))\nfor r in range(2):\n    for c in range(3):\n        ax[r, c].set(ylim=(0, 4))\n        ax[r, c].set(xlim=(0, 1))\n\nsns.distplot(X_train['cont1'], ax=ax[0, 0])\nsns.distplot(np.tanh(X_train['cont1']), ax=ax[0, 1], axlabel='tanh')\nsns.distplot(np.square(X_train['cont1']), ax=ax[0, 2], axlabel='square')\nsns.distplot(X_train['cont1']*_less(0.25, X_train['cont1']) * _less(X_train['cont1'], 0.75), ax=ax[1, 0], axlabel='(0.25,0.75)')\nsns.distplot(_crop_less(X_train['cont1'], 0.5), ax=ax[1, 1], axlabel='<0.5')\nsns.distplot(_crop_more(X_train['cont1'], 0.5), ax=ax[1, 2], axlabel='>0.5')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model\n\nHyperparameter selection is an interesting and quite challenging task. Feel free to play with them after reading official documentation.\n\nI significantly reduced crossover chance, increased mutation and kept tournament size low. This is because I want evolution to keep trying different random changes instead of fast converging into local suboptimal solution."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import os\nimport pickle\nfrom sklearn.metrics import mean_squared_error\nfrom gplearn.genetic import SymbolicRegressor, SymbolicClassifier\n\ndef train_model(X=X_train, X_test=X_test, Y=Y_train['target'], model_name='gpl', verbose=False, classifier=False):\n    if verbose: print(f'Starting training of {model_name} model')\n    # Load previous execution results to continue training\n    if os.path.isfile(f'{model_name}_model.pkl'):\n        with open(f'{model_name}_model.pkl', 'rb') as f:\n            model = pickle.load(f)\n        model.set_params(generations=len(model._programs)+25, warm_start=True)\n    else:\n        # or initialize model\n        if classifier:\n            model = SymbolicClassifier(population_size=1000, generations=25, random_state=17, verbose=1, low_memory=True,\n                       p_crossover=0.2, p_subtree_mutation=0.3, p_hoist_mutation=0.1, p_point_mutation=0.2,\n                       parsimony_coefficient=0.000005, max_samples=1, tournament_size = 25, n_jobs=-1,\n                       init_depth=(5, 10), init_method='full', const_range=(0.,1.), function_set=function_set)\n        else:\n            model = SymbolicRegressor(population_size=1000, generations=25, random_state=17, verbose=1, low_memory=True,\n                           p_crossover=0.2, p_subtree_mutation=0.3, p_hoist_mutation=0.1, p_point_mutation=0.2,\n                           parsimony_coefficient=0.000005, max_samples=1, tournament_size = 25, n_jobs=-1,\n                           init_depth=(5, 10), init_method='full', const_range=(0.,1.), function_set=function_set, metric='rmse')\n    # Train/predict\n    model.fit(X, Y)\n    if verbose: print(model)\n    P_train = model.predict(X_train)\n    P_test = model.predict(X_test)\n    if verbose: print('Train score: {0:.4f}.'.format(mean_squared_error(P_train, Y if classifier else Y_train['target'], squared=False)))\n\n    # Save results and model\n    pd.DataFrame(P_train, columns=['target'], index=X_train.index).to_csv(f'{model_name}_train_predictions.csv', index=True)\n    pd.DataFrame(P_test, columns=['target'], index=X_test.index).to_csv(f'{model_name}_test_predictions.csv', index=True)\n    with open(f'{model_name}_model.pkl', 'wb') as f:\n        pickle.dump(model, f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Direct prediction\n\nFirst let's try to predict target directly"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"DURATION = 4 * 3600\n\ntrain_model(X=X_train, X_test=X_test, Y=Y_train['target'], model_name='gpl', verbose=True)\n\nwhile time.time() - start_time < DURATION:\n    train_model(X=X_train, X_test=X_test, Y=Y_train['target'], model_name='gpl', verbose=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize program\n\nWe can visualize resulting program with graphviz tool"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import graphviz\nfrom IPython.display import SVG\n\ndef draw_model(model_name):\n    with open(f'{model_name}_model.pkl', 'rb') as f:\n        model = pickle.load(f)\n    dot_data = model._program.export_graphviz()\n    graph = graphviz.Source(dot_data)\n    display(SVG(graph.pipe(format='svg')))\n\ndraw_model('gpl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split Data by target\n\nAs suggested in [Handling Multimodal Distributions & FE Techniques](https://www.kaggle.com/iamleonie/handling-multimodal-distributions-fe-techniques), target data is likely bimodal.\n\nSplitting training data by Gaussian Mixture, and predicting target separately for each class can give more accurate predition results."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"gmm = GaussianMixture(n_components=2, random_state=17)\ngmm.fit(Y_train)\nY_class = gmm.predict(Y_train)\nY_train0 = Y_train[Y_class == 0]\nY_train1 = Y_train[Y_class == 1]\n\nX_train0 = X_train[Y_class == 0]\nX_train1 = X_train[Y_class == 1]\n\nf, ax = plt.subplots(nrows=1, ncols=1)\n\nsns.distplot(Y_train0, ax=ax)\nsns.distplot(Y_train1, ax=ax)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Is there a correlation between target class and features?\n\nLet's find out if any correlation exist between the target class we have just defined and feature distribution.\n\nThis should answer a question whether or not target class can be inferred from any single feature."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, ax = plt.subplots(nrows=2, ncols=7, figsize=(21, 6))\n\nfor r in range(2):\n    for c in range(7):\n        n = 7*r+c\n        ax[r, c].set(ylim=(0, 4+2*n_classes[n]))\n        sns.distplot(X_train0[f'cont{n+1}'], ax=ax[r, c])\n        sns.distplot(X_train1[f'cont{n+1}'], ax=ax[r, c])\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## No correlation?\n\nFrom above distributions it look's like even if correlation exists, it's minimal for any single feature. \n\nHope still exist that there is a better correlation with some combination of features.\n\nThe only single feature, that might have distinct distribution is 'cont2' as it looks like partially discrete feature. Let's visualize it separately."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"f, ax = plt.subplots(nrows=1, ncols=1)\n\nsns.distplot(X_train0[f'cont2'], ax=ax)\nsns.distplot(X_train1[f'cont2'], ax=ax)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction by class\n\nNow we can start training 3 independent models:\n* classifier that could predict target class (which of 2 distribution characteristics each sample has)\n* For each class predict appropriate target value\n\nLet's first see if data split by target class can be predicted more precisely."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_model(X=X_train0, X_test=X_test, Y=Y_train0['target'], model_name='gpl0', verbose=True)\ntrain_model(X=X_train1, X_test=X_test, Y=Y_train1['target'], model_name='gpl1', verbose=True)\n\ndraw_model('gpl0')\ndraw_model('gpl1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction of a target class\n\nNice! It looks like each subclass of data can be predicted quite precisely.\n\nSo can we also predict target class itsels. To do so we will use `SymbolicClassifier` version of model defined above.\n\nClassifier uses log-loss metric, which for binary classification should have value slightly lower than `-ln(0.5)=0.69314718056` in order to be useful."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"DURATION = 8 * 3600\n\ntrain_model(X=X_train, X_test=X_test, Y=Y_class, model_name='gplc', verbose=True, classifier=True)\n\nwhile time.time() - start_time < DURATION:\n    train_model(X=X_train, X_test=X_test, Y=Y_class, model_name='gplc', verbose=True, classifier=True)\n\ndraw_model('gplc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Verify combined prediction\n\nTrained program for each target class have made a prediction for each sample.\nResulting target can be built by choosing subclass prediction by predicted target class.\n\nLet's calculate final training score"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"P_train_class = pd.read_csv('gplc_train_predictions.csv', index_col='id', dtype=np.float64)\nP_train0 = pd.read_csv('gpl0_train_predictions.csv', index_col='id', dtype=np.float64)\nP_train1 = pd.read_csv('gpl1_train_predictions.csv', index_col='id', dtype=np.float64)\nP_train = P_train0\nP_train['target'] = np.where(P_train_class['target'] == 0, P_train0['target'], P_train1['target'])\nprint('Train score: {0:.4f}.'.format(mean_squared_error(P_train['target'], Y_train['target'], squared=False)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit combined predictions\n\nDue to poor target class prediction, resulting train prediction score is quite bad. Because train and test rows are probably sampled from the same dataset, we can expect similar score from final submission.\n\nSimilarly final prediction is produced for test data."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"P_test_class = pd.read_csv('gplc_test_predictions.csv', index_col='id', dtype=np.float64)\nP_test0 = pd.read_csv('gpl0_test_predictions.csv', index_col='id', dtype=np.float64)\nP_test1 = pd.read_csv('gpl1_test_predictions.csv', index_col='id', dtype=np.float64)\nP_test = P_test0\nP_test['target'] = np.where(P_test_class['target'] == 0, P_test0['target'], P_test1['target'])\nP_test.to_csv(\"submission_c01.csv\", index=True)\nprint(P_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}