{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction"},{"metadata":{},"cell_type":"markdown","source":"Hello Kagglers!\n\nIn this notebook I will try to find out which of the regression algorithms can improve the score  for this particular dataset and position in Kaggle competition. I found this competition challenging as this dataset is a bit tricky and I think there isn't too much to improve as the dataset has been preprocessed for us (correct me if I'm wrong). The only thing which comes to my mind is to choose the right algorithm and find the right hyperparametres."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom scipy.stats import skew","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sub = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/test.csv\")\ntest_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Take a sample of a dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.sample(frac=0.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers_df = pd.DataFrame({\"skewness\": train_df.drop(\"id\", axis=1).skew(),\n                            \"kurtosis\": train_df.drop(\"id\", axis=1).kurtosis()})\n\noutliers_df = outliers_df.sort_values(by='skewness', ascending=False)\noutliers_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(14,4), nrows=1, ncols=2)\n\n#kurtosis = outliers_df['']\n\nsns.barplot(x=outliers_df.index, y=outliers_df['skewness'].values, palette=\"mako_r\", ax=axes[0])\nsns.barplot(x=outliers_df.index, y=outliers_df['kurtosis'].sort_values(ascending=False).values,\n            palette='mako_r',ax=axes[1])\n\n\naxes[0].set_title(\"Skewness\")\naxes[0].set_xticklabels(outliers_df.index,rotation=45)\n\n\naxes[1].set_title(\"Kurtosis\")\naxes[1].set_xticklabels(outliers_df.index, rotation=45);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Kurtosis is a statistical measure that defines how heavily the tails of a distribution differ from the tails of a normal distribution. In other words, kurtosis identifies whether the tails of a given distribution contain extreme values.\n\nSkewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.\n\nSkewness essentially measures the relative size of the two tails. Kurtosis is a measure of the combined sizes of the two tails."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_df['target'], kde=True, color='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train_df['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ser = pd.Series(train_df['target'])\nser.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"q1 = np.quantile(train_df['target'],0.25)\nq3 = np.quantile(train_df['target'],0.75)\n\niqr = q3-q1\n\nlower_outlier = q1 - (1.5*iqr)\nupper_outlier = q3 + (1.5*iqr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_without_outliers = train_df[train_df['target'] >= lower_outlier].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_df_without_outliers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=train_df_without_outliers['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_df_without_outliers['target'], kde=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Usually, at this point we would think about whether to drop outliers, but I did it already with previous ran and it had no effect on results."},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,8), dpi=150)\ncorr_matrix = train_df.drop('id',axis=1).corr()\n\nsns.heatmap(corr_matrix, mask=corr_matrix < 0.8, annot=True, ax=ax, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix['target'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(\"cont12\", axis=1, inplace=True)\ntest_df.drop(\"cont12\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_with_target = train_df.corr()['target'].sort_values(ascending=False)[1:]\ncorr_with_target = corr_with_target.drop('id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib as mpl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = list(reversed(px.colors.qualitative.Dark24))\n#colors = list(reversed(['#636EFA', '#EF553B', '#00CC96', '#AB63FA', '#FFA15A']))\n\nfig = go.Figure(go.Bar(\n    x = corr_with_target.values,\n    y = corr_with_target.index,\n    text = corr_with_target.values,\n    textposition =\"auto\",\n    texttemplate = \"%{value:,s}\",\n    marker_color = colors,\n    orientation = \"h\",\n))\nfig.update_traces(\n    #marker_line_color = \"black\",\n    marker_line_width = 1,\n    opacity = 0.8,\n)\nfig.update_layout(\n    title = \"Features Correlation to the Target Column\"\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As always we should check for multicollinearity. In this case we can try to use regularization methods like Ridge and Lasso or ElasticNet)."},{"metadata":{},"cell_type":"markdown","source":"#### Spearman’s Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(12,8), dpi=150)\nsns.heatmap(train_df.drop('id',axis=1).corr(method='spearman'), annot=True, ax=ax, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A common aproach for highly correlated features is to do dimension reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = train_df.drop([\"id\", \"target\"], axis=1).columns\n\nskewed_feat = train_df[num_cols].skew().sort_values(ascending=False)\nskewed_feat = skewed_feat[skewed_feat > 0.5]\nskewed_index = skewed_feat.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in skewed_index:\n    q3 = np.quantile(train_df[col], 0.75)\n    q1 = np.quantile(train_df[col], 0.25)\n    iqr = q3 - q1\n    upper_limit = q3 + (1.5*iqr)\n    lower_limit = q1 - (1.5*iqr)\n    \n    upper_col_bool = train_df[col].apply(lambda x: x <= upper_limit)\n    lower_col_bool = train_df[col].apply(lambda x: x >= lower_limit)\n    \n    clean_train_df = train_df[upper_col_bool]\n    clean_train_df = train_df[lower_col_bool]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalize data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = clean_train_df.drop(['target','id'], axis=1)\ny = clean_train_df['target']\ntest = test_df.drop(\"id\", axis=1).values\nid_col = test_df['id'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nscaled_train = sc.fit_transform(X)\nscaled_train = pd.DataFrame(scaled_train, columns=train_df.drop(['target','id'], axis=1).columns)\n\nscaled_test = sc.transform(test)\nscaled_test = pd.DataFrame(scaled_test, columns=test_df.drop(\"id\", axis=1).columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluateModel(model):\n    \"\"\"\n    This function evaluate the model with\n    mean absolute error and root mean squared error\n    \"\"\"\n    model.fit(X_train, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    score_r2 = r2_score(y_test, y_pred)\n    \n    print(f\"MAE: {mae}\")\n    print(f\"RMSE: {rmse}\")\n    print(f\"R-square: {score_r2}\")\n    \n    return mae, rmse, score_r2, y_pred, model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(scaled_train, y, \n                                                    test_size=0.3, \n                                                    random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear regression"},{"metadata":{},"cell_type":"markdown","source":"Linear Regression can not work on all datasets. For a linear regression algorithm to work properly, it has to pass at least the following five assumptions:\n\n1. Linear relationship - the relation between independent and dependent features should be linear. Scatter plot is a good way to visualize it.\n2. Multiviariate Normal - each variable seperatly needs to be bell shape curve. This can be tested by plotting a histogram.\n3. No Multicollinearity - Multicollinearity happens when the independent variables are highly correlated with each other. Can be tested with correlation matrix.\n4. No Autocorrelation - Autocorrelation means a single column data values are related to each other. Test it with scatterplot.\n5. Homoscedasticity - This means “same variance” .In other words residuals are equal across regression line. Homoscedasticity can also be tested using scatter plot."},{"metadata":{},"cell_type":"markdown","source":" ### ElasticNet CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elastic_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], tol=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"el_cv_mae, le_cv_rmse, lr_cv_r2, lr_y_pred, elastic_model = evaluateModel(elastic_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For l1_ratio = 0 the penalty is an L2 penalty(Ridge). For l1_ratio = 1 it is an L1 penalty(Lasso). It looks like Lasso will be better model to choose for."},{"metadata":{},"cell_type":"markdown","source":"### Lasso CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LassoCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lassoCV_model = LassoCV(eps=0.01, n_alphas=100,cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"la_cv_mae, la_cv_rmse, la_cv_r2, la_y_pred, lassoCV_model = evaluateModel(lassoCV_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lassoCV_model.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean of target column\ntrain_df['target'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Residuals"},{"metadata":{"trusted":true},"cell_type":"code","source":"residuals = pd.Series(y_test - la_y_pred,name='residuals')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"residuals","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=y_test, y=residuals)\nplt.axhline(y=0, color='red', ls='--')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(residuals, bins=40, kde=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy as sp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(4,3), dpi=120)\n\n_ = sp.stats.probplot(residuals, plot=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"lassoCV_model.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef_ser = pd.Series(lassoCV_model.coef_)\ncoef_ser = coef_ser.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = list(reversed(px.colors.qualitative.Dark24))\n\nfig = go.Figure(go.Bar(\n    x = scaled_train.columns,\n    y = coef_ser,\n    text = coef_ser,\n    textposition = 'auto',\n    texttemplate = '%{value:,s}',\n    marker_color = colors,\n    orientation = 'v',\n))\nfig.update_traces(\n    marker_line_width = 1,\n    opacity = 0.8,\n)\nfig.update_layout(\n    title = \"Feature importances via coefficients in LassoCV\"\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check if our model is the best model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridgeCV_model = RidgeCV(alphas=(0.1, 1.0, 10.0),scoring='neg_mean_absolute_error', cv=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rid_cv_mae, rid_cv_rmse, rid_cv_r2, rid_y_pred, ridgeCV_model = evaluateModel(ridgeCV_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridgeCV_model.alpha_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridgeCV_model.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Residuals"},{"metadata":{"trusted":true},"cell_type":"code","source":"residuals_ridge = pd.Series(y_test - rid_y_pred, name='residuals')\nresiduals_ridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(residuals_ridge, bins=40, kde=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=y_test, y=residuals_ridge)\nplt.axhline(y=0, color='red',ls='--')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(4,3), dpi=120)\n\n_ = sp.stats.probplot(residuals_ridge, plot=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"coef_ser = pd.Series(ridgeCV_model.coef_)\ncoef_ser = coef_ser.sort_values(ascending=False)\n\ncolors = list(reversed(px.colors.qualitative.Dark24))\n\nfig = go.Figure(go.Bar(\n    x = scaled_train.columns,\n    y = coef_ser,\n    text = coef_ser,\n    textposition = 'auto',\n    texttemplate = '%{value:,s}',\n    marker_color = colors,\n    orientation = 'v',\n))\nfig.update_traces(\n    marker_line_width = 1,\n    opacity = 0.8,\n)\nfig.update_layout(\n    title = \"Feature importances via coefficients in RidgeCV\"\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVR\n\nsvr_model = LinearSVR(max_iter=1000000)\n\nsvr_mae, svr_rmse, svr_r2, svr_pred, svr_model = evaluateModel(svr_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Residuals"},{"metadata":{"trusted":true},"cell_type":"code","source":"residuals_svr = pd.Series(y_test - svr_pred,name=' LinearSVC residuals')\nresiduals_svr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(residuals_svr, bins=40,kde=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x=y_test, y=residuals_svr)\nplt.axhline(y=0, color='red',ls='--')\nplt.xlabel(\"y_actual\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems like this data set is not valid for linear regression.(If someone correct me if I am wrong). In other words if residuals plot shows clear pattern, Linear Regression is propably not a good choice."},{"metadata":{},"cell_type":"markdown","source":"There is constant error between residuals and actual data which leads us to very sophisticated term  homoscesdasticity, the word I still struggle to pronounce it :)."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(4,3), dpi=120)\n\n_ = sp.stats.probplot(residuals_svr, plot=ax)\n\nplt.title(\"Probability plot\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is an issue with dataset because residuals are skewing from linear regression line."},{"metadata":{},"cell_type":"markdown","source":"#### Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_model.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef_ser = pd.Series(svr_model.coef_)\ncoef_ser = coef_ser.sort_values(ascending=False)\n\ncolors = list(reversed(px.colors.qualitative.Dark24))\n\nfig = go.Figure(go.Bar(\n    x = scaled_train.columns,\n    y = coef_ser,\n    text = coef_ser,\n    textposition = 'auto',\n    texttemplate = '%{value:,s}',\n    marker_color = colors,\n    orientation = 'v',\n))\nfig.update_traces(\n    marker_line_width = 1,\n    opacity = 0.8,\n)\nfig.update_layout(\n    title = \"Feature importances via coefficients in LinearSVR\"\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training final models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso, Ridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lasso_model = Lasso(alpha=lassoCV_model.alpha_).fit(scaled_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ridge_model = Ridge(alpha=ridgeCV_model.alpha_).fit(scaled_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# linearSVR_model = LinearSVR(max_iter=1000000).fit(scaled_train, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make predictions and save it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#lasso_sub = pd.DataFrame(data={'id': id_col,\n                               #'target':lassoCV_model.predict(scaled_test)})\n\n#ridge_sub = pd.DataFrame(data={'id':id_col,\n                               #'target':ridge_model.predict(scaled_test)})\n\n#linearSVR_sub = pd.DataFrame(data={'id':id_col,\n                                   #'target': linearSVR_model.predict(scaled_test)})\n\n\n#print(len(lasso_sub['id']) == len(sample_sub['id']))\n#print(len(ridge_sub['id']) == len(sample_sub['id']))\n#print(len(linearSVR_sub['id']) == len(sample_sub['id']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lasso_sub.to_csv(\"submission_lasso.csv\", index=False)\n# ridge_sub.to_csv(\"submission_ridge.csv\", index=False)\n# linearSVR_sub.to_csv(\"submission_linearSVR.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Search for the best hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nlasso_model = Lasso(max_iter=1000000)\n\nparam_grid = {'alpha': [0.005, 0.02, 0.03, 0.05, 0.06, 0.1, 0.5, 1, 10, 100]}\n\nlasso_grid = GridSearchCV(lasso_model,param_grid, cv=10, scoring='neg_mean_squared_error')\nlasso_grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abs(lasso_grid.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_y_grid = lasso_grid.predict(X_test)\n\nlasso_grid_mae = mean_absolute_error(y_test, lasso_y_grid)\n\nlasso_grid_rmse = np.sqrt(mean_squared_error(y_test, lasso_y_grid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_grid_mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_grid_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Lasso Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lasso_model = Lasso(alpha=0.005)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lasso_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lasso_y_pred = best_lasso_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lasso_mae = mean_absolute_error(y_test, best_lasso_y_pred)\nbest_lasso_rmse = np.sqrt(mean_squared_error(y_test, best_lasso_y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lasso_mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_lasso_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submmit to Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_best_lasso = Lasso(alpha=0.005)\n\nfinal_best_lasso.fit(scaled_train, y)\n\nbest_lasso_sub = pd.DataFrame({'id': id_col,'target': final_best_lasso.predict(scaled_test)})\n\nbest_lasso_sub.to_csv(\"submission_best_lasso.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GridSearch CV for Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'alpha':[0.01, 0.05, 0.1, 1.0, 10.0],\n              'solver':['auto', 'svd', 'cholesky', 'lsqr', 'saga']}\n\nbest_ridge_model = Ridge(max_iter=1000000)\n\ngrid_ridge = GridSearchCV(best_ridge_model, param_grid, cv=10, \n                          scoring='neg_mean_squared_error', verbose=0)\n\ngrid_ridge.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_ridge.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"abs(grid_ridge.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_grid_pred = grid_ridge.predict(X_test)\n\nridge_grid_mae = mean_absolute_error(y_test, ridge_grid_pred)\nridge_grid_rmse = np.sqrt(mean_squared_error(y_test, ridge_grid_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_grid_mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_grid_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No Submmission here","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Polynomial Regression"},{"metadata":{},"cell_type":"markdown","source":"Running this code in kaggle notebook could use most of you alocated memory and the notebook can stop responding, so if you want to run it make sure you have enough memory. This algorithm didn't improve root mean squared error when I run it, beside there is no chance of using elbow method to find if any degree can yield better results."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# poly_converter = PolynomialFeatures(degree=3, include_bias=False)\n# poly_features = poly_converter.fit_transform(X)\n    \n# X_train, X_test, y_train, y_test = train_test_split(poly_features,\n                                                    # y,\n                                                    # test_size=0.3,\n                                                    # random_state=42)\n\n# poly_reg_model = LinearRegression()\n# poly_reg_model.fit(X_train, y_train)\n\n# poly_pred = poly_reg_model.predict(X_test)\n\n# poly_mae = mean_absolute_error(y_test, poly_pred)\n# poly_rmse = np.sqrt(mean_squared_error(y_test, poly_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# poly_mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# poly_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submmit to Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"# full_poly_reg = LinearRegression()\n# full_poly_reg.fit(poly_features, y)\n\n# poly_test_features = poly_converter.transform(test)\n\n# poly_sub = pd.DataFrame({'id': id_col,'target': full_poly_reg.predict(poly_test_features)})\n\n# poly_sub.to_csv(\"submission_poly.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GridSearch CV for SVR"},{"metadata":{},"cell_type":"markdown","source":"This will takes ages in my computer to find the right hyperparameters. Searching for best parameters can be exhausting and in some cases can break down. So, purely because of the time needed for gridsearch and memory I create smaller datasets to reduce computing time. I will also search only for C values in this case "},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.svm import SVR\n\n#svr_model = SVR()\n\n#param_grid = {'C':[0.001, 0.1, 1, 10],\n              #'max_iter':[1000, 10000]}\n\n#grid_svr = GridSearchCV(svr_model, param_grid, cv=2,verbose=1)\n\n#grid_svr.fit(X_train, y_train)\n\n#grid_svr_pred = grid_svr.predict(X_test)\n\n#grid_svr_mae = mean_absolute_error(y_test, grid_svr_pred)\n\n#grid_svr_rmse = np.sqrt(mean_squared_error(y_test, grid_svr_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grid_svr_mae","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grid_svr_rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Descent Algorithm for Regression "},{"metadata":{},"cell_type":"markdown","source":"I found out that linear regression algorithms are not suited for this dataset, and there is little improvement in rmse for these models. I also find out that algorithms using loss function are performing better, therefore I try XGBoost. The algorithm is an implementation of the gradient boosting ensemble algorithm for classification and regression."},{"metadata":{},"cell_type":"markdown","source":"#### SGDRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\n\nsgdr_model = SGDRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sgdr_mae, sgdr_rmse, sgdr_2r, sgdr_y_pred, sgdr_model = evaluateModel(sgdr_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KNeighborsRegressor"},{"metadata":{},"cell_type":"markdown","source":"Some of the  algorithms below run too long when I run it on kaggle notebook. So I am not going to do it again( no more time to do it)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nknn_model = KNeighborsRegressor(n_neighbors=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_mae, knn_rmse, knn_r2, knn_y_pred, knn_model = evaluateModel(knn_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Griedient Boosting Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr_model = GradientBoostingRegressor(n_estimators=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr_mae, gbr_rmse, gbr_r2, gbr_y_pred, gbr_model = evaluateModel(gbr_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks promising, let's tune hyperparameters and check if that improve our results."},{"metadata":{},"cell_type":"markdown","source":"#### GridSearch CV for Gradient Boosting Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"#param_grid = {\"n_estimators\":[300,1000], # previously [10,100,200,300]\n              #\"learning_rate\":[0.01, 0.1, 1.0],\n              #\"max_features\":[\"auto\",\"sqrt\"]}\n\n#gr_boost_reg = GradientBoostingRegressor()\n\n#grid_gbr_model = GridSearchCV(gr_boost_reg, param_grid,cv=2)\n\n#grid_gbr_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid_gbr_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# grid_gbr_model.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gbr_cv_mae, gbr_cv_rmse, gbr_cv_r2, gbr_y_pred, grid_gbr_model = evaluateModel(grid_gbr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submmit to Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_gbr = GradientBoostingRegressor(n_estimators=1000,\n                                     learning_rate=0.1,\n                                     loss=\"ls\",\n                                     max_features='sqrt')\n\ngbr_cv_mae, gbr_cv_rmse, gbr_cv_r2, gbr_y_pred, grid_gbr_model = evaluateModel(best_gbr)\n\n\n# full_data_gbr.fit(scaled_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gbr_sub = pd.DataFrame({\"id\":id_col,\n                        # \"target\":full_data_gbr.predict(scaled_test)})\n    \n# gbr_sub.to_csv(\"submission_gbr.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submission scored 0.70774, which gave 727 from 1049 position in the scoreboard on 19.01.2020. Not bad, Let's se if we can tune it better with slightly different hyperparameters.\n\n1. MAE: 0.5956168406066421\n2. RMSE: 0.7110476866137042\n\n\nHyperparameters found by GridSearch CV:\n\n{'learning_rate': 0.1, 'loss': 'huber', 'n_estimators': 300}"},{"metadata":{},"cell_type":"markdown","source":"#### GradientBoostingRegressor with best hyperparameters found with GridSearchCV"},{"metadata":{},"cell_type":"markdown","source":"Submission scored 0.70565, which is an improvement of your previous score of 0.70774. More tunning yielded small improvment and placed on scoreboard 706.\n\nHyperparameters:\n\n{'learning_rate': 0.1,\n 'loss': 'ls',\n 'max_features': 'sqrt',\n 'n_estimators': 1000}\n \nScore:\n\n1. MAE: 0.5937034963056688\n2. RMSE: 0.7088659238713398"},{"metadata":{},"cell_type":"markdown","source":"Good practice is to check for Variance-Bias Trade-Off by tunning in this case \"n_estimators\" hyperparameter and keep record of rmse for test and train test. This algorithm is fairly robust to overfitting so a large number usually results with better performance."},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestRegressor\n\n# rfc_model = RandomForestRegressor()\n\n# rfc_model.fit(X_train, y_train)\n\n# rfc_mae, rfc_rmse, rfc_r2, rfc_y_pred, rfc_model = evaluateModel(rfc_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rfc_model.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#coef_ser = pd.Series(rfc_model.feature_importances_)\n#coef_ser = coef_ser.sort_values(ascending=False)\n\n#colors = list(reversed(px.colors.qualitative.Dark24))\n\n#fig = go.Figure(go.Bar(\n    #x = scaled_train.columns,\n    #y = coef_ser.values,\n    #text = coef_ser.values,\n    #textposition = 'auto',\n    #texttemplate = '%{value:,s}',\n    #marker_color = colors,\n    #orientation = 'v',\n#))\n#fig.update_traces(\n   # marker_line_width = 1,\n    #opacity = 0.8,\n#)\n#fig.update_layout(\n    #title = \"Feature importances via coefficients in LassoCV\"\n#)\n#fig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GridSearch CV for RandomForestRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# param_grid = {\"n_estimators\":[300, 500, 1000],\n              # \"max_depth\":[2,3,5],\n              # \"max_features\":[\"auto\",\"sqrt\"]}\n\n# rfr_model = RandomForestRegressor()\n\n# rfr_grid_model = GridSearchCV(rfr_model,param_grid, cv=2)\n\n# rfr_grid_mae, rfr_grid_rmse, rfr_grid_r2, rfr_grid_y, rfr_grid_model  = evaluateModel(grid_rfr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# No improvements with this algorithm.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Artificial Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_model = Sequential()\n\n\n# input layer\nann_model.add(Dense(78, activation='relu', dtype='float32'))\n\n# hidden layer\nann_model.add(Dense(78, activation='relu', dtype='float32'))\nann_model.add(Dropout(0.5))\n\n# hidden layer\nann_model.add(Dense(78, activation='relu', dtype='float32'))\nann_model.add(Dropout(0.5))\n\n# hidden layer\nann_model.add(Dense(38, activation='relu',dtype='float32'))\nann_model.add(Dropout(0.5))\n\n# output layer\nann_model.add(Dense(1, activation='relu', dtype='float32'))\n\nann_model.compile(optimizer='adam', loss='mae')  # rmsprop, adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_model.fit(x=X_train, y=y_train,\n              validation_data=(X_test, y_test),\n              batch_size=64, epochs=15,verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = pd.DataFrame(ann_model.history.history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ann_model_pred = ann_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_mae, ann_rmse, ann_r2, ann_y_pred, ann_model  = evaluateModel(ann_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I bolive there is still a room for improvement by better parameters tunning but this is not the purpose of this notebook, but for now my winner so far is Griedient Boosting Regressor. Finally, I would like to try last algorythm which is XGBoost.\n\n\nXGBoost (Extreme Gradient Boosting) belongs to a family of boosting algorithms and uses the gradient boosting (GBM) framework at its core. It is an optimized distributed gradient boosting library. XGBoost is well known to provide better solutions than other machine learning algorithms. In fact, since its inception, it has become the \"state-of-the-art” machine learning algorithm to deal with structured data."},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dmatrix = xgb.DMatrix(data=scaled_train,\n                           label=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dmatrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 500)\n\nxgb_mae, xgb_rmse, xgb_r2, xgb_y_pred, xgb_model = evaluateModel(xg_reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.plot_importance(xg_reg)\nplt.rcParams['figure.figsize'] = [5, 5]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submmit to Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_final = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n               #  max_depth = 5, alpha = 10, n_estimators = 500)\n\n# xgb_final.fit(scaled_train.values,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_pred = xgb_final.predict(scaled_test.values)\n\n# xgb_sub = pd.DataFrame({\"id\":id_col, \"target\":xgb_pred})\n# xgb_sub.to_csv(\"submission_xgboost.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is great. Submission scored 0.70400, which is an improvement of your previous score of 0.70565.\n654 position on Kaggle scoreboard on 20.01.2020. Let's see if we can tune our model and get better results."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_results.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### XGBoost tunning hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"#param_grid = {'nthread':[1], #when use hyperthread, xgboost may become slower\n              #'objective':['reg:squarederror'],\n              #'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              # 'max_depth': [5, 6, 7],\n              #'alpha':[0.1, 1, 10],\n              #'colsample_bytree': [0.3, 0.7],\n              #'n_estimators': [500, 1000]}\n\n#xgb1 = xgb.XGBRegressor()\n\n#xgb_grid_model = GridSearchCV(xgb1, \n                              #param_grid,\n                              #cv=2,\n                              #verbose=1)\n\n#xgb_grid_mae , xgb_grid_rmse, xgb_grid_r2, xgb_grid_y, xgb_grid_model = evaluateModel(xgb_grid_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgb_grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Kaggle Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_best_model = xgb.XGBRegressor(n_estimators=1000,\n                                  learning_rate=0.03, \n                                  colsample_bytree=0.3, \n                                  alpha=10)\n\nxgb_best_mae , xgb_best_rmse, xgb_best_r2, xgb_best_y, xgb_best_model = evaluateModel(xgb_best_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_best_model.fit(scaled_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_best_sub = pd.DataFrame({\"id\": id_col,\n                             \"target\": xgb_best_model.predict(scaled_test)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_best_sub.to_csv(\"submission_xgb_best.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submission scored 0.70303, which is an improvement of your previous score of 0.70400. \n\nKaggle scorboard position:  628"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.ensemble import VotingRegressor\nensemble_model = VotingRegressor(estimators=[\n    (\"svr_base\", svr_model),\n    (\"lasso_gr\", lasso_grid.estimator),\n    (\"ridge_gr\", grid_ridge.estimator),\n    (\"sgdr_base\", sgdr_model),\n    (\"knn_base\", knn_model),\n    (\"gbr_base\", gbr_model),\n    (\"gbr_best\", grid_gbr_model),\n    (\"xgb_best\", xgb_best_model),])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_model.fit(scaled_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_pred = ensemble_model.predict(scaled_test)\n\nensemble_sub = pd.DataFrame({\"id\":id_col,\n                             \"target\": ensemble_pred})\n\nensemble_sub.to_csv(\"ensemble_tb_series_sub.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**StackingRegressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = StackingRegressor(estimators=[(\"sgdr_base\", sgdr_model),\n                                    (\"knn_base\", knn_model),\n                                    (\"gbr_best\", grid_gbr_model)],\n                                    final_estimator = xgb_best_model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(scaled_train, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg_sub = pd.DataFrame({\"id\":id_col,\n                        \"target\": reg.predict(scaled_test)})\n\nreg_sub.to_csv(\"stacking_reg_tps_sub.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Table with models and their score"},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_score_df = pd.DataFrame({\n    \"Model\": [\"LassoCV\",\"RidgeCV\",\"LinearSVR\",\"Lasso GridSearchCV\",\"Ridge GridSearchCV\",\n              \"SGDRegressor\",\"KNeighborsRegressor\",\"GradientBoostingRegressor\",\n              \"GradientBoostingRegressor GridSearchCV\",\"RandomForestRegressor\",\n              \"RandomForestRegressor GridSearchCV\",\"ANN Regression\",\"XGBRegressor\",\n              \"XGBRegressor GridSearchCV\"],\n    \"RMSE\":[lasso_rmse, ridge_rmse,svr_rmse,\n            lasso_grid_rmse,ridge_grid_rmse,\n            sgdr_rmse,knn_rmse, gbr_rmse,\n            gbr_cv_rmse,rfc_rmse, rfr_rmse,\n            ann_rmse, xgb_rmse, xgb_grid_rmse]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_score_df = rmse_score_df.sort_values(by=['RMSE'], ascending=True).reset_index()\nrmse_score_df = rmse_score_df.drop('index', axis=1)\nrmse_score_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems to me that in this particular dataset gradient boosting and decision trees algorithms perform much better than linear models. I also realize that there might be more improvement if you go deeper into hyperparameters and tune chosen model. Another suggestion could be dealing with correlated data. One popular rank correlation method in ML is the Principal Component Analysis. It’s a technique to find patterns in high dimensional data.\n\nI am sure there is more to explore to make predictions which would result with lower RMSE, but it is beyond my scope for now. There is still so much to learn...\n\nAfter this challange I found more question than the answers. One main question remains \"What else I could do to improve model score?, any suggestion please leave feedback.\n\nBe aware, that if you want to run this notebook it will take you 4 hours without the models I have comment.\n\nI hope you find this notebook interesting."},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install pytorch-tabnet==3.1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\n# X = train_df.drop(columns=['target']).values\n# sc = StandardScaler()\n# x=sc.fit_transform(x)\n# y = train_df['target'].values.reshape(-1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train models with AutoML\nre = TabNetRegressor()  #TabNetRegressor()\nre.fit(\n  p_X_train, y_train,\n  eval_set=[(p_X_val, y_val)],\n   eval_name=['train'],\n    eval_metric=['rmsle', 'mae', 'rmse', 'mse'],\n    max_epochs=1000,\n    patience=50,\n    batch_size=1024, virtual_batch_size=128,\n    num_workers=0,\n    drop_last=False\n)\npreds = re.predict(p_X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test MSE:\", mean_squared_error(y_val, preds, squared=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = re.predict(p_test).reshape(-1)\nsubmission = pd.DataFrame({\"id\":id_col,\n                          \"target\":preds})\nsubmission.to_csv('submission_pytorch.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LGBM model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_features = [col for col in train_df.columns if col.startswith(\"cont\")]\nlen(cont_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X = X.abs()\ny = train_df[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train_df))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(train_df):\n    X_train, X_val = train_df.iloc[train_index], train_df.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    X_train = X_train.abs()\n    \n    \n\n    X_train = X_train.abs()\n\n    \n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 200,\n              \"lambda_l1\":2,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.01,\n              'min_child_samples': 50,\n              \"bagging_fraction\":0.7,\n              \"bagging_freq\":1}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100\n                    )\n    \n        y_pred_list.append(model.predict(X_val[cont_features]))\n        test_preds.append(model.predict(test[cont_features]))\n        \n        \n        oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    X_train = X_train.abs()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}