{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tabular Playground - Scikit-Learn Ensemble\n\nThe competition description says this is a linear regression problem, so lets see what Scikit Learn can do\n- https://scikit-learn.org/stable/modules/linear_model.html#linear-model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport sklearn\nfrom operator import itemgetter\nfrom sklearn.linear_model import *\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nimport pprint\n\npp = pprint.PrettyPrinter(width=41, compact=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv', index_col='id')\ntest_df  = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv',  index_col='id')\n\n# display('train_df')\ndisplay( train_df )\n# display('test_df')\n# display( test_df )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nAdding PolynomialFeatures actually reduces the score, so this is indeed a pure linear regression problem\n- `degree=2` = `0.76347`\n- `degree=1` = `0.72935`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# DOCS: https://scikit-learn.org/stable/modules/preprocessing.html\n# DOCS: https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions\ndef preprocess_X(X, degree=1):\n    # NOTE: PolynomialFeatures needs to be done before scaling - https://towardsdatascience.com/preprocessing-with-sklearn-a-complete-and-comprehensive-guide-670cb98fcfb9\n    X = PolynomialFeatures(degree=degree).fit_transform(X)\n    X = StandardScaler().fit_transform(X)\n    return X\n\ncolumns = test_df.columns\nX_test  = preprocess_X( test_df[columns]  )\nX       = preprocess_X( train_df[columns] )\nY       = train_df['target']\nX_train, X_valid, Y_train, Y_valid = sklearn.model_selection.train_test_split(X, Y, test_size=0.05, random_state=42)\n\nprint('X_train.shape', X_train.shape)\nprint('Y_train.shape', Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Models\n\nLets try out each of the Linear Models from scikit-learn in a loop\n- https://scikit-learn.org/0.20/modules/classes.html#module-sklearn.linear_model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lazy Load models to avoid out of memory errors\nmodels = [\n    (sklearn.linear_model.ARDRegression, {}),          \n    # ([n_iter, tol, …])\tBayesian ARD regression.\n    \n    (sklearn.linear_model.BayesianRidge, {}),          \n    # ([n_iter, tol, …])\tBayesian ridge regression\n    \n    (sklearn.linear_model.ElasticNet, {}),             \n    # ([alpha, l1_ratio, …])\tLinear regression with combined L1 and L2 priors as regularizer.\n    \n    # (sklearn.linear_model.ElasticNetCV, {\"max_iter\":10_000}),           \n    # ([l1_ratio, eps, …])\tElastic Net model with iterative fitting along a regularization path.\n    \n    (sklearn.linear_model.HuberRegressor, {\"max_iter\":1000}),         \n    # ([epsilon, …])\tLinear regression model that is robust to outliers.\n    \n    (sklearn.linear_model.Lars, {}),                   \n    # ([fit_intercept, verbose, …])\tLeast Angle Regression model a.k.a.\n    \n    (sklearn.linear_model.LarsCV, {}),                 \n    # ([fit_intercept, …])\tCross-validated Least Angle Regression model.\n    \n    (sklearn.linear_model.Lasso, {}),                  \n    # ([alpha, fit_intercept, …])\tLinear Model trained with L1 prior as regularizer (aka the Lasso)\n    \n    (sklearn.linear_model.LassoCV, {\"max_iter\":10_000}), \n    # ([eps, n_alphas, …])\tLasso linear model with iterative fitting along a regularization path.\n    \n    (sklearn.linear_model.LassoLars, {}),              \n    # ([alpha, …])\tLasso model fit with Least Angle Regression a.k.a.\n    \n    (sklearn.linear_model.LassoLarsCV, {}),            \n    # ([fit_intercept, …])\tCross-validated Lasso, using the LARS algorithm.\n    \n    (sklearn.linear_model.LassoLarsIC, {}),            \n    # ([criterion, …])\tLasso model fit with Lars using BIC or AIC for model selection\n    \n    (sklearn.linear_model.LinearRegression, {}),       \n    # ([…])\tOrdinary least squares Linear Regression.  \n    \n    # (sklearn.linear_model.LogisticRegression, {}),     \n    # ([penalty, …])\tLogistic Regression (aka logit, MaxEnt) classifier.\n    \n    # (sklearn.linear_model.LogisticRegressionCV, {}),   \n    # ([Cs, …])\tLogistic Regression CV (aka logit, MaxEnt) classifier.\n    \n    # (sklearn.linear_model.MultiTaskLasso, {}),         \n    # ([alpha, …])\tMulti-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n    \n    # (sklearn.linear_model.MultiTaskElasticNet, {}),    \n    # ([alpha, …])\tMulti-task ElasticNet model trained with L1/L2 mixed-norm as regularizer\n    \n    # (sklearn.linear_model.MultiTaskLassoCV, {}),       \n    # ([eps, …])\tMulti-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n    \n    # (sklearn.linear_model.MultiTaskElasticNetCV, {}),  \n    # ([…])\tMulti-task L1/L2 ElasticNet with built-in cross-validation.\n    \n    (sklearn.linear_model.OrthogonalMatchingPursuit, {}),    \n    # ([…])\tOrthogonal Matching Pursuit model(OMP)\n    \n    (sklearn.linear_model.OrthogonalMatchingPursuitCV, {}),  \n    # ([…])\tCross-validated Orthogonal Matching Pursuit model (OMP).\n    \n    # (sklearn.linear_model.PassiveAggressiveClassifier, {}),  \n    # ([…])\tPassive Aggressive Classifier\n    \n    (sklearn.linear_model.PassiveAggressiveRegressor, {}),   \n    # ([C, …])\tPassive Aggressive Regressor # Poor Score\n    \n    # (sklearn.linear_model.Perceptron, {}),             \n    # ([penalty, alpha, …])\tRead more in the User Guide.\n    \n    (sklearn.linear_model.RANSACRegressor, {}),        \n    # ([…])\tRANSAC (RANdom SAmple Consensus) algorithm.  # Poor Score\n    \n    (sklearn.linear_model.Ridge, {}),                  \n    # ([alpha, fit_intercept, …])\tLinear least squares with l2 regularization.\n    \n    # (sklearn.linear_model.RidgeClassifier, {}),        \n    # ([alpha, …])\tClassifier using Ridge regression.\n    \n    # (sklearn.linear_model.RidgeClassifierCV, {}),      \n    # ([alphas, …])\tRidge classifier with built-in cross-validation.\n    \n    (sklearn.linear_model.RidgeCV, {}),                \n    # ([alphas, …])\tRidge regression with built-in cross-validation.\n    \n    # (sklearn.linear_model.SGDClassifier, {}),          \n    # ([loss, penalty, …])\tLinear classifiers(SVM, logistic regression, a.o.) with SGD training.\n    \n    (sklearn.linear_model.SGDRegressor, {}),           \n    # ([loss, penalty, …])\tLinear model fitted by minimizing a regularized empirical loss with SGD\n    \n    # (sklearn.linear_model.TheilSenRegressor, {}),      \n    # ([…])\tTheil-Sen Estimator: robust multivariate regression model.  # Cause OOM Exception\n    \n    # sklearn.linear_model.enet_path(X_train, Y_train),           \n    # (X, y[, l1_ratio, …])\tCompute elastic net path with coordinate descent\n    \n    # sklearn.linear_model.lars_path(X_train, Y_train),           \n    # (X, y[, Xy, Gram, …])\tCompute Least Angle Regression or Lasso path using LARS algorithm [1]\n    \n    # sklearn.linear_model.lasso_path(X_train, Y_train),          \n    # (X, y[, eps, …])\tCompute Lasso path with coordinate descent\n    \n    # sklearn.linear_model.logistic_regression_path(X_train, Y_train),  \n    # (X, y)\tCompute a Logistic Regression model for a list of regularization parameters.\n    \n    # sklearn.linear_model.orthogonal_mp(X_train, Y_train),       \n    # (X, y[, …])\tOrthogonal Matching Pursuit(OMP)\n    \n    # sklearn.linear_model.orthogonal_m_gram(X_train, Y_train),  \n    # (Gram, Xy[, …])\tGram Orthogonal Matching Pursuit(OMP)\n    \n    # sklearn.linear_model.ridge_regression(X_train, Y_train),    \n    # (X, y, alpha[, …])\tSolve the ridge equation by the method of normal equations.\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef fit_predict(model_class, kwargs, verbose=True):\n    name  = model_class.__name__ \n    if verbose: print(name)\n    model = model_class(**kwargs)  \n    model.fit(X_train, Y_train)\n    rmse       = sklearn.metrics.mean_squared_error(Y_valid, model.predict(X_valid), squared=False)\n    prediction = model.predict(X_test)\n    return name, rmse, prediction \n\n\nscores      = {}\npredictions = {}\nfor model_class, kwargs in models:\n    try:\n        name, rmse, prediction = fit_predict(model_class, kwargs)\n        scores[name]      = rmse\n        predictions[name] = prediction\n    except:\n        print('ERROR', model_class.__name__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = dict(sorted(scores.items(), key=itemgetter(1), reverse=False))\nscores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test = np.mean([\n    predictions[name]\n    for name, score in scores.items()\n    if score <= 0.8\n], axis=0)\nprint('Y_test.shape', Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.read_csv('../input/tabular-playground-series-jan-2021/sample_submission.csv', index_col='id')\nsubmission_df['target'] = Y_test\nsubmission_df.to_csv('submission.csv')\n!head submission.csv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further Reading\n\nThis notebook is part of a series exploring the [Tabular Playground](https://www.kaggle.com/c/tabular-playground-series-jan-2021)\n- 0.72935 - [scikit-learn Ensemble](https://www.kaggle.com/jamesmcguigan/tabular-playground-scikit-learn-ensemble)\n- 0.71423 - [Fast.ai Tabular Solver](https://www.kaggle.com/jamesmcguigan/fast-ai-tabular-solver)\n- 0.70426 - [XGBoost](https://www.kaggle.com/jamesmcguigan/tabular-playground-xgboost)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}