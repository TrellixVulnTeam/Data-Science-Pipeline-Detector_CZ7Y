{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport itertools\nimport os\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import mean_absolute_error as mae\n\nimport seaborn as sns\nsns.set(font_scale=1.4)\n\nimport matplotlib.pyplot as plt\n\ndef pl(nr=1, nc=1,fs1=20,fs2=7):\n    fig,axes=plt.subplots(nrows=nr, ncols=nc, figsize=(fs1, fs2))\n    return fig, axes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '/kaggle/input/tabular-playground-series-jan-2021/'\ntrain = pd.read_csv(PATH+'train.csv')\ntest = pd.read_csv(PATH+'test.csv')\nsample_submission = pd.read_csv(PATH+'sample_submission.csv')\n\nFT_COLS = [x for x in train.columns if 'cont' in x]\nLABEL='target'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#highlight outliers\ntrain['outlier_filter'] = np.where(train[LABEL]<4, True, False)\nprint('# outliers', sum(train['outlier_filter']))\n\nol_filt = ~train['outlier_filter']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Note: this notebook & NN notebooks have been tidied from original submission. Score is the same."},{"metadata":{},"cell_type":"markdown","source":"# Links to notebooks for submissions to be blended"},{"metadata":{},"cell_type":"markdown","source":"NN Models\n\nV1 with more features\n\nhttps://www.kaggle.com/davidedwards1/jan21-tabplayground-nn-final-more-features\n\n\nV2 with less features\n\nhttps://www.kaggle.com/davidedwards1/jan21-tabplayground-nn-final-fewer-features"},{"metadata":{},"cell_type":"markdown","source":"**Please note: I took public notebook parameters for running my LGBM and XGB. Therefore thanks to original authors for work on the tree model parameters **\n\nhttps://www.kaggle.com/hamditarek/tabular-playground-series-xgboost-lightgbm\n\n\nhttps://www.kaggle.com/hamzaghanmi/xgboost-hyperparameter-tuning-using-optuna"},{"metadata":{},"cell_type":"markdown","source":"# Load Inputs (OOF, Test predictions)"},{"metadata":{"trusted":true},"cell_type":"code","source":"NSUBS=4\n\nSUBMISSION_DESCR = ['LGBM','XGB','KERAS','KERAS FEWER FTS',]\ncolors=['Blue','Green','Red', 'Pink',]\n\nSUBMISSION_PATHS = ['/kaggle/input/jan21-lgbm-submission/',\n                   '/kaggle/input/jan21-tabular-xgb-sub/',\n                    '/kaggle/input/jan21-tabplayground-nn1-output/',\n                    '/kaggle/input/jan21-tabplayground-nn2-output/',                   \n                   ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction Distributions"},{"metadata":{"trusted":true},"cell_type":"code","source":"f,a=pl(nr=1,nc=2,fs1=18,fs2=5)\n\noof_list = []\nsubmission_list = []\n\nsns.histplot(train[LABEL], color='Black', alpha=0.5, ax=a[0])\nsns.histplot(train[LABEL], color='Black', alpha=0.5, ax=a[1])\n\nfor count,sp in enumerate(SUBMISSION_PATHS):\n    oof_list+=[pd.read_csv(SUBMISSION_PATHS[count]+'oof_predictions.csv')['oof_prediction'].values]\n    submission_list+=[pd.read_csv(SUBMISSION_PATHS[count]+'submission.csv')['target'].values]\n    \nfor count, w in enumerate(submission_list):    \n    sns.histplot(oof_list[count], color=colors[count], alpha=0.5, ax=a[0])\n    sns.histplot(submission_list[count], color=colors[count], alpha=0.3, ax=a[1])\n    \na[0].set_title('Train Labels & OOFs')\na[1].set_title('Train Labels & Test Predictions')\na[0].set_xlim(6,10)\na[1].set_xlim(6,10)\n\na[0].legend(['LABEL','LGBM','XGB','KERAS','KERAS2',], facecolor='White')\na[1].legend(['LABEL','LGBM - TEST PRD.','XGB - TEST PRD.','KERAS - TEST PRD.', 'KERAS2 - TEST PRD.',], facecolor='White')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check min and max predictions\nprint('min and max predictions')\nfor count, w in enumerate(submission_list): \n    print(SUBMISSION_DESCR[count],submission_list[count].min(), submission_list[count].max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check CV outcome depending on weighting of model"},{"metadata":{"trusted":true},"cell_type":"code","source":"weights_range = [0.001  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,\n       0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 0.999]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#create placeholder for results table\noutput_wts = np.zeros((194481,NSUBS+1))\nj=0\nimport itertools\nfor a,b,c,d in itertools.product(weights_range, repeat=NSUBS):\n    #get combination of weights, sum to 100%\n    sum_w = np.array([a,b,c,d]).sum()\n    wts = np.array([a,b,c,d]) / sum_w\n    \n    #get oof combination for weights\n    final_oof_preds = np.zeros((len(train),))\n\n    for count, w in enumerate(oof_list):\n        final_oof_preds+=oof_list[count] * wts[count]\n    \n    #get error and put into output table\n    output_wts[j,NSUBS] = np.sqrt(mse(train[LABEL], final_oof_preds))\n    \n    #record the associated weights\n    output_wts[j,0:NSUBS] = wts\n    \n    j+=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Look at outcomes"},{"metadata":{},"cell_type":"markdown","source":"Weights suggest approx 40% keras total across 2 models, 40% LGBM, 20% XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"output_wts = pd.DataFrame(columns=['wt_lgbm','wt_xgb','wt_keras','wt_keras2', 'oof_error'],data=output_wts)\n\nf,a=pl(nc=NSUBS,fs1=20)\n\nfor count,c in enumerate(['wt_lgbm','wt_xgb','wt_keras','wt_keras2', ]):\n    a[count].scatter(x=output_wts[c],y=output_wts['oof_error'],color=colors[count])\n    a[count].set_title(SUBMISSION_DESCR[count])\n    a[count].set_xlabel('blending weight')\n    a[count].set_ylabel('oof_score')\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_wts = output_wts.sort_values('oof_error').reset_index(drop=True)\noutput_wts.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#select weights\n\nselected_wts = output_wts.loc[0:200, ['wt_lgbm','wt_xgb','wt_keras','wt_keras2', ]].mean(axis=0)\nprint(selected_wts.sum())\nselected_wts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Estimate final CV error and create mix / submission"},{"metadata":{},"cell_type":"markdown","source":"Note: suspect my CV is somewhat underestimated due to label encoding etc across CV folds."},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test_preds = np.zeros((len(sample_submission),))\nfinal_oof_preds = np.zeros((len(train),))\n\nfor count, s in enumerate(submission_list):\n    final_test_preds+=submission_list[count] * selected_wts.values[count]\n    final_oof_preds+=oof_list[count] * selected_wts[count]\n    \nprint('final CV error', np.sqrt(mse(train[LABEL], final_oof_preds)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,a=pl(nr=1,nc=2,fs1=15,fs2=5)\n\n\nsns.kdeplot(train[LABEL], color='Black', alpha=0.5, ax=a[0])\nsns.kdeplot(train[LABEL], color='Black', alpha=0.5, ax=a[1])\nsns.kdeplot(final_test_preds, color='Green', alpha=0.5, ax=a[1])\n    \na[0].set_title('Train Labels')\na[1].set_title('Train Labels & Test Predictions')\na[0].set_xlim(4.5,10)\na[1].set_xlim(4.5,10)\n\na[1].legend(['LABEL','Test Predictions'], facecolor='White')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['target'] = final_test_preds\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}