{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%bash\ncd ../input/tabular-playground-series-jan-2021/train.csv\nls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndata_train = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ndata_test = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nLets explore the spread of the dataset and also try to find  if there are any missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as  plt\n\nfig,ax = plt.subplots(14,1,figsize = (15,50))\ncols = data_train.iloc[:,1:].columns\nfor col in range(0,len(cols) - 1):\n        plt.sca(ax[col]) \n        ax[col]  =  sns.distplot(data_train[cols[col]])\n        ax[col].set_xlabel(cols[col])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plots give us a small insight into the nature of the data . We can see that the dataset is quite intact and has no missing values. However there are some values which show a certain degree of corellation . Lets explore this a bit more."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (15,15))\nsns.heatmap(data_train.iloc[:,1:].corr(),annot = True,ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above heat map shows us that some  the data is corellated. Let us also study the relative rannge of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (25,15))\nsns.boxplot(ax = ax, x=\"variable\", y=\"value\", data=pd.melt(data_train.iloc[:,1:-1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will do some feature engineering to further improve the quality  of the data"},{"metadata":{},"cell_type":"markdown","source":"We will use PCA to reduce dimentionality"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(7)\nout = pca.fit_transform(data_train.iloc[:,1:-1])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (15,15))\nsns.heatmap(pd.DataFrame(out).corr(),annot = True,ax = ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Albation experiment\n\nWe will split the training , test and eval data , and then start training our neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_test_split(data,train_split = 0.1):\n    train_mask = data_train.loc[:,['id']].applymap(lambda x : abs(hash(str(x))) % 10000  < 10000 * train_split)\n    test_mask = data_train.loc[:,['id']].applymap(lambda x : abs(hash(str(x))) % 10000  >= 10000 * train_split)\n    return (data.iloc[[x[0] for x in train_mask.values],1:15] , data.iloc[[x[0] for x in test_mask.values],1:15] , data.iloc[[x[0] for x in train_mask.values],-1] , data.iloc[[x[0] for x in test_mask.values],-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test,y_train,y_test = train_test_split(data_train,0.7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will define our pre-processing pipeline based on tensorflow. This will help us make preprocessing part of the model itself.Also since we are using neural networks , we will use regularisation as well , so that it will work as dimentioality reducer."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\ndef generate_feature_column(x):\n    features = []\n    for i in x.columns:\n        features.append(tf.feature_column.numeric_column(i))\n    features.append(tf.feature_column.embedding_column(tf.feature_column.crossed_column(['cont6', 'cont9','cont10','cont11','cont12','cont13'], hash_bucket_size=3000),dimension = 100))\n    return features\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generate_feature_column(x_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will define the functions for dataset generation . We will first use a small dataset for albation experiments , then we will feed the full data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_train_data(x,y,albation = False,batch = 100,epochs = 10):\n    if(not albation):\n        return tf.data.Dataset.from_tensor_slices((dict(x), y.values)).shuffle(buffer_size=batch).repeat(count=epochs).batch(batch)\n    else:\n        return tf.data.Dataset.from_tensor_slices((dict(x.iloc[0:1000,:]),y[0:1000].values)).shuffle(buffer_size=batch).repeat(count=epochs).batch(batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_test_data(x,batch = 100):\n        return tf.data.Dataset.from_tensor_slices(dict(x)).batch(batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next(iter(generate_train_data(x_train,y_train)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data generator is working fine , lets start building our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = 100\nepochs = 5\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1,\n                              patience=1, min_lr=0.00001)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.DenseFeatures(generate_feature_column(x_train)),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001), loss = 'mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\nmodel.fit(generate_train_data(x_train,y_train,batch = batch,epochs = epochs) ,validation_data=generate_train_data(x_test,y_test),epochs=epochs,verbose = 1,workers=-1,batch_size = batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(generate_test_data(data_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'id':data_test.iloc[:,0].values,'target': np.reshape(pred,(200000,))}).to_csv('./result.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}