{"cells":[{"metadata":{},"cell_type":"markdown","source":"* No EDA as there are so many other nb detailing it \n* No hyperparam optimization on the shallow models ... again there are some amazing nb explaining it. I checked a couple of shallow models for the baseline\n* Just wanted to play with a NN on this issue\n;-)\n\n\n* RF = 0.7 ... NN = 0.717"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom operator import itemgetter    \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, cross_val_predict, StratifiedKFold, train_test_split, learning_curve, ShuffleSplit\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor\nimport catboost as cb\n\nimport tensorflow as tf\n\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils\nfrom keras.utils import to_categorical\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ninput_path = Path('/kaggle/input/tabular-playground-series-jan-2021/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data prep"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(input_path / 'test.csv', index_col='id')\nprint(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Any nulls ?\n\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate target from train data\n\ntarget = train.pop('target')\nprint(target.shape)\ntarget.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalize train and test separately so there will be no data leakage"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainNorm = train.copy()\nfor feature_name in train.columns:\n    mean_value = train[feature_name].mean()\n    std_value = train[feature_name].std()\n    trainNorm[feature_name] = (train[feature_name] - mean_value) / std_value\n    \ntrainNorm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testNorm = test.copy()\nfor feature_name in test.columns:\n    mean_value = test[feature_name].mean()\n    std_value = test[feature_name].std()\n    testNorm[feature_name] = (test[feature_name] - mean_value) / std_value\n    \ntestNorm.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split train into train and validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(trainNorm, target, test_size=0.2, random_state=7)\nprint ('X_train: ', X_train.shape)\nprint ('X_test: ', X_test.shape)\nprint ('y_train: ', y_train.shape)\nprint ('y_test: ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CROSS VALIDATION\n\ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test Models and evaluation metric\n# Lin reg ALL models HYPERPARAMS NOT optimized\n\nmodels = [LinearRegression(), Ridge(), Lasso(), ElasticNet(), SGDRegressor(), BayesianRidge(),\n          cb.CatBoostRegressor(), RandomForestRegressor(), ]\nnames = [\"LR\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"SGD\",\"BayesianRidge\", \"catboost\",\"RandomForestRegressor\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Run the models and compare\n\nModScores = {}\n\nfor name, model in zip(names, models):\n    score = rmse_cv(model, X_train, y_train)\n    ModScores[name] = score.mean()\n    print(\"{}: {:.2f}\".format(name,score.mean()))\n\nprint(\"_\"*100)\nfor key, value in sorted(ModScores.items(), key = itemgetter(1), reverse = False):\n    print(key, round(value,3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final predict on test"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = cb.CatBoostRegressor()\n\nmodel.fit(X_train, y_train)\n\nfinal_predictions = model.predict(X_test)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \nprint(\"RMSE on X_test \", round(final_rmse, 4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model evaluation on test\n\nfinal_predictions = model.predict(testNorm)\nfinal_predictions.shape\n\nsubm = pd.read_csv(input_path / 'sample_submission.csv')\nprint(subm.shape)\nsubm.head()\n\nid_col=subm.id\nsubm=id_col.to_frame()\nsubm['target'] = final_predictions\nsubm.set_index('id',inplace=True)\nsubm.head()\n\nsubm.to_csv('CBSubmission.csv')\n\nLoadSub = pd.read_csv('CBSubmission.csv')\nLoadSub.set_index('id',inplace=True)\n\nLoadSub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NN MODEL\n\nfrom keras import models\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],))) \nmodel.add(layers.Dense(256, activation='relu')) \nmodel.add(layers.Dropout(0.2)) \nmodel.add(layers.Dense(64, activation='relu'))\n\nmodel.add(layers.Dense(1))\n\nmodel.summary()\n\nmodel.compile(optimizer=optimizers.Adam(), loss='mse', metrics=['mae'])\nprint(\"model compiled\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train NN\n\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.2, \n                    verbose=1,\n                   epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot learning curves\n\nacc = history.history['mae']\nval_acc = history.history['val_mae']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'bo', label='Training error')\nplt.plot(epochs, val_acc, 'r', label='Validation error')\nplt.title('Training and validation ERROR')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation LOSS')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model evaluation on X_test\n\nfinal_predictions = model.predict(X_test)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) \nprint(\"RMSE on X_test \", round(final_rmse, 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Simple NN, 10 epochs RMSE = 0.7175\n\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(64, activation='relu'))\n\nmodel.add(layers.Dense(1))"},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model evaluation on test\n\nfinal_predictions = model.predict(testNorm)\nfinal_predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm = pd.read_csv(input_path / 'sample_submission.csv')\nprint(subm.shape)\nsubm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_col=subm.id\nsubm=id_col.to_frame()\nsubm['target'] = final_predictions\nsubm.set_index('id',inplace=True)\nsubm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subm.to_csv('NNSubmission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LoadSub = pd.read_csv('NNSubmission.csv')\nLoadSub.set_index('id',inplace=True)\n\nLoadSub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}