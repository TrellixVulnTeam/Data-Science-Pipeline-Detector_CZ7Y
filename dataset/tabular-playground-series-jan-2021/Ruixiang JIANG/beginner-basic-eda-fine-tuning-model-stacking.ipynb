{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Basic Exploratory Data Analysis(EDA) "},{"metadata":{},"cell_type":"markdown","source":"## preparations"},{"metadata":{"trusted":true},"cell_type":"code","source":"#load packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-jan-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-jan-2021/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Basic information**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# a quick look into the data\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#see if there are null values\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"great! no null values in this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#get some statistical information\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize target distribution\n\nsns.distplot(a=train['target'], rug = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"notice that there is a training sample whose target value is ”abnormally“ small."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['target']<4] # find the samples whose target value is smaller than 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#visulization of 14 features\nfig = plt.figure(figsize=(18,16))\ntrain_feature = train.drop(['id','target'],axis=1)\nfor index,col in enumerate(train_feature):\n    plt.subplot(5,3,index+1)\n    sns.distplot(train_feature.loc[:,col], kde = False)\nfig.tight_layout(pad=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# corralation heatmap\nmask = np.zeros_like(train_feature.corr())\nmask[np.tril_indices_from(mask)] = True\n\nfeature_corr = train_feature.corr()\nsns.heatmap(feature_corr,cmap= \"Blues\",mask = mask.T)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Regression"},{"metadata":{},"cell_type":"markdown","source":"train/test set split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_X, val_X,  train_Y, val_Y = train_test_split(\n    train_feature, train['target'], test_size=0.2, shuffle=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CatBoosting Baseline"},{"metadata":{},"cell_type":"markdown","source":"Let's use CatBoostRegressor as our baseline model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\ncat = CatBoostRegressor(random_state = 7, loss_function='RMSE', verbose = False)\ncat.fit(train_X, train_Y)\n\nval_pred = cat.predict(val_X)\nscore = np.sqrt(mean_squared_error(val_Y, val_pred)) \n\nprint(\"CB model RMSE: \",end = \"\")\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Export baseline model prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_pred = cat.predict(test.drop(\"id\",axis = 1))\n\nsubmission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\":test_pred\n    })\nsubmission.to_csv('baseline_cat.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"Moreover, we can easily derive feature importance after training the CatRegressor model. For more information, you may refer to official doc on [Feature importance - Catboost](https://catboost.ai/docs/features/feature-importances-calculation.html#feature-importances-calculation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.figure(figsize=(10, 10))\nplt.barh(cat.feature_names_, cat.feature_importances_,height =0.5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## model tuning"},{"metadata":{},"cell_type":"markdown","source":"We may use LightGBM, XGBoosting and CatBoost as our base models for model stacking. Before applying [model stacking](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/), we shall fine-tune the base models. [Bayesian Optimization](https://towardsdatascience.com/shallow-understanding-on-bayesian-optimization-324b6c1f7083) is a efficient optimizaion methods by practice."},{"metadata":{},"cell_type":"markdown","source":"### LGBM tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from bayes_opt import BayesianOptimization\nimport lightgbm\n\n#codes below are taken from https://www.kaggle.com/yevonnaelandrew/lgbm-cat-xgb-optimization-stacking\n\n\ndtrain = lightgbm.Dataset(data=train_feature, label=train['target'])\n\ndef hyp_lgbm(num_leaves, feature_fraction, bagging_fraction, max_depth, min_split_gain, min_child_weight, learning_rate):\n      \n        params = {'application':'regression','num_iterations': 5000,\n                  'early_stopping_round':100, 'metric':'rmse'}\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        params['learning_rate'] = learning_rate\n        cv_result = lightgbm.cv(params, dtrain, nfold=3, \n                                seed=7, stratified=False, \n                                verbose_eval =None, metrics=['rmse'])\n        \n        return -np.min(cv_result['rmse-mean']) \n        #add a minus because Bayesian Optimization can only be performed to approximate maxima.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pds = {\n    'num_leaves': (5, 50),\n    'feature_fraction': (0.2, 1),\n    'bagging_fraction': (0.2, 1),\n    'max_depth': (2, 20),\n    'min_split_gain': (0.001, 0.1),\n    'min_child_weight': (10, 50),\n    'learning_rate': (0.01, 0.5),\n      }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# codes below takes a long execution time, uncomment to see the process\n# optimizer = BayesianOptimization(hyp_lgbm,pds,random_state=7)\n# optimizer.maximize(init_points=10, n_iter=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer.max['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CatBoost Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cgb\n\ndef cat_hyp(depth, bagging_temperature, l2_leaf_reg, learning_rate):\n  params = {\"iterations\": 100,\n            \"loss_function\": \"RMSE\",\n            \"verbose\": False} \n  params[\"depth\"] = int(round(depth)) \n  params[\"bagging_temperature\"] = bagging_temperature\n  params[\"learning_rate\"] = learning_rate\n  params[\"l2_leaf_reg\"] = l2_leaf_reg\n  \n  cat_feat = [] # Categorical features list, we have nothing in this dataset\n  cv_dataset = cgb.Pool(data=train_feature, label=train['target'], cat_features=cat_feat)\n\n  scores = cgb.cv(cv_dataset,\n              params,\n              fold_count=3)\n  return -np.min(scores['test-RMSE-mean']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Search space\npds = {'depth': (3, 10),\n       'bagging_temperature': (0.1,10),\n       'l2_leaf_reg': (0.1, 10),\n       'learning_rate': (0.05, 0.3),\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer = BayesianOptimization(cat_hyp, pds, random_state=7)\n# optimizer.maximize(init_points=10, n_iter=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer.max['params']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoosting tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\ndtrain = xgb.DMatrix(train_feature, train['target'], feature_names=train_feature.columns.values)\ndef hyp_xgb(max_depth, subsample, colsample_bytree,min_child_weight, gamma, learning_rate):\n    params = {\n    'objective': 'reg:squarederror',\n    'eval_metric':'rmse',\n    'nthread':-1\n     }\n    \n    params['max_depth'] = int(round(max_depth))\n    params['subsample'] = max(min(subsample, 1), 0)\n    params['colsample_bytree'] = max(min(colsample_bytree, 1), 0)\n    params['min_child_weight'] = int(min_child_weight)\n    params['gamma'] = max(gamma, 0)\n    params['learning_rate'] = learning_rate\n    scores = xgb.cv(params, dtrain, num_boost_round=500,verbose_eval=False, \n                    early_stopping_rounds=10, nfold=3)\n    return -scores['test-rmse-mean'].iloc[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pds ={\n  'min_child_weight':(3, 20),\n  'gamma':(0, 5),\n  'subsample':(0.7, 1),\n  'colsample_bytree':(0.1, 1),\n  'max_depth': (3, 10),\n  'learning_rate': (0.01, 0.5)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer = BayesianOptimization(hyp_xgb, pds, random_state=7)\n# optimizer.maximize(init_points=4, n_iter=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## model stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"## parameters derived from Bayesian Optimizaion fine-tuning\nparam_lgbm = {\n     'bagging_fraction': 0.973905385549851,\n     'feature_fraction': 0.2945585590881137,\n     'learning_rate': 0.03750332268701348,\n     'max_depth': int(7.66),\n     'min_child_weight': int(41.36),\n     'min_split_gain': 0.04033836353603582,\n     'num_leaves': int(46.42),\n     'application':'regression',\n     'num_iterations': 5000,\n     'metric': 'rmse'\n}\n\nparam_cat = {\n     'bagging_temperature': 0.31768713094131684,\n     'depth': int(8.03),\n     'l2_leaf_reg': 1.3525686450404295,\n     'learning_rate': 0.18,\n     'iterations': 150,\n     'loss_function': 'RMSE',\n     'verbose': False\n}\n\n\nparam_xgb = {\n     'colsample_bytree': 0.8119098377889549,\n     'gamma': 2.244423418642122,\n     'learning_rate': 0.015800631696721114,\n     'max_depth': int(9.846),\n     'min_child_weight': int(15.664),\n     'subsample': 0.82345,\n     'objective': 'reg:squarederror',\n     'eval_metric':'rmse',\n     'num_boost_roun' : 500\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\nfrom xgboost import XGBRegressor\n\nestimators = [\n        ('lgbm', lightgbm.LGBMRegressor(**param_lgbm, random_state=7, n_jobs=-1)),\n        ('xgbr', XGBRegressor(**param_xgb, random_state=7, nthread=-1)),\n        ('cat', CatBoostRegressor(**param_cat))\n]\n\nreg = StackingRegressor(\n    estimators=estimators,\n    final_estimator=lightgbm.LGBMRegressor(),\n    n_jobs=-1,\n    cv=5\n)\n\ntrain_X, val_X,  train_Y, val_Y = train_test_split(\n    train_feature, train['target'], test_size=0.2, shuffle=True)\n\nreg.fit(train_X,train_Y)\n\nval_pred = reg.predict(val_X)\nscore = np.sqrt(mean_squared_error(val_Y, val_pred))\n\nprint(\"Final model RMSE: \",end = \"\")\nprint(score)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"finally we can make prediction to the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict\nreg = StackingRegressor(\n    estimators=estimators,\n    final_estimator=lightgbm.LGBMRegressor(),\n    n_jobs=-1,\n    cv=5\n)\n\nreg.fit(train_feature, train['target'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = reg.predict(test.drop(\"id\",axis = 1))\n\nsubmission = pd.DataFrame({\n        \"id\": test[\"id\"],\n        \"target\":test_pred\n    })\nsubmission.to_csv('stacking_sub.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}