{"cells":[{"metadata":{},"cell_type":"markdown","source":"# GBDTsの学習・検証曲線とOptunaを使ったパラメーターチューニング\n\nKaggle内で学習曲線や検証曲線についての記載が少ないと思ったので、その算出について記載しました。\n間違いがあったらご指摘いただけると嬉しいです。  \n\n学習曲線・検証曲線については以下参照  \nhttps://scikit-learn.org/stable/modules/learning_curve.html#validation-curve  \nhttps://www.dataquest.io/blog/learning-curves-machine-learning/  \n\nまた、XGBoost等の勾配ブースティングモデルのパラメーターチューニングにおいて、最適パラメーターを決める便利なパッケージOptunaの使い方について記載しました。  \n"},{"metadata":{},"cell_type":"markdown","source":"# ライブラリ―のインポート"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport optuna\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\n\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ntest  = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')\nsub = pd.read_csv('../input/tabular-playground-series-jan-2021/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking features and target columns\ndisplay(train.columns)\n# Checking dtypes\ndisplay(train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 注意\n本記事は、どちらかというと学習曲線や検証曲線の作成のために書いております。半分は自分の忘備録です。  \n学習曲線や検証曲線は時間がかかるので全データ（30万）ではなく、5%ランダムサンプリングしております。もしフルで行いたい人はランダムサンプリングしないで行ってください。"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_01 = train.sample(frac=0.05, replace=False, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_01[features]\ny = train_01['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost and Learning/Validation curves"},{"metadata":{},"cell_type":"markdown","source":"## 学習曲線（Learning curve）\n学習曲線とは、横軸にデータ数、縦軸にRSMEなどの指標を取り、TrainingセットとValidationセットとで別々にプロットしたものです。\n過学習や未学習の検討における視覚的ツールとなります。\nまた、現モデルにおいて、これ以上サンプルを追加する価値があるかどうかをある程度検討できるので、データ収集継続に関して有用な示唆を提供できる可能性があります。\n詳細は以下のUPL参照です。  \nhttps://scikit-learn.org/stable/modules/learning_curve.html#validation-curve  \nhttps://www.dataquest.io/blog/learning-curves-machine-learning/"},{"metadata":{"trusted":true},"cell_type":"code","source":"def learning_curves(estimator, title, X, y, cv= None, train_sizes=np.linspace(.3, 1.0, 5)):\n    \n    train_sizes, train_scores, validation_scores = \\\n        learning_curve(estimator, \n                       X,\n                       y,\n                       train_sizes = train_sizes,\n                       cv = cv, \n                       scoring = 'neg_mean_squared_error')\n\n    train_scores_mean = np.sqrt(-np.mean(train_scores, axis=1))\n    train_scores_std = np.sqrt(np.std(train_scores, axis=1))/2\n    validation_scores_mean = np.sqrt(-np.mean(validation_scores, axis=1))\n    validation_scores_std = np.sqrt(np.std(validation_scores, axis=1))/2\n    \n    plt.figure(figsize=[6.5,4])\n    plt.rcParams[\"font.size\"] = 12\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label = 'Training error')\n    plt.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\",label = 'Validation error')\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, validation_scores_mean - validation_scores_std,\n                     validation_scores_mean + validation_scores_std, alpha=0.1,\n                     color=\"g\")\n    \n    plt.rcParams[\"font.size\"] = 10\n    plt.ylabel('RMSE', fontsize = 14)\n    plt.xlabel('Training set size', fontsize = 14)\n    title = title\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.legend()\n    plt.ylim(0.575,0.775)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_xgb = {'lambda': 1,\n 'alpha': 0,\n 'colsample_bytree': 1,\n 'subsample': 1,\n 'learning_rate': 0.05,\n 'max_depth': 6,\n 'min_child_weight': 3,\n 'random_state': 48}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(**params_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'Learning curve'\nlearning_curves(model_xgb, title, X, y, cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trainingセットにおいては右肩上がりの、Validationセットにおいては若干右肩下がりのグラフになりました。典型的な学習曲線になります。  \nこのように、Trainingセットではサイズが小さいときには多くのデータにフィットするパラメーターが見つかるのでエラーが少なくなります。一方で、そのパラメーターはTrainingセットにのみ調整されているため、Validationセットにおいてはフィットが悪くエラーが大きくなります（図の左側）。  \nサイズが大きくなるにつれて、フィットが平均的になっていくため、Trainingセットのフィットは調整され、一見悪くなる様に見えますが、一方でValidationセットではフィットが良くなっていきます。  \nサイズを増やしても両者が離れたままだと過学習が残った状態といえます。\n今回の図を見ると、徐々に両者が近づいているので、サンプルサイズをもっと多くすればよりフィットが良くなると考えられます。今回はデータの5%しか使っていませんので、全てのデータを使えばより良いフィットが期待できると言えそうです。"},{"metadata":{},"cell_type":"markdown","source":"## 検証曲線（Validation curve）\n検証曲線とは、横軸にパラメーター（例えば正則化パラメーターであるalpha）を取り、縦軸にRSMEなどの指標を取ります。TrainingセットとValidationセット毎にプロットしていくのは同様です。そのパラメーターを変化させたときにTrainingセットとValidationセットでどのような挙動を取るのかを視覚的に見る事が出来ます。最終的なパラメーターの決定根拠として利用されることがあります。  \n注意：本来、検証曲線は利用可能なすべてのデータ（30万）で行うべきです。以下の結果は計算速度を優先したため、5%サンプリングのデータを使って検証曲線を作成しています。参考値として扱ってください。\n\n後述するOptunaのような半自動設定の場合には、学習曲線や検証曲線を確認する事はないのかもしれませんね（なのでKaggle内では記述が少ないのかもと思いました）。"},{"metadata":{"trusted":true},"cell_type":"code","source":"def validation_curves(estimator, title, X, y,\n                      cv= None, param_name= None, param_range=None):\n    \n    train_scores, test_scores = \\\n        validation_curve(estimator, \n                         X, \n                         y, \n                         param_name=param_name, \n                         param_range=param_range,\n                         cv = cv,\n                         scoring='neg_mean_squared_error', #'roc_auc'\n                         n_jobs=4)\n    train_scores_mean = np.sqrt(-np.mean(train_scores, axis=1))\n    train_scores_std = np.sqrt(np.std(train_scores, axis=1))\n    test_scores_mean = np.sqrt(-np.mean(test_scores, axis=1))\n    test_scores_std = np.sqrt(np.std(test_scores, axis=1))\n\n    plt.rcParams[\"font.size\"] = 12\n    plt.title(title, fontsize = 20)\n    plt.xlabel(param_name, fontsize =14)\n    plt.ylabel(\"Score\", fontsize = 14)\n    plt.ylim(0.5, 0.9)\n    lw = 2\n    plt.plot(param_range, train_scores_mean, label=\"Training score\",\n             color=\"darkorange\", lw=lw)\n    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.2,\n                     color=\"darkorange\", lw=lw)\n    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n             color=\"navy\", lw=lw)\n    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.2,\n                     color=\"navy\", lw=lw)\n    plt.rcParams[\"font.size\"] = 10\n    plt.legend(loc=\"best\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_range = np.linspace(0, 1, 10)\nparam_range","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_name = \"alpha\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Validation Curves for alpha\"\nvalidation_curves(model_xgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"alphaについては改善の余地がなさそうですね。"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_name = \"lambda\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Validation Curves for lambda\"\nvalidation_curves(model_xgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lambdaも改善の余地がなさそうです。"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_range = np.linspace(0.1, 1, 10)\nparam_range","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_name = 'colsample_bytree'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Validation Curves for colsample\"\nvalidation_curves(model_xgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"colsample_bytreeもValidationセットにおいては変化があまりなさそうですね。"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_name = 'subsample'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Validation Curves for subsample\"\nvalidation_curves(model_xgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"明確にどこが良いというのは無いように見えます。"},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"param_name = 'n_estimators'","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"param_range = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"title = \"Validation Curve for n_estimators\"\nvalidation_curves(model_xgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optunaを利用したハイパーパラメーターチューニング\nさて、上記の様に1つ1つみていくのも大事だと思いますが、あまり明確な結論は得られませんでした。また、パラメーターの数が多いのである程度自動的に決めてしまいたいと思います。  \nここではOptunaという便利な機能を利用したいと思います。\n以下のNotebookが参考になります。  \nhttps://www.kaggle.com/hamzaghanmi/xgboost-hyperparameter-tuning-using-optuna  \n"},{"metadata":{},"cell_type":"markdown","source":"### 以下は時間がかかるのでGPU使用設定にしています。（それでも時間がかかる）\n学習曲線の結果から、データ数が上昇すればするほど過学習が緩和され、予測性能が良くなる傾向がみられました。ここから行うOptunaを用いたチューニングでは、30万データ全てを用いる事にします。その結果を提出してスコアを見てみましょう。  \nかなり時間がかかるのでGPUを使用する設定にしております。"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[features]\ny = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial,data=X,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 1),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 1),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.1, 0.2, 0.3,0.5,0.7,0.9]),\n        'subsample': trial.suggest_categorical('subsample', [0.1, 0.2,0.3,0.4,0.5,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.0008, 0.01, 0.015, 0.02,0.03, 0.05,0.08,0.1]),\n        'n_estimators': 4000,\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20,23,25]),\n        'random_state': 48,\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 400),\n    }\n    model = xgb.XGBRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"study.best_trial.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Best_params_xgb = {'lambda': 0.001951466677835018,\n 'alpha': 0.7843235982110978,\n 'colsample_bytree': 0.5,\n 'subsample': 0.8,\n 'learning_rate': 0.01,\n 'max_depth': 11,\n 'min_child_weight': 205,\n 'n_estimators': 3000,\n 'random_state': 48,\n 'tree_method':'gpu_hist'}","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"X = train[features]\ny = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.15,random_state=42)\nmodel_xgb = xgb.XGBRegressor(**Best_params_xgb)\nmodel_xgb.fit(train_x,train_y,eval_set=[(test_x,test_y)],\n              early_stopping_rounds=100,verbose=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = pd.Series(model_xgb.feature_importances_, index = features)\nimportances = importances.sort_values()\nimportances.plot(kind = \"barh\")\nplt.title(\"imporance in the xgboost Model\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model_xgb.predict(test_x)\nrmse = mean_squared_error(test_y, preds,squared=False)\nrmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 提出用ファイル作成\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = test[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model_xgb.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target']=preds\nsub.to_csv('submission_xgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM and Learning/Validation curves\n上記同様、学習曲線と検証曲線の作成をまず行います。  \n次にOptunaを用いたパラメーターチューニングを行います。"},{"metadata":{},"cell_type":"markdown","source":"## 学習曲線"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_01[features]\ny = train_01['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params_lgb = {'num_leaves': 31,\n 'min_data_in_leaf': 20,\n 'min_child_weight': 0.001,\n 'max_depth': -1,\n 'learning_rate': 0.005,\n 'bagging_fraction': 1,\n 'feature_fraction': 1,\n 'lambda_l1': 0,\n 'lambda_l2': 0,\n 'random_state': 48}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb = lgb.LGBMRegressor(**params_lgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = 'Learning curve'\nlearning_curves(model_lgb, title, X, y, cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 検証曲線\n上記と同様にlambda_l1 (=alpha), lambda_l2, feature_fraction, bagging_fraction, n_estimatorsについて検証曲線を描きます。"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_range = np.linspace(0, 1, 10)\nparam_range","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_name = 'lambda_l1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Validation Curves for lambda_l1\"\nvalidation_curves(model_lgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_name = 'lambda_l2'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Validation Curves for lambda_l2\"\nvalidation_curves(model_lgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_name = 'feature_fraction'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Validation Curves for feature_fraction\"\nvalidation_curves(model_lgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_name = 'bagging_fraction'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Validation Curves for bagging_fraction\"\nvalidation_curves(model_lgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_name = 'n_estimators'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_range = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title = \"Validation Curves for n_estimators\"\nvalidation_curves(model_lgb, title, X, y, cv=5, \n                  param_name = param_name, param_range = param_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Optunaを用いたハイパーパラメーターチューニング\n上記同様にOptunaにお任せします。"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train[features]\ny = train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective_lgb(trial,data=X,target=y):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-3, 1),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-3, 1),\n        'feature_framcion': trial.suggest_categorical('feature_framcion', [0.1, 0.2, 0.3,0.5,0.7,0.9]),\n        'bagging_fraction': trial.suggest_categorical('bagging_framcion', [0.1, 0.2,0.3,0.4,0.5,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.0008, 0.01, 0.015, 0.02,0.03, 0.05,0.08,0.1]),\n        'n_estimators': 4000,\n        'num_leaves': trial.suggest_categorical('num_leaves', [31,50,150,200,250,300,350]),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20,23,25]),\n        'min_data_in_leaf': trial.suggest_categorical('min_data_in_leaf', [10,20,30]),\n        'min_child_weight': trial.suggest_categorical('min_child_weight', [0.001,0.005, 0.01, 0.05, 0.1,0.5]),\n        'random_state': 48\n    }\n    model = lgb.LGBMRegressor(**param)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(test_x)\n    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective_lgb, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study.best_trial.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Best_params_lgb = {'lambda_l2': 0.013616569506899653,\n 'lambda_l1': 0.006495842188985166,\n 'feature_framcion': 0.3,\n 'bagging_framcion': 0.3,\n 'learning_rate': 0.015,\n 'num_leaves': 200,\n 'max_depth': 25,\n 'min_data_in_leaf': 30,\n 'min_child_weight': 0.001,\n 'n_estimators': 3000,\n 'random_state': 48,\n 'tree_method':'gpu_hist'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.15,random_state=42)\nmodel_lgb = lgb.LGBMRegressor(**Best_params_lgb)\nmodel_lgb.fit(train_x,train_y,eval_set=[(test_x,test_y)],\n              early_stopping_rounds=100,verbose=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = pd.Series(model_lgb.feature_importances_, index = features)\nimportances = importances.sort_values()\nimportances.plot(kind = \"barh\")\nplt.title(\"imporance in the lightGBM Model\", fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model_lgb.predict(test_x)\nrmse = mean_squared_error(test_y, preds,squared=False)\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X = test[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model_lgb.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target']=preds\nsub.to_csv('submission_lgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### この記事がお役に立ったならば幸いです！\nよろしければイイね　(・∀・)ｲｲﾈ!!　お願いいたします。"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}