{"cells":[{"metadata":{},"cell_type":"markdown","source":"[KerasTuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) helps you find a best set of your neural network's hyperparameters.\n\nSo let's give it a go!"},{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import decomposition\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, train_test_split\nfrom tqdm.auto import tqdm\n\nimport tensorflow as tf \n# import tensorflow_addons as tfa\n# !pip install -q -U keras-tuner\nimport kerastuner as kt # keras tuner!\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('seaborn-colorblind')\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\nNFOLD = 10\nOUTPUT_DIR = ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logging is always nice for your experiment:)\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = init_logger()\nlogger.info('Start Logging...')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')\n\nfeatures = [f'cont{i}' for i in range(1, 15)]\ntarget_col = 'target'\n\nX_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop('id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test.shape)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning NN with kerastuner\nWe use a simple MLP and tune the hyperparameters!"},{"metadata":{},"cell_type":"markdown","source":"## Scaling\nTo make sure similar range across features"},{"metadata":{"trusted":true},"cell_type":"code","source":"prep = StandardScaler()\ndf = pd.concat([X_train[features], X_test[features]])\ndf[features] = prep.fit_transform(df[features].values)\nX_test[features] = df[features].iloc[len(X_train):]\nX_train[features] = df[features].iloc[:len(X_train)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_test.shape)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"# my default NN hyperparameters\nparams = {\n    'input_dim': len(features),\n    'input_dropout': 0.0,\n    'hidden_layers': 3,\n    'hidden_units': 256,\n    'hidden_activation': 'relu',\n    'lr': 1e-03,\n    'dropout': 0.2,\n    'batch_size': 128,\n    'epochs': 192\n}\nlogger.info('default NN params:')\nlogger.info(params)\n\ndef tuning_model(hp, params=params):\n    \"\"\"\n    model tuning with KerasTuner\n    \"\"\"\n    \n    inputs = tf.keras.layers.Input(shape=(params['input_dim'], ))\n    x = tf.keras.layers.BatchNormalization()(inputs)\n    x = tf.keras.layers.Dense(hp.Int('num_units_1', 128, 512, step=128), activation=params['hidden_activation'])(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(hp.Float('dropout_1', 0.0, 0.5, step=0.1, default=0.5))(x)\n\n    for i in range(hp.Int('num_layers', 1, 3)):\n        x = tf.keras.layers.Dense(hp.Int(f'num_units_{i+2}', 128, 512, step=128))(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dropout(hp.Float(f'dropout_{i+2}', 0.0, 0.5, step=0.1, default=0.5))(x)\n        \n    # output\n    out = tf.keras.layers.Dense(1, activation='linear', name = 'out')(x)\n    model = tf.keras.models.Model(inputs=inputs, outputs=out)\n   \n    # compile\n    loss = tf.keras.losses.MeanSquaredError()\n    opt = tf.keras.optimizers.Adam(lr=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n    model.compile(loss=loss, optimizer=opt, metrics=['mse'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dataset for NN based on a task\ntrain, valid, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.3, random_state=SEED)\n\ntrain_set = {'X': train[features].values, 'y': train_y.values}\nvalid_set = {'X': valid[features].values, 'y': valid_y.values}  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - RandomSearch, Hyperband, BayesianOptimization, and Sklearn.\n\nHere we use the BayesianOptimization tuner."},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a custom tuner to tune the batch size\nclass MyTuner(kt.tuners.BayesianOptimization):\n  def run_trial(self, trial, *args, **kwargs):\n    # You can add additional HyperParameters for preprocessing and custom training loops\n    # via overriding `run_trial`\n    kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 128, 8192, step=128)\n#     kwargs['epochs'] = trial.hyperparameters.Int('epochs', 10, 30)\n    super(MyTuner, self).run_trial(trial, *args, **kwargs)\n\n# instantiate KerasTuner\nmodel_ft = lambda hp: tuning_model(hp, params)\ntuner = MyTuner(\n    hypermodel=model_ft,\n    objective=kt.Objective('val_loss', direction='min'),\n    num_initial_points=4,\n    max_trials=20,\n    overwrite=True)\n\n# perform tuning\ntuner.search(train_set['X'], train_set['y'], verbose=2,\n             epochs = 8, validation_data = (valid_set['X'], valid_set['y']))\n\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n\n# Build the model with the optimal hyperparameters and train it on the data\nmodel = tuner.hypermodel.build(best_hps)\n\n# disp best params\nlogger.info('Best hyperparameters:')\nlogger.info(best_hps.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit with KFold\nLet's fit the model with the best parameters to the data with KFold."},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_model(tuner, best_hps, X_train, y_train, X_test, features=features, n_fold=NFOLD, seed=SEED):\n    cv = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n\n    models = []\n    oof_train = np.zeros((len(X_train),))\n    y_preds = np.zeros((len(X_test),))\n\n    for fold_id, (train_index, valid_index) in tqdm(enumerate(cv.split(X_train, y_train))):\n        # split\n        X_tr = X_train.loc[train_index, features].values\n        X_val = X_train.loc[valid_index, features].values\n        y_tr = y_train.loc[train_index].values\n        y_val = y_train.loc[valid_index].values\n        \n        # model\n        tf.keras.backend.clear_session()\n        model = tuner.hypermodel.build(best_hps)\n            \n        # callbacks\n        er = tf.keras.callbacks.EarlyStopping(patience=16, restore_best_weights=True, monitor='val_loss')\n        ReduceLR = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=1, mode='min')\n        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'mybestweight{fold_id}.hdf5', \n                                                              save_weights_only=True, verbose=0, monitor='val_loss', save_best_only=True)\n\n        # fit\n        history = model.fit(X_tr, y_tr, callbacks=[er, ReduceLR, model_checkpoint_callback], \n                            verbose=2, epochs=192, batch_size=best_hps.values['batch_size'],\n                            validation_data=(X_val, y_val)) \n        \n        # predict\n        oof_train[valid_index] = model.predict(X_val).ravel()\n        y_pred = model.predict(X_test[features].values).ravel()\n        y_preds += y_pred / n_fold\n        models.append(model)\n        \n    return oof_train, y_preds, models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_train, y_preds, models = fit_model(tuner, best_hps, X_train, y_train, X_test, features=features, n_fold=NFOLD, seed=SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CV score"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'CV (Tuned MLP): {mean_squared_error(y_train, oof_train, squared=False)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/tabular-playground-series-jan-2021/sample_submission.csv')\nsub['target'] = y_preds\nsub.to_csv('submission.csv', index=False)\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All done, good job!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}