{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as gbm\nimport matplotlib.pyplot as plt\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nfrom scipy.signal import find_peaks","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv').drop(columns=['id'])\ndf_test = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv').drop(columns=['id'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"counts = []\nbins = []\nfor i in range(1,15):\n    plt.figure(figsize=(14,7))\n    count_i, bins_i, _ = plt.hist(df_train[f'cont{i}'],bins=500)\n    counts.append(count_i)\n    bins.append(bins_i)\n    del count_i\n    del bins_i","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# pd.plotting.scatter_matrix(df_train, alpha=0.005, figsize=(200,100), hist_kwds={'bins':1000})\n# plt.savefig('scatter.png')\n# plt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# gm = GaussianMixture(n_components=10,\n#                      covariance_type='spherical'\n#                     ).fit(df_train['cont2'].values.reshape(-1, 1))\n# print(gm.bic(df_train['cont2'].values.reshape(-1, 1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# gm.weights_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sorted(list(zip(gm.means_,gm.covariances_)),key=lambda x:x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# list(zip(gm.means_,gm.covariances_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# print(np.arange(0,1,0.2).reshape(-1,1))\n# print(gm.predict(np.arange(0,1,0.2).reshape(-1,1)))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# feat_index = 14\n# hist_smooth = pd.DataFrame({'bins':bins[feat_index-1][:-1],'counts':counts[feat_index-1]}).set_index(\n#     'bins'\n# ).rolling(window=5,center=True).mean()\n# hist_smooth.plot(kind='line', figsize=(14,7))\n# peaks = find_peaks(hist_smooth['counts'], width=5)\n# for peak in hist_smooth.iloc[peaks[0]].index.values:\n#     plt.axvline(x=peak,c='r')\n# print(peaks)\n# print(hist_smooth.iloc[peaks[0]])\n# print(len(hist_smooth.iloc[peaks[0]]))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# gm = GaussianMixture(n_components=8,\n#                      covariance_type='spherical',\n#                      max_iter=1000, random_state=12345\n#                     ).fit(df_train['cont10'].values.reshape(-1, 1))\n# gm.means","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# sorted(list(zip(gm.weights_,list(map(lambda x: x[0], gm.means_)),gm.covariances_)),key=lambda x:x[1])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"\n# sorted(list(zip(gm.weights_,gm.means_,gm.covariances_)),key=lambda x:x[1])\n# pdf = (\n#     clf.weights_[i]\n#     * stats.norm(\n#         clf.means_[i, 0],\n#         np.sqrt(clf.covariances_[i, 0])\n#     ).pdf(xpdf)\n# )","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# feat_index = 6\n# hist_smooth = pd.DataFrame({'bins':bins[feat_index-1][:-1],'counts':counts[feat_index-1]}).set_index(\n#     'bins'\n# ).rolling(window=10,center=True).mean()\n# hist_smooth_dist = hist_smooth/hist_smooth.sum()\n# hist_smooth_dist.plot(kind='line', figsize=(14,7))\n# # peaks = find_peaks(hist_smooth['counts'], width=5)\n# # for dist in sorted(list(zip(gm.means_,gm.covariances_)),key=lambda x:x[0]):\n\n# pdf = np.array(list(map(lambda x: x[0]*stats.norm(x[1], np.sqrt(x[2])).pdf(np.linspace(0,1,1000).reshape((-1,1))),\n#                        sorted(list(zip(gm_hist[feat_index-1].weights_,list(map(lambda x: x[0], gm_hist[feat_index-1].means_)),gm_hist[feat_index-1].covariances_)),key=lambda x:x[1]))))\n# # plt.figure(figsize=(14,7))\n# plt.plot(np.linspace(0,1,1000), pdf.sum(axis=2).sum(axis=0)/pdf.sum())\n\n# for peak in gm_hist[feat_index-1].means_:#hist_smooth.iloc[peaks[0]].index.values:\n#     plt.axvline(x=peak[0],c='r')\n# # print(peaks)\n# # print(hist_smooth.iloc[peaks[0]])\n# # print(len(hist_smooth.iloc[peaks[0]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"hist_smooth_list = []\nfor feat_index in range(1,15):\n    hist_smooth_list.append(pd.DataFrame({'bins':bins[feat_index-1][:-1],'counts':counts[feat_index-1]}).set_index(\n        'bins'\n    ).rolling(window=10,center=True).mean().fillna(0))\nhist_smooth_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gm_hist = []\nmean_init = [\n    None,\n    None,\n    [0.24, 0.35, 0.5, 0.55, 0.64, 0.73, 0.86, 0.95],\n#     [0.24, 0.32, 0.35, 0.5, 0.55, 0.64, 0.73, 0.86, 0.95],\n    [0.22, 0.29, 0.38, 0.48, 0.56, 0.62, 0.75, 0.85],\n    None,\n    None,\n    None,\n    [0.28, 0.36, 0.45, 0.53, 0.63, 0.75, 0.83, 0.9, 0.98],\n    [0.1, 0.17, 0.3, 0.36, 0.42, 0.58, 0.61, 0.9],\n    [0.24, 0.32, 0.39, 0.44, 0.50, 0.61, 0.72, 0.82],\n    [0.23, 0.32, 0.43, 0.48, 0.6, 0.66, 0.81],\n    [0.28, 0.37, 0.48, 0.59, 0.69, 0.8, 0.92],\n    [0.35, 0.48, 0.62, 0.71, 0.8, 0.9],\n    None\n]\npeak_count = [4,10,8,8,9,7,4,9,8,8,7,7,6,9]\n\nfor i in range(14):\n    gm = GaussianMixture(n_components=peak_count[i],\n                         covariance_type='spherical',\n                         max_iter=1000, random_state=12345,\n                         means_init=np.array(mean_init[i]).reshape(-1,1) if mean_init[i] else None\n                        ).fit(df_train[f'cont{i+1}'].values.reshape(-1, 1))\n    gm_hist.append(gm)\n    del gm\n\n    \ncounts = []\nbins = []\nfor i in range(1,15):\n#     plt.figure(figsize=(14,7))\n    count_i, bins_i, _ = plt.hist(df_train[f'cont{i}'],bins=500)\n    counts.append(count_i)\n    bins.append(bins_i)\n    del count_i\n    del bins_i\n    plt.close()\n    \nhist_smooth_list = []\nfor feat_index in range(1,15):\n    hist_smooth_list.append(pd.DataFrame({'bins':bins[feat_index-1][:-1],'counts':counts[feat_index-1]}).set_index(\n        'bins'\n    ).rolling(window=10,center=True).mean().fillna(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# df_train.head(10)['cont14'].apply(lambda x: np.abs(gm_info[:,1]-x).argmin())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# gm_info[:,0][df_train.head(10)[f'cont{i}'].apply(lambda x: np.abs(gm_info[:,1]-x).argmin())]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"for i in range(1,15):\n    gm_info = np.array(sorted(list(zip(gm_hist[i-1].weights_,\n                                   list(map(lambda x: x[0], gm_hist[i-1].means_)),\n                                   gm_hist[i-1].covariances_)),key=lambda x:x[1]))\n    for j, item in enumerate(gm_info):\n        df_train[f'cont{i}_zscore_cluster_{j}'] = (df_train[f'cont{i}']-item[1])/np.sqrt(item[2])\n        df_train[f'cont{i}_prob_cluster_{j}'] = stats.norm.cdf(df_train[f'cont{i}_zscore_cluster_{j}'])\n#         df_train[f'cont{i}_weight_cluster_{j}'] = item[0]\n        \n        df_test[f'cont{i}_zscore_cluster_{j}'] = (df_test[f'cont{i}']-item[1])/np.sqrt(item[2])\n        df_test[f'cont{i}_prob_cluster_{j}'] = stats.norm.cdf(df_test[f'cont{i}_zscore_cluster_{j}'])\n#         df_test[f'cont{i}_weight_cluster_{j}'] = item[0]\n    \n    df_train[f'cont{i}_cluster'] = df_train[f'cont{i}'].apply(lambda x: np.abs(gm_info[:,1]-x).argmin())\n    df_train[f'cont{i}_cluster_weight'] = gm_info[:,0][df_train[f'cont{i}_cluster']]\n    df_train[f'cont{i}_zscore_overall'] = (df_train[f'cont{i}']-df_train[f'cont{i}'])/df_train[f'cont{i}'].std()\n    df_train[f'cont{i}_prob_overall'] = stats.norm.cdf(df_train[f'cont{i}_zscore_overall'])\n    df_train[f'cont{i}_rarity_overall'] = df_train[f'cont{i}'].apply(\n        lambda x: hist_smooth_list[i-1].iloc[[np.abs(hist_smooth_list[i-1].index.values - x).argmin()]]['counts'].values[0])\n    \n    df_test[f'cont{i}_cluster'] = df_test[f'cont{i}'].apply(lambda x: np.abs(gm_info[:,1]-x).argmin())\n    df_test[f'cont{i}_cluster_weight'] = gm_info[:,0][df_test[f'cont{i}_cluster']]\n    df_test[f'cont{i}_zscore_overall'] = (df_test[f'cont{i}']-df_train[f'cont{i}'])/df_train[f'cont{i}'].std()\n    df_test[f'cont{i}_prob_overall'] = stats.norm.cdf(df_test[f'cont{i}_zscore_overall'])\n    df_test[f'cont{i}_rarity_overall'] = df_test[f'cont{i}'].apply(\n        lambda x: hist_smooth_list[i-1].iloc[[np.abs(hist_smooth_list[i-1].index.values - x).argmin()]]['counts'].values[0])\n    print(i)\n    \ndf_train['contsum'] = df_train[[f'cont{i}' for i in range(1,15)]].sum(axis=1)\ndf_train['contprobsum'] = df_train[[f'cont{i}_prob_overall' for i in range(1,15)]].sum(axis=1)\ndf_train['contclustersum'] = df_train[[f'cont{i}_cluster' for i in range(1,15)]].sum(axis=1)\ndf_train['contclusterweightsum'] = df_train[[f'cont{i}_cluster_weight' for i in range(1,15)]].sum(axis=1)\ndf_train['contraritysum'] = df_train[[f'cont{i}_rarity_overall' for i in range(1,15)]].sum(axis=1)\n\ndf_test['contsum'] = df_test[[f'cont{i}' for i in range(1,15)]].sum(axis=1)\ndf_test['contprobsum'] = df_test[[f'cont{i}_prob_overall' for i in range(1,15)]].sum(axis=1)\ndf_test['contclustersum'] = df_test[[f'cont{i}_cluster' for i in range(1,15)]].sum(axis=1)\ndf_test['contclusterweightsum'] = df_test[[f'cont{i}_cluster_weight' for i in range(1,15)]].sum(axis=1)\ndf_test['contraritysum'] = df_test[[f'cont{i}_rarity_overall' for i in range(1,15)]].sum(axis=1)\ndf_train.to_csv('./df_train_feat.csv', index=False)\ndf_test.to_csv('./df_test_feat.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as gbm\n# import matplotlib.pyplot as plt\n# from sklearn.mixture import GaussianMixture\n# from scipy import stats\n# from scipy.signal import find_peaks","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train = pd.read_csv('./df_train_feat.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X = df_train.drop(columns=['target'])\ny = df_train['target']\ndel df_train","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# params from this kernel https://www.kaggle.com/kailex/tabular-playground\n# params={'random_state': 33,'n_estimators':5000,\n#  'min_data_per_group': 5,\n#  'boosting_type': 'gbdt',\n#  'num_leaves': 256,\n#  'max_depth': -1,\n#  'learning_rate': 0.02,\n#  'subsample_for_bin': 200000,\n#  'lambda_l1': 1.074622455507616e-05,\n#  'lambda_l2': 2.0521330798729704e-06,\n#  'n_jobs': -1,\n#  'cat_smooth': 1.0,\n#  'importance_type': 'split',\n#  'metric': 'rmse',\n#  'feature_pre_filter': False,\n#  'bagging_fraction': 0.8206341150202605,\n#  'min_data_in_leaf': 100,\n#  'min_sum_hessian_in_leaf': 0.001,\n#  'bagging_freq': 6,\n#  'feature_fraction': 0.5,\n#  'min_gain_to_split': 0.0,\n#  'min_child_samples': 20,\n#         'verbose':1\n# }\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'learning_rate': 0.025,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 5,\n    'verbose': 0,\n    \"max_depth\": 10,\n    \"num_leaves\": 1024,\n    \"max_bin\": 15,\n    \"num_iterations\": 100,\n    \"n_estimators\": 500,\n    'lambda_l1': 1.074622455507616e-05,\n    'lambda_l2': 2.0521330798729704e-06,\n}\n# X = df_train.drop(columns=['target'])\n# y = df_train['target']\n# del df_train\n\nX = gbm.Dataset(X, label=y, feature_name=X.columns.tolist())\nest = gbm.train(params, X)\ndel X\ndf_test = pd.read_csv('./df_test_feat.csv')\ny_pred = est.predict(df_test)\ndel df_test\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# y_pred = est.predict(df_test)\nsample_sub = pd.read_csv('../input/tabular-playground-series-jan-2021/sample_submission.csv')\nsample_sub['target'] = y_pred\nsample_sub.to_csv('./submit5.csv',index=False)\nsample_sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sorted(list(zip(est.feature_name(),est.feature_importance(importance_type='gain'))),key=lambda x:x[1])[::-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# 1 - 4\n# 2 - 10\n# 3 - 9\n# 4 - 7\n# 5 - 9\n# 6 - 7\n# 7 - 4\n# 8 - 9\n# 9 - 8\n# 10 - 7\n# 11 - 7\n# 12 - 7\n# 13 - 6\n# 14 - 9\n\n# 3 - [0.22, 0.3, 0.35, 0.5, 0.55, 0.64, 0.73, 0.86, 0.95]\n# 4 - [0.22, 0.29, 0.38, 0.48, 0.56, 0.62, 0.75, 0.85]\n# 8 - [0.28, 0.36, 0.45, 0.53, 0.63, 0.75, 0.83, 0.9, 0.98]\n# 9 - [0.1, 0.17, 0.3, 0.36, 0.42, 0.58, 0.61, 0.9]\n# 10 - [0.24, 0.32, 0.39, 0.44, 0.50, 0.61, 0.72, 0.82]\n# 11 - [0.23, 0.32, 0.43, 0.48, 0.6, 0.66, 0.81]\n# 12 - [0.28, 0.37, 0.48, 0.59, 0.69, 0.8, 0.92]\n# 13 - [0.35, 0.48, 0.62, 0.71, 0.8, 0.9]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# df_train.head(10)['cont1'].apply(lambda x: hist_smooth.iloc[[np.abs(hist_smooth.index.values - x).argmin()]]['counts'].values[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sorted(list(zip(gm_hist[feat_index-1].weights_,list(map(lambda x: x[0], gm_hist[feat_index-1].means_)),gm_hist[feat_index-1].covariances_)),key=lambda x:x[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# gm = GaussianMixture(n_components=7,\n#                  covariance_type='spherical', max_iter=1000, #means_init=np.array([0.22, 0.3, 0.35, 0.5, 0.55, 0.64, 0.73, 0.86, 0.95]).reshape(-1,1)\n#                 ).fit(df_train['cont4'].values.reshape(-1, 1))\n# sorted(list(zip(gm.means_,gm.covariances_)),key=lambda x:x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# hist_smooth = pd.DataFrame({'bins':bins[feat_index-1][:-1],'counts':counts[feat_index-1]}).set_index(\n#     'bins'\n# ).rolling(window=10,center=True).mean()\n# peaks = find_peaks(hist_smooth['counts'], width=5)\n# print(hist_smooth.iloc[peaks[0]])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}