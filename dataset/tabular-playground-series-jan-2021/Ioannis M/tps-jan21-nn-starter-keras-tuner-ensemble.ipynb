{"cells":[{"metadata":{},"cell_type":"markdown","source":"## NN Starter + Keras Tuner Ensemble\n\nThis notebook will show you steb-by-step how to:\n\n- Use a TF-Keras neural network for tabular data (regression)\n- Use `KerasTuner` to find high-performing model configurations\n- Ensemble a few of the top models to generate final predictions\n\nReferences:\n\n- https://www.kaggle.com/fchollet/moa-keras-kerastuner-best-practices\n- https://github.com/keras-team/keras-tuner\n\nNote: This notebook is addressed more to newers/mid-level in deep learning, aligned with the purpose of this Playground competition. \nExperienced kagglers probably won't learn anything new. "},{"metadata":{},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/keras-team/keras-tuner.git -q","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import os, sys, gc\nimport time, random\nimport numpy as np\nimport pandas as pd\nimport logging\nimport typing as tp\nfrom pathlib import Path\nfrom contextlib import contextmanager\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import *\n\nprint('TF version:', tf.__version__)\nprint('GPU devices:', tf.config.list_physical_devices('GPU'))\nprint(\"GPU available: \", tf.test.is_gpu_available())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET = \"target\"\n# IDVAR = 'id'\nN_OUTS = 1\nBS = 128\nEPOCHS = 10\n\nDIR = \"../input/tabular-playground-series-jan-2021/\"\nWORK = \"./\"\n\n\nnum_cols = [f'cont{i}' for i in range(1,15)]\nall_cols = num_cols  # + string_cols + categorical_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(DIR+\"train.csv\")\ntrain['id'] = train.index\n# train_labels = train[TARGET].values\n\ntest = pd.read_csv(DIR+\"test.csv\")\ntest_index = test['id']\n\nsub = pd.read_csv(DIR+\"sample_submission.csv\")\n\nprint('Raw data loaded!')\nprint(\"Train: {}, Test: {}, Sample sub: {}\".format(train.shape, test.shape, sub.shape))\n\n\n# split to train/valid sets\nprint('Split to train/valid sets:\\n')\nval_df = train[all_cols].sample(frac=0.2, random_state=2020)\ntrain_df = train[all_cols].drop(val_df.index)\nprint('Train shape:', train_df.shape)    \nprint('Valid shape:', val_df.shape)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train.sample(5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train[[TARGET]].plot(figsize=(16, 8));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Dataset"},{"metadata":{},"cell_type":"markdown","source":"1) Encode our features\n\nFirst we need to encode our input variables before passing them to the NN. \nSince we have only numerical features we use a single `Normalization` layer to encode each feature separately.\nThen, we concatenate the entire feature space into a single vector. \n\nWe wrap all the above steps in the following python method: `encode_numerical_feature` "},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers.experimental.preprocessing import Normalization\n\ndef encode_numerical_feature(feature, name, dataset):\n    normalizer = Normalization()                    # Create a Normalization layer for each feature\n    feature_ds = dataset.map(lambda x, y: x[name])  # Prepare a TF-Dataset that only yields our feature\n    normalizer.adapt(feature_ds)                    # Learn the statistics of the data\n    encoded_feature = normalizer(feature)           # Normalize the input feature\n    return encoded_feature\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's turn our dataframes into `tf.data.Dataset`, which we will use to train our Keras models in the next step.\nThe following method: `dataframe_to_dataset` does exactly that. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# code part coppied from: https://www.kaggle.com/nicapotato/keras-nn-tabular-regression-problem\n\ndef dataframe_to_dataset(dataframe, labels, role, BATCHSIZE):\n    dataframe = dataframe.copy()\n    if role != \"test\":\n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    else: \n        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if role == \"train\":\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(BATCHSIZE)\n    return ds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ds = dataframe_to_dataset(train_df, train.loc[train_df.index, TARGET], \"train\", BS)\nval_ds = dataframe_to_dataset(val_df, train.loc[val_df.index, TARGET], \"val\", BS)\ntest_ds = dataframe_to_dataset(test[all_cols], np.zeros((test.shape[0], N_OUTS)), \"test\", BS)\n\n# full dataset\nfull_train_ds = dataframe_to_dataset(train[all_cols], train[TARGET], \"train\", BS)\n\nprint('Training ds steps:', int(train_ds.cardinality()))\nprint('Validation ds steps:', int(val_ds.cardinality()))\nprint('Test ds steps:', int(test_ds.cardinality()))\nprint()\nprint('Full Training ds steps:', int(full_train_ds.cardinality()))\n\n# train_ds = train_ds.shuffle(1024).batch(BS).prefetch(8)\n# val_ds = val_ds.batch(BS).prefetch(8)\n# test_ds = test_ds.batch(BS).prefetch(8)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# sanity check \n\n# import pprint as pp\n\n# print('Look at Data')\n# for x, y in val_ds.take(1):\n#     pp.pprint(x)\n#     pp.pprint(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training a baseline model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We use TF Functional API to create aour NN model\n# For more info see here: \n\n\ndef base_model():\n\n    num_inputs = [Input(shape=(1,), name=x) for x in num_cols]\n    encoded_nums = [encode_numerical_feature(var_input, var_name, train_ds)\n                   for var_input, var_name in zip(num_inputs, num_cols)]\n    \n    all_feats = Concatenate()(encoded_nums)\n\n    x = Dropout(0.2)(all_feats)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(32, activation=\"relu\")(x)\n    x = Dropout(0.2)(x)\n    out = Dense(1, activation='linear')(x)\n    base_model = tf.keras.Model(num_inputs, out)\n    \n    # compile model \n    base_model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=2e-3),\n        loss=\"mse\",\n        metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n\n    return base_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = base_model()\nbase_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set callbacks \nes = EarlyStopping(monitor='val_loss', min_delta=0.0001,patience=5, verbose=1, mode='min',restore_best_weights=True)\nrlr = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=0.00001, verbose=0)\n# ckp = callbacks.ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\nhist = base_model.fit(train_ds, \n                      batch_size=BS, \n                      epochs=EPOCHS,\n                      validation_data=val_ds, \n                      verbose=1, \n                      callbacks=[es, rlr])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from nb: https://www.kaggle.com/nicapotato/keras-nn-tabular-regression-problem/\n\nplot_metrics = ['loss', 'rmse']\n\nf, ax = plt.subplots(1,2,figsize = [12,4])\nfor p_i,metric in enumerate(plot_metrics):\n    ax[p_i].plot(hist.history[metric], label='Train ' + metric, )\n    ax[p_i].plot(hist.history['val_' + metric], label='Val ' + metric)\n    ax[p_i].set_title(\"Loss Curve - {}\".format(metric))\n    ax[p_i].set_ylabel(metric.title())\n    ax[p_i].legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimize with Keras Tuner"},{"metadata":{},"cell_type":"markdown","source":"Here we use KerasTuner to make a hyperparameter search for our NN configuration. \n\nFor demo we use the following hyperparams: \n\n- `num_layers` (`Int`): The no. of layers in our NN (shallow or deep NN)\n\n- `units_i` (`Int`): The no. of dense neurons for each layer-i \n\n- `dp_i` (`Float`): The dropout rate for each layer-i \n\n- `final_dp` (`Float`): The dropout rate for at the last layer before output\n\n- `learning_rate` (`Float`): The learning rate for our optimizer"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import kerastuner as kt\n\ndef make_model(hp):\n    \n    num_inputs = [Input(shape=(1,), name=x) for x in num_cols]\n    encoded_nums = [encode_numerical_feature(var_input, var_name, train_ds)\n                   for var_input, var_name in zip(num_inputs, num_cols)]\n    all_feats = Concatenate()(encoded_nums)\n    x = all_feats\n    \n    num_layers = hp.Int('num_layers', min_value=2, max_value=5, step=1)\n    for i in range(num_layers):\n        units = hp.Int(f'units_{i}', min_value=128, max_value=512, step=64)\n        dp = hp.Float(f'dp_{i}', min_value=0., max_value=0.5)\n        x = Dropout(dp)(x)\n        x = Dense(units, activation='relu')(x)\n    \n    dp = hp.Float('final_dp', min_value=0.05, max_value=0.5)\n    x = Dropout(dp)(x)\n    outputs = Dense(1, activation='linear')(x)\n    model = tf.keras.Model(num_inputs, outputs)\n\n    lr = hp.Float('learning_rate', min_value=1e-4, max_value=5e-2)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr) # 1e-3\n    model.compile(loss='mse',\n                  optimizer=optimizer,\n                  metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n    #     model.summary()\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set KerasTurner\n\nMAX_TRIALS = 10  # 5  \n# Set to 5 for a quick run, but need 100+ for good results\n\n\ntuner = kt.tuners.BayesianOptimization(\n    make_model,\n    objective=kt.Objective('val_rmse', direction=\"min\"),  # 'val_loss',\n    max_trials=MAX_TRIALS,          \n    overwrite=True)\n\ntuner.search(train_ds, \n             validation_data=val_ds, \n             callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=5)], \n             epochs=60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reinstantiate the top N models and train them on the full dataset"},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def get_trained_model(hp):\n    model = make_model(hp)\n    # First, find the best number of epochs to train for\n    callbacks=[EarlyStopping(monitor='val_rmse', mode='min', patience=5, restore_best_weights=True)]\n    hist = model.fit(train_ds, validation_data=val_ds, epochs=50, callbacks=callbacks)\n    val_loss_per_epoch = hist.history['val_rmse']\n    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n    print('Best epoch: %d' % (best_epoch,))\n    # Increase epochs by 20% when training on the full dataset\n    model = make_model(hp)\n    model.fit(full_train_ds, epochs=int(best_epoch * 1.2), verbose=0)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# select top-N models\n\nn = 3     # e.g. n=10 for top ten models\nbest_hps = tuner.get_best_hyperparameters(n)\n\nall_preds = []\nfor hp in best_hps:\n    model = get_trained_model(hp)\n    preds = model.predict(test_ds)\n    all_preds.append(preds)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"for i in range(n):\n    sns.distplot(all_preds[i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble predictions from top-N models"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.zeros(shape=(len(test), 1))\nfor p in all_preds:\n    preds += p\npreds /= len(all_preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'] = preds\nsub.to_csv('nn_model.csv', index=False)\n\nprint('Submit!')\nsub.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'].plot(figsize=(16,4));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'].iloc[:1000].plot(figsize=(16,4));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sns.distplot(sub['target'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WiP - The notebook will be updated constantly with new tasks if there is interest. \n\n### Feel free to ask any questions you might have in the comments bellow \n\nNext steps: \n\n- Try different SoA optimizers\n\n- Try different LR schedulers \n\n- Tune different sets of hyperparams \n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}