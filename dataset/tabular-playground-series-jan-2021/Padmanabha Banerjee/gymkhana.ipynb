{"cells":[{"metadata":{},"cell_type":"markdown","source":"<body style=\"color:white\">\n    <h1 style='color:cyan' align='center'><font size=\"+3\">                   Welcome to the GYM:</font></h1>\n\n<h3 align='center'><font>Take a Monthly Membership <a href=https://www.kaggle.com/competitions\n                                                                              class=https://www.kaggle.com/competitions>HERE</a> and grind hard .</font></h3>\n   \n</body>\n"},{"metadata":{},"cell_type":"markdown","source":"<body style=\"color:white\">\n    <h1 style='color:red' align='center'><font size=\"+3\">AND THE RESULT IS BELOW !!!</font></h1>\n   \n</body>\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(filename=\"../input/images/kagglememe.png\", width=1200, height=300)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<body style=\"color:white\">\n    <h1 style='color:#F12626' align='center'><font size=\"+3\">Introduction</font></h1>\n\n<h3 align='center'><font>In this notebook, there are <font color=#000ECA>Two</font> parts : Understanding the data , Using Regressor models . \n    We have been provided with Tabular data , with <font color=#CA00BA>14</font> different independent dataset. In this notebook , 6 different regression techniques have been used , namely , <font color=#85C1E9>Linear Regression</font> , <font color=#27AE60>Decision Tree Regressor</font> ,<font color=#800080>eXtreme Gradient Boosting Regressor</font> , <font color=#EC7063>Light Gradient Boosting Regressor</font> , <font color=#2E86C1 >Category Boosting Regressor</font> and <font color=#FA520E>Regression Artificial Neural Network</font>. \n    I hope you will like this notebook and if you have , then please <font color=#2E86C1 >UPVOTE</font> and support the effort !!! I am a beginner , so any suggestion or constructive criticism is appreciated. Please do comment !!</font></h3>\n   \n</body>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import train_test_split\n        \ninput_path = Path('/kaggle/input/tabular-playground-series-jan-2021/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"test = pd.read_csv(input_path / 'test.csv', index_col='id')\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train.pop('target')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<body style=\"color:white\">\n    <h1 style='color:#F12626' align='center'><font size=\"+3\"><font color=#000ECA>Exploratory Data Analysis</font> : Understanding the Data</font></h1>\n\n<h3 align='center'><font>Here we will see the properties and variations in the data. We know that the data consists of 14 different features named as cont[N] where N->1,2,....13,14. The values of these range from : 0.0 - ~1.2. Where as the <font color=#000ECA>Target</font> has a range of approx 0->10.3. In the following cells we will se the Distribution of the values of the data , and also compare the values of the two provided Train and Test data and also compare the values with the Target.\n    Maybe because this is a <font color=#000ECA>Playground Competition</font> , the data provided is clean with no missing values.</font></h3>\n   \n</body>"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(14, 8))\ngs = fig.add_gridspec(1, 1)\nax = fig.add_subplot(gs[0, 0])\nsns.distplot(target ,ax=ax, color='#1BCBE5')\n\nbackground_color = \"#FFFFFF\"\n\nax.text(2.9, 0.74, 'Distribution of Target', fontsize=24, fontweight='bold', fontfamily='serif')\nax.get_yaxis().set_visible(False)\nax.set_facecolor(background_color)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We see here that the Target has two different peaks of distribution which are separated at the value 8."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, p = plt.subplots(7, 2, figsize=(20,22))\n#ax = fig.subplots()\n#ax.text(2.9, 0.74, 'Distribution of Target', fontsize=24, fontweight='bold', fontfamily='serif')\nn_bins = 50\n\nr=0\nc=0\ni=\"cont1\"\nfor i in train.columns:\n    \n    tr = train[i]\n    te = test[i]\n    \n    p[r, c].hist(tr, \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#ff0032\", \n                 label='train', \n                 linestyle='dashed',\n                 edgeColor = 'white')\n    \n    p[r, c].hist(te, \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#00a8f9\", \n                 label='test', \n                 alpha=0.6)\n    \n    p[r, c].legend(loc='upper right')\n    p[r, c].set_xlabel(i)\n    \n    if c == 0:\n        p[r, c].set_ylabel('frequency')\n    \n    if r == 0:\n        p[r, c].set_title('Histogram')\n    \n    if c < 1:\n        c+=1\n    else:\n        c=0\n        r+=1\n        \nplt.show()  \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The above plots shows that more or less the Test and Train data Cont Values are same with little difference and in ***cont5*** the values are **overlapping**. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, p = plt.subplots(7, 2, figsize=(20,22))\n#ax = fig.subplots()\n#ax.text(2.9, 0.74, 'Distribution of Target', fontsize=24, fontweight='bold', fontfamily='serif')\nn_bins = 50\n\nr=0\nc=0\ni=\"cont1\"\nfor i in train.columns:\n    \n    tr = train[i]\n    te = target\n    \n    p[r, c].hist((tr*10), \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#FF25C0\", \n                 label='train', \n                 linestyle='dashed',\n                 edgeColor = 'white')\n    \n    p[r, c].hist(te, \n                 n_bins, \n                 density=True, \n                 histtype='bar', \n                 color=\"#FFCC00\", \n                 label='target', \n                 alpha=0.6)\n    \n    p[r, c].legend(loc='upper right')\n    p[r, c].set_xlabel(i)\n    \n    if c == 0:\n        p[r, c].set_ylabel('frequency')\n    \n    if r == 0:\n        p[r, c].set_title('Histogram')\n    \n    if c < 1:\n        c+=1\n    else:\n        c=0\n        r+=1\n        \nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here in the above plot , we can see that there is no similarity between the distributions of Train and Target data. Here the Train data values have been multiplied by 10 , so that the values can be compared , because the ranges of the values of the two data are not the same. "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corr_mat = train.corr().stack().reset_index(name=\"correlation\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"g = sns.relplot(\n    data=corr_mat,\n    x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n    palette=\"Blues\", hue_norm=(-1, 1), edgecolor=\".7\",\n    height=10, sizes=(50, 250), size_norm=(-.2, .8),\n)\n\ng.set(xlabel=\"Train\", ylabel=\"Train\", aspect=\"equal\")\ng.despine(left=True, bottom=True)\ng.ax.margins(.2)\nfor label in g.ax.get_xticklabels():\n    label.set_rotation(90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In the above Correlation Heatmap , we can see that there is correlation between cont1, cont6, cont9, cont10, cont11, cont12 and cont13."},{"metadata":{},"cell_type":"markdown","source":"<body style=\"color:white\">\n    <h1 style='color:#F12626' align='center'><font size=\"+3\"><font color=#000ECA>Using Regressor Models</font></font></h1>\n\n<h3 align='center'><font>In the follwing cells we are using the regressor models which are as stated earlier , <font color=#85C1E9>Linear Regression</font> , <font color=#27AE60>Decision Tree Regressor</font>, <font color=#800080>eXtreme Gradient Boosting Regressor</font> , <font color=#EC7063>Light Gradient Boosting Regressor</font> , <font color=#2E86C1 >Category Boosting Regressor</font> and <font color=#FA520E>Regression Artificial Neural Network</font>. \nEach model works through 100 epochs , and the minimum loss function has been displayed for each regressor.To make my work easy , I have used the best parameters according to <a href=https://www.kaggle.com/dwin183287 class=Sharlto Cope><font color=#EC7063>Sharlto Cope's</font></a> notebook. It's a great Notebook , please check it out (<a href=https://www.kaggle.com/dwin183287/tps-jan-2021-eda-models class=Notebook >click here</a>) and support it by upvoting !!</font></h3>\n   \n</body>"},{"metadata":{},"cell_type":"markdown","source":"# Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_d=X_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear = LinearRegression()\nscores = cross_val_score(linear, X_train, y_train, scoring='neg_root_mean_squared_error', cv=100)\nlin_rmse_scrs = -scores\n#print('Linear Regression performance:', lin_rmse_scrs)\nprint(min(lin_rmse_scrs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree "},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeRegressor(max_depth= 10, max_features=9)\nscores = cross_val_score(tree, X_train, y_train, scoring='neg_root_mean_squared_error', cv=100)\ntree_rmse_scrs = -scores\n#print('Decision Tree Regressor performance:', tree_rmse_scrs)\nprint(min(tree_rmse_scrs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Light Gradient Boosting Machine "},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMRegressor(max_depth=11,learning_rate=0.3071070857621833)\nscores = cross_val_score(lgbm, X_train, y_train, scoring='neg_root_mean_squared_error', cv=100)\nlgbm_rmse_scrs = -scores\n#print('LGBM performance:', lgbm_rmse_scrs)\nprint(min(lgbm_rmse_scrs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"catb = CatBoostRegressor(verbose=False)\nscores = cross_val_score(catb, X_train, y_train, scoring='neg_root_mean_squared_error', cv=10)\ncatb_rmse_scrs = -scores\n#print('CatBoost performance:', catb_rmse_scrs)\nprint(min(catb_rmse_scrs))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### eXtreme Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_reg = XGBRegressor(random_state=42)\nscores = cross_val_score(xgb_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\nxgb_rmse_scores = np.sqrt(-scores)\n#print('XGBoost performance:', xgb_rmse_scores)\nprint(min(xgb_rmse_scores))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Artificial Neural Network"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom sklearn.metrics import classification_report\nfrom keras.callbacks import History \nhistory = History()\n\n\nnn  = Sequential()\nnn.add(Dense(units= 28, activation = 'linear', input_dim=14, kernel_initializer=\"uniform\"))\nnn.add(Dense(units= 28, activation = 'linear',kernel_initializer=\"uniform\"))\nnn.add(Dense(units= 1, activation = 'linear',kernel_initializer=\"uniform\"))\nnn.compile(optimizer='adam',\n              loss='mean_squared_error')\nresult=nn.fit(X_train, y_train, validation_split=0.2,\n                       verbose=0, epochs=100,callbacks=[history])\n\n#print(result.history['val_loss'])\nmin(result.history['val_loss'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Making predictions using the Neural Network\n\n* Because the Neural Network generates lesser loss , we are using it to make predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"target_ans=nn.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import DataFrame\nans = DataFrame(target_ans,columns=['target'])\nans.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Converting the list to a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"test1=pd.read_csv(input_path / 'test.csv', index_col='id')\ntest1.reset_index(drop=False, inplace=True)\n\ntest1=test1['id']\ntest1.to_frame()\nsub=pd.concat([test1, ans], axis=1, join='inner')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Generating the dataframe with only two columns - \"id\" and \"target\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submit.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Submitting the csv file created.\n"},{"metadata":{},"cell_type":"markdown","source":"<h1><center> <font size=\"+3\"><font color=#440BE3>⚜⚜⚜⚜</font><font color=#0B1AE3>⚜⚜⚜⚜</font><font color=#0B6CE3>⚜⚜⚜⚜</font><font color=#0BE317>⚜⚜⚜⚜</font><font color=#F7DF21>⚜⚜⚜⚜</font><font color=#F73821>⚜⚜⚜⚜</font><font color=#DA0A0A>⚜⚜⚜⚜</font><font color=#29EAEE>⚜⚜⚜⚜</font></font></center></h1>"},{"metadata":{},"cell_type":"markdown","source":" <h1 style='color:#FA0048' align='center'><font size=\"+3\">                  The END</font></h1>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}