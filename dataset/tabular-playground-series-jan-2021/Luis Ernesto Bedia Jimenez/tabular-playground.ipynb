{"cells":[{"metadata":{},"cell_type":"markdown","source":"# PRACTICA DE ML CON REGRESION#\n\nPara esta practica de aprendizaje automatico con regresión, vamos a usar el aumento de gradiente extremo, o XGB  y a compararlo con la maquina de aumento de gradiente ligero, o LGBM, por sus siglas en ingles\n\nLos pasos a seguir son:\n1. Cargar las librer'ias y los datos con pandas\n2. Hacer una exploración de los datos \n3. Separar los datos de entrenamiento en entrenamiento y validación\n4. Entrenar los modelos con los datos de entrenamiento y usar los datos de validación para evaluar nuestro entrenamiento\n5. Seleccionar el mejor modelo\n6. Subir los resultados a la competición"},{"metadata":{},"cell_type":"markdown","source":"# 1. Cargamos las librerías y los datos# \n\nEl primer paso es cargar las librerias necesarias para este trabajo"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cargamos los datos de entrenamiento y mostramos las cinco primeras filas"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mostramos los datos de test y mostramos las cinco primeras filas"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Hacemos una exploración de los datos\n\nPrimero comprobamos que no haya datos faltantes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.isnull().sum())\nprint(test_df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No faltan datos en las columnas"},{"metadata":{},"cell_type":"markdown","source":"Vemos como están correlacionados los datos entre sí"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(train_df.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Las columnas que mas condicionan el valor final son: cont2, cont3, cont7 y cont11"},{"metadata":{},"cell_type":"markdown","source":"Ponemos en una gráfica como se distribuye la variable 'target'"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (12, 6))\nsns.distplot(train_df['target'])\nax.xaxis.grid(True)\nax.set(ylabel = \"Valores\")\nax.set(xlabel = \"Target\")\nax.set(title = \"Distribuicion de target\")\nsns.despine(trim = True, left = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vemos que la distribuición de datos de la variable se mueve entre 6 y 10"},{"metadata":{},"cell_type":"markdown","source":"Con el siguiente codigo vamos a var como se ditribuyen los valores de train.\n\nprimero tomamos las columans de train_df, nuestro conjunto de entrenamiento pero sin la columna objetivo target ni la de identificacion id, despues, creamos una hoja que pueda ser usada para dibujar las 14 columnas de los dos conjuntos. Tras esto, dibujamos las graficas y las mostramos por pantalla"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [col for col in train_df.columns if col not in ['id','target']]\n\nfig, ax = plt.subplots(nrows = 7, ncols = 2, figsize = (15,15))\nfig.tight_layout()\n\ni = j = 0\nfor col in cols:\n    sns.kdeplot(data = train_df[col],shade=True,ax=ax[i][j],legend=False)\n    ax[i][j].set_title(col)\n    j += 1\n    if(j%2 == 0):\n        i += 1\n        j = 0\n        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Separamos los datos de entrenamiento y validación\n\nEsto lo hacemos eliminado la variable 'target' del conjunto de entrenamiento y dándosela a una variable y, el resto lo metemos en una variable X, también eliminamos la columna 'id'. Creamos una variable X_test eliminando la columna 'id' del conjunto test. Tras esto, mostramos el tamaño de los conjuntos train y val"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['target']\nX = train_df.drop(['id', 'target'], axis = 1)\nX_test = test_df.drop(['id'], axis = 1)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 597)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Entrenamos los modelos con los datos de entrenamiento y usamos los datos de validación para evaluar nuestro entrenamiento"},{"metadata":{},"cell_type":"markdown","source":"El primer sistema que vamos a usar es el XGB, para ello, primero definimos los parametros que vamos a usar"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 5,\n    \"eta\": 0.05,\n    \"random_state\": 751\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d_train = xgb.DMatrix(X_train, label = y_train)\nd_val = xgb.DMatrix(X_val, label = y_val)\nd_test = xgb.DMatrix(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1 = xgb.train(params = params,\n                    dtrain = d_train,\n                    num_boost_round = 10000,\n                    early_stopping_rounds = 20,\n                    verbose_eval = 10,\n                    evals = [(d_train, \"train\"), (d_val, \"val\")])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_1 = model_1.predict(d_val, ntree_limit = model_1.best_ntree_limit)\nrmse_model_1 = np.sqrt(mean_squared_error(y_val, predict_1))\nrmse_model_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_1 = model_1.predict(d_test, ntree_limit = model_1.best_ntree_limit)\npredict_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_2 = lgb.LGBMRegressor(random_state = 100, \n                        n_estimators = 500, \n                        min_data_per_group = 5, \n                        boosting_type = 'gbdt',\n                        num_leaves = 128, \n                        learning_rate = 0.005, \n                        subsample_for_bin = 200000, \n                        importance_type ='split', \n                        metric ='rmse', \n                        min_data_in_leaf = 50)\n\nmodel_2.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_2 = model_2.predict(X_val)\npredict_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_model_2 = np.sqrt(mean_squared_error(y_val, predict_2))\nrmse_model_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Seleccionar el mejor modelo\n\nComo podemos ver por el valor que nos devuelve el RMSE (Mean squared error regression loss), el mejopr modelo es el de lgb, con un 70% e acierto frente al 69% que nos da el xgb"},{"metadata":{},"cell_type":"markdown","source":"# 6. Subir los resultados a la competición"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predict = model_2.predict(X_test)\ntest_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({\"id\":X_test.index,'target':test_predict})\noutput","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.to_csv('./Submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}