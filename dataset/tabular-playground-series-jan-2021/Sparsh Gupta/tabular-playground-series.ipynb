{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tabular Playground Challenge"},{"metadata":{},"cell_type":"markdown","source":"# Step 1: Reading and Understanding the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport random\nseed = 12\nnp.random.seed(seed)\n\nfrom datetime import date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# important funtions\ndef datasetShape(df):\n    rows, cols = df.shape\n    print(\"The dataframe has\",rows,\"rows and\",cols,\"columns.\")\n    \n# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base = '/kaggle/input/tabular-playground-series-jan-2021/'\ndata_file = base + \"train.csv\"\ndf = pd.read_csv(data_file)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_file = base + \"test.csv\"\ndf_test = pd.read_csv(data_file)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# check dataset shape\ndatasetShape(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('id', inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for duplicates\nprint(df.shape)\ndf.drop_duplicates(inplace=True)\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 2: EDA"},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# boxplots of numerical features for outlier detection\n\nfig = plt.figure(figsize=(16,20))\nfor i in range(len(df.columns)):\n    fig.add_subplot(3, 5, i+1)\n    sns.boxplot(y=df.iloc[:,i])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for missing values\ndf.isna().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.gridspec as gridspec\nfig = plt.figure(constrained_layout=True, figsize=(16,6))\ngrid = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\nax1 = fig.add_subplot(grid[0, :2])\nax1.set_title('Histogram')\nsns.distplot(df.loc[:,'target'], norm_hist=True, ax = ax1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"sns.pairplot(df)\nplt.show()\n\n# correlation heatmap for all features\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No Data Cleaning required. Lets check for data skewness further."},{"metadata":{},"cell_type":"markdown","source":"# Step 3: Data Preparation\n\n### Outlier Treatment\n\nWe will take log of the feature values using np.log1p()"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot sample skewed feature\nplt.figure(figsize=(10,4))\nsns.distplot(df['cont1'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skewed_features = df.apply(lambda x: x.skew()).sort_values(ascending=False)\nskewed_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # transform skewed features\n# for feat in skewed_features.index:\n#     if abs(skewed_features.loc[feat]) > 0.0005:\n#         df[feat] = np.log1p(df[feat])\n#         if 'Close' not in feat:\n#             df_test[feat] = np.log1p(df_test[feat])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not applying skewness."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot sample treated feature\nplt.figure(figsize=(10,4))\nsns.distplot(df['cont1'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Step 4: Data Modelling\n\n### Split Train-Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# shuffle samples\ndf_shuffle = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\ndf_y = df_shuffle.pop('target')\ndf_X = df_shuffle\n\n# split into train dev and test\nX_train, X_test, y_train, y_test = skms.train_test_split(df_X, df_y, train_size=0.9, random_state=seed)\nprint(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]/len(df_shuffle)*100)}%\")\nprint(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]/len(df_shuffle)*100)}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.linear_model as sklm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaler = skp.RobustScaler()\nscaler = skp.MinMaxScaler()\n# scaler = skp.StandardScaler()\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n\n# scale test data with transform()\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)\n\n# view sample data\nX_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"def expm1(x):\n    return np.expm1(x)\ndef getRmse(y_train, y_train_pred):\n#     print(skm.mean_squared_error(expm1(y_train), expm1(y_train_pred)))\n    print(skm.mean_squared_error(y_train, y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge"},{"metadata":{"trusted":true},"cell_type":"code","source":"lmr = sklm.Ridge(alpha=0.001)\nlmr.fit(X_train, y_train)\n\n# predict\ny_train_pred = lmr.predict(X_train)\ny_test_pred = lmr.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.005, 0.01, 0.03, 0.05, 0.1, 0.5, 1.0, 5.0, 10]}\nridge = sklm.Ridge()\n\n# cross validation\nmodel_cv_ridge = skms.GridSearchCV(estimator = ridge, n_jobs=-1, param_grid = params, \n                             scoring= 'neg_mean_squared_error', cv = 5, \n                             return_train_score=True, verbose = 3)            \nmodel_cv_ridge.fit(X_train, y_train)\nprint(model_cv_ridge.best_estimator_)\ny_train_pred = model_cv_ridge.predict(X_train)\ny_test_pred = model_cv_ridge.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cb\n\ncbr = cb.CatBoostRegressor(loss_function='RMSE', verbose=0)\ncbr.fit(X_train, y_train, eval_set=(X_test, y_test))\nprint(cbr.best_score_)\n\ny_train_pred = cbr.predict(X_train)\ny_test_pred = cbr.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.ensemble as ske\n\nxgb = ske.GradientBoostingRegressor(criterion='mse', random_state=1)\nxgb.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb.predict(X_train)\ny_test_pred = xgb.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Trees"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = ske.ExtraTreesRegressor(criterion='mse', random_state=1)\nxgb.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb.predict(X_train)\ny_test_pred = xgb.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = ske.RandomForestRegressor(criterion='mse', random_state=1)\nxgb.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb.predict(X_train)\ny_test_pred = xgb.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xg\nxgb = xg.XGBRegressor(objective ='reg:squarederror', random_state=1)\nxgb.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb.predict(X_train)\ny_test_pred = xgb.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deep Learning Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(\"TF version:-\", tf.__version__)\nimport keras as k\ntf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0\nbestModelPath = './best_model.hdf5'\n\nclass myCallback(k.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('mse') < THRESHOLD):\n            print(\"\\n\\nStopping training as we have reached our goal.\")   \n            self.model.stop_training = True\n\nmycb = myCallback()\ncheckpoint = k.callbacks.ModelCheckpoint(filepath=bestModelPath, monitor='val_loss', verbose=1, save_best_only=True)\n\ncallbacks_list = [mycb,\n                  checkpoint\n                 ]\n            \ndef plotHistory(history):\n    print(\"Min. Validation MSE\",min(history.history[\"val_mse\"]))\n    pd.DataFrame(history.history).plot(figsize=(12,6))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"epochs = 40\n\nmodel_1 = k.models.Sequential([\n    k.layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n    k.layers.Dropout(0.2),\n    \n#     k.layers.Dense(4096, activation='relu'),\n#     k.layers.Dropout(0.2),\n\n    k.layers.Dense(256, activation='relu'),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(1, activation='linear'),\n])\nprint(model_1.summary())\n\nmodel_1.compile(optimizer='adam',\n              loss='mse',\n              metrics='mse'\n)\nhistory = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs,\n                 callbacks=[callbacks_list])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotHistory(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test Evaluation & Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getTestResults(m=None):\n    df_final = df.sample(frac=1, random_state=1).reset_index(drop=True)\n    test_cols = [x for x in df.columns if 'target' not in x]\n    df_final_test = df_test[test_cols]\n    df_y = df_final.pop('target')\n    df_X = df_final\n\n#     scaler = skp.RobustScaler()\n    scaler = skp.MinMaxScaler()\n#     scaler = skp.StandardScaler()\n\n    df_X = pd.DataFrame(scaler.fit_transform(df_X), columns=df_X.columns)\n\n    X_test = pd.DataFrame(scaler.transform(df_final_test), columns=df_X.columns)\n    \n    if m is None:\n\n#         lmr = sklm.Ridge(alpha=0.0001)\n#         lmr.fit(df_X, df_y)\n\n        lmr = cb.CatBoostRegressor(loss_function='RMSE', verbose=0)\n        lmr.fit(df_X, df_y)\n\n#         lmr = ske.ExtraTreesRegressor(criterion='mse', random_state=1)\n#         lmr.fit(df_X, df_y)\n\n#         lmr = ske.RandomForestRegressor(criterion='mse', random_state=1)\n#         lmr.fit(df_X, df_y)\n\n#         lmr = xg.XGBRegressor(objective ='reg:squarederror', random_state=1)\n#         lmr.fit(df_X, df_y)\n\n    else:\n        lmr = m\n\n    # predict\n    y_train_pred = lmr.predict(df_X)\n    y_test_pred = lmr.predict(X_test)\n    if m is not None:\n        y_test_pred = [y[0] for y in y_test_pred]\n    getRmse(df_y, y_train_pred)\n    return y_test_pred\n\n# ML models\nresults = getTestResults()\n\n# Neural Network model\n# results = getTestResults(k.models.load_model(bestModelPath))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n    'id': df_test['id'],\n    'target': results,\n})\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('./submission_Catboost.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Metrics - RMSE\n\n1 - NN - .71704\n\n2 - Ridge(0.0001) - .72782\n\n3 - CatBoost - .70001\n\n4 - ExtraTrees - .70887\n\n5 - RF - .70985\n\n6 - XGB - .70463\n\n#### CatBoost performed best."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}