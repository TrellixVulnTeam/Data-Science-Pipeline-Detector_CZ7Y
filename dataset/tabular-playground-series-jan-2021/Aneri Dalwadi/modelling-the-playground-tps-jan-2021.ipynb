{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook contains the application of the following regression models:\n1. Linear Regression \n2. Ridge Regression \n3. Lasso Regression\n4. Poisson Regression \n5. K-Neighbor Regressor\n6. LGBM\n7. XGB\n8. Random Forest","metadata":{}},{"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nsns.set_theme(color_codes=True)\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer, accuracy_score\n\nfrom sklearn.linear_model import LinearRegression\n\n# from cuml.ensemble import RandomForestRegressor as cuRFC\n# import cudf\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.ensemble import AdaBoostRegressor\n\nfrom sklearn.linear_model import PoissonRegressor\n\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import Lasso\n\nimport lightgbm as lgbm\nfrom lightgbm import LGBMRegressor\n\nimport xgboost\nfrom xgboost import XGBRegressor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-06T07:30:32.647941Z","iopub.execute_input":"2022-01-06T07:30:32.649039Z","iopub.status.idle":"2022-01-06T07:30:32.66069Z","shell.execute_reply.started":"2022-01-06T07:30:32.648989Z","shell.execute_reply":"2022-01-06T07:30:32.659539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = cudf.read_csv(\"../input/tabular-playground-series-jan-2021/train.csv\", index_col = \"id\")\n# tdf = cudf.read_csv(\"../input/tabular-playground-series-jan-2021/test.csv\")\n\ndf = pd.read_csv(\n    \"../input/tabular-playground-series-jan-2021/train.csv\", index_col=\"id\"\n    )\ntdf = pd.read_csv(\"../input/tabular-playground-series-jan-2021/test.csv\")\n\n# sns.regplot(x= df.drop(columns = [\"target\"]), y = df[\"target\"], data = df)\n# sns.lmplot(x= df.drop(columns = [\"target\"]), y = df[\"target\"], data = df)","metadata":{"execution":{"iopub.status.busy":"2022-01-06T07:30:28.286904Z","iopub.execute_input":"2022-01-06T07:30:28.287597Z","iopub.status.idle":"2022-01-06T07:30:31.712777Z","shell.execute_reply.started":"2022-01-06T07:30:28.287549Z","shell.execute_reply":"2022-01-06T07:30:31.711328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.astype(\"float32\")\ntdf = tdf.astype(\"float32\")","metadata":{"execution":{"iopub.status.busy":"2022-01-06T07:30:36.938307Z","iopub.execute_input":"2022-01-06T07:30:36.938576Z","iopub.status.idle":"2022-01-06T07:30:36.956857Z","shell.execute_reply.started":"2022-01-06T07:30:36.938549Z","shell.execute_reply":"2022-01-06T07:30:36.955909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(columns=\"target\")\ny = df[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2022-01-06T07:30:38.906045Z","iopub.execute_input":"2022-01-06T07:30:38.906325Z","iopub.status.idle":"2022-01-06T07:30:38.921551Z","shell.execute_reply.started":"2022-01-06T07:30:38.906295Z","shell.execute_reply":"2022-01-06T07:30:38.920261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* To understand the statistical meaning and data distribution of our data, pandas gives a feature: describe()\n* Following are the attributes provided:\n1. count (total number of values)\n2. mean (mean of the data)\n3. std (standard deviation)\n4. min, max (minimum and maximum value in the data)\n5. 25%, 50%, 75% (Respective quartile values)\n","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-01-06T07:31:42.017177Z","iopub.execute_input":"2022-01-06T07:31:42.017503Z","iopub.status.idle":"2022-01-06T07:31:42.246458Z","shell.execute_reply.started":"2022-01-06T07:31:42.017471Z","shell.execute_reply":"2022-01-06T07:31:42.245506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"dark\")\nsns.set_color_codes(palette=\"deep\")\nf, ax = plt.subplots(figsize=(9, 8))\n\nsns.distplot(df[\"target\"], color=\"c\")\n\nax.xaxis.grid(False)\nax.set(ylabel=\"values\")\nax.set(xlabel=\"target\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T16:59:57.256207Z","iopub.execute_input":"2022-01-04T16:59:57.256503Z","iopub.status.idle":"2022-01-04T16:59:58.937819Z","shell.execute_reply.started":"2022-01-04T16:59:57.256455Z","shell.execute_reply":"2022-01-04T16:59:58.937112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observing the corelation of the predictors","metadata":{}},{"cell_type":"code","source":"df.corr().style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:00:10.168679Z","iopub.execute_input":"2022-01-04T17:00:10.169385Z","iopub.status.idle":"2022-01-04T17:00:10.509607Z","shell.execute_reply.started":"2022-01-04T17:00:10.169339Z","shell.execute_reply":"2022-01-04T17:00:10.508796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Corelation = sns.heatmap(df.corr(), cmap=\"YlGnBu\")","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:00:13.767835Z","iopub.execute_input":"2022-01-04T17:00:13.76809Z","iopub.status.idle":"2022-01-04T17:00:14.482289Z","shell.execute_reply.started":"2022-01-04T17:00:13.768062Z","shell.execute_reply":"2022-01-04T17:00:14.481343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can observe that cont variables 1,6,7,8,9,10,11,12,13 are the most inter-correlated\n* Note: the corelation between the parameters and the target variable take maximum absolute value of 0.067.\n* Since this value is close to 0, the linear regression model will not be a good fit.","metadata":{}},{"cell_type":"markdown","source":"\n# Regression\n* The following are the mathematical models that will help perdict a continuous outcome (result) based on one or more input(s) (predictor variables).\n## Linear Regression\n* Simple approach for [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)\n* We assume that the _true_ relationship between  X nd Y takes form Y = f(x) + ϵ (f is an unknown function, ϵ is a mean-zero random error term)\n* Y = β0 + β1X + ϵ\n    * β0 - intercept term (, the expected value of Y when X = 0)\n    * β1 - slope (the average increase in Y associated with a one-unit increase in X)\n    * ϵ -catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in Y , and there may be measurement error\n* Analysing each individual variable\n* For estimating coefficients\n    * We choose _least squares method_ to choose the coefficients such that we minimise RSS (Residual Sum of Squares)\n* To predict the confidence interval: RSE (Residual Standard Error)\n\n","metadata":{}},{"cell_type":"markdown","source":"### Using [Cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) for model selection.","metadata":{}},{"cell_type":"code","source":"%%time\n\ncv = cross_validate(\n        estimator = LinearRegression(n_jobs = -1),\n        X = df.drop(columns = [\"target\"]),\n        y = df[\"target\"],\n        cv = 5,\n        scoring = [\"r2\",\"neg_mean_squared_error\"],\n        verbose = True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:02:37.343704Z","iopub.execute_input":"2022-01-04T17:02:37.343992Z","iopub.status.idle":"2022-01-04T17:02:37.874345Z","shell.execute_reply.started":"2022-01-04T17:02:37.343962Z","shell.execute_reply":"2022-01-04T17:02:37.873528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv[\"test_neg_mean_squared_error\"].mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:03:19.076952Z","iopub.execute_input":"2022-01-04T17:03:19.077699Z","iopub.status.idle":"2022-01-04T17:03:19.083704Z","shell.execute_reply.started":"2022-01-04T17:03:19.077656Z","shell.execute_reply":"2022-01-04T17:03:19.082741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sinc the R2 values are very less, it is pretty evident that the Linear Regression is not a suitable model to explain the variance of our data. \n* Underfitting","metadata":{}},{"cell_type":"code","source":"%%time\n# After crossvalidation, we will try to fit our model\ntdf\nmodel = LinearRegression(n_jobs=-1)\n# when using GPU\n# model = RFC(verbose=True)\nmodel.fit(df.drop(columns=[\"target\"]), df[\"target\"])\n# predicting the model\npred = model.predict(tdf.drop(columns=[\"id\"]))","metadata":{"execution":{"iopub.status.busy":"2021-12-30T12:08:41.927005Z","iopub.execute_input":"2021-12-30T12:08:41.927823Z","iopub.status.idle":"2021-12-30T12:08:42.034286Z","shell.execute_reply.started":"2021-12-30T12:08:41.927776Z","shell.execute_reply":"2021-12-30T12:08:42.03156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans = pd.DataFrame({\"id\": tdf[\"id\"], \"target\": pred})\nans[\"id\"] = ans[\"id\"].astype(int)\n# converting to submission file. Since we have set the id col, setting index = False\nans.to_csv(\"submission_LinearRegression.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T12:08:53.139225Z","iopub.execute_input":"2021-12-30T12:08:53.139563Z","iopub.status.idle":"2021-12-30T12:08:53.506469Z","shell.execute_reply.started":"2021-12-30T12:08:53.139529Z","shell.execute_reply":"2021-12-30T12:08:53.50566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_r2 is default variable of cv: getting mean of it\ncv[\"test_r2\"].mean()","metadata":{"execution":{"iopub.status.busy":"2021-12-30T12:09:11.102898Z","iopub.execute_input":"2021-12-30T12:09:11.103177Z","iopub.status.idle":"2021-12-30T12:09:11.109718Z","shell.execute_reply.started":"2021-12-30T12:09:11.103144Z","shell.execute_reply":"2021-12-30T12:09:11.108929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following are some ways in which the simple linear model can be improved,\nby replacing plain least squares fitting with some alternative fitting procedures.\n# Ridge\n* We perform regularisation on our linear regression model. \n* Regularisation will help reducing the coefficients. The parameters that have more role in determining the target/ result value will have less shinking coefficients as the value of alpha increases.\n* It shrinks the parameters, therefore it is mostly used to prevent multicollinearity.\n* Uses L2 regularization technique.","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns=[\"target\"])\ny = df['target'] \nkf = KFold(n_splits=5)\nkf.get_n_splits(X)\nprint(kf)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:01:37.814704Z","iopub.execute_input":"2022-01-04T17:01:37.815403Z","iopub.status.idle":"2022-01-04T17:01:37.828242Z","shell.execute_reply.started":"2022-01-04T17:01:37.815356Z","shell.execute_reply":"2022-01-04T17:01:37.827392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Using K-fold approach to implement RidgeCV model\n* RidgeCV will internally apply Cross-validation to choose the optimal value of tuning variable alpha","metadata":{}},{"cell_type":"code","source":"score = 0\nfor train_index, test_index in kf.split(X, df[\"target\"]):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    # train_index, test_index are integer indices based on the number of rows\n    # Thus we need iloc to access data\n    # iloc: Axes left out of the specification are assumed to be :,\n    # e.g. p.iloc['test_index'] is equivalent to p.iloc['test_index', :].\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    model = RidgeCV().fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    score += mean_squared_error(y_test, y_pred)\n# mean of MSE =  0.5275229\nprint((score / kf.get_n_splits(X)))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:01:41.060194Z","iopub.execute_input":"2022-01-04T17:01:41.060674Z","iopub.status.idle":"2022-01-04T17:01:42.443563Z","shell.execute_reply.started":"2022-01-04T17:01:41.060618Z","shell.execute_reply":"2022-01-04T17:01:42.442669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RidgeCV()\nmodel.fit(df.drop(columns=\"target\"), df[\"target\"])\n# Fit the data and get the optimal value of alpha chosen\n# Here: 10\nmodel.alpha_\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:06:06.154147Z","iopub.execute_input":"2022-01-04T17:06:06.154822Z","iopub.status.idle":"2022-01-04T17:06:06.435079Z","shell.execute_reply.started":"2022-01-04T17:06:06.154778Z","shell.execute_reply":"2022-01-04T17:06:06.434268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alphas = np.linspace(1, 100000, 100)\nridge = Ridge(max_iter=10000)\ncoefs = []\n\nfor a in alphas:\n    ridge.set_params(alpha=a)\n    ridge.fit(df.drop(columns=[\"target\"]), df[\"target\"])\n    coefs.append(ridge.coef_)\n\nax = plt.gca()\n\nax.plot(alphas, coefs)\nax.set_xscale(\"log\")\nplt.axis(\"tight\")\nplt.xlabel(\"alpha\")\nplt.legend(X.columns,bbox_to_anchor=(0.85, -0.25), fancybox=True, shadow=True, ncol=3)\nplt.ylabel(\"Standardized Coefficients\")\nplt.title(\"Ridge coefficients as a function of alpha\")","metadata":{"execution":{"iopub.status.busy":"2022-01-06T07:40:02.13481Z","iopub.execute_input":"2022-01-06T07:40:02.135542Z","iopub.status.idle":"2022-01-06T07:40:09.212546Z","shell.execute_reply.started":"2022-01-06T07:40:02.1355Z","shell.execute_reply":"2022-01-06T07:40:09.209914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can observe that, as we increase the value of alpha, the magnitude of the coefficients decreases, where the values reaches to zero but not absolute zero","metadata":{}},{"cell_type":"markdown","source":"# Lasso\n* Lasso is similar to ridge regression, however here the coefficients can actually take value = 0\n* Uses l1 regularisation technique\n* Used for feature selection","metadata":{}},{"cell_type":"code","source":"# Note: we use LassoCV which internally performs cross-validation to choose optimal value of tuning variable- alpha\ncv = cross_validate(\n    estimator=LassoCV(n_jobs=-1),\n    X=df.drop(columns=[\"target\"]),\n    y=df[\"target\"],\n    verbose=1,\n    return_train_score=True,\n    scoring=[\"r2\", \"neg_mean_squared_error\"],\n    cv=5,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:04:02.478982Z","iopub.execute_input":"2022-01-04T17:04:02.479254Z","iopub.status.idle":"2022-01-04T17:04:09.575789Z","shell.execute_reply.started":"2022-01-04T17:04:02.479225Z","shell.execute_reply":"2022-01-04T17:04:09.574798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv[\"test_neg_mean_squared_error\"].mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:05:34.698067Z","iopub.execute_input":"2022-01-04T17:05:34.698391Z","iopub.status.idle":"2022-01-04T17:05:34.704475Z","shell.execute_reply.started":"2022-01-04T17:05:34.698355Z","shell.execute_reply":"2022-01-04T17:05:34.703606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* To better understand the variation of the coefficients wth change in the tuning variable, we will plot the change in coefficients with respect to change in alpha.","metadata":{}},{"cell_type":"code","source":"model = LassoCV(n_jobs=-1)\nmodel.fit(df.drop(columns=\"target\"), df[\"target\"])\n# Fit the data and get the optimal value of alpha chosen\nmodel.alpha_\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:06:18.627798Z","iopub.execute_input":"2022-01-04T17:06:18.628056Z","iopub.status.idle":"2022-01-04T17:06:19.87805Z","shell.execute_reply.started":"2022-01-04T17:06:18.628029Z","shell.execute_reply":"2022-01-04T17:06:19.877134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alphas = np.linspace(2.7800706909230952e-05, 0.01, 100)\nlasso = Lasso(max_iter=10000)\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(df.drop(columns=[\"target\"]), df[\"target\"])\n    coefs.append(lasso.coef_)\n\nax = plt.gca()\n\nax.plot(alphas, coefs)\nax.set_xscale(\"log\")\nplt.axis(\"tight\")\nplt.xlabel(\"alpha\")\nplt.legend(X.columns,bbox_to_anchor=(0.85, -0.25), fancybox=True, shadow=True, ncol=3)\nplt.ylabel(\"Standardized Coefficients\")\nplt.title(\"Lasso coefficients as a function of alpha\")","metadata":{"execution":{"iopub.status.busy":"2022-01-06T07:40:45.274266Z","iopub.execute_input":"2022-01-06T07:40:45.275077Z","iopub.status.idle":"2022-01-06T07:40:59.682242Z","shell.execute_reply.started":"2022-01-06T07:40:45.275024Z","shell.execute_reply":"2022-01-06T07:40:59.681206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Poisson Regression\n* Poisson regression assumes the response variable Y has a Poisson distribution\n* It assumes the logarithm of its expected value can be modeled by a linear combination of unknown parameters. ","metadata":{}},{"cell_type":"code","source":"cv = cross_validate(\n    estimator=PoissonRegressor(),\n    X=df.drop(columns=[\"target\"]).astype(\"float32\"),\n    y=df[\"target\"].astype(\"float32\"),\n    verbose=1,\n    return_train_score=True,\n    scoring=[\"r2\", \"neg_mean_squared_error\", \"neg_mean_poisson_deviance\"],\n    cv=3,\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:06:32.485043Z","iopub.execute_input":"2022-01-04T17:06:32.485259Z","iopub.status.idle":"2022-01-04T17:06:34.215316Z","shell.execute_reply.started":"2022-01-04T17:06:32.485232Z","shell.execute_reply":"2022-01-04T17:06:34.214401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv[\"test_neg_mean_squared_error\"].mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:07:56.447352Z","iopub.execute_input":"2022-01-04T17:07:56.447948Z","iopub.status.idle":"2022-01-04T17:07:56.452911Z","shell.execute_reply.started":"2022-01-04T17:07:56.447906Z","shell.execute_reply":"2022-01-04T17:07:56.45248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# K-Neighbors Regressor","metadata":{}},{"cell_type":"code","source":"cv = cross_validate(\n    estimator=KNeighborsRegressor(n_neighbors=3, n_jobs=-1),\n    X=df.drop(columns=\"target\"),\n    y=df[\"target\"],\n    verbose=True,\n    cv=5,\n    scoring=[\"r2\", \"neg_mean_squared_error\"],\n    n_jobs=-1,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:21:49.935558Z","iopub.execute_input":"2022-01-04T17:21:49.935836Z","iopub.status.idle":"2022-01-04T17:25:22.874417Z","shell.execute_reply.started":"2022-01-04T17:21:49.935808Z","shell.execute_reply":"2022-01-04T17:25:22.873294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv[\"test_neg_mean_squared_error\"].mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:29:56.619941Z","iopub.execute_input":"2022-01-04T17:29:56.620281Z","iopub.status.idle":"2022-01-04T17:29:56.627099Z","shell.execute_reply.started":"2022-01-04T17:29:56.620242Z","shell.execute_reply":"2022-01-04T17:29:56.626531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Boosting\n* Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model.\n* When an output is mispredicted by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. By combining the whole set at the end converts weak learners into better performing model.\n* The final model is the weighted mean of all the models (weak learners).\n","metadata":{}},{"cell_type":"markdown","source":"# Light Gradient Boosting Model","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    df.drop(columns=\"target\"), df[\"target\"], test_size=0.15\n)\n\nparam = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"RMSE\",\n    \"learning_rate\": 0.0045,\n}\n\n\nmodel = LGBMRegressor(**param)\nmodel.fit(X_train, y_train)\n\n\nypred2 = model.predict(X_test)\n\n# rmse always takes in validation sets, eg. y test, x test predicted.\nprint(mean_squared_error(y_test, ypred2))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:16:37.933855Z","iopub.execute_input":"2022-01-04T17:16:37.93415Z","iopub.status.idle":"2022-01-04T17:16:39.779727Z","shell.execute_reply.started":"2022-01-04T17:16:37.934116Z","shell.execute_reply":"2022-01-04T17:16:39.778912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Light Gradient Boosting Model With k-fold","metadata":{}},{"cell_type":"code","source":"# Now add this to train and test And you will get the score\nX = df.drop(columns=[\"target\"])\nkf = KFold(n_splits=5)\n\nfor train_index, test_index in kf.split(X, df[\"target\"]):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    param = {\n        \"boosting_type\": \"gbdt\",\n        \"objective\": \"regression\",\n        \"metric\": \"RMSE\",\n        \"learning_rate\": 0.0045,\n    }\n\n    model = LGBMRegressor(**param)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    score = np.sqrt(mean_squared_error(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:18:12.82153Z","iopub.execute_input":"2022-01-04T17:18:12.821819Z","iopub.status.idle":"2022-01-04T17:18:21.783075Z","shell.execute_reply.started":"2022-01-04T17:18:12.82179Z","shell.execute_reply":"2022-01-04T17:18:21.782403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost\n* XGBoost is short for “eXtreme Gradient Boosting.” \n* The “eXtreme” refers to speed enhancements such as parallel computing and cache awareness that makes XGBoost approximately 10 times faster than traditional Gradient Boosting.\n* XGBoost is regularized, so default models often don’t overfit\n* It has extensive hyperparameters for fine-tuning\n","metadata":{}},{"cell_type":"code","source":"cv = cross_validate(\n    estimator=XGBRegressor(),\n    X=df.drop(columns=\"target\"),\n    y=df[\"target\"],\n    scoring=[\"r2\", \"neg_mean_squared_error\"],\n    verbose=True,\n    cv=5,\n    n_jobs=-1,\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T18:26:22.22763Z","iopub.execute_input":"2022-01-04T18:26:22.227939Z","iopub.status.idle":"2022-01-04T18:32:26.690693Z","shell.execute_reply.started":"2022-01-04T18:26:22.227906Z","shell.execute_reply":"2022-01-04T18:32:26.689295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv[\"test_neg_mean_squared_error\"].mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T18:33:45.671562Z","iopub.execute_input":"2022-01-04T18:33:45.673142Z","iopub.status.idle":"2022-01-04T18:33:45.683002Z","shell.execute_reply.started":"2022-01-04T18:33:45.673058Z","shell.execute_reply":"2022-01-04T18:33:45.681844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bagging","metadata":{}},{"cell_type":"markdown","source":"* Bagging is short for “bootstrap aggregation,” meaning that samples are chosen with replacement (bootstrapping), and combined (aggregated)\n* Decision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n# Random Forest\n* The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree via bagging. \n* It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. \n* This model generates decorelated trees by choosing a fresh sample of m predictors at each split (m ≈ √p)\n","metadata":{}},{"cell_type":"code","source":"%%time \n\ncv = cross_validate(\n    estimator=RandomForestRegressor(n_jobs=-1, verbose=True),\n    #     estimator=cuRFC(verbose=True),\n    X=df.drop(columns=[\"target\"]),\n    y=df[\"target\"],\n    cv=5,\n    scoring=[\"r2\", \"neg_mean_squared_error\"],\n    verbose=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T17:58:28.93374Z","iopub.execute_input":"2022-01-04T17:58:28.934022Z","iopub.status.idle":"2022-01-04T18:18:56.780835Z","shell.execute_reply.started":"2022-01-04T17:58:28.933991Z","shell.execute_reply":"2022-01-04T18:18:56.780104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv[\"test_neg_mean_squared_error\"].mean()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T18:19:45.861518Z","iopub.execute_input":"2022-01-04T18:19:45.861953Z","iopub.status.idle":"2022-01-04T18:19:45.869628Z","shell.execute_reply.started":"2022-01-04T18:19:45.861916Z","shell.execute_reply":"2022-01-04T18:19:45.868553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We are using the OOB(~ validation score) score to compare the training and test error\n# model = RandomForestRegressor(n_jobs=-1, verbose=True, oob_score = True)\n# model.fit(X, y)\n# # Training error\n# model.score(X,y)\n# # oob error\n# model.oob_score_\n# * We see the training score = 0.87 while the test score = 0.05 \n# * From the scores, we can say the our random forest is overfitting the training dataset \n# * Note that the CV and oob score are almost similar","metadata":{"execution":{"iopub.status.busy":"2022-01-04T12:39:13.721373Z","iopub.execute_input":"2022-01-04T12:39:13.721731Z","iopub.status.idle":"2022-01-04T12:44:43.894964Z","shell.execute_reply.started":"2022-01-04T12:39:13.721696Z","shell.execute_reply":"2022-01-04T12:44:43.893982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Negetive mean square errors\n1. Linear Regression --> -0.5274229884147644\n2. Ridge Regression --> (MSE) 0.5274229003487053\n3. Lasso Regression --> -0.5274227619171142\n4. Poisson Regression --> -0.5332794126312232\n5. K-Neighbor Regressor --> -0.656536448001861\n6. LGBM --> (MSE) 0.5258897237509819\n7. XGB --> -0.49416557550430296\n8. Random Forest --> -0.5009868281839414","metadata":{}}]}