{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\ntest=pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Shape of the train dataframe is {train.shape}')\nprint(f'Shape of the test dataframe is {test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for null values in the train dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_nulls=pd.DataFrame(np.c_[train.isna().sum()],columns=['Num_of_Nulls'],index=train.isna().sum().index)\ntrain_nulls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_nulls=pd.DataFrame(np.c_[test.isna().sum()],columns=['Num_of_Nulls'],index=test.isna().sum().index)\ntest_nulls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## No need to handle null values in both train and test datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dtypes=list(train.dtypes)\ntest_dtypes=list(test.dtypes)\nprint(f'Datatypes in train are {train_dtypes}')\nprint(f'Datatypes in test are {test_dtypes}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## All  the datatypes are float64 only"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cols=list(train.columns)\ntest_cols=list(test.columns)\nprint(f'Train columns are {train_cols}')\nprint(f'Test columns are {test_cols}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nimp_cols=train_cols[1:15]\ntarget_cols=train_cols[15]\ncorrl_matrix=train[imp_cols].corr()\nsns.heatmap(corrl_matrix,cbar=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrl_features=[]\nfor i in range(len(corrl_matrix)):\n    for j in range(i):\n        if abs(corrl_matrix.iloc[i,j])>0.8:\n            col_name=corrl_matrix.columns[i]\n            corrl_features.append(col_name)\nprint(corrl_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping this correlated features from Train and Test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.drop(columns=corrl_features,axis=1)\ntest=test.drop(columns=corrl_features,axis=1)\nprint(f'After dropping correlated features shape of train is {train.shape}')\nprint(f'After dropping correlated features shape of test is {test.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Check for duplicated rows in Train and Test Datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The number of duplicate rows in train dataset are {train.duplicated().sum()}\")\nprint(f\"The number of duplicate rows in test dataset are {test.duplicated().sum()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA on Train columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['target']);\nplt.xticks(range(0,10));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_cols=[col for col in train.columns if col.startswith('cont')]\nprint(f\"Length of important columns in train dataset are : {len(imp_cols)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(90,45))\nfor i in range(1,len(imp_cols)+1):\n    sns.distplot(train[imp_cols[i-1]],ax=plt.subplot(8,2,i))\n    plt.title(f'{imp_cols[i-1]}',fontsize=10)\nplt.savefig('./train.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Since most of the fields are having long tails have applied log transformations on those fields."},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\ntrain_copy=copy.deepcopy(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy_imp_cols=imp_cols+['target']\ntrain_copy_imp_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in train_copy_imp_cols:\n    train_copy[i]=train_copy[i].apply(lambda x:np.log1p(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_copy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(90,45))\nfor i in range(1,len(train_copy_imp_cols)+1):\n    sns.distplot(train_copy[train_copy_imp_cols[i-1]],ax=plt.subplot(8,2,i))\n    plt.title(f'{train_copy_imp_cols[i-1]}',fontsize=10)\nplt.savefig('./train_copy.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets build the model using simple linear regression and check hows the performance would be without any hyper parameter tuning."},{"metadata":{},"cell_type":"markdown","source":"* Lets fit it on train copy dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns=[col for col in train_copy.columns if col not in ['id','target']]\nX=train_copy[columns]\ny=train_copy['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(X,y)\nprint(f'Models score is {model.score(X,y)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying same log transformations on test dataset and lets predict the score on test datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in columns:\n    test[col]=test[col].apply(lambda x:np.log1p(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=test.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_values=model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df=pd.DataFrame(np.c_[test['id'],predicted_values],columns=['id','target'])\nsubmission_df['id']=submission_df['id'].astype('int')\nsubmission_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('./submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Normalizer\nnorm=Normalizer()\nX=norm.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers import Dense,Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_layer = Input(shape=(13,),dtype=\"float64\",name=\"input_layer\")\ndense_layer1= Dense(units=128,activation='relu',kernel_initializer='he_normal',name='dense_layer_1')(input_layer)\ndense_layer2= Dense(units=64,activation='relu',kernel_initializer='he_normal',name='dense_layer_2')(dense_layer1)\ndense_layer3= Dense(units=32,activation='relu',kernel_initializer='he_normal',name='dense_layer_3')(dense_layer2)\ndense_layer4= Dense(units=16,activation='relu',kernel_initializer='he_normal',name='dense_layer_4')(dense_layer3)\ndense_layer5= Dense(units=8,activation='relu',kernel_initializer='he_normal',name='dense_layer_5')(dense_layer4)\noutput_layer= Dense(units=1,activation='linear',name='output_layer')(dense_layer5)\nmodel=Model(inputs=input_layer,outputs=output_layer)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='sgd',loss=root_mean_squared_error,metrics=[tf.keras.metrics.RootMeanSquaredError()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X,y,epochs=30,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test=norm.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_values=model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df=pd.DataFrame(np.c_[test['id'],predicted_values],columns=['id','target'])\nsubmission_df['id']=submission_df['id'].astype('int')\nsubmission_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df['target']=submission_df['target'].apply(lambda x:np.exp(x)-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}