{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Contents:\n* 1. See data files\n* 2. Make a Validation Split\n* 3. Decide Best Model\n* 4. Tuning Gradient Boosting Model\n* 5. Tuning Random Forest Model\n* 6. Comparison of Tuned and Default Models\n* 7. Predict and Input Data based on Tuned-Gradient Boosting Model"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n     \n# import necessary modules\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error # import MSE to calculate RMSE use sqrt() or set squared=False\n        \ninput_path = Path('/kaggle/input/tabular-playground-series-jan-2021/')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. See data files\ncheck contents of: train.csv, test.csv, sample_submission.csv"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(input_path / 'test.csv', index_col='id')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(input_path / 'sample_submission.csv', index_col='id')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Make a Validation Split\nSplit train and test data from train.csv data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\ntarget = train.pop('target') # pull out target columnm from train DataFrame, target is Series\n(x_train, x_test, y_train, y_test) = train_test_split(train, target, test_size=0.2, random_state=0) # make train and test set from train.csv\n# just check split sucessfully\nprint(x_train)\nprint()\nprint(y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Decide Best Model\nMachine learning list:\n1. Dummy regression\n2. Lasso regression\n3. Ridge regression\n4. ElasticNet regression\n5. Decision Tree regression\n6. AdaBoost regression\n7. GradientBoosting regression\n8. Random Forest regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import machine laerning API\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n# add 2 methods, version 4 onwards\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.neural_network import MLPRegressor\n\ndef plot_models(name, y, y_model, lims=(0,12), figsize=(8,6)):\n    plt.figure(figsize=figsize)\n    score = mean_squared_error(y, y_model, squared=False)\n    plt.scatter(y, y_model)\n    plt.plot(lims, lims, color='black', linestyle='solid', linewidth=1.0)\n    plt.xlim(lims)\n    plt.ylim(lims)\n    plt.title(f'{name}: {score:.5f}', fontsize=18)\n    plt.show()\n    \n#model_names = ['DummyRegressor', 'Lasso', 'Ridge', 'ElasticNet',\n#               'DecisionTreeRegressor', 'AdaBoost', 'GradientBoostingRegressor', 'RandomForest']\nmodel_names = ['DummyRegressor', 'Lasso', 'Ridge', 'ElasticNet',\n               'DecisionTreeRegressor', 'AdaBoost', 'GradientBoostingRegressor', 'RandomForest', 'Bagging', 'MLPRegressor']\n#models = [DummyRegressor(), Lasso(), Ridge(), ElasticNet(), \n#          DecisionTreeRegressor(random_state=0), AdaBoostRegressor(random_state=0), \n#          GradientBoostingRegressor(random_state=0), RandomForestRegressor(n_jobs=-1, random_state=0)]\nmodels = [DummyRegressor(), Lasso(), Ridge(), ElasticNet(), \n          DecisionTreeRegressor(random_state=0), AdaBoostRegressor(random_state=0), \n          GradientBoostingRegressor(random_state=0), RandomForestRegressor(n_jobs=-1, random_state=0), \n          BaggingRegressor(n_jobs=-1, random_state=0), MLPRegressor(random_state=0)]\n\nfor name, model in zip(model_names, models):\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    plot_models(name, y_test, y_pred)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result is following:\n\n1. Dummy regression: 0.73239\n2. Lasso regression: 0.73239\n3. Ridge regression: 0.72552\n4. ElasticNet regression: 0.73239\n5. Decision Tree regression: 1.01322\n6. AdaBoost regression: 0.72655\n7. GradientBoosting regression: 0.71100\n8. Random Forest regression: 0.70831\n9. Bagging regression: 0.74023\n10. MLP Regression: 0.71572\n\nSince the less RMSE value has, the less difference between observed value and predicted value, i.e. Better Model. So, it seems Ramdom Forest regression is the best machine learning method. But it take much time to create model and predict the value. So let's try tuning GradientBoosting model at first, because the RMSE value is slightly different from that of Random Forest. Moreover, since some paramaters are same as Random Forest, those parameters can be used as Random Forest paramter tuning."},{"metadata":{},"cell_type":"markdown","source":"# 4. Tuning Gradient Boosting Model"},{"metadata":{},"cell_type":"markdown","source":"> # 1. Define parameter \"loss\" and \"max_features\""},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nloss_list = [\"ls\", \"lad\", \"huber\", \"quantile\"]\nfor loss in loss_list:\n    model = GradientBoostingRegressor(loss=loss, random_state=0)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(loss_list, RMSE, marker=\"o\")\nplt.xlabel('loss_list')\nplt.ylabel('RMSE value')\nplt.title('loss_list vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best parameter is loss=\"huber\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nmax_features_list = [\"auto\", \"sqrt\", \"log2\"]\nfor max_features in max_features_list:\n    model = GradientBoostingRegressor(loss=\"huber\", max_features=max_features, random_state=0)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(max_features_list, RMSE, marker=\"o\")\nplt.xlabel('max_features_list')\nplt.ylabel('RMSE value')\nplt.title('max_features vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best parameter is max_features=\"auto\"."},{"metadata":{},"cell_type":"markdown","source":"> # 2. Define paramter \"max_depth\"\nOn condition that loss=\"huber\" and max_features=\"auto\""},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nmax_depth_list = [i for i in range(3, 12, 2)]\nfor max_depth in max_depth_list:\n    model = GradientBoostingRegressor(loss=\"huber\", max_depth=max_depth, max_features=\"auto\", random_state=0)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(max_depth_list, RMSE, marker=\"o\")\nplt.xlabel('max_depth')\nplt.ylabel('RMSE value')\nplt.title('max_depth vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like RMSE is minimized where 7<=max_depth<=9, so let's search minimum value by running code again as 7<=max_depth<=9."},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nmax_depth_list = [i for i in range(7, 10)]\nfor max_depth in max_depth_list:\n    model = GradientBoostingRegressor(loss=\"huber\", max_depth=max_depth, max_features=\"auto\", random_state=0)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(max_depth_list, RMSE, marker=\"o\")\nplt.xlabel('max_depth')\nplt.ylabel('RMSE value')\nplt.title('max_depth vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph says that **max_depth=8** is best parameter. When max_depth=8 is set, RMSE is minimized."},{"metadata":{},"cell_type":"markdown","source":"> # 3. Define paramter \"n_estimators\"\nOn condition that loss=\"huber\", max_features=\"auto\" and max_depth=8"},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nn_estimators_list = [i for i in range(95,121,5)]\nfor n_estimators in n_estimators_list:\n    model = GradientBoostingRegressor(loss=\"huber\", max_depth=8, n_estimators=n_estimators, max_features=\"auto\", random_state=0)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(n_estimators_list, RMSE, marker=\"o\")\nplt.xlabel('n_estimators')\nplt.ylabel('RMSE value')\nplt.title('n_estimators vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It look like RMSE is minimized where 105<=n_estimators<=115, so let's search minimum value by running code again as 105<=n_estimators<=115."},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nn_estimators_list = [i for i in range(105, 116)]\n\nfor n_estimators in n_estimators_list:\n    model = GradientBoostingRegressor(loss=\"huber\", max_depth=8, n_estimators=n_estimators, max_features=\"auto\", random_state=0)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(n_estimators_list, RMSE, marker=\"o\")\nplt.xlabel('n_estimators')\nplt.ylabel('RMSE value')\nplt.title('n_estimators vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph says that **n_estimators=111** is best parameter When n_estimators=111 is set, RMSE is minimized."},{"metadata":{},"cell_type":"markdown","source":"**Conclusion is that best paramer to minimize RMSE: loss=\"huber\", max_features=\"auto\", max_depth=8, n_estimators=111**"},{"metadata":{},"cell_type":"markdown","source":"# 5. Tuning Random Forest Model"},{"metadata":{},"cell_type":"markdown","source":"> # 1. Define parameter \"max_features\""},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nmax_features_list = [\"auto\", \"sqrt\", \"log2\"]\nfor max_features in max_features_list:\n    model = RandomForestRegressor(max_features=max_features, random_state=0, n_jobs=-1)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(max_features_list, RMSE, marker=\"o\")\nplt.xlabel('max_features')\nplt.ylabel('RMSE value')\nplt.title('max_features vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best parameter is either \"sqrt\" or \"log2\". Let's use max_features=\"sqrt\"."},{"metadata":{},"cell_type":"markdown","source":"> # 2. Define paramter \"max_depth\"\nOn condition that max_features=\"sqrt\""},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nmax_depth_list = [i for i in range(5, 36, 5)]\nfor max_depth in max_depth_list:\n    model = RandomForestRegressor(max_features=\"sqrt\", max_depth=max_depth, random_state=0, n_jobs=-1)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(max_depth_list, RMSE, marker=\"o\")\nplt.xlabel('max_depth')\nplt.ylabel('RMSE value')\nplt.title('max_depth vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nmax_depth_list = [i for i in range(15, 26)]\nfor max_depth in max_depth_list:\n    model = RandomForestRegressor(max_features=\"sqrt\", max_depth=max_depth, random_state=0, n_jobs=-1)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(max_depth_list, RMSE, marker=\"o\")\nplt.xlabel('max_depth')\nplt.ylabel('RMSE value')\nplt.title('max_depth vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph says that **max_depth=22** is best parameter. When max_depth=22 is set, RMSE is minimized."},{"metadata":{},"cell_type":"markdown","source":"> # 3. Define paramter \"n_estimators\"\nOn condition that max_features=\"sqrt\" and max_depth=22"},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nn_estimators_list = [i for i in range(150, 251,10)]\nfor n_estimators in n_estimators_list:\n    model = RandomForestRegressor(max_features=\"sqrt\", max_depth=22, n_estimators=n_estimators, random_state=0, n_jobs=-1)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(n_estimators_list, RMSE, marker=\"o\")\nplt.xlabel('n_estimators')\nplt.ylabel('RMSE value')\nplt.title('n_estimators vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It look like RMSE is minimized where 110<=n_estimators<=113, so let's search minimum value by running code again as 110<=n_estimators<=113."},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nn_estimators_list = [i for i in range(125, 136)]\nfor n_estimators in n_estimators_list:\n    model = RandomForestRegressor(max_features=\"sqrt\", max_depth=22, n_estimators=n_estimators, random_state=0, n_jobs=-1)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(n_estimators_list, RMSE, marker=\"o\")\nplt.xlabel('n_estimators')\nplt.ylabel('RMSE value')\nplt.title('n_estimators vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph says that n_estimators=112 is best parameter When n_estimators=112 is set, RMSE is minimized."},{"metadata":{},"cell_type":"markdown","source":"**Conclusion is that best paramer to minimize RMSE: max_depth=18, n_estimators=112**"},{"metadata":{},"cell_type":"markdown","source":"> # 4. Define parameter \"min_sample_split\""},{"metadata":{"trusted":true},"cell_type":"code","source":"RMSE = []\nmin_samples_split_list = [2, 3, 4, 5, 6, 7]\nfor min_samples_split in min_samples_split_list:\n    model = RandomForestRegressor(max_features=\"sqrt\", max_depth=22, n_estimators=n_estimators, \n                                  min_samples_split=min_samples_split, random_state=0, n_jobs=-1)\n    model.fit(x_train, y_train)\n    y_predict = model.predict(x_test)\n    score = mean_squared_error(y_test, y_predict, squared=False)\n    RMSE.append(score)\n\nplt.plot(max_features_list, RMSE, marker=\"o\")\nplt.xlabel('max_features')\nplt.ylabel('RMSE value')\nplt.title('max_features vs RMSE value', fontsize=12)\nplt.grid()\nplt.show()\nprint(RMSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Comparison of Tuned and Default Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n\nRF_RMSE = []\nGB_RMSE = []\nlegend_list = [\"Default\", \"Tuned\"]\n\nGB_models = [GradientBoostingRegressor(random_state=0), \n             GradientBoostingRegressor(loss=\"huber\", max_features=\"auto\", max_depth=8, n_estimators=111,  random_state=0)]\nRF_models = [RandomForestRegressor(n_jobs=-1, random_state=0), \n             RandomForestRegressor(max_features=\"sqrt\", max_depth=22, n_estimators=112, min_samples_split=3, random_state=0, n_jobs=-1)]\n\nfor model in GB_models:\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    score = mean_squared_error(y_test, y_pred, squared=False)\n    GB_RMSE.append(score)\n\nfor model in RF_models:\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_test)\n    score = mean_squared_error(y_test, y_pred, squared=False)\n    RF_RMSE.append(score)\n\nprint(f\"GB Default model score: {GB_RMSE[0]}\")\nprint(f\"GB Tuned-model score: {GB_RMSE[1]}\")\nprint()\nprint(f\"RF Default model score: {RF_RMSE[0]}\")\nprint(f\"RF Tuned-model score: {RF_RMSE[1]}\")\n\nplt.figure(figsize=(8,6)) \nplt.plot(legend_list, GB_RMSE, label=\"GB:Gradient Boosting\", marker=\"o\")\nplt.plot(legend_list, RF_RMSE, label=\"RF:Random Forest\", marker=\"^\")\nplt.xlabel('name')\nplt.ylabel('RMSE value')\nplt.title(\"RMSE Visualization\", fontsize=16)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use Tuned-Random Forest regressor to predict the values."},{"metadata":{},"cell_type":"markdown","source":"# 7. Predict and Input Data based on Tuned-Gradient Boosting Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_model = GradientBoostingRegressor(loss=\"huber\", max_features=\"auto\", max_depth=8, n_estimators=111,  random_state=0)\nfinal_model.fit(x_train, y_train)\ny_pred = final_model.predict(x_test)\nfinal_score = mean_squared_error(y_test, y_pred, squared=False)\nprint(f\"GB Tuned-model score: {final_score}\")\n\nsubmission['target'] = final_model.predict(test)\nsubmission.to_csv('submission.csv')\n\ndf = pd.read_csv('submission.csv')\ndf.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}