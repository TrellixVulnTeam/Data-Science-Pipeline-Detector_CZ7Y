{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gc ##garbage collection\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom tqdm import tqdm #for seeing progress\n\nimport sklearn\nimport sklearn.svm\nimport sklearn.preprocessing\nimport datetime\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport torch #pytorch\nimport torch.nn as nn ##neural net stuff\nimport torch.nn.functional as F #useful torch function (eg.: activations)\nfrom torch.utils.data import TensorDataset, DataLoader, Subset #for easy data batches\nimport torch.optim as optim #optimisation functions\n\nfrom sklearn.model_selection import train_test_split #for creating test and train data\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler #scaling the data to be in range [0,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://lightgbm.readthedocs.io/en/latest/\n#https://www.kaggle.com/vinnsvinay/introduction-to-boosting-using-lgbm-lb-0-68357#Reading-data\n#https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n\nimport lightgbm as lgb #light gbm -> creates trees","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/tabular-playground-series-jan-2021/train.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = df.iloc[:,1:15]\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = df.iloc[:,15]\ntarget.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"#since we have 14 continuous features we will graph them against the target\nnrow, ncol = (7,2)\nnum_features = 14\nfig,ax = plt.subplots(nrow,ncol,figsize=(28,56)) #3 rows 5 columns\n\ndef plot_all_features(dataframe,nrow, ncol, num, y):\n    #we want to fill it up row by row\n    text = \"cont\"\n    for i in range(num):\n        tx = text+str(i+1)\n        \n        #so first we want to find which row it is in\n        row = i//ncol\n        col = i-(ncol*row)\n        \n        ax[row,col].scatter(dataframe[tx], dataframe[y])\n        ax[row,col].set_ylabel(y)\n        ax[row,col].set_xlabel(tx)\n\n\nplot_all_features(df,nrow,ncol,num_features,\"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##now we plot the distribution of these features with histograms\ndef plot_all_features_dist(dataframe,nrow,ncol, nbins=50):\n    text = \"cont\"\n    row=0\n    col=0\n    for i in dataframe.columns:\n        #tx = text+str(i+1)\n        #row = i//ncol\n        #col = i-(ncol*row)\n        \n        ax[row,col].hist(dataframe[i], nbins, density=True)\n        #ax[row,col].set_ylabel(y)\n        ax[row,col].set_xlabel(str(i))\n        col += 1\n        if col >= ncol:\n            row += 1\n            col = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_all_features_dist(dataframe, start, end, nrow, ncol, nbins=50):\n    text = \"cont\"\n    row=0\n    col=0\n    for i in dataframe.columns[start:end]:\n        #tx = text+str(i+1)\n        #row = i//ncol\n        #col = i-(ncol*row)\n        \n        ax[row,col].hist(dataframe[i], nbins, density=True)\n        #ax[row,col].set_ylabel(y)\n        ax[row,col].set_xlabel(str(i))\n        col += 1\n        if col >= ncol:\n            row += 1\n            col = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = fig,ax = plt.subplots(8,2,figsize=(28,56)) #3 rows 5 columns\nplot_all_features_dist(df, 1, 16, 7, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we normalise the data\n\ndef normalize(df, start, end):\n    for i in df.columns[start:end]:\n        df[i] = (df[i]-np.mean(df[i]))/np.std(df[i])\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_normed = normalize(df, 1, 15)\nfig,ax = fig,ax = plt.subplots(7,2,figsize=(28,56)) #3 rows 5 columns\nplot_all_features_dist(df_normed, 1, 15, 8, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"mean of target \", np.mean(df[df.columns[15]]))\nprint(\"std of target \", np.std(df[df.columns[15]]))\nprint(\"number of entries \", len(df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"##pytorch test\ncenters = 2\nx1 = 5*torch.rand(3,3)\nprint(x1.size())\nprint(x1)\nx1 = x1.unsqueeze(1)\nprint(x1.size())\nprint(x1)\nx1 = x1.expand(-1, centers, -1)\nprint(x1.size())\nprint(x1)\n\nc1 = torch.ones(centers,3)\nprint(c1)\n\nprint(x1-c1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Radial Basis Network\n\nhttps://www.programmersought.com/article/18665200813/\nhttps://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/\nhttps://www.cc.gatech.edu/~isbell/tutorials/rbf-intro.pdf"},{"metadata":{"trusted":true},"cell_type":"code","source":"def gaussian(x):\n    return torch.exp(-x.pow(2))\n\ndef linear(x):\n    return x\n\ndef quadratic(x):\n    return x.pow(2)\n\ndef multi_quadratic(x):\n    return (1+x.pow(2)).pow(0.5)\n\ndef inv_quadratic(x):\n    return (1+quadratic(x)).pow(-1)\n\ndef inv_multi_quad(x):\n    return multi_quadratic(x).pow(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RBFN(nn.Module):\n    \n    \n    def __init__(self, n_centers, in_features, out_features=1, basis_func=None):\n        super(RBFN, self).__init__()\n        #D = n_in\n        #N = n_centers\n        \n        self.out_features = out_features\n        self.in_features = in_features\n        \n        self.num_centers = n_centers #number of centers\n        \n        self.centers = nn.Parameter(torch.Tensor(self.num_centers, self.in_features), requires_grad = True) #we create N number of random centers, NxD\n        self.sigmas = nn.Parameter(torch.Tensor(self.num_centers), requires_grad=True) #(1, num_centers)\n        \n        self.linear = nn.Linear(self.num_centers, self.out_features, bias = True) #(num_centers, out_features)\n        self.reset_parameters()\n        \n        if basis_func != None:\n            self.basis_func = basis_func\n        else:\n            def rbf(x):\n                return torch.exp(-x.pow(2))\n                \n            self.basis_func = rbf\n        \n    def reset_parameters(self):\n        nn.init.normal_(self.centers, 0, 1)\n        nn.init.constant_(self.sigmas, 1)\n    \n        \n    def kernel(self, x):\n        #x is the input batch\n        #M = batch_size\n        #n = x.size(0) #the batch size\n        #x is size (M,in_features)\n        #x.unsqueeze(1) => (M, 1, in_features)\n        #x.expand(-1, self.num_centers, -1) => (M, num_centers, in_features)\n        x = x.unsqueeze(1).expand(-1, self.num_centers, -1)\n        c = self.centers.unsqueeze(0) #(1, num_centers, in_features)\n        #d (M, num_centers, in_features) => (M, num_centers)\n        d = (x-c).pow(2).sum(-1).pow(0.5) #distance calculation\n        d *= self.sigmas.unsqueeze(0) #(M, num_centers)\n        \n        return self.basis_func(d)  \n    \n        #1. calculate ||x - centers||\n        \n        #2. calculate B||x-centers||\n        \n        #3. return exp(B||x-centers||)\n        \n    def forward(self, x):\n        \"\"\"\n        Shapes:\n            x (input) has size (M, in_features) where M is an arbitrary batch size\n            \n            output has size (M, out_features)\n            \n            centers has size (N, in_features) where N is an arbitrary number of centers\n            \n            want to get kernel matrix of size (M,N) where K(i,j) = x_i - c_j\n        \n        \"\"\"\n        \n        x = self.kernel(x)\n        return self.linear(x)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dataset / Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_valid, Y_train, Y_valid = train_test_split(df_normed.iloc[:, 1:15].to_numpy(),\n                                                      df_normed.iloc[:,15].to_numpy(),\n                                                     test_size = 0.2,\n                                                     random_state = 42)\n\ntrain_set = TensorDataset(torch.from_numpy(X_train).type(torch.FloatTensor).to(device),\n                         torch.from_numpy(Y_train).type(torch.FloatTensor).to(device))\n\nvalid_set = TensorDataset(torch.from_numpy(X_valid).type(torch.FloatTensor).to(device),\n                         torch.from_numpy(Y_valid).type(torch.FloatTensor).to(device))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 512\n\ntrain_loader = DataLoader(train_set, batch_size, pin_memory = False)\nvalid_loader = DataLoader(valid_set, batch_size, pin_memory = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit(model, dl_train, dl_val, epochs = 50, test_epochs = 1):\n    \n    losses = []\n    valid_losses = []\n    for epoch in range(epochs):\n    \n        sum_loss = 0\n        n = 0\n        for x,y in tqdm(dl_train):\n            optimizer.zero_grad()\n            \n            pred_y = model(x)\n            y = y.unsqueeze(1)\n            #print(pred_y.size())\n            #print(y.size())\n            loss = error(pred_y, y)\n            \n            loss.backward()\n            optimizer.step()\n            \n            sum_loss += torch.sqrt(loss).cpu().data\n            n+=1\n        \n        ####end of train\n        losses.append(sum_loss / n) #store losses\n        prt_str = f\"epoch loss [{losses[-1]}]; \"\n        \n        ####validation\n        if epoch % test_epochs == 0:\n            model = model.eval()\n            \n            sum_loss = 0\n            n = 0\n            for x,y in tqdm(dl_val):\n                #optimizer.zero_grad()\n\n                pred_y = model(x)\n                y = y.unsqueeze(1)\n                loss = torch.sqrt(error(pred_y, y))\n\n                #loss.backward()\n                #optimizer.step()\n\n                sum_loss += loss.cpu().data\n                n+=1\n\n            valid_losses.append(sum_loss / n)\n            prt_str += f\"validation_loss [{valid_losses[-1]}]\"\n            \n            model = model.train()\n            \n        ## we print the epoch data\n        print(prt_str)\n        \n    return losses, valid_losses    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rbf_model = RBFN(14, 14, 1, gaussian).to(device).eval() #14 centers, 14 features, 1 output\nprint(rbf_model)\n#x1 = torch.ones(1,14)\n#print(x1)\n#y1 = rbf_model(x1)\n#print(y1.size())\n#print(y1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rbf_model = rbf_model.to(device).train()\nlr = 0.001\nweight_decay = 0 #1e-4\nerror = nn.MSELoss()\noptimizer = optim.Adam(rbf_model.parameters(), lr=lr, weight_decay = weight_decay, amsgrad = True)\n\n#losses, valid_losses = fit(rbf_model, train_loader, valid_loader, 50, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.plot(losses[1:])\n#plt.plot(valid_losses[1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(trial):\n    num_layers_centered = trial.suggest_int(\"num_centered_layers\", 1, 3)\n    num_layers_linear = trial.suggest_int(\"num_linear_layers\", 0, 3)\n    layers =[]\n    radial_func = gaussian\n    \n    in_features = 14\n    prev_centers = 14**3\n    max_out = 14**3\n    \n    for i in range(num_layers_centered):\n        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 2**10)\n        num_centers = trial.suggest_int(\"n_centers_l{}\".format(i), 2, 2**10)\n        layers.append(RBFN(num_centers, in_features, out_features, radial_func))\n        layers.append(nn.ReLU())\n        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.1, 0.5)\n        layers.append(nn.Dropout(p))\n        in_features = out_features\n        #max_out = out_features\n        #prev_centers = num_centers\n    \n    for i in range(num_layers_linear):\n        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 1+14**2)\n        layers.append(nn.Linear(in_features, out_features))\n        layers.append(nn.ReLU())\n        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.1, 0.5)\n        layers.append(nn.Dropout(p))\n        in_features = out_features\n    \n    layers.append(nn.Linear(in_features, 1))\n    return nn.Sequential(*layers)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" ##hyperparameter optimisation\n\ndef objective(trial):\n    dl_train = train_loader\n    dl_val = valid_loader\n    error = nn.MSELoss()\n    #num_c = trial.suggest_int(\"num_c\", 1, 14**3)\n    epochs = trial.suggest_int(\"epochs\", 1, 200)\n    \n    #model = RBFN(num_c,14,1, gaussian).to(device) ##fixed model\n    model = create_model(trial).to(device) #variational model\n    print(model)\n    \n    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)    \n    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n    \n    for epoch in tqdm(range(epochs)):\n    \n        for x,y in dl_train:\n            optimizer.zero_grad()\n            \n            pred_y = model(x)\n            y = y.unsqueeze(1)\n            #print(pred_y.size())\n            #print(y.size())\n            loss = error(pred_y, y)\n            \n            loss.backward()\n            optimizer.step()\n        \n        ####end of train\n        \n        ####validation\n        valid_loss = 0\n        if epoch % 1 == 0:\n            model = model.eval()\n            n=0\n            for x,y in dl_val:\n                #optimizer.zero_grad()\n\n                pred_y = model(x)\n                y = y.unsqueeze(1)\n                loss = torch.sqrt(error(pred_y, y))\n\n                #loss.backward()\n                #optimizer.step()\n\n                valid_loss += loss.cpu().data\n                n+=1\n\n            valid_loss /= n        \n            model = model.train()\n            \n        ## we print the epoch data\n        \n        trial.report(valid_loss, epoch)\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n        \n    return valid_loss ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.kernel_ridge import KernelRidge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#regr_model = sklearn.svm.SVR(kernel=\"rbf\", C=1.0, epsilon = 0.1)\n#regr_model = KernelRidge(alpha=1.0, kernel=\"rbf\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#regr_model.fit(df_normed.iloc[:,1:15], df_normed[\"target\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_df = pd.read_csv(\"../input/tabular-playground-series-jan-2021/test.csv\")\n#test_df = normalize(test_df, 1, 15)\n#predictions = regr_model.predict(test_df.iloc[:,1:15])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sam_sub = pd.read_csv(\"../input/tabular-playground-series-jan-2021/sample_submission.csv\")\n#sam_sub[\"target\"] = predictions\n\n#sam_sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}