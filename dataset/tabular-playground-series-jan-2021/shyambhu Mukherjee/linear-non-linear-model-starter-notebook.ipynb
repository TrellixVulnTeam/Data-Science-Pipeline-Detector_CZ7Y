{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Tabular Playground competition:\nIn this notebook, I have explored the dataset, plotted and analyzed the features. Then I have created features from multimodal analysis. Finally I have used number of models to train and create the submission file. The model parameters provided are not the optimal ones. But I have provided suggestions on how to reach the parameters in the respective model's starting markdown. Now, the contents of this notebook are:<br/>\n(1) [Basic data exploration](#section1)<br/>\n(2) [Multimodal distributions and fitting of training data](#section2)<br/>\n(3) [Modeling efforts](#section3)<br/>\nTry forking and optimizing the models and then finally get a good result. If you are using the notebook and like the work, consider showing your appreciation. Also I am open to suggestions for improving the notebook.<br/>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor as RfReg\nimport xgboost as xgb\nfrom sklearn.linear_model import LinearRegression as LinReg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id = 'section1'>Basic data exploration</a>:\nIn this section, we will load, check shape and column names of the data. Then we will plot the different columns of both train and test data and make basic insights and observations."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/train.csv')\ntest_data = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/test.csv')\nprint(\"train data shape is:\",train_data.shape)\nprint(\"test data shape is:\",test_data.shape)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop('id',axis = 1)\ntest_data = test_data.drop('id',axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def brief_col(data,col):\n    print(\"Name of the column:\",col)\n    print(\"the description of the column is:\")\n    print(\"Number of missing points is:\",data[col].isna().sum())\n    print(data[col].describe())\n    plt.figure(figsize = (10,10))\n    plt.hist(data[col].tolist())\n    plt.title(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_data.columns:\n    print(\"in training data:\")\n    brief_col(train_data,col)\n    print(\"in test data:\")\n    if col == 'target':\n        continue\n    brief_col(test_data,col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot these are the following important observations:\n(1) Number of variables are bimodal or multimodal. It is better idea to fit bimodal or multimodal distributions for outlier treatment and better modeling treatment.<br/>\n(2) The train and test columns look almost similar; but there are significant difference in range and spread in some cases. So before prediction, normalizing the data is needed.<br/>\n(3) We can get a concise normal distribution on the target dataset. On the prediction output also, we need to check the prediction's distribution so that it falls in similar distribution.<br/>"},{"metadata":{},"cell_type":"markdown","source":"## <a id = 'section2'>Multimodal distributions and fitting of training data</a>:\nIn this section, we will go through each of the columns; try and fit the optimal number of modes with them; and fit proper distributions on them to properly model the data.<br/>\nThe libraries we are using for this are sklearn, scipy and statsmodels. You can read about it more here.<br/>\nFor multimodal distribution fitting,<br/>\n(1)[read jakevdp's blog (he is the author of sklearn.kerneldensity)](https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/)<br/>\n(2)[read stackoverflow](https://stackoverflow.com/questions/6620471/fitting-empirical-distribution-to-theoretical-ones-with-scipy-python)<br/>\n(3)[sklearn kde fitting](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity)<br/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KernelDensity\nfrom scipy.stats import gaussian_kde,norm\nfrom statsmodels.nonparametric.kde import KDEUnivariate\nfrom statsmodels.nonparametric.kernel_density import KDEMultivariate","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using sklearn kernel density to fit multimodal distribution<br/>\nThe following example is adapted from [this example by sklearn](https://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html#sphx-glr-auto-examples-neighbors-plot-kde-1d-py)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.utils.fixes import parse_version\n\n# `normed` is being deprecated in favor of `density` in histograms\nif parse_version(matplotlib.__version__) >= parse_version('2.1'):\n    density_param = {'density': True}\nelse:\n    density_param = {'normed': True}\n\n# ----------------------------------------------------------------------\n# Plot the progression of histograms to kernels\ndef format_func(x, loc):\n    if x == 0:\n        return '0'\n    elif x == 1:\n        return 'h'\n    elif x == -1:\n        return '-h'\n    else:\n        return '%ih' % x\n\ndef plotstimate(X):    \n    np.random.seed(1)\n    X_plot = np.array(X)[:, np.newaxis]\n    #X_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n    bins = np.linspace(-5, 10, 10)\n\n    fig, ax = plt.subplots(1,2, sharex=True, sharey=True)\n    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n\n    # histogram 1\n    #ax[0, 0].hist(X, bins=bins, fc='#AAAAFF', **density_param)\n    #ax[0, 0].text(-3.5, 0.31, \"Histogram\")\n\n    # histogram 2\n    #ax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc='#AAAAFF', **density_param)\n    #ax[0, 1].text(-3.5, 0.31, \"Histogram, bins shifted\")\n\n    # tophat KDE\n    kde = KernelDensity(kernel='tophat', bandwidth=0.75).fit(X_plot)\n    log_dens = kde.score_samples(X_plot)\n    ax[0].hist(np.exp(log_dens),fc = '#AAAAFF')\n    #ax[0].scatter(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')\n    ax[0].set_title(\"Tophat Kernel Density\")\n\n    # Gaussian KDE\n    kde = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(X_plot)\n    log_dens = kde.score_samples(X_plot)\n    ax[1].hist(np.exp(log_dens),fc = '#AAAAFF')\n    #ax[1].fill(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')\n    ax[1].set_title(\"Gaussian Kernel Density\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotstimate(train_data.sample(frac = 0.01)['cont1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotstimate(train_data.sample(frac = 0.01)['cont2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotstimate(train_data.sample(frac = 0.01)['cont3'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This fitting effectively fits the densities therefore, but there is no actionable metric which I can take from it. I will therefore try and fit gaussian mixture models to the distributions to check effectively the different origins of the bimodal variables.<br/>\nWe will be following gaussian mixture model from[ sklearn's documentation](https://scikit-learn.org/stable/modules/mixture.html)."},{"metadata":{},"cell_type":"markdown","source":"### GMM models"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.mixture import GaussianMixture\nX = np.array(train_data[['cont1']])\ngm = GaussianMixture(n_components=2, random_state=0).fit(X)\nprint(gm.means_)\nprint(gm.weights_)\nprint(gm.covariances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, this is a tangible resource. Using this, we will break clearly bimodal distributions into two separate variables based on their predicted labels; and assign their value to respective labelled variables. The other one will be assigned a 0 in that case.<br/> \nAlso we will add a binary variable denoting high or low from these.<br/>\nLet's implement it now."},{"metadata":{},"cell_type":"markdown","source":"Before diving in, we will note what all features are bimodal, normal and all."},{"metadata":{"trusted":true},"cell_type":"code","source":"bimodal = ['cont1','cont2','cont4','cont11','cont12','cont13','cont14']\nnormal = ['cont3','cont6','cont7','cont9','cont10']\npoisson = ['cont5','cont8']\n#cont10 has a high pick near 0.8 model it as normal only, but put a check if it is 0.8 or near.\n#for poisson check if it near the lowest values which is near 0.3 for both.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in bimodal:\n    X = np.array(train_data[[col]])\n    gm = GaussianMixture(n_components=2, random_state=0).fit(X)\n    train_data[col+'_label_low'] = gm.predict(X)\n    train_data[col+'_label_high'] = 1-train_data[col+'_label_low']\n    train_data[col+'_val_low'] = train_data[col]*train_data[col+'_label_low']\n    train_data[col+'_val_high'] = train_data[col]*train_data[col+'_label_high']\n    test_data[col+'_label_low'] = gm.predict(np.array(test_data[col]).reshape(-1,1))\n    test_data[col+'_label_high'] = 1-test_data[col+'_label_low']\n    test_data[col+'_val_low'] = test_data[col]*test_data[col+'_label_low']\n    test_data[col+'_val_high'] = test_data[col]*test_data[col+'_label_high']\ndef is_low_val(x):\n    if x>0.2 and x<=0.3:\n        return 1\n    return 0\ndef is_near_peak(x):\n    if x>0.75 and x<=0.85:\n        return 1\n    return 0\nfor col in poisson:\n    train_data[col+'_lowest_val'] = train_data[col].apply(lambda x: is_low_val(x))\n    test_data[col+'_lowest_val'] = test_data[col].apply(lambda x: is_low_val(x))\ntrain_data['cont10_nearHighPeak'] = train_data['cont10'].apply(lambda x: is_near_peak(x))\ntest_data['cont10_nearHighPeak'] = test_data['cont10'].apply(lambda x: is_near_peak(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in bimodal:\n    brief_col(train_data,col+'_val_low')\n    brief_col(train_data,col+'_val_high')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clearly we have got clear separate normal distributions from the bimodal distributions. Now that we are done creating these; let's get to training different models."},{"metadata":{},"cell_type":"markdown","source":"##<a id='section3'> Modeling Efforts</a>:\nWe have tried out the following methods:<br/>\n(1) [Linear Model](#linear)<br/>\n(2) [Random forest regressor](#rf)<br/>\n(3) [MARS spline regressor](#mars)<br/>\n(4) [Xgboost regressor](#xgb)<br/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id = 'linear'>Linear model</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split as tts\nY_train = train_data['target']\nX_train = train_data.drop('target',axis = 1)\nX_trainer,X_train_val,Y_trainer,Y_train_val = tts(X_train,Y_train,test_size = 0.2,\n                                                  shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression as linreg\nlinmodel = linreg(normalize = True,n_jobs = -1)\nlinmodel.fit(X_trainer,Y_trainer)\npred_trainer = linmodel.predict(X_trainer)\nprint(rsc(pred_trainer,Y_trainer))\npred_test = linmodel.predict(X_train_val)\nprint(rsc(pred_test,Y_train_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id = 'rf'>Random Forest Regressor</a>\nBigger models are pretty slow: so need to implement the [GPU model](https://medium.com/rapids-ai/accelerating-random-forests-up-to-45x-using-cuml-dfb782a31bea). This will be implemented in later versions of the model."},{"metadata":{},"cell_type":"markdown","source":"### I have left the fine tuning for the models left. You can tune it on your own and submit.\nTips for optimizing:<br/>\n(1) increase n_estimators<br/>\n(2) increase and check max_depth<br/>\n(3) tune min_samples_split to optimal value, as in check different values<br/>\n(4) Try increasing max_samples and check when the r_square score becomes better<br/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor as rfreg\nfrom sklearn.metrics import r2_score as rsc\nregressor = rfreg(n_estimators = 128,\n                  max_depth = 4,\n                  min_samples_split = 1,\n                  max_features = 'auto',\n                  max_samples = 0.1,\n                  n_jobs = -1)\nregressor.fit(X_trainer,Y_trainer)\npred_train = regressor.predict(X_trainer)\nprint(\"train rmse is:\",rsc(Y_trainer,pred_train))\npred_train_val = regressor.predict(X_train_val)\nprint(\"test rmse is:\",rsc(Y_train_val,pred_train_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id= 'mars'>MARS model</a>\nRead about it from [machine learning mastery](https://machinelearningmastery.com/multivariate-adaptive-regression-splines-mars-in-python/)."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install sklearn-contrib-py-earth\nimport pyearth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyearth import Earth\nmars_model = Earth()\nmars_model.fit(X_trainer,Y_trainer)\npred_train = mars_model.predict(X_trainer)\nprint(\"train rmse is:\",rsc(Y_trainer,pred_train))\npred_train_val = mars_model.predict(X_train_val)\nprint(\"test rmse is:\",rsc(Y_train_val,pred_train_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='xgb'>Xgboost regressor</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxg_reg = xgb.XGBRegressor(objective ='reg:linear', \n                          colsample_bytree = 0.3, \n                          learning_rate = 0.3,\n                          max_depth = 2, \n                          alpha = 0, \n                          n_estimators = 100)\nxg_reg.fit(X_trainer,Y_trainer)\npred_trainer = xg_reg.predict(X_trainer)\nprint(rsc(pred_trainer,Y_trainer))\npred_test = xg_reg.predict(X_train_val)\nprint(rsc(pred_test,Y_train_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### create submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_file = pd.read_csv('/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv')\npred_submission = regressor.predict(test_data)\nsubmission_file['target'] = pred_submission\nsubmission_file.to_csv('third_submission.csv',index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}