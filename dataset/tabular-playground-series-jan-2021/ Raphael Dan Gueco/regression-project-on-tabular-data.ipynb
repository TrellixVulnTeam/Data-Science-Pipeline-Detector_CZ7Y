{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Regression analysis consists of a set of machine learning methods that allow us to predict a continuous outcome variable (y) based on the value of one or multiple predictor variables (x). ... Linear regression is the most simple and popular technique for predicting a continuous variable.\nhttp://www.sthda.com/english/wiki/regression-analysis-essentials-for-machine-learning"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Supervised learning Regression\nif the desired output consists of one or more continuous variables, then the task is called regression. An example of a regression problem would be the prediction of the length of a salmon as a function of its age and weight. \n\nFound from https://scikit-learn.org/stable/tutorial/basic/tutorial.html"},{"metadata":{},"cell_type":"markdown","source":"Note: y_val == y_validation == y_test\n\nNote: X_val == X_validation == X_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom catboost import CatBoostRegressor\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/test.csv\")\nsub = pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We can understand that all the freq are relatively the same"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.target.hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We can tell that further down the line id must be dropped"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# No nans\n# # Aswell as need to drop id as its a useless column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# now we can focus on the continous variable columns"},{"metadata":{},"cell_type":"markdown","source":"# Judgin by top to bottom, we can see that cont variables 1,6,7,8,9,10,11,12,13 are the most correlated.\nThis must have some purpose or meaning "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr(method = \"pearson\").style.background_gradient(cmap='Reds')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"prepare for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets first try with LGBM regressor by itself"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = ['cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cont11','cont12','cont13','cont14']\n#features = ['cont1','cont6','cont7','cont8','cont9','cont10','cont11','cont12','cont13']\ny = train['target']\n\nX_train,X_test, y_train,y_test = train_test_split(train[features],train['target'],test_size=0.15)\n\n#Shape wise doesnt matter until later, but we do want a more larger training size\n#85/15  train/test split\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\nimport lightgbm as lgbm\n#note a booster needs to be specified, dart wont work\nparam = {'boosting_type': 'gbdt',\n         'objective': \"regression\", 'metric': \"RMSE\",\n         'learning_rate': 0.0045,\n        }\n\n\nmodel = LGBMRegressor(**param)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"predict() returns an integer array \n\nAlso note that when using .fit() the input array lengths must both be the same. In this case its 200000\n\nThis was the goal from the beginning, predicting against the 200000 test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)\n\n# Used for combining data for submission\nypred = model.predict(test[features])\n\n#\nypred2 = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# As we can see below doing simple lgbm regression is Alright. The result was a score that is really bad."},{"metadata":{"trusted":true},"cell_type":"code","source":"#check what result we got for rmse\n#squared = FALSE , If True returns MSE value, if False returns RMSE value.\n\nimport math\n#rmse always takes in validation sets, eg. y test, x test predicted.\nrmse = np.sqrt(mean_squared_error(y_test, ypred2))\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['target'] = ypred \nsub.to_csv(\"submission.csv\",index=False)\nsub.head()\nprint(\"success!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Why dont we try k-fold with lgbm?\n"},{"metadata":{},"cell_type":"markdown","source":"# 1. Create kfold, shuffle true and number of splits being done is 3\n\n Shuffle the dataset randomly. Also we need our target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"#remember to not define y twice\ny = train['target']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Split the dataset into k groups"},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=3, shuffle=True, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.Create params, note that most params will be used for accuracy. \n\nMetrics will be RMSE \n\nboosting type will be gbdt used instead of dart. Dart does not allow early stopping\n\nUse small learning_rate with large num_iterations. This way you can get the model to learn more overtime with more large amount of data. Note that we cant specify num_iterations as this conflicts with the boost."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {   \"metric\": \"rmse\",\n              \"boosting_type\": \"gbdt\",\n              \"learning_rate\":0.01,\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Split data and fit data"},{"metadata":{},"cell_type":"markdown","source":"# code structure referenced from https://www.kaggle.com/fatihozturk/lgbm-model-initialisation-trick"},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\n#split the training data\nfor train_split, test_split in kf.split(train[features], train['target']):\n    \n    #acquire the train and test \n    X_train= train.iloc[train_split]\n    X_test = train.iloc[test_split]\n    y_train = y.iloc[train_split]\n    y_test = y.iloc[test_split]\n    \n    d_train = lgb.Dataset(X_train[features], label= y_train)\n    d_test = lgb.Dataset(X_test[features], label = y_test)\n    \n    #If you want early stopping, you need to provide validation set, \n    #as the error message clearly states. And you need to do it in a fit method\n    \n    #If verbose_eval is True then the evaluation metric on the \n    #validation set is printed at each boosting stage\n    \n    #num_boost_round = Number of boosting iterations.\n    model = lgbm.train(params,d_train,valid_sets=[d_train, d_test],verbose_eval=100,num_boost_round=1000,early_stopping_rounds=100)\n    \n    #Prediction\n    y_pred=model.predict(X_test[features])\n    \n    #For each rmse score obtained, add to the score_storage list \n    score = np.sqrt(mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}