{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook I want to share init_model parameter of a lightgbm model. Since we compete in 4th and even 5th decimals in this competition, we need to be able to squeeze from the data as much as we can. So I wanted to share this trick, which I also used for the first time."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-jan-2021/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-jan-2021/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_features = [col for col in train.columns if col.startswith(\"cont\")]\nlen(cont_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X = X.abs()\ny = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    X_train = X_train.abs()\n\n    \n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 200,\n              \"lambda_l1\":2,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.01,\n              'min_child_samples': 50,\n              \"bagging_fraction\":0.7,\n              \"bagging_freq\":1}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100\n                    )\n    \n        y_pred_list.append(model.predict(X_val[cont_features]))\n        test_preds.append(model.predict(test[cont_features]))\n        \n    \n   \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score_list)\nprint(np.mean(score_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we'll run 2nd models on top of 1st models with a lower learning rate thanks to lgbm init_model method."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    X_train = X_train.abs()\n\n    \n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 200,\n              \"lambda_l1\":2,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.01,\n              'min_child_samples': 50,\n              \"bagging_fraction\":0.7,\n              \"bagging_freq\":1}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100\n                    )\n        \n        # Extra boosting.\n        dtrain = lgbm.Dataset(X_train[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n        params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 300,\n              \"lambda_l1\":2,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.003,\n              'min_child_samples': 50,\n              \"bagging_fraction\":0.7,\n              \"bagging_freq\":1}\n\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                            dtrain,\n                            valid_sets=[dtrain, dvalid],\n                            verbose_eval=100,\n                            num_boost_round=1000,\n                           early_stopping_rounds=100,\n                           init_model = model\n                        )\n\n    \n    \n        y_pred_list.append(model.predict(X_val[cont_features]))\n        test_preds.append(model.predict(test[cont_features]))\n    \n   \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(score_list)\nprint(np.mean(score_list))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that results have improved.\n* 0.6957978243963224 -> 0.695761435448342\n* 3 points improvement in 5th decimal. Not so bad for this competition :)"},{"metadata":{},"cell_type":"markdown","source":"## What can be done more with this method?\n1. Now you can also try tuning your extra boosting model's parameters :)\n2. You can try stopping your first model earlier (without early stopping) and running extra boosting model longer. \n3. You can try 3rd extra boosting model on top of the 2nd.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}