{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install optuna","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport optuna\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import TransformerMixin\n\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на датасет."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Построим корреляционную матрицу, чтобы выкинуть сильно корреллирующие признаки."},{"metadata":{"trusted":true},"cell_type":"code","source":"figure, ax = plt.subplots(figsize=(16, 16))\nsns.heatmap(df.sample(n=1_000).corr(), annot=True, linewidths=.5, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Можно было бы выкинуть признак `cont6`, но так как признаков мало, оставим его.\n\nБудем использовать ансамбль из двух библиотек градиентного бустинга, `XGBoost` и `LightGBM`.\nОптимизируем гиперпараметры, используем библиотеку `optuna`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective_xgb(trial, data, target):\n    parameters = {\n        'tree_method': 'gpu_hist',\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008, 0.009, 0.01, 0.012, 0.014, 0.016, 0.018, 0.02]),\n        'n_estimators': 1000,\n        'max_depth': trial.suggest_categorical('max_depth', [5, 7, 9, 11, 13, 15, 17, 20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48, 2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    \n    # Пропускаем через кросс-валидацию, усредняем ошибку \n    folds = KFold(n_splits=5, random_state=1337, shuffle=True)\n    rmse = []\n    \n    for train_idx, test_idx in folds.split(data, target):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n    \n        model = xgb.XGBRegressor(**parameters)\n        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=100, verbose=False)\n    \n        rmse.append(mean_squared_error(y_test, model.predict(X_test), squared=False))\n    \n    print(f'Mean RMSE for all the folds: {np.mean(rmse)}')\n    \n    return np.mean(rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Считается долго и на GPU, так что выпишем ниже полученные оптимальные параметры."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nstudy_xgb = optuna.create_study(direction='minimize')\nstudy_xgb.optimize(objective_xgb, n_trials=50)\n\nprint(f'Number of finished trials: {len(study_xgb.trials)}')\nprint(f'Best trial: {study_xgb.best_trial.params}')\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_parameters = {\n    'objective': 'reg:squarederror',\n    'tree_method': 'gpu_hist',\n    'n_estimators': 1000,\n    'lambda': 7.610705234008646, \n    'alpha': 0.0019377246932580476, \n    'colsample_bytree': 0.5, \n    'subsample': 0.7, \n    'learning_rate': 0.012, \n    'max_depth': 20, \n    'random_state': 24, \n    'min_child_weight': 229\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Проделываем похожую процедуру с `LightGBM`."},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective_lgb(trial):\n    X, y = df.drop(columns=['target', 'id']).values, df['target'].values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1337)\n    \n    ds_train = lgb.Dataset(X_train, label=y_train)\n    ds_test = lgb.Dataset(X_test, label=y_test)\n   \n    parameters = {\n        'device_type': 'gpu',\n        'objective': 'regression',\n        'metric': 'rmse',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n\n    gbm = lgb.train(parameters, ds_train)\n    prediction = gbm.predict(X_test)\n    accuracy = mean_squared_error(y_test, prediction, squared=False)\n    \n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nstudy_lgb = optuna.create_study(direction='minimize')\nstudy_lgb.optimize(objective_lgb, n_trials=100)\n\nprint(f'Number of finished trials: {len(study_lgb.trials)}')\nprint(f'Best trial: {study_lgb.best_trial.params}')\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_parameters = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting': 'gbdt',\n    'lambda_l1': 3.2737454713243543e-07,\n    'lambda_l2': 3.685676983230042e-06,\n    'num_leaves': 190,\n    'feature_fraction': 0.47291296723211934,\n    'bagging_fraction': 0.8846579981793894,\n    'bagging_freq': 3,\n    'min_child_samples': 58,\n    'verbose': 0,\n    'device_type': 'gpu'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NonLinearTransformer(TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.drop(columns=['id'])\n    \n        for c in X.columns:\n            if c == 'target':\n                continue\n            X[f'{c}^2'] = X[c] ** 2\n            \n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_xgb = Pipeline([\n    ('custom', NonLinearTransformer()),\n    ('scaling', StandardScaler()),\n    ('regression', xgb.XGBRegressor(**xgb_parameters))\n])\n\npipe_lgb = Pipeline([\n    ('custom', NonLinearTransformer()),\n    ('scaling', StandardScaler()),\n    ('regression', lgb.LGBMRegressor(**lgb_parameters))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ndf_predict = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = df_train.drop(columns=['target']), df_train['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1337)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_xgb.fit(X_train, y_train)\npipe_lgb.fit(X_train, y_train)\n\nprint(f'XGB Score: {pipe_xgb.score(X_test, y_test)}, LGB Score: {pipe_lgb.score(X_test, y_test)}')\nprint(f'XGB RMSE: {mean_squared_error(y_test, pipe_xgb.predict(X_test), squared=False)}, LGB RMSE: {mean_squared_error(y_test, pipe_lgb.predict(X_test), squared=False)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ensemble_predict(X):\n    target_xgb = pipe_xgb.predict(X)\n    target_lgb = pipe_lgb.predict(X)\n\n    return [0.85 * x + 0.15 * l for (x, l) in zip(target_xgb, target_lgb)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Ensemble RMSE: {mean_squared_error(y_test, ensemble_predict(X_test), squared=False)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_xgb.fit(X, y)\npipe_lgb.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = pd.DataFrame({\n    'id': df_predict['id'], 'target': ensemble_predict(df_predict)\n})\ntarget.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}