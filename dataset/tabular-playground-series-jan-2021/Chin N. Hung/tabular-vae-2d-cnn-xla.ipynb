{"cells":[{"metadata":{},"cell_type":"markdown","source":"References:\n\nhttps://keras.io/examples/generative/vae/\n\nhttps://www.tensorflow.org/xla/tutorials/autoclustering_xla\n\nGoal:\n\n1) Expand 1D features to 2D\n\n2) VAE as front end preprocessing\n\na) New Latent features (not use in this case)\n\nb) VAE decoder output (this is used for 2D-CNN input)\n\n3) 2D-CNN\n\na) Evaluate XLA Enable/Disable speed"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport gc\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.regularizers import l2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n\nimport seaborn as sns\n        \ninput_path = Path('/kaggle/input/tabular-playground-series-jan-2021/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEBUG      = False     #True is debug mode\nVAE_OUTPUT = True     #True is Enable VAE, False is Disable VAE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(input_path / 'train.csv', index_col='id')\ndisplay(train.head(2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(input_path / 'test.csv', index_col='id')\ndisplay(test.head(2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(input_path / 'sample_submission.csv', index_col='id')\ndisplay(submission.head(2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape, test.shape, submission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.astype('float32') \ntest  = test.astype('float32')\n\nif DEBUG:\n    train_df = train[:20000].copy()\n    test_df  = test[:20000].copy()\nelse:\n    train_df = train.copy()\n    test_df  = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(nrows=3, ncols=5, figsize=(15, 12))\n\nk=1\nfor i in range(3):\n    for j in range(5):\n        if (k==15):\n            sns.distplot(train_df['target'], ax=ax[i, j])\n        else:\n            sns.distplot(train_df[f'cont{k}'].values, ax=ax[i, j])\n            ax[i, j].set_xlabel('cont'+str(k))\n        k +=1\n        \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train_df['target']\ny_test = target.mean() * np.ones(len(test_df))   \ndel train_df['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMG_SIZE = train_df.shape[1]\nIMG_SIZE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train_df.values\nx_test  = test_df.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Expand the dimension to 2D to get more degree of freedom\n#2D array = more like an estimate of covariance matrix on per-sample basis, diag elements = estimate variance, features square\n\ntrain_cov =   np.array([x_train]).T * [x_train]  \ntest_cov  =   np.array([x_test]).T * [x_test]\n\nnp.shape(train_cov), np.shape(test_cov)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cov = np.moveaxis(train_cov, 1, 0)        #target is its label\ntest_cov = np.moveaxis(test_cov, 1, 0)\ntrain_cov.shape, test_cov.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_cov = np.concatenate([train_cov, test_cov], axis=0)  \ntrain_test_cov = train_test_cov.reshape(-1,14,14,1)\n\nnp.shape(train_test_cov)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_df, test_df, train, test\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Sampling(layers.Layer):\n    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"latent_dim = 24\n\nencoder_inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 1))\nx = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\nx = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\nx = layers.Flatten()(x)\nx = layers.Dense(16, activation=\"relu\")(x)\nz_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\nz_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\nz = Sampling()([z_mean, z_log_var])\n\nencoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n\nencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"latent_inputs = keras.Input(shape=(latent_dim,))\nx = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)  \nx = layers.Reshape((7, 7, 64))(x)                               \nx = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)  \nx = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=1, padding=\"same\")(x)\ndecoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n\ndecoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n\ndecoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class VAE(keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def train_step(self, data):\n        if isinstance(data, tuple):\n            data = data[0]\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = encoder(data)\n            reconstruction = decoder(z)\n            reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(data, reconstruction))\n            reconstruction_loss *= (IMG_SIZE * IMG_SIZE)\n            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n            kl_loss = tf.reduce_mean(kl_loss)\n            kl_loss *= -0.5\n            total_loss = reconstruction_loss + kl_loss\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        return {\n            \"loss\": total_loss,\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss,\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compare_images(encoder,decoder, test_sample):\n    z_mean, z_log_var, z = encoder(test_sample)\n    predictions = decoder(z)                  #logits output = batch,14,14,1\n\n    #Plot 2D features covariance per sample\n    k = 0 \n    fig, ax = plt.subplots(nrows=2, ncols=8, figsize=(28, 8))\n    fig.subplots_adjust(hspace = .5)\n\n    for i in range(1):\n        for j in range(8):\n            ax[i,j].imshow(tf.squeeze(test_sample[k]) , cmap = \"gray\")\n            ax[i,j].set_title('ID'+str(j))\n            ax[i,j].set_xlabel('actual')\n            ax[i,j].set_ylabel('covariance')\n            \n            ax[i+1,j].imshow(tf.squeeze(predictions[k]) , cmap = \"gray\")\n            ax[i+1,j].set_title('ID'+str(j))\n            ax[i+1,j].set_xlabel('predict')\n            ax[i+1,j].set_ylabel('covariance')\n            k += 1\n    \n    #Plot 1D, Diag elements are the magnitude square of the features\n    fig, ax = plt.subplots(nrows=4, ncols=2, figsize=(28, 12))\n    fig.subplots_adjust(hspace = .5)\n\n    for i in range(4):\n        ax[i,0].plot(tf.linalg.diag_part(tf.squeeze(test_sample[2*i])),'b.-',label='actual')\n        ax[i,0].plot(tf.linalg.diag_part(tf.squeeze(predictions[2*i])),'r^-',label='predict')\n        ax[i,0].legend()\n        ax[i,0].set_title('ID'+str(2*i))\n        ax[i,0].set_ylabel('features square')\n        \n        ax[i,1].plot(tf.linalg.diag_part(tf.squeeze(test_sample[2*i+1])),'b.-',label='actual')\n        ax[i,1].plot(tf.linalg.diag_part(tf.squeeze(predictions[2*i+1])),'r^-',label='predict')\n        ax[i,1].legend()\n        ax[i,1].set_title('ID'+str(2*i+1))\n        ax[i,1].set_ylabel('features square')  \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vae = VAE(encoder, decoder)\nvae.compile(optimizer=keras.optimizers.Adam())\n\nif VAE_OUTPUT:\n    hist = vae.fit(train_test_cov, \n                   epochs=8, \n                   batch_size=128)\n    \n    test_sample = train_test_cov[:8]\n   \n    compare_images(encoder,decoder, test_sample)\n    \ndel train_test_cov    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cov = train_cov.reshape(-1,14,14,1)\ntest_cov = test_cov.reshape(-1,14,14,1)\n\nif VAE_OUTPUT:\n   \n    BATCH_SIZE = 10000\n    EPOCH_STEPS = len(train_cov)//BATCH_SIZE\n    VAL_STEPS = len(test_cov)//BATCH_SIZE\n\n    train_ds = (tf.data.Dataset\n                .from_tensor_slices((train_cov, target))\n                .batch(BATCH_SIZE) ) \n\n    test_ds = (tf.data.Dataset\n                .from_tensor_slices((test_cov, y_test))\n                .batch(BATCH_SIZE)) \n    \n    #Need to predict on per minibatch to avoid OOM:ERROR\n    x_train = []\n    x_test = []\n    \n    #x_train VAE decoder output\n    for x in list( train_ds.take(EPOCH_STEPS).as_numpy_iterator()):\n        x = x[0]     #(batch, 14, 14, 1)\n        _, _, z = encoder(x) \n        x_train.append(decoder(z).numpy())\n        \n    #x_test VAE decoder output\n    for x in list(test_ds.take(EPOCH_STEPS).as_numpy_iterator()):\n        x = x[0]     #(batch, 14, 14, 1)\n        _, _, z = encoder(x)\n        x_test.append(decoder(z).numpy())   #x_test(EPOCH_STEPS,batch,14,14,1) <-- logits output(batch,14,14,1) \n    \n    x_train = np.asarray(x_train).reshape(-1,14,14,1)\n    x_test = np.asarray(x_test).reshape(-1,14,14,1)\n    del z,x,train_ds,test_ds\n    \nelse:    \n    x_train = train_cov\n    x_test = test_cov\n    del train_cov,test_cov\n    \ngc.collect() \n\nx_train.shape, target.shape, x_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing XLA Enable/Disable"},{"metadata":{"trusted":true},"cell_type":"code","source":"assert(tf.test.gpu_device_name())\ntf.keras.backend.clear_session()\ntf.config.optimizer.set_jit(False) # Start with XLA disabled.\n\n\nx_train, x_val, y_train, y_val = train_test_split(x_train, target, train_size=0.60)\n\nx_train.shape, y_train.shape, x_val.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l2_reg = 5e-4  # Regularization rate for l2\n\ndef generate_model():\n    return tf.keras.models.Sequential([\n            tf.keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]),\n            #tf.keras.layers.Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:], kernel_regularizer=l2(l2_reg)),    \n            tf.keras.layers.Activation('relu'),\n            tf.keras.layers.Conv2D(32, (3, 3)),\n            #tf.keras.layers.Conv2D(32, (3, 3), kernel_regularizer=l2(l2_reg)),  \n            tf.keras.layers.Activation('relu'),\n            #tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n            #tf.keras.layers.Dropout(0.25),\n\n            tf.keras.layers.Conv2D(64, (3, 3), padding='same'),\n            #tf.keras.layers.Conv2D(64, (3, 3), padding='same', kernel_regularizer=l2(l2_reg)),  \n            tf.keras.layers.Activation('relu'),\n            #tf.keras.layers.Conv2D(64, (3, 3)),\n            #tf.keras.layers.Activation('relu'),\n            tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n            tf.keras.layers.Dropout(0.25),\n\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(512),\n            tf.keras.layers.Activation('relu'),\n            #tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(1),\n            tf.keras.layers.Activation('linear')\n          ])\n\nmodel = generate_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compile_model(model):\n    opt = tf.keras.optimizers.Adam(lr=0.0001,  beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n    model.compile(loss='mse', optimizer=opt, metrics=[tf.keras.metrics.MeanAbsoluteError()])\n    return model\n\nmodel = compile_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = keras.callbacks.EarlyStopping(\n        patience=10,\n        min_delta=0.001,\n        restore_best_weights=True,\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(model, x_train, y_train, x_val, y_val, epochs=20):\n    model.fit(x_train, y_train, \n              batch_size=256, \n              epochs=epochs, \n              validation_data=(x_val, y_val),\n              callbacks=[early_stopping],\n              shuffle=True)\n\ndef warmup(model, x_train, y_train, x_val, y_val):\n    # Warm up the JIT, we do not wish to measure the compilation time.\n    initial_weights = model.get_weights()\n    train_model(model, x_train, y_train, x_val, y_val, epochs=1)\n    model.set_weights(initial_weights)\n\nwarmup(model, x_train, y_train, x_val, y_val)\n\n%time train_model(model, x_train, y_train, x_val, y_val)\n\nscores = model.evaluate(x_val, y_val, verbose=1)\n\nprint('Val loss:', scores[0])\nprint('Val mae:', scores[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate evaluation metric: Root Mean Squared Error (RMSE)\ny_val_pred = model.predict(x_val)\nscore_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\nprint(f\"RMSE: {score_rmse:.5f}\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to clear the session to enable JIT in the middle of the program.\n\ntf.keras.backend.clear_session()\ntf.config.optimizer.set_jit(True) # Enable XLA.\n\nmodel = compile_model(generate_model())\nwarmup(model, x_train, y_train, x_val, y_val)\n%time train_model(model, x_train, y_train, x_val, y_val)\n\nscores = model.evaluate(x_val, y_val, verbose=1)\nprint('Val loss:', scores[0])\nprint('Val mae:', scores[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_results(name, y, yhat, num_to_plot=10000, lims=(0,12), figsize=(6,6)):\n    plt.figure(figsize=figsize)\n    score = mean_squared_error(y, yhat, squared=False)\n    plt.scatter(y[:num_to_plot], yhat[:num_to_plot])\n    plt.plot(lims, lims)\n    plt.ylim(lims)\n    plt.xlim(lims)\n    plt.title(f'{name}: {score:0.5f}', fontsize=18)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate evaluation metric: Root Mean Squared Error (RMSE)\ny_val_pred = model.predict(x_val)\nscore_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\nprint(f\"RMSE: {score_rmse:.5f}\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" plot_results('2D-CNN', y_val, y_val_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not DEBUG:\n    submission['target'] = model.predict(x_test)\n    submission.to_csv('2DCNN.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusions:\n\n1) VAE KL-Loss need some tuning\n\n2) For 20 Epochs, Enable XLA speed is about 6 secs faster than Disable.  Not to much of improvement."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}