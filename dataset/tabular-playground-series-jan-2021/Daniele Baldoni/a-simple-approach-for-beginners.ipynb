{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<h1 style='background:#26A2AB; border:0; color:black'><center>Our job will be structured in this way:</center></h1> \n\n\n\n1) First of all let's read the datasets of training and test and visualize it to have in a glance the first idea of what our datasets consists in.\n\n2) Make some data cleaning searching if we have outliers and missing values and than make some feature engineering if we found something strange.\n\n3) Using some visualizations to search some further insight that we couldn't see before just reading the data.\n\n4) Preparing the data for the model with scaling or normalization and splitting it in train and validation set\n\n5) Apply some regression beginnning from simple linear regression understanding the general interpretability of the model and using after that some regularizations\n\n6) Try other kind of models different from linear Regression and choose which one could be the best to get a better interpretation of the data for this case\n","metadata":{}},{"cell_type":"markdown","source":"READ THE DATA\n---","metadata":{}},{"cell_type":"code","source":"train=pd.read_csv(\"../input/tabular-playground-series-jan-2021/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:31.450856Z","iopub.execute_input":"2022-06-16T17:57:31.451404Z","iopub.status.idle":"2022-06-16T17:57:32.187523Z","shell.execute_reply.started":"2022-06-16T17:57:31.451366Z","shell.execute_reply":"2022-06-16T17:57:32.18673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:32.188983Z","iopub.execute_input":"2022-06-16T17:57:32.189251Z","iopub.status.idle":"2022-06-16T17:57:32.208113Z","shell.execute_reply.started":"2022-06-16T17:57:32.189215Z","shell.execute_reply":"2022-06-16T17:57:32.207254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=pd.read_csv(\"../input/tabular-playground-series-jan-2021/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:32.210889Z","iopub.execute_input":"2022-06-16T17:57:32.211202Z","iopub.status.idle":"2022-06-16T17:57:32.691084Z","shell.execute_reply.started":"2022-06-16T17:57:32.211127Z","shell.execute_reply":"2022-06-16T17:57:32.690329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:32.693131Z","iopub.execute_input":"2022-06-16T17:57:32.693718Z","iopub.status.idle":"2022-06-16T17:57:32.710575Z","shell.execute_reply.started":"2022-06-16T17:57:32.693674Z","shell.execute_reply":"2022-06-16T17:57:32.709859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.read_csv(\"/kaggle/input/tabular-playground-series-jan-2021/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:32.711785Z","iopub.execute_input":"2022-06-16T17:57:32.712194Z","iopub.status.idle":"2022-06-16T17:57:32.758698Z","shell.execute_reply.started":"2022-06-16T17:57:32.712157Z","shell.execute_reply":"2022-06-16T17:57:32.758003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:32.759869Z","iopub.execute_input":"2022-06-16T17:57:32.760209Z","iopub.status.idle":"2022-06-16T17:57:32.773851Z","shell.execute_reply.started":"2022-06-16T17:57:32.760171Z","shell.execute_reply":"2022-06-16T17:57:32.773185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I want to see the general information of both train and test data\n---","metadata":{}},{"cell_type":"code","source":"train.shape,test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:32.774918Z","iopub.execute_input":"2022-06-16T17:57:32.775561Z","iopub.status.idle":"2022-06-16T17:57:32.780967Z","shell.execute_reply.started":"2022-06-16T17:57:32.775525Z","shell.execute_reply":"2022-06-16T17:57:32.780173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:32.78476Z","iopub.execute_input":"2022-06-16T17:57:32.78525Z","iopub.status.idle":"2022-06-16T17:57:32.978795Z","shell.execute_reply.started":"2022-06-16T17:57:32.785142Z","shell.execute_reply":"2022-06-16T17:57:32.97809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:32.980219Z","iopub.execute_input":"2022-06-16T17:57:32.980743Z","iopub.status.idle":"2022-06-16T17:57:33.117011Z","shell.execute_reply.started":"2022-06-16T17:57:32.980703Z","shell.execute_reply":"2022-06-16T17:57:33.116124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#We can see in the previous table that the distributions of the variables for training and testing set are quite similar\na=train.describe()\nb=test.describe()\na-b\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:33.118545Z","iopub.execute_input":"2022-06-16T17:57:33.118811Z","iopub.status.idle":"2022-06-16T17:57:33.431724Z","shell.execute_reply.started":"2022-06-16T17:57:33.118776Z","shell.execute_reply":"2022-06-16T17:57:33.431008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It was just to have a glance and verify that the distributions of our features are quite similar for the 2 datasets","metadata":{}},{"cell_type":"code","source":"train.shape,test.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:33.432886Z","iopub.execute_input":"2022-06-16T17:57:33.433511Z","iopub.status.idle":"2022-06-16T17:57:33.439744Z","shell.execute_reply.started":"2022-06-16T17:57:33.433469Z","shell.execute_reply":"2022-06-16T17:57:33.438972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our datasets are composed of 30k records for the train that we will use to train our final model and 20k records withouth the target variable for the test dataset. Our task for this competition is just to predict the value for the target variable.\nThat's why the number of column is different. The only differene is in the variable to predict.","metadata":{}},{"cell_type":"markdown","source":"EXPLORATORY DATA ANALYSIS\n===","metadata":{}},{"cell_type":"markdown","source":"we have different ooerations to do before to start:\n\n- check the missing values\n- find the outliers\n- study the correlations and drop unuseful variables\n- some visualizations\n- make some transformations if needed like changing.\n\n","metadata":{}},{"cell_type":"markdown","source":"Check of the missing Values\n---\nWe have no missing values for both the datasets. \nThe cell below show the code to visualize the number of null values in the different columns of the 2 datasets.","metadata":{}},{"cell_type":"markdown","source":"We have no missing values for both the datasets","metadata":{}},{"cell_type":"code","source":"train.isnull().sum(),test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:33.440933Z","iopub.execute_input":"2022-06-16T17:57:33.441543Z","iopub.status.idle":"2022-06-16T17:57:33.470829Z","shell.execute_reply.started":"2022-06-16T17:57:33.441482Z","shell.execute_reply":"2022-06-16T17:57:33.470112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A first blick to the target variable\n===\n\nLet's see the distribution of the target variable. As we can imagine the nature is numeric continuous and the distribution seems to be bell shaped. We can see the distribution in form of histograms.\nWhile this variable is not continuous the histogram could be misleading aniway.","metadata":{}},{"cell_type":"code","source":"train.target.hist()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:33.472053Z","iopub.execute_input":"2022-06-16T17:57:33.472824Z","iopub.status.idle":"2022-06-16T17:57:33.679824Z","shell.execute_reply.started":"2022-06-16T17:57:33.472785Z","shell.execute_reply":"2022-06-16T17:57:33.679142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we increase the number of bins we can realize that we have a bimodal distribution and it's not really well shaped. \nWe could use some specific test to evaluate the normality of the variable and i'll leave some code later about  how to verify it.","metadata":{}},{"cell_type":"code","source":"train.target.hist(bins =20)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:33.681103Z","iopub.execute_input":"2022-06-16T17:57:33.681584Z","iopub.status.idle":"2022-06-16T17:57:33.894842Z","shell.execute_reply.started":"2022-06-16T17:57:33.681544Z","shell.execute_reply":"2022-06-16T17:57:33.894168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the nest command we can have a better idea using the distplot of which is the real shape of the distribution.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.distplot(train.target)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:33.896078Z","iopub.execute_input":"2022-06-16T17:57:33.896747Z","iopub.status.idle":"2022-06-16T17:57:35.175779Z","shell.execute_reply.started":"2022-06-16T17:57:33.896706Z","shell.execute_reply":"2022-06-16T17:57:35.175081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the normality of the curve\n===\nWe want to see if the target variable is normally distributed and we can find inspiratio from this link triyng some useful tests of normality:\nhttps://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/","metadata":{}},{"cell_type":"code","source":"from scipy import stats\na=train.target\na=np.array(a)\nk2,p=stats.normaltest(a)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:35.176932Z","iopub.execute_input":"2022-06-16T17:57:35.177573Z","iopub.status.idle":"2022-06-16T17:57:35.190714Z","shell.execute_reply.started":"2022-06-16T17:57:35.177531Z","shell.execute_reply":"2022-06-16T17:57:35.190034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpha = 1e-3\nprint(\"p = {:g}\".format(p))\nif p < alpha:  # null hypothesis: x comes from a normal distribution\n    print(\"The null hypothesis can be rejected\")\nelse:\n    print(\"The null hypothesis cannot be rejected\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:35.192136Z","iopub.execute_input":"2022-06-16T17:57:35.192439Z","iopub.status.idle":"2022-06-16T17:57:35.198586Z","shell.execute_reply.started":"2022-06-16T17:57:35.192394Z","shell.execute_reply":"2022-06-16T17:57:35.197784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"you can see more about this test (scipy.normaltest) on this link:\n\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html","metadata":{}},{"cell_type":"markdown","source":"OUTLIERS\n===\n\nTo detect the outliers the best strategy is using the boxplots","metadata":{}},{"cell_type":"code","source":"plt.boxplot(train.target)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:35.20029Z","iopub.execute_input":"2022-06-16T17:57:35.200835Z","iopub.status.idle":"2022-06-16T17:57:35.33762Z","shell.execute_reply.started":"2022-06-16T17:57:35.200797Z","shell.execute_reply":"2022-06-16T17:57:35.336882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.target.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:35.339157Z","iopub.execute_input":"2022-06-16T17:57:35.339754Z","iopub.status.idle":"2022-06-16T17:57:35.365153Z","shell.execute_reply.started":"2022-06-16T17:57:35.339715Z","shell.execute_reply":"2022-06-16T17:57:35.364497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We didn't see before some ourliers in the distribution. There is a suspicious value of 0 for the target value that is probably an outlier. Let's check it out below the sorted extreme values.\nWe could in case cut this observation from the train data if we think it can affect our final prediction, just we should try to understand if is an aberrant value or a real outlie. The same with the extreme  values of the distribution. \nThe dataset is aniway enough large so it shouldn't be a problem to cut the zero value and some extreme observations.","metadata":{}},{"cell_type":"code","source":"train.target.sort_values()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:35.366565Z","iopub.execute_input":"2022-06-16T17:57:35.367037Z","iopub.status.idle":"2022-06-16T17:57:35.416159Z","shell.execute_reply.started":"2022-06-16T17:57:35.367Z","shell.execute_reply":"2022-06-16T17:57:35.415506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dropping the outlier for the target variable\n---","metadata":{}},{"cell_type":"code","source":"train.drop(index=170514,inplace=True)\ntrain.target.min()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:35.417457Z","iopub.execute_input":"2022-06-16T17:57:35.417702Z","iopub.status.idle":"2022-06-16T17:57:35.447612Z","shell.execute_reply.started":"2022-06-16T17:57:35.41767Z","shell.execute_reply":"2022-06-16T17:57:35.446741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Outliers for the predictors\n---\nNow we can search the outliers in the the other variables.\nWith the block of code below we can simoultaneuosly control it for both the datasets and all the feature except the target variable.","metadata":{}},{"cell_type":"code","source":"for col in train.columns[:-1]:\n    plt.boxplot([train[col],test[col]], labels=['train', 'test'])\n    plt.title(col)\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:35.449324Z","iopub.execute_input":"2022-06-16T17:57:35.449609Z","iopub.status.idle":"2022-06-16T17:57:38.430404Z","shell.execute_reply.started":"2022-06-16T17:57:35.449573Z","shell.execute_reply":"2022-06-16T17:57:38.42954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this outliers treatment i took inspiration by the work of Gaurav Rajesh Sahani:\nhttps://www.kaggle.com/code/gauravsahani/detailed-eda-with-lightgbm-score-0-698","metadata":{}},{"cell_type":"code","source":"Q1_train = train.quantile(0.25)\nQ3_train = train.quantile(0.75)\nIQR_train = train - Q1_train\n((train < train - 1.5*IQR_train) | (train > Q3_train + 1.5*IQR_train)).agg([sum, 'mean'])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:38.435507Z","iopub.execute_input":"2022-06-16T17:57:38.435707Z","iopub.status.idle":"2022-06-16T17:57:38.737825Z","shell.execute_reply.started":"2022-06-16T17:57:38.435682Z","shell.execute_reply":"2022-06-16T17:57:38.737124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****We can find outliers for the variables cont7 and cont9 as well as we saw for the target variable. One possible strategy could be to replace it with the median or the 1st quartile of the distribution or triyng to estimate it with a model of regression. For the moment we keep it invariate. Indeed the outliers when present are both in the train and in the test data. If i had outliers just in training data i'd drop that rows but it's not the case.","metadata":{}},{"cell_type":"code","source":"Q1_test = test.quantile(0.25)\nQ3_test = test.quantile(0.75)\nIQR_test = Q3_test - Q1_test\n((test < Q1_test - 1.5 * IQR_test) | (test > Q3_test + 1.5 * IQR_test)).agg([sum, 'mean'])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:38.739096Z","iopub.execute_input":"2022-06-16T17:57:38.739929Z","iopub.status.idle":"2022-06-16T17:57:38.884837Z","shell.execute_reply.started":"2022-06-16T17:57:38.73989Z","shell.execute_reply":"2022-06-16T17:57:38.884134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create a function to remove the outliers:","metadata":{}},{"cell_type":"code","source":"def replace_outliers(data):\n    for col in data.columns:\n        Q1 = data[col].quantile(0.25)\n        Q3 = data[col].quantile(0.75)\n        IQR = Q3 - Q1\n        median_ = data[col].median()\n        data.loc[((data[col] < Q1 - 1.5*IQR) | (data[col] > Q3 + 1.5*IQR)), col] = median_\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:38.886326Z","iopub.execute_input":"2022-06-16T17:57:38.886756Z","iopub.status.idle":"2022-06-16T17:57:38.893706Z","shell.execute_reply.started":"2022-06-16T17:57:38.886709Z","shell.execute_reply":"2022-06-16T17:57:38.892953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train=replace_outliers(train)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:38.895126Z","iopub.execute_input":"2022-06-16T17:57:38.895966Z","iopub.status.idle":"2022-06-16T17:57:39.149561Z","shell.execute_reply.started":"2022-06-16T17:57:38.895875Z","shell.execute_reply":"2022-06-16T17:57:39.148797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test=replace_outliers(test)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:39.150875Z","iopub.execute_input":"2022-06-16T17:57:39.15113Z","iopub.status.idle":"2022-06-16T17:57:39.310448Z","shell.execute_reply.started":"2022-06-16T17:57:39.151093Z","shell.execute_reply":"2022-06-16T17:57:39.309695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Multivariate analysis of the dataset\n===\nLet's start from t the correlations between the features.\nWe will check it by correlation matrix and heatmap for the train dataset.\nWe could visualize it with a simply line of comman (train.corr()) but if we want a more intuitive version of the correlation matrix we can add some detail about the style.","metadata":{}},{"cell_type":"code","source":"corr=train.corr()\ncorr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:39.311845Z","iopub.execute_input":"2022-06-16T17:57:39.312118Z","iopub.status.idle":"2022-06-16T17:57:39.565264Z","shell.execute_reply.started":"2022-06-16T17:57:39.312082Z","shell.execute_reply":"2022-06-16T17:57:39.564487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can cat the correlation matrix inside a heatmap with the use of masks","metadata":{}},{"cell_type":"markdown","source":"The first thing to look is the last row of the table. WE can observe any particularly strong correlation between thetarget variable and the other while we can observe many high correlations between the other continuous variable that can lead to a problem of multicollinearity in the data.\nIn the next cell we execute the command to highlights the correlations with the use of the heatmap","metadata":{}},{"cell_type":"code","source":"#we create a mask to visualize values just once\nplt.figure(figsize=(14,8))\nmask=np.zeros_like(corr)\nmask[np.triu_indices_from(mask,1)]=True\n\nplt.subplots(figsize=(15,12))\nsns.heatmap(train.corr(),mask=mask,vmax=0.9,cmap=\"viridis\",annot=True, square =True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:39.566445Z","iopub.execute_input":"2022-06-16T17:57:39.567188Z","iopub.status.idle":"2022-06-16T17:57:40.69956Z","shell.execute_reply.started":"2022-06-16T17:57:39.567146Z","shell.execute_reply":"2022-06-16T17:57:40.698681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This table show what we note before about the lack of strong correlation between target variable and the predictors","metadata":{}},{"cell_type":"code","source":"train.corr()[15:16].T","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:40.703234Z","iopub.execute_input":"2022-06-16T17:57:40.703518Z","iopub.status.idle":"2022-06-16T17:57:41.033844Z","shell.execute_reply.started":"2022-06-16T17:57:40.703483Z","shell.execute_reply":"2022-06-16T17:57:41.033139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split,KFold,cross_val_score\n\nX=train.drop([\"id\"],axis=1)\ny=train.target","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:41.035221Z","iopub.execute_input":"2022-06-16T17:57:41.035664Z","iopub.status.idle":"2022-06-16T17:57:41.048356Z","shell.execute_reply.started":"2022-06-16T17:57:41.035624Z","shell.execute_reply":"2022-06-16T17:57:41.047625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I can drop the variable id that is not particularly significant in this case","metadata":{}},{"cell_type":"code","source":"train=train.drop(\"id\",axis =1)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:41.049724Z","iopub.execute_input":"2022-06-16T17:57:41.049996Z","iopub.status.idle":"2022-06-16T17:57:41.068716Z","shell.execute_reply.started":"2022-06-16T17:57:41.049956Z","shell.execute_reply":"2022-06-16T17:57:41.067843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some visualizations about the single variables\n===","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\ntrain.hist(bins=10,figsize=(10,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:41.071417Z","iopub.execute_input":"2022-06-16T17:57:41.071927Z","iopub.status.idle":"2022-06-16T17:57:42.493812Z","shell.execute_reply.started":"2022-06-16T17:57:41.071879Z","shell.execute_reply":"2022-06-16T17:57:42.493118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.skew()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.495044Z","iopub.execute_input":"2022-06-16T17:57:42.497572Z","iopub.status.idle":"2022-06-16T17:57:42.57886Z","shell.execute_reply.started":"2022-06-16T17:57:42.497528Z","shell.execute_reply":"2022-06-16T17:57:42.578149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Many variables has problem of skewness. We could make some transformation as well to solve this problem if we find it necessary. In the previous cell of describe method we didn't find particularly problem about the different magnitude of the features so we should scale the data but in this case the situations shouldn't change too much. ","metadata":{}},{"cell_type":"markdown","source":"Pairplot\n---","metadata":{}},{"cell_type":"code","source":"#sns.pairplot(train)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.580158Z","iopub.execute_input":"2022-06-16T17:57:42.580608Z","iopub.status.idle":"2022-06-16T17:57:42.584635Z","shell.execute_reply.started":"2022-06-16T17:57:42.580567Z","shell.execute_reply":"2022-06-16T17:57:42.583888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the dataset for the validation\n===","metadata":{}},{"cell_type":"markdown","source":"i want to create a categorical variable for the target with the aim to create a split of the dataset who proportionally represent the dataset","metadata":{}},{"cell_type":"code","source":"X.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.58593Z","iopub.execute_input":"2022-06-16T17:57:42.586606Z","iopub.status.idle":"2022-06-16T17:57:42.609911Z","shell.execute_reply.started":"2022-06-16T17:57:42.586565Z","shell.execute_reply":"2022-06-16T17:57:42.608871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=X.drop(\"target\",axis=1)\nX","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.611548Z","iopub.execute_input":"2022-06-16T17:57:42.611883Z","iopub.status.idle":"2022-06-16T17:57:42.645041Z","shell.execute_reply.started":"2022-06-16T17:57:42.611846Z","shell.execute_reply":"2022-06-16T17:57:42.64422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n#grr=pd.plotting.scatter_matrix(X[0:80],c=y[0:80],figsize=(15,10))\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.646601Z","iopub.execute_input":"2022-06-16T17:57:42.646869Z","iopub.status.idle":"2022-06-16T17:57:42.653823Z","shell.execute_reply.started":"2022-06-16T17:57:42.646832Z","shell.execute_reply":"2022-06-16T17:57:42.652949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[\"cont5_13\"]=X[\"cont5\"]*X[\"cont13\"]\nX[\"cont7_10\"]=X[\"cont7\"]*X[\"cont10\"]\nX[\"cont1_5\"]=X[\"cont1\"]*X[\"cont5\"]\n\ntest[\"cont5_13\"]=test[\"cont5\"]*test[\"cont13\"]\ntest[\"cont7_10\"]=test[\"cont7\"]*test[\"cont10\"]\ntest[\"cont1_5\"]=test[\"cont1\"]*test[\"cont5\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.655663Z","iopub.execute_input":"2022-06-16T17:57:42.656551Z","iopub.status.idle":"2022-06-16T17:57:42.672772Z","shell.execute_reply.started":"2022-06-16T17:57:42.656504Z","shell.execute_reply":"2022-06-16T17:57:42.672103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score, train_test_split,GridSearchCV\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.675103Z","iopub.execute_input":"2022-06-16T17:57:42.675607Z","iopub.status.idle":"2022-06-16T17:57:42.67972Z","shell.execute_reply.started":"2022-06-16T17:57:42.675569Z","shell.execute_reply":"2022-06-16T17:57:42.678961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.3,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.681525Z","iopub.execute_input":"2022-06-16T17:57:42.682241Z","iopub.status.idle":"2022-06-16T17:57:42.776422Z","shell.execute_reply.started":"2022-06-16T17:57:42.682202Z","shell.execute_reply":"2022-06-16T17:57:42.775615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean for the 2 splitted vector of y are similar","metadata":{}},{"cell_type":"code","source":"y_train.mean(),y_val.mean()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.777958Z","iopub.execute_input":"2022-06-16T17:57:42.778225Z","iopub.status.idle":"2022-06-16T17:57:42.785491Z","shell.execute_reply.started":"2022-06-16T17:57:42.778188Z","shell.execute_reply":"2022-06-16T17:57:42.784541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.787587Z","iopub.execute_input":"2022-06-16T17:57:42.787909Z","iopub.status.idle":"2022-06-16T17:57:42.807641Z","shell.execute_reply.started":"2022-06-16T17:57:42.78787Z","shell.execute_reply":"2022-06-16T17:57:42.806817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.809188Z","iopub.execute_input":"2022-06-16T17:57:42.809497Z","iopub.status.idle":"2022-06-16T17:57:42.823888Z","shell.execute_reply.started":"2022-06-16T17:57:42.809458Z","shell.execute_reply":"2022-06-16T17:57:42.822967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet, Lasso,Ridge\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.825411Z","iopub.execute_input":"2022-06-16T17:57:42.825668Z","iopub.status.idle":"2022-06-16T17:57:42.830612Z","shell.execute_reply.started":"2022-06-16T17:57:42.825633Z","shell.execute_reply":"2022-06-16T17:57:42.829831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a function to evaluate the cross-validation score","metadata":{}},{"cell_type":"code","source":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.832494Z","iopub.execute_input":"2022-06-16T17:57:42.833275Z","iopub.status.idle":"2022-06-16T17:57:42.840175Z","shell.execute_reply.started":"2022-06-16T17:57:42.83323Z","shell.execute_reply":"2022-06-16T17:57:42.83939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_model=LinearRegression()\nlr_model.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.841965Z","iopub.execute_input":"2022-06-16T17:57:42.842682Z","iopub.status.idle":"2022-06-16T17:57:42.942831Z","shell.execute_reply.started":"2022-06-16T17:57:42.842639Z","shell.execute_reply":"2022-06-16T17:57:42.94204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the command below we create a function that can help us to have an idea of the interpretation of the Linear Regression model and we will understand its utility soon.","metadata":{}},{"cell_type":"code","source":"def linearRegressionSummary(model, column_names):\n    '''Show a summary of the trained linear regression model'''\n\n    # Plot the coeffients as bars\n    fig = plt.figure(figsize=(8,len(column_names)/3))\n    fig.suptitle('Linear Regression Coefficients', fontsize=16)\n    rects = plt.barh(column_names, model.coef_,color=\"lightblue\")\n\n    # Annotate the bars with the coefficient values\n    for rect in rects:\n        width = round(rect.get_width(),4)\n        plt.gca().annotate('  {}  '.format(width),\n                    xy=(0, rect.get_y()),\n                    xytext=(0,2),  \n                    textcoords=\"offset points\",  \n                    ha='left' if width<0 else 'right', va='bottom')        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.94449Z","iopub.execute_input":"2022-06-16T17:57:42.944986Z","iopub.status.idle":"2022-06-16T17:57:42.954428Z","shell.execute_reply.started":"2022-06-16T17:57:42.944946Z","shell.execute_reply":"2022-06-16T17:57:42.953613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linearRegressionSummary(lr_model,X.columns)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:42.956662Z","iopub.execute_input":"2022-06-16T17:57:42.957455Z","iopub.status.idle":"2022-06-16T17:57:43.268767Z","shell.execute_reply.started":"2022-06-16T17:57:42.957391Z","shell.execute_reply":"2022-06-16T17:57:43.268074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We just used the previous function written above to have a visualization of the impact of the predictors in the Regression model. The ones with the bar on the left affected our prediction in negative way decreasing the value of the target ( in particular cont10 while the ones on the right influence the result in positive way increasing the value of the target (expecially cont6).\nThe more the bar is far from the centre the more the predictor has an influence in our model.","metadata":{}},{"cell_type":"code","source":"print(rmsle_cv(lr_model))\nscore=rmsle_cv(lr_model)\nprint(\"\\nmodel score: {:.4f} {:.4f}\\n\".format(score.mean(),score.std()))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:43.27017Z","iopub.execute_input":"2022-06-16T17:57:43.270458Z","iopub.status.idle":"2022-06-16T17:57:44.771614Z","shell.execute_reply.started":"2022-06-16T17:57:43.27042Z","shell.execute_reply":"2022-06-16T17:57:44.768422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred=lr_model.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:44.773054Z","iopub.execute_input":"2022-06-16T17:57:44.773345Z","iopub.status.idle":"2022-06-16T17:57:44.78766Z","shell.execute_reply.started":"2022-06-16T17:57:44.773305Z","shell.execute_reply":"2022-06-16T17:57:44.786804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res_df = pd.DataFrame({\n    'Actual': y_val, \n    'Predicted':pred\n})\n\nres_df\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:44.788987Z","iopub.execute_input":"2022-06-16T17:57:44.789268Z","iopub.status.idle":"2022-06-16T17:57:44.819995Z","shell.execute_reply.started":"2022-06-16T17:57:44.789226Z","shell.execute_reply":"2022-06-16T17:57:44.819229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can have another view of our predictions with this cell of command","metadata":{}},{"cell_type":"code","source":"df1 = res_df.head(25)\ndf1.plot(kind='bar',figsize=(16,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:44.821218Z","iopub.execute_input":"2022-06-16T17:57:44.821468Z","iopub.status.idle":"2022-06-16T17:57:45.226987Z","shell.execute_reply.started":"2022-06-16T17:57:44.821435Z","shell.execute_reply":"2022-06-16T17:57:45.226321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntest=test.drop(\"id\",axis=1)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:45.229193Z","iopub.execute_input":"2022-06-16T17:57:45.229845Z","iopub.status.idle":"2022-06-16T17:57:45.276632Z","shell.execute_reply.started":"2022-06-16T17:57:45.229803Z","shell.execute_reply":"2022-06-16T17:57:45.275768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_model=LinearRegression()\nlr_model.fit(X,y)\npredictions=lr_model.predict(test)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:45.277975Z","iopub.execute_input":"2022-06-16T17:57:45.278315Z","iopub.status.idle":"2022-06-16T17:57:45.4388Z","shell.execute_reply.started":"2022-06-16T17:57:45.278266Z","shell.execute_reply":"2022-06-16T17:57:45.437697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:45.440425Z","iopub.execute_input":"2022-06-16T17:57:45.440677Z","iopub.status.idle":"2022-06-16T17:57:45.450567Z","shell.execute_reply.started":"2022-06-16T17:57:45.440643Z","shell.execute_reply":"2022-06-16T17:57:45.44975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(lr_model.intercept_)\nlr_model.coef_","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:45.45232Z","iopub.execute_input":"2022-06-16T17:57:45.453132Z","iopub.status.idle":"2022-06-16T17:57:45.464034Z","shell.execute_reply.started":"2022-06-16T17:57:45.453088Z","shell.execute_reply":"2022-06-16T17:57:45.463204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output=pd.DataFrame({\"id\":id,\"target\":predictions})\noutput\noutput.to_csv('sample_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:45.468587Z","iopub.execute_input":"2022-06-16T17:57:45.469358Z","iopub.status.idle":"2022-06-16T17:57:46.174258Z","shell.execute_reply.started":"2022-06-16T17:57:45.469317Z","shell.execute_reply":"2022-06-16T17:57:46.17346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.175513Z","iopub.execute_input":"2022-06-16T17:57:46.176301Z","iopub.status.idle":"2022-06-16T17:57:46.190324Z","shell.execute_reply.started":"2022-06-16T17:57:46.176241Z","shell.execute_reply":"2022-06-16T17:57:46.189606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1st little improving: Introducing scaling\n===","metadata":{}},{"cell_type":"code","source":"#Let's try to use scaling on the same model\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler,MinMaxScaler,StandardScaler\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.191752Z","iopub.execute_input":"2022-06-16T17:57:46.19224Z","iopub.status.idle":"2022-06-16T17:57:46.196406Z","shell.execute_reply.started":"2022-06-16T17:57:46.192202Z","shell.execute_reply":"2022-06-16T17:57:46.195495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs = RobustScaler()\nrs.fit(X_train)\nX_train_rs = pd.DataFrame(rs.transform(X_train), columns=X_train.columns, index=X_train.index)\nX_val_rs = pd.DataFrame(rs.transform(X_val), columns=X_val.columns, index=X_val.index)\nX_rs=pd.DataFrame(rs.transform(X), columns=X_val.columns, index=X.index)\n\nX_train.shape, X_train_rs.shape, X_val.shape, X_val_rs.shape,X_rs.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.197965Z","iopub.execute_input":"2022-06-16T17:57:46.198656Z","iopub.status.idle":"2022-06-16T17:57:46.382806Z","shell.execute_reply.started":"2022-06-16T17:57:46.198558Z","shell.execute_reply":"2022-06-16T17:57:46.381967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_rs.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.384954Z","iopub.execute_input":"2022-06-16T17:57:46.385223Z","iopub.status.idle":"2022-06-16T17:57:46.404756Z","shell.execute_reply.started":"2022-06-16T17:57:46.385185Z","shell.execute_reply":"2022-06-16T17:57:46.404059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.406263Z","iopub.execute_input":"2022-06-16T17:57:46.40679Z","iopub.status.idle":"2022-06-16T17:57:46.429908Z","shell.execute_reply.started":"2022-06-16T17:57:46.406749Z","shell.execute_reply":"2022-06-16T17:57:46.429171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val_rs.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.431024Z","iopub.execute_input":"2022-06-16T17:57:46.431344Z","iopub.status.idle":"2022-06-16T17:57:46.450415Z","shell.execute_reply.started":"2022-06-16T17:57:46.431308Z","shell.execute_reply":"2022-06-16T17:57:46.449503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.451828Z","iopub.execute_input":"2022-06-16T17:57:46.452458Z","iopub.status.idle":"2022-06-16T17:57:46.473793Z","shell.execute_reply.started":"2022-06-16T17:57:46.45242Z","shell.execute_reply":"2022-06-16T17:57:46.47317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Further ideas to improve the model\n===\n\nThere are some library that you could use to improve your model making some hyperparameter tuning like *optuna* for a specific model or trying to evaluate more model simoultaneously on the same dataset using the library *lazypredict*. \nfeel free to try this 2 options for this dataset and evaluate the results.\nfor the moment i will use a pipeline for some of the best models available for this kind of problem like lightgbm,xgboost and random forest in general","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\ndef get_model():\n    pipe = Pipeline( steps=[\n        (\"regressor\", None)\n    ])\n    return pipe","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.474796Z","iopub.execute_input":"2022-06-16T17:57:46.475437Z","iopub.status.idle":"2022-06-16T17:57:46.479824Z","shell.execute_reply.started":"2022-06-16T17:57:46.475399Z","shell.execute_reply":"2022-06-16T17:57:46.479123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"from sklearn.model_selection import  GridSearchCV\nparam_grid = {\n    'regressor': [\n        LinearRegression(),\n        Ridge(alpha=0.5, random_state=42),\n        Lasso(alpha=0.5, random_state=42),\n        RandomForestRegressor(max_depth=5, random_state=42)\n    ]\n}\n\nsearch = GridSearchCV(\n    get_model(), \n    param_grid, \n    cv=KFold(n_splits=5, shuffle=True, random_state=123), \n    scoring=\"neg_mean_squared_error\",\n    return_train_score=True, \n    verbose=3\n)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.481118Z","iopub.execute_input":"2022-06-16T17:57:46.481847Z","iopub.status.idle":"2022-06-16T17:57:46.489498Z","shell.execute_reply.started":"2022-06-16T17:57:46.481809Z","shell.execute_reply":"2022-06-16T17:57:46.488591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"%%time\nsearch.fit(X_rs, y)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.49776Z","iopub.execute_input":"2022-06-16T17:57:46.498413Z","iopub.status.idle":"2022-06-16T17:57:46.503895Z","shell.execute_reply.started":"2022-06-16T17:57:46.498375Z","shell.execute_reply":"2022-06-16T17:57:46.503044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.505197Z","iopub.execute_input":"2022-06-16T17:57:46.50589Z","iopub.status.idle":"2022-06-16T17:57:46.510319Z","shell.execute_reply.started":"2022-06-16T17:57:46.505853Z","shell.execute_reply":"2022-06-16T17:57:46.509552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result was:\nPipeline(steps=[('regressor',\n                 RandomForestRegressor(max_depth=5, random_state=42))])","metadata":{}},{"cell_type":"code","source":"#search.best_score_","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.511662Z","iopub.execute_input":"2022-06-16T17:57:46.512494Z","iopub.status.idle":"2022-06-16T17:57:46.517581Z","shell.execute_reply.started":"2022-06-16T17:57:46.512453Z","shell.execute_reply":"2022-06-16T17:57:46.516762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While the best result is obtained with the RandomForestRegressor i can decide to oveercome the problem of transformers because is not affected by problem of scaling so i'll apply on the X dataset. It doesn't mean that this is the best model in absolute... we could try something more extreeme like lightgbm as well , but it's the best between the four models tested for this problem.\nWe can still improve it using the GridSearchCV for the same model changing some parameters like the maximum depth or the number of leaf in the trees. ","metadata":{}},{"cell_type":"markdown","source":"RandomForestModel\n===","metadata":{}},{"cell_type":"code","source":"\"\"\"lasso=make_pipeline(MinMaxScaler(),Lasso(alpha=0.0001,random_state=42))\nprint(rmsle_cv(lasso))\nscore=rmsle_cv(lasso)\nprint(\"\\nmodel score: {:.4f} {:.4f}\\n\".format(score.mean(),score.std()))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.518863Z","iopub.execute_input":"2022-06-16T17:57:46.519706Z","iopub.status.idle":"2022-06-16T17:57:46.526974Z","shell.execute_reply.started":"2022-06-16T17:57:46.519664Z","shell.execute_reply":"2022-06-16T17:57:46.526055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nrf_model=RandomForestRegressor(max_depth=5, random_state=42)\nrf_model.fit(X,y)\npredictions=rf_model.predict(test)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.528485Z","iopub.execute_input":"2022-06-16T17:57:46.529437Z","iopub.status.idle":"2022-06-16T17:57:46.535553Z","shell.execute_reply.started":"2022-06-16T17:57:46.529394Z","shell.execute_reply":"2022-06-16T17:57:46.534627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf=RandomForestRegressor()\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.537275Z","iopub.execute_input":"2022-06-16T17:57:46.5378Z","iopub.status.idle":"2022-06-16T17:57:46.551403Z","shell.execute_reply.started":"2022-06-16T17:57:46.537764Z","shell.execute_reply":"2022-06-16T17:57:46.550683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output=pd.DataFrame({\"id\":id,\"target\":predictions})\noutput\noutput.to_csv('sample_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:46.552418Z","iopub.execute_input":"2022-06-16T17:57:46.553097Z","iopub.status.idle":"2022-06-16T17:57:47.20723Z","shell.execute_reply.started":"2022-06-16T17:57:46.55306Z","shell.execute_reply":"2022-06-16T17:57:47.206486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:47.208583Z","iopub.execute_input":"2022-06-16T17:57:47.209341Z","iopub.status.idle":"2022-06-16T17:57:47.224123Z","shell.execute_reply.started":"2022-06-16T17:57:47.209296Z","shell.execute_reply":"2022-06-16T17:57:47.223322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"XGBRegressor\n===","metadata":{}},{"cell_type":"code","source":"\"\"\"import xgboost\nfrom xgboost import XGBRegressor\nXGB=XGBRegressor(random_state=42)\nXGB.fit(X,y)\npredictions=XGB.predict(test)\npredictions1=predictions\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:47.225359Z","iopub.execute_input":"2022-06-16T17:57:47.226051Z","iopub.status.idle":"2022-06-16T17:57:47.231891Z","shell.execute_reply.started":"2022-06-16T17:57:47.226011Z","shell.execute_reply":"2022-06-16T17:57:47.231021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"output=pd.DataFrame({\"id\":id,\"target\":predictions})\noutput\noutput.to_csv('sample_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:47.233299Z","iopub.execute_input":"2022-06-16T17:57:47.234089Z","iopub.status.idle":"2022-06-16T17:57:47.24095Z","shell.execute_reply.started":"2022-06-16T17:57:47.234052Z","shell.execute_reply":"2022-06-16T17:57:47.240016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could try an Averaging model as well","metadata":{}},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:47.242368Z","iopub.execute_input":"2022-06-16T17:57:47.24327Z","iopub.status.idle":"2022-06-16T17:57:47.259677Z","shell.execute_reply.started":"2022-06-16T17:57:47.24323Z","shell.execute_reply":"2022-06-16T17:57:47.258955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:47.260793Z","iopub.execute_input":"2022-06-16T17:57:47.261413Z","iopub.status.idle":"2022-06-16T17:57:47.266977Z","shell.execute_reply.started":"2022-06-16T17:57:47.261374Z","shell.execute_reply":"2022-06-16T17:57:47.266251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridge_model=Ridge()\nridge_model.fit(X_train,y_train)\nrmsle_cv(ridge_model)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:47.268568Z","iopub.execute_input":"2022-06-16T17:57:47.269093Z","iopub.status.idle":"2022-06-16T17:57:47.625368Z","shell.execute_reply.started":"2022-06-16T17:57:47.269055Z","shell.execute_reply":"2022-06-16T17:57:47.624624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alphas = [ 1, 5, 10 ,12,13]\ncv_ridge = [rmsle_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:47.626885Z","iopub.execute_input":"2022-06-16T17:57:47.627431Z","iopub.status.idle":"2022-06-16T17:57:49.19492Z","shell.execute_reply.started":"2022-06-16T17:57:47.627375Z","shell.execute_reply":"2022-06-16T17:57:49.194011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_ridge","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:49.196601Z","iopub.execute_input":"2022-06-16T17:57:49.197227Z","iopub.status.idle":"2022-06-16T17:57:49.203485Z","shell.execute_reply.started":"2022-06-16T17:57:49.197177Z","shell.execute_reply":"2022-06-16T17:57:49.202656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_ridge=ridge_model.predict(test)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:49.205326Z","iopub.execute_input":"2022-06-16T17:57:49.206122Z","iopub.status.idle":"2022-06-16T17:57:49.222392Z","shell.execute_reply.started":"2022-06-16T17:57:49.206077Z","shell.execute_reply":"2022-06-16T17:57:49.221711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_ridge","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:49.223836Z","iopub.execute_input":"2022-06-16T17:57:49.224365Z","iopub.status.idle":"2022-06-16T17:57:49.233179Z","shell.execute_reply.started":"2022-06-16T17:57:49.224324Z","shell.execute_reply":"2022-06-16T17:57:49.23247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lightgbm\n===","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nLGB = lgb.LGBMRegressor(random_state=33, n_estimators=5000, min_data_per_group=5, boosting_type='gbdt',\n num_leaves=246, learning_rate=0.005, subsample_for_bin=200000,\n lambda_l1= 1.074622455507616e-05, lambda_l2= 2.0521330798729704e-06, n_jobs=-1, cat_smooth=1.0, \n importance_type='split', metric='rmse', min_child_samples=20, min_gain_to_split=0.0, feature_fraction=0.5, \n bagging_freq=6, min_sum_hessian_in_leaf=0.001, min_data_in_leaf=100, bagging_fraction=0.82063411)\n\nLGB.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T17:57:49.235757Z","iopub.execute_input":"2022-06-16T17:57:49.236603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I founded outside the parameters for this model","metadata":{}},{"cell_type":"code","source":"predictions=LGB.predict(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output=pd.DataFrame({\"id\":submission[\"id\"],\"target\":predictions})\noutput\noutput.to_csv('sample_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**And Finally....**\n====\n\nWe can see in the graph below the incidence of the different features in the model\n\nThis idea was inspired by the Notebook of Imnaho:\nhttps://www.kaggle.com/code/imnaho/eda-predict-with-lgbmclassifier","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\n\nimportances=pd.Series(LGB.feature_importances_,index=X.columns)\nimportances=importances.sort_values()\nimportances.plot(kind=\"barh\")\nplt.title(\"Feature importances in the LGB model\")\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_rs.shape,X_val_rs.shape,y_train.shape,y_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}