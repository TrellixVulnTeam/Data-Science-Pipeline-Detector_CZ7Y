{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nFor this competition, we will be predicting a continuous target variable with 14 continuous independent variables.\n\nSubmissions are evaluated on the Root-Mean-Squared-Error (RMSE).   Thus, the goal of this notebook is to obtain the lowest RMSE.\n\nNotebook Layout\n\n* Exploratory Data Analysis\n* Model Creation\n* Model Stacking\n* Predictions\n\nIf you have any feedback, please let me know!\n\n## Let's get started!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\n# Stats\nfrom scipy.stats import skew, norm\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\npd.set_option('display.max_columns', None)\nfrom pathlib import Path\ninput_path = Path('/kaggle/input/tabular-playground-series-jan-2021/')\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data as dataframes\ntrain = pd.read_csv(input_path / 'train.csv', index_col='id')\ntest = pd.read_csv(input_path / 'test.csv', index_col='id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training data has 300,000 observations and 15 columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The test data has 200,000 observations and 14 columns.\n* The 15th column is absent because it is what we are trying to predict."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.isna().any().any())\nprint(test.isna().any().any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neither of the datasets have missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train and dest data look very similar.  Let's first look at the target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nfig=plt.figure(figsize=(6,6))\nax = sns.distplot(train['target'], color=\"b\")\nax.set(xlabel=\"Target\", ylabel=\"Frequency\", title=\"Target Variable Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target variable is bimodal and has one outlier valued at 0.  Let's remove it.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(train[train['target'] == 0].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at the remainder of the variables with boxplots."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.boxplot(column = list(train.columns[0:14]), figsize= (15,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.boxplot(column = list(test.columns[0:14]), figsize= (15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The boxplots confirm that the two datasets are similar.  The boxplots of each variable are roughly the same in both datasets.\n* Cont5 and Cont7 have similar outliers in both datasets.\n* Let's not remove any of them for now.  We should consider using cooks distance to remove influential outliers.\n* Let's plot the distributions of each variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dataset with only numerical values\nnumericaldata = train.select_dtypes(exclude='object')\n\n# Create plot space\nfig = plt.figure(figsize=(10,15))\n\n# Create subplot for each loop\nfor i in range(len(numericaldata.columns)):\n    fig.add_subplot(6,4,i+1)\n    sns.distplot(numericaldata.iloc[:,i])\n    plt.xlabel(numericaldata.columns[i])\n\n# Display plots        \nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The variables are mostly bimodal and multimodal.\n* None of the variables seem skewed.  Let's check just in case."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loop through each feature\nfor column in train:\n    \n    # calculate skew for feature\n    sk = round(train[column].skew(), 2)\n    \n    # Print if skew is significant\n    if sk > 2 or sk < -2:\n        print(\"Skew for\", column, \"is\", sk)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* None of the variables were highly skewed.\n* Let's explore the relationships between variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create plotting space\nplt.subplots(figsize=(16,12))\n\n# Calculate correlations\ncorr = train[train.columns[1:]].corr()\n\n# Plot the correlations\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True, annot=True)\n\nplt.yticks(rotation=0, fontsize = 15)\nplt.xticks(rotation=0, fontsize = 15)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Unfortunately, none of the predictor variables are highy correlated with the target variable.  \n* There is a cluster of variables that are correlated with each other."},{"metadata":{},"cell_type":"markdown","source":"# Modeling\n\n* We cannot test our models on the given test dataset, because it doesn't contain the target variable.\n* Let's create a new train and test dataset out of the given train dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create test and train data\nX = train.loc[:, ((train.columns != 'target') & (train.columns != 'id'))] \ny = train.target\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create model\nlm = LinearRegression()\n\n# Fit the model\nlm.fit(X_train, y_train)\n\n# Make predictions \ny_pred = lm.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(lm, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cross validated RMSE for the linear regression is 0.7265.\n\nThe minimum and maximum of the target variable are 3.70 and 10.27, respectively.  Range: 10.27 - 3.70 = 6.57.\n\nThe RMSE/Range ratio is .7265/6.57 = 0.111.  \n\nBecause RMSE is the standard deviation of the model's residuals, we want this to be as low as possible.\n\nThis is a good start.  Let's see if we can obtain a better score with different models."},{"metadata":{},"cell_type":"markdown","source":"# Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the parameter grid\nparams = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Create the base model\nlasso = Lasso(random_state=42)\n\n# Create exhaustive search over parameters\ngrid = GridSearchCV(estimator=lasso, param_grid=params,\n                   scoring=\"neg_mean_squared_error\", cv = 5, verbose = 0)\n\n# Fit the grid search to the training data\ngrid.fit(X_train,y_train)\n\n# Print the best parameters\nprint(\"Best parameters found: \", grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set best parameter to model\nlasso.set_params(random_state = 42, alpha = 0.001)\n\n# Fit the new model\nlasso.fit(X_train,y_train)\n\n# Make predictions \ny_pred = lasso.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(lasso, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The cross validated RMSE for the lasso model is 0.7269.  This model performed roughly as well as the linear regression."},{"metadata":{},"cell_type":"markdown","source":"# Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the parameter grid\nparams = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Create the base model\nridge = Ridge(random_state=42)\n\n# Create exhaustive search over parameters\ngrid = GridSearchCV(estimator=ridge, param_grid=params,\n                   scoring=\"neg_mean_squared_error\", cv = 5, verbose = 0)\n\n# Fit the grid search to the training data\ngrid.fit(X_train,y_train)\n\n# Print the best parameters\nprint(\"Best parameters found: \", grid.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set best parameter to model\nridge.set_params(random_state = 42, alpha = 10)\n\n# Fit the new model\nridge.fit(X_train,y_train)\n\n# Make predictions \ny_pred = ridge.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(ridge, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cross validated RMSE for the ridge regression is 0.7265.  This model performed roughly as well as the linear and ridge regression models."},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the base model\nrf_model = RandomForestRegressor(random_state = 42, n_jobs = -1)\n\n# Fit the model\nrf_model.fit(X_train,y_train)\n\n# Create predictions\ny_pred = rf_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(rf_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The cross validated RMSE for the base random forest model is 0.7089.  This is our best model so far.\n* The hidden code below shows the parameter grid used to tune the model.  The code is commented to decrease the computational expense."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# Create the parameter grid\nparams = {'max_depth': [5,10,15,20],\n         'max_features': ['auto', 'sqrt']}\n\n# Create exhaustive search over parameters\ngrid = GridSearchCV(estimator=rf_model, param_grid=params,\n                   scoring=\"neg_mean_squared_error\", cv = 3, verbose = 1)\n\n# Fit the grid search to the training data\ngrid.fit(X_train,y_train)\n\n# Print the best parameters\nprint(\"Best parameters found: \", grid.best_params_)\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set best parameters to model\nrf_model.set_params(random_state = 42, max_depth=20, max_features = 'sqrt') \n\n# Fit the new model\nrf_model.fit(X_train,y_train)\n\n# Make predictions \ny_pred = rf_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(rf_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cross validated RMSE for the tuned random forest model is 0.7058.  This is now our best model!"},{"metadata":{},"cell_type":"markdown","source":"# XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the base model\nxgb_model = xgb.XGBRegressor(random_state=42)\n\n# Fit the model\nxgb_model.fit(X_train,y_train)\n\n# Make predictions\npreds = xgb_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(xgb_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The cross validated RMSE for the base random XGBoost is 0.7050.  Looks like we keep improving!  This is now our best model\n* The hidden code below shows the parameter grid used to tune the model.  The code is commented to decrease the computational expense."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {'colsample_bytree': [.1,.2,.3,.4,.5,.6,.7,.8,.9],\n          'n_estimators': [100],\n          'max_depth': [4,5,6],\n          'learning_rate': [0.05, 0.10, .015, 0.20, 0.25, 0.30]}\n\nxgb_model = gbm.XGBRegressor()\n\ngrid = GridSearchCV(estimator=xgb_model, param_grid=params,\n                   scoring=\"neg_mean_squared_error\", cv = 4, verbose = 1)\n\ngrid.fit(X_train,y_train)\n\nprint(\"Best parameters found: \", grid.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(grid.best_score_)))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set best parameters to model\nxgb_model.set_params(random_state=42, colsample_bytree=0.5, learning_rate= 0.2,max_depth=6, n_estimators=100) \n# Fit the new model\nxgb_model.fit(X_train,y_train)\n\n# Create predictions\ny_pred = xgb_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(xgb_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5, verbose= 0)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The cross validated RMSE for the tuned XGBoost model is 0.7030.  We continue to lower the score!"},{"metadata":{},"cell_type":"markdown","source":"# Light GBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the base model\nlgbm_model = LGBMRegressor(random_state=42)\n\n# Fit the model\nlgbm_model.fit(X_train,y_train)\n\n# Make predictions\ny_pred = lgbm_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(lgbm_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5, verbose= 0)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The cross validated RMSE for the base LGBM m odel is 0.7034.\n* The hidden code below shows the parameter grid used to tune the model.  The code is commented to decrease the computational expense."},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {'feature_fraction': [.1,.2,.3,.4,.5,.6,.7,.8,.9],\n         'max_depth': [2,4,6,8,10,12,14,16,18,20],\n         'max_bin': [5,10,15,20],\n         'n_estimators': [100,500,1000,2000,4000]}\n\ngrid = GridSearchCV(estimator=lgbm_reg, param_grid=params,\n                   scoring=\"neg_mean_squared_error\", cv = 4, verbose = False)\n\ngrid.fit(X_train,y_train)\n\nprint(\"Best parameters found: \", grid.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(grid.best_score_)))\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the best parameters\nlgbm_model.set_params(random_state=42, feature_fraction=.4, max_depth=6, max_bin = 20, n_estimators=500) \n\n# Fit the new model\nlgbm_model.fit(X_train,y_train)\n\n# Create predictions\ny_pred = lgbm_model.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(lgbm_model, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5, verbose= 0)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The cross validated RMSE for the tuned LGBM model is 0.7034."},{"metadata":{},"cell_type":"markdown","source":"# Voting"},{"metadata":{},"cell_type":"markdown","source":"After experimenting with different weights, it became clear that the voting model performs best without the random forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the voting model \nclf_voting = VotingRegressor(\n    \n    estimators=[\n        ('XGBoost',xgb_model),\n        ('LGBoost',lgbm_model)],\n    \n    #Choose weights for each model\n    weights = [.5,.5]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model\nclf_voting.fit(X_train,y_train)\n\n#Make predictions\ny_pred = clf_voting.predict(X_test)\n\n# Calculate root mean squared error\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n\n# Calculate the cross validated RMSE\nscores = cross_val_score(clf_voting, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 5, verbose= 0)\nprint(\"RMSE with cross validation: \", np.mean(np.abs(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The cross validated RMSE for the base voting regressor is .7014, which is officially our lowest score. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create submission\nsubmission = pd.read_csv(input_path / 'sample_submission.csv', index_col='id')\nsubmission['target'] = clf_voting.predict(test)\nsubmission.to_csv('Jan_Tab_Playground.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}