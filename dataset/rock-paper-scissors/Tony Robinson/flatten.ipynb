{"cells":[{"metadata":{},"cell_type":"markdown","source":"Very early on I wrote this strategy which I call Flatten.   The idea is very simple, just invert the distribution that the opponent expects you to play and play that instead most of the time (controlled by pFlatten).  The idea is that your play then appears completely random to your opponent so you can sneak in some exploitation.  This is only 68 lines, but very dissapointingly at the end of the contest it's my second highest scoring agent - it currently has score 990.3 and effective rank 77 on 29Jan21 (a repeat submission has just tanked)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import math\nimport random\nimport numpy as np\n\npFlatten = 0.7 # prob of playing Flatten, else do simple predict\noffset   = 2.0\nhalfLife = 100.0\ncountPow = math.exp(math.log(2)/halfLife)\n\nnaction = 3\nreward  = np.array([[0.0, -1.0, 1.0], [1.0, 0.0, -1.0], [-1.0, 1.0, 0.0]])\n\nclass Agent:\n  def __init__(self):\n    self.countInc = 1e-30\n    self.countOp  = self.countInc * np.ones((naction, naction, naction))\n    self.countAg  = self.countInc * np.ones((naction, naction, naction))\n    self.histAgent = []    # Agent history\n    self.histOpponent = [] # Opponent history\n    self.nwin = 0\n    \n  # make a move\n  def move(self, lastOpponentAction, step):\n    if step == 0:\n      dist = np.ones(naction)\n    else:\n      # store last opponent action\n      self.histOpponent.append(lastOpponentAction)\n\n      # score last game\n      self.nwin += reward[self.histAgent[-1], lastOpponentAction]\n      print('step: ', step, 'win: ', self.nwin)\n\n      if step > 1:\n        # increment predictors\n        self.countOp[self.histOpponent[-2], self.histAgent[-2], self.histOpponent[-1]] += self.countInc\n        self.countAg[self.histOpponent[-2], self.histAgent[-2], self.histAgent[-1]] += self.countInc\n                    \n      # decide on what strategy to play\n      if len(self.histOpponent) < 2:\n        dist = np.ones(naction)\n      else:\n        if random.random() < pFlatten:\n          # stochastically flatten the distribution\n          count = self.countAg[self.histOpponent[-1], self.histAgent[-1]]\n          dist  = (offset + 1) * count.max() - offset * count.min() - count\n        else:\n          # simple prediction of opponent\n          count = self.countOp[self.histOpponent[-1], self.histAgent[-1]]\n          gain  = np.dot(reward, count)\n          dist  = gain + gain.min()\n          \n    agentAction = random.choices(range(naction), weights=dist)[0]\n    self.histAgent.append(agentAction)\n    self.countInc *= countPow\n    \n    return(agentAction)\n\nagent = None\n\ndef main(observation, configuration):\n  global agent\n\n  if agent == None:\n    agent = Agent()\n    return agent.move(None, observation.step)\n  else:\n    return agent.move(observation.lastOpponentAction, observation.step)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}