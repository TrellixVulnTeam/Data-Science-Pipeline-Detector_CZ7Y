{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%writefile my_agent.py\n\nfrom collections import defaultdict\nimport operator\nimport random\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier\nimport operator\nimport numpy as np\nimport cmath\nfrom typing import List\nfrom collections import namedtuple\nimport traceback\nimport sys\n\n\nbasis = np.array(\n    [1, cmath.exp(2j * cmath.pi * 1 / 3), cmath.exp(2j * cmath.pi * 2 / 3)]\n)\n\n\nHistMatchResult = namedtuple(\"HistMatchResult\", \"idx length\")\n\n\ndef find_all_longest(seq, max_len=None) -> List[HistMatchResult]:\n    \"\"\"\n    Find all indices where end of `seq` matches some past.\n    \"\"\"\n    result = []\n\n    i_search_start = len(seq) - 2\n\n    while i_search_start > 0:\n        i_sub = -1\n        i_search = i_search_start\n        length = 0\n\n        while i_search >= 0 and seq[i_sub] == seq[i_search]:\n            length += 1\n            i_sub -= 1\n            i_search -= 1\n\n            if max_len is not None and length > max_len:\n                break\n\n        if length > 0:\n            result.append(HistMatchResult(i_search_start + 1, length))\n\n        i_search_start -= 1\n\n    result = sorted(result, key=operator.attrgetter(\"length\"), reverse=True)\n\n    return result\n\n\ndef probs_to_complex(p):\n    return p @ basis\n\n\ndef _fix_probs(probs):\n    \"\"\"\n    Put probs back into triangle. Sometimes this happens due to rounding errors or if you\n    use complex numbers which are outside the triangle.\n    \"\"\"\n    if min(probs) < 0:\n        probs -= min(probs)\n\n    probs /= sum(probs)\n\n    return probs\n\n\ndef complex_to_probs(z):\n    probs = (2 * (z * basis.conjugate()).real + 1) / 3\n    probs = _fix_probs(probs)\n    return probs\n\n\ndef z_from_action(action):\n    return basis[action]\n\n\ndef sample_from_z(z):\n    probs = complex_to_probs(z)\n    return np.random.choice(3, p=probs)\n\n\ndef bound(z):\n    return probs_to_complex(complex_to_probs(z))\n\n\ndef norm(z):\n    return bound(z / abs(z))\n\n\nclass Pred:\n    def __init__(self, *, alpha):\n        self.offset = 0\n        self.alpha = alpha\n        self.last_feat = None\n\n    def train(self, target):\n        if self.last_feat is not None:\n            offset = target * self.last_feat.conjugate()   # fixed\n\n            self.offset = (1 - self.alpha) * self.offset + self.alpha * offset\n\n    def predict(self, feat):\n        \"\"\"\n        feat is an arbitrary feature with a probability on 0,1,2\n        anything which could be useful anchor to start with some kind of sensible direction\n        \"\"\"\n        feat = norm(feat)\n\n        # offset = mean(target - feat)\n        # so here we see something like: result = feat + mean(target - feat)\n        # which seems natural and accounts for the correlation between target and feat\n        # all RPSContest bots do no more than that as their first step, just in a different way\n        \n        result = feat * self.offset\n\n        self.last_feat = feat\n\n        return result\n    \n    \nclass BaseAgent:\n    def __init__(self):\n        self.my_hist = []\n        self.opp_hist = []\n        self.my_opp_hist = []\n        self.outcome_hist = []\n        self.step = None\n\n    def __call__(self, obs, conf):\n        try:\n            if obs.step == 0:\n                action = np.random.choice(3)\n                self.my_hist.append(action)\n                return action\n\n            self.step = obs.step\n\n            opp = int(obs.lastOpponentAction)\n            my = self.my_hist[-1]\n\n            self.my_opp_hist.append((my, opp))\n            self.opp_hist.append(opp)\n\n            outcome = {0: 0, 1: 1, 2: -1}[(my - opp) % 3]\n            self.outcome_hist.append(outcome)\n\n            action = self.action()\n\n            self.my_hist.append(action)\n\n            return action\n        except Exception:\n            traceback.print_exc(file=sys.stderr)\n            raise\n\n    def action(self):\n        pass\n\n\nclass Agent(BaseAgent):\n    def __init__(self, alpha=0.01):\n        super().__init__()\n\n        self.predictor = Pred(alpha=alpha)\n\n    def action(self):\n        self.train()\n\n        pred = self.preds()\n\n        return_action = sample_from_z(pred)\n\n        return return_action\n\n    def train(self):\n        last_beat_opp = z_from_action((self.opp_hist[-1] + 1) % 3)\n        self.predictor.train(last_beat_opp)\n\n    def preds(self):\n        hist_match = find_all_longest(self.my_opp_hist, max_len=20)\n\n        if not hist_match:\n             return 0\n\n        feat = z_from_action(self.opp_hist[hist_match[0].idx])\n\n        pred = self.predictor.predict(feat)\n\n        return pred\n    \n    \n\nclass GeomAgent:\n    def __init__(self):\n        self.agent = Agent()\n    def action(self, state):\n        return self.agent(state[\"obs\"], state[\"conf\"])\n\n#def call_agent(obs, conf):\n#    return agent(obs, conf)\n\nRANDOM_PROBA = 0. / 3\nWINDOW_SIZE = 20\nHIDDEN_LAYERS = (100, 100)\n\n\ndef make_X(start, stop):\n    X1 = state[\"moves\"][\"my\"][start:stop]\n    X2 = state[\"moves\"][\"his\"][start:stop]\n    \n    X1 = np.eye(3)[np.array(X1)].ravel()\n    X2 = np.eye(3)[np.array(X2)].ravel()\n    \n    return np.array([np.hstack([X1, X2])])\n\nclass MLPAgent:\n    def __init__(self, layers=HIDDEN_LAYERS, stochastic=True):\n        self.clf = MLPClassifier(hidden_layer_sizes=layers)\n        self.stochastic = stochastic\n    def action(self, state):\n        if len(state[\"moves\"][\"my\"]) <= WINDOW_SIZE:\n            my = random.choice([0,1,2])\n        else:\n            X = make_X(-WINDOW_SIZE-1, -1)\n            y = np.array([state[\"moves\"][\"his\"][-1]])\n\n            self.clf.partial_fit(X, y, classes=np.array([0,1,2]))\n\n            X1 = make_X(-WINDOW_SIZE, None)\n            his_proba =  np.array(self.clf.predict_proba(X1))[0]\n            EVs = [rewards[x].dot(his_proba) for x in [\"r\", \"p\", \"s\"]]\n            if self.stochastic:\n                my = random.choices([0, 1, 2], weights=EVs, k=1)[0]\n            else:\n                my = int(np.argmax(EVs))\n        return my\n\nstate = {}\nstate[\"moves\"] = defaultdict(list)\nstate[\"freqs_1\"] = np.zeros(3)\nstate[\"freqs_095\"] = np.zeros(3)\n\nstate[\"rolls_1\"] = np.zeros(3)\nstate[\"rolls_095\"] = np.zeros(3)\n\nstate[\"mc_1\"] = np.zeros((3,3))\nstate[\"mc_098\"] = np.zeros((3,3))\nstate[\"mc_095\"] = np.zeros((3,3))\nstate[\"mc_085\"] = np.zeros((3,3))\n\nstate[\"tm_1\"] = np.zeros((3,3))\nstate[\"tm_095\"] = np.zeros((3,3))\n\nstate[\"freqs_1_my\"] = np.zeros(3)\nstate[\"freqs_095_my\"] = np.zeros(3)\n\nstate[\"rolls_1_my\"] = np.zeros(3)\nstate[\"rolls_095_my\"] = np.zeros(3)\n\nstate[\"mc_1_my\"] = np.zeros((3,3))\nstate[\"mc_098_my\"] = np.zeros((3,3))\nstate[\"mc_095_my\"] = np.zeros((3,3))\nstate[\"mc_085_my\"] = np.zeros((3,3))\n\nstate[\"tm_1_my\"] = np.zeros((3,3))\nstate[\"tm_095_my\"] = np.zeros((3,3))\n\n\nrewards = {\"r\": np.array([1, 0, 2]), \n           \"p\": np.array([2, 1, 0]), \n           \"s\": np.array([0, 2, 1])}\n\n\nstrategies = {}\nmeta_strategies = {}\n\ndef getRoll(first, second):\n    if second == first:\n        return 0\n    if second > first:\n        return second - first\n    else:\n        return 2 - first + second + 1\n\ndef applyRoll(act, roll):\n    if not roll:\n        return act\n    newact = act + roll\n    return newact % 3\n\ndef result(my, his):\n    res_roll = {0:1, 1:2, 2:0}\n    roll = getRoll(his, my)\n    return res_roll[roll]\n    \nclass Rock:\n    def action(self, state):\n        return 0\n\nclass Paper:\n    def action(self, state):\n        return 1\n\nclass Scissors:\n    def action(self, state):\n        return 2\n\nclass Random:\n    def action(self, state):\n        return random.choice([0, 1, 2])\n\nclass SameRoll:\n    def __init__(self, roll):\n        self.roll = roll\n    def action(self, state):\n        return applyRoll(state[\"moves\"][\"my\"][-1], self.roll)\n\nclass Freq:\n    def __init__(self, key, stochastic=True):\n        self.key = key\n        self.stochastic = stochastic\n    def action(self, state):\n        mat = state[self.key]\n        ev_freq = [rewards[x].dot(mat) for x in [\"r\", \"p\", \"s\"]]\n        if self.stochastic:\n            my = random.choices([0, 1, 2], weights=ev_freq, k=1)[0]\n        else:\n            my = int(np.argmax(ev_freq))\n        return my\n\nclass Roll:\n    def __init__(self, key, stochastic=True):\n        self.key = key\n        self.stochastic = stochastic\n    def action(self, state):\n        mat = state[self.key]\n        #hisroll = int(np.argmax(mat))\n        hisprev = state[\"moves\"][\"his\"][-1]\n        hismat  = np.roll(mat,  hisprev)\n        ev_freq = [rewards[x].dot(hismat) for x in [\"r\", \"p\", \"s\"]]\n        if self.stochastic:\n            my = random.choices([0, 1, 2], weights=ev_freq, k=1)[0]\n        else:\n            my = int(np.argmax(ev_freq))\n        return my\n\nclass RollMirror:\n    def __init__(self, roll):\n        self.roll = roll\n    def action(self, state):\n        return applyRoll(state[\"moves\"][\"his\"][-1], self.roll)\n\nclass RollFreq:\n    def __init__(self, key, stochastic=True):\n        self.key = key\n        self.stochastic = stochastic\n    def action(self, state):\n        mat_mc = state[self.key]\n        rst = int(result(state[\"moves\"][\"his\"][-1], state[\"moves\"][\"my\"][-1] ))\n        mat = mat_mc[rst] \n        hisprev = state[\"moves\"][\"his\"][-1]\n        hismat  = np.roll(mat,  hisprev)\n        ev_freq = [rewards[x].dot(hismat) for x in [\"r\", \"p\", \"s\"]]\n        if self.stochastic:\n            my = random.choices([0, 1, 2], weights=ev_freq, k=1)[0]\n        else:\n            my = int(np.argmax(ev_freq))\n        return my\n    \nclass TM:\n    def __init__(self, key, stochastic=True):\n        self.key = key\n        self.stochastic = stochastic\n    def action(self, state):\n        mat_mc = state[self.key]\n        hismat = mat_mc[state[\"moves\"][\"his\"][-1]] \n        ev_freq = [rewards[x].dot(hismat) for x in [\"r\", \"p\", \"s\"]]\n        if self.stochastic:\n            my = random.choices([0, 1, 2], weights=ev_freq, k=1)[0]\n        else:\n            my = int(np.argmax(ev_freq))\n        return my\n    \nclass CounterStrategy:\n    def __init__(self, strategy, roll=2):\n        self.strategy = strategy\n        self.roll = roll\n\n    def action(self, state):\n        act = self.strategy.action(state)\n        return applyRoll(act, self.roll)\n\n\n\n\n'''\nstrategies[\"rock\"] = Rock()\nstrategies[\"paper\"] = Paper()\nstrategies[\"scissors\"] = Scissors()\nstrategies[\"random\"] = Random()\nstrategies[\"sameroll0\"] = SameRoll(0)\nstrategies[\"sameroll1\"] = SameRoll(1)\nstrategies[\"sameroll2\"] = SameRoll(2)\nstrategies[\"freq1\"] = Freq(\"freqs_1\")\nstrategies[\"freq095\"] = Freq(\"freqs_095\")\n\nstrategies[\"rollmirror0\"] = RollMirror(0)\nstrategies[\"rollmirror1\"] = RollMirror(1)\nstrategies[\"rollmirror2\"] = RollMirror(2)\n'''\n\nstrategies[\"rolls1\"] = Roll(\"rolls_1\")\n\nstrategies[\"rolls095\"] = Roll(\"rolls_095\")\n\nstrategies[\"rollfreq1\"] = RollFreq(\"mc_1\")\nstrategies[\"rollfreq095\"] = RollFreq(\"mc_095\")\nstrategies[\"rollfreq098\"] = RollFreq(\"mc_098\")\nstrategies[\"rollfreq085\"] = RollFreq(\"mc_085\")\n\nstrategies[\"tm1\"] = TM(\"tm_1\")\nstrategies[\"tm095\"] = TM(\"tm_095\")\n\n#strategies[\"random\"] = Random()\n#strategies[\"rollfreq095\"] = RollFreq(\"mc_095\")\nstrategies[\"freq095\"] = Freq(\"freqs_095\")\nstrategies[\"freq1\"] = Freq(\"freqs_1\")\n\nstrategies[\"mlp\"] = MLPAgent()\nstrategies[\"mlp2\"] = MLPAgent([100])\nstrategies[\"mlp3\"] = MLPAgent([1000])\nstrategies[\"mlp4\"] = MLPAgent([100, 100, 100])\n\nstrategies[\"geom\"] = GeomAgent()\n\nstrategies[\"counter_freq095\"] = CounterStrategy(Freq(\"freqs_095_my\"), 1)\nstrategies[\"counter_freq1\"] = CounterStrategy(Freq(\"freqs_1_my\"), 1)\n\nstrategies[\"counter_tm1\"] = CounterStrategy(TM(\"tm_1_my\"), 1)\nstrategies[\"counter_tm095\"] = CounterStrategy(TM(\"tm_095_my\"), 1)\n\nstrategies[\"counter_rollfreq_1\"] = CounterStrategy(RollFreq(\"mc_1_my\"), 1)\nstrategies[\"counter_rollfreq_095\"] = CounterStrategy(RollFreq(\"mc_095_my\"), 1)\n\n\n\n'''\nfor k in list(strategies.keys()):\n    if k == \"random\":\n        continue\n    for roll in [1, 2]:\n        k_new = k + f\"counter_{roll}\"\n        strategies[k_new] = CounterStrategy(strategies[k], roll)\n'''\n\nclass MetaStrategy:\n    def __init__(self, nitems, EMA=0.85, meta_meta=False, verbose=False):\n        self.nitems = nitems\n        self.beta = EMA ** np.arange(nitems)[::-1]  \n        if meta_meta:\n            self.strategies = meta_strategies\n        else:\n            self.strategies = strategies\n        self.verbose = verbose\n    def best(self, state):\n        his_shift = state[\"moves\"][\"his\"]\n        incomes = {}\n        for k in self.strategies.keys():\n            \n            \n            my = state[\"moves\"][k][:-1]\n            #if self.verbose:\n            #    print(k, my[-10:], his_shift[-10:], [result(m, h) - 1 for m, h in zip(my[-10:], his_shift[-10:])],\n            #         self.beta[-10:])\n                \n            nitems = min(len(my), self.nitems)\n            nitems = min(len(his_shift), nitems)\n\n            income = sum([(result(m, h) - 1) * b for (m, h, b) in zip(my[-nitems:], \n            his_shift[-nitems:],\n            self.beta[-nitems:],\n            )])\n            #if self.verbose:\n            #    print(income)\n            incomes[k] = income\n        \n        k =  list(incomes.keys())\n        v = [incomes[kk] for kk in k]\n        v = np.clip(np.array(v), 0, None)\n        if v.sum():\n            p = v / v.sum()\n        else:\n            p = np.ones(len(v))\n        #best = max(incomes.items(), key=operator.itemgetter(1))[0]\n        best = random.choices(k, weights=p, k=1)[0]\n        if self.verbose:\n            print(best)\n        #best = random.choice(k)\n        #print(best, incomes[best], incomes[\"random\"])\n        return best\n\n    \nfor nitems in [5, 10, 20, 50, 100, 200, 300, 500, 1000]:\n    verbose=False\n    #if nitems == 1000:\n    #    verbose=True\n    ms = MetaStrategy(nitems, 0.99, verbose=verbose)\n    meta_strategies[f\"meta_{nitems}\"] = ms\n\nmms = MetaStrategy(1000, 0.99, meta_meta=True)\n\ndef processOppActions(act):\n    state[\"moves\"][\"his\"].append(act)\n\n    addfreq = np.zeros(3)\n    addfreq[act] = 1\n    state[\"freqs_1\"] = state[\"freqs_1\"] + addfreq\n    state[\"freqs_095\"] = state[\"freqs_095\"] * 0.95 + addfreq\n\n    if len(state[\"moves\"][\"his\"]) > 1:\n        roll = getRoll(state[\"moves\"][\"his\"][-2], state[\"moves\"][\"his\"][-1])\n        addroll = np.zeros(3)\n        addroll[roll] = 1\n        state[\"rolls_1\"] = state[\"rolls_1\"] + addroll\n        state[\"rolls_095\"] = state[\"rolls_095\"] * 0.95 + addroll\n\n        rst = int(result(state[\"moves\"][\"his\"][-2], state[\"moves\"][\"my\"][-2] ))\n\n        addroll = np.zeros((3, 3))\n        addroll[rst, roll] = 1\n        state[\"mc_1\"] = state[\"mc_1\"] + addroll\n        state[\"mc_098\"] = state[\"mc_098\"] * 0.98 + addroll\n        state[\"mc_095\"] = state[\"mc_095\"] * 0.95 + addroll\n        state[\"mc_085\"] = state[\"mc_085\"] * 0.85 + addroll\n        addroll = np.zeros((3, 3))\n        addroll[state[\"moves\"][\"his\"][-2], state[\"moves\"][\"his\"][-1]] = 1\n        state[\"tm_1\"] = state[\"tm_1\"] + addroll\n        state[\"tm_095\"] = state[\"tm_1\"] * 0.95 + addroll\n\n\ndef processMyActions():\n    #state[\"moves\"][\"his\"].append(act)\n    act = state[\"moves\"][\"my\"][-1]\n    addfreq = np.zeros(3)\n    addfreq[act] = 1\n    state[\"freqs_1_my\"] = state[\"freqs_1_my\"] + addfreq\n    state[\"freqs_095_my\"] = state[\"freqs_095_my\"] * 0.95 + addfreq\n\n    if len(state[\"moves\"][\"my\"]) > 1 and len(state[\"moves\"][\"his\"]) > 1:\n        roll = getRoll(state[\"moves\"][\"my\"][-2], state[\"moves\"][\"my\"][-1])\n        addroll = np.zeros(3)\n        addroll[roll] = 1\n        state[\"rolls_1_my\"] = state[\"rolls_1_my\"] + addroll\n        state[\"rolls_095_my\"] = state[\"rolls_095_my\"] * 0.95 + addroll\n\n        rst = int(result(state[\"moves\"][\"my\"][-2], state[\"moves\"][\"his\"][-2] ))\n\n        addroll = np.zeros((3, 3))\n        addroll[rst, roll] = 1\n        state[\"mc_1_my\"] = state[\"mc_1_my\"] + addroll\n        state[\"mc_098_my\"] = state[\"mc_098_my\"] * 0.98 + addroll\n        state[\"mc_095_my\"] = state[\"mc_095_my\"] * 0.95 + addroll\n        state[\"mc_085_my\"] = state[\"mc_085_my\"] * 0.85 + addroll\n        addroll = np.zeros((3, 3))\n        addroll[state[\"moves\"][\"my\"][-2], state[\"moves\"][\"my\"][-1]] = 1\n        state[\"tm_1_my\"] = state[\"tm_1_my\"] + addroll\n        state[\"tm_095_my\"] = state[\"tm_1_my\"] * 0.95 + addroll\ndef my_agent(obs, conf):\n    state[\"obs\"] = obs\n    state[\"conf\"] = conf\n    if obs[\"step\"] == 0:\n        my = random.choice([0,1,2])\n        state[\"moves\"][\"my\"].append(my)\n        strategies[\"geom\"].action(state)\n        processMyActions()\n        return my\n    processOppActions(obs[\"lastOpponentAction\"])\n    \n    for k, v in strategies.items():\n        mymove = v.action(state)\n        state[\"moves\"][k].append(mymove)\n\n    for ms_name, ms in meta_strategies.items():\n        best_strategy = ms.best(state)\n        bestmove = state[\"moves\"][best_strategy][-1]\n        state[\"moves\"][ms_name].append(bestmove)\n    \n    best_meta_strategy = mms.best(state)\n    #print(best_meta_strategy)\n    bestmove = state[\"moves\"][best_meta_strategy][-1]\n    random_move = int(np.random.choice(range(3)))\n    bestmove = int(np.random.choice([bestmove, random_move], p=[1 - RANDOM_PROBA, RANDOM_PROBA]))\n    state[\"moves\"][\"my\"].append(bestmove)\n    processMyActions()\n    return bestmove\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile simple_rolling.py\n\ndef my_agent(obs, conf):\n    return obs[\"step\"] % 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile rolling.py\n\ndef my_agent(obs, conf):\n    return (obs[\"step\"] + int(obs[\"step\"] / 10)) % 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import evaluate, make, utils\nenv = make(\"rps\", debug=True)\nenv.run(['my_agent.py', 'rolling.py'])\nenv.render(mode=\"ipython\", width=500, height=450)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}