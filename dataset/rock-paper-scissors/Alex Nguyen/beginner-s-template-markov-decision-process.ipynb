{"cells":[{"metadata":{},"cell_type":"markdown","source":"<center><h1> Markov Decision Process: A Q-learning approach </h1></center>"},{"metadata":{},"cell_type":"markdown","source":"<center><h2>The Markov property states that,\n“ The future is independent of the past given the present.”\nOnce the current state in known, the history of information encountered so far may be thrown away, and that state is a sufficient statistic that gives us the same characterization of the future as if we have all the history.<h2p></center>"},{"metadata":{},"cell_type":"markdown","source":"### Component of MDPs:\n\n\nS: set of states\n<br>\nA: set of actions\n<br>\nR: reward function\n<br>\nP: transition probability function\n<br>\nγ: discount for future rewards\n    \n<p>In mathematical terms, a state St has the Markov property, if and only if;<br>\nP[St+1 | St] = P[St+1 | S1, ….. , St],</p>\n\n### Approach:\n\n<p>The MDP is more than just Multi-armed bandit approach which does not consider the state space but instead evaluate \"arm-weights\" through reward. However, the reward in each turn is temporary, so it is not as informative to tell which strategy the player should optimize for. When considering a state space, we are taking previous set of action into evaluation.</p>\n<br>\n<p>In this approach, a state space can consist of the tuple of each player action corresponding with each of the opponent action. Therefore, we can initialize our Q-table policy with a 2D matrix with the shape (9,3). Each of the element in the first dimension is one scenario where a player take a particular action, and an opponent takes a particular action. Each element in the second dimension is the available action that this player can take.</p>\n<!-- <p>We </p> -->\n\n"},{"metadata":{},"cell_type":"markdown","source":"### First we install the library\nLet's go!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install kaggle-environments\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom kaggle_environments import make, evaluate\nimport keras\nimport collections\nimport sys\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%writefile mdp.py\nimport pandas as pd\nimport random\nimport numpy as np\nfrom kaggle_environments.envs.rps.utils import get_score\n\n# Util method for getting the state in the q table\ndef get_state(action, op_action):\n    return action * 3 + op_action\n# Current action\ncur_action = 0\n# Epsilon: Exploration rate\neps = 0.1\n# History\nhistory = []\n# Q_table (Policies) Shape: (9, 3)\npolicies = [[0] *3] * 9\n#Learning rate\nlr = 0.7\n# Discount rate for q_table\ndiscount_rate = 0.3\n# Epsilon decay rate\ndecay_rate = 0.9\n\ndef update_q_table(op_action):\n    global policies\n    global discount_rate\n    global lr\n    global history\n    reward = get_score(cur_action, op_action)\n    if len(history) > 1:\n        previous_state_id = get_state(history[len(history) - 2][0], history[len(history) - 2][1])\n        state_id = get_state(cur_action, op_action)\n        policies[previous_state_id][cur_action] = policies[previous_state_id][cur_action] * (1 - lr) \\\n        + lr * (reward + discount_rate * np.max(policies[state_id][:]))\n\ndef mdp(observation, configuration):\n    global cur_action\n    global history\n    global policies\n    if observation.step > 0:\n        history.append([cur_action, observation.lastOpponentAction])\n        update_q_table(observation.lastOpponentAction)\n    \n    explore_rate = np.random.random()\n    if explore_rate < eps:\n        cur_action = random.randint(0, 2)\n        explore_rate *= decay_rate\n    else:\n        if observation.step > 0:\n            state_id = get_state(cur_action, observation.lastOpponentAction)\n            cur_action = int(np.argmax(policies[state_id][:]))\n        else:\n            cur_action = random.randint(0, 2)\n    return cur_action","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation Result\n<p> Although the approach is not perfect, it can beat most static strategy </p>\n<p> We can say that this version is a loose version of reactionary, so that it cannot be countered by countered_reactionary bot</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"rps\", configuration={\"episodeSteps\": 1000}, debug=\"True\")\nenv.reset()\nenv.run([\"mdp.py\", \"statistical\"])\nenv.render(mode=\"ipython\", width=400, height=400)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}