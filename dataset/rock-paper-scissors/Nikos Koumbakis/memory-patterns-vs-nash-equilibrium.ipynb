{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Memory Patterns vs Nash Equilibrium: Rock Paper Scissors\n\n\n### 100 seasons of Memory Patterns vs Nash on Rock Paper Scissors\n### 1000 episodes per season\n\n### Bonus: Dataset generation"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Nash Equilibrium<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/John_Forbes_Nash%2C_Jr._by_Peter_Badge.jpg/220px-John_Forbes_Nash%2C_Jr._by_Peter_Badge.jpg)"},{"metadata":{},"cell_type":"markdown","source":"*...if we all go for the blonde we are blocking each other.*"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Memory Patterns\n    <center><h2>"},{"metadata":{},"cell_type":"markdown","source":"[Memory_Patterns](https://www.kaggle.com/yegorbiryukov/rock-paper-scissors-with-memory-patterns) by [Yegor Biryukov](https://www.kaggle.com/yegorbiryukov)"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"%%writefile memory_patterns.py\n\nimport random\n\n# how many steps in a row are in the pattern (multiplied by two)\nmemory_length = 12\n# current memory of the agent\ncurrent_memory = []\n# list of memory patterns\nmemory_patterns = []\n\ndef find_pattern(memory):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # if it's not first step, add opponent's last action to agent's current memory\n    if obs[\"step\"] > 0:\n        current_memory.append(obs[\"lastOpponentAction\"])\n    # if length of current memory is bigger than necessary for a new memory pattern\n    if len(current_memory) > memory_length:\n        # get momory of the previous step\n        previous_step_memory = current_memory[:memory_length]\n        previous_pattern = find_pattern(previous_step_memory)\n        if previous_pattern == None:\n            previous_pattern = {\n                \"actions\": previous_step_memory.copy(),\n                \"opp_next_actions\": [\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            memory_patterns.append(previous_pattern)\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n        # delete first two elements in current memory (actions of the oldest step in current memory)\n        del current_memory[:2]\n    my_action = random.randint(0, 2)\n    pattern = find_pattern(current_memory)\n    if pattern != None:\n        my_action_amount = 0\n        for action in pattern[\"opp_next_actions\"]:\n            # if this opponent's action occurred more times than currently chosen action\n            # or, if it occured the same amount of times, choose action randomly among them\n            if (action[\"amount\"] > my_action_amount or\n                    (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                my_action_amount = action[\"amount\"]\n                my_action = action[\"response\"]\n    current_memory.append(my_action)\n    return my_action\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Validate<center><h2>\n\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from kaggle_environments import make, evaluate\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 1000})\n\nenv.run([\"memory_patterns.py\", \"nash_equilibrium.py\"])\n\nenv.render(mode=\"ipython\", width=800, height=800)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Action<center><h2>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"seasons = 100\nepisodes = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import make\n\nfrom IPython.display import Markdown as md\n\naction_board = pd.DataFrame(columns = [\"season\",\n                                      \"episode\",\n                                      \"Memory Action\",\n                                      \"Nash Action\",\n                                      \"Memory Reward\",\n                                      \"Nash Reward\"])\nleaderboard = pd.DataFrame(columns = [\"season\",\n                                      \"Memory Reward\",\n                                      \"Nash Reward\"])\n\n\nindex = 0\nenv = make(\"rps\", configuration={\"episodeSteps\": episodes})\n\nfor season in range(seasons):\n    env.reset()\n    results = env.run([\"memory_patterns.py\", \"nash_equilibrium.py\"])\n    for result in results:\n        if (result[0].observation.step == 0):\n            continue\n        action_board = action_board.append({\"season\": season,\n                              \"episode\": result[0].observation.step,\n                              \"Memory Action\": result[0].action,\n                              \"Nash Action\": result[1].action,\n                              \"Memory Reward\": result[0].reward,\n                              \"Nash Reward\": result[1].reward},\n                                        ignore_index=True)\n        if result[0].status == \"DONE\":\n            leaderboard = leaderboard.append({\"season\": season,\n                              \"Memory Reward\": result[0].reward,\n                              \"Nash Reward\": result[1].reward},\n                                        ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:#FBE338; border:0; color:black'><center>Result<center><h1>\n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"md('# Memory Patterns - Nash Equilibrium : {} - {}'.format(len(leaderboard[leaderboard[\"Memory Reward\"] > 0]), len(leaderboard[leaderboard[\"Nash Reward\"] > 0])))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"md('# Tie : {}'.format(len(leaderboard[leaderboard[\"Memory Reward\"] == 0])))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"if (len(leaderboard[leaderboard[\"Memory Reward\"] > 0]) == len(leaderboard[leaderboard[\"Nash Reward\"] > 0])):\n    winner = \"Tie!\"\nelif (len(leaderboard[leaderboard[\"Memory Reward\"] > 0]) > len(leaderboard[leaderboard[\"Nash Reward\"] > 0])):\n    winner = \"Winner is Memory Patterns!\"\nelse:\n    winner = \"Winner is Nash!\"\nmd('<a id=\"11\"></a><h1 style=\\'background:#FBE338; border:0; color:black\\'><center>{}<center><h2>'.format(winner))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:#FBE338; border:0; color:black'><center>Analysis<center><h1>"},{"metadata":{},"cell_type":"markdown","source":"# Season's results"},{"metadata":{"trusted":true},"cell_type":"code","source":"leaderboard.plot(subplots=True, figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Season's reward histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"leaderboard[['Memory Reward', 'Nash Reward']].plot.hist(bins=10,  alpha=0.5, figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Actions histogram"},{"metadata":{"trusted":true},"cell_type":"code","source":"action_board[['Memory Action', 'Nash Action']].plot.hist(bins=3, alpha=0.5, xticks=[0,1,2], figsize=(15,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## All episodes reward"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nfor i, g in action_board.groupby('season'):\n    g.plot(x='episode', y='Memory Reward', ax=ax, legend=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First half rewards"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[(action_board['episode']<episodes/2)].groupby('season'):\n    g.plot(x='episode', y='Memory Reward', ax=ax, legend=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Mid-episodes reward"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[((action_board['episode']>episodes/3) & (action_board['episode']<2*episodes/3))].groupby('season'):\n    g.plot(x='episode', y='Memory Reward', ax=ax, legend=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Last half rewards"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,15))\nfor i, g in action_board[action_board['episode']>episodes/2].groupby('season'):\n    g.plot(x='episode', y='Memory Reward', ax=ax, legend=False )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:#FBE338; border:0; color:black'><center>Conclusion<center><h1>"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"<h1 style='background:#FBE338; border:0; color:black'><center>Dataset<center><h1>"},{"metadata":{},"cell_type":"markdown","source":"Dataset is exported, collected and publicly shared in [Rock Paper Scissors Agents Battles](https://www.kaggle.com/jumaru/rock-paper-scissors-agents-battles) dataset."},{"metadata":{},"cell_type":"markdown","source":"## Leaderboard"},{"metadata":{},"cell_type":"markdown","source":"### First 5 seasons rewards"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"leaderboard.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Last 5 seasons rewards"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"leaderboard.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Rewards Statistics "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"leaderboard.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Action board"},{"metadata":{},"cell_type":"markdown","source":"## First 5 actions"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"action_board.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Last 5 actions"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"action_board.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Actions Statistics"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"action_board.drop(columns='season').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data export"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Report boards\nleaderboard_csv = 'Memory_Patterns_leaderboard_S' + str(seasons) + 'E' + str(episodes) + '.csv'\naction_board_csv = 'Memory_Patterns_action_board_S'+ str(seasons) + 'E' + str(episodes) + '.csv'\nleaderboard.to_csv(leaderboard_csv)\naction_board.to_csv(action_board_csv)\nprint(leaderboard_csv)\nprint(action_board_csv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References"},{"metadata":{},"cell_type":"markdown","source":"* [Rock Paper Scissors - Nash Equilibrium Strategy](https://www.kaggle.com/ihelon/rock-paper-scissors-nash-equilibrium-strategy) & [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison) by [Yaroslav Isaienkov](https://www.kaggle.com/ihelon)\n* [(Not so) Markov](https://www.kaggle.com/alexandersamarin/not-so-markov) by [Alexander Samarin](https://www.kaggle.com/alexandersamarin)\n* [LB simulation](https://www.kaggle.com/superant/lb-simulation) by [Ant 🐜](https://www.kaggle.com/superant)\n* [Memory_Patterns](https://www.kaggle.com/yegorbiryukov/rock-paper-scissors-with-memory-patterns) by [Yegor Biryukov](https://www.kaggle.com/yegorbiryukov)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}