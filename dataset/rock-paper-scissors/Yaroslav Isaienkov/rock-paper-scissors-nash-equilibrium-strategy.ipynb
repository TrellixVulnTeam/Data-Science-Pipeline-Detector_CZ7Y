{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Rock Paper Scissors - Nash Equilibrium Strategy\n\nExample of using Nash Equilibrium principle in Rock-Paper-Scissors game   \nCreating simple agent for the [Rock, Paper, Scissors](https://www.kaggle.com/c/rock-paper-scissors) competition"},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/22838/logos/header.png?t=2020-11-02-21-55-44)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#FBE338; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation</center></h3>\n\n* [1. Nash Equilibrium Overview](#1)\n* [2. Agent Code](#2)\n* [3. Battle Examples (Optional)](#3)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>1. Nash Equilibrium Overview<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"In game theory, the Nash equilibrium, named after the mathematician John Forbes Nash Jr., is a proposed solution of a non-cooperative game involving two or more players in which each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy. [Wikipedia](https://en.wikipedia.org/wiki/Nash_equilibrium#cite_note-Osborne-1)"},{"metadata":{},"cell_type":"markdown","source":"Consider Rock-Paper-Scissors awards matrix (our reward/action is blue, the reward/action of the opponent is red):"},{"metadata":{},"cell_type":"markdown","source":"<img style=\"height:400px\" src=\"https://i.imgur.com/aEL9IKd.png\">\n<cite>The image from YouTube: <a href=\"https://www.youtube.com/watch?v=-1GDMXoMdaY&ab_channel=CrashCourse\">Game Theory 101: Rock, Paper, Scissors</a></cite>"},{"metadata":{},"cell_type":"markdown","source":"If we played each action with equal probability 1/3 then the opponent must do the same.   \nOtherwise if the opponent will play all the time Rock, then:\n- he ties a third of the time against Rock, \n- he loses a third of the time against Paper,\n- and he wins a third of the time against Scissors.\n\nThen he will get reward 1/3 \\* 0 + 1/3 \\* (-1) + 1/3 * 1 = 0.    \n**But in this case, we can change our strategy to Paper and win all the time.**"},{"metadata":{},"cell_type":"markdown","source":"<img style=\"height:400px\" src=\"https://i.imgur.com/5FYS8L4.png\">\n<cite>The image from YouTube: <a href=\"https://www.youtube.com/watch?v=-1GDMXoMdaY&ab_channel=CrashCourse\">Game Theory 101: Rock, Paper, Scissors</a></cite>"},{"metadata":{},"cell_type":"markdown","source":"If the opponent will play all the time Paper, then:\n- he wins a third of the time against Rock,\n- he ties a third of the time against Paper,\n- and he loses a third of the time against Scissors.\n\nThen he will get reward 1/3 \\* 1 + 1/3 \\* 0 + 1/3 * (-1) = 0.    \n**But in this case, we can change our strategy to Scissors and win all the time.**"},{"metadata":{},"cell_type":"markdown","source":"<img style=\"height:400px\" src=\"https://i.imgur.com/doHd5dP.png\">\n<cite>The image from YouTube: <a href=\"https://www.youtube.com/watch?v=-1GDMXoMdaY&ab_channel=CrashCourse\">Game Theory 101: Rock, Paper, Scissors</a></cite>"},{"metadata":{},"cell_type":"markdown","source":"If the opponent will play all the time Scissors, then:\n- he loses a third of the time against Rock,\n- he wins a third of the time against Paper,\n- and he ties a third of the time against Scissors.\n\nThen he will get reward 1/3 \\* (-1) + 1/3 \\* 1 + 1/3 * (0) = 0.    \n**But in this case, we can change our strategy to Rock and win all the time.**"},{"metadata":{},"cell_type":"markdown","source":"<img style=\"height:400px\" src=\"https://i.imgur.com/yjy0yCx.png\">\n<cite>The image from YouTube: <a href=\"https://www.youtube.com/watch?v=-1GDMXoMdaY&ab_channel=CrashCourse\">Game Theory 101: Rock, Paper, Scissors</a></cite>"},{"metadata":{},"cell_type":"markdown","source":"The remaining option in order to be in equilibrium is that both players need to play a random strategy, then there is no point in changing their strategy - which is the Nash equilibrium"},{"metadata":{},"cell_type":"markdown","source":"Slides and more information: [Game Theory 101: Rock, Paper, Scissors](https://www.youtube.com/watch?v=-1GDMXoMdaY&ab_channel=CrashCourse)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>2. Agent Code<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"To create the agent for this competition, we must put its code in \\*.py file.   \nTo do this, we can use the [magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html) of Jupyter Notebooks    \nOne of these commands is [writefile](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) which writes the contents of the cell to a file."},{"metadata":{},"cell_type":"markdown","source":"Let's create an agent that will generate a random number from 0 to 3 each time (Nash Equilibrium Strategy)   \n**You must also put all the necessary imports to the \\*.py file, in our example, this is a RANDOM module**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%writefile submission.py\n\nimport random\n\ndef nash_equilibrium_agent(observation, configuration):\n    return random.randint(0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>3. Battle Examples (Optional)<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"This part is devoted to simulating and testing battles with other agents."},{"metadata":{},"cell_type":"markdown","source":"We need to import the library for creating environments and simulating agent battles"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Upgrade kaggle_environments using pip before import\n!pip install -q -U kaggle_environments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a rock-paper-scissors environment (RPS), and set 1000 episodes for each simulation"},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\n    \"rps\", \n    configuration={\"episodeSteps\": 1000}\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take the agent that will copy our previous action from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_copy_opponent_path = \"../input/rock-paper-scissors-agents-comparison/copy_opponent.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start simulating the battle nash_equilibrium_agent vs copy_opponent_agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs copy_opponent_agent\nenv.run(\n    [\"submission.py\", agent_copy_opponent_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take the agent that will hit our previous action from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_reactionary_path = \"../input/rock-paper-scissors-agents-comparison/reactionary.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start simulating the battle nash_equilibrium_agent vs reactionary"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs reactionary\nenv.run(\n    [\"submission.py\", agent_reactionary_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simulating the battle between two nash_equilibrium_agent agents"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs nash_equilibrium_agent\nenv.run(\n    [\"submission.py\", \"submission.py\"]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take the agent that will always use Rock from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_rock_path = \"../input/rock-paper-scissors-agents-comparison/rock.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simulating the battle between nash_equilibrium_agent and rock"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs rock\nenv.run(\n    [\"submission.py\", agent_rock_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take the agent that will always use Scissors from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_scissors_path = \"../input/rock-paper-scissors-agents-comparison/scissors.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simulating the battle between nash_equilibrium_agent and scissors"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs scissors\nenv.run(\n    [\"submission.py\", agent_scissors_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let's take the agent that will always use Paper from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_paper_path = \"../input/rock-paper-scissors-agents-comparison/paper.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simulating the battle between nash_equilibrium_agent and paper"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs scissors\nenv.run(\n    [\"submission.py\", agent_paper_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let's take the agent that will hit self last actions from [Rock Paper Scissors - Agents Comparison](https://www.kaggle.com/ihelon/rock-paper-scissors-agents-comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"agent_hit_the_last_own_action_path = \"../input/rock-paper-scissors-agents-comparison/hit_the_last_own_action.py\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simulating the battle between nash_equilibrium_agent and agent_hit_the_last_own_action_path"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs scissors\nenv.run(\n    [\"submission.py\", agent_hit_the_last_own_action_path]\n)\n\nenv.render(mode=\"ipython\", width=500, height=400)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}