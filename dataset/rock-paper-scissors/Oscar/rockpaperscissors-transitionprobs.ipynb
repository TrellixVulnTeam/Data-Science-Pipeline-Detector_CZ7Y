{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install kaggle-environments --upgrade\nfrom kaggle_environments import make\nimport numpy as np\nimport tqdm as tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile submission.py\n\n#Bigram with backoff probability model\nimport random\nimport numpy as np\nimport math\nimport copy\nfrom itertools import product\n\nlast_choice = None\ntransitions = None\ncur_state = None\nstates = None\n\ndef get_states(order,vals):\n    if order == 1:\n        st = {str(i)+'-'+str(j):False for i in range(vals) for j in range(vals)}\n    else:\n        tmp = [str(i)+'-'+str(j) for i in range(vals) for j in range(vals)]\n        st = {'+'.join(c):False for c in product(tmp,repeat=order)}\n    return st\n\ndef agent(observation, configuration,method='indexes',lamb=3,memory=0.5,ngram=2,backoff=True):\n    global last_choice, transitions, cur_state,states\n    if observation.step == 0:\n        #Store which states have been seen\n        states = {}\n        for i in range(1,ngram+1):\n            states={**states,**(get_states(i,configuration.signs))}\n         #Populate the transition probabilities\n        transitions = dict()\n        for state in states:\n            transitions[state] = dict()\n            for i in range(configuration.signs):\n                transitions[state][i] = 1/configuration.signs\n        #Choose a random current state\n        cur_state = random.choice([s for s in list(states.keys()) if len(s.split('+'))==(ngram)])\n    else:\n        #Last opponent choice\n        last_oppo_choice = observation.lastOpponentAction\n        #Increase probability for last opponent choice, decrease for the rest\n        #For all possible states within history\n        for i in range(configuration.signs):\n            for j in range(0,ngram):\n                update_state = '+'.join(cur_state.split('+')[j:])\n                if i!=last_oppo_choice:\n                    transitions[update_state][i] = (0.0 * (1-memory)) + (transitions[update_state][i] * memory)\n                else:\n                    transitions[update_state][i] = (1.0 * (1-memory)) + (transitions[update_state][i] * memory)\n                states[update_state] = True\n        #Update current state\n        if ngram == 1:\n            cur_state = str(last_oppo_choice)+'-'+str(last_choice)\n        else:\n            cur_state = '+'.join(cur_state.split('+')[1:]) + '+' + str(last_oppo_choice)+'-'+str(last_choice)\n    prob_state = copy.deepcopy(cur_state)\n    #Do backoff if the whole history has not been seen\n    if backoff:\n        while not states[prob_state] and len(prob_state.split('+'))>1:\n            prob_state = '+'.join(prob_state.split('+')[1:])\n    if method == 'best':\n        #Just pick the most likely next opponent choice\n        next_oppo_choice = int(np.argsort([transitions[prob_state][i] for i in range(configuration.signs)])[-1])\n    elif method == 'indexes':\n        #Find the most likely next opponent choice, but use a exponential distribution\n        #to reduce predictability\n        sorted_vals = np.argsort([transitions[prob_state][i] for i in range(configuration.signs)])[::-1]\n        next_oppo_choice = int(sorted_vals[min(configuration.signs-1,math.floor(np.random.exponential(1/lamb)*configuration.signs))])\n    elif method == 'probs':\n        #Find the most likely next opponent choice, but weight based on actual probabilities of each choice\n        #Use exponential distribution\n        sorted_vals = np.argsort([transitions[prob_state][i] for i in range(configuration.signs)])[::-1]\n        sorted_probs = np.sort([transitions[prob_state][i] for i in range(configuration.signs)])[::-1]\n        rnd = min(0.999,np.random.exponential(1/lamb)) \n        next_oppo_choice = int(sorted_vals[np.where((np.cumsum(sorted_probs)/np.sum(sorted_probs))>=rnd)[0][0]])\n    #Our next choice is whatever beats the decision on the next opponent choice\n    next_choice = (next_oppo_choice + 1) % configuration.signs\n    last_choice = next_choice\n    return next_choice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile bigram.py\n\nimport random\nimport numpy as np\nimport math\nimport copy\nfrom itertools import product\n\nlast_choice = None\ntransitions = None\ncur_state = None\nstates = None\n\ndef get_states(order,vals):\n    if order == 1:\n        st = {str(i)+'-'+str(j):False for i in range(vals) for j in range(vals)}\n    else:\n        tmp = [str(i)+'-'+str(j) for i in range(vals) for j in range(vals)]\n        st = {'+'.join(c):False for c in product(tmp,repeat=order)}\n    return st\n\ndef agent(observation, configuration,method='indexes',lamb=3,memory=0.5,ngram=2,backoff=False):\n    global last_choice, transitions, cur_state,states\n    if observation.step == 0:\n        #Store which states have been seen\n        states = {}\n        for i in range(1,ngram+1):\n            states={**states,**(get_states(i,configuration.signs))}\n         #Populate the transition probabilities\n        transitions = dict()\n        for state in states:\n            transitions[state] = dict()\n            for i in range(configuration.signs):\n                transitions[state][i] = 1/configuration.signs\n        #Choose a random current state\n        cur_state = random.choice([s for s in list(states.keys()) if len(s.split('+'))==(ngram)])\n    else:\n        #Last opponent choice\n        last_oppo_choice = observation.lastOpponentAction\n        #Increase probability for last opponent choice, decrease for the rest\n        #For all possible states within history\n        for i in range(configuration.signs):\n            for j in range(0,ngram):\n                update_state = '+'.join(cur_state.split('+')[j:])\n                if i!=last_oppo_choice:\n                    transitions[update_state][i] = (0.0 * (1-memory)) + (transitions[update_state][i] * memory)\n                else:\n                    transitions[update_state][i] = (1.0 * (1-memory)) + (transitions[update_state][i] * memory)\n                states[update_state] = True\n        #Update current state\n        if ngram == 1:\n            cur_state = str(last_oppo_choice)+'-'+str(last_choice)\n        else:\n            cur_state = '+'.join(cur_state.split('+')[1:]) + '+' + str(last_oppo_choice)+'-'+str(last_choice)\n    prob_state = copy.deepcopy(cur_state)\n    #Do backoff if the whole history has not been seen\n    if backoff:\n        while not states[prob_state] and len(prob_state.split('+'))>1:\n            prob_state = '+'.join(prob_state.split('+')[1:])\n    if method == 'best':\n        #Just pick the most likely next opponent choice\n        next_oppo_choice = int(np.argsort([transitions[prob_state][i] for i in range(configuration.signs)])[-1])\n    elif method == 'indexes':\n        #Find the most likely next opponent choice, but use a exponential distribution\n        #to reduce predictability\n        sorted_vals = np.argsort([transitions[prob_state][i] for i in range(configuration.signs)])[::-1]\n        next_oppo_choice = int(sorted_vals[min(configuration.signs-1,math.floor(np.random.exponential(1/lamb)*configuration.signs))])\n    elif method == 'probs':\n        #Find the most likely next opponent choice, but weight based on actual probabilities of each choice\n        #Use exponential distribution\n        sorted_vals = np.argsort([transitions[prob_state][i] for i in range(configuration.signs)])[::-1]\n        sorted_probs = np.sort([transitions[prob_state][i] for i in range(configuration.signs)])[::-1]\n        rnd = min(0.999,np.random.exponential(1/lamb)) \n        next_oppo_choice = int(sorted_vals[np.where((np.cumsum(sorted_probs)/np.sum(sorted_probs))>=rnd)[0][0]])\n    #Our next choice is whatever beats the decision on the next opponent choice\n    next_choice = (next_oppo_choice + 1) % configuration.signs\n    last_choice = next_choice\n    return next_choice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile unigram.py\n\nimport random\nimport numpy as np\nimport math\nimport copy\nfrom itertools import product\n\nlast_choice = None\ntransitions = None\ncur_state = None\nstates = None\n\ndef get_states(order,vals):\n    if order == 1:\n        st = {str(i)+'-'+str(j):False for i in range(vals) for j in range(vals)}\n    else:\n        tmp = [str(i)+'-'+str(j) for i in range(vals) for j in range(vals)]\n        st = {'+'.join(c):False for c in product(tmp,repeat=order)}\n    return st\n\ndef agent(observation, configuration,method='indexes',lamb=3,memory=0.5,ngram=1,backoff=False):\n    global last_choice, transitions, cur_state,states\n    if observation.step == 0:\n        #Store which states have been seen\n        states = {}\n        for i in range(1,ngram+1):\n            states={**states,**(get_states(i,configuration.signs))}\n         #Populate the transition probabilities\n        transitions = dict()\n        for state in states:\n            transitions[state] = dict()\n            for i in range(configuration.signs):\n                transitions[state][i] = 1/configuration.signs\n        #Choose a random current state\n        cur_state = random.choice([s for s in list(states.keys()) if len(s.split('+'))==(ngram)])\n    else:\n        #Last opponent choice\n        last_oppo_choice = observation.lastOpponentAction\n        #Increase probability for last opponent choice, decrease for the rest\n        #For all possible states within history\n        for i in range(configuration.signs):\n            for j in range(0,ngram):\n                update_state = '+'.join(cur_state.split('+')[j:])\n                if i!=last_oppo_choice:\n                    transitions[update_state][i] = (0.0 * (1-memory)) + (transitions[update_state][i] * memory)\n                else:\n                    transitions[update_state][i] = (1.0 * (1-memory)) + (transitions[update_state][i] * memory)\n                states[update_state] = True\n        #Update current state\n        if ngram == 1:\n            cur_state = str(last_oppo_choice)+'-'+str(last_choice)\n        else:\n            cur_state = '+'.join(cur_state.split('+')[1:]) + '+' + str(last_oppo_choice)+'-'+str(last_choice)\n    prob_state = copy.deepcopy(cur_state)\n    #Do backoff if the whole history has not been seen\n    if backoff:\n        while not states[prob_state] and len(prob_state.split('+'))>1:\n            prob_state = '+'.join(prob_state.split('+')[1:])\n    if method == 'best':\n        #Just pick the most likely next opponent choice\n        next_oppo_choice = int(np.argsort([transitions[prob_state][i] for i in range(configuration.signs)])[-1])\n    elif method == 'indexes':\n        #Find the most likely next opponent choice, but use a exponential distribution\n        #to reduce predictability\n        sorted_vals = np.argsort([transitions[prob_state][i] for i in range(configuration.signs)])[::-1]\n        next_oppo_choice = int(sorted_vals[min(configuration.signs-1,math.floor(np.random.exponential(1/lamb)*configuration.signs))])\n    elif method == 'probs':\n        #Find the most likely next opponent choice, but weight based on actual probabilities of each choice\n        #Use exponential distribution\n        sorted_vals = np.argsort([transitions[prob_state][i] for i in range(configuration.signs)])[::-1]\n        sorted_probs = np.sort([transitions[prob_state][i] for i in range(configuration.signs)])[::-1]\n        rnd = min(0.999,np.random.exponential(1/lamb)) \n        next_oppo_choice = int(sorted_vals[np.where((np.cumsum(sorted_probs)/np.sum(sorted_probs))>=rnd)[0][0]])\n    #Our next choice is whatever beats the decision on the next opponent choice\n    next_choice = (next_oppo_choice + 1) % configuration.signs\n    last_choice = next_choice\n    return next_choice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate against the unigram baseline\nenv = make(\"rps\", debug=True)\nresults = []\nfor i in tqdm.tqdm(range(50)):\n    env.reset()\n    state = env.run([\"submission.py\",\"unigram.py\"])\n    results.append(state[-1][0]['observation']['reward'])\n#env.render(mode=\"ipython\", width=400, height=350)\nprint('Victories: {0:d} of {1:d}'.format(sum([1 for r in results if r>0]),len(results)))\nprint('Average winning result: {0:.0f}'.format(np.mean([r for r in results if r>0])))\nprint('Average winning result: {0:.0f}'.format(np.mean([r for r in results if r<0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluate against the bigram baseline\nenv = make(\"rps\", debug=True)\nresults = []\nfor i in tqdm.tqdm(range(50)):\n    env.reset()\n    state = env.run([\"submission.py\",\"bigram.py\"])\n    results.append(state[-1][0]['observation']['reward'])\n#env.render(mode=\"ipython\", width=400, height=350)\nprint('Victories: {0:d} of {1:d}'.format(sum([1 for r in results if r>0]),len(results)))\nprint('Average winning result: {0:.0f}'.format(np.mean([r for r in results if r>0])))\nprint('Average winning result: {0:.0f}'.format(np.mean([r for r in results if r<0])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}