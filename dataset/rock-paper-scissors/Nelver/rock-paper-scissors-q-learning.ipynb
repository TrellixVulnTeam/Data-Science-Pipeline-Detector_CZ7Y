{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%writefile submission.py\n\nimport numpy as np\nimport random\n\ndef determine_winner(you,opponent):\n    winning_situations = [[0,2],[2,1],[1,0]]\n    if [you,opponent] in winning_situations:\n        return 1\n    elif you == opponent:\n        return 0\n    else:\n        return -1\n\n\nQ = np.zeros((9, 3))\nalpha = 0.7004648211071717 \nalpha_decay = 1\ndiscount = 0.31635680630654883\nepsilon = 0.8254597834978199\nepsilon_decay = 0.9999999912474902\n\nSTATES = {(0, 0): 0,\n          (0, 1): 1,\n          (0, 2): 2,\n          (1, 0): 3,\n          (1, 1): 4,\n          (1, 2): 5,\n          (2, 0): 6,\n          (2, 1): 7,\n          (2, 2): 8}\n\ncurrent_state = 0\ncurrent_action = 0\n\n\ndef move(observation):\n    global current_state\n    global current_action\n    global STATES\n    global discount\n    global alpha\n    global Q\n    global epsilon\n    global epsilon_decay\n    global alpha_decay\n    \n    if observation.step == 0:\n        current_action = int(np.random.randint(0,3))\n        return current_action\n    elif observation.step == 1:\n        current_state = STATES[(current_action,observation.lastOpponentAction)]\n        \n        if epsilon > random.uniform(0,1):\n            current_action = int(np.random.randint(0,3))\n            return current_action\n        else:\n            current_action = int(Q[current_state,:].argmax())\n            return current_action\n        \n        return current_action \n    else:\n        reward = determine_winner(current_action,observation.lastOpponentAction)\n        next_state = STATES[(current_action,observation.lastOpponentAction)]\n        \n        discounted_next_state = alpha*(reward+\n                                       discount*Q[next_state,Q[next_state,:].argmax()] - \n                                       Q[current_state,current_action])\n        \n        Q[current_state,current_action] = Q[current_state,current_action] + discounted_next_state\n        current_state = STATES[(current_action,observation.lastOpponentAction)]\n        \n        \n        \n        if epsilon > random.uniform(0,1):\n            current_action = int(np.random.randint(0,3))\n        else:\n            current_action = int(Q[current_state,:].argmax())\n         \n        alpha*=alpha_decay\n        epsilon*=epsilon_decay\n        return current_action\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}