{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Simple Run Test\n\nMy agents kept producing errors, so this is a very basic setup to test them.\n\nThe current agent is defined below:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%writefile test_agent.py\n\nimport random\n\nROCK, PAPER, SCISSORS = 0, 1, 2\nOPTIONS = (ROCK, PAPER, SCISSORS)\n\ndef make_choice(step, score, oppo, mine):\n    if step == 0:\n        return random.choice(OPTIONS)\n    if step == 1:\n        # expect different sign than last time\n        return oppo[-1] + 2\n    # default to random choice\n    return random.choice(OPTIONS)\n\nop_choices = []\nmy_choices = []\nmy_score = 0\n\ndef agent_call(observation, configuration):\n    global op_choices\n    global my_choices\n    global my_score\n    # log last rounds opponent choice before making decision\n    if observation.step > 0 and observation.lastOpponentAction in OPTIONS:\n        op_choices.append(observation.lastOpponentAction)\n        if my_choices[-1] == (op_choices[-1]+1)%3:\n            my_score += 1\n        elif my_choices[-1] == (op_choices[-1]-1)%3:\n            my_score -= 1\n        elif my_choices[-1] != op_choices[-1]:\n            print(my_choices[-1], op_choices[-1])\n            raise Exception(\"Tie but not the same choice!\")\n    choice = make_choice(observation.step, my_score, op_choices, my_choices)\n    choice %= 3 # allows to add 1 to an option to get the beating option\n    my_choices.append(choice)\n    return choice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A simple benchmark agent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile copy_opponent.py\n    \ndef copy_opponent_agent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from kaggle_environments import make\nenv = make(\"rps\", configuration={\"episodeSteps\": 1000})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"# make a test run and show logs for debugging\nenv.reset()\nenv.run([\"test_agent.py\", \"copy_opponent.py\"])\nfor entry in env.logs:\n    for subentry in entry:\n        for key,val in subentry.items():\n            print(key, ':')\n            print(val)\n        print(80*'-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# see rendered outcome of test run\nenv.render(mode=\"ipython\", width=550, height=500)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}