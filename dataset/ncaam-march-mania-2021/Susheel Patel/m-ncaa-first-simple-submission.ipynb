{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Before I start off\n\nKudos to @theoviel (https://www.kaggle.com/theoviel/ncaa-starter-the-simpler-the-better) for the reference code notebook.\n\nI am trying to learnhow to solve these problems and these simple implementations really help me practice my python and help me in implementation in case I get lost","metadata":{}},{"cell_type":"markdown","source":"# Initial setup\n\nKaggle setup for listing directories and files available","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing important libraries\n\nImport statements for all libraries used throughout the code","metadata":{}},{"cell_type":"code","source":"# Importing required libraries\n\nimport os\nimport re\nimport warnings\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom itertools import product, combinations\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import f1_score # Will use for validation of our model by prediction throgh various models\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set useful options \n\nSetting display options for pandas dataframe to make visualization easier in notebook","metadata":{}},{"cell_type":"code","source":"# Setting important options to make visualization easier\n\npd.set_option('display.max_rows', 400)\npd.set_option('display.max_columns', 160)\npd.set_option('display.max_colwidth', 40)\npd.set_option('display.max_columns', None)\n\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing datasets\n\nImport required datasets for stage 1 competition\n\nWe will mainly focus on results and seeds for base set of predictions\n","metadata":{}},{"cell_type":"code","source":"base_path = \"/kaggle/input/ncaam-march-mania-2021/MDataFiles_Stage1/\"\n\nfor filename in os.listdir(base_path):\n    print(filename)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import basic datasets to get started on a base model\n# We will skip using the regular tournamen data for the base model\n# Teams.csv, MSeasons.csv, MNCAATourneySeeds.csv, MRegularSeasonCompactResults.csv, MNCAATourneyCompactResults.csv, MSampleSubmissionStage1.csv\n\nteams = pd.read_csv(base_path + \"MTeams.csv\")\nseasons = pd.read_csv(base_path + \"MSeasons.csv\")\nseeds = pd.read_csv(base_path + \"MNCAATourneySeeds.csv\")\nresults = pd.read_csv(base_path + \"MNCAATourneyCompactResults.csv\")\nregular_results = pd.read_csv(base_path + \"MRegularSeasonCompactResults.csv\")\n\nsample = pd.read_csv(base_path + \"MSampleSubmissionStage1.csv\")\n\nlist_datasets = [teams, seasons, seeds, results,regular_results, sample]\n\nstring_list_datasets = ['teams', 'seasons', 'seeds', 'results', 'regular_results','sample']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the datasets\n\nPrint top 5 rows to get general idea of datasets\n\nAlso, generate summary of datasets for few preliminary observations","metadata":{}},{"cell_type":"code","source":"for i, dataset in enumerate(list_datasets):\n    print(string_list_datasets[i])\n    print(\"\\n\")\n    print(dataset.head())\n    print(\"\\n\")\n    print(dataset.tail())\n    print(\"\\n\\n\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, dataset in enumerate(list_datasets):\n    print(string_list_datasets[i])\n    display(dataset.describe())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset comments\n\n0. regular_results:\n    - has non-NCAA results starting from 1985 until 2019\n    - We can use this to create few metrics that can support our model. All features can be siilar to ones made for NCAA as data format is same\n    - Wloc and NumOT can be safely removed as it might create significant bias in model and we have no way of predicting if some team will run into an overtime\n1. results:\n    - has NCAA results starting from 1985 until 2019\n    - We can directly use this to evaluate our model since task is to predict 2015 to 2019 matches\n    - Wloc and NumOT can be safely removed as it might create significant bias in model and we have no way of predicting if some team will run into an overtime\n2. teams:\n    - Has Team Ids and their respecitve years for first tournament match played\n    - Can be used to calculate age of teams. It might help our model to factor in experience of the team into results\n\n3. seasons:\n    - Has start dates and region areas mentioned\n    - We can safely ignore this dataset as results already have a standardized date variable\n\n4. Seeds:\n    - Has the seed information for the teams per season.\n    - Since it directly correlates with the round of play, we may provide it as a factor which can account for player exhaustion\n\n5. sample:\n    - Sample submission.\n    - We can use this file as base for creating our model dataframe and test models on it","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing and creating base lists\n\nWe will be using datasets to understand participating teams, regions, etc and creating submission file based on all possible team combinations for predictions","metadata":{}},{"cell_type":"markdown","source":"## Base list creation\n\nWe need to predict for 2015 to 2019 seasons.\nWe can create a dataset containing all years, all team combinations on which we can keep adding features","metadata":{}},{"cell_type":"markdown","source":"all_teams = teams['TeamID'].unique()\nall_seeds = seeds['Seed'].unique()\nall_years = results['Season'].unique()\n\nprint(\"# of seeds: n = %d\\n\" % len(all_seeds))\n\nprint(\"# of teams: n = %d\\n\" % len(all_teams))\n\nprint(\"years: %d to %d \\n\" % (min(all_years),max(all_years)))\n\n# Create base file with IDs in required format\n\nbase_team_matchups =list(combinations(all_teams, 2))\n\nprint(\"Combinations array: \\n\")\nprint(base_team_matchups[1:10])","metadata":{"trusted":true}},{"cell_type":"code","source":"all_teams = teams['TeamID'].unique()\nall_seeds = seeds['Seed'].unique()\nall_years = results['Season'].unique()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA and feature creation\n\nMain focus is on using results table to generate basic variables\n\nFollowing variables will be created:\n1. Avg. wins/loss per team per season\n2. Avg. Score for win/lose team per team per season\n3. Avg. Score gap","metadata":{}},{"cell_type":"markdown","source":"## Generate similar variables based on regular season results\n\nWe will use dtaa for matches played before actual NCAA tournaments for computing similar features","metadata":{}},{"cell_type":"code","source":"# We will focus on just 1 main dataset.. i.e. results dataset for first pass at baseline\n\ndf = regular_results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a score gap variable by Winning score - Losing score","metadata":{}},{"cell_type":"code","source":"# Create a score gap variable\n\ndf['ScoreGap'] = df['WScore'] - df['LScore']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function for quick rollups","metadata":{}},{"cell_type":"code","source":"def rollup_df(base_df,rollup_cols,aggregation):\n    temp = base_df.groupby(rollup_cols).agg(aggregation)\n    temp = temp.reset_index()\n    return temp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get # of wins, avg score and avg. score gap for eac set of winnng and losing teams","metadata":{}},{"cell_type":"code","source":"# Get number of wins and losses per year per team\n\nnum_wins = rollup_df(df,['Season','WTeamID'],{'DayNum':'count','WScore':'mean','ScoreGap':'mean'})\n\nnum_wins = num_wins.rename(columns = {'WTeamID':'TeamID','DayNum':'num_wins','WScore':'WScore_avg','ScoreGap':'WScoreGap_avg'})\n\nnum_loss = rollup_df(df,['Season','LTeamID'],{'DayNum':'count','LScore':'mean','ScoreGap':'mean'})\n\nnum_loss = num_loss.rename(columns = {'LTeamID':'TeamID','DayNum':'num_loss','LScore':'LScore_avg','ScoreGap':'LScoreGap_avg'})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create base dataframe with all teams and the created variables\n\nWe remove distinction b/w winning and losing teams and join all features.\nSince same team can have won and lost match, we can deduplicate to ensure we have 1 entry per team per season\n","metadata":{}},{"cell_type":"code","source":"# Create set with all possible combos of season and teams\n\ndf_feat_merged = num_wins[['Season','TeamID']].append(num_loss[['Season','TeamID']])\n\ndf_feat_merged = df_feat_merged.drop_duplicates().reset_index().drop(['index'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Join previously created features for each team\n\nHere, we make a single consistent dataset with appropriate wins/loss values attached to each team","metadata":{}},{"cell_type":"code","source":"df_feat_1 = pd.merge(df_feat_merged,num_wins, on = ['Season','TeamID'], how = 'left')\ndf_feat_2 = pd.merge(df_feat_1,num_loss, on = ['Season','TeamID'], how = 'left')\n\ndf_feat_2 = df_feat_2.fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_feat_2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Computing features based on wins and losses\n\nWe compute a ratio of # of wins for the team in the specific season\n\nAlso, we will average out the score gap by using # of wins and win score gap (vice versa for losing score) and establish a general score gap team establishes in each season","metadata":{}},{"cell_type":"code","source":"df_feat_final = df_feat_2\n\ndf_feat_final['WinRatio'] = df_feat_final['num_wins']/(df_feat_final['num_wins'] + df_feat_final['num_loss'])\n\ndf_feat_final['total_win_gap'] = df_feat_final['WScore_avg']* df_feat_final['num_wins']\n\ndf_feat_final['total_lose_gap'] = df_feat_final['LScore_avg']* df_feat_final['num_loss']\n                                  \ndf_feat_final['ScoreGapAvg'] = (df_feat_final['total_win_gap'] - df_feat_final['total_lose_gap'])/(df_feat_final['num_wins'] + df_feat_final['num_loss']) \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop columns from dataset\n\nWe will drop few columns which cannot be considered as features for final predictions.\ni.e. num_wins, num_loss, Score Gaps","metadata":{}},{"cell_type":"code","source":"df_feat_final = df_feat_final.drop(['num_wins','num_loss','total_win_gap','total_lose_gap','WScoreGap_avg','LScoreGap_avg'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_feat_final.head(5)\ndf_feat_final.tail(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_reg_season_feat = df_feat_final.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using NCAA compact results to generate features based on the tournament","metadata":{}},{"cell_type":"code","source":"df = results\n\ndf = df.drop(columns = ['WLoc','NumOT'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Join match seeds as we will use this dataset as base for our predictions","metadata":{}},{"cell_type":"code","source":"df = pd.merge(\n    df,\n    seeds,\n    how = 'left',\n    left_on=['Season','WTeamID'],\n    right_on = ['Season','TeamID']).drop(columns = ['TeamID'],axis = 1).rename(columns = {'Seed':'WSeed'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(\n    df,\n    seeds,\n    how = 'left',\n    left_on=['Season','LTeamID'],\n    right_on = ['Season','TeamID']\n    ).drop(columns = ['TeamID'],axis = 1).rename(columns = {'Seed':'LSeed'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove the regions from seeds. We wont need the regions in seeds for our model","metadata":{}},{"cell_type":"code","source":"def clean_seed(seed):\n    return int(re.sub(\"[^0-9]\",\"\",seed))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['LSeed'] = df['LSeed'].apply(clean_seed)\ndf['WSeed'] = df['WSeed'].apply(clean_seed)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_reg_season_feat.head()\ndf_reg_season_feat.columns.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merge the NCAA results with season features we made previously","metadata":{}},{"cell_type":"code","source":"df = pd.merge(df,\n             df_reg_season_feat,\n             how = 'left',\n             left_on = ['Season','WTeamID'],\n             right_on = ['Season','TeamID'],\n             ).rename(columns = \n                      {'WScore_avg':'WScore_avg_W', \n                       'LScore_avg':'LScore_avg_W',\n                       'WinRatio':'WinRatio_W',\n                       'ScoreGapAvg':'ScoreGapAvg_W'}\n                     ).drop(['TeamID'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(df,\n             df_reg_season_feat,\n             how = 'left',\n             left_on = ['Season','WTeamID'],\n             right_on = ['Season','TeamID'],\n             ).rename(columns = \n                      {'WScore_avg':'WScore_avg_L', \n                       'LScore_avg':'LScore_avg_L',\n                       'WinRatio':'WinRatio_L',\n                       'ScoreGapAvg':'ScoreGapAvg_L'}\n                     ).drop(['TeamID'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since our data only has winning teams as base, we need to create a general dataset which has winning and losing teams both in same columns.\n\nWe will duplicate the dataframe and swap winning and losing rows for this","metadata":{}},{"cell_type":"code","source":"# Rename winning team identifiers as A and losing as B\nwin_rename = {'WTeamID':'TeamID_A',\n              'WScore':'Score_A',\n              'LTeamID':'TeamID_B',\n              'LScore':'Score_B',\n              'WSeed':'Seed_A',\n              'LSeed':'Seed_B',\n              'WScore_avg_W':'WScore_avg_A',\n              'LScore_avg_W':'LScore_avg_A',\n              'WinRatio_W':'WinRatio_A',\n              'ScoreGapAvg_W':'ScoreGapAvg_A',\n              'WScore_avg_L':'WScore_avg_B',\n              'LScore_avg_L':'LScore_avg_B',\n              'WinRatio_L':'WinRatio_B',\n              'ScoreGapAvg_L':'ScoreGapAvg_B'}\n\n# Rename losing team identifiers as A and winning as B\nlose_rename = {'WTeamID':'TeamID_B',\n              'WScore':'Score_B',\n              'LTeamID':'TeamID_A',\n              'LScore':'Score_A',\n              'WSeed':'Seed_B',\n              'LSeed':'Seed_A',\n              'WScore_avg_W':'WScore_avg_B',\n              'LScore_avg_W':'LScore_avg_B',\n              'WinRatio_W':'WinRatio_B',\n              'ScoreGapAvg_W':'ScoreGapAvg_B',\n              'WScore_avg_L':'WScore_avg_A',\n              'LScore_avg_L':'LScore_avg_A',\n              'WinRatio_L':'WinRatio_A',\n              'ScoreGapAvg_L':'ScoreGapAvg_A'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"win_df = df.copy()\nwin_df = win_df.rename(columns = win_rename)\n\nlose_df = df.copy()\nlose_df = lose_df.rename(columns = lose_rename)\n\nfinal_df = pd.concat([win_df,lose_df], axis = 0).reset_index().drop(columns = ['index'])\n\nfinal_df['Win_A'] = final_df.apply(lambda x: 1 if x['Score_A'] > x['Score_B'] else 0, axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### COmpute difference b/w team A and team B\n\nWe will compute difference feature as it will help us assess if A is worse/better than B","metadata":{}},{"cell_type":"code","source":"final_df['SeedDiff'] = final_df['Seed_A'] - final_df['Seed_B']\n\nfinal_df['ScoreGapDiff'] = final_df['ScoreGapAvg_A'] - final_df['ScoreGapAvg_B']\n\nfinal_df['WinRatioDiff'] = final_df['WinRatio_A'] - final_df['WinRatio_B']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build test/submission dataset","metadata":{}},{"cell_type":"code","source":"sample.head()\n\ndf_test = sample.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split out the team ID and Season year","metadata":{}},{"cell_type":"code","source":"df_test['Season'] = df_test['ID'].apply(lambda x: int(x.split('_')[0])) \n\ndf_test['TeamID_A'] = df_test['ID'].apply(lambda x: int(x.split('_')[1])) \n\ndf_test['TeamID_B'] = df_test['ID'].apply(lambda x: int(x.split('_')[2])) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Join match Seeds","metadata":{}},{"cell_type":"code","source":"df_test = pd.merge(df_test,\n                  seeds,\n                  how = 'left',\n                  left_on = ['Season','TeamID_A'],\n                  right_on = ['Season','TeamID']\n                  ).rename(columns = {'Seed':'Seed_A'}).drop(['TeamID'], axis = 1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.merge(df_test,\n                  seeds,\n                  how = 'left',\n                  left_on = ['Season','TeamID_B'],\n                  right_on = ['Season','TeamID']\n                  ).rename(columns = {'Seed':'Seed_B'}).drop(['TeamID'], axis = 1) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Clean seeds","metadata":{}},{"cell_type":"code","source":"df_test['Seed_A'] = df_test['Seed_A'].apply(clean_seed)\ndf_test['Seed_B'] = df_test['Seed_B'].apply(clean_seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['SeedDiff'] = df_test['Seed_A'] - df_test['Seed_B']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Join Season stats","metadata":{}},{"cell_type":"code","source":"df_test = pd.merge(df_test,\n             df_reg_season_feat,\n             how = 'left',\n             left_on = ['Season','TeamID_A'],\n             right_on = ['Season','TeamID'],\n             ).rename(columns = \n                      {'WScore_avg':'WScore_avg_A', \n                       'LScore_avg':'LScore_avg_A',\n                       'WinRatio':'WinRatio_A',\n                       'ScoreGapAvg':'ScoreGapAvg_A'}\n                     ).drop(['TeamID'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.merge(df_test,\n             df_reg_season_feat,\n             how = 'left',\n             left_on = ['Season','TeamID_B'],\n             right_on = ['Season','TeamID'],\n             ).rename(columns = \n                      {'WScore_avg':'WScore_avg_B', \n                       'LScore_avg':'LScore_avg_B',\n                       'WinRatio':'WinRatio_B',\n                       'ScoreGapAvg':'ScoreGapAvg_B'}\n                     ).drop(['TeamID'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['SeedDiff'] = df_test['Seed_A'] - df_test['Seed_B']\n\ndf_test['ScoreGapDiff'] = df_test['ScoreGapAvg_A'] - df_test['ScoreGapAvg_B']\n\ndf_test['WinRatioDiff'] = df_test['WinRatio_A'] - df_test['WinRatio_B']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validate model via k-fold validations \n\nWe use older seasons to predict model output for next season and check score ","metadata":{}},{"cell_type":"code","source":"print(final_df.columns.values)\n\nprint(final_df.Season.unique())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [\n    'Seed_A',\n    'Seed_B',\n    'WinRatio_A',\n    'ScoreGapAvg_A',\n    'WinRatio_B',\n    'ScoreGapAvg_B',\n    'SeedDiff',\n    'WinRatioDiff',\n    'ScoreGapDiff']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating copies of required datasets to make it easier to process","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df\ndf_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seasons = final_df.Season.unique()    \n\nf_score = [] # Store the season and the f-score for predictions.\npredictions = [] # Store the predictions on test set based on all intermediate models. We will average the predictions\n\nSEED = 11\n\nfor season_yr in seasons[10:]:\n    \n    model = LogisticRegression(C = 10, random_state=SEED)\n    \n    std_scaler = StandardScaler()\n    \n    train_X = final_df.loc[final_df['Season'] < season_yr,features]\n    val_X = final_df.loc[final_df['Season'] == season_yr,features]\n    \n    train_y = final_df.loc[final_df['Season'] < season_yr, ['Win_A']]\n    val_y = final_df.loc[final_df['Season'] == season_yr, ['Win_A']]\n    \n    train_X = std_scaler.fit_transform(train_X)\n    val_X = std_scaler.transform(val_X)\n    test_X = std_scaler.transform(df_test[features])\n    \n    model.fit(train_X, train_y)\n    \n    pred_y = model.predict(val_X)\n    \n    f_score.append([season_yr, f1_score(val_y, pred_y)])\n    \n    test_y = model.predict_proba(test_X)[:,1]\n    \n    predictions.append(test_y)\n    \npredictions = np.vstack(predictions).transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_df = pd.DataFrame(predictions)\n\nexp_df.to_csv('./temp.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f_score\n#pred_y\n#predictions.shape\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = np.mean(predictions,axis = 1)\nsubmission = df_test.loc[:,['ID']]\n\nsubmission['Pred'] = results\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}