{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import csv\nimport math\nimport re\nfrom math import *\nimport numpy as np \nimport pandas as pd \nimport pickle\n\nimport category_encoders as ce\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor, Pool\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split, KFold, StratifiedKFold\n\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer, MissingIndicator\n\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.linear_model import LogisticRegression, BayesianRidge, RidgeClassifier, RidgeClassifierCV, TheilSenRegressor, HuberRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split, KFold, StratifiedKFold\n\nfrom sklearn.metrics import roc_auc_score, auc, accuracy_score, mean_squared_error\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier \nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor \n#from from sklearn.ensemble  VoltingClassifier\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, SGDRegressor, ElasticNet, ElasticNetCV, Lars\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!ls /kaggle/input/ncaam-march-mania-2021","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FILE_TRAIN_2015 = \"google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MEvents2015.csv\"\nFILE_TRAIN_2016 = \"google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MEvents2016.csv\"\nFILE_TRAIN_2017 = \"google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MEvents2017.csv\"\nFILE_TRAIN_2018 = \"google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MEvents2018.csv\"\nFILE_TRAIN_2019 = \"google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MEvents2019.csv\"\nFILE_TEST = \"google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MSampleSubmissionStage1_2020.csv\"\n\nNAN_STRING_TO_REPLACE = 'zz'\nNAN_VALUE_FLOAT = 8888.0\nNAN_VALUE_INT = 8888\nNAN_VALUE_STRING = '8888'\n\nBATCH_SIZE = 1000\nEPOCHS = 5\nN_NEURONS = 10\n\nSEED = 8888\nSPLITS = 5\n\nSMOOTHING = 0.2\nOTHER_NAN = 0\n\nIMPUTING_STRATEGY = 'median'\n\nPARAMS_CATBOOST = dict()\n# PARAMS_CATBOOST['logging_level'] = 'Silent'\nPARAMS_CATBOOST['eval_metric'] = 'Logloss'\nPARAMS_CATBOOST['custom_metric'] = 'Logloss'\nPARAMS_CATBOOST['loss_function'] = 'Logloss'\nPARAMS_CATBOOST['iterations'] = 125 # best 125\nPARAMS_CATBOOST['od_type'] = 'Iter' # IncToDec, Iter\nPARAMS_CATBOOST['random_seed'] = SEED\nPARAMS_CATBOOST['learning_rate'] = 0.003 # alpha, default 0.03 if no l2_leaf_reg\nPARAMS_CATBOOST['task_type'] = 'CPU'\nPARAMS_CATBOOST['use_best_model']: True\nPARAMS_CATBOOST['l2_leaf_reg'] = 3.0 # lambda, default 3, S: 300\n\nPARAM_LGB = dict()\nPARAM_LGB['metric'] = 'rmse'\nPARAM_LGB['objective'] = 'regression'\n\n'''\nPARAMS_CATBOOST['depth'] = 3\nPARAMS_CATBOOST['bagging_temperature'] = 0.8\nPARAMS_CATBOOST['random_strength']: 0.8\nPARAMS_CATBOOST['bootstrap_type'] = 'Bayesian'\nPARAMS_CATBOOST['nan_mode'] = 'Min'\nPARAMS_CATBOOST['thread_count'] = 4\nPARAMS_CATBOOST['l2_leaf_reg'] = 300 # lambda, default 3\n'''\n\nPARAMS_CATBOOST_REGRESSOR = dict()\n#PARAMS_CATBOOST_REGRESSOR['logging_level'] = 'Silent'\nPARAMS_CATBOOST_REGRESSOR['eval_metric'] = 'RMSE'\nPARAMS_CATBOOST_REGRESSOR['custom_metric'] = 'RMSE'\nPARAMS_CATBOOST_REGRESSOR['loss_function'] = 'RMSE'\nPARAMS_CATBOOST_REGRESSOR['iterations'] = 100\nPARAMS_CATBOOST_REGRESSOR['od_type'] = 'Iter' # IncToDec, Iter\nPARAMS_CATBOOST_REGRESSOR['random_seed'] = SEED\nPARAMS_CATBOOST_REGRESSOR['learning_rate'] = 0.1 # alpha, default 0.03 if no l2_leaf_reg\nPARAMS_CATBOOST_REGRESSOR['task_type'] = 'CPU'\nPARAMS_CATBOOST_REGRESSOR['use_best_model']: True\nPARAMS_CATBOOST_REGRESSOR['l2_leaf_reg'] = 50.0 # lambda, default 3, S: 300\n\n'''\nPARAMS_CATBOOST_REGRESSOR['depth'] = 3\nPARAMS_CATBOOST_REGRESSOR['bagging_temperature'] = 0.8\nPARAMS_CATBOOST_REGRESSOR['random_strength']: 0.8\nPARAMS_CATBOOST_REGRESSOR['bootstrap_type'] = 'Bayesian'\nPARAMS_CATBOOST_REGRESSOR['nan_mode'] = 'Min'\nPARAMS_CATBOOST_REGRESSOR['thread_count'] = 4\nPARAMS_CATBOOST_REGRESSOR['l2_leaf_reg'] = 300 # lambda, default 3\n'''\n\nPARAMS_XGB = dict()\nPARAMS_XGB['objective']='binary:logistic'\nPARAMS_XGB['eval_metric'] = 'mae'\nPARAMS_XGB['booster'] = 'gbtree'\nPARAMS_XGB['eta'] = 0.02\nPARAMS_XGB['subsample'] = 0.35\nPARAMS_XGB['colsample_bytree'] = 0.7\nPARAMS_XGB['num_parallel_tree'] = 10\nPARAMS_XGB['min_child_weight'] = 40\nPARAMS_XGB['gamma'] = 10\nPARAMS_XGB['max_depth'] = 3\n\n\nW_FEATURES = [\n    'WTeamID', \n    'WFGM', \n    'WFGA', \n    'WFGM3', \n    'WFGA3', \n    'WFTM', \n    'WFTA', \n    'WOR', \n    'WDR', \n    'WAst', \n    'WTO', \n    'WStl', \n    'WBlk', \n    'WPF', \n    'WScore', \n    'Final_WTeam', \n    #'Semi_Final_WTeam', \n    'WTeam_W_count', \n    'WScore_mean',\n    'WScore_median', \n    'WScore_sum',\n    'WTeam_Seed',\n    'WTeam_PerCent',\n    'Diff_WTeam',\n    'WFGA_min', \n    #'WFGA_max', \n    'WFGA_mean',\n    'WFGA_median'\n    #'WAst_mean',\n    #'WBlk_mean'\n]\n\nL_FEATURES = [\n    'LTeamID', \n    'LFGM', \n    'LFGA', \n    'LFGM3', \n    'LFGA3', \n    'LFTM', \n    'LFTA', \n    'LOR', \n    'LDR', \n    'LAst', \n    'LTO', \n    'LStl', \n    'LBlk', \n    'LPF', \n    'LScore',\n    'Final_LTeam', \n    #'Semi_Final_LTeam', \n    'LTeam_L_count', \n    'LScore_mean',  \n    'LScore_median', \n    'LScore_sum',\n    'LTeam_Seed',\n    'LTeam_PerCent',\n    'Diff_LTeam',\n    'LFGA_min', \n    #'LFGA_max', \n    'LFGA_mean', \n    'LFGA_median'\n    #'LAst_mean',\n    #'LBlk_mean'\n]\n\nW_FEATURES_1 = [\n    'Final_WTeam', \n    #'Semi_Final_WTeam', \n    'WTeam_W_count', \n    'WScore_mean',\n    'WScore_median', \n    'WScore_sum',\n    'WTeam_Seed',\n    'WTeam_PerCent',\n    'Diff_WTeam',\n    'WFGA_min', \n    #'WFGA_max', \n    'WFGA_mean', \n    'WFGA_median'\n    #'WAst_mean',\n    #'WBlk_mean'\n   \n]\nL_FEATURES_1 = [\n    'Final_LTeam', \n    #'Semi_Final_LTeam', \n    'LTeam_L_count', \n    'LScore_mean',  \n    'LScore_median', \n    'LScore_sum',\n    'LTeam_Seed',\n    'LTeam_PerCent',\n    'Diff_LTeam',\n    'LFGA_min', \n    #'LFGA_max', \n    'LFGA_mean', \n    'LFGA_median'\n    #'LAst_mean',\n    #'LBlk_mean'\n]\n\nlgb_params = {'objective': 'binary',\n              'metric': 'binary_logloss',\n              'boosting': 'gbdt',\n              'num_leaves': 32,\n              'feature_fraction': 0.6,\n              'bagging_fraction': 0.6,\n              'bagging_freq': 5,\n              'learning_rate': 0.05\n}\n\nxgb_params = {'max_depth':63,\n              #'objective':'binary:logistic',\n              'objective':'reg:squarederror',\n              'min_child_weight': 10,\n              'learning_rate': 0.0001,\n              'eta'      :0.3,\n              'subsample':0.8,\n              #'lambda '  :4,\n              'eval_metric':'logloss',\n              #'n_estimators':2000,\n              #'colsample_bytree ':0.9,\n              'colsample_bylevel':1\n              }\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n'''\nDescription: Read Data from CSV file into Pandas DataFrame\n'''\ndef read_data(inFile, sep=','):\n    df_op = pd.read_csv(filepath_or_buffer=inFile, low_memory=False, encoding='utf-8', sep=sep)\n    return df_op\n\n'''\nDescription: Write Pandas DataFrame into CSV file\n'''\ndef write_data(df, outFile):\n    f = open(outFile+'.csv', 'w')\n    r = df.to_csv(index=False, path_or_buf=f)\n    f.close()\n\ndef print_submission_into_file(y_pred, df_test_id, algo=\"\"):\n    l = []\n    for myindex in range(y_pred.shape[0]):\n        Y0 = y_pred[myindex]\n        l.insert(myindex, Y0)\n    \n    df_pred = pd.DataFrame(pd.Series(l), columns=[\"Pred\"])\n    df_result = pd.concat([df_test_id, df_pred], axis=1, sort=False)\n     \n    f = open('submission'+algo+'.csv', 'w')\n    r = df_result.to_csv(index=False, path_or_buf=f)\n    f.close()\n\n    return df_result\n\ndef print_submission(y_pred, df_test_id):\n    l = []\n    for myindex in range(y_pred.shape[0]):\n        Y0 = y_pred[myindex]\n        l.insert(myindex, Y0)\n    \n    df_pred = pd.DataFrame(pd.Series(l), columns=[\"Pred\"])\n    df_result = pd.concat([df_test_id, df_pred], axis=1, sort=False)\n     \n    f = open('submission.csv', 'w')\n    r = df_result.to_csv(index=False, path_or_buf=f)\n    f.close()\n\n    return df_result\n\n\ndef vectorize_more_weight_out(x):\n    if x>0.5:\n        return 0.9\n    #elif x<0.5 and x>0.3:\n    #    return 0.3\n    \n    else:\n        return x\n\ndef vectorize_str(x):\n    return str(x)\n\n\ndef vectorize_float(x):\n    return float(x)\n\ndef vectorize_exp(x):\n    r = math.exp(x)\n    return r\n\ndef vectorize_expm1(x):\n    r = math.expm1(x)\n    return r\n\ndef vectorize_log(x):\n    r = math.log(x)\n    return r\n\ndef vectorize_log1p(x):\n    r = math.log1p(x)\n    return r\n\ndef vectorize_pred_result_063945(x):\n    if x < 0.2:\n        return 0.2\n    elif x > 0.7:\n        return 0.6\n    else:\n        return x\n\ndef vectorize_pred_result(x):\n    if x < 0.02:\n        return 0.02\n    elif x > 0.95:\n        return 0.95\n    else:\n        return x\n\ndef vectorize_more_weight_out(x):\n    if x>0.5:\n        return 0.9\n    #elif x<0.5 and x>0.3:\n    #    return 0.3\n    \n    else:\n        return x\n        \ndef log_loss(y_01, y_p):\n    n = y_01.shape[0]\n    v = np.multiply(y_01, np.log(y_p)) + np.multiply((1-y_01), np.log(1-y_p))\n    \n    res = -(np.sum(v)/float(n)) \n    return res\n\n\ndef set_aggregation(row, se_agg, se_col, r_col, op_col):\n    df_s = se_agg[se_agg[se_col] == row[r_col]]\n    df = df_s[df_s['Season']==row['Season']].reset_index(drop=True)\n    if df.shape[0] == 0:\n        return 0\n    else:\n        return df.at[0, op_col]\n\n\ndef get_value_for_count(team, team_name, team_count):\n    if team in team_count.index:\n        return team_count.loc[team, 'Count']\n    else:\n        return 0\n\ndef set_WLoc(row):\n    if row==1:\n        return 2\n    elif row==2:\n        return 1\n    else:\n        return 0\n\ndef get_value_for_count(team, team_name, team_count):\n    if team in team_count.index:\n        return team_count.loc[team, 'Count']\n    else:\n        return 0\n    \ndef concat_row(r):\n    if r['WTeamID'] < r['LTeamID']:\n        res = str(r['Season'])+\"_\"+str(r['WTeamID'])+\"_\"+str(r['LTeamID'])\n    else:\n        res = str(r['Season'])+\"_\"+str(r['LTeamID'])+\"_\"+str(r['WTeamID'])\n    return res\n\ndef write_label(r):\n    if r['WTeamID'] < r['LTeamID']:\n        return 1\n    else:\n        return 0\n\n# Delete leaked from train\ndef delete_leaked_from_df_train(df_train, df_test):\n    # Delete leaked from train\n    dft = df_train.loc[:, ['Season','WTeamID','LTeamID']]\n    df_train['Concats'] = df_train.apply(concat_row, axis=1)\n    df2 = df_test[df_test['ID'].isin(df_train['Concats'].unique())]\n\n    df_train_duplicates = df_train[df_train['Concats'].isin(df_test['ID'].unique())]\n    df_train_idx = df_train_duplicates.index.values\n    \n    df_train = df_train.drop(df_train_idx)\n    df_train = df_train.drop('Concats', axis=1)\n    \n    return df_train\n\ndef get_labels_df_train(df_train, df_test):\n    # Delete leaked from train\n    df_train['Concats'] = df_train.apply(concat_row, axis=1)\n    \n    df_train_good = df_train[df_train['Concats'].isin(df_test['ID'].unique())]\n    \n    #df_train_good = df_train[~df_train['Concats'].isin(df_test['ID'].unique())]\n    df_train_good['Label'] = df_train_good.apply(write_label, axis=1)\n    write_data(df_train_good, 'labels_train')\n    \n\ndef replace_seed(s):\n    s = s.replace('W', '1')\n    s = s.replace('X', '2')\n    s = s.replace('Y', '3')\n    s = s.replace('Z', '4')\n    \n    if re.search('(a|b)', s):\n        s = s.replace('a', '1')\n        s = s.replace('b', '2')\n    else:\n        s = s+'0'\n    res = - int(s)\n    return res\n\ndef replace_seed_2(s):\n    rank = 0\n    seed = s\n    if re.search('W', s):\n        rank = '1'\n        seed = seed.replace('W', '')\n    elif re.search('X', s):\n        rank = '2'\n        seed = seed.replace('X', '')\n    elif re.search('Y', s):\n        rank = '3'\n        seed = seed.replace('Y', '')\n    elif re.search('Z', s):\n        rank = '4'\n        seed = seed.replace('Z', '')\n    \n    if re.search('a', s):\n        seed = seed.replace('a', '1')\n    if re.search('b', s):    \n        seed = seed.replace('b', '2')\n    else:\n        seed = seed+'0'\n       \n    final_seed = seed+rank\n    res = - int(final_seed)\n    return res\n\ndef replace_seed_only(s):\n    s = s.replace('W', '')\n    s = s.replace('X', '')\n    s = s.replace('Y', '')\n    s = s.replace('Z', '')\n    \n    if re.search('(a|b)', s):\n        s = s.replace('a', '')\n        s = s.replace('b', '')\n    else:\n        s = s+'0'\n     \n    return int(s)\n\ndef replace_rank_only(s):\n    if re.search('(a|b)', s):\n        s = re.sub('(a|b)', '', s)\n    \n    s = re.sub('(\\d)+', '', s)\n    \n    s = s.replace('W', '1')\n    s = s.replace('X', '2')\n    s = s.replace('Y', '3')\n    s = s.replace('Z', '4')\n         \n    return int(s)\n\ndef convert_categoricals_to_numbers(df):\n    sorted_EventType = sorted(df.EventType.dropna().unique())\n    mapping_EventType = dict(zip(sorted_EventType, range(len(sorted_EventType))))\n    df['EventType'] = df.loc[df.EventType.notnull(), 'EventType'].map(mapping_EventType)\n\n    df = df.reset_index(drop=True)\n    \n    sorted_EventSubType = sorted(df.EventSubType.dropna().unique())\n    mapping_EventSubType = dict(zip(sorted_EventSubType, range(len(sorted_EventSubType))))\n    df['EventSubType'] = df.loc[df.EventSubType.notnull(), 'EventSubType'].map(mapping_EventSubType)\n    \n    return df\n\n# Convert categories to numeric\ndef convert_data_to_numeric_2(df, df_all, ar_train_transformed=None, enc=None):\n    columns_for_ordinal_encoder = ['EventType', 'EventSubType']\n    \n    enc_new = enc\n    \n    # Fillna with ZZ which will be the last value in alphabetical order. \n    df_ordinal = df[columns_for_ordinal_encoder].fillna(NAN_STRING_TO_REPLACE).applymap(lambda x: str(x))\n    df_ordinal_all = df_all[columns_for_ordinal_encoder].fillna(NAN_STRING_TO_REPLACE).applymap(lambda x: str(x))\n    ar_train_transformed_new = ar_train_transformed\n    if enc == None and ar_train_transformed == None:\n        enc_new = OrdinalEncoder(dtype=np.int16)\n        enc_new.fit(df_ordinal_all)\n        \n        \n        ar_train_transformed_new = enc_new.transform(df_ordinal_all)\n    \n    ar_ordinal_transformed = enc_new.transform(df_ordinal)\n    count_columns = 0\n    for cn in columns_for_ordinal_encoder:\n        this_col_all = ar_train_transformed_new[:,count_columns]\n        mx = this_col_all.max() # Find the index of ZZ, last one always, in our case, as OrdinalEncoder encodes by alphabetic order\n        \n        this_col_train = ar_ordinal_transformed[:,count_columns]\n        this_col_nan_train = np.where(this_col_train==mx, NAN_VALUE_INT, this_col_train)\n    \n        ar_ordinal_transformed[:,count_columns] = this_col_nan_train\n        count_columns = count_columns+1\n\n    df_ordinal_transformed = pd.DataFrame(ar_ordinal_transformed, columns=columns_for_ordinal_encoder)\n    \n    df.update(df_ordinal_transformed)      \n    \n    # Fill NaN with -1 first : \n    df = df.fillna(-1)\n     \n    # Optimize :\n    for cn in df.columns:\n        cn_size = df[cn].unique().size\n        \n        if cn_size <= np.iinfo(np.int8).max:\n            df[cn] = df[cn].astype('int8')\n        elif cn_size > np.iinfo(np.int8).max  and cn_size <= np.iinfo(np.int16).max:\n            df[cn] = df[cn].astype('int16')\n        elif cn_size > np.iinfo(np.int16).max  and cn_size <= np.iinfo(np.int32).max:\n            df[cn] = df[cn].astype('int32')    \n    \n    df = df.applymap(lambda x: NAN_VALUE_INT if x == -1 else x)\n    \n    return (df, ar_train_transformed_new, enc_new)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nPATH = \"/kaggle/input/ncaam-march-mania-2021/MDataFiles_Stage2/\"\n\ndf_mncaa_tourney_detailed_results = read_data(PATH+\"MNCAATourneyDetailedResults.csv\") # OK\ndf_regular_season_detailed_results = read_data(PATH+\"MRegularSeasonDetailedResults.csv\") # OK\ndf_tourney_seeds = read_data(PATH+\"MNCAATourneySeeds.csv\")\ndf_test = read_data(PATH+\"MSampleSubmissionStage2.csv\")\n\ndf_train = df_mncaa_tourney_detailed_results\ndf_train = delete_leaked_from_df_train(df_train, df_test)\n\n# df_train = df_train.append(df_regular_season_detailed_results, ignore_index=True)\n# df_train = df_train.drop('WLoc', axis=1)\n\n# Seeds\ndf_tourney_seeds['SeedID'] = df_tourney_seeds['Seed'].apply(replace_seed_only)\n\ndf_train_tcr = df_mncaa_tourney_detailed_results\n\nmapping_WLoc = {'N':0, 'A':1, 'H':2}\ndf_train['WLoc'] = df_train.loc[df_train.WLoc.notnull(), 'WLoc'].map(mapping_WLoc)\n\n# Features to parse\nfeatures = df_train.columns\n\ndf_train_features = df_train[features]\ndf_train_features = df_train_features.fillna(NAN_VALUE_INT)\n\n# Create simple imputer\nsi_mf = SimpleImputer(missing_values=NAN_VALUE_INT, strategy=IMPUTING_STRATEGY)\nar_train = si_mf.fit_transform(df_train_features)\ndf_train = pd.DataFrame(ar_train, columns=features)\n\ndf_train_tcr = df_train.copy()\ndf_train_tcr = df_train_tcr.fillna(NAN_VALUE_INT)\nfeatures_tcr = ['Season', 'WTeamID', 'LTeamID', 'WScore', 'LScore', 'NumOT']\n\n# FINAL\ndf_train_tcr_final = df_train_tcr[df_train_tcr['DayNum']==154]\nar_tcr_final_teams = df_train_tcr_final.loc[:,['WTeamID', 'LTeamID']].to_numpy()\nar_tcr_final_teams = np.unique(ar_tcr_final_teams)\ndf_tcr_final_teams = pd.DataFrame(ar_tcr_final_teams)\n\ndf_tcr_final_teams_2 = ar_tcr_final_teams.flatten()\n\nar_final_teams_count = np.array(np.unique(df_tcr_final_teams_2, return_counts=True)).T\ndf_final_teams_count = pd.DataFrame(ar_final_teams_count, columns=['TeamID','Count'])\n\n# SEMI FINAL\ndf_train_semi_final = df_train_tcr[df_train_tcr['DayNum']==152]\nar_semi_final_teams = df_train_semi_final.loc[:,['WTeamID', 'LTeamID']].to_numpy()\nar_semi_final_teams = np.unique(ar_semi_final_teams)\ndf_semi_final_teams = pd.DataFrame(ar_semi_final_teams)\ndf_semi_final_teams_2 = ar_semi_final_teams.flatten()\nar_semi_final_teams_count = np.array(np.unique(df_semi_final_teams_2, return_counts=True)).T\ndf_semi_final_teams_count = pd.DataFrame(ar_semi_final_teams_count, columns=['TeamID','Count'])\n\n# Aggregates\nwt_se_agg = df_train_tcr.groupby(['Season', 'WTeamID']).agg({'WScore':['sum','mean','median', 'count']})\nlt_se_agg = df_train_tcr.groupby(['Season', 'LTeamID']).agg({'WScore':['sum','mean','median', 'count']})\nwt_se_mean = df_train_tcr.groupby(['WTeamID', 'Season']).mean()\n\nwt_se_agg.columns = ['sum', 'mean', 'median', 'count']\nwt_se_agg = wt_se_agg.reset_index()\n\nlt_se_agg.columns = ['sum', 'mean', 'median', 'count']\nlt_se_agg = lt_se_agg.reset_index()\n\n# Sum and mean\nwt_mean = df_train_tcr.groupby('WTeamID').mean()\nwt_sum = df_train_tcr.groupby('WTeamID').sum()\nwt_median = df_train_tcr.groupby('WTeamID').median()\nlt_mean = df_train_tcr.groupby('LTeamID').mean()\nlt_sum = df_train_tcr.groupby('LTeamID').sum()\nlt_median = df_train_tcr.groupby('LTeamID').median()\n\n# Nb wins, lose\nwt_count = df_train_tcr.groupby('WTeamID').size().to_frame()\nlt_count = df_train_tcr.groupby('LTeamID').size().to_frame()\n\nwt_count.columns = ['Count']\nlt_count.columns = ['Count']\n\n# Min\nwt_min = df_train_tcr.groupby('WTeamID').min()\nlt_min = df_train_tcr.groupby('LTeamID').min()\n\n# Min\nwt_max = df_train_tcr.groupby('WTeamID').max()\nlt_max = df_train_tcr.groupby('LTeamID').max()\n\n\ndf_train['WTeam_Seed'] = df_train.loc[:,['Season','WTeamID']].apply(lambda row: set_aggregation(row, df_tourney_seeds, 'TeamID', 'WTeamID', 'SeedID'), axis=1)\ndf_train['LTeam_Seed'] = df_train.loc[:,['Season','LTeamID']].apply(lambda row: set_aggregation(row, df_tourney_seeds, 'TeamID', 'LTeamID', 'SeedID'), axis=1)\n\nfeatures = df_train.columns\ndf_train_features = df_train.fillna(NAN_VALUE_INT)\n\n# Create simple imputer\nsi_mf = SimpleImputer(missing_values=NAN_VALUE_INT, strategy=IMPUTING_STRATEGY)\nsi_mf.fit(df_train_features)\n   \ndf_test_id = df_test[\"ID\"]\ndf_test = df_test[\"ID\"].apply(lambda x: pd.Series(x.split(\"_\"))).astype('int16')\ndf_test.columns = ['Season', 'WTeamID', 'LTeamID']\n\ndf_test['WTeam_Seed'] = df_test.loc[:,['Season','WTeamID']].apply(lambda row: set_aggregation(row, df_tourney_seeds, 'TeamID', 'WTeamID', 'SeedID'), axis=1)\ndf_test['LTeam_Seed'] = df_test.loc[:,['Season','LTeamID']].apply(lambda row: set_aggregation(row, df_tourney_seeds, 'TeamID', 'LTeamID', 'SeedID'), axis=1)\n\ndf_test['DayNum'] = NAN_VALUE_INT\ndf_test['NumOT'] = df_train_tcr['NumOT'].max()\ndf_test['WLoc'] = 0\n\nimputation = 3\n\nw_features = ['WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF', 'WScore']\nl_features = ['LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', 'LScore']\n\nfeatures = df_train.columns\nif imputation == 0:\n    for cn in features:\n        if cn in ['Season', 'WTeamID', 'LTeamID', 'WLoc', 'DayNum', 'WTeam_Seed', 'LTeam_Seed']:\n            continue\n        df_test[cn] = NAN_VALUE_INT\n\n    # Impute to df_test\n    df_test = df_test.fillna(NAN_VALUE_INT)\n    ar_test = si_mf.transform(df_test)\n    df_test = pd.DataFrame(ar_test, columns=features).astype('float64')        \nelif imputation == 1:\n    df_test['DayNum'] = df_train['DayNum'].median()\n    df_test['NumOT']  = df_train['NumOT'].median()\n     \n    agg_strategy = 'mean'\n    \n    for i in range(len(w_features)):\n        cn_w = w_features[i]\n        cn_l = l_features[i]\n        \n        wt_agg = df_train.groupby(['Season', 'WTeamID']).agg({cn_w:['sum', 'mean', 'median']})\n        wt_agg.columns = ['sum', 'mean', 'median']\n        wt_agg = wt_agg.reset_index()\n        df_test[cn_w] = df_test.loc[:, ['Season', 'WTeamID']].apply(lambda row: set_aggregation(row, wt_agg, 'WTeamID', 'WTeamID', agg_strategy), axis=1)\n        \n        lt_agg = df_train.groupby(['Season', 'LTeamID']).agg({cn_l:['sum', 'mean', 'median']})\n        lt_agg.columns = ['sum', 'mean', 'median']\n        lt_agg = lt_agg.reset_index()\n        df_test[cn_l] = df_test.loc[:,['Season','LTeamID']].apply(lambda row: set_aggregation(row, lt_agg, 'LTeamID', 'LTeamID',agg_strategy), axis=1)\n       \n    '''\n    for cn in w_features:\n        # df_test[cn] = df_test['WTeamID'].map(wt_sum[cn])\n        wt_agg = df_train.groupby(['Season', 'WTeamID']).agg({cn:['sum', 'mean', 'median']})\n        wt_agg.columns = ['sum', 'mean', 'median']\n        wt_agg = wt_agg.reset_index()\n        df_test[cn] = df_test.loc[:, ['Season', 'WTeamID']].apply(lambda row: set_aggregation(row, wt_agg, 'WTeamID', 'WTeamID', agg_strategy), axis=1)\n\n    for cn in l_features:\n        #df_test[cn] = df_test['LTeamID'].map(lt_sum[cn])\n        lt_se_agg = df_train.groupby(['Season', 'LTeamID']).agg({cn:['sum', 'mean', 'median']})\n        lt_se_agg.columns = ['sum', 'mean', 'median']\n        lt_se_agg = lt_se_agg.reset_index()\n        df_test[cn] = df_test.loc[:,['Season','LTeamID']].apply(lambda row: set_aggregation(row, lt_se_agg, 'LTeamID', 'LTeamID',agg_strategy), axis=1)\n    '''    \n\nelif imputation == 2: # mean\n    df_test['DayNum'] = df_train['DayNum'].mean()\n    df_test['NumOT']  = df_train['NumOT'].mean()\n \n    for i in range(len(w_features)):\n        cn_w = w_features[i]\n        cn_l = l_features[i]\n\n        df_test[cn_w] = df_test['WTeamID'].map(wt_mean[cn_w])\n        df_test[cn_l] = df_test['LTeamID'].map(lt_mean[cn_l])\n   \n    df_test = df_test.fillna(0)\n\n\nelif imputation == 3: # median\n    df_test['DayNum'] = df_train['DayNum'].median()\n    df_test['NumOT']  = df_train['NumOT'].median()\n    \n    for i in range(len(w_features)):\n        cn_w = w_features[i]\n        cn_l = l_features[i]\n\n        df_test[cn_w] = df_test['WTeamID'].map(wt_median[cn_w])\n        df_test[cn_l] = df_test['LTeamID'].map(lt_median[cn_l])\n   \n    df_test = df_test.fillna(0)\n\n# Final df_train\ndf_train_final = df_train.loc[df_train['WTeamID'].isin(ar_tcr_final_teams)]\ndf_train_final_indexes = df_train_final.index.values\ndf_train = df_train.assign(Final_WTeam=0)\ndf_train_final = df_train_final.assign(Final_WTeam=0)\ndf_train_final.loc[df_train_final_indexes, 'Final_WTeam'] = df_train_final['WTeamID'].map(df_final_teams_count.set_index('TeamID')['Count'])\ndf_train.update(df_train_final)\n\ndf_train_final = df_train.loc[df_train['LTeamID'].isin(ar_tcr_final_teams)]\ndf_train_final_indexes = df_train_final.index.values\ndf_train = df_train.assign(Final_LTeam= 0)\ndf_train_final = df_train_final.assign(Final_LTeam=0)\ndf_train_final.loc[df_train_final_indexes, 'Final_LTeam'] = df_train_final['LTeamID'].map(df_final_teams_count.set_index('TeamID')['Count'])\ndf_train.update(df_train_final)\n\n# Semi final df_train\ndf_train_final = df_train.loc[df_train['WTeamID'].isin(ar_semi_final_teams)]\ndf_train_final_indexes = df_train_final.index.values\ndf_train = df_train.assign(Semi_Final_WTeam=0)\ndf_train_final = df_train_final.assign(Semi_Final_WTeam=0)\ndf_train_final.loc[df_train_final_indexes, 'Semi_Final_WTeam'] = df_train_final['WTeamID'].map(df_semi_final_teams_count.set_index('TeamID')['Count'])\ndf_train.update(df_train_final)\n\ndf_train_final = df_train.loc[df_train['LTeamID'].isin(ar_semi_final_teams)]\ndf_train_final_indexes = df_train_final.index.values\ndf_train = df_train.assign(Semi_Final_LTeam=0)\ndf_train_final = df_train_final.assign(Semi_Final_LTeam=0)\ndf_train_final.loc[df_train_final_indexes, 'Semi_Final_LTeam'] = df_train_final['LTeamID'].map(df_semi_final_teams_count.set_index('TeamID')['Count'])\ndf_train.update(df_train_final)\n\n'''\ndf_train['WScore_mean'] = df_train['WTeamID'].map(wt_mean['WScore'])\ndf_train['LScore_mean'] = df_train['LTeamID'].map(wt_mean['LScore'])\ndf_train['WScore_median'] = df_train['WTeamID'].map(wt_median['WScore'])\ndf_train['LScore_median'] = df_train['LTeamID'].map(wt_median['LScore'])\ndf_train['WScore_sum'] = df_train['WTeamID'].map(wt_sum['WScore'])\ndf_train['LScore_sum'] = df_train['LTeamID'].map(wt_sum['LScore'])\n'''\n\ndf_train = df_train.assign(WFGA_mean=0)\ndf_train = df_train.assign(LFGA_mean=0)\ndf_train = df_train.assign(WFGA_median=0)\ndf_train = df_train.assign(LFGA_median=0)\ndf_train = df_train.assign(WFGA_min=0)\ndf_train = df_train.assign(LFGA_min=0)\n\ndf_train['WFGA_mean'] = df_train['WTeamID'].map(wt_mean['WFGA'])\ndf_train['LFGA_mean'] = df_train['LTeamID'].map(lt_mean['LFGA'])\n\ndf_train['WFGA_median'] = df_train['WTeamID'].map(wt_median['WFGA'])\ndf_train['LFGA_median'] = df_train['LTeamID'].map(lt_median['LFGA'])\n\ndf_train['WFGA_min'] = df_train['WTeamID'].map(wt_min['WFGA'])\ndf_train['LFGA_min'] = df_train['LTeamID'].map(lt_min['LFGA'])\n\n'''\ndf_train['WAst_mean'] = df_train['WTeamID'].map(wt_mean['WAst'])\ndf_train['LAst_mean'] = df_train['LTeamID'].map(lt_mean['LAst'])\n\ndf_train['WBlk_mean'] = df_train['WTeamID'].map(wt_mean['WBlk'])\ndf_train['LBlk_mean'] = df_train['LTeamID'].map(lt_mean['LBlk'])\n'''\n\n#df_train['WFGA_max'] = df_train['WTeamID'].map(wt_max['WFGA'])\n#df_train['LFGA_max'] = df_train['LTeamID'].map(lt_max['LFGA'])\n\ndf_train['WScore_mean'] = df_train.loc[:,['Season','WTeamID']].apply(lambda row: set_aggregation(row, wt_se_agg, 'WTeamID', 'WTeamID', 'mean'), axis=1)\ndf_train['LScore_mean'] = df_train.loc[:,['Season','LTeamID']].apply(lambda row: set_aggregation(row, lt_se_agg, 'LTeamID', 'LTeamID', 'mean'), axis=1)\ndf_train['WScore_median'] = df_train.loc[:,['Season','WTeamID']].apply(lambda row: set_aggregation(row, wt_se_agg, 'WTeamID', 'WTeamID', 'median'), axis=1)\ndf_train['LScore_median'] = df_train.loc[:,['Season','LTeamID']].apply(lambda row: set_aggregation(row, lt_se_agg, 'LTeamID', 'LTeamID', 'median'), axis=1)\ndf_train['WScore_sum'] = df_train.loc[:,['Season','WTeamID']].apply(lambda row: set_aggregation(row, wt_se_agg, 'WTeamID', 'WTeamID', 'sum'), axis=1)\ndf_train['LScore_sum'] = df_train.loc[:,['Season','LTeamID']].apply(lambda row: set_aggregation(row, lt_se_agg, 'LTeamID', 'LTeamID', 'sum'), axis=1)\n\n# Counts\ndf_train['WTeam_W_count'] = OTHER_NAN\ndf_train['LTeam_L_count'] = OTHER_NAN\n\ncount_wt_win = df_train['WTeamID'].map(wt_count['Count'])\ncount_lt_lose = df_train['LTeamID'].map(lt_count['Count'])\ncount_wt_lose = df_train['WTeamID'].apply(lambda row: get_value_for_count(row, 'LTeamID', lt_count))\ncount_lt_win = df_train['LTeamID'].apply(lambda row: get_value_for_count(row, 'WTeamID', wt_count))\n\ndf_train['WTeam_W_count'] = count_wt_win\ndf_train['LTeam_L_count'] = count_lt_lose\n\ndf_train['Diff_WTeam'] = count_wt_win - count_wt_lose\ndf_train['Diff_LTeam'] = count_lt_win - count_lt_lose\n\ndf_train['WTeam_PerCent'] = count_wt_win / (count_wt_win + count_wt_lose)\ndf_train['LTeam_PerCent'] = count_lt_win / (count_lt_win + count_lt_lose)\n\ndf_train['WTeam_W_count'] = df_train['WTeam_W_count'].fillna(OTHER_NAN)\ndf_train['LTeam_L_count'] = df_train['LTeam_L_count'].fillna(OTHER_NAN)\n\n\n# Preprocess test \n\n# Test final\ndf_test_final = df_test.loc[df_test['WTeamID'].isin(ar_tcr_final_teams)]\ndf_train_final_indexes = df_test_final.index.values\ndf_test = df_test.assign(Final_WTeam=0)\ndf_test_final = df_test_final.assign(Final_WTeam=0)\ndf_test_final.loc[df_train_final_indexes, 'Final_WTeam'] = df_test_final['WTeamID'].map(df_final_teams_count.set_index('TeamID')['Count'])\ndf_test.update(df_test_final)\n\ndf_test_final = df_test[df_test['LTeamID'].isin(ar_tcr_final_teams)]\ndf_train_final_indexes = df_test_final.index.values\ndf_test = df_test.assign(Final_LTeam=0)\ndf_test_final = df_test_final.assign(Final_LTeam=0)\ndf_test_final.loc[df_train_final_indexes, 'Final_LTeam'] = df_test_final['LTeamID'].map(df_final_teams_count.set_index('TeamID')['Count'])\ndf_test.update(df_test_final)\n\n\n\n# Test semi final\ndf_test_semi_final = df_test.loc[df_test['WTeamID'].isin(ar_semi_final_teams)]\n\ndf_test_final_indexes = df_test_semi_final.index.values\ndf_test['Semi_Final_WTeam'] = 0\ndf_test_semi_final['Semi_Final_WTeam'] = 0\ndf_test_semi_final.loc[df_test_final_indexes, 'Semi_Final_WTeam'] = df_test_semi_final['WTeamID'].map(df_semi_final_teams_count.set_index('TeamID')['Count'])\ndf_test.update(df_test_semi_final)\n\ndf_test_semi_final = df_test[df_test['LTeamID'].isin(ar_semi_final_teams)]\ndf_test_final_indexes = df_test_semi_final.index.values\ndf_test['Semi_Final_LTeam'] = 0\ndf_test_semi_final['Semi_Final_LTeam'] = 0\ndf_test_semi_final.loc[df_test_final_indexes, 'Semi_Final_LTeam'] = df_test_semi_final['LTeamID'].map(df_semi_final_teams_count.set_index('TeamID')['Count'])\ndf_test.update(df_test_semi_final)\n\n\n'''\ndf_test['WScore_mean'] = df_test['WTeamID'].map(wt_mean['WScore'])\ndf_test['LScore_mean'] = df_test['LTeamID'].map(wt_mean['LScore'])\ndf_test['WScore_median'] = df_test['WTeamID'].map(wt_median['WScore'])\ndf_test['LScore_median'] = df_test['LTeamID'].map(wt_median['LScore'])\ndf_test['WScore_sum'] = df_test['WTeamID'].map(wt_sum['WScore'])\ndf_test['LScore_sum'] = df_test['LTeamID'].map(wt_sum['LScore'])\n'''\n\ndf_test['WFGA_mean'] = df_test['WTeamID'].map(wt_mean['WFGA'])\ndf_test['LFGA_mean'] = df_test['LTeamID'].map(lt_mean['LFGA'])\n\ndf_test['WFGA_median'] = df_test['WTeamID'].map(wt_median['WFGA'])\ndf_test['LFGA_median'] = df_test['LTeamID'].map(lt_median['LFGA'])\n\ndf_test['WFGA_min'] = df_test['WTeamID'].map(wt_min['WFGA'])\ndf_test['LFGA_min'] = df_test['LTeamID'].map(lt_min['LFGA'])\n\nprint(\"Here 8\")\n\n'''\ndf_test['WAst_mean'] = df_test['WTeamID'].map(wt_mean['WAst'])\ndf_test['LAst_mean'] = df_test['LTeamID'].map(lt_mean['LAst'])\n\ndf_test['WBlk_mean'] = df_test['WTeamID'].map(wt_mean['WBlk'])\ndf_test['LBlk_mean'] = df_test['LTeamID'].map(lt_mean['LBlk'])\n'''\n\n#df_test['WFGA_max'] = df_test['WTeamID'].map(wt_max['WFGA'])\n#df_test['LFGA_max'] = df_test['LTeamID'].map(lt_max['LFGA'])\n\ndf_test['WScore_mean'] = df_test.loc[:,['Season','WTeamID']].apply(lambda row: set_aggregation(row, wt_se_agg, 'WTeamID', 'WTeamID', 'mean'), axis=1)\ndf_test['LScore_mean'] = df_test.loc[:,['Season','LTeamID']].apply(lambda row: set_aggregation(row, lt_se_agg, 'LTeamID', 'LTeamID', 'mean'), axis=1)\ndf_test['WScore_median'] = df_test.loc[:,['Season','WTeamID']].apply(lambda row: set_aggregation(row, wt_se_agg, 'WTeamID', 'WTeamID', 'median'), axis=1)\ndf_test['LScore_median'] = df_test.loc[:,['Season','LTeamID']].apply(lambda row: set_aggregation(row, lt_se_agg, 'LTeamID', 'LTeamID', 'median'), axis=1)\ndf_test['WScore_sum'] = df_test.loc[:,['Season','WTeamID']].apply(lambda row: set_aggregation(row, wt_se_agg, 'WTeamID', 'WTeamID', 'sum'), axis=1)\ndf_test['LScore_sum'] = df_test.loc[:,['Season','LTeamID']].apply(lambda row: set_aggregation(row, lt_se_agg, 'LTeamID', 'LTeamID', 'sum'), axis=1)\n\nprint(\"Here 9\")\n\n# Counts\ncount_wt_win = df_test['WTeamID'].map(wt_count['Count'])\ncount_lt_lose = df_test['LTeamID'].map(lt_count['Count'])\ncount_wt_lose = df_test['WTeamID'].apply(lambda row: get_value_for_count(row, 'LTeamID', lt_count))\ncount_lt_win = df_test['LTeamID'].apply(lambda row: get_value_for_count(row, 'WTeamID', wt_count))\n\ndf_test = df_test.assign(WTeam_W_count=OTHER_NAN)\ndf_test = df_test.assign(LTeam_L_count=OTHER_NAN)\n\ndf_test.loc[:, 'WTeam_W_count'] = count_wt_win\ndf_test.loc[:, 'LTeam_L_count'] = count_lt_lose\n\ndf_test['WTeam_W_count'] = df_test['WTeam_W_count'].fillna(OTHER_NAN)\ndf_test['LTeam_L_count'] = df_test['LTeam_L_count'].fillna(OTHER_NAN)\n\ndf_test['Diff_WTeam'] = count_wt_win - count_wt_lose\ndf_test['Diff_LTeam'] = count_lt_win - count_lt_lose\n\ndf_test['Diff_WTeam'] = df_test['Diff_WTeam'].fillna(OTHER_NAN)\ndf_test['Diff_LTeam'] = df_test['Diff_LTeam'].fillna(OTHER_NAN)\n\ndf_test['WTeam_PerCent'] = count_wt_win / (count_wt_win + count_wt_lose)\ndf_test['LTeam_PerCent'] = count_lt_win / (count_lt_win + count_lt_lose)\n\ndf_test['WTeam_PerCent'] = df_test['WTeam_PerCent'].fillna(OTHER_NAN)\ndf_test['LTeam_PerCent'] = df_test['LTeam_PerCent'].fillna(OTHER_NAN)\n\nfeatures = df_train.columns\n\ncategory_features_names = ['Season', 'DayNum', 'WLoc', 'WTeamID', 'LTeamID']\n\ndf_train = df_train.fillna(OTHER_NAN)\ndf_test = df_test.fillna(OTHER_NAN)\n\nprint(\"Here 10\")\n\n#df_train[category_features_names] = df_train[category_features_names].astype('int64').astype('category')\n#df_test[category_features_names] = df_test[category_features_names].astype('int64').astype('category')\n\nx1 = df_train.shape[0]\ndf_train_inverse = df_train.copy()\n\nfor i in range(len(W_FEATURES)):\n    v_w = W_FEATURES[i]\n    v_l = L_FEATURES[i]\n    df_train_inverse[v_w] = df_train[v_l]\n    df_train_inverse[v_l] = df_train[v_w]\n\ndf_train_inverse['WLoc'] = df_train_inverse['WLoc'].apply(set_WLoc)    \ndf_train = df_train.append(df_train_inverse, ignore_index=True)\n\ntrain_features = [cn for cn in df_train.columns if cn not in ['WScore', 'LScore', \"NumOT\"]]\nY = df_train['WScore'] - df_train['LScore']\n\nmms = MinMaxScaler()\nmms.fit(Y.values.reshape(-1,1))\nyyy = mms.transform(Y.values.reshape(-1,1))\nY = pd.DataFrame(data=yyy, columns=[\"Y\"])\n\nX_train = df_train[train_features]\nX_test = df_test[train_features]\n\nX_train[category_features_names] = X_train[category_features_names].astype('int64').astype('category')\nX_test[category_features_names] = X_test[category_features_names].astype('int64').astype('category')\n\nfinal_encoding = 1\ncat_features = []\n\nif final_encoding==0: # all data encoded with TE, cat_features empty\n    X_train = X_train.applymap(lambda x: str(x))\n    X_test = X_test.applymap(lambda x: str(x))\n    te = ce.TargetEncoder(smoothing=0.2)\n    te.fit(X_train, Y)\n    X_train = te.transform(X_train, Y)\n    X_test = te.transform(X_test)\n    cat_features = []\nelif final_encoding==1: # TE only on category features, cat_features = empty\n    te = ce.TargetEncoder(cols=category_features_names, smoothing=0.2)\n    te.fit(X_train, Y)\n    X_train = te.transform(X_train, Y)\n    X_test = te.transform(X_test)\n    cat_features = []\n    # cat_features = category_features_names\nelif final_encoding==2:\n    non_cat = [cn for cn in X_train.columns if cn not in category_features_names]\n    \n    X_train_numeric = X_train[non_cat].applymap(lambda x: str(x))\n    X_test_numeric = X_test[non_cat].applymap(lambda x: str(x))\n\n    te = ce.TargetEncoder(smoothing=0.2)\n    te.fit(X_train_numeric, Y)\n    X_train_numeric = te.transform(X_train_numeric, Y)\n    X_test_numeric = te.transform(X_test_numeric)\n    \n    X_train[non_cat] = X_train_numeric\n    X_test.update(X_test_numeric)\n    cat_features = category_features_names\nelif final_encoding==3: # all data without cat features\n    X_train = X_train.drop(['Season', 'DayNum', 'WTeamID', 'LTeamID'], axis=1)\n    X_test = X_test.drop(['Season', 'DayNum', 'WTeamID', 'LTeamID'], axis=1)\n    #cat_features = ['WLoc']\nelse:\n    cat_features = category_features_names\n\nX = X_train\nX_testset= X_test\n\n\nprint('Max train :')\nprint(X.max().max())\n\nprint('Max test :')\nprint(X_testset.max().max())\n\n'''\nprint(\"Has 8888 test \")\nprint(X_testset[X_testset.eq(8888.0).any(1)])\n\nprint(\"Has 8888 train \")\nprint(X[X.eq(8888.0).any(1)])\n'''\n\nnames = [\n         #\"Ridge\",\n         #\"RidgeCV\",\n         #\"LinearSVM\",\n         \"XGB_Regressor\", \n         \"XGBOOST\",  \n         \"GBC_Regressor\",\n         \"HGBC_Regressor\",\n         \"ETC_Regressor\",\n         #\"LDA\",\n         #\"QDA\",\n         #\"GaussianProcess\",\n         \"DecisionTree\",\n         \"RandomForest_Regressor\",\n         \"AdaBoost_Regressor\",\n         \"NaiveBayes\",\n         \"LogisticRegression\",\n         #\"RBF SVM\",\n         \"CatBoost_Regressor\",\n         \"LGB\",   \n         \"Huber_Regressor\",\n         \"Theil_Regressor\"\n         #\"NeuralNet\"\n         #\"NearestNeighbors\"\n    ]\n\nclassifiers = [\n        #RidgeClassifier(),\n        #RidgeClassifierCV(),\n        #SVC(kernel=\"linear\", C=0.025),\n        XGBRegressor(),\n        xgb,\n        GradientBoostingRegressor(),\n        HistGradientBoostingRegressor(),\n        ExtraTreesRegressor(),\n        #LinearDiscriminantAnalysis(),\n        #QuadraticDiscriminantAnalysis(),\n        #GaussianProcessClassifier(1.0 * RBF(1.0)),\n        DecisionTreeRegressor(max_depth=5),\n        RandomForestRegressor(max_depth=5, n_estimators=500),\n        AdaBoostRegressor(),\n        GaussianNB(),\n        LogisticRegression(max_iter=10000),\n        #SVC(gamma=2, C=1),\n        CatBoostRegressor(**PARAMS_CATBOOST_REGRESSOR),\n        lgb,\n        HuberRegressor(),\n        TheilSenRegressor()\n        #MLPClassifier(alpha=0.1, max_iter=50, batch_size=50, tol=0.0001, verbose=False)\n        #KNeighborsClassifier()\n    ]\n\nnames = [\n         #\"LogisticRegression\",\n         #\"RBF SVM\",\n         \"CatBoost_Regressor\",\n         \"LGB\",   \n         \"Huber_Regressor\",\n         \"Theil_Regressor\"\n         #\"NeuralNet\"\n         #\"NearestNeighbors\"\n    ]\n\nclassifiers = [\n        #LogisticRegression(max_iter=10000),\n        #SVC(gamma=2, C=1),\n        CatBoostRegressor(**PARAMS_CATBOOST_REGRESSOR),\n        lgb,\n        HuberRegressor(),\n        TheilSenRegressor()\n        #MLPClassifier(alpha=0.1, max_iter=50, batch_size=50, tol=0.0001, verbose=False)\n        #KNeighborsClassifier()\n    ]\n\n#names = [\"XGBOOST\", \"LGB\"]\n#classifiers = [xgb, lgb]\n\ndef vectorize_switch_zero_one(x):\n    if x == 1:\n        return 0\n    else:\n        return 1\n\nfunction_vectorize_switch_zero_one = np.vectorize(vectorize_switch_zero_one)\nfunction_vectorize_pred_result = np.vectorize(vectorize_pred_result)\n\nSPLITS = 3\nkf = KFold(n_splits=SPLITS, shuffle=True, random_state=SEED)\n\nformat_data = 'df'\n\nfor name, clf in zip(names, classifiers):\n    print(\"Classifier \"+name)\n        \n    test_preds = np.zeros((X_testset.shape[0],1))\n    test_score = 0\n    train_score = 0\n    p_test_score = 0\n    count = 0\n    \n    if name in [\"CatBoost_Classifier\", \"CatBoost_Regressor\"]:\n        pass\n    else:\n        X = X.astype('float64')\n        X_testset = X_testset.astype('float64')\n    \n    \"\"\"\n    clf.fit(X, Y.values.ravel()) \n    y_pred_proba = clf.predict(X_testset) \n    y_pred_proba = y_pred_proba / float(10)\n     \n    y_pred_proba = function_vectorize_pred_result(y_pred_proba) \n    print_submission_into_file(y_pred_proba, df_test_id, \"_\"+str(name))\n    \"\"\"\n    oof = np.zeros((X.shape[0],1))\n    \n    for train_index, test_index in kf.split(X, Y):\n        count = count+1\n        print(\"Split \"+str(count)+\" ... \"+name)\n     \n        if format_data == 'np':\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = Y[train_index], Y[test_index]\n        elif format_data == 'df':\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n        \n        if name in [\"CatBoost_Classifier\", \"CatBoost_Regressor\"]:\n            train_dataset = Pool(data=X_train, label=y_train, cat_features=cat_features)\n            eval_dataset = Pool(data=X_test, label=y_test, cat_features=cat_features)\n            clf.fit(\n                train_dataset, \n                use_best_model=True, \n                eval_set=[eval_dataset], \n                plot=True\n            ) # Get predicted classes\n     \n            print(\"Count of trees in model = {}\".format(clf.tree_count_))\n        \n        elif name == \"LGB\":\n            train_data = lgb.Dataset(X_train.to_numpy(), label=y_train.values.flatten())\n            validation_data = lgb.Dataset(X_test.to_numpy(), label=y_test.values.flatten())\n            prediction_data = lgb.Dataset(X_testset.to_numpy())\n            \n            #clf = lgb.train(PARAM_LGB, train_data, valid_sets=[validation_data])\n            clf = lgb.train(lgb_params, train_data, valid_sets=[validation_data])\n            \n        elif name == \"XGBOOST\":\n            train_set = xgb.DMatrix(X_train, y_train)\n            val_set = xgb.DMatrix(X_test, y_test)\n            prediction_data = xgb.DMatrix(X_testset)\n            clf = xgb.train(xgb_params, train_set, num_boost_round=4000, evals=[(train_set, 'train'), (val_set, 'val')], verbose_eval=100)\n        else:\n            #clf.fit(X_train, y_train.values.ravel())\n            clf.fit(X_train, y_train.values)\n        \n        # save the model to disk\n        #filename = 'model_ALL_'+str(SPLITS)+'_splits_'+name+'_'+str(count)+'.sav'\n        #pickle.dump(clf, open(filename, 'wb'))\n           \n        if name == \"XGBOOST\":    \n            y_train_predict = clf.predict(train_set)\n            y_test_predict = clf.predict(val_set)\n            y_pred_proba = clf.predict(prediction_data)\n        else:\n            y_train_predict = clf.predict(X_train)\n            y_test_predict = clf.predict(X_test)\n            y_pred_proba = clf.predict(X_testset)    \n        \n        #print(\"prediction shape\", y_pred_proba.shape, y_train.shape, y_test.shape)\n        \n        if name in [\"LGB\", \"XGBOOST\"]:\n            score_test = mean_squared_error(y_test, y_test_predict)\n            score_train = mean_squared_error(y_train, y_train_predict)\n        else:\n            score_test = clf.score(X_test, y_test)\n            score_train = clf.score(X_train, y_train)\n        \n            #y_test_predict = y_test_predict.reshape(-1, 1)\n            #y01 = y_test.to_numpy().reshape((y_test.shape[0], 1))\n            #p = log_loss(y01, y_test_predict)\n            '''\n            y_train_predict = y_train_predict.reshape(-1, 1)\n            y01 = y_train.to_numpy().reshape((y_train.shape[0], 1))\n            pp = log_loss(y01, y_train_predict)\n            '''\n            #oof[test_index] = y_test_predict\n\n            print(\"Score Test : \"+str(score_test))\n            #print(\"Score Train : \"+str(score_train))\n            #print(\"Score Test : \"+str(p))\n            #print(\"Score Train : \"+str(pp))\n\n            test_score += score_test\n            #p_test_score += p\n            # train_score += score_train\n            #test_preds += y_pred_proba/float(SPLITS)\n        test_preds += y_pred_proba.reshape(-1,1)\n        \n        #print_submission_into_file(y_pred_proba, df_test_id, \"_ALL_\"+str(name)+'_'+str(SPLITS)+'_splits_'+str(count))\n    \n    test_preds /= float(SPLITS)\n    test_preds = test_preds / float(10)\n     \n    test_preds = function_vectorize_pred_result(test_preds) \n    \n    stest = test_score / float(SPLITS)\n    #s_rmselog = p_test_score / float(SPLITS)\n    print(\"Score Test : \"+str(stest))\n    #print(\"Score Test RMSE LOG : \"+str(s_rmselog))\n    '''\n    strain = train_score / float(SPLITS)\n    print(\"Score Train : \"+str(strain))\n    '''\n    \n    test_preds = test_preds.flatten().flatten()\n    print_submission_into_file(test_preds.ravel(), df_test_id, \"_S2_\"+str(name))\n    \n    if name == \"XGBOOST\":\n        print_submission(test_preds.ravel(), df_test_id)\n\n    #score_test = accuracy_score(Y.to_numpy(), oof.flatten())\n    #print(\"Score Test FINAL : \"+str(score_test))\n    \n    \n    \nprint(\"OK\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}