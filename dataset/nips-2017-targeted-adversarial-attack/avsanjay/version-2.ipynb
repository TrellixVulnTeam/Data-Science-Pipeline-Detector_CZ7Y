{"cells":[{"metadata":{"_uuid":"b3e830764983c610f9dcc9e754ea7e00385bb591","_cell_guid":"0e9423e8-eab6-482b-8ae0-c72ff59d1c30","collapsed":true},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport csv\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.misc import imread\nfrom scipy.misc import imsave\n\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.nets import inception\n\nslim = tf.contrib.slim\n\nimport sys\n\n#sys.path.insert(0, r\"./cleverhans\")\n\nimport sys\nsys.path.insert(0, r\"C:\\Users\\avsanjay\\PycharmProjects\\kaggle\\cleverhans-master\\cleverhans-master\")\n\nfrom cleverhans.attacks import FastGradientMethod\nfrom cleverhans.attacks import BasicIterativeMethod\nfrom cleverhans.attacks import CarliniWagnerL2\n\ntensorflow_master = \"\"\n\ninput_dir = r\"./input\"\n\n\noutput_dir = \"./output\"\ntf.flags.DEFINE_string(\n    'master', '', 'The address of the TensorFlow master to use.')\n\ntf.flags.DEFINE_string(\n    'checkpoint_path', '/home/nrajeshrao/projects/kaggle/nips_2017/kagglesubmission1/inception_v3.ckpt',\n    'Path to checkpoint for inception network.')\n\ntf.flags.DEFINE_string(\n    'input_dir', '/home/nrajeshrao/projects/kaggle/nips_2017/kagglesubmission1/input', 'Input directory with images.')\n\ntf.flags.DEFINE_string(\n    'output_dir', '/home/nrajeshrao/projects/kaggle/nips_2017/kagglesubmission1/output',\n    'Output directory with images.')\n\ntf.flags.DEFINE_float(\n    'max_epsilon', 16.0, 'Maximum size of adversarial perturbation.')\n\ntf.flags.DEFINE_integer(\n    'image_width', 299, 'Width of each input images.')\n\ntf.flags.DEFINE_integer(\n    'image_height', 299, 'Height of each input images.')\n\ntf.flags.DEFINE_integer(\n    'batch_size', 16, 'How many images process at one time.')\n\nFLAGS = tf.flags.FLAGS\n\n\ndef load_target_class(input_dir):\n    \"\"\"Loads target classes.\"\"\"\n    with tf.gfile.Open(os.path.join(input_dir, 'target_class.csv')) as f:\n        return {row[0]: int(row[1]) for row in csv.reader(f) if len(row) >= 2}\n\n\ndef load_images(input_dir, batch_shape):\n    \"\"\"Read png images from input directory in batches.\n  \n    Args:\n      input_dir: input directory\n      batch_shape: shape of minibatch array, i.e. [batch_size, height, width, 3]\n  \n    Yields:\n      filenames: list file names without path of each image\n        Lenght of this list could be less than batch_size, in this case only\n        first few images of the result are elements of the minibatch.\n      images: array with all images from this batch\n    \"\"\"\n    # input_dir = r\"C:\\Users\\avsanjay\\PycharmProjects\\kaggle\\cleverhans-master\\cleverhans-master\\examples\\nips17_adversarial_competition\\sample_attacks\\fgsm\\input\"\n    images = np.zeros(batch_shape)\n    filenames = []\n    idx = 0\n    batch_size = batch_shape[0]\n    for filepath in tf.gfile.Glob(os.path.join(input_dir, '*.png')):\n        # with tf.gfile.Open(filepath) as f:\n        # image = np.array(Image.open(f).convert('RGB')).astype(np.float) / 255.0\n        # Images for inception classifier are normalized to be in [-1, 1] interval.\n        # images[idx, :, :, :] = image * 2.0 - 1.0\n        with tf.gfile.Open(filepath, \"rb\") as f:\n            images[idx, :, :, :] = imread(f, mode='RGB').astype(np.float) * 2.0 / 255.0 - 1.0\n\n        filenames.append(os.path.basename(filepath))\n        idx += 1\n        if idx == batch_size:\n            yield filenames, images\n            filenames = []\n            images = np.zeros(batch_shape)\n            idx = 0\n    if idx > 0:\n        yield filenames, images\n\n\ndef save_images(images, filenames, output_dir):\n    \"\"\"Saves images to the output directory.\n  \n    Args:\n      images: array with minibatch of images\n      filenames: list of filenames without path\n        If number of file names in this list less than number of images in\n        the minibatch then only first len(filenames) images will be saved.\n      output_dir: directory where to save images\n    \"\"\"\n    for i, filename in enumerate(filenames):\n        # Images for inception classifier are normalized to be in [-1, 1] interval,\n        # so rescale them back to [0, 1].\n        with tf.gfile.Open(os.path.join(output_dir, filename), 'w') as f:\n            imsave(f, (images[i, :, :, :] + 1.0) * 0.5, format='png')\n\n\nglobLogits = 0\nglobEnd_points = 0\n\n\nclass InceptionModel(object):\n    \"\"\"Model class for CleverHans library.\"\"\"\n\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.built = False\n\n    def __call__(self, x_input):\n        \"\"\"Constructs model and return probabilities for given input.\"\"\"\n        reuse = True if self.built else None\n        with slim.arg_scope(inception.inception_v3_arg_scope()):\n            logits, end_points = inception.inception_v3(\n                x_input, num_classes=self.num_classes, is_training=False,\n                reuse=reuse)\n        global globLogits\n        global globEnd_points\n\n        # globLogits = logits\n        # globEnd_points = end_points\n        self.built = True\n        output = end_points['Predictions']\n        # Strip off the extra reshape op at the output\n        probs = output.op.inputs[0]\n        return probs\n\n\neps = 2.0 * FLAGS.max_epsilon / 255.0\nbatch_shape = [FLAGS.batch_size, FLAGS.image_height, FLAGS.image_width, 3]\nnum_classes = 1001\n\ncategories = pd.read_csv(r\"./input/categories.csv\")\n\nimage_classes = pd.read_csv(r\"./input/images.csv\")\n\nall_images_target_class = {image_classes[\"ImageId\"][i] + \".png\": image_classes[\"TargetClass\"][i] for i in\n                           image_classes.index}\n\noutput = pd.DataFrame.from_dict(all_images_target_class, orient='index')\noutput.to_csv(\n    r\"C:\\Users\\avsanjay\\PycharmProjects\\kaggle\\cleverhans-master\\cleverhans-master\\examples\\nips17_adversarial_competition\\sample_targeted_attacks\\step_target_class\\input\\target_class.csv\")\n\nimage_metadata = pd.DataFrame()\n\nfor filenames, images in load_images(input_dir, batch_shape):\n    image_metadata1 = pd.DataFrame({\"ImageId\": [f[:-4] for f in filenames]}).merge(image_classes, on=\"ImageId\")\n    frames = [image_metadata, image_metadata1]\n    image_metadata = pd.concat(frames)\n\nimage_metadata2 = image_metadata\nimage_metadata2['PredictedClass'] = 0\n\nimage_metadata2.index = range(len(image_metadata2.index))\n\nwith tf.Graph().as_default():\n    # Prepare graph\n    x_input = tf.placeholder(tf.float32, shape=batch_shape)\n\n    model = InceptionModel(num_classes)\n\n    target_class_input = tf.placeholder(tf.int32, shape=[FLAGS.batch_size])\n    one_hot_target_class = tf.one_hot(target_class_input, num_classes)\n\n    # session_creator = tf.train.ChiefSessionCreator(\n    #    scaffold=tf.train.Scaffold(saver=saver),\n    #    checkpoint_filename_with_path=FLAGS.checkpoint_path,\n    #    master=FLAGS.master)\n\n\n    # https://stackoverflow.com/questions/43245231/how-do-monitored-training-sessions-work\n    # with tf.train.MonitoredSession(session_creator=session_creator) as sess:\n    with tf.Session() as sess:\n        basicIterative = BasicIterativeMethod(model, sess=sess)\n        x_adv1 = basicIterative.generate(x_input, y_target=one_hot_target_class, eps=eps, clip_min=-1., clip_max=1.)\n\n        carliniWagner = CarliniWagnerL2(model, sess=sess)\n        x_adv2 = carliniWagner.generate(x_input, y_target=one_hot_target_class, clip_min=-1., clip_max=1.)\n\n        x_adv = (x_adv1 + x_adv2) / 2\n        x_adv = tf.clip_by_value(x_adv, -1.0, 1.0)\n\n        saver = tf.train.Saver(slim.get_model_variables())\n\n        ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_path)\n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(sess, ckpt.model_checkpoint_path)\n        sess.run(tf.global_variables_initializer())\n        for filenames, images in load_images(input_dir, batch_shape):\n            print(\"looping thru.... i should have multiple of this printouts\")\n            target_class_for_batch = (\n                [all_images_target_class[n] for n in filenames]\n                + [0] * (FLAGS.batch_size - len(filenames)))\n\n            targeted_images = sess.run(x_adv,\n                                       feed_dict={\n                                           x_input: images,\n                                           target_class_input: target_class_for_batch})\n\n            save_images(targeted_images, filenames, output_dir)","cell_type":"code","outputs":[],"execution_count":null}],"metadata":{"language_info":{"version":"3.6.1","pygments_lexer":"ipython3","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","nbconvert_exporter":"python","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat_minor":1,"nbformat":4}