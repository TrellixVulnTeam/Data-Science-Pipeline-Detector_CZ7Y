{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os, random, sys, time, re\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader,Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertModel, DistilBertConfig,DistilBertTokenizer,BertModel,BertTokenizer,RobertaTokenizer,RobertaModel\nfrom transformers import AutoModel, BertTokenizerFast","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_csv = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n\nsubm = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n\ny = (train_csv.target.values > 0).astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_csv[\"target\"].min())\nprint(train_csv[\"target\"].max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('../input/bert-base-uncased',do_lower_case=True)\ntokenizer(\"hello how are you\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from nltk.stem import WordNetLemmatizer\n# lem=WordNetLemmatizer()\n# train_csv.excerpt=train_csv.excerpt.apply(lambda x: lem.lemmatize(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length=max(train_csv.excerpt.apply(lambda x: len(tokenizer(x)['input_ids'])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitDataSet(Dataset):\n    def __init__(self, review,tokenizer,max_len,target=None,is_test=False):\n        self.review = review\n        self.target = target\n        self.is_test = is_test\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.review)\n    \n    def __getitem__(self, idx):\n        review = str(self.review[idx])\n        review = ' '.join(review.split())\n        global inputs\n        \n        inputs = self.tokenizer.encode_plus(\n            review,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True\n        )        \n        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n#         token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n        \n        if self.is_test:\n            return (ids,mask,test_csv['id'][idx])\n                    #token_type_ids)\n                 \n            \n        else:    \n            targets = torch.tensor(self.target[idx], dtype=torch.float)\n            return (ids,mask,targets)\n#                   token_type_ids)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traindata = LitDataSet(train_csv['excerpt'].values,tokenizer,max_length,train_csv['target'].values,is_test=False)\ntestdata = LitDataSet(test_csv['excerpt'].values,tokenizer,max_length,target=None,is_test=True)\n\ntrain_loader = DataLoader(traindata, batch_size=4,\n                       shuffle=True, num_workers=4)\ntest_loader = DataLoader(testdata, batch_size=1,\n                       shuffle=False, num_workers=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x,y,z=next(iter(train_loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = RobertaModel.from_pretrained('../input/roberta-base')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Bert(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = AutoModel.from_pretrained('../input/bert-base-uncased')\n#         self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(768, 1)\n    \n    def forward(self, idss, mask):\n        _,out = self.bert(idss,mask,return_dict=False)\n#         output = self.dropout(output)\n        out = self.fc(out)\n        return out\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = Bert().to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=torch.load(\"../input/commonlit-pytorch-bert/Robert_trained.pth\").to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_fn=nn.MSELoss()\n# optimizer=optim.AdamW(model.parameters(), 0.0001,\\\n#                     betas=(0.9, 0.999), weight_decay=1e-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# param_optimizer = list(model.named_parameters())\n# no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n# optimizer_parameters = [\n#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n#      'weight_decay': 0.0001},\n#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n#      'weight_decay': 0.0}\n#     ]  \noptimizer = optim.AdamW(model.parameters(), lr=2e-5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nkfold=KFold(3,shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs=4\nfor fold, (train_ids, val_ids) in enumerate(kfold.split(traindata)):\n    \n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n    print(f'FOLD {fold+1}')\n    print('--------------------------------')\n    t_loader = DataLoader(traindata, batch_size=16,\\\n                       num_workers=8,sampler=train_subsampler)\n    val_loader = DataLoader(traindata, batch_size=16,\n                    num_workers=8,sampler=val_subsampler)\n\n    for epoch in range(1, epochs+1):      \n                start_time = time.time()\n                tloss = [] \n                vloss=[]\n                model.train()\n\n                for texts, attns,target in tqdm(t_loader):\n                        texts, attns,target=texts.to(device), attns.to(device), target.to(device)\n                        optimizer.zero_grad()\n                        outputs = model(texts,attns)\n                        loss = loss_fn(outputs.view(-1).float(), target.view(-1).float()).float()\n                        tloss.append(loss.item())\n                        loss.backward()\n                        optimizer.step()\n        #                 scheduler.step()\n                tloss = np.array(tloss).mean()\n                del loss, outputs\n                model.eval()\n                with torch.no_grad():\n                    for texts, attns,target in (val_loader):\n                            texts, attns,target=texts.to(device), attns.to(device), target.to(device)\n                            outputs = model(texts,attns)\n                            loss = loss_fn(outputs.view(-1).float(), target.view(-1).float()).float()\n                            vloss.append(loss.item())\n\n                    vloss = np.array(vloss).mean()\n\n                print(f\"Epoch....:{epoch} t_oss...:{tloss}...vlosss:{vloss}\")\n                del loss, outputs\n            \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# epochs=7\n# from tqdm import tqdm\n# for epoch in range(1, epochs+1):      \n#             start_time = time.time()\n#             tloss = []          \n#             model.train()\n    \n#             for texts, attns,target in tqdm(train_loader):\n#                 texts, attns,target=texts.to(device), attns.to(device), target.to(device)\n#                 optimizer.zero_grad()\n#                 outputs = model(texts,attns)\n#                 loss = loss_fn(outputs.view(-1).float(), target.view(-1).float()).float()\n#                 tloss.append(loss.item())\n#                 loss.backward()\n#                 optimizer.step()\n# #                 scheduler.step()\n#             tloss = np.array(tloss).mean()\n# #             vloss = validation_fn(model, vloader, loss_fn)\n#             tmetric = tloss**.5\n# #             vmetric = vloss**.5\n# #             print(raw_line.format(epoch,tloss,tmetric,vmetric,(time.time()-start_time)/60**1))\n#             print(f\"Epoch....:{epoch} tloss...:{tloss}...tmetric:{tmetric}\")\n#             del loss, outputs\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model,\"bert_trained.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\npreds={}\nfor texts,attns,idx in tqdm(test_loader):\n                texts, attns=texts.to(device), attns.to(device)\n                outputs = model(texts,attns)\n                preds[idx[0]]=outputs.item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub=pd.DataFrame({\"id\":preds.keys(),\"target\":preds.values()})\nsub.to_csv(\"submission.csv\",index=False)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.target.max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}