{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport random\nimport math\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom torch import nn\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom transformers.tokenization_utils import BatchEncoding\nfrom torch.cuda.amp import GradScaler, autocast\n\n\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-17T01:35:30.74013Z","iopub.execute_input":"2021-07-17T01:35:30.740459Z","iopub.status.idle":"2021-07-17T01:35:37.552015Z","shell.execute_reply.started":"2021-07-17T01:35:30.740356Z","shell.execute_reply":"2021-07-17T01:35:37.551135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def merge_dicts(input):\n    # list of dict --> dict of list\n    keys = input[0].keys()\n    ret = dict()\n    for key in keys:\n        temp = [x[key] for x in input]\n        ret[key] = temp\n    return ret","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:35:37.555472Z","iopub.execute_input":"2021-07-17T01:35:37.555715Z","iopub.status.idle":"2021-07-17T01:35:37.56252Z","shell.execute_reply.started":"2021-07-17T01:35:37.55569Z","shell.execute_reply":"2021-07-17T01:35:37.56175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test_dataset(train_df, test_df, anchor_num = 50):\n    data = []\n    interval = len(train_df)//anchor_num\n    anchor_idx = [interval*x for x in range(anchor_num)]\n    sampled_train = train_df.sort_values(by='etarget')\n    sampled_train = train_df.iloc[anchor_idx]\n    for idx, row in test_df.iterrows():\n        df = pd.DataFrame()\n        df['anchor_text'] = sampled_train['excerpt']\n        df['anchor_target'] = sampled_train['target']\n        df['anchor_etarget'] = sampled_train['etarget']\n        df['excerpt'] = row['excerpt']\n        df['id'] = row['id']\n        data.append(df)\n    new_df = pd.concat(data, ignore_index=True, sort=False).reset_index(drop=True)\n    return new_df","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:35:37.566224Z","iopub.execute_input":"2021-07-17T01:35:37.566498Z","iopub.status.idle":"2021-07-17T01:35:37.575572Z","shell.execute_reply.started":"2021-07-17T01:35:37.566472Z","shell.execute_reply":"2021-07-17T01:35:37.574622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyValidationDataset(Dataset):\n\n    def __init__(self, df, tokenizer, exp=False) -> None:\n        super().__init__()\n        self.df = df\n        self.texts = df['excerpt'].drop_duplicates().tolist()\n        self.anchor_texts = df['anchor_text'].drop_duplicates().tolist()\n        self.anchor_targets = df['anchor_target'].tolist()\n        self.anchor_etargets = df['anchor_etarget'].tolist()\n        self.tokenizer = tokenizer\n        self.exp = exp\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n\n        inputs = self.tokenizer.encode_plus(text, return_tensors='pt')\n        anchor_inputs = self.tokenizer.batch_encode_plus([a for a in self.anchor_texts], \n                                                         max_length=512,\n                                                                return_tensors='pt',\n                                                                truncation=True,\n                                                                padding=True)\n        return inputs, anchor_inputs, {}\n\n    def __len__(self):\n        return len(self.texts)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:35:37.577295Z","iopub.execute_input":"2021-07-17T01:35:37.577759Z","iopub.status.idle":"2021-07-17T01:35:37.590116Z","shell.execute_reply.started":"2021-07-17T01:35:37.57772Z","shell.execute_reply":"2021-07-17T01:35:37.589179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyValidationCollator:\n\n    def __init__(self, token_pad_value=0, type_pad_value=1):\n        super().__init__()\n        self.token_pad_value = token_pad_value\n        self.type_pad_value = type_pad_value\n\n    def __call__(self, batch):\n        inputs, anchor_inputs, labels = zip(*batch)\n\n        tokens = pad_sequence([d['input_ids'][0] for d in inputs], batch_first=True,\n                              padding_value=self.token_pad_value)\n        masks = pad_sequence([d['attention_mask'][0]\n                              for d in inputs], batch_first=True, padding_value=0)\n        features = {\n            'input_ids': tokens,\n            'attention_mask': masks\n        }\n        if 'token_type_ids' in inputs[0]:\n            type_ids = pad_sequence(\n                [d['token_type_ids'][0] for d in inputs], batch_first=True, padding_value=self.type_pad_value)\n            features['token_type_ids'] = type_ids\n\n        anchor_fetures = anchor_inputs[0]\n\n        labels = merge_dicts(labels)\n        for key, value in labels.items():\n            labels[key] = torch.cat(value, dim=0)\n        return {'features':features, 'anchor_features':anchor_fetures}, labels\n","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:35:37.591151Z","iopub.execute_input":"2021-07-17T01:35:37.591392Z","iopub.status.idle":"2021-07-17T01:35:37.604611Z","shell.execute_reply.started":"2021-07-17T01:35:37.591368Z","shell.execute_reply":"2021-07-17T01:35:37.603665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n\n    def __init__(self, pretrained_model_name, config=None, pooling='cls', grad_checkpoint=False, **kwargs):\n        super().__init__()\n        if config is not None:\n            self.bert = AutoModel.from_config(config)\n        else:\n            config = AutoConfig.from_pretrained(pretrained_model_name)\n            if grad_checkpoint:\n                self.bert.config.gradient_checkpointing = True\n            if kwargs.get('hidden_dropout'):\n                config.hidden_dropout_prob = kwargs['hidden_drop']   \n            if kwargs.get('attention_dropput'):\n                config.attention_probs_dropout_prob = kwargs['attention_dropout']\n            if kwargs.get('layer_norm_eps'):\n                config.layer_norm_eps = kwargs['layer_norm_eps']\n            self.bert = AutoModel.from_pretrained(pretrained_model_name, config=config)\n        self.pooling = pooling\n        \n             \n        self.hidden_size = self.bert.config.hidden_size\n\n        self.attention = nn.Sequential(            \n            nn.Linear(self.hidden_size, 256),            \n            nn.GELU(),                       \n            nn.Linear(256, 1)\n        )\n\n    def forward(self, features):\n\n        output_states = self.bert(input_ids=features.get('input_ids'),\n                                  attention_mask=features.get(\n                                      'attention_mask'),\n                                  token_type_ids=features.get('token_type_ids'))\n        out = output_states[0]  # embedding for all tokens\n        if self.pooling == 'cls':\n            pooled_out = out[:, 0, :]  # CLS token is first token\n        elif self.pooling == 'mean':\n            attention_mask = features['attention_mask']\n            input_mask_expanded = attention_mask.unsqueeze(\n                -1).expand(out.size()).float()\n            sum_embeddings = torch.sum(out * input_mask_expanded, 1)\n            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n            pooled_out = sum_embeddings / sum_mask\n        elif self.pooling=='att':\n            weights = self.attention(out)\n            attention_mask = features['attention_mask'].unsqueeze(\n                -1).expand(weights.size())\n            weights.masked_fill_(attention_mask==0, -float('inf'))\n            weights = torch.softmax(weights, dim=1)\n            pooled_out = torch.sum(out*weights, dim=1)\n        return pooled_out, out","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:35:37.606824Z","iopub.execute_input":"2021-07-17T01:35:37.60726Z","iopub.status.idle":"2021-07-17T01:35:37.623085Z","shell.execute_reply.started":"2021-07-17T01:35:37.607223Z","shell.execute_reply":"2021-07-17T01:35:37.622379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Comparer(nn.Module):\n\n    def __init__(self, pretrained_model_name, config=None, pooling='mean', esim=True, grad_checkpoint=False):\n        super().__init__()\n        self.encoder = Encoder(pretrained_model_name, config=config,\n                               pooling=pooling, grad_checkpoint=grad_checkpoint)\n        self.esim = esim\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(self.encoder.hidden_size*2, self.encoder.hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(self.encoder.hidden_size, 1)\n        )\n\n        self.etarget_head = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.encoder.hidden_size, self.encoder.hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(self.encoder.hidden_size, 1)\n        )\n\n    def inference(self, seq1, seq2, mask1=None, mask2=None):\n        # seq1: B*l1*D\n        # seq2: B*l2*D\n        # mask1: B*l1\n        # mask2: B*l2\n        score = torch.bmm(seq1, seq2.permute(0, 2, 1))  # B*l1*l2\n        # score1 = mask2.unsqueeze(1).expand(score.size())\n        # score1.masked_fill_(score1==0, -float('inf'))\n        # score1 = torch.softmax(score, dim=-1)\n        new_seq1 = torch.bmm(torch.softmax(score, dim=-1), seq2*mask2.unsqueeze(-1))\n        # del score1\n\n        # score2 = mask1.unsqueeze(-1).expand(score.size())\n        # score2.masked_fill_(score2==0, -float('inf'))\n        # score2 = torch.softmax(score.masked_fill(score2==0, -float('inf')), dim=1)\n        new_seq2 = torch.bmm(torch.softmax(score, dim=1).permute(0, 2, 1), seq1*mask1.unsqueeze(-1))\n\n        new_seq1 = torch.sum(new_seq1*mask1.unsqueeze(-1),dim=1)/torch.sum(mask1)\n        new_seq2 = torch.sum(new_seq2*mask2.unsqueeze(-1), dim=1)/torch.sum(mask2)\n        return new_seq1, new_seq2\n\n    def forward(self, features, anchor_features=None):\n\n        pooled_emb, seq_emb = self.encoder(features)\n\n        etarget_out = self.etarget_head(pooled_emb)\n\n        bs, length, dim = seq_emb.size()\n        if anchor_features is None:\n            # embeddings: B*D\n            # 111,222,333\n            pooled_emb1 = pooled_emb.unsqueeze(\n                1).expand(-1, bs, -1).reshape(-1, dim)\n            # 123,123,123\n            pooled_emb2 = pooled_emb.unsqueeze(\n                0).expand(bs, -1, -1).reshape(-1, dim)\n\n            seq_emb1 = seq_emb.unsqueeze(\n                1).expand(-1, bs, -1, -1).reshape(-1, length, dim)\n            seq_emb2 = seq_emb.unsqueeze(0).expand(\n                bs, -1, -1, -1).reshape(-1, length, dim)\n\n            mask1 = features['attention_mask'].unsqueeze(\n                1).expand(-1, bs, -1).reshape(-1, length)\n            mask2 = features['attention_mask'].unsqueeze(\n                0).expand(bs, -1, -1).reshape(-1, length)\n            new_emb1, new_emb2 = self.inference(\n                seq_emb1, seq_emb2, mask1, mask2)\n        else:\n            anchor_pooled_emb, anchor_seq_emb = self.encoder(anchor_features)\n            anchor_bs, _ = anchor_pooled_emb.size()\n            pooled_emb = pooled_emb.unsqueeze(\n                1).expand(-1, anchor_bs, -1).reshape(-1, dim)\n            anchor_pooled_emb = anchor_pooled_emb.repeat(bs, 1)\n            pooled_emb1 = pooled_emb\n            pooled_emb2 = anchor_pooled_emb\n\n            seq_emb1 = seq_emb.unsqueeze(\n                1).expand(-1, anchor_bs, -1, -1).reshape(-1, length, dim)\n            seq_emb2 = anchor_seq_emb.repeat(bs, 1, 1)\n\n            mask1 = features['attention_mask'].unsqueeze(\n                1).expand(-1, anchor_bs, -1).reshape(-1, length)\n            mask2 = anchor_features['attention_mask'].repeat(bs, 1)\n            new_emb1, new_emb2 = self.inference(seq_emb1, seq_emb2, mask1, mask2)\n\n        fc_input = torch.cat(\n            [pooled_emb1-pooled_emb2, new_emb2-new_emb1], dim=1)\n        # fc_input = pooled_emb1-pooled_emb2+new_emb2-new_emb1], dim=1)\n        output = self.fc(fc_input)\n        ret = {'pred': output, 'etarget_pred': etarget_out}\n        return ret\n\ndef _prepare_inputs(inputs):\n    for k, v in inputs.items():\n        if isinstance(v, torch.Tensor):\n            inputs[k] = v.cuda()\n        elif isinstance(v, BatchEncoding): # for embedding training\n            inputs[k] = v.cuda()\n        elif isinstance(v, dict): # for embedding training\n            inputs[k] = _prepare_inputs(v)\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:35:37.624325Z","iopub.execute_input":"2021-07-17T01:35:37.624673Z","iopub.status.idle":"2021-07-17T01:35:37.650215Z","shell.execute_reply.started":"2021-07-17T01:35:37.624639Z","shell.execute_reply":"2021-07-17T01:35:37.649057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../input","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:35:37.653294Z","iopub.execute_input":"2021-07-17T01:35:37.653727Z","iopub.status.idle":"2021-07-17T01:35:38.324773Z","shell.execute_reply.started":"2021-07-17T01:35:37.653686Z","shell.execute_reply":"2021-07-17T01:35:38.323754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/clrp-compare-base/train_with_folds.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\ntokenizer = AutoTokenizer.from_pretrained('../input/clrp-roberta-large-att/')","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:35:38.328318Z","iopub.execute_input":"2021-07-17T01:35:38.32861Z","iopub.status.idle":"2021-07-17T01:35:38.577618Z","shell.execute_reply.started":"2021-07-17T01:35:38.328577Z","shell.execute_reply":"2021-07-17T01:35:38.576715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained('../input/clrp-roberta-large-att')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:35:38.579097Z","iopub.execute_input":"2021-07-17T01:35:38.579459Z","iopub.status.idle":"2021-07-17T01:35:44.817089Z","shell.execute_reply.started":"2021-07-17T01:35:38.579419Z","shell.execute_reply":"2021-07-17T01:35:44.816159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_results = []\nfor fold in range(5):\n    print(f'fold {fold}')\n    train_fold = train[train['fold']!=fold]\n    new_test = make_test_dataset(train_fold, test)\n    valid_set = MyValidationDataset(new_test, tokenizer)\n    valid_collator = MyValidationCollator(token_pad_value=tokenizer.pad_token_id, \n                          type_pad_value=tokenizer.pad_token_id)\n    loader = torch.utils.data.DataLoader(\n        valid_set,\n        shuffle=False,\n        batch_size=8,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4,\n    collate_fn=valid_collator\n    )\n    \n    state = torch.load(f'../input/clrp-roberta-large-att/best-model-{fold}.pt')\n    model = Comparer(None, config, pooling='att')\n    model.load_state_dict(state['model'])\n    model.eval()\n    model.cuda()\n    \n    results=[]\n    with torch.no_grad():\n        for inputs, _ in tqdm(loader): \n            inputs = _prepare_inputs(inputs)\n            with autocast(enabled=True):\n                outputs = model(**inputs)\n            results.append(outputs)\n    predicts = merge_dicts(results)\n    for key in predicts.keys():\n        predicts[key] = torch.cat(predicts[key], dim=0)\n    pred = torch.sigmoid(predicts['pred']).cpu().numpy()\n    \n    df = pd.DataFrame()\n    df['id']= new_test['id']\n    df['pred'] = pred.flatten()\n    df['anchor_target'] = new_test['anchor_etarget']\n    df['pred_etarget'] = df['pred']*df['anchor_target']/(1-df['pred'])\n    df['pred_etarget'] = np.clip(df['pred_etarget'],a_min = 0.025, a_max=5.6)\n    df['pred_target'] = np.log(df['pred_etarget'])\n    all_results.append(df[['id','pred_target']])\n    \n    del model\n    gc.collect()\n    \ndf = pd.concat(all_results, ignore_index=True, sort=False)\npred = df.groupby('id')['pred_target'].mean().reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:38:48.943001Z","iopub.execute_input":"2021-07-17T01:38:48.943423Z","iopub.status.idle":"2021-07-17T01:40:17.766801Z","shell.execute_reply.started":"2021-07-17T01:38:48.943366Z","shell.execute_reply":"2021-07-17T01:40:17.765982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred['target'] = pred['pred_target']\npred[['id','target']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:36:17.924027Z","iopub.status.idle":"2021-07-17T01:36:17.924396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(all_results[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:36:17.925441Z","iopub.status.idle":"2021-07-17T01:36:17.925949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:36:17.926945Z","iopub.status.idle":"2021-07-17T01:36:17.927524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T01:36:17.928758Z","iopub.status.idle":"2021-07-17T01:36:17.929324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}