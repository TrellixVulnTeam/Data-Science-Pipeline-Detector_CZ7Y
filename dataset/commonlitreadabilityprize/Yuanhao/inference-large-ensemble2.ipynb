{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport random\nimport math\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom torch import nn\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom transformers.tokenization_utils import BatchEncoding\nfrom torch.cuda.amp import GradScaler, autocast\n\n\nfrom transformers import AutoModel, AutoConfig, AutoTokenizer\nimport gc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-01T03:05:35.80709Z","iopub.execute_input":"2021-08-01T03:05:35.807795Z","iopub.status.idle":"2021-08-01T03:05:43.235989Z","shell.execute_reply.started":"2021-08-01T03:05:35.807626Z","shell.execute_reply":"2021-08-01T03:05:43.234937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def merge_dicts(input):\n    # list of dict --> dict of list\n    keys = input[0].keys()\n    ret = dict()\n    for key in keys:\n        temp = [x[key] for x in input]\n        ret[key] = temp\n    return ret","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:43.240195Z","iopub.execute_input":"2021-08-01T03:05:43.24049Z","iopub.status.idle":"2021-08-01T03:05:43.248083Z","shell.execute_reply.started":"2021-08-01T03:05:43.24046Z","shell.execute_reply":"2021-08-01T03:05:43.24692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test_dataset(train_df, test_df, anchor_num = 50):\n    data = []\n    interval = len(train_df)//anchor_num\n    anchor_idx = [interval*x for x in range(anchor_num)]\n    sampled_train = train_df.sort_values(by='etarget')\n    sampled_train = train_df.iloc[anchor_idx]\n    for idx, row in test_df.iterrows():\n        df = pd.DataFrame()\n        df['anchor_text'] = sampled_train['excerpt']\n        df['anchor_target'] = sampled_train['target']\n        df['anchor_etarget'] = sampled_train['etarget']\n        df['excerpt'] = row['excerpt']\n        df['id'] = row['id']\n        data.append(df)\n    new_df = pd.concat(data, ignore_index=True, sort=False).reset_index(drop=True)\n    return new_df","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:43.250822Z","iopub.execute_input":"2021-08-01T03:05:43.251191Z","iopub.status.idle":"2021-08-01T03:05:43.264308Z","shell.execute_reply.started":"2021-08-01T03:05:43.251132Z","shell.execute_reply":"2021-08-01T03:05:43.262933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyValidationDataset(Dataset):\n\n    def __init__(self, df, tokenizer, exp=False) -> None:\n        super().__init__()\n        self.df = df\n        self.texts = df['excerpt'].drop_duplicates().tolist()\n        self.anchor_texts = df['anchor_text'].drop_duplicates().tolist()\n        self.anchor_targets = df['anchor_target'].tolist()\n        self.anchor_etargets = df['anchor_etarget'].tolist()\n        self.tokenizer = tokenizer\n        self.exp = exp\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n\n        inputs = self.tokenizer.encode_plus(text, return_tensors='pt')\n        anchor_inputs = self.tokenizer.batch_encode_plus([a for a in self.anchor_texts], \n                                                         max_length=512,\n                                                                return_tensors='pt',\n                                                                truncation=True,\n                                                                padding=True)\n        return inputs, anchor_inputs, {}\n\n    def __len__(self):\n        return len(self.texts)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:43.267002Z","iopub.execute_input":"2021-08-01T03:05:43.267571Z","iopub.status.idle":"2021-08-01T03:05:43.279852Z","shell.execute_reply.started":"2021-08-01T03:05:43.267527Z","shell.execute_reply":"2021-08-01T03:05:43.278273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_pooling_mask(encode_result):\n    stm = encode_result['special_tokens_mask'][0]\n    for sep_pos in range(1, len(stm)):\n        if stm[sep_pos] ==1:\n            break\n    mask = torch.LongTensor([[1]*sep_pos+[2]*(len(stm)-sep_pos)])\n    encode_result['pooling_mask'] = mask\n    return encode_result\n\nclass OneBertValidationDataset(Dataset):\n\n    def __init__(self, df, tokenizer, exp=False) -> None:\n        super().__init__()\n        self.df = df\n        self.texts = df['excerpt'].tolist()\n        self.anchor_texts = df['anchor_text'].tolist()\n        self.anchor_targets = df['anchor_target'].tolist()\n        self.anchor_etargets = df['anchor_etarget'].tolist()  \n        self.tokenizer = tokenizer\n        self.exp = exp\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n\n        inputs = self.tokenizer.encode_plus(text, self.anchor_texts[idx], \n                                             return_tensors='pt', return_special_tokens_mask=True,\n                                             truncation='only_second',\n                                             max_length=512,\n                                             padding=True)\n        inputs = add_pooling_mask(inputs)\n        return inputs, {}\n\n    def __len__(self):\n        return len(self.texts)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:43.281685Z","iopub.execute_input":"2021-08-01T03:05:43.282414Z","iopub.status.idle":"2021-08-01T03:05:43.296475Z","shell.execute_reply.started":"2021-08-01T03:05:43.282368Z","shell.execute_reply":"2021-08-01T03:05:43.29541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyValidationCollator:\n\n    def __init__(self, token_pad_value=0, type_pad_value=1):\n        super().__init__()\n        self.token_pad_value = token_pad_value\n        self.type_pad_value = type_pad_value\n\n    def __call__(self, batch):\n        inputs, anchor_inputs, labels = zip(*batch)\n\n        tokens = pad_sequence([d['input_ids'][0] for d in inputs], batch_first=True,\n                              padding_value=self.token_pad_value)\n        masks = pad_sequence([d['attention_mask'][0]\n                              for d in inputs], batch_first=True, padding_value=0)\n        features = {\n            'input_ids': tokens,\n            'attention_mask': masks\n        }\n        if 'token_type_ids' in inputs[0]:\n            type_ids = pad_sequence(\n                [d['token_type_ids'][0] for d in inputs], batch_first=True, padding_value=self.type_pad_value)\n            features['token_type_ids'] = type_ids\n\n        anchor_fetures = anchor_inputs[0]\n\n        labels = merge_dicts(labels)\n        for key, value in labels.items():\n            labels[key] = torch.cat(value, dim=0)\n        return {'features':features, 'anchor_features':anchor_fetures}, labels\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:43.298107Z","iopub.execute_input":"2021-08-01T03:05:43.298915Z","iopub.status.idle":"2021-08-01T03:05:43.312896Z","shell.execute_reply.started":"2021-08-01T03:05:43.298866Z","shell.execute_reply":"2021-08-01T03:05:43.31144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OneBertCompareCollator:\n\n    def __init__(self, token_pad_value=0, type_pad_value=1):\n        super().__init__()\n        self.token_pad_value = token_pad_value\n        self.type_pad_value = type_pad_value\n\n    def __call__(self, batch):\n        inputs, labels = zip(*batch)\n\n        tokens = pad_sequence([d['input_ids'][0] for d in inputs], batch_first=True,\n                              padding_value=self.token_pad_value)\n        masks = pad_sequence([d['attention_mask'][0]\n                              for d in inputs], batch_first=True, padding_value=0)\n        pooling_mask = pad_sequence([d['pooling_mask'][0]\n                              for d in inputs], batch_first=True, padding_value=0)\n        features = {\n            'input_ids': tokens,\n            'attention_mask': masks,\n            'pooling_mask': pooling_mask\n        }\n        if 'token_type_ids' in inputs[0]:\n            type_ids = pad_sequence(\n                [d['token_type_ids'][0] for d in inputs], batch_first=True, padding_value=self.type_pad_value)\n            features['token_type_ids'] = type_ids\n\n        labels = merge_dicts(labels)\n        for key, value in labels.items():\n            labels[key] = torch.cat(value, dim=0)\n        \n        return {'features':features}, labels","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:43.314995Z","iopub.execute_input":"2021-08-01T03:05:43.315676Z","iopub.status.idle":"2021-08-01T03:05:43.329622Z","shell.execute_reply.started":"2021-08-01T03:05:43.315632Z","shell.execute_reply":"2021-08-01T03:05:43.328037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n\n    def __init__(self, pretrained_model_name, config=None, pooling='cls', grad_checkpoint=False, **kwargs):\n        super().__init__()\n        if config is not None:\n            self.bert = AutoModel.from_config(config)\n        else:\n            config = AutoConfig.from_pretrained(pretrained_model_name)\n            if grad_checkpoint:\n                self.bert.config.gradient_checkpointing = True\n            if kwargs.get('hidden_dropout'):\n                config.hidden_dropout_prob = kwargs['hidden_drop']   \n            if kwargs.get('attention_dropput'):\n                config.attention_probs_dropout_prob = kwargs['attention_dropout']\n            if kwargs.get('layer_norm_eps'):\n                config.layer_norm_eps = kwargs['layer_norm_eps']\n            self.bert = AutoModel.from_pretrained(pretrained_model_name, config=config)\n        self.pooling = pooling\n        \n        self.hidden_size = self.bert.config.hidden_size\n        if pooling!='cls':\n            self.bert.pooler = nn.Identity()\n        self.attention = nn.Sequential(            \n            nn.Linear(self.hidden_size, 256),            \n            nn.GELU(),                       \n            nn.Linear(256, 1)\n        )\n\n    def forward(self, features):\n\n        output_states = self.bert(input_ids=features.get('input_ids'),\n                                  attention_mask=features.get(\n                                      'attention_mask'),\n                                  token_type_ids=features.get('token_type_ids'))\n        out = output_states[0]  # embedding for all tokens\n        if self.pooling == 'cls':\n            pooled_out = output_states[1]  # CLS token is first token\n        elif self.pooling == 'mean':\n            attention_mask = features['attention_mask']\n            input_mask_expanded = attention_mask.unsqueeze(\n                -1).expand(out.size()).float()\n            sum_embeddings = torch.sum(out * input_mask_expanded, 1)\n            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n            pooled_out = sum_embeddings / sum_mask\n        elif self.pooling=='att':\n            weights = self.attention(out)\n            attention_mask = features['attention_mask'].unsqueeze(\n                -1).expand(weights.size())\n            weights.masked_fill_(attention_mask==0, -float('inf'))\n            weights = torch.softmax(weights, dim=1)\n            pooled_out = torch.sum(out*weights, dim=1)\n        return pooled_out, out","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:43.334301Z","iopub.execute_input":"2021-08-01T03:05:43.334685Z","iopub.status.idle":"2021-08-01T03:05:43.351906Z","shell.execute_reply.started":"2021-08-01T03:05:43.334655Z","shell.execute_reply":"2021-08-01T03:05:43.350659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Comparer(nn.Module):\n\n    def __init__(self, pretrained_model_name, config=None, pooling='mean', esim=True, grad_checkpoint=False):\n        super().__init__()\n        self.encoder = Encoder(pretrained_model_name, config=config,\n                               pooling=pooling, grad_checkpoint=grad_checkpoint)\n        self.esim = esim\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(self.encoder.hidden_size*2, self.encoder.hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(self.encoder.hidden_size, 1)\n        )\n\n        self.anchor_pooled_emb, self.anchor_seq_emb = None, None\n\n    def inference(self, seq1, seq2, mask1=None, mask2=None):\n        # seq1: B*l1*D\n        # seq2: B*l2*D\n        # mask1: B*l1\n        # mask2: B*l2\n\n        score = torch.bmm(seq1, seq2.permute(0, 2, 1))\n        new_seq1 = torch.bmm(torch.softmax(score, dim=-1), seq2*mask2.unsqueeze(-1)) #\n        new_seq1 = torch.sum(new_seq1*mask1.unsqueeze(-1),dim=1)/torch.sum(mask1, dim=1).unsqueeze(-1)\n        # del score1\n\n        new_seq2 = torch.bmm(torch.softmax(score, dim=1).permute(0, 2, 1), seq1*mask1.unsqueeze(-1)) #\n        new_seq2 = torch.sum(new_seq2*mask2.unsqueeze(-1), dim=1)/torch.sum(mask2, dim=1).unsqueeze(-1)\n        return new_seq1, new_seq2\n\n\n    def forward(self, features, anchor_features=None):\n\n        pooled_emb, seq_emb = self.encoder(features)\n\n#         etarget_out = self.etarget_head(pooled_emb)\n\n        bs, length, dim = seq_emb.size()\n        if anchor_features is None:\n            # embeddings: B*D\n            # 111,222,333\n            pooled_emb1 = pooled_emb.unsqueeze(\n                1).expand(-1, bs, -1).reshape(-1, dim)\n            # 123,123,123\n            pooled_emb2 = pooled_emb.unsqueeze(\n                0).expand(bs, -1, -1).reshape(-1, dim)\n\n            seq_emb1 = seq_emb.unsqueeze(\n                1).expand(-1, bs, -1, -1).reshape(-1, length, dim)\n            seq_emb2 = seq_emb.unsqueeze(0).expand(\n                bs, -1, -1, -1).reshape(-1, length, dim)\n\n            mask1 = features['attention_mask'].unsqueeze(\n                1).expand(-1, bs, -1).reshape(-1, length)\n            mask2 = features['attention_mask'].unsqueeze(\n                0).expand(bs, -1, -1).reshape(-1, length)\n            new_emb1, new_emb2 = self.inference(\n                seq_emb1, seq_emb2, mask1, mask2)\n        else:\n            if self.anchor_pooled_emb is None:\n                anchor_pooled_emb, anchor_seq_emb = self.encoder(anchor_features)\n                self.anchor_pooled_emb, self.anchor_seq_emb = anchor_pooled_emb, anchor_seq_emb\n            else:\n                anchor_pooled_emb, anchor_seq_emb = self.anchor_pooled_emb, self.anchor_seq_emb\n            anchor_bs, _ = anchor_pooled_emb.size()\n            pooled_emb = pooled_emb.unsqueeze(\n                1).expand(-1, anchor_bs, -1).reshape(-1, dim)\n            anchor_pooled_emb = anchor_pooled_emb.repeat(bs, 1)\n            pooled_emb1 = pooled_emb\n            pooled_emb2 = anchor_pooled_emb\n\n            seq_emb1 = seq_emb.unsqueeze(\n                1).expand(-1, anchor_bs, -1, -1).reshape(-1, length, dim)\n            seq_emb2 = anchor_seq_emb.repeat(bs, 1, 1)\n\n            mask1 = features['attention_mask'].unsqueeze(\n                1).expand(-1, anchor_bs, -1).reshape(-1, length)\n            mask2 = anchor_features['attention_mask'].repeat(bs, 1)\n            new_emb1, new_emb2 = self.inference(seq_emb1, seq_emb2, mask1, mask2)\n\n        fc_input = torch.cat(\n            [pooled_emb1-pooled_emb2, new_emb2-new_emb1], dim=1)\n        # fc_input = pooled_emb1-pooled_emb2+new_emb2-new_emb1], dim=1)\n        output = self.fc(fc_input)\n        ret = {'pred': output}\n        return ret\n\ndef _prepare_inputs(inputs):\n    for k, v in inputs.items():\n        if isinstance(v, torch.Tensor):\n            inputs[k] = v.cuda()\n        elif isinstance(v, BatchEncoding): # for embedding training\n            inputs[k] = v.cuda()\n        elif isinstance(v, dict): # for embedding training\n            inputs[k] = _prepare_inputs(v)\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:43.354559Z","iopub.execute_input":"2021-08-01T03:05:43.355218Z","iopub.status.idle":"2021-08-01T03:05:43.383218Z","shell.execute_reply.started":"2021-08-01T03:05:43.355172Z","shell.execute_reply":"2021-08-01T03:05:43.381636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OneBertComparer(nn.Module):\n\n    def __init__(self, pretrained_model_name, config=None, pooling='cls', sep_pooling=False):\n        super().__init__()\n        self.encoder = Encoder(pretrained_model_name,\n                               config=config, pooling='mean')\n        self.sep_pooling = sep_pooling\n        self.pooling = pooling\n        self.fc = nn.Sequential(\n            nn.Dropout(0.25),\n            nn.Linear(self.encoder.hidden_size, 256),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Linear(256, 1)\n        )\n\n        self.attention = nn.Sequential(            \n            nn.Linear(self.encoder.hidden_size, 256),            \n            nn.GELU(),                       \n            nn.Linear(256, 1)\n        )   \n\n    def masked_mean_pooling(self, seq_emb, pooling_mask):\n        mask1 = (pooling_mask == 1)\n        pooled_emb_1 = torch.sum(seq_emb*mask1.unsqueeze(-1), dim=1)/torch.sum(mask1, dim=1, keepdim=True)\n\n        mask2 = (pooling_mask == 2)\n        pooled_emb_2 = torch.sum(seq_emb*mask2.unsqueeze(-1), dim=1)/torch.sum(mask2, dim=1, keepdim=True)\n\n        return pooled_emb_1, pooled_emb_2\n\n    def masked_att_pooling(self, seq_emb, pooling_mask):\n        \n        weights = self.attention(seq_emb)\n        mask1 = (pooling_mask == 2).unsqueeze(-1).expand(weights.size())\n        weight1 = torch.softmax(weights.masked_fill(mask1, -float('inf')), dim=1)\n\n        pooled_emb_1 = torch.sum(seq_emb*weight1, dim=1)\n\n        mask2 = (pooling_mask == 1).unsqueeze(-1).expand(weights.size())\n        weight2 = torch.softmax(weights.masked_fill(mask2, -float('inf')), dim=1)\n        pooled_emb_2 = torch.sum(seq_emb*weight2, dim=1)\n\n        return pooled_emb_1, pooled_emb_2\n\n    def forward(self, features):\n        pooled_emb, seq_emb = self.encoder(features)\n        if self.sep_pooling:\n            if self.pooling=='att':\n                emb1, emb2 = self.masked_att_pooling(seq_emb, features['pooling_mask'])\n            else:\n                emb1, emb2 = self.masked_mean_pooling(seq_emb, features['pooling_mask'])\n            sep_pooled_emb = emb1-emb2\n            # output = self.fc(torch.cat([pooled_emb, sep_pooled_emb], dim=1))\n            output = self.fc(sep_pooled_emb)\n        else:\n            output = self.fc(pooled_emb)\n        ret = {'pred': output}\n        return ret","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:43.385523Z","iopub.execute_input":"2021-08-01T03:05:43.386087Z","iopub.status.idle":"2021-08-01T03:05:43.406501Z","shell.execute_reply.started":"2021-08-01T03:05:43.38604Z","shell.execute_reply":"2021-08-01T03:05:43.405322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../input","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:43.409581Z","iopub.execute_input":"2021-08-01T03:05:43.410663Z","iopub.status.idle":"2021-08-01T03:05:44.206632Z","shell.execute_reply.started":"2021-08-01T03:05:43.410615Z","shell.execute_reply":"2021-08-01T03:05:44.205309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/clrp-compare-base/train_with_folds.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\ntokenizer = AutoTokenizer.from_pretrained('../input/clrp-roberta-reg/')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:44.208614Z","iopub.execute_input":"2021-08-01T03:05:44.209152Z","iopub.status.idle":"2021-08-01T03:05:44.468628Z","shell.execute_reply.started":"2021-08-01T03:05:44.209087Z","shell.execute_reply":"2021-08-01T03:05:44.467595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = AutoConfig.from_pretrained('../input/clrp-roberta-reg')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:44.4712Z","iopub.execute_input":"2021-08-01T03:05:44.471521Z","iopub.status.idle":"2021-08-01T03:05:44.482133Z","shell.execute_reply.started":"2021-08-01T03:05:44.471492Z","shell.execute_reply":"2021-08-01T03:05:44.480959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_results = []\nfor fold in range(5):\n    print(f'fold {fold}')\n    train_fold = train[train['fold']!=fold]\n    new_test = make_test_dataset(train_fold, test, 50)\n    valid_set = MyValidationDataset(new_test, tokenizer)\n    valid_collator = MyValidationCollator(token_pad_value=tokenizer.pad_token_id, \n                          type_pad_value=tokenizer.pad_token_id)\n    loader = torch.utils.data.DataLoader(\n        valid_set,\n        shuffle=False,\n        batch_size=16,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4,\n    collate_fn=valid_collator\n    )\n    \n    state = torch.load(f'../input/clrp-roberta-reg/best-model-{fold}.pt')\n    model = Comparer(None, config, pooling='att')\n    model.load_state_dict(state['model'])\n    model.eval()\n    model.cuda()\n    \n    results=[]\n    with torch.no_grad():\n        for inputs, _ in tqdm(loader): \n            inputs = _prepare_inputs(inputs)\n            with autocast(enabled=True):\n                outputs = model(**inputs)\n            results.append(outputs)\n    predicts = merge_dicts(results)\n    for key in predicts.keys():\n        predicts[key] = torch.cat(predicts[key], dim=0)\n    pred = predicts['pred'].cpu().numpy()\n    \n    df = pd.DataFrame()\n    df['id']= new_test['id']\n    df['pred'] = pred.flatten()\n    df['anchor_target'] = new_test['anchor_target']\n    df['pred_target'] = df['pred']+df['anchor_target']\n    all_results.append(df[['id','pred_target']])\n    \n    del model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:05:44.483941Z","iopub.execute_input":"2021-08-01T03:05:44.484636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(5):\n    print(f'fold {fold}')\n    train_fold = train[train['fold']!=fold]\n    new_test = make_test_dataset(train_fold, test, 50)\n    valid_set = MyValidationDataset(new_test, tokenizer)\n    valid_collator = MyValidationCollator(token_pad_value=tokenizer.pad_token_id, \n                          type_pad_value=tokenizer.pad_token_id)\n    loader = torch.utils.data.DataLoader(\n        valid_set,\n        shuffle=False,\n        batch_size=16,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4,\n    collate_fn=valid_collator\n    )\n    \n    state = torch.load(f'../input/clrp-roberta-v2/best-model-{fold}.pt')\n    model = Comparer(None, config, pooling='att')\n    model.load_state_dict(state['model'])\n    model.eval()\n    model.cuda()\n    \n    results=[]\n    with torch.no_grad():\n        for inputs, _ in tqdm(loader): \n            inputs = _prepare_inputs(inputs)\n            with autocast(enabled=True):\n                outputs = model(**inputs)\n            results.append(outputs)\n    predicts = merge_dicts(results)\n    for key in predicts.keys():\n        predicts[key] = torch.cat(predicts[key], dim=0)\n    pred = torch.sigmoid(predicts['pred']).cpu().numpy()\n    \n    df = pd.DataFrame()\n    df['id']= new_test['id']\n    df['pred'] = pred.flatten()\n    df['anchor_target'] = new_test['anchor_etarget']\n    df['pred_etarget'] = df['pred']*df['anchor_target']/(1-df['pred'])\n    df['pred_etarget'] = np.clip(df['pred_etarget'],a_min = 0.025, a_max=5.6)\n    df['pred_target'] = np.log(df['pred_etarget'])\n    all_results.append(df[['id','pred_target']])\n    \n    del model\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# onebert binary\nfor fold in [0,1,2]:\n    print(f'fold {fold}')\n    train_fold = train[train['fold']!=fold]\n    new_test = make_test_dataset(train_fold, test, 20)\n    valid_set = OneBertValidationDataset(new_test, tokenizer)\n    valid_collator = OneBertCompareCollator(token_pad_value=tokenizer.pad_token_id, \n                          type_pad_value=tokenizer.pad_token_type_id)\n    loader = torch.utils.data.DataLoader(\n        valid_set,\n        shuffle=False,\n        batch_size=16,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4,\n    collate_fn=valid_collator\n    )\n    \n    state = torch.load(f'../input/clrp-roberta-one/best-model-{fold}.pt')\n    model = OneBertComparer(None, config, sep_pooling=True, pooling='att')\n    model.load_state_dict(state['model'])\n    model.eval()\n    model.cuda()\n    \n    results=[]\n    with torch.no_grad():\n        for inputs, _ in tqdm(loader): \n            inputs = _prepare_inputs(inputs)\n            with autocast(enabled=True):\n                outputs = model(**inputs)\n            results.append(outputs)\n    predicts = merge_dicts(results)\n    for key in predicts.keys():\n        predicts[key] = torch.cat(predicts[key], dim=0)\n    pred = torch.sigmoid(predicts['pred']).cpu().numpy()\n    \n    df = pd.DataFrame()\n    df['id']= new_test['id']\n    df['pred'] = pred.flatten()\n    df['anchor_target'] = new_test['anchor_etarget']\n    df['pred_etarget'] = df['pred']*df['anchor_target']/(1-df['pred'])\n    df['pred_etarget'] = np.clip(df['pred_etarget'],a_min = 0.025, a_max=5.6)\n    df['pred_target'] = np.log(df['pred_etarget'])\n    all_results.append(df[['id','pred_target']])\n    \n    del model\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# onebert reg\nfor fold in [2,3,4]:\n    print(f'fold {fold}')\n    train_fold = train[train['fold']!=fold]\n    new_test = make_test_dataset(train_fold, test, 20)\n    valid_set = OneBertValidationDataset(new_test, tokenizer)\n    valid_collator = OneBertCompareCollator(token_pad_value=tokenizer.pad_token_id, \n                          type_pad_value=tokenizer.pad_token_type_id)\n    loader = torch.utils.data.DataLoader(\n        valid_set,\n        shuffle=False,\n        batch_size=16,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4,\n    collate_fn=valid_collator\n    )\n    \n    state = torch.load(f'../input/clrp-onebert-reg/best-model-{fold}.pt')\n    model = OneBertComparer(None, config, sep_pooling=True, pooling='att')\n    model.load_state_dict(state['model'])\n    model.eval()\n    model.cuda()\n    \n    results=[]\n    with torch.no_grad():\n        for inputs, _ in tqdm(loader): \n            inputs = _prepare_inputs(inputs)\n            with autocast(enabled=True):\n                outputs = model(**inputs)\n            results.append(outputs)\n    predicts = merge_dicts(results)\n    for key in predicts.keys():\n        predicts[key] = torch.cat(predicts[key], dim=0)\n    pred = predicts['pred'].cpu().numpy()\n    \n    df = pd.DataFrame()\n    df['id']= new_test['id']\n    df['pred'] = pred.flatten()\n    df['anchor_target'] = new_test['anchor_target']\n    df['pred_target'] = df['pred']+df['anchor_target']\n    all_results.append(df[['id','pred_target']])\n    \n    del model\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat(all_results, ignore_index=True, sort=False)\npred = df.groupby('id')['pred_target'].mean().reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred['target'] = pred['pred_target']\npred[['id','target']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(all_results[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}