{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cleaning Text can be Fun. And, painful ;)\n\nIt's kind of an EDA but kind of Not an EDA. NDA :)\nSometimes you have to just play around with data to see what surprises might popup. \n\nGet your Bag on! We are going to work using the Bag of Words method with scikit-learn next.\n\n# Bag-of-Words!\n\nThe model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document. This can be done by assigning each word a unique number.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#taking only the id,excerpt,target\ndf = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\",usecols=[\"id\",\"excerpt\",\"target\"])\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint(\"train shape\",df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the bag-of-words model in which we are only concerned with encoding schemes that represent what words are present or the degree to which they are present in encoded documents without any information about order.\n\nSteps:\n* Create an instance of the CountVectorizer class\n* Call the fit() function in order to learn a vocabulary from one or more documents.\n* Call the transform() function on one or more documents as needed to encode each as a vector.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So what is all this text about anyway? Do we have sentences? punctuation? Capital letters, etc. etc. etc.?? Let's look at one to see. \n\nAnd then on to fun!","metadata":{}},{"cell_type":"code","source":"df.loc[0,'excerpt']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So. Well. We have some work to do here. \n1. We have capital letters.\n2. We have punctuation (which we may want/need to understand complexity).\n3. We have \\n for new line.\n4. And finally, we have sentences. \n\nMaybe a fun exercise will be to clean all of it up and test it in various ways! ","metadata":{}},{"cell_type":"code","source":"vectorizer = CountVectorizer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words_excerpt = df.loc[0,'excerpt']\nwords_excerpt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Intially attempted to just put this in as (words_excerpt) but ran into a string error. \n# added as array and no error. \nvector = vectorizer.fit([words_excerpt])\nvocab = vector.vocabulary_\n# summarize your tokenization of vocab that was just built\nprint(vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode your excerpt\nvect_enc = vectorizer.transform([words_excerpt])\nprint(vect_enc.shape)\nprint(type(vect_enc))\nprint(vect_enc.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 104 words in the vocab and encoded vectors have a length of 104. \n\nThe encoded vector is a sparse matrix. \n\nWe output an array version of the encoded vector showing a count of 1 when there is one occurrence and number counts for the rest. One shows up 19 times!","metadata":{}},{"cell_type":"markdown","source":"# Lets try TF-IDF\n\nThis is an acronym that stands for Term Frequency - Inverse Document Frequency which are the components of the resulting scores assigned to each word.\n\n* Term Frequency: This summarizes how often a given word appears within a document. Ùè∞Ä \n* Inverse Document Frequency: This downscales words that appear a lot across documents.\n\nTF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# create the transform\nvectorizer = TfidfVectorizer()\n#tokenize and build vocab\nvectorizer.fit([words_excerpt])\n# summarize\nprint(vectorizer.vocabulary_)\nprint(vectorizer.idf_)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode document\nvect_tf = vectorizer.transform([words_excerpt])\n# summarize encoded vector\nprint(vect_tf.shape)\nprint(vect_tf.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have normalized scores between 0 and 1 and the encoded document vectors can then be used directly with most machine learning algorithms.","metadata":{}},{"cell_type":"markdown","source":"# Hashing\n\nWhy Hash? The above counts and frequencies can be useful, however the vocabulary can become very large which will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms. \n\nWe can use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word.\n\nThe example below demonstrates the HashingVectorizer for encoding a single document. An arbitrary fixed-length vector size of 20 was chosen. This corresponds to the range of the hash function, where small values (like 20) may result in hash collisions.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import HashingVectorizer\n\nvectorizer = HashingVectorizer(n_features=20)\n# encode text\nvect_hash = vectorizer.transform([words_excerpt])\n# summarize\nprint(vect_hash.shape)\nprint(vect_hash.toarray())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running the example encodes the sample document as a 20-element sparse array. The values of the encoded document correspond to normalized word counts by default in the range of -1 to 1, but could be made simple integer counts by changing the default configuration.","metadata":{}}]}