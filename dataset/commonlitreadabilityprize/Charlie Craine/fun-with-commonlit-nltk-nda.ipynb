{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cleaning Text can be Fun. And, painful ;)\n\nIt's kind of an EDA but kind of Not an EDA. NDA :)\nSometimes you have to just play around with data to see what surprises might popup. \n\nThis code is using NLTK to clean up data. People will call it data cleaning, feature engineering. The intent is to just break the excerpt data down to common NLP data cleanup. \n\nI'll do another with scikit-learn next and link here.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#taking only the id,excerpt,target\ndf = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\",usecols=[\"id\",\"excerpt\",\"target\"])\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint(\"train shape\",df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets get NLTK on the job because doing all this manually is not fun. ","metadata":{}},{"cell_type":"code","source":"import nltk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So what is all this text about anyway? Do we have sentences? punctuation? Capital letters, etc. etc. etc.?? Let's look at one to see. \n\nAnd then on to fun!","metadata":{}},{"cell_type":"code","source":"df.loc[0,'excerpt']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So. Well. We have some work to do here. \n1. We have capital letters.\n2. We have punctuation (which we may want/need to understand complexity).\n3. We have \\n for new line.\n4. And finally, we have sentences. \n\nMaybe a fun exercise will be to clean all of it up and test it in various ways! ","metadata":{}},{"cell_type":"code","source":"from nltk import sent_tokenize\n\nsentence_excerpt = df.loc[0,'excerpt']\nsentences = sent_tokenize(sentence_excerpt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have now created one excerpt with sentences. \n\nPerhaps we should now create words.?! Should we rank words be difficulty level of reading? Perhaps we do that by how difficult a sentence is as well? It does make you think we might want to think about putting these into their own dataframes at some point. Right? ","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\nwords_excerpt = df.loc[0,'excerpt']\nwords = word_tokenize(words_excerpt)\nprint(words[:50])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We still see periods and commas and apostrophes. Now we should just filter out punctuation.","metadata":{}},{"cell_type":"code","source":"words_only = [word for word in words if word.isalpha()]\nprint(words_only[:50])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we can finally remove stop words. Again, these could be valuable at some point in determining whether words are more or less complex for reading skill levels. But for now let's remove and then we can build pipelines later. ","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nstop_words = stopwords.words('english')\nprint(stop_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline!\n\nLet's put it all together.\n\n* Load the raw text.\n* Split into tokens.\n* Convert to lowercase.\n* Remove punctuation from each token.\n* Filter out remaining tokens that are not alphabetic.\n* Filter out tokens that are stop words.","metadata":{}},{"cell_type":"code","source":"import re\nimport string\n\n# split into words\nwords_excerpt = df.loc[0,'excerpt']\nwords = word_tokenize(words_excerpt)\n\n#convert to lowercase\nwords = [w.lower() for w in words]\n\n# prepare regex for char filtering\nre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n\n# remove punctuation from each word\nstripped = [re_punc.sub('', w) for w in words]\n\n# remove words that are not alphabetic\nwords = [word for word in stripped if word.isalpha()]\n\n# filter out stop words\nstop_words = set(stopwords.words('english'))\nwords = [w for w in words if not w in stop_words]\nprint(words[:50])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running this pipeline you will see that in addition to all of the other transforms, stop words like 'a' and 'to' have been removed. However, you may end up with tokens like nt.","metadata":{}},{"cell_type":"markdown","source":"One final cleanup you might be keen on is stemming. Stemming refers to the process of reducing each word to its root or base. There are many stemming algorithms, although a popular and long-standing method is the Porter Stemming algorithm. This method is available in NLTK via the PorterStemmer class.","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer\n\nwords_excerpt = df.loc[0,'excerpt']\nwords = word_tokenize(words_excerpt)\nporter = PorterStemmer()\nstemmed = [porter.stem(word) for word in stem_words]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(stemmed[:100])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}