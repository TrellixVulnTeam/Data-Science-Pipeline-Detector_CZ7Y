{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cleaning Text can be Fun. And, painful ;)\n\nThe Keras deep learning library provides some basic tools to help you prepare your text data.\n\nMy goal with this notebook is to do the following: \n* Split words with text to word sequence\n* Encoding with one_hot\n* Hash Encoding with hashing trick\n* Tokenizer API","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#taking only the id,excerpt,target\ndf = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\",usecols=[\"id\",\"excerpt\",\"target\"])\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint(\"train shape\",df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An often necessary step in NLP is to split text it into words. Words are called tokens and the process of splitting text into tokens is called tokenization. Keras provides the text to word sequence() function that you can use to split text into a list of words.\n\nKeras provides text_to_word_sequence and that helps you with a few bits of feature engineering:\n1. splits words by space\n2. filters out punctuation\n3. converts text to lowercase (lower=True)","metadata":{}},{"cell_type":"markdown","source":"# Split words with text to word sequence","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import text_to_word_sequence","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[0,'excerpt']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words_excerpt = df.loc[0,'excerpt']\nwords_excerpt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = text_to_word_sequence(words_excerpt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Running the example creates an array containing all of the words in the document. The list\nof words is printed for review.","metadata":{}},{"cell_type":"code","source":"print(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a good start however we will need to do more pre-processing.","metadata":{}},{"cell_type":"markdown","source":"# Encoding with one_hot\n\nKeras provides the one hot() function to tokenize and integer encode a text document in one step. The name is a bit misleading as this isn't about one hot encoding -- isntead it offers hashing_trick() function that returns an integer encoded version of the document. The use of a hash function means that there may be collisions and not all words will be assigned unique integer values.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import one_hot\n\n# estimate the size of the vocab\nvocab_size = len(result)\nprint(vocab_size)\n# integrate encode the document\nresult_vs = one_hot(words_excerpt, round(vocab_size*1.3))\nprint(result_vs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The size of the vocabulary as 181. \n\nThe encoded items are printed as an array of integer encoded words.","metadata":{}},{"cell_type":"markdown","source":"# Hashing\n\nWhy Hash? The above counts and frequencies can be useful, however the vocabulary can become very large which will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms. \n\nWe can use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. This avoids the need to keep track of a vocabulary, which is faster and requires less memory.\n\nA downside is that the hash is a one-way function so there is no way to convert the encoding back to a word.\n\nBelow is an example of integer encoding a document using the md5 hash function.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import hashing_trick\n\nresult_hash = hashing_trick(words_excerpt, round(vocab_size*1.3), hash_function='md5')\nprint(result_hash)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The different hash function results in consistent, but different integers for words as the one_hot() function in the previous section.","metadata":{}},{"cell_type":"markdown","source":"# Tokenizer API\n\nKeras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple text documents. \n\nThe Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents.","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n# fit our result from above where we retrieved individual words\ntext = [result]\n# create the tokenizer\nt = Tokenizer()\n# fit the tokenizer on the documents\nt.fit_on_texts(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tokenizer provides four attributes that you can use to query what has been\nlearned about your documents:\n\n1. Word counts: A dictionary mapping of words and their occurrence counts when the Tokenizer was fit.\n2. Word docs: A dictionary mapping of words and the number of documents that reach appears in.\n3. Word index: A dictionary of words and their uniquely assigned integers.\n4. document count: A dictionary mapping and the number of documents they appear in calculated during the fit.","metadata":{}},{"cell_type":"code","source":"#1 Word Counts\nprint(t.word_counts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2 Word Docs\nprint(t.word_docs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3 Word Index\nprint(t.word_index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#4 Document Count\nprint(t.document_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More to come in future notebooks on NLP!","metadata":{}}]}