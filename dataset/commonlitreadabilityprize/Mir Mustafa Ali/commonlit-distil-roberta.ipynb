{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [CommonLit] ðŸ¤— distil-Roberta","metadata":{}},{"cell_type":"markdown","source":"Kickstarter for training model using ðŸ¤— trainer","metadata":{}},{"cell_type":"markdown","source":"#### Loading Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport transformers\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom transformers import TrainingArguments, Trainer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2021-05-22T04:01:29.825474Z","iopub.execute_input":"2021-05-22T04:01:29.825837Z","iopub.status.idle":"2021-05-22T04:01:29.83228Z","shell.execute_reply.started":"2021-05-22T04:01:29.825807Z","shell.execute_reply":"2021-05-22T04:01:29.83048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Loading files","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntrain = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:57:07.224907Z","iopub.execute_input":"2021-05-22T03:57:07.225356Z","iopub.status.idle":"2021-05-22T03:57:07.341243Z","shell.execute_reply.started":"2021-05-22T03:57:07.225315Z","shell.execute_reply":"2021-05-22T03:57:07.340081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Splitting data into training and validation set","metadata":{}},{"cell_type":"code","source":"train_df, val_df = train_test_split(train, test_size=.01)\nlen(train_df), len(val_df)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:57:07.343472Z","iopub.execute_input":"2021-05-22T03:57:07.343935Z","iopub.status.idle":"2021-05-22T03:57:07.365439Z","shell.execute_reply.started":"2021-05-22T03:57:07.343891Z","shell.execute_reply":"2021-05-22T03:57:07.364061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Loading model and tokenizer.\nmodel is a pretrained roberta sequence classifier","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('../input/distilrobertabasesequenceclassification/distilroberta-base-sequence_classification/')\nmodel = AutoModelForSequenceClassification.from_pretrained('../input/distilrobertabasesequenceclassification/distilroberta-base-sequence_classification/',num_labels=1 )\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:57:07.367638Z","iopub.execute_input":"2021-05-22T03:57:07.368089Z","iopub.status.idle":"2021-05-22T03:57:14.554551Z","shell.execute_reply.started":"2021-05-22T03:57:07.368017Z","shell.execute_reply":"2021-05-22T03:57:14.553377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### tokenizing the inputs","metadata":{"execution":{"iopub.status.busy":"2021-05-21T20:12:26.812746Z","iopub.execute_input":"2021-05-21T20:12:26.813221Z","iopub.status.idle":"2021-05-21T20:12:26.822078Z","shell.execute_reply.started":"2021-05-21T20:12:26.813188Z","shell.execute_reply":"2021-05-21T20:12:26.82013Z"}}},{"cell_type":"code","source":"train_encodings = tokenizer(list(train_df.excerpt), truncation=True, padding=True)\nval_encodings = tokenizer(list(val_df.excerpt), truncation=True, padding=True)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:57:14.556349Z","iopub.execute_input":"2021-05-22T03:57:14.556768Z","iopub.status.idle":"2021-05-22T03:57:16.43069Z","shell.execute_reply.started":"2021-05-22T03:57:14.556729Z","shell.execute_reply":"2021-05-22T03:57:16.429564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### creating Datasets class for training","metadata":{}},{"cell_type":"code","source":"class CommonLitDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n        \n        \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n        return item\n        \n        \n    def __len__(self):\n        return len(self.labels)\n    \n    \ntrain_dataset = CommonLitDataset(train_encodings, list(train_df.target))\nval_dataset = CommonLitDataset(val_encodings, list(val_df.target))\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:57:16.432279Z","iopub.execute_input":"2021-05-22T03:57:16.432707Z","iopub.status.idle":"2021-05-22T03:57:16.441173Z","shell.execute_reply.started":"2021-05-22T03:57:16.43265Z","shell.execute_reply":"2021-05-22T03:57:16.439799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We'll be using default hyperparameters for training","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',          # output directory  \n    per_device_train_batch_size=32,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    do_train=True,\n    do_eval=True, \n    report_to=\"none\",\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\"\n    \n)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:57:16.442972Z","iopub.execute_input":"2021-05-22T03:57:16.443939Z","iopub.status.idle":"2021-05-22T03:57:16.506633Z","shell.execute_reply.started":"2021-05-22T03:57:16.443894Z","shell.execute_reply":"2021-05-22T03:57:16.505672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = torch.nn.MSELoss()\n        loss = loss_fct( logits.view(-1, self.model.config.num_labels), \n                          labels.float().view(-1, model.config.num_labels))\n\n        return (loss, outputs) if return_outputs else loss\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:57:16.51008Z","iopub.execute_input":"2021-05-22T03:57:16.510521Z","iopub.status.idle":"2021-05-22T03:57:16.517454Z","shell.execute_reply.started":"2021-05-22T03:57:16.510481Z","shell.execute_reply":"2021-05-22T03:57:16.515936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = CommonLitTrainer(\n    model = model, \n    args = training_args, \n    train_dataset = train_dataset, \n    eval_dataset = val_dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:57:31.495966Z","iopub.execute_input":"2021-05-22T03:57:31.496445Z","iopub.status.idle":"2021-05-22T03:57:32.008824Z","shell.execute_reply.started":"2021-05-22T03:57:31.496412Z","shell.execute_reply":"2021-05-22T03:57:32.007536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's train !!!!","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:07:20.767765Z","iopub.execute_input":"2021-05-22T03:07:20.768143Z","iopub.status.idle":"2021-05-22T03:07:20.772194Z","shell.execute_reply.started":"2021-05-22T03:07:20.768105Z","shell.execute_reply":"2021-05-22T03:07:20.771206Z"}}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T03:57:32.753991Z","iopub.execute_input":"2021-05-22T03:57:32.754523Z","iopub.status.idle":"2021-05-22T04:00:29.143276Z","shell.execute_reply.started":"2021-05-22T03:57:32.75448Z","shell.execute_reply":"2021-05-22T04:00:29.140904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluating the model","metadata":{"execution":{"iopub.status.busy":"2021-05-22T02:49:44.559047Z","iopub.execute_input":"2021-05-22T02:49:44.559415Z","iopub.status.idle":"2021-05-22T02:49:44.566439Z","shell.execute_reply.started":"2021-05-22T02:49:44.559386Z","shell.execute_reply":"2021-05-22T02:49:44.564382Z"}}},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T04:00:42.539547Z","iopub.execute_input":"2021-05-22T04:00:42.539879Z","iopub.status.idle":"2021-05-22T04:00:43.079005Z","shell.execute_reply.started":"2021-05-22T04:00:42.53985Z","shell.execute_reply":"2021-05-22T04:00:43.077677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Save","metadata":{}},{"cell_type":"code","source":"tokenizer.save_pretrained('roberta-model')\ntrainer.save_model('roberta-model')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T04:00:54.049691Z","iopub.execute_input":"2021-05-22T04:00:54.050049Z","iopub.status.idle":"2021-05-22T04:00:55.412649Z","shell.execute_reply.started":"2021-05-22T04:00:54.050018Z","shell.execute_reply":"2021-05-22T04:00:55.411319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Dataset class and inference method is used from [notebook](https://www.kaggle.com/abhishek/fork-of-fork-of-yum-yum-yum-93f968) by [Abhishek Thakur](https://www.kaggle.com/abhishek)","metadata":{}},{"cell_type":"code","source":"# Todo merge this with the Training Dataset class above \nclass Dataset:\n    def __init__(self, excerpt, tokenizer, max_len):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n        }","metadata":{"execution":{"iopub.status.busy":"2021-05-22T04:00:57.476387Z","iopub.execute_input":"2021-05-22T04:00:57.476934Z","iopub.status.idle":"2021-05-22T04:00:57.488111Z","shell.execute_reply.started":"2021-05-22T04:00:57.4769Z","shell.execute_reply":"2021-05-22T04:00:57.486699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_predictions(model_path, max_len):\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model.to(\"cuda\")\n    model.eval()\n    \n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    \n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=4, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(\"cuda\")\n            output = model(**data)\n            output = output.logits.detach().cpu().numpy().ravel().tolist()\n            final_output.extend(output)\n    \n    torch.cuda.empty_cache()\n    return np.array(final_output)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T04:01:35.405384Z","iopub.execute_input":"2021-05-22T04:01:35.405759Z","iopub.status.idle":"2021-05-22T04:01:35.417743Z","shell.execute_reply.started":"2021-05-22T04:01:35.405731Z","shell.execute_reply":"2021-05-22T04:01:35.413715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### generating predictions","metadata":{}},{"cell_type":"code","source":"preds = generate_predictions('roberta-model', max_len=256)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T04:01:36.112495Z","iopub.execute_input":"2021-05-22T04:01:36.112893Z","iopub.status.idle":"2021-05-22T04:01:43.672837Z","shell.execute_reply.started":"2021-05-22T04:01:36.112861Z","shell.execute_reply":"2021-05-22T04:01:43.669257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Atlast!! Submitting predictions","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmission.target = preds\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T04:01:46.166781Z","iopub.execute_input":"2021-05-22T04:01:46.167353Z","iopub.status.idle":"2021-05-22T04:01:46.588832Z","shell.execute_reply.started":"2021-05-22T04:01:46.167289Z","shell.execute_reply":"2021-05-22T04:01:46.58754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Work in Progress !!!*","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}