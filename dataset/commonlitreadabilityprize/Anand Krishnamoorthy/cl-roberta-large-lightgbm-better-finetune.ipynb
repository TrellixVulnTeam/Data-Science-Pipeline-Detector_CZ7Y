{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimport pandas as pd\nfrom pandas import DataFrame\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\n\nimport re\n\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\ntorch.cuda.empty_cache()\n\nimport sys\nsys.path = [\n    '../input/readability-package',\n] + sys.path\nimport readability\nimport spacy\n\nfrom sklearn import model_selection\n\nimport transformers\nimport torch\nimport pytorch_lightning as pl\nfrom transformers import BertModel, BertTokenizer, BertForSequenceClassification\nfrom transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\n\nimport random\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\nfrom fastprogress.fastprogress import  progress_bar","metadata":{"_uuid":"0bba5c7e-9921-4079-b4cc-72d3f86f848b","_cell_guid":"b547acd6-e998-4cbb-9596-4a256748f902","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-29T15:22:31.172155Z","iopub.execute_input":"2021-07-29T15:22:31.172544Z","iopub.status.idle":"2021-07-29T15:22:38.382873Z","shell.execute_reply.started":"2021-07-29T15:22:31.172461Z","shell.execute_reply":"2021-07-29T15:22:38.382023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n#train_df=train_df.head(10)\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n\ntrain_df['excerpt'] = train_df['excerpt'].apply(lambda e: e.replace('\\n', ''))\ntest_df['excerpt'] = test_df['excerpt'].apply(lambda e: e.replace('\\n', ''))","metadata":{"_uuid":"6642bb9b-dbf2-4b43-9eb5-b66bf5c8a239","_cell_guid":"d24622d2-072f-43e1-b2c4-48f8e5f55a5f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-29T15:22:38.384529Z","iopub.execute_input":"2021-07-29T15:22:38.384841Z","iopub.status.idle":"2021-07-29T15:22:38.483005Z","shell.execute_reply.started":"2021-07-29T15:22:38.384811Z","shell.execute_reply":"2021-07-29T15:22:38.482182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning Texts Function","metadata":{"_uuid":"94f5cc66-486b-4dc5-ae19-2d9ad6f9f219","_cell_guid":"777bc61a-30ca-4cd6-9001-dc52e4fe0c58","trusted":true}},{"cell_type":"code","source":"train_df['excerpt_preprocessed'] =  train_df['excerpt']\ntrain_df['excerpt_preprocessed'] = train_df['excerpt_preprocessed'].replace('\\s+', ' ', regex=True)\n\ntest_df[\"excerpt_preprocessed\"] = train_df['excerpt']\ntest_df['excerpt_preprocessed'] = test_df['excerpt_preprocessed'].replace('\\s+', ' ', regex=True)","metadata":{"_uuid":"b4878df3-4f21-4e39-9f0c-69004f01f6ff","_cell_guid":"0e0e268b-84a1-446f-bdc3-560dc2c8ecf2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-29T15:22:38.485843Z","iopub.execute_input":"2021-07-29T15:22:38.486187Z","iopub.status.idle":"2021-07-29T15:22:38.496008Z","shell.execute_reply.started":"2021-07-29T15:22:38.486158Z","shell.execute_reply":"2021-07-29T15:22:38.495149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\n#pd.set_option('display.max_colwidth', None)\ntrain_df['excerpt_preprocessed'].head(3)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:22:38.498021Z","iopub.execute_input":"2021-07-29T15:22:38.498697Z","iopub.status.idle":"2021-07-29T15:22:38.509415Z","shell.execute_reply.started":"2021-07-29T15:22:38.498622Z","shell.execute_reply":"2021-07-29T15:22:38.508624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fetch some features","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"#source: https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline/data\n\ndef readability_measurements(passage: str):\n    \"\"\"\n    This function uses the readability library for feature engineering.\n    It includes textual statistics, readability scales and metric, and some pos stats\n    \"\"\"\n    results = readability.getmeasures(passage, lang='en')\n    \n    chars_per_word = results['sentence info']['characters_per_word']\n    syll_per_word = results['sentence info']['syll_per_word']\n    words_per_sent = results['sentence info']['words_per_sentence']\n    \n    \n    tobeverb = results['word usage']['tobeverb']\n    auxverb = results['word usage']['auxverb']\n    conjunction = results['word usage']['conjunction']\n    pronoun = results['word usage']['pronoun']\n    preposition = results['word usage']['preposition']\n    nominalization = results['word usage']['nominalization']\n    \n    pronoun_b = results['sentence beginnings']['pronoun']\n    interrogative = results['sentence beginnings']['interrogative']\n    article = results['sentence beginnings']['article']\n    subordination = results['sentence beginnings']['subordination']\n    conjunction_b = results['sentence beginnings']['conjunction']\n    preposition_b = results['sentence beginnings']['preposition']\n\n    \n    return [chars_per_word, syll_per_word, words_per_sent,\n            tobeverb, auxverb, conjunction, pronoun, preposition, nominalization,\n            pronoun_b, interrogative, article, subordination, conjunction_b, preposition_b]","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-29T15:22:38.512237Z","iopub.execute_input":"2021-07-29T15:22:38.512545Z","iopub.status.idle":"2021-07-29T15:22:38.521913Z","shell.execute_reply.started":"2021-07-29T15:22:38.512517Z","shell.execute_reply":"2021-07-29T15:22:38.521125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def spacy_features(df: pd.DataFrame):\n    \"\"\"\n    This function generates features using spacy en_core_wb_lg\n    I learned about this from these resources:\n    https://www.kaggle.com/konradb/linear-baseline-with-cv\n    https://www.kaggle.com/anaverageengineer/comlrp-baseline-for-complete-beginners\n    \"\"\"\n    \n    nlp = spacy.load('en_core_web_lg')\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in df.excerpt])\n        \n    return vectors\n\ndef get_spacy_col_names():\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n        \n    return names","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-29T15:22:38.523141Z","iopub.execute_input":"2021-07-29T15:22:38.523538Z","iopub.status.idle":"2021-07-29T15:22:38.533174Z","shell.execute_reply.started":"2021-07-29T15:22:38.523499Z","shell.execute_reply":"2021-07-29T15:22:38.532339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pos_tag_features(passage: str):\n    \"\"\"\n    This function counts the number of times different parts of speech occur in an excerpt\n    \"\"\"\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    \n    tags = pos_tag(word_tokenize(passage))\n    tag_list= list()\n    \n    for tag in pos_tags:\n        tag_list.append(len([i[0] for i in tags if i[1] == tag]))\n    \n    return tag_list","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-29T15:22:38.534667Z","iopub.execute_input":"2021-07-29T15:22:38.535155Z","iopub.status.idle":"2021-07-29T15:22:38.545316Z","shell.execute_reply.started":"2021-07-29T15:22:38.535115Z","shell.execute_reply":"2021-07-29T15:22:38.544311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_other_features(passage: str):\n    \"\"\"\n    This function is where I test miscellaneous features\n    This is experimental\n    \"\"\"\n    # punctuation count\n    hyphens = passage.count(\"-\")\n    periods = passage.count(\".\")\n    commas = passage.count(\",\")\n    semis = passage.count(\";\")\n    exclaims = passage.count(\"!\")\n    questions = passage.count(\"?\")\n    \n    # Some other stats\n    num_char = len(passage)\n    num_words = len(passage.split(\" \"))\n    unique_words = len(set(passage.split(\" \") ))\n    word_diversity = unique_words/num_words\n    \n    word_len = [len(w) for w in passage.split(\" \")]\n    longest_word = np.max(word_len)\n    avg_len_word = np.mean(word_len)\n    \n    return [hyphens,periods, commas, semis, exclaims, questions,\n            num_char, num_words, unique_words, word_diversity,\n            longest_word, avg_len_word]","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-29T15:22:38.54671Z","iopub.execute_input":"2021-07-29T15:22:38.547179Z","iopub.status.idle":"2021-07-29T15:22:38.556559Z","shell.execute_reply.started":"2021-07-29T15:22:38.547134Z","shell.execute_reply":"2021-07-29T15:22:38.555523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##Just testing\n\njust_testingdf=pd.DataFrame(train_df[\"excerpt_preprocessed\"].head().apply(lambda p : generate_other_features(p)).tolist(),\n                                columns=[\"hyphens\",\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n                                         \"num_char\", \"num_words\", \"unique_words\", \"word_diversity\",\n                                         \"longest_word\", \"avg_len_word\"])\njust_testingdf","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:22:38.560254Z","iopub.execute_input":"2021-07-29T15:22:38.560635Z","iopub.status.idle":"2021-07-29T15:22:38.586807Z","shell.execute_reply.started":"2021-07-29T15:22:38.560597Z","shell.execute_reply":"2021-07-29T15:22:38.585604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/pyphen011whl/pyphen-0.11.0-py3-none-any.whl\n!pip install ../input/textstatwhl/textstat-0.7.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:22:38.58936Z","iopub.execute_input":"2021-07-29T15:22:38.589765Z","iopub.status.idle":"2021-07-29T15:23:31.597074Z","shell.execute_reply.started":"2021-07-29T15:22:38.589722Z","shell.execute_reply":"2021-07-29T15:23:31.596036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport spacy\nimport timeit\nimport re\nimport textstat\n\n\nnlp = spacy.load('en_core_web_sm')\npunct=\";|!|:|;|,|-|'\"\nstop=set(stopwords.words('english'))\n\ndef preprocess_dataframe(df):\n    #Set a unique Numbering for each exerpt\n    df=df.reset_index()  \n    #Average excerpt length\n    train_df['excerpt_length']=train_df['excerpt'].str.len()\n    avg_excerpt_len=round(train_df['excerpt_length'].mean(),0) #Avg. excerpt length\n    #Convert all text to lowecase\n    df['excerpt_preprocess']=df['excerpt_preprocessed'].str.lower()         \n    #FEATURE ENGINEERING: Get the legth of each excerpt\n    df['excerpt_actual_length']=df['excerpt_preprocess'].str.len()\n    #Remove common words from excerpt\n    df['excerpt_preprocess']=df['excerpt_preprocess'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n    #FEATURE ENGINEERING: Get the legth of the preprocessed excerpt\n    df['excerpt_preprocessed_length']=df['excerpt_preprocess'].str.len()\n    #FEATURE ENGINEERING: Percent frequent words\n    df['excerpt_stopword_freq']=(df['excerpt_actual_length']-df['excerpt_preprocessed_length'])/df['excerpt_actual_length']\n    #FEATURE ENGINEERING: Get count of punctuations in the excerpt\n    df['excerpt_punct_count']=df['excerpt'].apply(lambda x: len(re.findall(punct, x)))\n    #Convert excerpt into setences\n    df['excerpt_sentence'] = df['excerpt_preprocess'].apply(lambda x: list(nlp(x).sents))\n    #Convert each setence of the exerpt into a pandas row\n    df=df.explode('excerpt_sentence')\n    #Convert spacy object to string object\n    df['excerpt_sentence']=df['excerpt_sentence'].apply(lambda x: x.text)    \n    ##FEATURE ENGINEERING: Get sentence length\n    df['sentence_length']=df['excerpt_sentence'].str.len()\n    ##FEATURE ENGINEERING: Get word count\n    df['totalwords'] = df['excerpt_sentence'].str.split().map(len)\n    ##FEATURE ENGINEERING: Get normalized word count\n    df['normalized_word_count'] = round(df['sentence_length']/df['totalwords'],2)\n    ##FEATURE ENGINEERING: Get normalized stopword frequency\n    df['normalized_stopword_freq']=round(df['excerpt_stopword_freq']*avg_excerpt_len,1)\n    ##FEATURE ENGINEERING: Get average senetence length\n    df['avg sent length']=df[['sentence_length', 'index']].groupby(['index']).agg(['median'])\n    ##FEATURE ENGINEERING: Get average senetence length\n    df=df[['index','excerpt','avg sent length','normalized_word_count','normalized_stopword_freq']].drop_duplicates(subset ='index').set_index('index')\n    \n    #Features from textstat\n    df['flesch_reading_ease']=df['excerpt'].apply(lambda x: textstat.flesch_reading_ease(x))\n    df['smog_index']=df['excerpt'].apply(lambda x: textstat.smog_index(x))\n    df['flesch_kincaid_grade']=df['excerpt'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n    df['coleman_liau_index']=df['excerpt'].apply(lambda x: textstat.coleman_liau_index(x))\n    df['automated_readability_index']=df['excerpt'].apply(lambda x: textstat.automated_readability_index(x))\n    df['dale_chall_readability_score']=df['excerpt'].apply(lambda x: textstat.dale_chall_readability_score(x))\n    df['difficult_words']=df['excerpt'].apply(lambda x: textstat.difficult_words(x))\n    df['linsear_write_formula']=df['excerpt'].apply(lambda x: textstat.linsear_write_formula(x))\n    df['gunning_fog']=df['excerpt'].apply(lambda x: textstat.gunning_fog(x))\n    df['text_standard']=df['excerpt'].apply(lambda x: textstat.text_standard(x, float_output=True))\n    df['fernandez_huerta']=df['excerpt'].apply(lambda x: textstat.fernandez_huerta(x))\n    df['szigriszt_pazos']=df['excerpt'].apply(lambda x: textstat.szigriszt_pazos(x))\n    df['gutierrez_polini']=df['excerpt'].apply(lambda x: textstat.gutierrez_polini(x))\n    df['crawford']=df['excerpt'].apply(lambda x: textstat.crawford(x))\n    return df.drop(columns=['excerpt'])","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:31.598735Z","iopub.execute_input":"2021-07-29T15:23:31.599074Z","iopub.status.idle":"2021-07-29T15:23:32.973177Z","shell.execute_reply.started":"2021-07-29T15:23:31.599033Z","shell.execute_reply":"2021-07-29T15:23:32.972324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\ntrain_df['excerpt_preprocessed'].head(2)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:32.974414Z","iopub.execute_input":"2021-07-29T15:23:32.97475Z","iopub.status.idle":"2021-07-29T15:23:32.984916Z","shell.execute_reply.started":"2021-07-29T15:23:32.974715Z","shell.execute_reply":"2021-07-29T15:23:32.983984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess_dataframe(train_df.head(3))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:32.986492Z","iopub.execute_input":"2021-07-29T15:23:32.986851Z","iopub.status.idle":"2021-07-29T15:23:33.159794Z","shell.execute_reply.started":"2021-07-29T15:23:32.986812Z","shell.execute_reply":"2021-07-29T15:23:33.15903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.merge(just_testingdf, preprocess_dataframe(train_df.head()), left_index=True, right_index=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:33.161147Z","iopub.execute_input":"2021-07-29T15:23:33.16151Z","iopub.status.idle":"2021-07-29T15:23:33.326809Z","shell.execute_reply.started":"2021-07-29T15:23:33.161453Z","shell.execute_reply":"2021-07-29T15:23:33.325851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_folds(data: pd.DataFrame, num_splits: int):\n    \"\"\" \n    This function creates a kfold cross validation system based on this reference: \n    https://www.kaggle.com/abhishek/step-1-create-folds\n    \"\"\"\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-29T15:23:33.32827Z","iopub.execute_input":"2021-07-29T15:23:33.328622Z","iopub.status.idle":"2021-07-29T15:23:33.336078Z","shell.execute_reply.started":"2021-07-29T15:23:33.328587Z","shell.execute_reply":"2021-07-29T15:23:33.335057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRDataset:\n    \"\"\"\n    This is my CommonLit Readability Dataset.\n    By calling the get_df method on an object of this class,\n    you will have a fully feature engineered dataframe\n    \"\"\"\n    def __init__(self, df: pd.DataFrame, train: bool, n_folds=2):\n        self.df = df\n        self.excerpts = df[\"excerpt_preprocessed\"]\n        \n        self._extract_features()\n        \n        if train:\n            self.df = create_folds(self.df, n_folds)\n        \n    def _extract_features(self):\n        scores_df = pd.DataFrame(self.df[\"excerpt_preprocessed\"].apply(lambda p : readability_measurements(p)).tolist(), \n                                 columns=[\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n                                          \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n                                          \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"])\n        self.df = pd.merge(self.df, scores_df, left_index=True, right_index=True)\n        \n        spacy_df = pd.DataFrame(spacy_features(self.df), columns=get_spacy_col_names())\n        self.df = pd.merge(self.df, spacy_df, left_index=True, right_index=True)\n        \n        pos_df = pd.DataFrame(self.df[\"excerpt_preprocessed\"].apply(lambda p : pos_tag_features(p)).tolist(),\n                              columns=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                                       \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                                       \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"])\n        self.df = pd.merge(self.df, pos_df, left_index=True, right_index=True)\n        \n        other_df = pd.DataFrame(self.df[\"excerpt_preprocessed\"].apply(lambda p : generate_other_features(p)).tolist(),\n                                columns=[\"hyphens\",\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n                                         \"num_char\", \"num_words\", \"unique_words\", \"word_diversity\",\n                                         \"longest_word\", \"avg_len_word\"])\n        \n        self.df = pd.merge(self.df, other_df, left_index=True, right_index=True)\n        \n        self.df = pd.merge(self.df, preprocess_dataframe(self.df), left_index=True, right_index=True)\n        \n    def get_df(self):\n        return self.df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        pass","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-29T15:23:33.337611Z","iopub.execute_input":"2021-07-29T15:23:33.338008Z","iopub.status.idle":"2021-07-29T15:23:33.354514Z","shell.execute_reply.started":"2021-07-29T15:23:33.33797Z","shell.execute_reply":"2021-07-29T15:23:33.353753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CLRDataset(train_df, train=True)\ntrain_df = dataset.get_df()\n\ntrain_df.head(3)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-29T15:23:33.355913Z","iopub.execute_input":"2021-07-29T15:23:33.356341Z","iopub.status.idle":"2021-07-29T15:23:43.78752Z","shell.execute_reply.started":"2021-07-29T15:23:33.356298Z","shell.execute_reply":"2021-07-29T15:23:43.78593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = CLRDataset(test_df, train=False)\ntest_df = test_dataset.get_df()\n\ntest_df.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-29T15:23:43.789148Z","iopub.execute_input":"2021-07-29T15:23:43.789524Z","iopub.status.idle":"2021-07-29T15:23:50.305432Z","shell.execute_reply.started":"2021-07-29T15:23:43.789486Z","shell.execute_reply":"2021-07-29T15:23:50.30439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorize By BERT Function","metadata":{"_uuid":"65e2949c-190c-4485-a017-d6873cb4d702","_cell_guid":"0ee90223-7c1e-4353-af95-fd2af3ff709e","trusted":true}},{"cell_type":"markdown","source":"## Fine Tuning","metadata":{}},{"cell_type":"code","source":"MODEL_PATH = '../input/huggingface-roberta/roberta-large'\n\ntokenizer = RobertaTokenizer.from_pretrained(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:50.306771Z","iopub.execute_input":"2021-07-29T15:23:50.307161Z","iopub.status.idle":"2021-07-29T15:23:50.452285Z","shell.execute_reply.started":"2021-07-29T15:23:50.30712Z","shell.execute_reply":"2021-07-29T15:23:50.45133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaForSequenceClassification_pl(pl.LightningModule):\n    def __init__(self, model_name, num_labels, lr):\n        super().__init__()\n        \n        self.save_hyperparameters()\n        \n        self.roberta_sc = RobertaForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=num_labels\n        )\n        \n    def training_step(self, batch, batch_idx):\n        output = self.roberta_sc(**batch)\n        loss = output.loss\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        output = self.roberta_sc(**batch)\n        val_loss = output.loss\n        self.log('val_loss', val_loss)\n        \n    def test_step(self, batch, batch_idx):\n        labels = batch.pop('labels')\n        output = self.roberta_sc(**batch)\n        labels_predicted = output.logits.argmax(-1)\n        num_correct = (labels_predicted == labels).sum().item()\n        accuracy = num_correct / labels.size(0)\n        self.log('accuracy', accuracy)\n        \n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:50.453656Z","iopub.execute_input":"2021-07-29T15:23:50.454053Z","iopub.status.idle":"2021-07-29T15:23:50.464189Z","shell.execute_reply.started":"2021-07-29T15:23:50.454011Z","shell.execute_reply":"2021-07-29T15:23:50.463225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = pl.callbacks.ModelCheckpoint(\n    monitor='val_loss',\n    mode='min',\n    save_top_k=1,\n    save_weights_only=True,\n    dirpath='model/'\n)\n\ntrainer = pl.Trainer(\n    gpus=1,\n    max_epochs=25,\n    callbacks=[checkpoint]\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:50.465679Z","iopub.execute_input":"2021-07-29T15:23:50.466066Z","iopub.status.idle":"2021-07-29T15:23:50.543113Z","shell.execute_reply.started":"2021-07-29T15:23:50.466025Z","shell.execute_reply":"2021-07-29T15:23:50.542152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createRobertaFineDataSet(excerpts, targets):\n    data = []    \n    for excerpt, target in zip(excerpts, targets):\n        encoding = tokenizer(\n            excerpt,\n            max_length = 240,\n            padding='max_length',\n            truncation=True\n        )\n\n        encoding['labels'] = target\n        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n\n        data.append(encoding)\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:50.544617Z","iopub.execute_input":"2021-07-29T15:23:50.545277Z","iopub.status.idle":"2021-07-29T15:23:50.552275Z","shell.execute_reply.started":"2021-07-29T15:23:50.545233Z","shell.execute_reply":"2021-07-29T15:23:50.551332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# update kfold values for fine tune\nkfolds = []\n\nfor i in progress_bar(train_df.index):\n    kfolds.append(i % 5)\n    \ntrain_df['kfold'] = kfolds","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:50.55347Z","iopub.execute_input":"2021-07-29T15:23:50.553752Z","iopub.status.idle":"2021-07-29T15:23:50.57741Z","shell.execute_reply.started":"2021-07-29T15:23:50.553724Z","shell.execute_reply":"2021-07-29T15:23:50.576532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:50.578767Z","iopub.execute_input":"2021-07-29T15:23:50.579297Z","iopub.status.idle":"2021-07-29T15:23:50.58293Z","shell.execute_reply.started":"2021-07-29T15:23:50.579258Z","shell.execute_reply":"2021-07-29T15:23:50.582158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RobertaForSequenceClassification_pl(\n    MODEL_PATH,\n    num_labels=1,\n    lr=1e-5\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:23:50.584231Z","iopub.execute_input":"2021-07-29T15:23:50.584802Z","iopub.status.idle":"2021-07-29T15:24:15.002799Z","shell.execute_reply.started":"2021-07-29T15:23:50.584764Z","shell.execute_reply":"2021-07-29T15:24:15.001989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in progress_bar(train_df['kfold'].unique()):\n    train_df_for_fine_tune = train_df[train_df['kfold'] != i]\n    test_df_for_fine_tune = train_df[train_df['kfold'] == i]\n    \n    dataset_train = createRobertaFineDataSet(\n        train_df_for_fine_tune['excerpt'],\n        train_df_for_fine_tune['target']\n    )\n    \n    dataset_val = createRobertaFineDataSet(\n        test_df_for_fine_tune['excerpt'],\n        test_df_for_fine_tune['target']\n    )\n\n    train_dataloader = DataLoader(\n        dataset_train,\n        batch_size=8,\n        shuffle=True\n    )\n    val_dataloader = DataLoader(\n        dataset_val, \n        batch_size=128\n    )\n\n    trainer.fit(model, train_dataloader, val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:24:15.004219Z","iopub.execute_input":"2021-07-29T15:24:15.004796Z","iopub.status.idle":"2021-07-29T15:25:07.817215Z","shell.execute_reply.started":"2021-07-29T15:24:15.004753Z","shell.execute_reply":"2021-07-29T15:25:07.81639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_path = checkpoint.best_model_path\n\nmodel = RobertaForSequenceClassification_pl.load_from_checkpoint(\n    best_model_path\n)\n\nFINE_TUNED_MODEL_PATH = '/kaggle/working/model_transformers'\n\nmodel.roberta_sc.save_pretrained(FINE_TUNED_MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:25:07.82384Z","iopub.execute_input":"2021-07-29T15:25:07.825808Z","iopub.status.idle":"2021-07-29T15:25:35.80796Z","shell.execute_reply.started":"2021-07-29T15:25:07.825766Z","shell.execute_reply":"2021-07-29T15:25:35.805936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Roberta interface","metadata":{}},{"cell_type":"code","source":"class RobertaDataset(nn.Module):\n    def __init__(self, df, tokenizer, max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(\n            self.excerpt[idx],\n            return_tensors='pt',\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True\n        )\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \n\ndef get_embeddings(df, path, plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    MODEL_PATH = path\n    model = RobertaModel.from_pretrained(MODEL_PATH, num_labels=1)\n    model.to(device)\n    model.eval()\n    \n    ds = RobertaDataset(df, tokenizer, config['max_len'])\n    dl = DataLoader(\n        ds,\n        batch_size=config[\"batch_size\"],\n        shuffle=False,\n        num_workers = 4,\n        pin_memory=True,\n        drop_last=False\n    )\n\n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in progress_bar(list(enumerate(dl))):\n            inputs = {key:val.reshape(val.shape[0], -1).to(device) for key, val in inputs.items()}\n            outputs = model(**inputs)\n            #outputs = outputs[0][:, -1].detach().cpu().numpy()\n            outputs = np.sum(outputs[0][:, -4:].detach().cpu().numpy(),axis=1)\n            embeddings.extend(outputs)\n            \n    return np.array(embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:25:35.813333Z","iopub.execute_input":"2021-07-29T15:25:35.813612Z","iopub.status.idle":"2021-07-29T15:25:35.825873Z","shell.execute_reply.started":"2021-07-29T15:25:35.813584Z","shell.execute_reply":"2021-07-29T15:25:35.82502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:25:35.827294Z","iopub.execute_input":"2021-07-29T15:25:35.827784Z","iopub.status.idle":"2021-07-29T15:25:36.446761Z","shell.execute_reply.started":"2021-07-29T15:25:35.827744Z","shell.execute_reply":"2021-07-29T15:25:36.445809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'batch_size': 8,\n    'max_len': 240,\n    'seed': 42,\n}\nseed_everything(seed=config['seed'])\n\ntrain_embeddings =  get_embeddings(train_df, FINE_TUNED_MODEL_PATH)\ntest_embeddings = get_embeddings(test_df, FINE_TUNED_MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:25:36.448355Z","iopub.execute_input":"2021-07-29T15:25:36.448736Z","iopub.status.idle":"2021-07-29T15:25:56.06758Z","shell.execute_reply.started":"2021-07-29T15:25:36.448695Z","shell.execute_reply":"2021-07-29T15:25:56.066679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:25:56.069626Z","iopub.execute_input":"2021-07-29T15:25:56.070032Z","iopub.status.idle":"2021-07-29T15:25:56.077264Z","shell.execute_reply.started":"2021-07-29T15:25:56.069979Z","shell.execute_reply":"2021-07-29T15:25:56.076175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare train and test data","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\n\nbest_feat_idx=train_df.filter(regex='^(?!.*spacy_).*$').corr().query('target < -0.2 | 0.2 < target')['target']\nbest_feat_idx","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:25:56.078774Z","iopub.execute_input":"2021-07-29T15:25:56.079365Z","iopub.status.idle":"2021-07-29T15:25:56.155795Z","shell.execute_reply.started":"2021-07-29T15:25:56.079322Z","shell.execute_reply":"2021-07-29T15:25:56.154966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns=set(best_feat_idx.index.values)\n\nintersection = columns. intersection(test_df.columns) #Find common elements of set and list.\ncolumns = list(intersection)\ncolumns","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:25:56.157336Z","iopub.execute_input":"2021-07-29T15:25:56.157707Z","iopub.status.idle":"2021-07-29T15:25:56.165504Z","shell.execute_reply.started":"2021-07-29T15:25:56.157668Z","shell.execute_reply":"2021-07-29T15:25:56.164505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.DataFrame(train_embeddings)\nX_train = pd.concat([X_train, train_df[columns]], axis=1)\n\nX_test = pd.DataFrame(test_embeddings)\nX_test = pd.concat([X_test, test_df[columns]], axis=1)","metadata":{"_uuid":"80b92578-7842-4fa3-b5f6-72a3f7cf5800","_cell_guid":"b4e32d23-e46a-4025-9494-9f28f24aefcc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-29T15:25:56.166977Z","iopub.execute_input":"2021-07-29T15:25:56.167355Z","iopub.status.idle":"2021-07-29T15:25:56.191136Z","shell.execute_reply.started":"2021-07-29T15:25:56.167316Z","shell.execute_reply":"2021-07-29T15:25:56.190151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train_df[['target']]","metadata":{"_uuid":"3b997aa7-4278-4135-b7b4-2740d88c85e0","_cell_guid":"cb4805aa-4d11-48b1-bc61-8ce69ff831c6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-29T15:25:56.192698Z","iopub.execute_input":"2021-07-29T15:25:56.193068Z","iopub.status.idle":"2021-07-29T15:25:56.198142Z","shell.execute_reply.started":"2021-07-29T15:25:56.19303Z","shell.execute_reply":"2021-07-29T15:25:56.196984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=71)\n\ncv = list(kf.split(X_train, y_train))","metadata":{"_uuid":"ccacfdb6-0a4c-4a00-9adb-c250ee2700d8","_cell_guid":"55b2b7c9-e6c5-4ce9-a44b-c08bdf0e3d7b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-29T15:25:56.199696Z","iopub.execute_input":"2021-07-29T15:25:56.200241Z","iopub.status.idle":"2021-07-29T15:25:56.21527Z","shell.execute_reply.started":"2021-07-29T15:25:56.200139Z","shell.execute_reply":"2021-07-29T15:25:56.214201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Light GBM","metadata":{"_uuid":"6b9d9e08-7e85-46a6-a05a-8cefae7a2176","_cell_guid":"141efa19-cc0e-44b8-8ddc-dc9e9d438db6","trusted":true}},{"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'seed': 42,\n    'learning_rate': 0.04,\n    'max_depth': 6,\n    'n_jobs': -1,\n    'verbose': -1,\n    'num_leaves': 10,\n    'max_bin': 63,\n    'feature_fraction': 0.25,\n    'extra_trees': True,\n    'path_smooth':0.1\n}\npred = np.zeros(X_test.shape[0])\nrmses = []\n\nfor tr_idx, val_idx in progress_bar(cv):\n    x_tr, x_va = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n\n    train_set = lgb.Dataset(x_tr, y_tr)\n    val_set = lgb.Dataset(x_va, y_va, reference=train_set)\n\n    model = lgb.train(\n        params,\n        train_set, \n        num_boost_round=10000,\n        early_stopping_rounds=50,\n        valid_sets=[train_set, val_set], \n        verbose_eval=-1\n    )\n\n    y_pred = model.predict(x_va)\n    rmse = np.sqrt(mse(y_va, y_pred))\n    rmses.append(rmse)\n    \n    tmp_pred = model.predict(X_test)\n    pred += tmp_pred / 5\n    \nprint(\"\\n\", \"Mean Fold RMSE:\", np.mean(rmses))","metadata":{"_uuid":"db191234-a015-4f4e-a23d-55fd2a7e4170","_cell_guid":"dab4ecc7-457b-4122-bef3-f524e3838258","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-29T15:25:56.216536Z","iopub.execute_input":"2021-07-29T15:25:56.21707Z","iopub.status.idle":"2021-07-29T15:25:56.532577Z","shell.execute_reply.started":"2021-07-29T15:25:56.21703Z","shell.execute_reply":"2021-07-29T15:25:56.5316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check xgboost version\n#import xgboost\n#print(xgboost.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:25:56.53388Z","iopub.execute_input":"2021-07-29T15:25:56.53425Z","iopub.status.idle":"2021-07-29T15:25:56.538329Z","shell.execute_reply.started":"2021-07-29T15:25:56.534211Z","shell.execute_reply":"2021-07-29T15:25:56.537209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create an xgboost regression model\n#model = xgboost.XGBRegressor(n_estimators=1000, max_depth=8, eta=0.1, subsample=0.7, colsample_bytree=0.8)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:25:56.539817Z","iopub.execute_input":"2021-07-29T15:25:56.540377Z","iopub.status.idle":"2021-07-29T15:25:56.551743Z","shell.execute_reply.started":"2021-07-29T15:25:56.540334Z","shell.execute_reply":"2021-07-29T15:25:56.550779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define model evaluation method\n#cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\n#scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:25:56.553358Z","iopub.execute_input":"2021-07-29T15:25:56.553816Z","iopub.status.idle":"2021-07-29T15:25:56.561317Z","shell.execute_reply.started":"2021-07-29T15:25:56.553767Z","shell.execute_reply":"2021-07-29T15:25:56.560396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# force scores to be positive\n#scores = absolute(scores)\n#print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:25:56.562756Z","iopub.execute_input":"2021-07-29T15:25:56.563235Z","iopub.status.idle":"2021-07-29T15:25:56.570441Z","shell.execute_reply.started":"2021-07-29T15:25:56.563192Z","shell.execute_reply":"2021-07-29T15:25:56.569507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = pred\npredictions.to_csv(\"submission.csv\", index=False)\n\npredictions","metadata":{"_uuid":"f002712c-1d9f-4828-9292-811e60004207","_cell_guid":"04fac150-c0fd-4bbc-8c5e-dd97dfe7b7e5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-29T15:25:56.571992Z","iopub.execute_input":"2021-07-29T15:25:56.572577Z","iopub.status.idle":"2021-07-29T15:25:56.65754Z","shell.execute_reply.started":"2021-07-29T15:25:56.572532Z","shell.execute_reply":"2021-07-29T15:25:56.656533Z"},"trusted":true},"execution_count":null,"outputs":[]}]}