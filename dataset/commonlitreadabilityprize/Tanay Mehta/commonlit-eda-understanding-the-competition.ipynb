{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"color:blue\"><center>CommonLit Readability Prize Competition</center></h1>\n<h3><center>What's it's all about?</center></h3>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"**If you found this notebook helpful, you can leave a vote!**\n\nüìå**PyTorch BERT Multi-Model Trainer + KFoldsüéØ (Training Notebook) - https://www.kaggle.com/heyytanay/pytorch-bert-multi-model-trainer-kfolds**\n\nüìå**Vanilla PyTorch BERT Starter (Submission Notebook)üéØ - https://www.kaggle.com/heyytanay/submission-nb-vanilla-pytorch-bert-starter**\n\n## What is this Competition about? ü§∑‚Äç‚ôÇÔ∏è\n\n* In this competition, we are required to build algorithms to rate the complexity of reading passages for grade 3-12 classroom use.\n\n* To accomplish this, we'll have to pair our machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.\n\n* If successfull, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.\n\n<hr>","metadata":{}},{"cell_type":"markdown","source":"## And what is our task? üéØ\n\nIn technical terms,\n\n* Given a `training.csv` file in which we will have (among other) 2 columns: `excerpt` and `target`, we will have to train Machine Learning model(s) that can approximate the relationship between excerpt and the target.\n\n* In simple words, we will have to train a Model which can predict the target value given a text excerpt.\n\n* This can be formulated as a Regression problem with text\n\n<hr>","metadata":{}},{"cell_type":"markdown","source":"## Files and what they contain üìÇ\n\nIn this competition, we are provided with 3 files:\n\n* üìÑ `train.csv` :  This is the main training file, it consists of 6 columns: `id`, `excerpt`, `license`, `url_legal`, `target`, `standard_error`.\n    \n* üìÑ `test.csv` :  This is the testing file, it consists of 4 columns: `id`, `url_legal`, `license`, `excerpt`\n    \n* üìÑ `sample_submission.csv` :  This is a sample submission file that guides us how to form our submission file during inference\n\n\n<hr>","metadata":{}},{"cell_type":"markdown","source":"<h2>Evaluation Metrics üñä</h2>\n\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\n$$RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big(\\frac{y_i - \\hat{y_i}}{\\sigma_i}\\Big)^2}}$$\n\nwhere $y_i$ is the predicted value, $\\hat{y_i}$ is the original value, and $n$ is the number of rows in the test data.","metadata":{}},{"cell_type":"markdown","source":"<hr>\n<h2> EDA time! üìä </h2>\n\nEnough talking, let's do some light EDA!","metadata":{}},{"cell_type":"code","source":"# Some imports :)\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nfrom tqdm.notebook import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom wordcloud import WordCloud\nfrom plotly.offline import iplot\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nplt.style.use('classic')\nsns.set_palette(sns.color_palette('winter_r'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cprint(string:str, end=\"\\n\"):\n    \"\"\"\n    A little utility function for printing and stuff\n    \"\"\"\n    _pprint(f\"[black]{string}[/black]\", end=end)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing data\ntraining_file = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_file = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nsubmission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_file.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_file.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Peeking at the `target` column üëÄ\n\nLet's take a look at the target column to see how it is distributed.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nplt.title(f\"Target Column Distribution\")\nsns.histplot(training_file['target'], stat='density')\nsns.kdeplot(training_file['target'], color='blue')\nplt.axvline(training_file['target'].mean(), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(training_file['target'].mean()*1.05, max_ylim*0.96, 'Mean (Œº): {:.2f}'.format(training_file['target'].mean()))\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `target` columns looks very much normally distributed.","metadata":{}},{"cell_type":"markdown","source":"## Peeking at the `standard_error` column üëÄ\n\nLet's take a look at the `standard_error` column to see how it looks like.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nplt.title(f\"Standard Error Column Distribution\")\nsns.histplot(training_file['standard_error'], stat='density')\nsns.kdeplot(training_file['standard_error'], color='magenta')\nplt.axvline(training_file['standard_error'].mean(), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(training_file['standard_error'].mean()*1.05, max_ylim*0.96, 'Mean (Œº): {:.2f}'.format(training_file['standard_error'].mean()))\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysis on Excerpt Columns (`excerpt`) üéØ\n\nLet's now do some text related analysis on `excerpt` column and see how the text in it is structured.","metadata":{}},{"cell_type":"markdown","source":"### Character Frequency Count","metadata":{}},{"cell_type":"code","source":"char_freq_count = training_file['excerpt'].str.len()\n\nplt.figure(figsize=(8, 8))\nplt.title(f\"Character Frequency Count\")\nsns.histplot(char_freq_count, stat='density', color=\"#00a9ff\")\nsns.kdeplot(char_freq_count, color='purple')\nplt.axvline(np.mean(char_freq_count), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(np.mean(char_freq_count)*0.64, max_ylim*0.96, 'Mean (Œº): {:.2f}'.format(np.mean(char_freq_count)))\nplt.xlabel(\"Count\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Count Distribution\n\n","metadata":{}},{"cell_type":"code","source":"word_count = training_file['excerpt'].str.split().map(lambda x: len(x))\n\nplt.figure(figsize=(8, 8))\nplt.title(f\"Word Count Distribution\")\nsns.histplot(word_count, stat='density', color=\"magenta\")\nsns.kdeplot(word_count, color='purple')\nplt.axvline(np.mean(word_count), color='red', linestyle='--', linewidth=0.8)\nmin_ylim, max_ylim = plt.ylim()\nplt.text(np.mean(word_count)*0.86, max_ylim*0.93, 'Mean (Œº): {:.2f}'.format(np.mean(word_count)))\nplt.xlabel(\"Word Count\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*The above word count distribution can help us identify what `max_len` should we use when training our models*","metadata":{}},{"cell_type":"markdown","source":"### Unique Word Count","metadata":{}},{"cell_type":"code","source":"unq_word_count = training_file['excerpt'].apply(lambda x: len(set(str(x).split()))).to_list()\n\nfig = ff.create_distplot([unq_word_count], ['Excerpt Text'])\nfig.update_layout(title_text=\"Unique Word Count Distribution\")\niplot(fig)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WordCloud","metadata":{}},{"cell_type":"code","source":"# Generate WordCloud\nwords = \" \".join(training_file['excerpt'].tolist())\nwc = WordCloud(width = 5000, height = 4000, background_color ='black', min_font_size = 10).generate(words)\n\n# Plot it\nplt.figure(figsize = (12, 12), facecolor = 'k', edgecolor = 'k' ) \nplt.imshow(wc) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling\n\nLet's try some basic modelling and then make a submission using that!","metadata":{}},{"cell_type":"code","source":"# Split the data roughly\ndata = training_file[['excerpt', 'target']]\ndata = data.sample(frac=1).reset_index(drop=True)\nexcerpt, targets = training_file['excerpt'].values, training_file['target'].values\n\nt_X, v_X = excerpt[:2750], excerpt[2750:]\nt_Y, v_Y = targets[:2750], targets[2750:]\n\nprint(t_X.shape, v_X.shape)\nprint(t_Y.shape, v_Y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make an Sklearn pipeline for this Ridge Regression\nbackbone_ridge = Ridge(fit_intercept=True, normalize=False)\npipeline_ridge = make_pipeline(\n    TfidfVectorizer(binary=True, ngram_range=(1, 1)),\n    backbone_ridge\n)\n\n# Do training\npipeline_ridge.fit(t_X, t_Y)\n\n# Evaluate the performance on validation set\npreds = pipeline_ridge.predict(v_X)\nmse_loss = mean_squared_error(v_Y, preds)\n\nprint(f\"MSE Loss using Ridge and TfIdfVectorizer: {mse_loss}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make an Sklearn pipeline for this Linear Regression\nbackbone_linear = LinearRegression(fit_intercept=True, normalize=False)\npipeline_linear = make_pipeline(\n    TfidfVectorizer(binary=True, ngram_range=(1, 1)),\n    backbone_linear\n)\n\n# Do training\npipeline_linear.fit(t_X, t_Y)\n\n# Evaluate the performance on validation set\npreds = pipeline_linear.predict(v_X)\nmse_loss = mean_squared_error(v_Y, preds)\n\nprint(f\"MSE Loss using Linear Regression and TfIdfVectorizer: {mse_loss}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Weights for blending later\nlin_wgt = 0.2\nrig_wgt = 0.8","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the testing file\ntest = test_file[['id', 'excerpt']]\ntest_ids = test['id'].tolist()\ntest_text = test['excerpt'].values\n\n# Do Predictions on testing set\ntest_preds_ridge = pipeline_ridge.predict(test_text)\ntest_preds_linear = pipeline_linear.predict(test_text)\n\n# Form a submissions file and save it\nsubmission = pd.DataFrame()\nsubmission['id'] = test_ids\nsubmission['target'] = (test_preds_ridge + test_preds_linear) / 2\nsubmission.to_csv(\"submission.csv\", index=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}