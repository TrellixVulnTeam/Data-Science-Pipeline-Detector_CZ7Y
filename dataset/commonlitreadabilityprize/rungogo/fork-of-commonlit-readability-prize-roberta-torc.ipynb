{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## RoBERTa Base Fine-Tuning with Better Training Strategies\n\n### Steps:\n1. [RoBERTa Pretraining](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-itpt) on Task Data\n2. [RoBERTa Finetuing]() - This Notebook.\n3. [RoBERTa Inference](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer) with weights available.\n\n\n### Updates\n- For further improvement, consider experimenting with various performance optimization strategies explained in this notebook [Speeding up Transformer w/ Optimization Strategies\n](https://www.kaggle.com/rhtsingh/speeding-up-transformer-w-optimization-strategies). The notebook explains 5 optimization strategies in-depth with code. All these techniques are promising and can improve the model performance both in terms of speed and accuracy.\n   - Dynamic Padding and Uniform Length Batching\n   - Gradient Accumulation\n   - Freeze Embedding\n   - Numeric Precision Reduction\n   - Gradient Checkpointing\n   \n- After many experiments, I think complex scheduling strategies like `linear with warmup` or `cosine with warmup` etc. might not work with pretraining as well as finetuning as the dataset is small. I don't exactly remember where but I read simple scheduling generally works better than complex ones with small corpus. I am not sure about this one but it's worth experimenting with. I have two suggestions to try,\n  - Scheduling after epoch.\n  - No scheduling at all.\n\n- Another thing worth trying is using RAdam with Lookahead instead of AdamW. RAdam does not require the warm-up tuning that depends on data size. I'll add the code here soon.  ","metadata":{"papermill":{"duration":0.021497,"end_time":"2021-06-05T15:27:24.876412","exception":false,"start_time":"2021-06-05T15:27:24.854915","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Data","metadata":{"id":"vietnamese-nursery","papermill":{"duration":0.019989,"end_time":"2021-06-05T15:27:24.916679","exception":false,"start_time":"2021-06-05T15:27:24.89669","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"id":"PyldEJ_jq4yx","papermill":{"duration":0.120784,"end_time":"2021-06-05T15:27:25.058046","exception":false,"start_time":"2021-06-05T15:27:24.937262","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.335894Z","iopub.execute_input":"2021-06-10T02:16:34.336234Z","iopub.status.idle":"2021-06-10T02:16:34.373769Z","shell.execute_reply.started":"2021-06-10T02:16:34.336202Z","shell.execute_reply":"2021-06-10T02:16:34.372997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection\ndef create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n    for f, (t_, v_) in enumerate(kf.split(X=data)):\n        data.loc[v_, 'kfold'] = f\n    return data\ntrain = create_folds(train, num_splits=5)","metadata":{"id":"nearby-expert","papermill":{"duration":0.813227,"end_time":"2021-06-05T15:27:25.891928","exception":false,"start_time":"2021-06-05T15:27:25.078701","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.375184Z","iopub.execute_input":"2021-06-10T02:16:34.375554Z","iopub.status.idle":"2021-06-10T02:16:34.388698Z","shell.execute_reply.started":"2021-06-10T02:16:34.375516Z","shell.execute_reply":"2021-06-10T02:16:34.387796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Dependencies - Modelling","metadata":{"id":"legal-float","papermill":{"duration":0.020198,"end_time":"2021-06-05T15:27:25.935025","exception":false,"start_time":"2021-06-05T15:27:25.914827","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%matplotlib inline\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport json\nfrom collections import defaultdict\nimport gc\ngc.enable()","metadata":{"id":"complete-breakdown","papermill":{"duration":0.028974,"end_time":"2021-06-05T15:27:25.984889","exception":false,"start_time":"2021-06-05T15:27:25.955915","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.391051Z","iopub.execute_input":"2021-06-10T02:16:34.391477Z","iopub.status.idle":"2021-06-10T02:16:34.39842Z","shell.execute_reply.started":"2021-06-10T02:16:34.391439Z","shell.execute_reply":"2021-06-10T02:16:34.39716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import AutoConfig\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup,\n    get_linear_schedule_with_warmup\n)\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\nfrom IPython.display import clear_output\nfrom tqdm import tqdm, trange","metadata":{"id":"willing-affairs","papermill":{"duration":6.32148,"end_time":"2021-06-05T15:27:32.326323","exception":false,"start_time":"2021-06-05T15:27:26.004843","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.400107Z","iopub.execute_input":"2021-06-10T02:16:34.400454Z","iopub.status.idle":"2021-06-10T02:16:34.408371Z","shell.execute_reply.started":"2021-06-10T02:16:34.40042Z","shell.execute_reply":"2021-06-10T02:16:34.40751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Convert Examples to Features","metadata":{"id":"palestinian-immunology","papermill":{"duration":0.020474,"end_time":"2021-06-05T15:27:32.367643","exception":false,"start_time":"2021-06-05T15:27:32.347169","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n    data = data.replace('\\n', '')\n    tok = tokenizer.encode_plus(\n        data, \n        max_length=max_len, \n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True\n    )\n    curr_sent = {}\n    padding_length = max_len - len(tok['input_ids'])\n    curr_sent['input_ids'] = tok['input_ids'] + ([tokenizer.pad_token_id] * padding_length)\n    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n        ([0] * padding_length)\n    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n        ([0] * padding_length)\n    return curr_sent","metadata":{"id":"irish-fossil","papermill":{"duration":0.028964,"end_time":"2021-06-05T15:27:32.417302","exception":false,"start_time":"2021-06-05T15:27:32.388338","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.411139Z","iopub.execute_input":"2021-06-10T02:16:34.411503Z","iopub.status.idle":"2021-06-10T02:16:34.4259Z","shell.execute_reply.started":"2021-06-10T02:16:34.411478Z","shell.execute_reply":"2021-06-10T02:16:34.424138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Retriever","metadata":{"id":"subject-entertainment","papermill":{"duration":0.020189,"end_time":"2021-06-05T15:27:32.457763","exception":false,"start_time":"2021-06-05T15:27:32.437574","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        if 'excerpt' in self.data.columns:\n            self.excerpts = self.data.excerpt.values.tolist()\n        else:\n            self.excerpts = self.data.text.values.tolist()\n        self.targets = self.data.target.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        excerpt, label = self.excerpts[item], self.targets[item]\n        features = convert_examples_to_features(\n            excerpt, self.tokenizer, \n            self.max_len, self.is_test\n        )\n        return {\n            'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n            'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n            'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            'label':torch.tensor(label, dtype=torch.double),\n        }","metadata":{"id":"solar-group","papermill":{"duration":0.03126,"end_time":"2021-06-05T15:27:32.509269","exception":false,"start_time":"2021-06-05T15:27:32.478009","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.428155Z","iopub.execute_input":"2021-06-10T02:16:34.428815Z","iopub.status.idle":"2021-06-10T02:16:34.44392Z","shell.execute_reply.started":"2021-06-10T02:16:34.428782Z","shell.execute_reply":"2021-06-10T02:16:34.442785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"id":"proprietary-genre","papermill":{"duration":0.019906,"end_time":"2021-06-05T15:27:32.549244","exception":false,"start_time":"2021-06-05T15:27:32.529338","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = AutoModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        #self.regressor = nn.Linear(config.hidden_size*2, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # max-avg head\n        # average_pool = torch.mean(sequence_output, 1)\n        # max_pool, _ = torch.max(sequence_output, 1)\n        # concat_sequence_output = torch.cat((average_pool, max_pool), 1)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits /= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            # regression task\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output","metadata":{"id":"resident-kazakhstan","papermill":{"duration":0.03566,"end_time":"2021-06-05T15:27:32.604982","exception":false,"start_time":"2021-06-05T15:27:32.569322","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.454492Z","iopub.execute_input":"2021-06-10T02:16:34.454766Z","iopub.status.idle":"2021-06-10T02:16:34.479175Z","shell.execute_reply.started":"2021-06-10T02:16:34.454743Z","shell.execute_reply":"2021-06-10T02:16:34.477986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lamb Optimizer","metadata":{"id":"going-tractor","papermill":{"duration":0.019916,"end_time":"2021-06-05T15:27:32.644912","exception":false,"start_time":"2021-06-05T15:27:32.624996","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Lamb(Optimizer):\n    # Reference code: https://github.com/cybertronai/pytorch-lamb\n\n    def __init__(\n        self,\n        params,\n        lr: float = 1e-3,\n        betas = (0.9, 0.999),\n        eps: float = 1e-6,\n        weight_decay: float = 0,\n        clamp_value: float = 10,\n        adam: bool = False,\n        debias: bool = False,\n    ):\n        if lr <= 0.0:\n            raise ValueError('Invalid learning rate: {}'.format(lr))\n        if eps < 0.0:\n            raise ValueError('Invalid epsilon value: {}'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                'Invalid beta parameter at index 0: {}'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                'Invalid beta parameter at index 1: {}'.format(betas[1])\n            )\n        if weight_decay < 0:\n            raise ValueError(\n                'Invalid weight_decay value: {}'.format(weight_decay)\n            )\n        if clamp_value < 0.0:\n            raise ValueError('Invalid clamp value: {}'.format(clamp_value))\n\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.clamp_value = clamp_value\n        self.adam = adam\n        self.debias = debias\n\n        super(Lamb, self).__init__(params, defaults)\n\n    def step(self, closure = None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    msg = (\n                        'Lamb does not support sparse gradients, '\n                        'please consider SparseAdam instead'\n                    )\n                    raise RuntimeError(msg)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(\n                        p, memory_format=torch.preserve_format\n                    )\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(\n                        p, memory_format=torch.preserve_format\n                    )\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                # Paper v3 does not use debiasing.\n                if self.debias:\n                    bias_correction = math.sqrt(1 - beta2 ** state['step'])\n                    bias_correction /= 1 - beta1 ** state['step']\n                else:\n                    bias_correction = 1\n\n                # Apply bias to lr to avoid broadcast.\n                step_size = group['lr'] * bias_correction\n\n                weight_norm = torch.norm(p.data).clamp(0, self.clamp_value)\n\n                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])\n                if group['weight_decay'] != 0:\n                    adam_step.add_(p.data, alpha=group['weight_decay'])\n\n                adam_norm = torch.norm(adam_step)\n                if weight_norm == 0 or adam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm / adam_norm\n                state['weight_norm'] = weight_norm\n                state['adam_norm'] = adam_norm\n                state['trust_ratio'] = trust_ratio\n                if self.adam:\n                    trust_ratio = 1\n\n                p.data.add_(adam_step, alpha=-step_size * trust_ratio)\n\n        return loss","metadata":{"id":"photographic-crack","papermill":{"duration":0.039535,"end_time":"2021-06-05T15:27:32.704571","exception":false,"start_time":"2021-06-05T15:27:32.665036","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.480795Z","iopub.execute_input":"2021-06-10T02:16:34.481284Z","iopub.status.idle":"2021-06-10T02:16:34.516863Z","shell.execute_reply.started":"2021-06-10T02:16:34.481249Z","shell.execute_reply":"2021-06-10T02:16:34.514475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Differential Learning Rate and Weight Decay","metadata":{"id":"exceptional-asbestos","papermill":{"duration":0.019961,"end_time":"2021-06-05T15:27:32.744811","exception":false,"start_time":"2021-06-05T15:27:32.72485","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def get_optimizer_params(model):\n    # differential learning rate and weight decay\n    param_optimizer = list(model.named_parameters())\n    learning_rate = 5e-5\n    no_decay = ['bias', 'gamma', 'beta']\n    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n    optimizer_parameters = [\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.01},\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.01, 'lr': learning_rate/2.6},\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.01, 'lr': learning_rate},\n        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.01, 'lr': learning_rate*2.6},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': learning_rate/2.6},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': learning_rate},\n        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': learning_rate*2.6},\n        {'params': [p for n, p in model.named_parameters() if \"roberta\" not in n], 'lr':1e-3, \"momentum\" : 0.99},\n    ]\n    return optimizer_parameters","metadata":{"id":"worth-distance","papermill":{"duration":0.036164,"end_time":"2021-06-05T15:27:32.800982","exception":false,"start_time":"2021-06-05T15:27:32.764818","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.521845Z","iopub.execute_input":"2021-06-10T02:16:34.522131Z","iopub.status.idle":"2021-06-10T02:16:34.548829Z","shell.execute_reply.started":"2021-06-10T02:16:34.522095Z","shell.execute_reply":"2021-06-10T02:16:34.548077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utilities","metadata":{"id":"respiratory-improvement","papermill":{"duration":0.019992,"end_time":"2021-06-05T15:27:32.840954","exception":false,"start_time":"2021-06-05T15:27:32.820962","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def make_model(model_name='../input/robertaitpt/', num_labels=1):\n    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n    config = AutoConfig.from_pretrained(model_name)\n    config.update({'num_labels':num_labels})\n    model = CommonLitModel(model_name, config=config)\n    return model, tokenizer\n\ndef make_optimizer(model, optimizer_name=\"AdamW\"):\n    optimizer_grouped_parameters = get_optimizer_params(model)\n    kwargs = {\n            'lr':5e-5,\n            'weight_decay':0.01,\n            # 'betas': (0.9, 0.98),\n            # 'eps': 1e-06\n    }\n    if optimizer_name == \"LAMB\":\n        optimizer = Lamb(optimizer_grouped_parameters, **kwargs)\n        return optimizer\n    elif optimizer_name == \"Adam\":\n        from torch.optim import Adam\n        optimizer = Adam(optimizer_grouped_parameters, **kwargs)\n        return optimizer\n    elif optimizer_name == \"AdamW\":\n        optimizer = AdamW(optimizer_grouped_parameters, **kwargs)\n        return optimizer\n    else:\n        raise Exception('Unknown optimizer: {}'.format(optimizer_name))\n\ndef make_scheduler(optimizer, decay_name='linear', t_max=None, warmup_steps=None):\n    if decay_name == 'step':\n        scheduler = optim.lr_scheduler.MultiStepLR(\n            optimizer,\n            milestones=[30, 60, 90],\n            gamma=0.1\n        )\n    elif decay_name == 'cosine':\n        scheduler = lrs.CosineAnnealingLR(\n            optimizer,\n            T_max=t_max\n        )\n    elif decay_name == \"cosine_warmup\":\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=warmup_steps,\n            num_training_steps=t_max\n        )\n    elif decay_name == \"linear\":\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer, \n            num_warmup_steps=warmup_steps, \n            num_training_steps=t_max\n        )\n    else:\n        raise Exception('Unknown lr scheduler: {}'.format(decay_type))    \n    return scheduler    \n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n    fold=0\n):\n    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n    train_dataset = DatasetRetriever(train_set, tokenizer, max_len)\n    valid_dataset = DatasetRetriever(valid_set, tokenizer, max_len)\n\n    train_sampler = RandomSampler(train_dataset)\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        sampler=train_sampler, \n        pin_memory=True, \n        drop_last=False, \n        num_workers=4\n    )\n\n    valid_sampler = SequentialSampler(valid_dataset)\n    valid_loader = DataLoader(\n        valid_dataset, \n        batch_size=batch_size // 2, \n        sampler=valid_sampler, \n        pin_memory=True, \n        drop_last=False, \n        num_workers=4\n    )\n\n    return train_loader, valid_loader","metadata":{"id":"ahead-steering","papermill":{"duration":0.034623,"end_time":"2021-06-05T15:27:32.895666","exception":false,"start_time":"2021-06-05T15:27:32.861043","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.552551Z","iopub.execute_input":"2021-06-10T02:16:34.554417Z","iopub.status.idle":"2021-06-10T02:16:34.588408Z","shell.execute_reply.started":"2021-06-10T02:16:34.554378Z","shell.execute_reply":"2021-06-10T02:16:34.587529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics","metadata":{"id":"written-resort","papermill":{"duration":0.023357,"end_time":"2021-06-05T15:27:32.940911","exception":false,"start_time":"2021-06-05T15:27:32.917554","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.max = 0\n        self.min = 1e5\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n        if val > self.max:\n            self.max = val\n        if val < self.min:\n            self.min = val","metadata":{"id":"changed-monte","papermill":{"duration":0.028473,"end_time":"2021-06-05T15:27:32.990304","exception":false,"start_time":"2021-06-05T15:27:32.961831","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.592991Z","iopub.execute_input":"2021-06-10T02:16:34.595097Z","iopub.status.idle":"2021-06-10T02:16:34.60398Z","shell.execute_reply.started":"2021-06-10T02:16:34.595043Z","shell.execute_reply":"2021-06-10T02:16:34.603235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trainer","metadata":{"id":"atmospheric-correspondence","papermill":{"duration":0.019971,"end_time":"2021-06-05T15:27:33.030493","exception":false,"start_time":"2021-06-05T15:27:33.010522","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, optimizer, scheduler, scalar=None, log_interval=1, evaluate_interval=1):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.scalar = scalar\n        self.log_interval = log_interval\n        self.evaluate_interval = evaluate_interval\n        self.evaluator = Evaluator(self.model, self.scalar)\n\n    def train(self, train_loader, valid_loader, epoch, \n              result_dict, tokenizer, fold):\n        count = 0\n        losses = AverageMeter()\n        self.model.train()\n        \n        for batch_idx, batch_data in enumerate(train_loader):\n            input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n                batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n            input_ids, attention_mask, token_type_ids, labels = \\\n                input_ids.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n            \n            if self.scalar is not None:\n                with torch.cuda.amp.autocast():\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids,\n                        labels=labels\n                    )\n            else:\n                outputs = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_type_ids=token_type_ids,\n                    labels=labels\n                )\n\n            loss, logits = outputs[:2]\n            count += labels.size(0)\n            losses.update(loss.item(), input_ids.size(0))\n            \n            if self.scalar is not None:\n                self.scalar.scale(loss).backward()\n                self.scalar.step(self.optimizer)\n                self.scalar.update()\n            else:\n                loss.backward()\n                self.optimizer.step()\n\n            self.scheduler.step()\n            self.optimizer.zero_grad()\n\n            if batch_idx % self.log_interval == 0:\n                _s = str(len(str(len(train_loader.sampler))))\n                ret = [\n                    ('epoch: {:0>3} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_loader.sampler), 100 * count / len(train_loader.sampler)),\n                    'train_loss: {: >4.5f}'.format(losses.avg),\n                ]\n                print(', '.join(ret))\n            \n            if batch_idx % self.evaluate_interval == 0:\n                result_dict = self.evaluator.evaluate(\n                    valid_loader, \n                    epoch, \n                    result_dict, \n                    tokenizer\n                )\n                if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n                    print(\"{} epoch, best epoch was updated! valid_loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n                    result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]\n                    torch.save(self.model.state_dict(), f\"model{fold}.bin\")\n\n        result_dict['train_loss'].append(losses.avg)\n        return result_dict","metadata":{"id":"chubby-liberty","papermill":{"duration":0.037688,"end_time":"2021-06-05T15:27:33.088441","exception":false,"start_time":"2021-06-05T15:27:33.050753","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.608801Z","iopub.execute_input":"2021-06-10T02:16:34.61109Z","iopub.status.idle":"2021-06-10T02:16:34.631888Z","shell.execute_reply.started":"2021-06-10T02:16:34.611036Z","shell.execute_reply":"2021-06-10T02:16:34.630975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluator","metadata":{"id":"interested-glass","papermill":{"duration":0.020473,"end_time":"2021-06-05T15:27:33.128896","exception":false,"start_time":"2021-06-05T15:27:33.108423","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n    \n    def worst_result(self):\n        ret = {\n            'loss':float('inf'),\n            'accuracy':0.0\n        }\n        return ret\n\n    def result_to_str(self, result):\n        ret = [\n            'epoch: {epoch:0>3}',\n            'loss: {loss: >4.2e}'\n        ]\n        for metric in self.evaluation_metrics:\n            ret.append('{}: {}'.format(metric.name, metric.fmtstr))\n        return ', '.join(ret).format(**result)\n\n    def save(self, result):\n        with open('result_dict.json', 'w') as f:\n            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n    \n    def load(self):\n        result = self.worst_result\n        if os.path.exists('result_dict.json'):\n            with open('result_dict.json', 'r') as f:\n                try:\n                    result = json.loads(f.read())\n                except:\n                    pass\n        return result\n\n    def evaluate(self, data_loader, epoch, result_dict, tokenizer):\n        losses = AverageMeter()\n\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n                input_ids, attention_mask, token_type_ids, labels = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            labels=labels\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids,\n                        labels=labels\n                    )\n                \n                loss, logits = outputs[:2]\n                losses.update(loss.item(), input_ids.size(0))\n\n        print('----Validation Results Summary----')\n        print('Epoch: [{}] valid_loss: {: >4.5f}'.format(epoch, losses.avg))\n\n        result_dict['val_loss'].append(losses.avg)        \n        return result_dict","metadata":{"id":"demonstrated-delhi","papermill":{"duration":0.036136,"end_time":"2021-06-05T15:27:33.185402","exception":false,"start_time":"2021-06-05T15:27:33.149266","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:16:34.636037Z","iopub.execute_input":"2021-06-10T02:16:34.636591Z","iopub.status.idle":"2021-06-10T02:16:34.656804Z","shell.execute_reply.started":"2021-06-10T02:16:34.636559Z","shell.execute_reply":"2021-06-10T02:16:34.655835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{"id":"solved-ownership","papermill":{"duration":0.020141,"end_time":"2021-06-05T15:27:33.225855","exception":false,"start_time":"2021-06-05T15:27:33.205714","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def config(fold=0):\n    torch.manual_seed(2021)\n    torch.cuda.manual_seed(2021)\n    torch.cuda.manual_seed_all(2021)\n    epochs = 8\n    max_len = 250\n    batch_size = 16\n\n    model, tokenizer = make_model(model_name=f'{MODEL_PATH}/output/', num_labels=1)\n    \n    train_loader, valid_loader = make_loader(\n        train, tokenizer, max_len=max_len,\n        batch_size=batch_size, fold=fold\n    )\n\n    import math\n    num_update_steps_per_epoch = len(train_loader)\n    max_train_steps = epochs * num_update_steps_per_epoch\n    warmup_proportion = 0\n    if warmup_proportion != 0:\n        warmup_steps = math.ceil((max_train_steps * 2) / 100)\n    else:\n        warmup_steps = 0\n\n    optimizer = make_optimizer(model, \"AdamW\")\n    scheduler = make_scheduler(\n        optimizer, decay_name='cosine_warmup', \n        t_max=max_train_steps, \n        warmup_steps=warmup_steps\n    )    \n\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n\n    result_dict = {\n        'epoch':[], \n        'train_loss': [], \n        'val_loss' : [], \n        'best_val_loss': np.inf\n    }\n    return (\n        model, tokenizer, \n        optimizer, scheduler, \n        scaler, train_loader, \n        valid_loader, result_dict, \n        epochs\n    )","metadata":{"_kg_hide-output":true,"id":"addressed-function","papermill":{"duration":0.031861,"end_time":"2021-06-05T15:27:33.277842","exception":false,"start_time":"2021-06-05T15:27:33.245981","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:17:45.523757Z","iopub.execute_input":"2021-06-10T02:17:45.524099Z","iopub.status.idle":"2021-06-10T02:17:45.535969Z","shell.execute_reply.started":"2021-06-10T02:17:45.524051Z","shell.execute_reply":"2021-06-10T02:17:45.53499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run","metadata":{"id":"nonprofit-causing","papermill":{"duration":0.019993,"end_time":"2021-06-05T15:27:33.317894","exception":false,"start_time":"2021-06-05T15:27:33.297901","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def run(fold=0):\n    model, tokenizer, optimizer, scheduler, scaler, \\\n        train_loader, valid_loader, result_dict, epochs = config(fold)\n    \n    import time\n    trainer = Trainer(model, optimizer, scheduler, scaler)\n    train_time_list = []\n\n    for epoch in range(epochs):\n        result_dict['epoch'] = epoch\n\n        torch.cuda.synchronize()\n        tic1 = time.time()\n\n        result_dict = trainer.train(train_loader, valid_loader, epoch, \n                                    result_dict, tokenizer, fold)\n\n        torch.cuda.synchronize()\n        tic2 = time.time() \n        train_time_list.append(tic2 - tic1)\n\n    torch.cuda.empty_cache()\n    del model, tokenizer, optimizer, scheduler, \\\n        scaler, train_loader, valid_loader,\n    gc.collect()\n    return result_dict","metadata":{"id":"suspended-anniversary","papermill":{"duration":0.029398,"end_time":"2021-06-05T15:27:33.367679","exception":false,"start_time":"2021-06-05T15:27:33.338281","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:17:47.865847Z","iopub.execute_input":"2021-06-10T02:17:47.866196Z","iopub.status.idle":"2021-06-10T02:17:47.872985Z","shell.execute_reply.started":"2021-06-10T02:17:47.866164Z","shell.execute_reply":"2021-06-10T02:17:47.872046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_list = []\nfor fold in range(5):\n    print('----')\n    print(f'FOLD: {fold}')\n    result_dict = run(fold)\n    result_list.append(result_dict)\n    print('----')","metadata":{"id":"flush-clause","outputId":"0bb37720-4c50-4662-8128-176e7008e03d","papermill":{"duration":31465.646307,"end_time":"2021-06-06T00:11:59.034197","exception":false,"start_time":"2021-06-05T15:27:33.38789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-10T02:17:48.168256Z","iopub.execute_input":"2021-06-10T02:17:48.168549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best Validation Loss per Fold","metadata":{"papermill":{"duration":2.862857,"end_time":"2021-06-06T00:12:04.956549","exception":false,"start_time":"2021-06-06T00:12:02.093692","status":"completed"},"tags":[]}},{"cell_type":"code","source":"[print(\"FOLD::\", i, \"Loss:: \", fold['best_val_loss']) for i, fold in enumerate(result_list)]","metadata":{"id":"lt2jASMiPH1R","outputId":"473119ae-40c4-4b31-fa9a-b6c42feaf3fb","papermill":{"duration":3.123907,"end_time":"2021-06-06T00:12:10.909223","exception":false,"start_time":"2021-06-06T00:12:07.785316","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### OOF Prediction","metadata":{"papermill":{"duration":2.905577,"end_time":"2021-06-06T00:12:17.321393","exception":false,"start_time":"2021-06-06T00:12:14.415816","status":"completed"},"tags":[]}},{"cell_type":"code","source":"oof = np.zeros(len(train))\nfor fold in tqdm(range(5), total=5):\n    model, tokenizer = make_model()\n    model.load_state_dict(\n        torch.load(f'model{fold}.bin')\n    )\n    model.cuda()\n    model.eval()\n    val_index = train[train.kfold==fold].index.tolist()\n    train_loader, val_loader = make_loader(train, tokenizer, 250, 16, fold=fold)\n    # scalar = torch.cuda.amp.GradScaler()\n    scalar = None\n    preds = []\n    for index, data in enumerate(val_loader):\n        input_ids, attention_mask, token_type_ids, labels = data['input_ids'], \\\n            data['attention_mask'], data['token_type_ids'], data['label']\n        input_ids, attention_mask, token_type_ids, labels = input_ids.cuda(), \\\n            attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n        if scalar is not None:\n            with torch.cuda.amp.autocast():\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_type_ids=token_type_ids,\n                    labels=labels\n                )\n        else:\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n                labels=labels\n            )\n        \n        loss, logits = outputs[:2]\n        preds += logits.cpu().detach().numpy().tolist()\n    oof[val_index] = preds","metadata":{"id":"_aaj35kCjxzT","outputId":"a1830cca-986e-4797-d4ee-06466df3e0d3","papermill":{"duration":6.739784,"end_time":"2021-06-06T00:12:26.898633","exception":false,"start_time":"2021-06-06T00:12:20.158849","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compute Local CV Score","metadata":{"papermill":{"duration":2.817029,"end_time":"2021-06-06T00:12:32.622642","exception":false,"start_time":"2021-06-06T00:12:29.805613","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nround(np.sqrt(mean_squared_error(train.target.values, oof)), 4)","metadata":{"id":"js2Li0srR1Cq","outputId":"bd44fd20-ccf3-4e42-eb81-677648c73105","papermill":{"duration":2.839039,"end_time":"2021-06-06T00:12:38.487048","exception":false,"start_time":"2021-06-06T00:12:35.648009","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":3.31326,"end_time":"2021-06-06T00:12:44.732598","exception":false,"start_time":"2021-06-06T00:12:41.419338","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}