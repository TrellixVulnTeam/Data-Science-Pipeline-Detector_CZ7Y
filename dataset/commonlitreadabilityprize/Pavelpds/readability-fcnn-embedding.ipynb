{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Loading required packages\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport collections\nimport re\n\nfrom itertools import chain\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"id":"VkOFjdksoeHM","outputId":"a1700984-a22e-4aa5-dcdf-deb7b8aa4077","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set seed to get the same results each time\nnp.random.seed(0)","metadata":{"id":"dhatXsJD3N96","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the training data\ndf = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ndf.head()","metadata":{"id":"jgpgMiKzoFte","outputId":"7e649eef-8583-460b-9f05-847f4155d935","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the test data\ntest = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntest.head()","metadata":{"id":"BVcF0oTaznfe","outputId":"fd7051b7-1d0f-4abf-a2ec-d016a70aaa7a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"dKA9AND5oFtj","outputId":"c29667ea-6b8d-47e6-8401-f37bf1ffd917","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.target.describe()","metadata":{"id":"r5MqyRFCoFtk","outputId":"423d28b9-c545-4cc3-81fa-24c74374af70","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.standard_error.describe()","metadata":{"id":"k41KiHWxoFtl","outputId":"cff617b7-2f95-44e7-d41c-7bf26b43bd9d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train-test split\nexcerpt_train, excerpt_val, y_train, y_val = train_test_split(df.excerpt, df.target, test_size=0.20)","metadata":{"id":"8vK7EPguoFtm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pd.series to array\ntrain_data = excerpt_train.to_numpy()\nval_data = excerpt_val.to_numpy()\ntest_data = test.excerpt.to_numpy()\n\n# tokenizer and vocab\nTOKEN_RE = re.compile(r'[\\w\\d]+')\n\ndef tokenize_text_simple_regex(txt, min_token_size=4):\n    txt = txt.lower()\n    all_tokens = TOKEN_RE.findall(txt)\n    return [token for token in all_tokens if len(token) >= min_token_size]\n\ndef tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n\ndef add_fake_token(word2id, token='<PAD>'):\n    word2id_new = {token: i + 1 for token, i in word2id.items()}\n    word2id_new[token] = 0\n    return word2id_new\n\ndef texts_to_token_ids(tokenized_texts, word2id):\n    return [[word2id[token] for token in text if token in word2id]\n            for text in tokenized_texts]\n\n\ndef build_vocabulary(tokenized_texts, max_size=10000, max_doc_freq=0.8, \n                     min_count=5, pad_word=None):\n    word_counts = collections.defaultdict(int)\n    doc_n = 0\n\n    # count the number of documents in which each word is used\n    # as well as the total number of documents\n    for txt in tokenized_texts:\n        doc_n += 1\n        unique_text_tokens = set(txt)\n        for token in unique_text_tokens:\n            word_counts[token] += 1\n\n    # remove too rare and too frequent words\n    word_counts = {word: cnt for word, cnt in word_counts.items()\n                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n\n    # sort words by descending frequency\n    sorted_word_counts = sorted(word_counts.items(),\n                                reverse=True,\n                                key=lambda pair: pair[1])\n\n    # add a nonexistent word with index 0 for batch processing convenience\n    if pad_word is not None:\n        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n\n    # if we still have too many words, leave only the max_size of the most frequent ones\n    if len(word_counts) > max_size:\n        sorted_word_counts = sorted_word_counts[:max_size]\n\n    # we number the words\n    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n\n    # normalize the frequency of words\n    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n\n    return word2id, word2freq","metadata":{"id":"b6SyJviXoFtm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tokenized = tokenize_corpus(train_data)\nval_tokenized = tokenize_corpus(val_data)\ntest_tokenized = tokenize_corpus(test_data)\n\nprint(' '.join(train_tokenized[0]))","metadata":{"id":"88ey4FDNoFtn","outputId":"26f8e92a-97a7-4783-f636-8da47fcb6103","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary, word_doc_freq = build_vocabulary(train_tokenized, \n                                             max_doc_freq=0.8, \n                                             min_count=5, \n                                             pad_word='<PAD>')\n\nUNIQUE_WORDS_N = len(vocabulary)\nprint('Number of unique tokens', UNIQUE_WORDS_N)\nprint(list(vocabulary.items())[:10])","metadata":{"id":"t9oTG-veoFtn","outputId":"2b5cd499-bc50-4d00-866b-49a921eb35e1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(word_doc_freq, bins=20)\nplt.title('Distribution of relative word frequencies')\nplt.yscale('log');","metadata":{"id":"HVJ8NneloFto","outputId":"8c905c3e-dee3-4011-e764-015de1b67a3c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# numbers of tokens\ntrain_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\nval_token_ids = texts_to_token_ids(val_tokenized, vocabulary)\ntest_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n\nprint('\\n'.join(' '.join(str(t) for t in sent)\n                for sent in train_token_ids[:10]))","metadata":{"id":"FdBDn8jaoFto","outputId":"4c9ff1b5-53d3-45e3-b839-1bc0a06a5cf1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist([len(s) for s in train_token_ids], bins=20);\nplt.title('Histogram of article lengths');","metadata":{"id":"TVqTgt6zqlxy","outputId":"bf83b711-96e9-4359-d8c8-6058cc6f30ca","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SEQ_LEN = 256 # Final sequence length\n\ntrain_data = tf.keras.preprocessing.sequence.pad_sequences(\n    train_token_ids,\n    value=vocabulary[\"<PAD>\"],\n    padding='post',\n    maxlen=MAX_SEQ_LEN)\n\nval_data = tf.keras.preprocessing.sequence.pad_sequences(\n    val_token_ids,\n    value=vocabulary[\"<PAD>\"],\n    padding='post',\n    maxlen=MAX_SEQ_LEN)\n\ntest_data = tf.keras.preprocessing.sequence.pad_sequences(\n    test_token_ids,\n    value=vocabulary[\"<PAD>\"],\n    padding='post',\n    maxlen=MAX_SEQ_LEN)\n\nprint(\"Length examples: {}\".format([len(train_data[0]), len(train_data[1])]))\nprint('=====================================')\nprint(\"Entry example: {}\".format(train_data[0]))","metadata":{"id":"gn_9D3-cqmKP","outputId":"0b29a0ba-ca0e-483f-ab99-58467eafd680","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the model\nEMB_SIZE = 32 # The size of the vector representation (embedding)\n    \nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(UNIQUE_WORDS_N, 32),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(32, activation=tf.nn.relu),\n    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(1),\n])\n\nmodel.summary()","metadata":{"id":"1tntZQZArSIX","outputId":"0e8ae14d-5bac-47c5-a3bd-24ff96c01691","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's train \nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4), \n              loss='mae'\n              )\n\nhistory = model.fit(\n    train_data, y_train,\n    validation_data=(val_data, y_val),\n    batch_size=128,\n    epochs=100)","metadata":{"id":"MUKlylYjrSsg","outputId":"befa9921-e4fa-4abc-ea80-b4f6a4ab1915","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate performance on validation set\nreadable_preds = model.predict(val_data)\nval_mae = mean_absolute_error(y_val,readable_preds)\nprint(\"Validation MAE for Deep learning Model: {}\".format(val_mae))","metadata":{"id":"XDarPdupsFNe","outputId":"b802fc4d-9db4-4849-c6c1-bd7d926f5d1b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's test it\ntest_preds = model.predict(test_data)\ntest_preds = test_preds.tolist()\ntest_preds = list(chain.from_iterable(test_preds))","metadata":{"id":"wUdmfUkKxRC0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame({'id': test.id.tolist(), \n                              'target': test_preds})","metadata":{"id":"2jh8wwJYxUeP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index = False)","metadata":{"id":"jNvA7geNxYs0","trusted":true},"execution_count":null,"outputs":[]}]}