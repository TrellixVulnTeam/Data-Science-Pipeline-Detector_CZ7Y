{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nimport logging\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import backend as K\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom kaggle_datasets import KaggleDatasets\ntf.get_logger().setLevel(logging.ERROR)\nfrom kaggle_datasets import KaggleDatasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configurations\nEPOCHS = 70\n# Batch size\nBATCH_SIZE = 24\n# Seed\nSEED = 123\n# Learning rate\nLR = 0.000040\n# Verbosity\nVERBOSE = 2\n# Number of folds for training\nFOLDS = 5\n\n# Max length\nMAX_LEN = 250\n\n# Get the trained model we want to use\nMODEL = 'roberta-base'\n\n# Let's load our model tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(MODEL)\n\n# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\n# This function tokenize the text according to a transformers model tokenizer\ndef regular_encode(texts, tokenizer, maxlen = MAX_LEN):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        padding = 'max_length',\n        truncation = True,\n        max_length = maxlen,\n    )\n    \n    return np.array(enc_di['input_ids'])\n\n# This function encode our training sentences\ndef encode_texts(x_train, x_val, MAX_LEN):\n    x_train = regular_encode(x_train.tolist(), tokenizer, maxlen = MAX_LEN)\n    x_val = regular_encode(x_val.tolist(), tokenizer, maxlen = MAX_LEN)\n    return x_train, x_val\n\n# Function to transform arrays to tensors\ndef transform_to_tensors(x_train, x_val, y_train, y_val):\n    \n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_train, y_train))\n        .repeat()\n        .shuffle(2048)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n    \n    valid_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_val, y_val))\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n    \n    return train_dataset, valid_dataset\n\n# Function to build our model\ndef build_roberta_base_model(max_len = MAX_LEN):\n    transformer = TFRobertaModel.from_pretrained(MODEL)\n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [output])\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = LR),\n                  loss = [tf.keras.losses.MeanSquaredError()],\n                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n    return model\n\n# Function to train and evaluate our model\ndef train_and_evaluate():\n    \n    # Read our training data\n    df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n    # Seed everything\n    seed_everything(SEED)\n    \n    # Initiate kfold object with shuffle and a specific seed\n    kfold = KFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n    # Create out of folds array to store predictions\n    oof_predictions = np.zeros(len(df))\n    \n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(df)):\n        print('\\n')\n        print('-'*50)\n        print(f'Training fold {fold + 1}')\n        K.clear_session()\n        # Get text features and target\n        x_train, x_val = df['excerpt'].iloc[trn_ind], df['excerpt'].iloc[val_ind]\n        y_train, y_val = df['target'].iloc[trn_ind].values, df['target'].iloc[val_ind].values\n        # Encode our text with Roberta tokenizer\n        x_train, x_val = encode_texts(x_train, x_val, MAX_LEN)\n        # Function to transform our numpy array to a tf Dataset\n        train_dataset, valid_dataset = transform_to_tensors(x_train, x_val, y_train, y_val)\n        # Build model\n        model = build_roberta_base_model(max_len = MAX_LEN)\n        # Model checkpoint\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Roberta_Base_{SEED}_{fold + 1}.h5', \n                                                        monitor = 'val_root_mean_squared_error', \n                                                        verbose = VERBOSE, \n                                                        save_best_only = True,\n                                                        save_weights_only = True, \n                                                        mode = 'min')\n        steps = x_train.shape[0] // (BATCH_SIZE * 16)\n        # Training phase\n        history = model.fit(train_dataset,\n                            batch_size = BATCH_SIZE,\n                            epochs = EPOCHS,\n                            verbose = VERBOSE,\n                            callbacks = [checkpoint],\n                            validation_data = valid_dataset,\n                            steps_per_epoch = steps)\n        \n        \n        # Load best epoch weights\n        model.load_weights(f'Roberta_Base_{SEED}_{fold + 1}.h5')\n        # Predict validation set to save them in the out of folds array\n        val_pred = model.predict(valid_dataset)\n        oof_predictions[val_ind] = val_pred.reshape(-1)\n        \n    print('\\n')\n    print('-'*50)\n    # Calculate out of folds root mean squared error\n    oof_rmse = np.sqrt(mean_squared_error(df['target'], oof_predictions))\n    print(f'Our out of folds RMSE is {oof_rmse}')\n    \n\ntrain_and_evaluate()","metadata":{},"execution_count":null,"outputs":[]}]}