{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## First NLP Competition \nThis is the first NLP competition for me and I had never explored transformers before this. So, firstly, I'd like to mention some resources that helped me to learn and achieve this result.\n\n* World's first 4xGM [Abhishek Thakur](https://www.kaggle.com/abhishek)'s book [Approaching (Almost) Any Machine Learning Problem](https://github.com/abhishekkrthakur/approachingalmost) and his [youtube](https://www.youtube.com/user/abhisheksvnit) channel.\n* [Notebooks](https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference) by [Maunish dave](https://www.kaggle.com/maunish)\n* Some YouTube videos:\n    * [BERT Neural Network - EXPLAINED! by CodeEmporium](https://youtu.be/xI0HHN5XKDo)\n    * [Grandmaster Series â€“ Building World-Class NLP Models with Transformers and Hugging Face by NVIDIA Developer](https://youtu.be/PXc_SlnT2g0)","metadata":{}},{"cell_type":"markdown","source":"### Approach used:\n* pretraining roberta \n* Tokenizer: RobertaTokenizer\n* model: roberta for sequence classification\n* 5 folds, 7 epochs\n\n### scores upon submission:\n\n| Model | RMSE on LB |\n| --- | --- |\n| RoBERTa for seq classif | 0.481 |\n| RoBERTa for seq classif stacked with lgbm | 0.482 |\n| RoBERTa for seq classif stacked with ridge | 0.483 |","metadata":{}},{"cell_type":"code","source":"# specifying basic config\nimport transformers\n\nMAX_LEN = 256\n\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 32\nTEST_BATCH_SIZE = 32\n\nEPOCHS = 5\n\nBERT_PATH = \"../input/roberta-base\"\n\nMODEL_PATH = \"../input/roberta-de-nero/\"\n\ndevice = 'cuda'\n\nTOKENIZER = transformers.RobertaTokenizer.from_pretrained(BERT_PATH)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-19T10:45:43.163741Z","iopub.execute_input":"2021-06-19T10:45:43.164085Z","iopub.status.idle":"2021-06-19T10:45:43.288661Z","shell.execute_reply.started":"2021-06-19T10:45:43.164055Z","shell.execute_reply":"2021-06-19T10:45:43.287538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nclass BERTDataset:\n    def __init__(self, text):\n        self.text = text\n    \n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, item):\n        encode = self.tokenizer(self.text[item],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        return encode","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:45:43.905047Z","iopub.execute_input":"2021-06-19T10:45:43.905412Z","iopub.status.idle":"2021-06-19T10:45:45.283689Z","shell.execute_reply.started":"2021-06-19T10:45:43.905383Z","shell.execute_reply":"2021-06-19T10:45:45.282309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# attention head\nfrom torch import nn \n\nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:45:45.28779Z","iopub.execute_input":"2021-06-19T10:45:45.288083Z","iopub.status.idle":"2021-06-19T10:45:45.2966Z","shell.execute_reply.started":"2021-06-19T10:45:45.288053Z","shell.execute_reply":"2021-06-19T10:45:45.295171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom torch import nn\n\nclass ROBERTA_(nn.Module):\n    def __init__(self):\n        super(ROBERTA_, self).__init__()\n        self.bert = transformers.AutoModelForSequenceClassification.from_pretrained(\n            BERT_PATH,\n            num_labels=1\n        )\n        self.head = AttentionHead(768, 768, 1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(768, 1)\n\n    def forward(self, **param_mehta):\n        \n        x = self.bert(**param_mehta)\n        x = x[\"logits\"].squeeze(-1)\n        \n#         x = x[0]\n#         x = self.head(x)\n#         x = self.dropout(x)\n#         x = self.linear(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:45:45.299337Z","iopub.execute_input":"2021-06-19T10:45:45.300081Z","iopub.status.idle":"2021-06-19T10:45:45.310336Z","shell.execute_reply.started":"2021-06-19T10:45:45.300036Z","shell.execute_reply":"2021-06-19T10:45:45.309228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\n\nnum_bins = int(np.floor(1 + np.log2(len(train))))\ntrain.loc[:,'bins'] = pd.cut(train['target'], bins=num_bins, labels=False)\n\nyyy = train.target.values\nbins = train.bins.values\n\ntrain_dataset = BERTDataset(\n    text = train.excerpt.values\n)\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size = TRAIN_BATCH_SIZE,\n)\n\n\ntest_dataset = BERTDataset(\n    text = test.excerpt.values\n)\ntest_dataloader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size = TEST_BATCH_SIZE,\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:45:45.312075Z","iopub.execute_input":"2021-06-19T10:45:45.315746Z","iopub.status.idle":"2021-06-19T10:45:45.418056Z","shell.execute_reply.started":"2021-06-19T10:45:45.315701Z","shell.execute_reply":"2021-06-19T10:45:45.416988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ROBERTA_()","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:45:45.420161Z","iopub.execute_input":"2021-06-19T10:45:45.420467Z","iopub.status.idle":"2021-06-19T10:45:56.199268Z","shell.execute_reply.started":"2021-06-19T10:45:45.420438Z","shell.execute_reply":"2021-06-19T10:45:56.198091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(test_data, model, model_path):\n    preds = []\n    \n    state = torch.load(model_path)\n    model.load_state_dict(state['state_dict'])\n    \n    model.to(device)\n    model.eval()\n    \n    with torch.no_grad():\n        for d in test_data:\n            inputs = {key: val.reshape(val.shape[0], -1).to(device) for key, val in d.items()}\n            output = model(**inputs)\n            \n            preds.extend(output.cpu().numpy())\n    \n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:45:56.201074Z","iopub.execute_input":"2021-06-19T10:45:56.201467Z","iopub.status.idle":"2021-06-19T10:45:56.211231Z","shell.execute_reply.started":"2021-06-19T10:45:56.20143Z","shell.execute_reply":"2021-06-19T10:45:56.207741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test = [], []\nprint('Fold:', end=' ')\nfor i in range(5):\n    model_path_ = MODEL_PATH + 'model_' + str(i) + '.pth'\n    x_train.append(predict(train_dataloader, model, model_path_))\n    x_test.append(predict(test_dataloader, model, model_path_))\n    print(f'{i}', end=' ')","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:45:56.213364Z","iopub.execute_input":"2021-06-19T10:45:56.214021Z","iopub.status.idle":"2021-06-19T10:49:35.098096Z","shell.execute_reply.started":"2021-06-19T10:45:56.213975Z","shell.execute_reply":"2021-06-19T10:49:35.096989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First, I have created a submission dataframe with targets just from RoBERTa model.","metadata":{}},{"cell_type":"code","source":"preds_sub = None\nfor i in x_test:\n    try:\n        preds_sub += np.array(i)\n    except:\n        preds_sub = np.array(i)\npreds_sub /= 5\n\nsub_nostack = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\nsub_nostack['target'] = preds_sub\nprint(sub_nostack)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:50:46.540825Z","iopub.execute_input":"2021-06-19T10:50:46.54131Z","iopub.status.idle":"2021-06-19T10:50:46.871638Z","shell.execute_reply.started":"2021-06-19T10:50:46.541242Z","shell.execute_reply":"2021-06-19T10:50:46.870599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\ndef rmse_score(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:50:48.2199Z","iopub.execute_input":"2021-06-19T10:50:48.220322Z","iopub.status.idle":"2021-06-19T10:50:48.225495Z","shell.execute_reply.started":"2021-06-19T10:50:48.220271Z","shell.execute_reply":"2021-06-19T10:50:48.223987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stacking on RoBERTa\n\nThe following cell is for stacking a model on RoBERTa. I have used 5 folds again.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import Ridge\nfrom lightgbm import LGBMRegressor\n\n# add model of choice for stacking\nstacking_models_dispatcher = {\n    'ridge': Ridge(alpha=50),\n    'lgbm': LGBMRegressor()\n}\n\ntarget = None\nfor train_data, test_data in zip(x_train, x_test):\n    \n    kfold = StratifiedKFold(n_splits=5)\n    \n    preds = None\n    sum_scores = 0\n    for k, (train_idx, valid_idx) in enumerate(kfold.split(train_data, bins)):\n        \n        train_data = np.array(train_data).reshape(-1, 1)\n        test_data = np.array(test_data).reshape(-1, 1)\n        \n        stacking_model = stacking_models_dispatcher['ridge'] # specify model of choice for stacking\n        X_train, y_train = train_data[train_idx], yyy[train_idx]\n        X_valid, y_valid = train_data[valid_idx], yyy[valid_idx]\n        \n    \n        stacking_model.fit(X_train, y_train)\n        prediction = stacking_model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k}, rmse score: {score}')\n        \n        sum_scores += score\n        try:\n            preds += stacking_model.predict(test_data)\n        except:\n            preds = stacking_model.predict(test_data)\n \n    print(f'MEAN RMSE: {sum_scores / 5}')\n        \n    preds /= 5\n    try:\n        target += preds\n    except:\n        target = preds \ntarget /= 5","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:50:49.748544Z","iopub.execute_input":"2021-06-19T10:50:49.74899Z","iopub.status.idle":"2021-06-19T10:50:52.26661Z","shell.execute_reply.started":"2021-06-19T10:50:49.74896Z","shell.execute_reply":"2021-06-19T10:50:52.265436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_stack = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\nsub_stack['target'] = target\nprint(sub_stack)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:50:57.131106Z","iopub.execute_input":"2021-06-19T10:50:57.131669Z","iopub.status.idle":"2021-06-19T10:50:57.157384Z","shell.execute_reply.started":"2021-06-19T10:50:57.13161Z","shell.execute_reply":"2021-06-19T10:50:57.156139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### If you would like to use this notebook, make sure you use the correct data frame to save the submission file in the next cell.","metadata":{}},{"cell_type":"code","source":"sub_nostack.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-19T10:50:58.723363Z","iopub.execute_input":"2021-06-19T10:50:58.723724Z","iopub.status.idle":"2021-06-19T10:50:58.735715Z","shell.execute_reply.started":"2021-06-19T10:50:58.723694Z","shell.execute_reply":"2021-06-19T10:50:58.73448Z"},"trusted":true},"execution_count":null,"outputs":[]}]}