{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers --no-index --find-links=file:///kaggle/input/trans-47-whl/transformers==4.8.1/","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:01:42.011584Z","iopub.execute_input":"2021-07-28T01:01:42.011964Z","iopub.status.idle":"2021-07-28T01:01:55.509932Z","shell.execute_reply.started":"2021-07-28T01:01:42.011873Z","shell.execute_reply":"2021-07-28T01:01:55.508646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import RobertaModel\nimport torch.nn as nn\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport gc\ngc.enable()\ndevice = \"cuda:0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-28T01:01:55.514256Z","iopub.execute_input":"2021-07-28T01:01:55.514637Z","iopub.status.idle":"2021-07-28T01:01:58.546849Z","shell.execute_reply.started":"2021-07-28T01:01:55.514602Z","shell.execute_reply":"2021-07-28T01:01:58.545479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = \"../input/roberta-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True, model_max_length=256)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:01:58.549661Z","iopub.execute_input":"2021-07-28T01:01:58.550067Z","iopub.status.idle":"2021-07-28T01:01:58.766148Z","shell.execute_reply.started":"2021-07-28T01:01:58.550021Z","shell.execute_reply":"2021-07-28T01:01:58.764892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_params(module_lst):\n    for module in module_lst:\n        for param in module.parameters():\n            if param.dim() > 1:\n                torch.nn.init.xavier_uniform_(param)\n    return\n\nclass SpatialDropout(nn.Module):\n    def __init__(self,p=0.5):\n        super(SpatialDropout, self).__init__()\n        self.dropout2D = nn.Dropout2d(p=p)\n        \n    def forward(self, x):\n        x = x.unsqueeze(2)\n        x = x.permute(0, 3, 2, 1)\n        x = self.dropout2D(x)\n        x = x.permute(0, 3, 2, 1)\n        x = x.squeeze(2)\n        return x\n\nclass Custom_bert(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(model_dir)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(model_dir, config=config)  \n        \n        dim = self.roberta.pooler.dense.bias.shape[0]\n        \n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)\n        \n        n_weights = 24\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n            \n        self.attention = nn.Sequential(\n            nn.Linear(1024, 1024),            \n            nn.Tanh(),\n            nn.Linear(1024, 1),\n            nn.Softmax(dim=1)\n        ) \n        self.cls = nn.Sequential(\n            nn.Linear(dim,1)\n        )\n        init_params([self.cls,self.attention])\n        \n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)\n\n        cls_outputs = torch.stack(\n            [self.dropout(layer) for layer in roberta_output[2][-24:]], dim=0\n        )\n        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n    \n        logits = torch.mean(\n            torch.stack(\n                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n                dim=0,\n            ),\n            dim=0,\n        )\n        return self.cls(logits)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:01:58.768182Z","iopub.execute_input":"2021-07-28T01:01:58.768658Z","iopub.status.idle":"2021-07-28T01:01:58.788318Z","shell.execute_reply.started":"2021-07-28T01:01:58.768625Z","shell.execute_reply":"2021-07-28T01:01:58.786787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=256,\n                                padding='max_length',truncation=True)\n        encoded = {'input_ids':encode['input_ids'][0],\n                   'attention_mask':encode['attention_mask'][0]\n                  }\n        \n        return encoded\n    \n    def __len__(self):\n        return len(self.excerpt)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:01:58.789929Z","iopub.execute_input":"2021-07-28T01:01:58.790624Z","iopub.status.idle":"2021-07-28T01:01:58.804081Z","shell.execute_reply.started":"2021-07-28T01:01:58.790579Z","shell.execute_reply":"2021-07-28T01:01:58.802782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:01:58.805737Z","iopub.execute_input":"2021-07-28T01:01:58.80621Z","iopub.status.idle":"2021-07-28T01:01:58.831111Z","shell.execute_reply.started":"2021-07-28T01:01:58.806163Z","shell.execute_reply":"2021-07-28T01:01:58.830076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_fold(fold_num,pth):\n    model = Custom_bert().to(device)\n    _ = model.eval()\n    model.load_state_dict(torch.load(f\"{pth}/roberta_large_{fold_num}.pt\"), strict=False)\n    \n    test_ds = CLRPDataset(test_data,tokenizer)\n    test_dl = DataLoader(test_ds,\n                        batch_size = 16,\n                        shuffle=False,\n                        pin_memory=True)\n    \n    pred = []\n    with torch.no_grad():\n        for batch in test_dl:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            output = model(input_ids, attention_mask)\n            pred.extend(output.detach().cpu().numpy())\n            \n    del model, test_dl, test_ds\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return np.array(pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:01:58.83289Z","iopub.execute_input":"2021-07-28T01:01:58.83342Z","iopub.status.idle":"2021-07-28T01:01:58.844881Z","shell.execute_reply.started":"2021-07-28T01:01:58.833376Z","shell.execute_reply":"2021-07-28T01:01:58.843508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_single_model(pth):\n    pred0 = run_fold(0,pth)\n    pred1 = run_fold(1,pth)\n    pred2 = run_fold(2,pth)\n    pred3 = run_fold(3,pth)\n    pred4 = run_fold(4,pth)\n    \n    return np.expand_dims(np.mean(np.concatenate((pred0, pred1, pred2, pred3, pred4),axis=1),axis=1),axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:01:58.849794Z","iopub.execute_input":"2021-07-28T01:01:58.850218Z","iopub.status.idle":"2021-07-28T01:01:58.859431Z","shell.execute_reply.started":"2021-07-28T01:01:58.850188Z","shell.execute_reply":"2021-07-28T01:01:58.85783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_1 = get_single_model('../input/clrp-bag2/4808 LB448')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:01:58.86271Z","iopub.execute_input":"2021-07-28T01:01:58.863502Z","iopub.status.idle":"2021-07-28T01:04:17.836079Z","shell.execute_reply.started":"2021-07-28T01:01:58.863425Z","shell.execute_reply":"2021-07-28T01:04:17.834958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = \"../input/deberta-l\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True, model_max_length=256)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:04:17.837932Z","iopub.execute_input":"2021-07-28T01:04:17.838647Z","iopub.status.idle":"2021-07-28T01:04:18.083621Z","shell.execute_reply.started":"2021-07-28T01:04:17.838598Z","shell.execute_reply":"2021-07-28T01:04:18.08252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_params(module_lst):\n    for module in module_lst:\n        for param in module.parameters():\n            if param.dim() > 1:\n                torch.nn.init.xavier_uniform_(param)\n    return\n\nclass Custom_bert(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(model_dir)\n        config.update({\"output_hidden_states\":True,\n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.base = AutoModel.from_pretrained(model_dir, config=config)  \n        \n        dim = 1024\n        \n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)\n        \n        n_weights = 24\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n            \n        self.attention = nn.Sequential(\n            nn.Linear(1024, 1024),            \n            nn.Tanh(),\n            nn.Linear(1024, 1),\n            nn.Softmax(dim=1)\n        ) \n        self.cls = nn.Sequential(\n            nn.Linear(dim,1)\n        )\n        init_params([self.cls,self.attention])\n        \n    def forward(self, input_ids, attention_mask):\n        base_output = self.base(input_ids=input_ids,\n                                      attention_mask=attention_mask)\n        \n        cls_outputs = torch.stack(\n            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n        )\n        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n    \n        logits = torch.mean(\n            torch.stack(\n                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n                dim=0,\n            ),\n            dim=0,\n        )\n        return self.cls(logits)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:04:18.085304Z","iopub.execute_input":"2021-07-28T01:04:18.085777Z","iopub.status.idle":"2021-07-28T01:04:18.103607Z","shell.execute_reply.started":"2021-07-28T01:04:18.085731Z","shell.execute_reply":"2021-07-28T01:04:18.102421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_fold(fold_num,pth):\n    model = Custom_bert().to(device)\n    _ = model.eval()\n    model.load_state_dict(torch.load(f\"{pth}/deberta_large_{fold_num}.pt\"), strict=False)\n    \n    test_ds = CLRPDataset(test_data,tokenizer)\n    test_dl = DataLoader(test_ds,\n                        batch_size = 16,\n                        shuffle=False,\n                        pin_memory=True)\n    \n    pred = []\n    with torch.no_grad():\n        for batch in test_dl:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            output = model(input_ids, attention_mask)\n            pred.extend(output.detach().cpu().numpy())\n            \n    del model, test_dl, test_ds\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return np.array(pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:04:18.10509Z","iopub.execute_input":"2021-07-28T01:04:18.10555Z","iopub.status.idle":"2021-07-28T01:04:18.118297Z","shell.execute_reply.started":"2021-07-28T01:04:18.105493Z","shell.execute_reply":"2021-07-28T01:04:18.117134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_2 = get_single_model('../input/clrp-bag/4724 LB448')\npred_3 = get_single_model('../input/clrp-bag/4714 LB448')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:04:18.120218Z","iopub.execute_input":"2021-07-28T01:04:18.120756Z","iopub.status.idle":"2021-07-28T01:07:48.686322Z","shell.execute_reply.started":"2021-07-28T01:04:18.120703Z","shell.execute_reply":"2021-07-28T01:07:48.683638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_1","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:07:48.72813Z","iopub.execute_input":"2021-07-28T01:07:48.730686Z","iopub.status.idle":"2021-07-28T01:07:48.751303Z","shell.execute_reply.started":"2021-07-28T01:07:48.730641Z","shell.execute_reply":"2021-07-28T01:07:48.750092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_2","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:07:48.757182Z","iopub.execute_input":"2021-07-28T01:07:48.76039Z","iopub.status.idle":"2021-07-28T01:07:48.773315Z","shell.execute_reply.started":"2021-07-28T01:07:48.76035Z","shell.execute_reply":"2021-07-28T01:07:48.771342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_3","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:07:48.777353Z","iopub.execute_input":"2021-07-28T01:07:48.778059Z","iopub.status.idle":"2021-07-28T01:07:48.791314Z","shell.execute_reply.started":"2021-07-28T01:07:48.778017Z","shell.execute_reply":"2021-07-28T01:07:48.789465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat = np.concatenate((pred_1,pred_2,pred_3),axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:09:29.98843Z","iopub.execute_input":"2021-07-28T01:09:29.988904Z","iopub.status.idle":"2021-07-28T01:09:29.995589Z","shell.execute_reply.started":"2021-07-28T01:09:29.988857Z","shell.execute_reply":"2021-07-28T01:09:29.992585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sort = np.sort(cat, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:09:30.888897Z","iopub.execute_input":"2021-07-28T01:09:30.889282Z","iopub.status.idle":"2021-07-28T01:09:30.895565Z","shell.execute_reply.started":"2021-07-28T01:09:30.889249Z","shell.execute_reply":"2021-07-28T01:09:30.894094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = np.mean(sort,axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:09:32.020574Z","iopub.execute_input":"2021-07-28T01:09:32.021123Z","iopub.status.idle":"2021-07-28T01:09:32.027438Z","shell.execute_reply.started":"2021-07-28T01:09:32.02108Z","shell.execute_reply":"2021-07-28T01:09:32.025744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:09:33.690896Z","iopub.execute_input":"2021-07-28T01:09:33.691293Z","iopub.status.idle":"2021-07-28T01:09:33.698818Z","shell.execute_reply.started":"2021-07-28T01:09:33.691261Z","shell.execute_reply":"2021-07-28T01:09:33.697323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample['target'] = pred\nsample.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:09:45.536171Z","iopub.execute_input":"2021-07-28T01:09:45.536582Z","iopub.status.idle":"2021-07-28T01:09:46.036531Z","shell.execute_reply.started":"2021-07-28T01:09:45.536552Z","shell.execute_reply":"2021-07-28T01:09:46.035379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample","metadata":{"execution":{"iopub.status.busy":"2021-07-28T01:09:46.038628Z","iopub.execute_input":"2021-07-28T01:09:46.039145Z","iopub.status.idle":"2021-07-28T01:09:46.058359Z","shell.execute_reply.started":"2021-07-28T01:09:46.0391Z","shell.execute_reply":"2021-07-28T01:09:46.056633Z"},"trusted":true},"execution_count":null,"outputs":[]}]}