{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CommonLit: Signal vs. Noise\n\n### The nature of standard_error\n\nThe standard error is a *measure of spread of scores among multiple raters for each excerpt.* This is an ensemble method of building the dataset. To take an example from the training data set:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntrain.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-22T15:03:04.371695Z","iopub.execute_input":"2021-05-22T15:03:04.372149Z","iopub.status.idle":"2021-05-22T15:03:04.524702Z","shell.execute_reply.started":"2021-05-22T15:03:04.372059Z","shell.execute_reply":"2021-05-22T15:03:04.523527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sample *id c12129c31* has a target of *-0.340* and a standard_error of *0.464*. An example calculation which approximates this result is:","metadata":{}},{"cell_type":"code","source":"from scipy.stats import norm\n\n# Generate random scores from 100 raters\nscores = norm.rvs(loc=train['target'][0], scale=train['standard_error'][0], size=100, random_state=42)\nprint(\"\"\"Mean: {:.3f}\nStd: {:.3f}\nRange: {:.3f} to {:.3f}\"\"\".format(scores.mean(), scores.std(), scores.min(), scores.max()))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:07:10.500328Z","iopub.execute_input":"2021-05-22T16:07:10.500716Z","iopub.status.idle":"2021-05-22T16:07:10.509107Z","shell.execute_reply.started":"2021-05-22T16:07:10.500685Z","shell.execute_reply":"2021-05-22T16:07:10.508081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While not representative of the actual observerations, the calculation is illustrative of the range of views of difficulty that a human labeler might apply from -1.556 to 0.519. This suggests there is substantial variation in the human labels.\n\n### Applying standard_error to the dataset\n\nThe standard_error may be used to simulate the views of a human labeller. I am assuming that the errors follow a normal distribution:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Split for multiple datasets and simulations\nk_folds = KFold(5, shuffle=True)\n\npredictions = [] # store the predictions\nfor i, (train_ix, _) in enumerate(k_folds.split(train), 1):\n    target, error = train.loc[train_ix, ['target','standard_error']].to_numpy().T # get the targets\n    normal_sample = norm.rvs(size=len(train_ix), random_state=i) # normal sample from standard normal\n    prediction = target + error * normal_sample # pro-rata to the scale implied by the train set\n    rmse = mean_squared_error(target, prediction)**.5 # how does this compare to the actuals\n    predictions.append(rmse)\n    print('Fold {} {:.3f}'.format(i, rmse))\nprint('Average RMSE {:.3f}'.format(sum(predictions)/len(predictions)))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T15:56:03.312092Z","iopub.execute_input":"2021-05-22T15:56:03.312605Z","iopub.status.idle":"2021-05-22T15:56:03.334912Z","shell.execute_reply.started":"2021-05-22T15:56:03.31256Z","shell.execute_reply":"2021-05-22T15:56:03.333911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This suggests that an average human labeller would have an RMSE of 0.490. The leading models on the leaderboard are pushing towards an RMSE of 0.460. The 0.490s on the other hand are around the 375 rank or top 1/3 of competitors. For me this raises three questions:\n- What is the distribution and nature of the errors?\n- Does a RMSE of substantially below 0.490 indicate overfitting?\n- What does it mean for a model to be a *better* labeller of difficulty than a human labeller?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}