{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebooks shows how to pretrain any language model easily\n\n\n1. Pretrain Roberta Model: this notebook\n2. Finetune Roberta Model: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune<br/>\n   Finetune Roberta Model [TPU]: https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune-tpu\n3. Inference Notebook: https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference\n4. Roberta + SVM: https://www.kaggle.com/maunish/clrp-roberta-svm\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom transformers import (AutoModel,AutoModelForMaskedLM, \n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-27T09:05:25.16493Z","iopub.execute_input":"2021-06-27T09:05:25.16526Z","iopub.status.idle":"2021-06-27T09:05:34.068777Z","shell.execute_reply.started":"2021-06-27T09:05:25.165162Z","shell.execute_reply":"2021-06-27T09:05:34.067625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv', index_col='id')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\naux_data = pd.read_csv('../input/clrauxdata/aux_data.csv', index_col='id', converters={'aux_text': eval})\n\naux_data['excerpt'] = aux_data.aux_text.apply('\\n'.join)\n\ntrain_data.loc[aux_data.index] = aux_data\n\ndata = pd.concat([train_data,test_data])\ntext  = '\\n'.join(data.excerpt.tolist())\n\nwith open('text.txt','w') as f:\n    f.write(text)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T09:05:34.07021Z","iopub.execute_input":"2021-06-27T09:05:34.070539Z","iopub.status.idle":"2021-06-27T09:05:34.330815Z","shell.execute_reply.started":"2021-06-27T09:05:34.0705Z","shell.execute_reply":"2021-06-27T09:05:34.330009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'roberta-base'\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained('./clrp_roberta_base');","metadata":{"execution":{"iopub.status.busy":"2021-06-27T09:05:34.332676Z","iopub.execute_input":"2021-06-27T09:05:34.333029Z","iopub.status.idle":"2021-06-27T09:06:09.454465Z","shell.execute_reply.started":"2021-06-27T09:05:34.332994Z","shell.execute_reply":"2021-06-27T09:06:09.453585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\",\n    block_size=256)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\", \n    block_size=256)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./clrp_roberta_base_chk\", \n    overwrite_output_dir=True,\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=8,\n    evaluation_strategy= 'steps',\n    save_total_limit=2,\n    eval_steps=150,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end =True,\n    prediction_loss_only=True,\n    report_to = \"none\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T09:06:09.455902Z","iopub.execute_input":"2021-06-27T09:06:09.456264Z","iopub.status.idle":"2021-06-27T09:06:24.43156Z","shell.execute_reply.started":"2021-06-27T09:06:09.456222Z","shell.execute_reply":"2021-06-27T09:06:24.430677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model(f'./clrp_roberta_base')","metadata":{"execution":{"iopub.status.busy":"2021-06-27T09:06:24.432856Z","iopub.execute_input":"2021-06-27T09:06:24.433284Z","iopub.status.idle":"2021-06-27T09:48:40.08832Z","shell.execute_reply.started":"2021-06-27T09:06:24.43324Z","shell.execute_reply":"2021-06-27T09:48:40.087437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Done')","metadata":{"execution":{"iopub.status.busy":"2021-06-27T09:48:40.09001Z","iopub.execute_input":"2021-06-27T09:48:40.090629Z","iopub.status.idle":"2021-06-27T09:48:40.096597Z","shell.execute_reply.started":"2021-06-27T09:48:40.090589Z","shell.execute_reply":"2021-06-27T09:48:40.095603Z"},"trusted":true},"execution_count":null,"outputs":[]}]}