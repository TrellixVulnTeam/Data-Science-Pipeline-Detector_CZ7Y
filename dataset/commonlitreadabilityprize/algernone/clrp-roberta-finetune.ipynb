{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.nn.functional import mse_loss\nfrom transformers import AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup, AutoConfig, AdamW\n\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\ng_ = Fore.GREEN\nsr_ = Style.RESET_ALL","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-28T13:17:27.275916Z","iopub.execute_input":"2021-06-28T13:17:27.276264Z","iopub.status.idle":"2021-06-28T13:17:27.287414Z","shell.execute_reply.started":"2021-06-28T13:17:27.276228Z","shell.execute_reply":"2021-06-28T13:17:27.286512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/train-val-split/train.csv')\nval_df = pd.read_csv('../input/train-val-split/val.csv')\n\n# kfold_df = pd.read_csv('../input/train-val-split/kfold_data.csv')\naux_df = pd.read_csv('../input/clrauxdata/aux_data_embed.csv', index_col='index', converters={'aux_text': eval})\n","metadata":{"execution":{"iopub.status.busy":"2021-06-28T13:17:27.289192Z","iopub.execute_input":"2021-06-28T13:17:27.289795Z","iopub.status.idle":"2021-06-28T13:17:27.502994Z","shell.execute_reply.started":"2021-06-28T13:17:27.289756Z","shell.execute_reply":"2021-06-28T13:17:27.502099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    model_name = 'roberta-base'\n    pretrained_model_path = '../input/clrp-roberta-pretrain/clrp_roberta_base'\n    output_hidden_states = True\n    epochs = 3\n    evaluate_interval = 10\n    batch_size = 8\n    device = 'cuda'\n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    wd = 0.01\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=Config.seed)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-28T13:17:27.50516Z","iopub.execute_input":"2021-06-28T13:17:27.5055Z","iopub.status.idle":"2021-06-28T13:17:27.513346Z","shell.execute_reply.started":"2021-06-28T13:17:27.505466Z","shell.execute_reply":"2021-06-28T13:17:27.512536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef convert_examples_to_features(text, tokenizer, max_len):\n\n    tok = tokenizer.encode_plus(\n        text, \n        max_length=max_len, \n        truncation=True,\n        padding='max_length',\n    )\n    return tok\n\n\nclass CLRPDataset(Dataset):\n    def __init__(self, data, tokenizer, aux_data=None, is_test=False):\n        self.data = data\n        if not is_test:\n            self.targets = self.data.target.tolist()\n            self.aux_data = aux_data\n            \n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = Config.max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            \n            if self.aux_data is not None:\n                # if we have augments for the data sample\n                row_index = self.data.id[item]\n                if row_index in self.aux_data.index:\n                    # then we choose one from all the options randomly\n                    excerpt = random.choice(self.aux_data.loc[row_index].aux_text + [self.data.excerpt[item]])\n                else:\n                    excerpt = self.data.excerpt[item]\n            else:\n                excerpt = self.data.excerpt[item]\n            label = self.targets[item]\n#             label = np.random.normal(self.targets[item], self.data.standard_error[item] ** 2)\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.float),\n            }\n        else:\n            excerpt = self.data.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","metadata":{"execution":{"iopub.status.busy":"2021-06-28T13:17:27.515093Z","iopub.execute_input":"2021-06-28T13:17:27.515655Z","iopub.status.idle":"2021-06-28T13:17:27.528804Z","shell.execute_reply.started":"2021-06-28T13:17:27.515618Z","shell.execute_reply":"2021-06-28T13:17:27.528056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(CLRPModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size, self.h_size, 1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.h_size, 1)\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n        x = self.head(transformer_out[0])\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-28T13:17:27.53135Z","iopub.execute_input":"2021-06-28T13:17:27.531695Z","iopub.status.idle":"2021-06-28T13:17:27.543214Z","shell.execute_reply.started":"2021-06-28T13:17:27.531658Z","shell.execute_reply":"2021-06-28T13:17:27.542347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AvgCounter:\n    def __init__(self):\n        self.reset()\n        \n    def update(self, loss, n_samples):\n        self.loss += loss * n_samples\n        self.n_samples += n_samples\n        \n    def avg(self):\n        return self.loss / self.n_samples\n    \n    def reset(self):\n        self.loss = 0\n        self.n_samples = 0\n\nclass LossesRecorder:\n    def __init__(self, suffix=''):\n        self.best_val_loss = float('inf')\n        self.tb = SummaryWriter(filename_suffix=suffix)\n        \n    def update_train_loss(self, loss, step):\n        self.tb.add_scalar(\"train loss\", loss, step)\n        self.tb.flush()\n        \n    def update_val_loss(self, loss, step):\n        self.tb.add_scalar(\"val loss\", loss, step)\n        self.tb.flush()\n    \n    def close(self):\n        tb.close()\n        \ndef make_dataloader(data, tokenizer, aux_data=None, is_train=True):\n    dataset = CLRPDataset(data, tokenizer=tokenizer, aux_data=None)\n    if is_train:\n        sampler = RandomSampler(dataset)\n        shuffle = True\n    else:\n        sampler = SequentialSampler(dataset)\n        shuffle = False\n\n    batch_dataloader = DataLoader(dataset, sampler=sampler, batch_size=Config.batch_size, pin_memory=True, drop_last=False)\n    return batch_dataloader\n                   \n            \nclass Trainer:\n    def __init__(self, train_dl, val_dl, model, optimizer, scheduler, criterion):\n        self.train_dl = train_dl\n        self.val_dl = val_dl\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = Config.device\n        self.batches_per_epoch = len(self.train_dl)\n        self.criterion = criterion\n        \n    def run(self):\n        losses_recorder = LossesRecorder()\n        train_loss_counter = AvgCounter()\n        \n        for epoch in range(Config.epochs):\n            print(f'{r_}Epoch: {epoch+1}/{Config.epochs}{sr_}')\n            for step, batch in enumerate(self.train_dl):\n                train_loss = self.train(batch)\n                \n                train_loss_counter.update(train_loss, len(batch))\n                losses_recorder.update_train_loss(train_loss.item(), epoch*self.batches_per_epoch+step+1)\n\n                if step % Config.evaluate_interval == 0 or ((step + 1) == self.batches_per_epoch):\n                    val_loss = self.evaluate()\n                    \n                    losses_recorder.update_val_loss(val_loss.item(), epoch*self.batches_per_epoch+step+1)\n                    print(f'\\t{epoch+1}#[{step+1}/{self.batches_per_epoch}]: train loss - {train_loss_counter.avg()} | val loss - {val_loss}',)\n                    train_loss_counter.reset()\n\n                    if val_loss < losses_recorder.best_val_loss:\n                        print(f\"\\t\\t{g_}Val loss decreased from {losses_recorder.best_val_loss} to {val_loss}{sr_}\")\n                        losses_recorder.best_val_loss = val_loss.item()\n                        torch.save(self.model, f'best_model.pt')\n\n    def train(self, batch):\n        self.model.train()\n        sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device), \n        self.model.zero_grad() \n        preds = self.model(sent_id, mask)\n        train_loss = self.criterion(preds, labels.unsqueeze(1))\n        train_loss.backward()\n        self.optimizer.step()\n        self.scheduler.step()\n        return train_loss\n\n    def evaluate(self):\n        self.model.eval()\n        val_loss_counter = AvgCounter()\n\n        for step,batch in enumerate(self.val_dl):\n            sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n            with torch.no_grad():\n                preds = self.model(sent_id, mask)\n                loss = self.criterion(preds,labels.unsqueeze(1))\n                val_loss_counter.update(loss, len(labels))\n        return val_loss_counter.avg()\n    \n    \ndef rmse_loss(y_true,y_pred):\n    return torch.sqrt(nn.functional.mse_loss(y_true,y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-06-28T13:17:27.544763Z","iopub.execute_input":"2021-06-28T13:17:27.5452Z","iopub.status.idle":"2021-06-28T13:17:27.568181Z","shell.execute_reply.started":"2021-06-28T13:17:27.545164Z","shell.execute_reply":"2021-06-28T13:17:27.567336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ntokenizer = AutoTokenizer.from_pretrained(Config.model_name)\ntrain_dl = make_dataloader(train_df, tokenizer)\nval_dl = make_dataloader(val_df, tokenizer, is_train=False)\nconfig = AutoConfig.from_pretrained(Config.model_name)\ntransformer = AutoModel.from_pretrained(Config.pretrained_model_path, output_hidden_states=True)  \n\nmodel = CLRPModel(transformer, config)\nmodel = model.to(Config.device)\noptimizer = optim.AdamW(model.parameters(), lr = Config.lr, weight_decay=Config.wd)  \nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=10*len(train_dl)\n)\n\ncriterion = rmse_loss\n","metadata":{"execution":{"iopub.status.busy":"2021-06-28T13:17:27.569523Z","iopub.execute_input":"2021-06-28T13:17:27.569881Z","iopub.status.idle":"2021-06-28T13:17:31.25584Z","shell.execute_reply.started":"2021-06-28T13:17:27.569845Z","shell.execute_reply":"2021-06-28T13:17:31.254839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(train_dl, val_dl, model, optimizer, scheduler, criterion)\ntrainer.run()\n!date '+%A %W %Y %X' > execution_time","metadata":{"execution":{"iopub.status.busy":"2021-06-28T13:17:31.257935Z","iopub.execute_input":"2021-06-28T13:17:31.2583Z","iopub.status.idle":"2021-06-28T13:28:38.981684Z","shell.execute_reply.started":"2021-06-28T13:17:31.258265Z","shell.execute_reply":"2021-06-28T13:28:38.980697Z"},"trusted":true},"execution_count":null,"outputs":[]}]}