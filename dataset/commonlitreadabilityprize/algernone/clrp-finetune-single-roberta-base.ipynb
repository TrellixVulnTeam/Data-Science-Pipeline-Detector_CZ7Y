{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Notebooks sequence;)**\n* Train-val split notebook [here](https://www.kaggle.com/chamecall/train-val-split).<br>\n* Pretrain roberta-base on mlm with the competition data notebook [here](https://www.kaggle.com/chamecall/clrp-pretrain).<br>\n* Finetune pretrained roberta-base on readability task notebook [*CURRENT ONE*].<br>\n* Inference model notebook [here](https://www.kaggle.com/chamecall/clrp-inference).<br>","metadata":{}},{"cell_type":"markdown","source":"# **Credits**<br>\n*The notebooks series was influenced by the following works: *\n* https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n* https://www.kaggle.com/maunish/clrp-pytorch-roberta-finetune\n* https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit\n* https://www.kaggle.com/andretugan/pre-trained-roberta-solution-in-pytorch","metadata":{}},{"cell_type":"code","source":"from IPython.core.magic import register_cell_magic\nimport os\nfrom pathlib import Path\n\n## define custom magic to save most useful classes and use them in inference notebook \n## instead of copying the code every time you have changes in the classes\n@register_cell_magic\ndef write_and_run(line, cell):\n    argz = line.split()\n    file = argz[-1]\n    mode = 'w'\n    if len(argz) == 2 and argz[0] == '-a':\n        mode = 'a'\n    with open(file, mode) as f:\n        f.write(cell)\n    get_ipython().run_cell(cell)\n    \nPath('/kaggle/working/scripts').mkdir(exist_ok=True)\nmodels_dir = Path('/kaggle/working/models')\nmodels_dir.mkdir(exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T06:58:41.897005Z","iopub.execute_input":"2021-07-24T06:58:41.897397Z","iopub.status.idle":"2021-07-24T06:58:41.904839Z","shell.execute_reply.started":"2021-07-24T06:58:41.897355Z","shell.execute_reply":"2021-07-24T06:58:41.90371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%write_and_run scripts/imports.py\n\nimport os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler, Sampler\nfrom torch.nn.functional import mse_loss\nfrom transformers import AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup, AutoConfig, AdamW\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-talk')\n# print(plt.style.available)\nfrom time import time\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nw_ = Fore.WHITE\nbb_ = Back.BLACK\nsr_ = Style.RESET_ALL\n\n","metadata":{"_uuid":"d9a1a99e-de9d-4024-9715-bab69dafd9d6","_cell_guid":"17ca5c94-c7c7-49a0-8822-05b59ceaaa5e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-20T09:49:34.781313Z","iopub.execute_input":"2021-07-20T09:49:34.781848Z","iopub.status.idle":"2021-07-20T09:49:42.430846Z","shell.execute_reply.started":"2021-07-20T09:49:34.781798Z","shell.execute_reply":"2021-07-20T09:49:42.42996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%write_and_run scripts/config.py\n\nclass Config:\n    model_name = 'roberta-base'\n    pretrained_model_path = '../input/clrp-pretrain/clrp_roberta_base'\n    output_hidden_states = True\n    epochs = 3\n    evaluate_interval = 10\n    batch_size = 16\n    device = 'cuda'\n    seed = 42\n    max_len = 256\n    lr = 2e-5\n    wd = 0.01\n    eval_schedule = [(float('inf'), 16), (0.5, 8), (0.49, 4), (0.48, 2), (0.47, 1), (0, 0)]\n\n    ","metadata":{"_uuid":"3d4bc2bb-36f6-4d6a-aa6c-b9ec39f40c45","_cell_guid":"757a446c-b2dc-4eea-91d6-8f8bcd1155c3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-20T09:49:42.434815Z","iopub.execute_input":"2021-07-20T09:49:42.435068Z","iopub.status.idle":"2021-07-20T09:49:42.442834Z","shell.execute_reply.started":"2021-07-20T09:49:42.435043Z","shell.execute_reply":"2021-07-20T09:49:42.442099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=Config.seed)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T09:49:42.444803Z","iopub.execute_input":"2021-07-20T09:49:42.445088Z","iopub.status.idle":"2021-07-20T09:49:42.458172Z","shell.execute_reply.started":"2021-07-20T09:49:42.445056Z","shell.execute_reply":"2021-07-20T09:49:42.457333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = pd.read_csv('../input/k/chamecall/train-val-split/train.csv')\n# val_df = pd.read_csv('../input/k/chamecall/train-val-split/val.csv')\n\nkfold_df = pd.read_csv('../input/k/chamecall/train-val-split/kfold.csv')\n# aux_df = pd.read_csv('../input/clrauxdata/aux_data_embed.csv', index_col='index', converters={'aux_text': eval})","metadata":{"_uuid":"f7aa8d00-0b7b-4704-b00c-45c214001ecb","_cell_guid":"89d2d6a9-c024-4852-a1cc-2892d7e2aab9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-20T09:49:42.459394Z","iopub.execute_input":"2021-07-20T09:49:42.459849Z","iopub.status.idle":"2021-07-20T09:49:42.534078Z","shell.execute_reply.started":"2021-07-20T09:49:42.459814Z","shell.execute_reply":"2021-07-20T09:49:42.533107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%write_and_run scripts/dataset.py\n\nfrom torch.utils.data import Dataset\nimport torch\n\ndef convert_examples_to_features(text, tokenizer, max_len):\n\n    tok = tokenizer.encode_plus(\n        text, \n        max_length=max_len, \n        truncation=True,\n        padding='max_length',\n    )\n    return tok\n\n\nclass CLRPDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.tolist()\n        if not is_test:\n            self.targets = self.data.target.tolist()\n            \n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt = self.excerpts[item]\n            label = self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.float),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","metadata":{"_uuid":"f34eca01-c56c-4b8f-8c8c-5188ac148878","_cell_guid":"740b0f57-cad8-4758-995b-c2119dc3eac3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-20T09:49:42.536106Z","iopub.execute_input":"2021-07-20T09:49:42.536377Z","iopub.status.idle":"2021-07-20T09:49:42.549501Z","shell.execute_reply.started":"2021-07-20T09:49:42.536352Z","shell.execute_reply":"2021-07-20T09:49:42.548601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%write_and_run scripts/model.py\n\nimport torch\nimport torch.nn as nn\n\n\nclass AttentionHead(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(CLRPModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size)\n        self.linear = nn.Linear(self.h_size, 1)\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n        x = self.head(transformer_out.last_hidden_state)\n        x = self.linear(x)\n        return x\n\n    \n","metadata":{"_uuid":"024b10c3-9bdc-48a6-be51-cd8f8a064738","_cell_guid":"3fff34f9-07fe-4b91-a284-a06ce99fd43f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-20T09:49:42.553407Z","iopub.execute_input":"2021-07-20T09:49:42.553647Z","iopub.status.idle":"2021-07-20T09:49:42.562199Z","shell.execute_reply.started":"2021-07-20T09:49:42.553625Z","shell.execute_reply":"2021-07-20T09:49:42.561356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:197]    \n    attention_parameters = named_parameters[199:203]\n    regressor_parameters = named_parameters[203:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = Config.lr\n\n        if layer_num >= 69:        \n            lr = Config.lr * 2.5\n\n        if layer_num >= 133:\n            lr = Config.lr * 5\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return optim.AdamW(parameters)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T09:49:42.563831Z","iopub.execute_input":"2021-07-20T09:49:42.564213Z","iopub.status.idle":"2021-07-20T09:49:42.573183Z","shell.execute_reply.started":"2021-07-20T09:49:42.564178Z","shell.execute_reply":"2021-07-20T09:49:42.572293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass AvgCounter:\n    def __init__(self):\n        self.reset()\n        \n    def update(self, loss, n_samples):\n        self.loss += loss * n_samples\n        self.n_samples += n_samples\n        \n    def avg(self):\n        return self.loss / self.n_samples\n    \n    def reset(self):\n        self.loss = 0\n        self.n_samples = 0\n\nclass EvaluationScheduler:\n    def __init__(self, evaluation_schedule, penalize_factor=1, max_penalty=8):\n        self.evaluation_schedule = evaluation_schedule\n        self.evaluation_interval = self.evaluation_schedule[0][1]\n        self.last_evaluation_step = 0\n        self.prev_loss = float('inf')\n        self.penalize_factor = penalize_factor\n        self.penalty = 0\n        self.prev_interval = -1\n        self.max_penalty = max_penalty\n\n    def step(self, step):\n        # should we to make evaluation right now\n        if step >= self.last_evaluation_step + self.evaluation_interval:\n            self.last_evaluation_step = step\n            return True\n        else:\n            return False\n        \n            \n    def update_evaluation_interval(self, last_loss):\n        # set up evaluation_interval depending on loss value\n        cur_interval = -1\n        for i, (loss, interval) in enumerate(self.evaluation_schedule[:-1]):\n            if self.evaluation_schedule[i+1][0] < last_loss < loss:\n                self.evaluation_interval = interval\n                cur_interval = i\n                break\n#         if last_loss > self.prev_loss and self.prev_interval == cur_interval:\n#             self.penalty += self.penalize_factor\n#             self.penalty = min(self.penalty, self.max_penalty)\n#             self.evaluation_interval += self.penalty\n#         else:\n#             self.penalty = 0\n            \n        self.prev_loss = last_loss\n        self.prev_interval = cur_interval\n        \n          \n        \ndef make_dataloader(data, tokenizer, is_train=True):\n    dataset = CLRPDataset(data, tokenizer=tokenizer, max_len=Config.max_len)\n    if is_train:\n        sampler = RandomSampler(dataset)\n    else:\n        sampler = SequentialSampler(dataset)\n\n    batch_dataloader = DataLoader(dataset, sampler=sampler, batch_size=Config.batch_size, pin_memory=True)\n    return batch_dataloader\n                   \n            \nclass Trainer:\n    def __init__(self, train_dl, val_dl, model, optimizer, scheduler, criterion, model_num):\n        self.train_dl = train_dl\n        self.val_dl = val_dl\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = Config.device\n        self.batches_per_epoch = len(self.train_dl)\n        self.criterion = criterion\n        self.model_num = model_num\n                \n    def run(self):\n        record_info = {\n            'train_loss': [],\n            'val_loss': [],\n        }\n        \n        best_val_loss = float('inf')\n        evaluation_scheduler = EvaluationScheduler(Config.eval_schedule)\n        train_loss_counter = AvgCounter()\n        step = 0\n        \n        for epoch in range(Config.epochs):\n            \n            print(f'{r_}Epoch: {epoch+1}/{Config.epochs}{sr_}')\n            start_epoch_time = time()\n            \n            for batch_num, batch in enumerate(self.train_dl):\n                train_loss = self.train(batch)\n#                 print(f'{epoch+1}#[{step+1}/{len(self.train_dl)}]: train loss - {train_loss.item()}')\n\n                train_loss_counter.update(train_loss, len(batch))\n                record_info['train_loss'].append((step, train_loss.item()))\n\n                if evaluation_scheduler.step(step):\n                    val_loss = self.evaluate()\n                    \n                    record_info['val_loss'].append((step, val_loss.item()))        \n                    print(f'\\t\\t{epoch+1}#[{batch_num+1}/{self.batches_per_epoch}]: train loss - {train_loss_counter.avg()} | val loss - {val_loss}',)\n                    train_loss_counter.reset()\n\n                    if val_loss < best_val_loss:\n                        best_val_loss = val_loss\n                        print(f\"\\t\\t{g_}Val loss decreased from {best_val_loss} to {val_loss}{sr_}\")\n                        torch.save(self.model, models_dir / f'best_model_{self.model_num}.pt')\n                        \n                    evaluation_scheduler.update_evaluation_interval(val_loss.item())\n                        \n\n                step += 1\n            end_epoch_time = time()\n            print(f'{bb_}{y_}The epoch took {end_epoch_time - start_epoch_time} sec..{sr_}')\n\n        return record_info, best_val_loss\n            \n\n    def train(self, batch):\n        self.model.train()\n        sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device), \n        self.model.zero_grad() \n        preds = self.model(sent_id, mask)\n        train_loss = self.criterion(preds, labels.unsqueeze(1))\n        \n        train_loss.backward()\n        self.optimizer.step()\n        self.scheduler.step()\n        return torch.sqrt(train_loss)\n\n    def evaluate(self):\n        self.model.eval()\n        val_loss_counter = AvgCounter()\n\n        for step,batch in enumerate(self.val_dl):\n            sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n            with torch.no_grad():\n                preds = self.model(sent_id, mask)\n                loss = self.criterion(preds,labels.unsqueeze(1))\n                val_loss_counter.update(torch.sqrt(loss), len(labels))\n        return val_loss_counter.avg()\n    \n    \ndef mse_loss(y_true,y_pred):\n    return nn.functional.mse_loss(y_true,y_pred)","metadata":{"_uuid":"0a678160-5a09-49b1-8f51-fce52d18992e","_cell_guid":"7428183c-f79b-452c-af9f-1b76b4684032","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-20T09:49:42.574742Z","iopub.execute_input":"2021-07-20T09:49:42.575143Z","iopub.status.idle":"2021-07-20T09:49:42.604383Z","shell.execute_reply.started":"2021-07-20T09:49:42.575111Z","shell.execute_reply":"2021-07-20T09:49:42.603407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_scores = []\n\nfor model_num in range(5): \n    print(f'{bb_}{w_}  Model#{model_num+1}  {sr_}')\n\n    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n    config = AutoConfig.from_pretrained(Config.pretrained_model_path)\n    config.update({\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n            }) \n\n    train_dl = make_dataloader(kfold_df[kfold_df.fold!=model_num], tokenizer)\n    val_dl = make_dataloader(kfold_df[kfold_df.fold==model_num], tokenizer, is_train=False)\n\n#     train_dl = make_dataloader(train_df, tokenizer)\n#     val_dl = make_dataloader(val_df, tokenizer, is_train=False)\n\n    transformer = AutoModel.from_pretrained(Config.pretrained_model_path, config=config)  \n    model = CLRPModel(transformer, config)\n    model = model.to(Config.device)\n    optimizer = create_optimizer(model)\n    scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_training_steps=Config.epochs * len(train_dl),\n            num_warmup_steps=50)  \n\n    criterion = mse_loss\n\n    trainer = Trainer(train_dl, val_dl, model, optimizer, scheduler, criterion, model_num)\n    record_info, best_val_loss = trainer.run()\n    best_scores.append(best_val_loss)    \n    \n    steps, train_losses = list(zip(*record_info['train_loss']))\n    plt.plot(steps, train_losses, label='train_loss')\n    steps, val_losses = list(zip(*record_info['val_loss']))\n    plt.plot(steps, val_losses, label='val_loss')\n    plt.legend()\n    plt.show()\n    \nprint('Best val losses:', best_scores)\nprint('Avg val loss:', np.array(best_scores).mean())\n!date '+%A %W %Y %X' > execution_time","metadata":{"_uuid":"5e857d09-968b-4593-a035-c066e1690120","_cell_guid":"24f14f18-7620-41dc-be98-522e589a14d7","jupyter":{"outputs_hidden":false},"collapsed":false,"execution":{"iopub.status.busy":"2021-07-20T09:49:42.605744Z","iopub.execute_input":"2021-07-20T09:49:42.606206Z","iopub.status.idle":"2021-07-20T10:47:06.466119Z","shell.execute_reply.started":"2021-07-20T09:49:42.606165Z","shell.execute_reply":"2021-07-20T10:47:06.463351Z"},"trusted":true},"execution_count":null,"outputs":[]}]}