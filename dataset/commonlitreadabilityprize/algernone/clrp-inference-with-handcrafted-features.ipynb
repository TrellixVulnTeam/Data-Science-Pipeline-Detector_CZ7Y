{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ../input/packages-for-creating-text-features/*.whl\n!pip install ../input/packages-for-creating-text-features/ReadabilityCalculator-0.2.37/ReadabilityCalculator-0.2.37\n\n\n\nfrom textblob.tokenizers import SentenceTokenizer, WordTokenizer\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport os \nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\nimport textstat\nplt.style.use('seaborn-talk')\nfrom readcalc import readcalc\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\nimport spacy\nsp = spacy.load('en_core_web_sm')\n\ndef pos_to_id(pos_name):\n    return sp.vocab[pos_name].orth\n\ncontent_poss = ['ADJ', 'NOUN', 'VERB', 'ADV']\n\ndef count_poss(text, poss_names):\n    text = sp(text)\n    poss_ids = [pos_to_id(pos_name) for pos_name in poss_names]\n    pos_freq_dict = text.count_by(spacy.attrs.POS)\n    poss_sum = sum([pos_freq_dict.get(pos_id, 0) for pos_id in poss_ids])\n    return poss_sum\n\n\ncount_poss('my name is', ['PRON', 'NOUN'])\n\n# !pip download textstat ReadabilityCalculator \n# !pip install *.whl\n\nsent_tokenizer = SentenceTokenizer()\nword_tokenizer = WordTokenizer()\n\nwith open('../input/clrauxdata/dale-chall-3000-words.txt') as f:\n    words = f.readlines()[0].split()\n    \ncommon_words = dict(zip(words, [True] * len(words)))\n# df.sent_cnt.plot(kind='kde')\n\nfeats_to_drop = ['sents_n', 'words_n', 'long_words_n', 'difficult_words_n', 'content_words_n', 'prons_n', 'chars_n', 'syllables_n']\ndoc_feats = ['chars_per_word', 'chars_per_sent', 'syllables_per_word',\n       'syllables_per_sent', 'words_per_sent', 'long_words_doc_ratio',\n       'difficult_words_doc_ratio', 'prons_doc_ratio', 'flesch_reading_ease',\n       'flesch_kincaid_grade', 'ari', 'cli', 'gunning_fog', 'lix', 'rix',\n       'smog', 'dcrs', 'lexical_diversity', 'content_diversity', 'lwf']\n\ndef create_handcrafted_features(df):\n    df['sents_n'] = df.excerpt.apply(textstat.sentence_count)\n    df['words_n'] = df.excerpt.apply(textstat.lexicon_count)\n    df['long_words_n'] = df.excerpt.apply(lambda t: readcalc.ReadCalc(t).get_words_longer_than_X(6))\n    df['difficult_words_n'] = df.excerpt.apply(lambda t: sum([bool(common_words.get(word)) for word in word_tokenizer.tokenize(t, include_punc=False)]))\n    df['content_words_n'] = df.excerpt.apply(lambda t: count_poss(t, content_poss))\n    df['prons_n'] = df.excerpt.apply(lambda t: count_poss(t, ['PRON']))\n    df['chars_n'] = df.excerpt.str.len()\n    df['syllables_n'] = df.excerpt.apply(textstat.syllable_count)\n    print('\\tstage 1 finished..')\n\n    df['chars_per_word'] = df.chars_n / df.words_n\n    df['chars_per_sent'] = df.chars_n / df.sents_n\n    df['syllables_per_word'] = df.syllables_n / df.words_n\n    df['syllables_per_sent'] = df.syllables_n / df.sents_n\n\n    df['words_per_sent'] = df.words_n / df.sents_n\n    df['long_words_doc_ratio'] = df.long_words_n / df.words_n\n    df['difficult_words_doc_ratio'] = df.difficult_words_n / df.words_n\n    df['prons_doc_ratio'] = df.prons_n / df.words_n\n\n    print('\\tstage 2 finished..')\n\n    df['flesch_reading_ease'] = df.excerpt.apply(textstat.flesch_reading_ease)\n    df['flesch_kincaid_grade'] = df.excerpt.apply(textstat.flesch_kincaid_grade)\n    df['ari'] = df.excerpt.apply(textstat.automated_readability_index)\n    df['cli'] = df.excerpt.apply(textstat.coleman_liau_index)\n    df['gunning_fog'] = df.excerpt.apply(textstat.gunning_fog)\n\n    df['lix'] = df.excerpt.apply(lambda t: readcalc.ReadCalc(t).get_lix_index())\n    df['rix'] = df.long_words_n / df.sents_n\n    df['smog'] = df.excerpt.apply(lambda t: readcalc.ReadCalc(t).get_smog_index())\n    df['dcrs'] = df.excerpt.apply(textstat.dale_chall_readability_score)\n\n    df['lexical_diversity'] = len(set(df.words_n)) / df.words_n\n    df['content_diversity'] = df.content_words_n / df.words_n\n    df['lwf'] = df.excerpt.apply(textstat.linsear_write_formula)\n\n    print('\\tstage 3 finished..')\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-07-24T11:29:35.797446Z","iopub.execute_input":"2021-07-24T11:29:35.797761Z","iopub.status.idle":"2021-07-24T11:30:51.897124Z","shell.execute_reply.started":"2021-07-24T11:29:35.797688Z","shell.execute_reply":"2021-07-24T11:30:51.89627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nfrom pathlib import Path\nin_folder_path = Path('../input/clrp-distil-roberta')\nscripts_dir = Path(in_folder_path / 'scripts')","metadata":{"_uuid":"ef575956-b04d-49d6-a5ec-c4ede72227f6","_cell_guid":"58aa9358-0979-4bb5-a6ce-4292869a720c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-24T11:30:51.898574Z","iopub.execute_input":"2021-07-24T11:30:51.898936Z","iopub.status.idle":"2021-07-24T11:30:51.906464Z","shell.execute_reply.started":"2021-07-24T11:30:51.898896Z","shell.execute_reply":"2021-07-24T11:30:51.905677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nos.chdir(scripts_dir)\nexec(Path(\"imports.py\").read_text())\nexec(Path(\"config.py\").read_text())\nexec(Path(\"dataset.py\").read_text())\nexec(Path(\"model.py\").read_text())\nos.chdir('/kaggle/working')","metadata":{"_uuid":"fdc9f439-b48a-4525-810b-718f44f61fcf","_cell_guid":"729f8813-aaf6-49f9-a4c8-abefb0f32c31","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-24T11:30:51.908567Z","iopub.execute_input":"2021-07-24T11:30:51.909026Z","iopub.status.idle":"2021-07-24T11:30:57.59261Z","shell.execute_reply.started":"2021-07-24T11:30:51.908991Z","shell.execute_reply":"2021-07-24T11:30:57.591676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import joblib \n\ntest_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n\ntokenizer = torch.load('../input/tokenizers/roberta-tokenizer.pt')\nmodels_folder_path = Path(in_folder_path / 'models')\nmodels_preds = []\nn_models = 5\n\nfor model_num in range(n_models):\n    scaler = joblib.load(f'../input/clrauxdata/std_scaler{model_num}.pkl')\n    new_df = create_handcrafted_features(test_df)\n    new_df[doc_feats] = scaler.transform(new_df[doc_feats])\n\n    print(f'Inference#{model_num+1}/{n_models}')\n    test_ds = CLRPDataset(data=new_df, tokenizer=tokenizer, max_len=Config.max_len, is_test=True)\n    test_sampler = SequentialSampler(test_ds)\n    test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=Config.batch_size)\n    model = torch.load(models_folder_path / f'best_model_{model_num}.pt').to(Config.device)\n\n    all_preds = []\n    model.eval()\n\n    for step,batch in enumerate(test_dataloader):\n        sent_id, mask, doc_features = batch['input_ids'].to(Config.device), batch['attention_mask'].to(Config.device), batch['doc_features'].to(Config.device)\n        with torch.no_grad():\n            preds = model(sent_id, mask, doc_features)\n            all_preds += preds.flatten().cpu().tolist()\n    \n    models_preds.append(all_preds)","metadata":{"_uuid":"3bcc8398-d3ba-4caa-a24b-cadc4eeadf94","_cell_guid":"0960a7a6-aae2-466b-8d31-0918a65eb559","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-24T11:30:57.594307Z","iopub.execute_input":"2021-07-24T11:30:57.594671Z","iopub.status.idle":"2021-07-24T11:31:21.599908Z","shell.execute_reply.started":"2021-07-24T11:30:57.594618Z","shell.execute_reply":"2021-07-24T11:31:21.598993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_preds = np.array(models_preds)\nprint(models_preds.shape)\nprint(models_preds)\nall_preds = models_preds.mean(axis=0)\nprint(all_preds.shape)\nresult_df = pd.DataFrame(\n    {\n        'id': test_df.id,\n        'target': all_preds\n    })\n\n\nresult_df.to_csv('submission.csv', index=False)\nresult_df.head(10)","metadata":{"_uuid":"e496e1ec-baf8-4e23-a7bf-63fc4e7c5d1a","_cell_guid":"ebdd4e76-65d4-45d0-aa8f-0c7705198e57","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-24T11:31:21.601272Z","iopub.execute_input":"2021-07-24T11:31:21.601619Z","iopub.status.idle":"2021-07-24T11:31:21.755218Z","shell.execute_reply.started":"2021-07-24T11:31:21.601585Z","shell.execute_reply":"2021-07-24T11:31:21.754188Z"},"trusted":true},"execution_count":null,"outputs":[]}]}