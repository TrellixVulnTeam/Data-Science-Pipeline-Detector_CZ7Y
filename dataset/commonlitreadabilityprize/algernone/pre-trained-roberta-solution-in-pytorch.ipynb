{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\nThis is kernel is almost the same as [Lightweight Roberta solution in PyTorch](https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch), but instead of \"roberta-base\", it starts from [Maunish's pre-trained model](https://www.kaggle.com/maunish/clrp-roberta-base).\n\nAcknowledgments: some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish).","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch import optim\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport gc\nfrom torch.utils.data import RandomSampler, SequentialSampler, Sampler\n\ngc.enable()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-06T09:44:53.02795Z","iopub.execute_input":"2021-07-06T09:44:53.028426Z","iopub.status.idle":"2021-07-06T09:44:53.036771Z","shell.execute_reply.started":"2021-07-06T09:44:53.02835Z","shell.execute_reply":"2021-07-06T09:44:53.035091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 256\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\nTOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSEED = 42","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:44:53.03869Z","iopub.execute_input":"2021-07-06T09:44:53.039087Z","iopub.status.idle":"2021-07-06T09:44:53.049976Z","shell.execute_reply.started":"2021-07-06T09:44:53.039046Z","shell.execute_reply":"2021-07-06T09:44:53.049079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True\n    \nset_random_seed(SEED)    \n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:44:53.051464Z","iopub.execute_input":"2021-07-06T09:44:53.051841Z","iopub.status.idle":"2021-07-06T09:44:53.06011Z","shell.execute_reply.started":"2021-07-06T09:44:53.051813Z","shell.execute_reply":"2021-07-06T09:44:53.058965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n\ntest_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")\nfolds_df = pd.read_csv('../input/train-val-split/kfold.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:44:53.062061Z","iopub.execute_input":"2021-07-06T09:44:53.062557Z","iopub.status.idle":"2021-07-06T09:44:53.109066Z","shell.execute_reply.started":"2021-07-06T09:44:53.062516Z","shell.execute_reply":"2021-07-06T09:44:53.108249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:44:53.111912Z","iopub.execute_input":"2021-07-06T09:44:53.112163Z","iopub.status.idle":"2021-07-06T09:44:53.278552Z","shell.execute_reply.started":"2021-07-06T09:44:53.112137Z","shell.execute_reply":"2021-07-06T09:44:53.277573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"def convert_examples_to_features(text, tokenizer, max_len):\n    tok = tokenizer.encode_plus(\n        text, \n        max_length=max_len, \n        truncation=True,\n        padding='max_length',\n    )\n    return tok\n\nclass CLRPDataset(Dataset):\n    def __init__(self, data, tokenizer, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.tolist()\n        if not is_test:\n            self.targets = self.data.target.tolist()\n            \n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n\n            excerpt = self.excerpts[item]\n            label = self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.float),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:44:53.280428Z","iopub.execute_input":"2021-07-06T09:44:53.280993Z","iopub.status.idle":"2021-07-06T09:44:53.292045Z","shell.execute_reply.started":"2021-07-06T09:44:53.28095Z","shell.execute_reply":"2021-07-06T09:44:53.291087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\nThe model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm).","metadata":{}},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(768, 512)\n        self.V = nn.Linear(512, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(CLRPModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size)\n        self.linear = nn.Linear(self.h_size, 1)\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n        x = self.head(transformer_out.last_hidden_state)\n        x = self.linear(x)\n        return x\n\n    \ndef create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:197]    \n    attention_parameters = named_parameters[199:203]\n    regressor_parameters = named_parameters[203:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = 2e-5\n\n        if layer_num >= 69:        \n            lr = 5e-5\n\n        if layer_num >= 133:\n            lr = 1e-4\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return optim.AdamW(parameters)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:44:53.294179Z","iopub.execute_input":"2021-07-06T09:44:53.294555Z","iopub.status.idle":"2021-07-06T09:44:53.308136Z","shell.execute_reply.started":"2021-07-06T09:44:53.294517Z","shell.execute_reply":"2021-07-06T09:44:53.30703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_mse(model, data_loader):\n    model.eval()            \n    mse_sum = 0\n\n    with torch.no_grad():\n        for batch_num, batch in enumerate(data_loader):\n            input_ids, attention_mask, target = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE), batch['label'].to(DEVICE)\n\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)                        \n            target = target.to(DEVICE)           \n            \n            pred = model(input_ids, attention_mask)                       \n\n            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n                \n\n    return mse_sum / len(data_loader.dataset)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:44:53.310078Z","iopub.execute_input":"2021-07-06T09:44:53.310504Z","iopub.status.idle":"2021-07-06T09:44:53.320621Z","shell.execute_reply.started":"2021-07-06T09:44:53.310446Z","shell.execute_reply":"2021-07-06T09:44:53.319863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, batch in enumerate(data_loader):\n            input_ids, attention_mask = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE)\n\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:44:53.322028Z","iopub.execute_input":"2021-07-06T09:44:53.322462Z","iopub.status.idle":"2021-07-06T09:44:53.333622Z","shell.execute_reply.started":"2021-07-06T09:44:53.322427Z","shell.execute_reply":"2021-07-06T09:44:53.332724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, model_path, train_loader, val_loader,\n          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n    best_val_rmse = None\n    best_epoch = 0\n    step = 0\n    last_eval_step = 0\n    eval_period = EVAL_SCHEDULE[0][1]    \n\n    start = time.time()\n    losses_info = {\n        'train_loss': [],\n        'val_loss': [],\n    }\n    for epoch in range(num_epochs):                           \n        val_rmse = None         \n\n        for batch_num, batch in enumerate(train_loader):\n            input_ids, attention_mask, target = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE), batch['label'].to(DEVICE)\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)            \n            target = target.to(DEVICE)                        \n\n            optimizer.zero_grad()\n            \n            model.train()\n\n            pred = model(input_ids, attention_mask)\n                                                        \n            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n            print(f'{epoch+1}#[{step+1}/{len(train_loader)}]: train loss - {mse}')\n\n            mse.backward()\n            losses_info['train_loss'].append((step, math.sqrt(mse)))\n            \n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            \n            if step >= last_eval_step + eval_period:\n                # Evaluate the model on val_loader.\n                elapsed_seconds = time.time() - start\n                num_steps = step - last_eval_step\n                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n                last_eval_step = step\n                \n                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n                losses_info['val_loss'].append((step, val_rmse))\n\n                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n                      f\"val_rmse: {val_rmse:0.4}\")\n\n                for rmse, period in EVAL_SCHEDULE:\n                    if val_rmse >= rmse:\n                        eval_period = period\n                        break                               \n                \n                if not best_val_rmse or val_rmse < best_val_rmse:                    \n                    best_val_rmse = val_rmse\n                    best_epoch = epoch\n#                     torch.save(model.state_dict(), model_path)\n                    torch.save(model, model_path)\n                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n                else:       \n                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n                          f\"(from epoch {best_epoch})\")                                    \n                    \n                start = time.time()\n                                            \n            step += 1\n                        \n    \n    return best_val_rmse, losses_info","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:44:53.335273Z","iopub.execute_input":"2021-07-06T09:44:53.3361Z","iopub.status.idle":"2021-07-06T09:44:53.350496Z","shell.execute_reply.started":"2021-07-06T09:44:53.336073Z","shell.execute_reply":"2021-07-06T09:44:53.349628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\n\nlist_val_rmse = []\n\nfor fold in range(5):\n    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n    model_path = f\"model_{fold + 1}.pth\"\n        \n#     set_random_seed(SEED + fold)\n    train_dataset = CLRPDataset(folds_df[folds_df.fold!=fold], tokenizer=tokenizer)    \n    val_dataset = CLRPDataset(folds_df[folds_df.fold==fold], tokenizer=tokenizer) \n  \n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              drop_last=True, shuffle=True)    \n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                            drop_last=False, shuffle=False)    \n        \n#     set_random_seed(SEED + fold)    \n    \n    config = AutoConfig.from_pretrained(ROBERTA_PATH)\n    config.update({\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n            }) \n\n    transformer = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n    \n    model = CLRPModel(transformer, config).to(DEVICE)\n    \n    optimizer = create_optimizer(model)                        \n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_training_steps=NUM_EPOCHS * len(train_loader),\n        num_warmup_steps=50)    \n    \n    val_rmse, losses_info = train(model, model_path, train_loader,\n                               val_loader, optimizer, scheduler=scheduler)\n    list_val_rmse.append(val_rmse)\n\n    del model\n    gc.collect()\n    \n    \n    steps, train_losses = list(zip(*losses_info['train_loss']))\n    plt.plot(steps, train_losses, label='train_loss')\n    steps, val_losses = list(zip(*losses_info['val_loss']))\n    plt.plot(steps, val_losses, label='val_loss')\n    plt.legend()\n    plt.show()\n    \n    print(\"\\nPerformance estimates:\")\n    print(list_val_rmse)\n    print(\"Mean:\", np.array(list_val_rmse).mean())\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:44:53.351841Z","iopub.execute_input":"2021-07-06T09:44:53.352205Z","iopub.status.idle":"2021-07-06T09:45:43.534609Z","shell.execute_reply.started":"2021-07-06T09:44:53.352169Z","shell.execute_reply":"2021-07-06T09:45:43.531785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"all_predictions = np.zeros((5, len(test_df)))\n\ntest_dataset = CLRPDataset(test_df, tokenizer, is_test=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\nfor index in range(5):            \n    model_path = f\"../input/pre-trained-roberta-solution-in-pytorch/model_{index+1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n#     transformer = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n     \n#     model = CLRPModel(transformer, config)\n#     model.load_state_dict(torch.load(model_path))    \n    model = torch.load(model_path)\n    model.to(DEVICE)\n#     torch.save(model, f'model_full_{index}.pth')\n    \n    all_predictions[index] = predict(model, test_loader)\n    torch.save(model, f'model_{index+1}.pth')\n    del model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:45:43.53593Z","iopub.status.idle":"2021-07-06T09:45:43.536735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_predictions)\npredictions = all_predictions.mean(axis=0)\nsubmission_df.target = predictions\nprint(submission_df)\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:45:43.537924Z","iopub.status.idle":"2021-07-06T09:45:43.538537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folds_df[folds_df.fold==4].head()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T09:45:43.539697Z","iopub.status.idle":"2021-07-06T09:45:43.540297Z"},"trusted":true},"execution_count":null,"outputs":[]}]}