{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip download textstat\n!pip install textstat\n\n!pip install spacy\n!python3 -m spacy download en_core_web_sm\n!pip install textblob","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:55:02.750592Z","iopub.execute_input":"2021-06-09T13:55:02.751079Z","iopub.status.idle":"2021-06-09T13:55:37.347683Z","shell.execute_reply.started":"2021-06-09T13:55:02.751043Z","shell.execute_reply":"2021-06-09T13:55:37.346215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-09T13:55:37.352632Z","iopub.execute_input":"2021-06-09T13:55:37.353007Z","iopub.status.idle":"2021-06-09T13:55:37.361513Z","shell.execute_reply.started":"2021-06-09T13:55:37.35297Z","shell.execute_reply":"2021-06-09T13:55:37.3603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport textstat\nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import permutation_importance\n\n\nclass regressor_stratified:\n    def __init__(self,n_splits=1,group_count=10,random_state=42,strategy='quantile',val_size=0.2):\n        self.group_count=group_count\n        self.strategy=strategy\n        self.cvkwargs=dict(n_splits=n_splits,test_size=val_size,random_state=random_state)\n        self.cv=StratifiedShuffleSplit(**self.cvkwargs)\n        self.discretizer=KBinsDiscretizer(n_bins=self.group_count,encode='ordinal',strategy=self.strategy)  \n            \n    def split(self,X,y,groups=None):\n        kgroups=self.discretizer.fit_transform(y[:,None])[:,0]\n        return self.cv.split(X,kgroups,groups)\n    \n    def get_n_splits(self,X,y,groups=None):\n        return self.cv.get_n_splits(X,y,groups)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:55:37.364358Z","iopub.execute_input":"2021-06-09T13:55:37.364825Z","iopub.status.idle":"2021-06-09T13:55:37.378343Z","shell.execute_reply.started":"2021-06-09T13:55:37.364775Z","shell.execute_reply":"2021-06-09T13:55:37.377291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ..\ndf = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:55:37.379826Z","iopub.execute_input":"2021-06-09T13:55:37.380206Z","iopub.status.idle":"2021-06-09T13:55:38.227196Z","shell.execute_reply.started":"2021-06-09T13:55:37.380173Z","shell.execute_reply":"2021-06-09T13:55:38.225766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['char_cnt'] = df.excerpt.str.len()\ndf['syllable_cnt'] = df.excerpt.apply(textstat.syllable_count)\ndf['word_cnt'] = df.excerpt.apply(textstat.lexicon_count)\ndf['sent_cnt'] = df.excerpt.apply(textstat.sentence_count)\ndf['flesch_reading_ease'] = df.excerpt.apply(textstat.flesch_reading_ease)\ndf['flesch_kincaid_grade'] = df.excerpt.apply(textstat.flesch_kincaid_grade)\ndf['gunning_fog'] = df.excerpt.apply(textstat.gunning_fog)\ndf['ari'] = df.excerpt.apply(textstat.automated_readability_index)\ndf['cli'] = df.excerpt.apply(textstat.coleman_liau_index)\ndf['lwf'] = df.excerpt.apply(textstat.linsear_write_formula)\ndf['dcrs'] = df.excerpt.apply(textstat.dale_chall_readability_score)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:55:38.229299Z","iopub.execute_input":"2021-06-09T13:55:38.229645Z","iopub.status.idle":"2021-06-09T13:55:48.691227Z","shell.execute_reply.started":"2021-06-09T13:55:38.229608Z","shell.execute_reply":"2021-06-09T13:55:48.690024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def determine_polarity_and_subjectivity_stats(text):\n    tb = TextBlob(text)\n    polarities = []\n    subjectivities = []\n    for sentence in tb.sentences:\n        polarities.append(sentence.polarity)\n        subjectivities.append(sentence.subjectivity)\n            \n    return np.mean(polarities), np.median(polarities), np.ptp(polarities), np.percentile(polarities, 75) - np.percentile(polarities, 25), np.std(polarities), \\\n        np.mean(subjectivities), np.median(subjectivities), np.ptp(subjectivities), np.percentile(subjectivities, 75) - np.percentile(subjectivities, 25), np.std(subjectivities)\n\n    \ndf['pol_mean'], df['pol_median'], df['pol_range'], df['pol_cutted_range'], df['pol_std'], \\\ndf['subj_mean'], df['subj_median'], df['subj_range'], df['subj_cutted_range'], df['subj_std'] = zip(*df.excerpt.apply(determine_polarity_and_subjectivity_stats))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:55:48.69298Z","iopub.execute_input":"2021-06-09T13:55:48.693293Z","iopub.status.idle":"2021-06-09T13:56:09.767922Z","shell.execute_reply.started":"2021-06-09T13:55:48.693263Z","shell.execute_reply":"2021-06-09T13:56:09.766579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def avg_word(sentence):\n  words = sentence.split()\n  return (sum(len(word) for word in words)/len(words))\n\ndf['avg_word'] = df.excerpt.apply(avg_word)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:56:09.769636Z","iopub.execute_input":"2021-06-09T13:56:09.770035Z","iopub.status.idle":"2021-06-09T13:56:09.853454Z","shell.execute_reply.started":"2021-06-09T13:56:09.769997Z","shell.execute_reply":"2021-06-09T13:56:09.852257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop = stopwords.words('english')\n\ndf['stopwords_cnt'] = df.excerpt.apply(lambda x: len([x for x in x.split() if x in stop]))\ndf['avg_sent'] = df['word_cnt']/df['sent_cnt']\ndf['stopwords_vs_words'] = df['stopwords_cnt']/df['word_cnt']\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:56:09.856795Z","iopub.execute_input":"2021-06-09T13:56:09.8573Z","iopub.status.idle":"2021-06-09T13:56:10.659018Z","shell.execute_reply.started":"2021-06-09T13:56:09.857239Z","shell.execute_reply":"2021-06-09T13:56:10.656767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sp = spacy.load('en_core_web_sm')\n\n# NOUN, PRON, PROPN, ADJ, NUM, PUNCT\npos_ids_to_look_for = [92, 95, 96, 84, 93, 97]\ndef look_for_pos_freq(text):\n    text = sp(text)\n    pos_freq_dict = text.count_by(spacy.attrs.POS)\n    pos_freq_to_look_for = [pos_freq_dict.get(pos_id, 0) for pos_id in pos_ids_to_look_for]\n    return pos_freq_to_look_for\n\n\ndf['noun'], df['pron'], df['propn'], df['adj'], df['num'], df['punct']= zip(*df.excerpt.apply(look_for_pos_freq))\n\ndf.head(1)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:56:10.663385Z","iopub.execute_input":"2021-06-09T13:56:10.663963Z","iopub.status.idle":"2021-06-09T13:58:04.731659Z","shell.execute_reply.started":"2021-06-09T13:56:10.663889Z","shell.execute_reply":"2021-06-09T13:58:04.730607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = regressor_stratified_cv(group_count=10,random_state=42,strategy='uniform')\n\ntrain, val = next(cv.split(df[['excerpt']], df['target']))\n\ndf_train = df.loc[df.index.isin(train)]\ndf_train.target.plot(kind='kde')\n\ndf_val = df.loc[df.index.isin(val)]\ndf_val.target.plot(kind='kde')\n\nprint(len(df_train), len(df_val))\n\nX_train, X_val = df_train.drop(['target'], axis=1), df_val.drop(['target'], axis=1)\ny_train, y_val = df_train['target'], df_val['target']\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:58:04.733122Z","iopub.execute_input":"2021-06-09T13:58:04.733421Z","iopub.status.idle":"2021-06-09T13:58:04.995026Z","shell.execute_reply.started":"2021-06-09T13:58:04.733393Z","shell.execute_reply":"2021-06-09T13:58:04.993767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nvectors = vectorizer.fit(train_X.excerpt)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:58:04.996555Z","iopub.execute_input":"2021-06-09T13:58:04.996861Z","iopub.status.idle":"2021-06-09T13:58:05.556029Z","shell.execute_reply.started":"2021-06-09T13:58:04.996829Z","shell.execute_reply":"2021-06-09T13:58:05.554884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['tfidf'] = X_train.excerpt.apply(lambda t: vectorizer.transform([t]).data)\nX_val['tfidf'] = X_val.excerpt.apply(lambda t: vectorizer.transform([t]).data)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:58:05.557464Z","iopub.execute_input":"2021-06-09T13:58:05.557782Z","iopub.status.idle":"2021-06-09T13:58:08.363899Z","shell.execute_reply.started":"2021-06-09T13:58:05.557752Z","shell.execute_reply":"2021-06-09T13:58:08.362842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['tfidf_mean'] = X_train.tfidf.apply(np.mean)\nX_val['tfidf_mean'] = X_val.tfidf.apply(np.mean)\nX_train['tfidf_median'] = X_train.tfidf.apply(np.median)\nX_val['tfidf_median'] = X_val.tfidf.apply(np.median)\nX_train['tfidf_std'] = X_train.tfidf.apply(np.std)\nX_val['tfidf_std'] = X_val.tfidf.apply(np.std)\nX_train['tfidf_range'] = X_train.tfidf.apply(np.ptp)\nX_val['tfidf_range'] = X_val.tfidf.apply(np.ptp)\n\nX_train['tfidf_cutted_range'] = X_train.tfidf.apply(lambda l: np.percentile(l, 75) - np.percentile(l, 25))\nX_val['tfidf_cutted_range'] = X_val.tfidf.apply(lambda l: np.percentile(l, 75) - np.percentile(l, 25))","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:58:08.365359Z","iopub.execute_input":"2021-06-09T13:58:08.36567Z","iopub.status.idle":"2021-06-09T13:58:09.416108Z","shell.execute_reply.started":"2021-06-09T13:58:08.365641Z","shell.execute_reply":"2021-06-09T13:58:09.41495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.drop(['id', 'url_legal', 'license', 'excerpt', 'standard_error', 'tfidf'], axis=1)\nX_val = X_val.drop(['id', 'url_legal', 'license', 'excerpt', 'standard_error', 'tfidf'], axis=1)\n\nprint(X_train.columns, len(X_train.columns) == len(X_val.columns))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:58:09.417726Z","iopub.execute_input":"2021-06-09T13:58:09.418062Z","iopub.status.idle":"2021-06-09T13:58:09.437972Z","shell.execute_reply.started":"2021-06-09T13:58:09.41803Z","shell.execute_reply":"2021-06-09T13:58:09.436694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.values.shape, y_train.values.shape)\nprint(type(X_train.values), type(y_train.values))\nprint(X_train.values[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:58:09.439568Z","iopub.execute_input":"2021-06-09T13:58:09.440037Z","iopub.status.idle":"2021-06-09T13:58:09.449129Z","shell.execute_reply.started":"2021-06-09T13:58:09.439987Z","shell.execute_reply":"2021-06-09T13:58:09.447998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = {\n    \"n_estimators\": np.arange(100, 1000, step=100),\n    \"max_depth\": np.arange(3, 8, step=1),\n    \"min_samples_split\": np.arange(2, 10, step=2),\n    'learning_rate': np.arange(1, 20, step=5) / 100,\n    'loss': ['ls', 'lad', 'huber']\n}\n\nreg = GradientBoostingRegressor()\nrandom_search = RandomizedSearchCV(reg, param_grid, n_iter=20, n_jobs=-1, random_state=42)\nrandom_search.fit(X_train.values, y_train.values)\n\nprint('Best params:')\nprint(random_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T13:58:09.451218Z","iopub.execute_input":"2021-06-09T13:58:09.451643Z","iopub.status.idle":"2021-06-09T14:05:39.419413Z","shell.execute_reply.started":"2021-06-09T13:58:09.451599Z","shell.execute_reply":"2021-06-09T14:05:39.418049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reg = random_search.best_estimator_\nrmse = np.sqrt(mean_squared_error(y_val, reg.predict(X_val.values)))\nprint(\"The root mean squared error (RMSE) on val set: {:.4f}\".format(rmse))\n\n\nreg2 = GradientBoostingRegressor(n_estimators=500, max_depth=4, min_samples_split=3, learning_rate=0.01, loss='ls')\nreg2.fit(X_train.values, y_train.values)\nrmse = np.sqrt(mean_squared_error(y_val, reg2.predict(X_val.values)))\nprint(\"The root mean squared error (RMSE) on val set: {:.4f}\".format(rmse))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:05:39.421185Z","iopub.execute_input":"2021-06-09T14:05:39.421518Z","iopub.status.idle":"2021-06-09T14:05:50.985044Z","shell.execute_reply.started":"2021-06-09T14:05:39.421481Z","shell.execute_reply":"2021-06-09T14:05:50.983897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance = reg.feature_importances_\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nfig = plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.array(X_train.columns)[sorted_idx])\nplt.title('Feature Importance (MDI)')\n\nresult = permutation_importance(reg2, X_val, y_val, n_repeats=10,\n                                random_state=42, n_jobs=2)\nsorted_idx = result.importances_mean.argsort()\nplt.subplot(1, 2, 2)\nplt.boxplot(result.importances[sorted_idx].T,\n            vert=False, labels=np.array(X_train.columns)[sorted_idx])\nplt.title(\"Permutation Importance (test set)\")\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-09T14:05:50.986559Z","iopub.execute_input":"2021-06-09T14:05:50.98691Z","iopub.status.idle":"2021-06-09T14:05:57.17344Z","shell.execute_reply.started":"2021-06-09T14:05:50.986859Z","shell.execute_reply":"2021-06-09T14:05:57.17178Z"},"trusted":true},"execution_count":null,"outputs":[]}]}