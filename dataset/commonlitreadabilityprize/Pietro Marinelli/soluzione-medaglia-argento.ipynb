{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%time\n!pip install ../input/textstats/textstat-master\n!pip install ../input/pyphen/Pyphen-master","metadata":{"execution":{"iopub.status.busy":"2022-01-31T17:07:58.088989Z","iopub.execute_input":"2022-01-31T17:07:58.089319Z","iopub.status.idle":"2022-01-31T17:08:54.728154Z","shell.execute_reply.started":"2022-01-31T17:07:58.089249Z","shell.execute_reply":"2022-01-31T17:08:54.72711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile ensemble.py\n\nimport numpy as np \nimport pandas as pd \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n\nimport torch\nimport transformers\n\nimport random\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nclass CLRPDataset():\n    def __init__(self,df,max_len, tokenizer):\n        self.excerpt = df['excerpt'].values\n        self.max_len = max_len\n        self.tokenizer = tokenizer \n\n\n        if \"target\" in df.columns:\n            self.target = df['target'].values\n        else:\n            self.target = None\n    \n    def __getitem__(self,index):\n        encode = self.tokenizer(self.excerpt[index],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                return_token_type_ids = True,\n                                truncation=True)  \n\n        #token_ids = encode['input_ids'].squeeze(0)\n        #attn_masks = encode['attention_mask'].squeeze(0)\n        #token_type_ids = encode['token_type_ids'].squeeze(0)\n\n        token_ids = encode['input_ids'][0]\n        attn_masks = encode['attention_mask'][0]\n        token_type_ids = encode['token_type_ids'][0]\n        \n        \n        if self.target is None:\n            return token_ids, attn_masks, token_type_ids\n\n\n        target = self.target[index]\n        target = torch.tensor(target).float()    \n\n        return token_ids, attn_masks, token_type_ids, target  \n\n\n    def __len__(self):\n        return len(self.excerpt)\n    \nclass BertRegreesion(torch.nn.Module):\n\n\n    def __init__(self, dropout, bert_model, model_path,  freeze_bert=False):\n        super(BertRegreesion, self).__init__()\n        \n        self.bert_layer = transformers.AutoModel.from_pretrained(model_path)\n        \n        #  Fix the hidden-state size of the encoder outputs (If you want to add other pre-trained models here, search for the encoder output size)\n        if bert_model == \"roberta-base\":  \n            hidden_size = 768\n        elif bert_model == \"roberta-large\":  \n            hidden_size = 1024\n        elif bert_model == \"microsoft/deberta-large\":  \n            hidden_size = 1024\n\n\n\n        # Freeze bert layers and only train the regression layer weights\n        if freeze_bert:\n            for p in self.bert_layer.parameters():\n                p.requires_grad = False\n\n        # ReGression layer\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.head = torch.nn.Linear(hidden_size, 1)\n        self.bert_model = bert_model\n    \n    def forward(self, input_ids, attn_masks, token_type_ids):\n\n        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n        if  self.bert_model == \"microsoft/deberta-large\":\n            output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n            output = output[0]\n            output = output[:,0,:].squeeze(1)\n\n        else:  \n            cont_reps, output = self.bert_layer(input_ids, attn_masks, token_type_ids,  return_dict=False)\n\n        output = self.head(self.dropout(output))\n\n        return output\n    \nBATCH_SIZE = 4\nFOLDS = 5\nNUM_WORKERS = 4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass Config:\n    epochs = 20 \n    eval_per_epoch = 8 #4\n    dropout = 0.4\n    max_len = 256\n    bert_model = \"roberta-base\"\n    warm_up_steps = 300\n\ndef get_preds (bert_model, instances_path, max_len, df_test):\n    print(instances_path)\n    model_path = \"/kaggle/input/transformers/\" + bert_model + \"-hf\"\n    print(f\"model_path:{model_path}\")\n\n    vocab_path = model_path\n    instances_path = \"/kaggle/input/\" + instances_path\n    instance_name = bert_model.replace(\"/\",\"_\")\n\n    \n    device = DEVICE\n    tokenizer = transformers.AutoTokenizer.from_pretrained(vocab_path)\n\n    test_data = CLRPDataset(df_test,max_len, tokenizer=tokenizer)\n    test_loader = torch.utils.data.DataLoader(test_data,\n                                          batch_size=BATCH_SIZE,\n                                          shuffle=False,\n                                          num_workers=NUM_WORKERS)\n\n    p = np.zeros((len(df_test),))\n    for fold in range(FOLDS): \n        preds = []\n\n        model = BertRegreesion (dropout = Config.dropout, bert_model=bert_model, model_path= model_path, freeze_bert=False)\n        \n        \n        filename = f\"{instances_path}/{instance_name}_{fold}.pt\"\n        model.load_state_dict(torch.load(filename, map_location=torch.device(device)))\n        model.to(device)\n        model.eval()\n    \n        with torch.no_grad():\n            for token_ids, attn_masks, token_type_ids in tqdm(test_loader):\n                token_ids = token_ids.to(device)\n                attn_masks = attn_masks.to(device)\n                token_type_ids = token_type_ids.to(device)\n\n                output = model.forward(token_ids, attn_masks, token_type_ids)\n                output = output.detach().cpu()[:,0]\n\n                preds.append(output)\n        preds = np.concatenate(preds)\n        p += preds\n        del model\n    \n    return p/FOLDS\n\ndef main():\n    from numba import cuda \n    device = cuda.get_current_device()\n    device.reset()\n    \n    df_test = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n    args = sys.argv[1:]\n    \n    preds = get_preds (args[0],args[1], max_len = Config.max_len, df_test = df_test)\n    df_test [args[1]] = preds\n    \n    df_test[[args[1]]].to_pickle(args[1]+\".pkl\")\n\nif __name__ == \"__main__\":\n    main()            \n            ","metadata":{"execution":{"iopub.status.busy":"2022-01-31T17:08:54.730245Z","iopub.execute_input":"2022-01-31T17:08:54.730619Z","iopub.status.idle":"2022-01-31T17:08:54.7405Z","shell.execute_reply.started":"2022-01-31T17:08:54.730574Z","shell.execute_reply":"2022-01-31T17:08:54.738984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile ensemble_atthead.py\n\nimport numpy as np \nimport pandas as pd \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\n\n\nimport torch\nimport transformers\n\nimport random\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nclass CLRPDataset():\n    def __init__(self,df,max_len, tokenizer):\n        self.excerpt = df['excerpt'].values\n        self.max_len = max_len\n        self.tokenizer = tokenizer \n\n\n        if \"target\" in df.columns:\n            self.target = df['target'].values\n        else:\n            self.target = None\n    \n    def __getitem__(self,index):\n        encode = self.tokenizer(self.excerpt[index],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                return_token_type_ids = True,\n                                truncation=True)  \n\n        #token_ids = encode['input_ids'].squeeze(0)\n        #attn_masks = encode['attention_mask'].squeeze(0)\n        #token_type_ids = encode['token_type_ids'].squeeze(0)\n\n        token_ids = encode['input_ids'][0]\n        attn_masks = encode['attention_mask'][0]\n        token_type_ids = encode['token_type_ids'][0]\n        \n        \n        if self.target is None:\n            return token_ids, attn_masks, token_type_ids\n\n\n        target = self.target[index]\n        target = torch.tensor(target).float()    \n\n        return token_ids, attn_masks, token_type_ids, target  \n\n\n    def __len__(self):\n        return len(self.excerpt)\n    \nclass AttentionHead(torch.nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = torch.nn.Linear(in_features, hidden_dim)\n        self.V = torch.nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\n\nclass BertRegreesion(torch.nn.Module):\n\n\n\n    def __init__(self, dropout, bert_model, model_path,  freeze_bert=False):\n        super(BertRegreesion, self).__init__()\n        \n        self.bert_layer = transformers.AutoModel.from_pretrained(model_path, output_hidden_states=True)\n\n\n        #  Fix the hidden-state size of the encoder outputs (If you want to add other pre-trained models here, search for the encoder output size)\n        if bert_model == \"roberta-base\":  \n            hidden_size = 768\n        elif bert_model == \"roberta-large\":  \n            hidden_size = 1024\n        elif bert_model == \"microsoft/deberta-large\":  \n            hidden_size = 1024\n\n\n\n        # Freeze bert layers and only train the regression layer weights\n        if freeze_bert:\n            for p in self.bert_layer.parameters():\n                p.requires_grad = False\n\n        self.head = AttentionHead(hidden_size,hidden_size,1)\n                \n        # ReGression layer\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.linear = torch.nn.Linear(hidden_size, 1)\n        \n        \n        \n        \n        self.bert_model = bert_model\n    \n    def forward(self, input_ids, attn_masks, token_type_ids):\n\n        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n        if  self.bert_model == \"microsoft/deberta-large\":\n            output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n            output = output[0]\n\n        else:  \n            output = self.bert_layer(input_ids, attn_masks, token_type_ids,  return_dict=False)\n            output = output[0]\n        \n        \n        output = self.head(output)\n        output = self.dropout(output)\n        output = self.linear(output)\n\n        return output\n    \nBATCH_SIZE = 4\nFOLDS = 5\nNUM_WORKERS = 4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass Config:\n    epochs = 20 \n    eval_per_epoch = 8 #4\n    dropout = 0.4\n    max_len = 256\n    bert_model = \"roberta-base\"\n    warm_up_steps = 300\n\ndef get_preds (bert_model, instances_path, max_len, df_test):\n    print(instances_path)\n    model_path = \"/kaggle/input/transformers/\" + bert_model + \"-hf\"\n    print(f\"model_path:{model_path}\")\n\n    vocab_path = model_path\n    instances_path = \"/kaggle/input/\" + instances_path\n    instance_name = bert_model.replace(\"/\",\"_\")\n\n    \n    device = DEVICE\n    tokenizer = transformers.AutoTokenizer.from_pretrained(vocab_path)\n\n    test_data = CLRPDataset(df_test,max_len, tokenizer=tokenizer)\n    test_loader = torch.utils.data.DataLoader(test_data,\n                                          batch_size=BATCH_SIZE,\n                                          shuffle=False,\n                                          num_workers=NUM_WORKERS)\n\n    p = np.zeros((len(df_test),))\n    for fold in range(FOLDS): \n        preds = []\n\n        model = BertRegreesion (dropout = Config.dropout, bert_model=bert_model, model_path= model_path, freeze_bert=False)\n        \n        \n        filename = f\"{instances_path}/{instance_name}_{fold}.pt\"\n        model.load_state_dict(torch.load(filename, map_location=torch.device(device)))\n        model.to(device)\n        model.eval()\n    \n        with torch.no_grad():\n            for token_ids, attn_masks, token_type_ids in tqdm(test_loader):\n                token_ids = token_ids.to(device)\n                attn_masks = attn_masks.to(device)\n                token_type_ids = token_type_ids.to(device)\n\n                output = model.forward(token_ids, attn_masks, token_type_ids)\n                output = output.detach().cpu()[:,0]\n\n                preds.append(output)\n        preds = np.concatenate(preds)\n        p += preds\n        del model\n    \n    return p/FOLDS\n\ndef main():\n    from numba import cuda \n    device = cuda.get_current_device()\n    device.reset()\n    \n    df_test = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n    args = sys.argv[1:]\n    \n    preds = get_preds (args[0],args[1], max_len = Config.max_len, df_test = df_test)\n    df_test [args[1]] = preds\n    \n    df_test[[args[1]]].to_pickle(args[1]+\".pkl\")\n\nif __name__ == \"__main__\":\n    main()            \n            ","metadata":{"execution":{"iopub.status.busy":"2022-01-31T17:08:54.743173Z","iopub.execute_input":"2022-01-31T17:08:54.743447Z","iopub.status.idle":"2022-01-31T17:08:54.752341Z","shell.execute_reply.started":"2022-01-31T17:08:54.743416Z","shell.execute_reply":"2022-01-31T17:08:54.751547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n!python ensemble_atthead.py \"roberta-large\" \"clrp-roberta-large-2h-atthead-se\"\n\n!python ensemble_atthead.py \"microsoft/deberta-large\" \"clrp-deberta-large-ppln4-atthead\"\n!python ensemble.py \"roberta-large\" \"clrp-roberta-large-2f-se\"\n\n!python ensemble.py \"microsoft/deberta-large\" \"clrp-deberta-large-2-se\"\n!python ensemble.py \"microsoft/deberta-large\" \"clrp-deberta-large-4-se\"\n","metadata":{"execution":{"iopub.status.busy":"2022-01-31T17:08:54.753782Z","iopub.execute_input":"2022-01-31T17:08:54.754457Z","iopub.status.idle":"2022-01-31T17:20:48.13187Z","shell.execute_reply.started":"2022-01-31T17:08:54.754403Z","shell.execute_reply":"2022-01-31T17:20:48.12861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -lart *pkl","metadata":{"execution":{"iopub.status.busy":"2022-01-31T17:20:48.137454Z","iopub.execute_input":"2022-01-31T17:20:48.13784Z","iopub.status.idle":"2022-01-31T17:20:48.93298Z","shell.execute_reply.started":"2022-01-31T17:20:48.137795Z","shell.execute_reply":"2022-01-31T17:20:48.932016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport sklearn.linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nimport textstat\nimport spacy.lang.en \n\n\nFOLDS = 5\n\ndef read_oof (name, return_all = True):\n    oof = pd.read_csv(f\"/kaggle/input/{name}/oof.csv\")\n    \n    if \"pred_x\" in oof.columns:\n        oof = oof.rename (columns={\"pred_x\":name})\n    else:\n        oof = oof.rename (columns={\"pred\":name})\n        \n    if return_all:\n        return oof\n    else:\n        return oof[[\"id\",name]]\n\n\nname_1 = \"clrp-roberta-large-2f-se\"\nname_2 = \"clrp-deberta-large-2-se\"\nname_3 = \"clrp-deberta-large-4-se\"\nname_4 = \"clrp-deberta-large-ppln4-atthead\"\nname_5 = \"clrp-roberta-large-2h-atthead-se\"\n\noof = read_oof (name_1, return_all =True)\nprint(f\"oof shape:{oof.shape}\")\ntmp = read_oof (name_2, return_all =False)\noof = oof.merge(tmp, on =\"id\")\ntmp = read_oof (name_3, return_all =False)\noof = oof.merge(tmp, on =\"id\")\ntmp = read_oof (name_4, return_all =False)\noof = oof.merge(tmp, on =\"id\")\ntmp = read_oof (name_5, return_all =False)\noof = oof.merge(tmp, on =\"id\")\n\noof_textstats = pd.read_csv(\"/kaggle/input/clrp-textstats/textstats.csv\")\noof = oof.merge(oof_textstats, on=\"id\")\n\noof[\"t1\"] = oof[\"syllable_count\"]**2\noof[\"t2\"] = oof[\"coleman_liau_index\"]**2\n\n\nprint(f\"oof shape:{oof.shape}\")\n\ntextstats_feats = [\n    'syllable_count', \n    'gunning_fog', 'automated_readability_index', 'coleman_liau_index', 'text_standard', \n    \"dale_chall_readability_score\",\n    \"t1\", \n    \"t2\", \n] \n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-31T17:20:48.934921Z","iopub.execute_input":"2022-01-31T17:20:48.935353Z","iopub.status.idle":"2022-01-31T17:20:51.887016Z","shell.execute_reply.started":"2022-01-31T17:20:48.935305Z","shell.execute_reply":"2022-01-31T17:20:51.884169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://pypi.org/project/textstat/\n\n* **syllable_count**: Returns the number of syllables present in the given text.\n* **gunning_fog**: Returns the FOG index of the given text. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document.\n* **automated_readability_index**: Returns the ARI (Automated Readability Index) which outputs a number that approximates the grade level needed to comprehend the text.\n* **coleman_liau_index**: Returns the grade level of the text using the Coleman-Liau Formula. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document.\n* **text_standard**: Based upon all the above tests, returns the estimated school grade level required to understand the text.\n* **dale_chall_readability_score**: Different from other tests, since it uses a lookup table of the most commonly used 3000 English words. Thus it returns the grade level using the New Dale-Chall Formula\n* **t1-t2**: syllable_count^2 and coleman_liau_index^2\n\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\",usecols=['id','target'])\ntrain = pd.merge(train,oof_textstats, on=['id'] )","metadata":{"execution":{"iopub.status.busy":"2022-01-31T17:21:43.615131Z","iopub.execute_input":"2022-01-31T17:21:43.615576Z","iopub.status.idle":"2022-01-31T17:21:43.708025Z","shell.execute_reply.started":"2022-01-31T17:21:43.615512Z","shell.execute_reply":"2022-01-31T17:21:43.707185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[['gunning_fog', 'automated_readability_index', 'coleman_liau_index', 'text_standard', \n    \"dale_chall_readability_score\",'target']].corr()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T17:21:45.382255Z","iopub.execute_input":"2022-01-31T17:21:45.382581Z","iopub.status.idle":"2022-01-31T17:21:45.413417Z","shell.execute_reply.started":"2022-01-31T17:21:45.382547Z","shell.execute_reply":"2022-01-31T17:21:45.412482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndf_test = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n\n\npred = pd.read_pickle(name_1 + \".pkl\")\ndf_test[name_1] = pred.values\n\npred = pd.read_pickle(name_2+ \".pkl\")\ndf_test[name_2] = pred.values\n\npred = pd.read_pickle(name_3+ \".pkl\")\ndf_test[name_3] = pred.values\n\npred = pd.read_pickle(name_4+ \".pkl\")\ndf_test[name_4] = pred.values\n\npred = pd.read_pickle(name_5+ \".pkl\")\ndf_test[name_5] = pred.values\n\n\ndf_test[\"syllable_count\"] = df_test [\"excerpt\"].map(lambda x: textstat.syllable_count(x))\ndf_test[\"gunning_fog\"] = df_test [\"excerpt\"].map(lambda x: textstat.gunning_fog(x))\ndf_test[\"automated_readability_index\"] = df_test [\"excerpt\"].map(lambda x: textstat.automated_readability_index(x))\ndf_test[\"coleman_liau_index\"] = df_test [\"excerpt\"].map(lambda x: textstat.coleman_liau_index(x))\ndf_test[\"text_standard\"] = df_test [\"excerpt\"].map(lambda x: textstat.text_standard(x, float_output=True))\ndf_test[\"dale_chall_readability_score\"] = df_test [\"excerpt\"].map(lambda x: textstat.dale_chall_readability_score(x))\n\n\ndf_test[\"t1\"] = df_test[\"syllable_count\"]**2\ndf_test[\"t2\"] = df_test[\"coleman_liau_index\"]**2\n\n\n\ndf_test[\"target\"] = 0\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-18T18:09:03.27868Z","iopub.execute_input":"2022-01-18T18:09:03.278967Z","iopub.status.idle":"2022-01-18T18:09:04.96762Z","shell.execute_reply.started":"2022-01-18T18:09:03.278939Z","shell.execute_reply":"2022-01-18T18:09:04.966612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [name_1, name_2, name_3, name_4, name_5] + textstats_feats","metadata":{"execution":{"iopub.status.busy":"2022-01-18T18:10:33.7411Z","iopub.execute_input":"2022-01-18T18:10:33.741577Z","iopub.status.idle":"2022-01-18T18:10:33.750427Z","shell.execute_reply.started":"2022-01-18T18:10:33.741532Z","shell.execute_reply":"2022-01-18T18:10:33.74922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler().fit(oof[cols])\n\noof[cols] = scaler.transform(oof[cols])\ndf_test[cols] = scaler.transform(df_test[cols])\n\noof_pred = []\noof_target = []\n\nX_test = df_test[cols].values\n\nfor fold in range(FOLDS):\n    df_val = oof.query(\"kfold == @fold\")\n    X_val = df_val[cols].values\n    y_val = df_val[\"target\"].values\n    \n    \n    df_train = oof.query(\"kfold != @fold\")\n    X_train = df_train[cols].values\n    y_train = df_train[\"target\"].values\n    \n    model = sklearn.linear_model.Ridge(alpha=5.0)\n    model.fit (X_train, y_train)\n    p_val = model.predict (X_val)\n    \n    df_test[\"target\"] += model.predict (X_test)\n    \n    oof_pred.append(p_val)\n    oof_target.append (y_val)\n    score = mean_squared_error (y_val, p_val, squared=False)\n    print(f\"fold:{fold}  ens: {score:.5f}\"  )\n\noof_pred = np.concatenate (oof_pred)\noof_target = np.concatenate (oof_target)\nens_score = mean_squared_error (oof_target, oof_pred, squared=False)\nprint(f\"oof  ens: {ens_score:.5f}\"  )","metadata":{"execution":{"iopub.status.busy":"2022-01-18T18:10:34.996674Z","iopub.execute_input":"2022-01-18T18:10:34.997045Z","iopub.status.idle":"2022-01-18T18:10:35.142577Z","shell.execute_reply.started":"2022-01-18T18:10:34.997011Z","shell.execute_reply":"2022-01-18T18:10:35.141708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[\"target\"] /= FOLDS  \n\n\ndf_test[[\"id\",\"target\"]].to_csv(\"submission.csv\", index=False)\n\ndf_test[[\"id\",\"target\"]]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T18:10:36.195582Z","iopub.execute_input":"2022-01-18T18:10:36.195967Z","iopub.status.idle":"2022-01-18T18:10:36.22425Z","shell.execute_reply.started":"2022-01-18T18:10:36.19593Z","shell.execute_reply":"2022-01-18T18:10:36.223386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}