{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## CommonLit Readability Prize\n\n- *Motivation*\nIn this competition, youâ€™ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.\n\n- *Problem* \nSo this competition has given mainly text and we need to predict a difficulty score for the model . So this is a supervised regression problem. \n\n- *My Approach* \nAlways start simple algorithim first so I am going to implement a simple TF-IDF vectorizer for extracting features from the model and then fit it with a simple Ridge Regression . With this approach I was able to get 0.7 on the private leader board. \n\nAlso I have not optimized the model yet. \n\nBelow notbook documents my approach\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n%matplotlib inline \n\nimport nltk\nimport re\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import linear_model,metrics\n\nimport xgboost as xgb\nimport joblib\nimport pickle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import the data","metadata":{}},{"cell_type":"code","source":"df_train=pd.read_csv('../input/commonlitreadabilityprize/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['target'].hist();\n\n## Target is normally distributed and also kind of scaled .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Just keep the regular columns \ndf_train=df_train[['excerpt','target']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###  . Lets fit a simple Liner regression model to this dataset and see the score . I am going to do some basic text cleaning\nimport spacy \nnlp=spacy.load('en_core_web_sm')\nstopwords=nlp.Defaults.stop_words\n\nimport nltk \nfrom nltk.tokenize import word_tokenize\n\n\ndef preprocess(df):\n    df.loc[:,'cleaned']=df['excerpt'].apply(lambda x: str(x))      #convert to string \n    df.loc[:,'cleaned']=df['cleaned'].apply(lambda x: x.lower())   #lowercase the words \n    df.loc[:,'cleaned']=df['cleaned'].apply(lambda x: re.sub(r'[^\\w\\s]','',x)) #removes punctuation and spaces ,newline,tab\n    df.loc[:,'cleaned']=df['cleaned'].apply(lambda x: word_tokenize(x)) # split words \n    df.loc[:,'cleaned']=df['cleaned'].apply(lambda x: [word for word in x  if word not in stopwords]) #remove stopwords \n    df.loc[:,'cleaned']=df['cleaned'].apply(lambda x: \" \".join(x)) # join back ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess(df=df_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Create folds \n\nfrom sklearn import model_selection\ndf_train['fold']=-1\n\nkf=model_selection.KFold(n_splits=3,shuffle=True, random_state=42)\n\nfor fold,(train_index,valid_index) in enumerate(kf.split(df_train['cleaned'])):\n    df_train.loc[valid_index,'fold']=fold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##3 Let see if TF-IDF vectorizer with Ridge regrssion works well \n## Building the custom cross validation loop \n\ntrain_RMSE=[]\nval_RMSE=[]\ni=1\nfor fold in np.arange(df_train['fold'].nunique()):\n    \n    X_train,y_train= df_train[df_train['fold']!=fold]['cleaned'],df_train[df_train['fold']!=fold]['target']\n    X_val,y_val    = df_train[df_train['fold']==fold]['cleaned'],df_train[df_train['fold']==fold]['target']\n    \n    tfidf=TfidfVectorizer(lowercase=False,tokenizer=word_tokenize,ngram_range=(1,4),max_features=1000)\n    \n    tfidf.fit(X_train)\n    train_transform=tfidf.transform(X_train)\n    val_transform=tfidf.transform(X_val)\n    \n    lr=linear_model.Ridge()\n    lr.fit(train_transform,y_train)\n    \n    train_preds=lr.predict(train_transform)\n    \n    val_preds=lr.predict(val_transform)\n    \n    train_score =metrics.mean_squared_error(y_train,train_preds)\n    val_score   =metrics.mean_squared_error(y_val,val_preds)\n    \n    train_RMSE.append(train_score)\n    val_RMSE.append(val_score)\n    \n    \n    print(f\"*** FINISHED FOLD {i} Train_RMSE={train_score} and Valid_RMSE={val_score} ***\")\n    i=i+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dump model for prediction","metadata":{}},{"cell_type":"code","source":"## Train a model for prediction \n\n\ntfidf=TfidfVectorizer(lowercase=False,tokenizer=word_tokenize,ngram_range=(1,4),max_features=1000)\n\ntransformed_text=tfidf.fit_transform(df_train['cleaned'])\n\nmodel=linear_model.Ridge()\nmodel.fit(transformed_text,df_train['target'])\n\n# save the model to disk\nfilename = 'Ridge_Regression_1000features.sav'\njoblib.dump(model, filename)\n\npickle.dump(tfidf, open(\"tfidf.pickle\", \"wb\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}