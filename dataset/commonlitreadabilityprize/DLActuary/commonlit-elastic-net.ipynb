{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Block below starts project up with needed files, libraries, and miscellaneous settings.","metadata":{}},{"cell_type":"code","source":"#import project libraries\nimport string\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.preprocessing import StandardScaler\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport time\n\n#import redabillity\n#thanks to Ravi Shah for the tips\nimport sys\nsys.path = [\n    '../input/readability-package',\n] + sys.path\nimport readability\n\n#supress SettingwithCopy warning\npd.options.mode.chained_assignment = None\n\n#set seed\nnp.random.seed(31415)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-25T01:15:58.485561Z","iopub.execute_input":"2021-05-25T01:15:58.48598Z","iopub.status.idle":"2021-05-25T01:15:58.494434Z","shell.execute_reply.started":"2021-05-25T01:15:58.485944Z","shell.execute_reply":"2021-05-25T01:15:58.492915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Raw data files are brought in, and sliced for relevant information. ","metadata":{}},{"cell_type":"code","source":"#bring in raw data\nraw_train = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\nraw_test = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n\n#split into X/y\nX = pd.DataFrame(raw_train[\"excerpt\"], columns = [\"excerpt\"])\ny = pd.DataFrame(raw_train[\"target\"], columns = [\"target\"])","metadata":{"execution":{"iopub.status.busy":"2021-05-25T01:15:58.497067Z","iopub.execute_input":"2021-05-25T01:15:58.497569Z","iopub.status.idle":"2021-05-25T01:15:58.565634Z","shell.execute_reply.started":"2021-05-25T01:15:58.49752Z","shell.execute_reply":"2021-05-25T01:15:58.564494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a function to bind column features to each excerpt.","metadata":{}},{"cell_type":"code","source":"def add_read_meas(df):\n\n    t1 = time.time()\n    \n    #thanks to Ravi Shah for the tips\n    #create df of readability by row in training data\n    read_meas = [readability.getmeasures(e, lang = \"en\") for e in df[\"excerpt\"]]\n    read_meas = pd.DataFrame(read_meas)\n    \n    #bind readability features to df\n    r_list = [\"readability grades\", \"sentence info\", \"word usage\", \"sentence beginnings\"]\n    for item in r_list:\n        df = df.join(pd.json_normalize(read_meas[item]), lsuffix = \"_tot\")\n    \n    #create word type ratios from nominals\n    w_list = [\"wordtypes\", \"long_words\", \"complex_words\", \"complex_words_dc\", \"tobeverb\", \"auxverb\", \"conjunction_tot\", \"pronoun_tot\", \"preposition_tot\", \"nominalization\"]\n    df[w_list] = df[w_list].div(df.words, axis = 0)\n    \n    #create per sentence ratios from nominals\n    b_list = [\"pronoun\", \"interrogative\", \"article\", \"subordination\", \"conjunction\", \"preposition\"]\n    df[b_list] = df[b_list].div(df.sentences, axis = 0)\n    df = df.drop([\"words\", \"characters\", \"syllables\", \"wordtypes\", \"sentences\", \"paragraphs\"], axis = 1)\n    \n    t2 = time.time()\n    \n    print(f\"readability measures: {t2 - t1}\")\n    \n    #thanks to Ravi Shah for the tips\n    ###################\n    \n    t3 = time.time()\n    \n    #create vectored words array\n    \n    #first step: tokenize, lemmatize, remove stop words\n    \n    #copy text\n    spacy_df = df[\"excerpt\"].copy()\n\n    #set lang\n    nlp = spacy.load('en_core_web_lg')\n\n    #set to lowercase\n    spacy_df = [e.lower() for e in spacy_df]\n\n    #remove punctuation\n    spacy_df = [e.translate(str.maketrans('', '', string.punctuation)) for e in spacy_df]\n\n    #bring text into df\n    spacy_df = pd.DataFrame(spacy_df, columns = [\"excerpt\"])\n\n    #tokenize\n    tokenizer = nlp.tokenizer\n    spacy_df[\"excerpt\"] = spacy_df[\"excerpt\"].apply(lambda row: list(tokenizer(row)))\n\n    #remove stop words\n    stop_words = set(stopwords.words('english')).union(nlp.Defaults.stop_words)\n    spacy_df[\"excerpt\"] = spacy_df[\"excerpt\"].apply(lambda row: [str(token) for token in row if str(token) not in stop_words])\n    \n    #bring rows back to string\n    spacy_df[\"excerpt\"] = spacy_df[\"excerpt\"].str.join(\" \")\n    \n    #second step: vectorize and bind\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in spacy_df.excerpt])\n    \n    #create column list\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n    \n    #create spacy df\n    spacy_cols = pd.DataFrame(vectors, columns = names)\n    \n    #add spacy features\n    df = df.join(spacy_cols)\n    \n    t4 = time.time()\n    \n    print(f\"spacy measures: {t4 - t3}\")\n    \n    #thanks to Ravi Shah for the tips\n    ###################\n    \n    t5 = time.time()\n    \n    #parts of speach tag list and df\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    pos = pd.DataFrame(columns = pos_tags)\n    \n    #loop through the rows and add counts for each pos\n    for e in df[\"excerpt\"]:\n        \n        #break excerpt into list of tokens\n        tags = pos_tag(word_tokenize(e))\n        \n        #instantiate/clear row dict\n        tag_row = dict()\n        \n        #iterate through tags and set to 0\n        for p in pos_tags:\n            tag_row[p] = 0\n\n        #update counts for each word\n        for tag in tags:\n            try:\n                tag_row[tag[1]] += 1\n            except:\n                pass\n        \n        #add row to pos\n        pos = pos.append(tag_row, ignore_index = True)\n    \n    #create ratio of tags\n    pos = pos/pos.sum(axis=1)[:,None]\n    \n    #add pos features\n    df = df.join(pos)\n    \n    t6 = time.time()\n    \n    print(f\"pos measures: {t6 - t5}\")\n    \n    return df   ","metadata":{"execution":{"iopub.status.busy":"2021-05-25T01:15:58.568179Z","iopub.execute_input":"2021-05-25T01:15:58.568637Z","iopub.status.idle":"2021-05-25T01:15:58.592717Z","shell.execute_reply.started":"2021-05-25T01:15:58.568591Z","shell.execute_reply":"2021-05-25T01:15:58.591427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bind features to training data and view.","metadata":{}},{"cell_type":"code","source":"#add read meas\nX = add_read_meas(X)\n\n#drop excerpt\nX = X.drop(\"excerpt\", axis = 1)\n\n#show data\nprint(X.head())\n\n#show data shape\nprint(X.shape)\n\n#scale data\nsc = StandardScaler().fit(X)\nX = sc.transform(X)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T01:15:58.594658Z","iopub.execute_input":"2021-05-25T01:15:58.594959Z","iopub.status.idle":"2021-05-25T01:18:17.57253Z","shell.execute_reply.started":"2021-05-25T01:15:58.594932Z","shell.execute_reply":"2021-05-25T01:18:17.571416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create the elastic net model function.","metadata":{}},{"cell_type":"code","source":"#create elastic net CV function\ndef ENCV(X, y, folds=10):\n    \n    t1 = time.time()\n    \n    #cv arg\n    cv = RepeatedKFold(n_splits = folds, n_repeats = 3, random_state = 684)\n    \n    #define possible penalty values\n    l_ones = np.arange(.01, 1, .01)\n    \n    #instantiate model\n    model = ElasticNetCV(l1_ratio = l_ones,\n                        alphas = None,\n                        cv = cv,\n                        n_jobs = -1,\n                        normalize = False,\n                        fit_intercept = True,\n                        tol = .1)\n    \n    #fit model\n    fitted_model = model.fit(X, y)\n    \n    #get predictios, calcualte RMSE\n    pred = fitted_model.predict(X)\n    rmse = mean_squared_error(y, pred, squared = False)\n    \n    t2 = time.time()\n    \n    print(f\"elastic net run time: {t2 - t1}\")\n    \n    #return model and RMSE\n    return fitted_model, rmse","metadata":{"execution":{"iopub.status.busy":"2021-05-25T01:18:17.574113Z","iopub.execute_input":"2021-05-25T01:18:17.574633Z","iopub.status.idle":"2021-05-25T01:18:17.582115Z","shell.execute_reply.started":"2021-05-25T01:18:17.574594Z","shell.execute_reply":"2021-05-25T01:18:17.581292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model is run and output and RMSE is saved.","metadata":{}},{"cell_type":"code","source":"#run model and save data\nmodel, score = ENCV(X, y)\n\n#show parameters and RMSE score\nprint(model.alpha_, model.l1_ratio_)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T01:18:17.583321Z","iopub.execute_input":"2021-05-25T01:18:17.583735Z","iopub.status.idle":"2021-05-25T01:23:04.370576Z","shell.execute_reply.started":"2021-05-25T01:18:17.583703Z","shell.execute_reply":"2021-05-25T01:23:04.369385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RMSE is not great, but I am proud of it.\n\n\nTest data is run and submitted below.","metadata":{}},{"cell_type":"code","source":"#prepare test features\ntest = pd.DataFrame(raw_test[\"excerpt\"], columns = [\"excerpt\"])\ntest = add_read_meas(test)\ntest = test.drop(\"excerpt\", axis = 1)\ntest = sc.transform(test)\n\n#build submission\ndata = [raw_test[\"id\"], pd.Series(model.predict(test))]\nheaders = [\"id\", \"target\"]\nsubmission = pd.concat(data, axis=1, keys = headers)\n\n#show submission\nprint(submission)\n\n#save submission\nsubmission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T01:23:04.376619Z","iopub.execute_input":"2021-05-25T01:23:04.380009Z","iopub.status.idle":"2021-05-25T01:23:15.284186Z","shell.execute_reply.started":"2021-05-25T01:23:04.379948Z","shell.execute_reply":"2021-05-25T01:23:15.282706Z"},"trusted":true},"execution_count":null,"outputs":[]}]}