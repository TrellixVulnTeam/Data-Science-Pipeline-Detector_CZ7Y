{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-10T05:16:17.564307Z","iopub.execute_input":"2021-07-10T05:16:17.565108Z","iopub.status.idle":"2021-07-10T05:16:17.588203Z","shell.execute_reply.started":"2021-07-10T05:16:17.564971Z","shell.execute_reply":"2021-07-10T05:16:17.587163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# modification\n# 1. lr_scheduler\n# 2. data augument\n# 3. more epochs\n# 4. loss\n# 5. data overlap\n# 6. Ensemble\n# 7. The 12th layer\n# 8. scaler\n\n# 'output_hidden_states':True,\"hidden_dropout_prob\": 0.0\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:17.589848Z","iopub.execute_input":"2021-07-10T05:16:17.590216Z","iopub.status.idle":"2021-07-10T05:16:17.594349Z","shell.execute_reply.started":"2021-07-10T05:16:17.590179Z","shell.execute_reply":"2021-07-10T05:16:17.593149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport os\nimport re\n#import cmudictloss = criteon(logits, y)\n\nimport torch\nfrom transformers import BertTokenizer, BertModel, BertConfig,BertForSequenceClassification, AdamW\n# from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:17.596786Z","iopub.execute_input":"2021-07-10T05:16:17.597166Z","iopub.status.idle":"2021-07-10T05:16:24.905986Z","shell.execute_reply.started":"2021-07-10T05:16:17.597128Z","shell.execute_reply":"2021-07-10T05:16:24.905095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Load Data","metadata":{}},{"cell_type":"code","source":"standard = False\neda_ = False\n\nscaler = StandardScaler()\ntrain = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest=pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ntrain['excerpt'] = train['excerpt'].apply(lambda x: x.replace('\\n',''))\n\narray = list(range(len(train))) \nnp.random.shuffle(array)\nval = train.iloc[array[:100],]\ntrain = train.iloc[array[100:],]\n\nif standard:\n    target_ = scaler.fit_transform(np.expand_dims(train['target'].to_numpy(),axis=1))\n    train['target'] = np.squeeze(target_)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:24.9084Z","iopub.execute_input":"2021-07-10T05:16:24.908672Z","iopub.status.idle":"2021-07-10T05:16:25.063807Z","shell.execute_reply.started":"2021-07-10T05:16:24.908646Z","shell.execute_reply":"2021-07-10T05:16:25.062906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train\ntext = train.loc[0, 'excerpt']\n\n#Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n#Display the Generated image:\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:25.065085Z","iopub.execute_input":"2021-07-10T05:16:25.065409Z","iopub.status.idle":"2021-07-10T05:16:25.361428Z","shell.execute_reply.started":"2021-07-10T05:16:25.065376Z","shell.execute_reply":"2021-07-10T05:16:25.360537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\ntext = test.loc[0, 'excerpt']\n\n#Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n#Display the Generated image:\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:25.362616Z","iopub.execute_input":"2021-07-10T05:16:25.363032Z","iopub.status.idle":"2021-07-10T05:16:25.635931Z","shell.execute_reply.started":"2021-07-10T05:16:25.36299Z","shell.execute_reply":"2021-07-10T05:16:25.635011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n\t\t\t'into', 'through', 'during', 'before', 'after', \n\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n\t\t\t'should', 'now', '']\n\nfrom nltk.corpus import wordnet \n\nimport re\ndef get_only_chars(line):\n\n    clean_line = \"\"\n\n    line = line.replace(\"â€™\", \"\")\n    line = line.replace(\"'\", \"\")\n    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n    line = line.replace(\"\\t\", \" \")\n    line = line.replace(\"\\n\", \" \")\n    line = line.lower()\n\n    for char in line:\n        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n            clean_line += char\n        else:\n            clean_line += ' '\n\n    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n    if clean_line[0] == ' ':\n        clean_line = clean_line[1:]\n    return clean_line\n\n\ndef synonym_replacement(words, n):\n\tnew_words = words.copy()\n\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n\trandom.shuffle(random_word_list)\n\tnum_replaced = 0\n\tfor random_word in random_word_list:\n\t\tsynonyms = get_synonyms(random_word)\n\t\tif len(synonyms) >= 1:\n\t\t\tsynonym = random.choice(list(synonyms))\n\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n\t\t\tnum_replaced += 1\n\t\tif num_replaced >= n: #only replace up to n words\n\t\t\tbreak\n\n\t#this is stupid but we need it, trust me\n\tsentence = ' '.join(new_words)\n\tnew_words = sentence.split(' ')\n\n\treturn new_words\n\ndef get_synonyms(word):\n\tsynonyms = set()\n\tfor syn in wordnet.synsets(word): \n\t\tfor l in syn.lemmas(): \n\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n\t\t\tsynonyms.add(synonym) \n\tif word in synonyms:\n\t\tsynonyms.remove(word)\n\treturn list(synonyms)\n\n\n########################################################################\n# Random deletion\n# Randomly delete words from the sentence with probability p\n########################################################################\n\ndef random_deletion(words, p):\n\n\t#obviously, if there's only one word, don't delete it\n\tif len(words) == 1:\n\t\treturn words\n\n\t#randomly delete words with probability p\n\tnew_words = []\n\tfor word in words:\n\t\tr = random.uniform(0, 1)\n\t\tif r > p:\n\t\t\tnew_words.append(word)\n\n\t#if you end up deleting all words, just return a random word\n\tif len(new_words) == 0:\n\t\trand_int = random.randint(0, len(words)-1)\n\t\treturn [words[rand_int]]\n\n\treturn new_words\n\n########################################################################\n# Random swap\n# Randomly swap two words in the sentence n times\n########################################################################\n\ndef random_swap(words, n):\n\tnew_words = words.copy()\n\tfor _ in range(n):\n\t\tnew_words = swap_word(new_words)\n\treturn new_words\n\ndef swap_word(new_words):\n\trandom_idx_1 = random.randint(0, len(new_words)-1)\n\trandom_idx_2 = random_idx_1\n\tcounter = 0\n\twhile random_idx_2 == random_idx_1:\n\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n\t\tcounter += 1\n\t\tif counter > 3:\n\t\t\treturn new_words\n\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n\treturn new_words\n\n########################################################################\n# Random insertion\n# Randomly insert n words into the sentence\n########################################################################\n\ndef random_insertion(words, n):\n\tnew_words = words.copy()\n\tfor _ in range(n):\n\t\tadd_word(new_words)\n\treturn new_words\n\ndef add_word(new_words):\n\tsynonyms = []\n\tcounter = 0\n\twhile len(synonyms) < 1:\n\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n\t\tsynonyms = get_synonyms(random_word)\n\t\tcounter += 1\n\t\tif counter >= 10:\n\t\t\treturn\n\trandom_synonym = synonyms[0]\n\trandom_idx = random.randint(0, len(new_words)-1)\n\tnew_words.insert(random_idx, random_synonym)\n    \n    \ndef eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=1):\n\t\n\tsentence = get_only_chars(sentence)\n\twords = sentence.split(' ')\n\twords = [word for word in words if word is not '']\n\tnum_words = len(words)\n\t\n\taugmented_sentences = []\n\tnum_new_per_technique = int(num_aug/4)+1\n\n# \t#sr\n\tif (alpha_sr > 0):\n\t\tn_sr = max(1, int(alpha_sr*num_words))\n\t\tfor _ in range(num_new_per_technique):\n\t\t\ta_words = synonym_replacement(words, n_sr)\n\t\t\taugmented_sentences.append(' '.join(a_words))\n\n\t#ri\n\tif (alpha_ri > 0):\n\t\tn_ri = max(1, int(alpha_ri*num_words))\n\t\tfor _ in range(num_new_per_technique):\n\t\t\ta_words = random_insertion(words, n_ri)\n\t\t\taugmented_sentences.append(' '.join(a_words))\n\n\t#rs\n\tif (alpha_rs > 0):\n\t\tn_rs = max(1, int(alpha_rs*num_words))\n\t\tfor _ in range(num_new_per_technique):\n\t\t\ta_words = random_swap(words, n_rs)\n\t\t\taugmented_sentences.append(' '.join(a_words))\n\n\t#rd\n\tif (p_rd > 0):\n\t\tfor _ in range(num_new_per_technique):\n\t\t\ta_words = random_deletion(words, p_rd)\n\t\t\taugmented_sentences.append(' '.join(a_words))\n\n\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n\tshuffle(augmented_sentences)\n\n\t#trim so that we have the desired number of augmented sentences\n\tif num_aug >= 1:\n\t\taugmented_sentences = augmented_sentences[:num_aug]\n\telse:\n\t\tkeep_prob = num_aug / len(augmented_sentences)\n\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n\n\t#append the original sentence\n\taugmented_sentences.append(sentence)\n\n\treturn augmented_sentences","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-07-10T05:16:25.637544Z","iopub.execute_input":"2021-07-10T05:16:25.637911Z","iopub.status.idle":"2021-07-10T05:16:26.338894Z","shell.execute_reply.started":"2021-07-10T05:16:25.637875Z","shell.execute_reply":"2021-07-10T05:16:26.337756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_aug = 1\nimport random\nfrom random import shuffle\nfrom tqdm import tqdm\nrandom.seed(1)\ntrain_aug = train['excerpt'].tolist()\ntarget_aug = train['target'].tolist()\nprint(\"Input shape:\", np.array(train_aug).shape)\n\nif eda_:\n    for i, line in tqdm(enumerate(train.to_numpy())):\n        label = line[4]  # \n        sentence = line[3]\n    \n        aug_sentences = eda(sentence)\n        train_aug.append(aug_sentences)\n        target_aug.append(label)\n\nprint(\"Output shape:\", np.array(train_aug).shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:26.344068Z","iopub.execute_input":"2021-07-10T05:16:26.346302Z","iopub.status.idle":"2021-07-10T05:16:26.389391Z","shell.execute_reply.started":"2021-07-10T05:16:26.34626Z","shell.execute_reply":"2021-07-10T05:16:26.388505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train.drop(train.index[int(np.where(train['target'].to_numpy()==0)[0])]) \n\nplt.scatter(train['target'].to_numpy(), train['standard_error'].to_numpy())   # 0.0 - > std 0.64967129\n\n# remove\n# incorrect :-(","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:26.39538Z","iopub.execute_input":"2021-07-10T05:16:26.39783Z","iopub.status.idle":"2021-07-10T05:16:26.587399Z","shell.execute_reply.started":"2021-07-10T05:16:26.397788Z","shell.execute_reply":"2021-07-10T05:16:26.586539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_path = '../input/bert-base-uncased'\ntokenizer= BertTokenizer.from_pretrained(model_path)\nmodel = BertForSequenceClassification.from_pretrained(model_path, num_labels=1)\nmodel\n\nmodel.to(device)\noptim = AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)      #  params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\nscheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=3, gamma=0.1, last_epoch=-1)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:26.58912Z","iopub.execute_input":"2021-07-10T05:16:26.589386Z","iopub.status.idle":"2021-07-10T05:16:42.170782Z","shell.execute_reply.started":"2021-07-10T05:16:26.58936Z","shell.execute_reply":"2021-07-10T05:16:42.169899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#create a loader","metadata":{}},{"cell_type":"code","source":"class loader(torch.utils.data.Dataset):\n    def __init__(self, train_tokens, labels):\n        self.train_tokens = train_tokens\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        \n        #train_tokens= tokenizer.batch_encode_plus(self.text_list,max_length=512,padding='longest',truncation=True)\n        \n\n        item = {key: torch.tensor(val[idx]) for key, val in self.train_tokens.items()}\n        item['labels'] = torch.tensor(self.labels[idx],dtype=torch.float)\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:42.172163Z","iopub.execute_input":"2021-07-10T05:16:42.172488Z","iopub.status.idle":"2021-07-10T05:16:42.178718Z","shell.execute_reply.started":"2021-07-10T05:16:42.172453Z","shell.execute_reply":"2021-07-10T05:16:42.177889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(target_aug)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:42.179972Z","iopub.execute_input":"2021-07-10T05:16:42.180491Z","iopub.status.idle":"2021-07-10T05:16:42.190475Z","shell.execute_reply.started":"2021-07-10T05:16:42.180452Z","shell.execute_reply":"2021-07-10T05:16:42.189373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if eda_:\n    train_tokens= tokenizer.batch_encode_plus(train_aug, max_length=512, padding='longest', truncation=True)\n    train_dataset = loader(train_tokens, target_aug)\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  \nelse:\n    train_tokens= tokenizer.batch_encode_plus(train['excerpt'].tolist(), max_length=512, padding='longest', truncation=True)\n    train_dataset = loader(train_tokens, train.target.values)\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # 4-> 32\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:42.191926Z","iopub.execute_input":"2021-07-10T05:16:42.192374Z","iopub.status.idle":"2021-07-10T05:16:56.468901Z","shell.execute_reply.started":"2021-07-10T05:16:42.192335Z","shell.execute_reply":"2021-07-10T05:16:56.468037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_tokens= tokenizer.batch_encode_plus(val['excerpt'].tolist(),max_length=512,padding='longest',truncation=True)\nval_dataset = loader(val_tokens, val.target.values)\n\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\ntest_tokens= tokenizer.batch_encode_plus(test['excerpt'].tolist(),max_length=512,padding='longest',truncation=True)\ntest['target']=0\ntest_dataset = loader(test_tokens, test.target.values)\n\ntest_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:56.47015Z","iopub.execute_input":"2021-07-10T05:16:56.47051Z","iopub.status.idle":"2021-07-10T05:16:57.031987Z","shell.execute_reply.started":"2021-07-10T05:16:56.470476Z","shell.execute_reply":"2021-07-10T05:16:57.031167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModel,AutoConfig, AutoTokenizer,get_cosine_schedule_with_warmup\n\nlr_scheduler = get_cosine_schedule_with_warmup(optim,num_warmup_steps=0,num_training_steps=  int(len(train)/10))   # 10 *\n   \n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:57.033164Z","iopub.execute_input":"2021-07-10T05:16:57.03355Z","iopub.status.idle":"2021-07-10T05:16:57.344947Z","shell.execute_reply.started":"2021-07-10T05:16:57.033512Z","shell.execute_reply":"2021-07-10T05:16:57.344029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_test_preds = []\nval_rmse = []\nfrom tqdm import tqdm\nfor epoch in range(3):    #3\n    for batch in tqdm(train_loader):\n   \n#         optim.zero_grad()  ->\n        optim.zero_grad(set_to_none=True) \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]  # MseLossBackward\n#         print(\"loss:\", outputs[0], flush=True)\n        # sqrt\n        # return torch.sqrt(nn.MSELoss()(outputs,targets))\n        loss.backward()\n        optim.step()\n        if lr_scheduler:\n            lr_scheduler.step()\n    \n    \n        # validation\n    total_preds = []\n    # iterate over batches\n    for step, batch in enumerate(val_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n        with torch.no_grad():\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n            \n            preds = outputs[1].detach().cpu().numpy()\n            if standard:\n                preds = scaler.inverse_transform(preds)\n                \n            total_preds.extend(preds-labels.detach().cpu().numpy())\n\n    val_rmse.append(np.sqrt(np.mean(np.square(total_preds))))\n    print(\"Epoch \", epoch, \" RMSE: \", np.sqrt(np.mean(np.square(total_preds))))\n    # test\n    total_preds = []\n    for step, batch in enumerate(test_loader):\n        print(step)\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n        with torch.no_grad():\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n\n            preds = outputs[1].detach().cpu().numpy()\n            if standard:\n                    preds = scaler.inverse_transform(preds)\n\n            total_test_preds.append(preds)\n    print(total_test_preds)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:16:57.34629Z","iopub.execute_input":"2021-07-10T05:16:57.346616Z","iopub.status.idle":"2021-07-10T05:21:03.585263Z","shell.execute_reply.started":"2021-07-10T05:16:57.34658Z","shell.execute_reply":"2021-07-10T05:21:03.584399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_preds = []\nfor step, batch in enumerate(val_loader):\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n    with torch.no_grad():\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n        loss = outputs[0].detach().cpu().numpy()\n        preds = outputs[1].detach().cpu().numpy()\n        if standard:\n                preds = scaler.inverse_transform(preds)\n\n        total_preds.extend(preds-labels.detach().cpu().numpy())\n\n\nprint(\"RMSE: \", np.sqrt(np.mean(np.square(total_preds))))","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-07-10T05:21:03.586544Z","iopub.execute_input":"2021-07-10T05:21:03.586946Z","iopub.status.idle":"2021-07-10T05:21:04.504756Z","shell.execute_reply.started":"2021-07-10T05:21:03.586904Z","shell.execute_reply":"2021-07-10T05:21:04.503878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.array(total_test_preds))\nval_rmse","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:21:04.505982Z","iopub.execute_input":"2021-07-10T05:21:04.506336Z","iopub.status.idle":"2021-07-10T05:21:04.514378Z","shell.execute_reply.started":"2021-07-10T05:21:04.506298Z","shell.execute_reply":"2021-07-10T05:21:04.513382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_preds = []\nfor step, batch in enumerate(test_loader):\n    print(step)\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['labels'].to(device).reshape(attention_mask.shape[0],-1)\n    with torch.no_grad():\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)        \n\n        preds = outputs[1].detach().cpu().numpy()\n        if standard:\n                preds = scaler.inverse_transform(preds)\n\n        total_preds.append(preds)\nprint(total_preds)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:45:03.173429Z","iopub.execute_input":"2021-07-10T05:45:03.173797Z","iopub.status.idle":"2021-07-10T05:45:03.24715Z","shell.execute_reply.started":"2021-07-10T05:45:03.173765Z","shell.execute_reply":"2021-07-10T05:45:03.246174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total_test_preds = np.array(total_test_preds)\n# total_preds = total_test_preds[np.where(val_rmse==np.min(val_rmse))]\n# print( np.concatenate(total_preds, axis=0))     \n\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\nsample['target'] = np.concatenate(total_preds, axis=0)\n\nsample.to_csv('/kaggle/working/submission.csv', index=False)\nprint(sample)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T05:45:05.59656Z","iopub.execute_input":"2021-07-10T05:45:05.596918Z","iopub.status.idle":"2021-07-10T05:45:05.615854Z","shell.execute_reply.started":"2021-07-10T05:45:05.596888Z","shell.execute_reply":"2021-07-10T05:45:05.614744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}