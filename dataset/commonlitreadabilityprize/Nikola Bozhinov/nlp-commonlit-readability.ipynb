{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"palette = [\"#7209B7\",\"#3F88C5\",\"#136F63\",\"#F72585\",\"#FFBA08\"]\nsns.set(style=\"whitegrid\", palette=palette)\nsns.color_palette(palette)\n\n# palette2 = sns.diverging_palette(240, -240, n=10)\n# sns.color_palette(palette2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **id** - unique ID for excerpt\n- **url_legal** - URL of source - this is blank in the test set.\n- **license** - license of source material - this is blank in the test set.\n- **excerpt** - text to predict reading ease of\n- **target** - reading ease\n- **standard_error** - measure of spread of scores among multiple raters for each excerpt. Not included for test data.","metadata":{}},{"cell_type":"code","source":"# sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\n# def custom_palette(custom_colors):\n#     customPalette = sns.set_palette(sns.color_palette(custom_colors))\n#     sns.palplot(sns.color_palette(custom_colors), size=0.8)\n#     plt.tick_params(axis='both', labelsize=0, length = 0)\n\n# palette = [\"#7209B7\",\"#3F88C5\",\"#136F63\",\"#F72585\",\"#FFBA08\"]\n# palette2 = sns.diverging_palette(120, 220, n=20)\n# custom_palette(palette)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_data = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nprint(train_data.shape)\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(train_data, color=palette[1], sort=\"ascending\", figsize=(10,5), fontsize=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(20,6))\nsns.kdeplot(ax=ax[0], data=train_data, x=\"target\", shade=True)\nsns.kdeplot(ax=ax[1], data=train_data, x='standard_error', shade=True, color=palette[1]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nsns.countplot(data=train_data, y=\"license\", color=palette[1], order = train_data['license'].value_counts().index)\nplt.title(\"License Distribution\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"# a=train_data[train_data['excerpt'].str.contains('/')].reset_index()\n# print(a['excerpt'][0])\n\nprint(\"<: \", train_data['excerpt'].str.contains('<').sum())\nprint(\">: \", train_data['excerpt'].str.contains('>').sum())\nprint(\"/: \", train_data['excerpt'].str.contains('/').sum())\nprint(\"http: \", train_data['excerpt'].str.contains('http').sum())\nprint(\"<br />: \", train_data['excerpt'].str.contains('<br />').sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install contractions","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import contractions\nimport re\nimport string\nfrom nltk.corpus import wordnet\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\n\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nwnl = WordNetLemmatizer()\n\ndef get_wordnet_pos(word):\n    tag = pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)\n\ndef remove_punct(text):\n    message=[]\n    \n    for word in text:\n        message_not_punc = []\n        \n        if word not in stop_words:\n            for char in word:\n                if char not in string.punctuation:\n                    message_not_punc.append(char)\n\n            text_nopunct = \"\".join(message_not_punc)\n            \n            if text_nopunct!=\"\":\n                message.append(text_nopunct)\n                \n    return message\n\ndef preprocessing(text):\n    text = text.lower().strip()\n    text = text.replace(\"/\",\" \")\n    text = contractions.fix(text)\n    text=word_tokenize(text)\n    \n    message = []\n    \n    for word in text:\n        message.append(wnl.lemmatize(word, get_wordnet_pos(word)))\n    \n    message = remove_punct(message)\n    message = \" \".join(message)\n    \n    return message\n\n\ntrain_data['excerpt_clean'] = train_data['excerpt'].apply(lambda x: preprocessing(x))\ntest_data['excerpt_clean'] = test_data['excerpt'].apply(lambda x: preprocessing(x))\n\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# n-grams","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom nltk import ngrams\n\ntext = ' '.join(train_data['excerpt_clean'].tolist())\ntext_tokenize = word_tokenize(text)\n\ndef most_common_words(text, n, num, title):\n    most_common = dict(Counter(ngrams(text, n)).most_common()[:num])\n    df = pd.DataFrame.from_dict(most_common, orient='index').reset_index()\n    df = df.rename(columns={'index':'Word', 0:'Count'})\n    df['Word'] = df['Word'].apply(lambda x: ' '.join(x))\n\n    fig = plt.figure(figsize = (20,6))\n    sns.barplot(data=df, x=\"Count\", y=\"Word\", color=palette[1], orient='h')\n    plt.title(title)\n    plt.xlabel('Frequency')\n    plt.ylabel('')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_common_words(text_tokenize, 1, 20, 'UniGram')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_common_words(text_tokenize, 2, 20, 'BiGram')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_common_words(text_tokenize, 3, 20, 'TriGram')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create features","metadata":{}},{"cell_type":"markdown","source":"## Sentence length","metadata":{}},{"cell_type":"code","source":"train_data['length'] = train_data['excerpt'].apply(lambda x: len(x) - x.count(\" \"))\ntest_data['length'] = test_data['excerpt'].apply(lambda x: len(x) - x.count(\" \"))\n\nsns.displot(data=train_data, x=\"length\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Count sentence","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\n\ntrain_data['count_sent'] = train_data['excerpt'].apply(lambda x: len(sent_tokenize(x)))\ntest_data['count_sent'] = test_data['excerpt'].apply(lambda x: len(sent_tokenize(x)))\n\nsns.displot(data=train_data, x=\"count_sent\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Average sentence length","metadata":{}},{"cell_type":"code","source":"def avg_sent_len(text):  \n    text=sent_tokenize(text)\n    \n    for i, val in enumerate(text):\n        text[i]=len(val)\n    \n    return round(np.mean(text), 3)\n\ntrain_data['avg_sent_len'] = train_data['excerpt'].apply(lambda x: avg_sent_len(x))\ntest_data['avg_sent_len'] = test_data['excerpt'].apply(lambda x: avg_sent_len(x))\n\nsns.displot(data=train_data, x=\"avg_sent_len\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentence syllables","metadata":{}},{"cell_type":"code","source":"!pip install syllables","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import syllables\n\ntrain_data['count_sent_syll'] = train_data['excerpt'].apply(lambda x: syllables.estimate(x))\ntest_data['count_sent_syll'] = test_data['excerpt'].apply(lambda x: syllables.estimate(x))\n\nsns.displot(data=train_data, x=\"count_sent_syll\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count of punctuations marks","metadata":{}},{"cell_type":"code","source":"def count_punct(text):\n    count=0\n    \n    for char in text:\n        if char in string.punctuation:\n            count+=1\n    \n    return count\n\ntrain_data['count_punct'] = train_data['excerpt'].apply(lambda x: count_punct(x))\ntest_data['count_punct'] = test_data['excerpt'].apply(lambda x: count_punct(x))\n\nsns.displot(data=train_data, x=\"count_punct\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count uppercase letters","metadata":{}},{"cell_type":"code","source":"def count_uppercase(text):  \n    count=0\n    \n    for char in text:\n        if char.isupper():\n            count+=1\n    \n    return count\n\ntrain_data['count_uppercase'] = train_data['excerpt'].apply(lambda x: count_uppercase(x))\ntest_data['count_uppercase'] = test_data['excerpt'].apply(lambda x: count_uppercase(x))\n\nsns.displot(data=train_data, x=\"count_uppercase\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count exclamation marks","metadata":{}},{"cell_type":"code","source":"train_data['exclamation_marks'] = train_data['excerpt'].apply(lambda x: x.count(\"!\"))\ntest_data['exclamation_marks'] = test_data['excerpt'].apply(lambda x: x.count(\"!\"))\n\nsns.displot(data=train_data, x=\"exclamation_marks\", bins=20, aspect=1.5, color=palette[4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count words","metadata":{}},{"cell_type":"code","source":"train_data['count_words'] = train_data['excerpt'].apply(lambda x: len(word_tokenize(x)))\ntest_data['count_words'] = test_data['excerpt'].apply(lambda x: len(word_tokenize(x)))\n\nsns.displot(data=train_data, x=\"count_words\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Word length","metadata":{}},{"cell_type":"code","source":"def word_len(text): \n    message_not_punc = []\n    \n    for char in text:\n        if char not in string.punctuation:\n            message_not_punc.append(char)\n            \n    text_nopunct = \"\".join(message_not_punc)\n    \n    words_length = {}\n    words_list = word_tokenize(text_nopunct)\n    \n    for word in words_list:\n        if len(word) in words_length:\n            words_length[len(word)] += 1\n        else:\n            words_length[len(word)] = 1\n        \n    \n    words_length = dict(sorted(words_length.items()))\n    \n    return words_length\n\nwords_length_train = train_data['excerpt'].apply(lambda x: word_len(x.lower()))\nfor i in range(1,31):\n    if i not in words_length_train[0]:\n        words_length_train[0][i]=0\n        \nwords_length_df_train = pd.DataFrame.from_records(words_length_train)\ncols_train = words_length_df_train.columns.tolist()\nwords_length_df_train = words_length_df_train[sorted(cols_train)].fillna(0).astype(int)\n\nwords_length_test = test_data['excerpt'].apply(lambda x: word_len(x.lower()))\nfor i in range(1,31):\n    if i not in words_length_test[0]:\n        words_length_test[0][i]=0\n        \nwords_length_df_test = pd.DataFrame.from_records(words_length_test)\ncols_test = words_length_df_test.columns.tolist()\nwords_length_df_test = words_length_df_test[sorted(cols_test)].fillna(0).astype(int)\n# X_len = pd.concat([train_data, words_length_df], axis=1)\n# X_len\n\ndata_words_length=word_len(text.lower())\nkeys = data_words_length.keys()\nvals = data_words_length.values()\n\nfig = plt.figure(figsize = (20,6))\nplt.bar(keys, vals, align='center')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Average word length","metadata":{}},{"cell_type":"code","source":"def avg_word_len(text):  \n    text=word_tokenize(text)\n    \n    for i, val in enumerate(text):\n        text[i]=len(val)\n    \n    return round(np.mean(text), 3)\n\ntrain_data['avg_word_len'] = train_data['excerpt'].apply(lambda x: avg_word_len(x))\ntest_data['avg_word_len'] = test_data['excerpt'].apply(lambda x: avg_word_len(x))\n\nsns.displot(data=train_data, x=\"avg_word_len\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Average syllables by word","metadata":{}},{"cell_type":"code","source":"def avg_syll_word(text):  \n    text=word_tokenize(text)\n    \n    for i, val in enumerate(text):\n        text[i]=syllables.estimate(val)\n    \n    return round(np.mean(text), 3)\n\ntrain_data['avg_syll_len'] = train_data['excerpt'].apply(lambda x: avg_syll_word(x))\ntest_data['avg_syll_len'] = test_data['excerpt'].apply(lambda x: avg_syll_word(x))\n\nsns.displot(data=train_data, x=\"avg_syll_len\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Count unique words","metadata":{}},{"cell_type":"code","source":"# train_data['unique_word_count'] = train_data['excerpt'].apply(lambda x: len(pd.unique(x.lower().split())))\ntrain_data['unique_word_count_clean'] = train_data['excerpt_clean'].apply(lambda x: len(pd.unique(word_tokenize(x.lower()))))\ntest_data['unique_word_count_clean'] = test_data['excerpt_clean'].apply(lambda x: len(pd.unique(word_tokenize(x.lower()))))\n\n# fig, ax = plt.subplots(1,2,figsize=(20,6))\n# sns.histplot(ax=ax[0], data=train_data, x=\"unique_word_count\", bins=20)\n# sns.histplot(ax=ax[1], data=train_data, x='unique_word_count_clean', bins=20, color=palette[1]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install textstat\n!pip install py-readability-metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import textstat\nfrom readability import Readability\n# test_data=train_data['excerpt'][0]\n# r = Readability(test_data)\n\n# Flesch Reading Ease\n# train_data['flesch_reading_ease1'] = train_data['excerpt'].apply(lambda x: textstat.flesch_reading_ease(x))\ntrain_data['flesch_reading_ease2'] = train_data['excerpt'].apply(lambda x: Readability(x).flesch().score)\ntest_data['flesch_reading_ease2'] = test_data['excerpt'].apply(lambda x: Readability(x).flesch().score)\n# print('flesch_reading_ease ',textstat.flesch_reading_ease(test_data))\n# print('flesch ',r.flesch().score)\n\n# fig, ax = plt.subplots(1,2,figsize=(20,6))\n# sns.histplot(ax=ax[0], data=train_data, x=\"flesch_reading_ease1\", bins=20)\n# sns.histplot(ax=ax[1], data=train_data, x='flesch_reading_ease2', bins=20, color=palette[1]);\n\n\n# Flesch-Kincaid Grade Level\n# train_data['flesch_kincaid_grade1'] = train_data['excerpt'].apply(lambda x: textstat.flesch_kincaid_grade(x))\ntrain_data['flesch_kincaid_grade2'] = train_data['excerpt'].apply(lambda x: Readability(x).flesch_kincaid().score)\ntest_data['flesch_kincaid_grade2'] = test_data['excerpt'].apply(lambda x: Readability(x).flesch_kincaid().score)\n# print('flesch_kincaid_grade ',textstat.flesch_kincaid_grade(test_data))\n# print('flesch_kincaid ',r.flesch_kincaid().score)\n# print(fk.score)\n# print(fk.grade_level)\n\n# fig, ax = plt.subplots(1,2,figsize=(20,6))\n# sns.histplot(ax=ax[0], data=train_data, x=\"flesch_kincaid_grade1\", bins=20)\n# sns.histplot(ax=ax[1], data=train_data, x='flesch_kincaid_grade2', bins=20, color=palette[1]);\n\n\n# SMOG\ntrain_data['smog_index'] = train_data['excerpt'].apply(lambda x: textstat.smog_index(x))\ntest_data['smog_index'] = test_data['excerpt'].apply(lambda x: textstat.smog_index(x))\n# print('smog_index ',textstat.smog_index(test_data))\n# print('smog ',r.smog(all_sentences=True))\n\n# sns.displot(data=train_data, x=\"smog_index\", bins=20, aspect=1.5)\n\n\n# Coleman Liau Index\n# train_data['coleman_liau_index1'] = train_data['excerpt'].apply(lambda x: textstat.coleman_liau_index(x))\ntrain_data['coleman_liau_index2'] = train_data['excerpt'].apply(lambda x: Readability(x).coleman_liau().score)\ntest_data['coleman_liau_index2'] = test_data['excerpt'].apply(lambda x: Readability(x).coleman_liau().score)\n# print('coleman_liau_index ',textstat.coleman_liau_index(test_data))\n# print('coleman_liau ',r.coleman_liau().score)\n\n# fig, ax = plt.subplots(1,2,figsize=(20,6))\n# sns.histplot(ax=ax[0], data=train_data, x=\"coleman_liau_index1\", bins=20)\n# sns.histplot(ax=ax[1], data=train_data, x='coleman_liau_index2', bins=20, color=palette[1]);\n\n\n# Automated Readability Index (ARI)\n# train_data['automated_readability_index1'] = train_data['excerpt'].apply(lambda x: textstat.automated_readability_index(x))\ntrain_data['automated_readability_index2'] = train_data['excerpt'].apply(lambda x: Readability(x).ari().score)\ntest_data['automated_readability_index2'] = test_data['excerpt'].apply(lambda x: Readability(x).ari().score)\n# print('automated_readability_index ',textstat.automated_readability_index(test_data))\n# print('ari ',r.ari().score)\n\n# fig, ax = plt.subplots(1,2,figsize=(20,6))\n# sns.histplot(ax=ax[0], data=train_data, x=\"automated_readability_index1\", bins=20)\n# sns.histplot(ax=ax[1], data=train_data, x='automated_readability_index2', bins=20, color=palette[1]);\n\n\n# Dale Chall Readability\n# train_data['dale_chall_readability_score1'] = train_data['excerpt'].apply(lambda x: textstat.dale_chall_readability_score(x))\ntrain_data['dale_chall_readability_score2'] = train_data['excerpt'].apply(lambda x: Readability(x).dale_chall().score)\ntest_data['dale_chall_readability_score2'] = test_data['excerpt'].apply(lambda x: Readability(x).dale_chall().score)\n# print('dale_chall_readability_score ',textstat.dale_chall_readability_score(test_data))\n# print('dale_chall ',r.dale_chall().score)\n\n# fig, ax = plt.subplots(1,2,figsize=(20,6))\n# sns.histplot(ax=ax[0], data=train_data, x=\"dale_chall_readability_score1\", bins=20)\n# sns.histplot(ax=ax[1], data=train_data, x='dale_chall_readability_score2', bins=20, color=palette[1]);\n\n\n# Linsear Write\n# train_data['linsear_write_formula1'] = train_data['excerpt'].apply(lambda x: textstat.linsear_write_formula(x))\ntrain_data['linsear_write_formula2'] = train_data['excerpt'].apply(lambda x: Readability(x).linsear_write().score)\ntest_data['linsear_write_formula2'] = test_data['excerpt'].apply(lambda x: Readability(x).linsear_write().score)\n# print('linsear_write_formula ',textstat.linsear_write_formula(test_data))\n# print('linsear_write ',r.linsear_write().score)\n\n# fig, ax = plt.subplots(1,2,figsize=(20,6))\n# sns.histplot(ax=ax[0], data=train_data, x=\"linsear_write_formula1\", bins=20)\n# sns.histplot(ax=ax[1], data=train_data, x='linsear_write_formula2', bins=20, color=palette[1]);\n\n\n# Gunning Fog\n# train_data['gunning_fog1'] = train_data['excerpt'].apply(lambda x: textstat.gunning_fog(x))\ntrain_data['gunning_fog2'] = train_data['excerpt'].apply(lambda x: Readability(x).gunning_fog().score)\ntest_data['gunning_fog2'] = test_data['excerpt'].apply(lambda x: Readability(x).gunning_fog().score)\n# print('gunning_fog ',textstat.gunning_fog(test_data))\n# print('gunning_fog ',r.gunning_fog().score)\n\n# fig, ax = plt.subplots(1,2,figsize=(20,6))\n# sns.histplot(ax=ax[0], data=train_data, x=\"gunning_fog1\", bins=20)\n# sns.histplot(ax=ax[1], data=train_data, x='gunning_fog2', bins=20, color=palette[1]);\n\n\n# SPACHE\ntrain_data['spache'] = train_data['excerpt'].apply(lambda x: Readability(x).spache().score)\ntest_data['spache'] = test_data['excerpt'].apply(lambda x: Readability(x).spache().score)\n# print('spache',r.spache().score)\n\n# sns.displot(data=train_data, x=\"spache\", bins=20, aspect=1.5)\n\n\n# Syllable Count\n# print('syllable_count ',textstat.syllable_count(test_data))\n\n# Lexicon Count\n# print('lexicon_count ',textstat.lexicon_count(test_data, removepunct=True))\n\n# Sentence Count\n# print('sentence_count ',textstat.sentence_count(test_data))\n\n# Difficult words\ntrain_data['difficult_words'] = train_data['excerpt'].apply(lambda x: textstat.difficult_words(x))\ntest_data['difficult_words'] = test_data['excerpt'].apply(lambda x: textstat.difficult_words(x))\n# print('difficult_words ',textstat.difficult_words(test_data))\n\n# sns.displot(data=train_data, x=\"difficult_words\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grammar checker","metadata":{}},{"cell_type":"code","source":"!pip install language_tool_python","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import language_tool_python\ntool = language_tool_python.LanguageTool('en-US')\n\n# matches = tool.check(train_data['excerpt'][5])\n# print(len(matches))\n# print(matches[0])\n# print(matches[1])\n# print(matches[2])\n\n# correct=tool.correct(train_data['excerpt'][5])\n# print(correct)\n\ntrain_data['gramm_error'] = train_data['excerpt'].apply(lambda x: len(tool.check(x)))\ntest_data['gramm_error'] = test_data['excerpt'].apply(lambda x: len(tool.check(x)))\nsns.displot(data=train_data, x=\"gramm_error\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lexical richness","metadata":{}},{"cell_type":"code","source":"!pip install lexicalrichness","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lexicalrichness import LexicalRichness\n\n# lex = LexicalRichness(train_data['excerpt'][5])\n\n# print('word count',lex.words)\n# print('unique term count',lex.terms)\n# print('type-token ratio',lex.ttr)\n# print('root type-token ratio',lex.rttr)\n# print('corrected type-token ratio',lex.cttr)\n# print('mean segmental type-token ratio',lex.msttr(segment_window=25))\n# print('moving average type-token ratio',lex.mattr(window_size=25))\n# print('Measure of Textual Lexical Diversity',lex.mtld(threshold=0.72))\n# print('hypergeometric distribution diversity',lex.hdd(draws=42))\n\ntrain_data['ttr'] = train_data['excerpt'].apply(lambda x: LexicalRichness(x).ttr)\ntest_data['ttr'] = test_data['excerpt'].apply(lambda x: LexicalRichness(x).ttr)\nsns.displot(data=train_data, x=\"ttr\", bins=20, aspect=1.5)\n\ntrain_data['rttr'] = train_data['excerpt'].apply(lambda x: LexicalRichness(x).rttr)\ntest_data['rttr'] = test_data['excerpt'].apply(lambda x: LexicalRichness(x).rttr)\nsns.displot(data=train_data, x=\"rttr\", bins=20, aspect=1.5)\n\ntrain_data['cttr'] = train_data['excerpt'].apply(lambda x: LexicalRichness(x).cttr)\ntest_data['cttr'] = test_data['excerpt'].apply(lambda x: LexicalRichness(x).cttr)\nsns.displot(data=train_data, x=\"cttr\", bins=20, aspect=1.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = train_data.corr(method = \"pearson\")\n# corr = train_data.corr(method = \"spearman\")\n# corr = train_data.corr(method = \"kendall\")\n\nf, ax = plt.subplots(figsize=(27, 27))\n\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax, annot=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"train_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data[['length', 'count_sent', 'avg_sent_len','count_sent_syll', 'count_punct', 'count_uppercase','exclamation_marks', \n                'count_words', 'avg_word_len', 'avg_syll_len', 'unique_word_count_clean', 'flesch_reading_ease2', 'flesch_kincaid_grade2', \n                'smog_index', 'coleman_liau_index2', 'automated_readability_index2', 'dale_chall_readability_score2', \n                'linsear_write_formula2', 'gunning_fog2', 'spache', 'difficult_words', 'gramm_error', 'ttr', 'rttr', 'cttr']]\n\ny = train_data['target']\n\nX_sub = test_data[['length', 'count_sent', 'avg_sent_len','count_sent_syll', 'count_punct', 'count_uppercase','exclamation_marks', \n                   'count_words', 'avg_word_len', 'avg_syll_len', 'unique_word_count_clean', 'flesch_reading_ease2', 'flesch_kincaid_grade2', \n                   'smog_index', 'coleman_liau_index2', 'automated_readability_index2', 'dale_chall_readability_score2', \n                   'linsear_write_formula2', 'gunning_fog2', 'spache', 'difficult_words', 'gramm_error', 'ttr', 'rttr', 'cttr']]\n\nprint(X.shape)\nprint(X_sub.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(words_length_df_train.shape)\nprint(words_length_df_test.shape)\n\nX = pd.concat([X, words_length_df_train], axis=1)\nX_sub = pd.concat([X_sub, words_length_df_test], axis=1)\n\nprint(X.shape)\nprint(X_sub.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscale=StandardScaler().fit(X_train)\nX_train_sc = scale.transform(X_train)\nX_test_sc = scale.transform(X_test)\nX_sub_sc = scale.transform(X_sub)\n\nprint(X_train_sc.shape)\nprint(X_test_sc.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import SGDRegressor, LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.metrics import explained_variance_score, max_error, mean_absolute_error, r2_score, explained_variance_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.metrics import median_absolute_error, mean_poisson_deviance, mean_gamma_deviance\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models=[(\"Linear Regression\", LinearRegression()),\n        (\"Ridge Regression\", Ridge()),\n        (\"Lasso Regression\", Lasso()),\n        (\"Elastic-Net Regression\", ElasticNet()),\n        (\"Stochastic Gradient Descent\", SGDRegressor()),\n        (\"Decision Tree\", DecisionTreeRegressor()),\n        (\"Random Forest\", RandomForestRegressor()),\n        (\"Extra Trees\", ExtraTreesRegressor()),\n        (\"Gradient Boostin\", GradientBoostingRegressor()),\n        (\"KNeighbors\", KNeighborsRegressor()),\n        (\"SVM linear\", SVR(kernel='linear')),\n        (\"SVM rbf\", SVR(kernel='rbf')),\n        (\"Ada Boost\", AdaBoostRegressor())]\n\nfor name, model in models:\n    results = cross_val_score(model, X_train_sc, y_train, cv=10)\n    print(f\"\\x1b[96m{name}\\x1b[0m: \\x1b[93m{results.mean():.4f}\\x1b[0m ± {results.std():.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rdg = Ridge(alpha=0.4)\nrdg.fit(X_train_sc, y_train)\n\nrdg_predict = rdg.predict(X_test_sc)\nprint(mean_squared_error(y_test, rdg_predict, squared=False))\nprint(\"max_error: \", max_error(y_test, rdg_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svr = SVR(C=0.6, gamma=0.01, kernel='rbf')\nsvr.fit(X_train_sc, y_train)\n\nsvr_predict = svr.predict(X_test_sc)\nprint(mean_squared_error(y_test, svr_predict, squared=False))\nprint(\"max_error: \", max_error(y_test, svr_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# features = np.arange(1, 50, 1)\n# results_test = []\n\n# for feature in features:\n#     rf = KNeighborsRegressor(n_neighbors=feature, n_jobs=-1)\n#     rf.fit(X_train_sc, y_train)\n    \n#     results_test.append(mean_squared_error(y_test, rf.predict(X_test_sc), squared=False))\n\n# fig, ax = plt.subplots(figsize=(25,8)) \n# plt.plot(features, results_test, 'b')\n\n# ax.set_axisbelow(True)\n# ax.minorticks_on()\n# ax.grid(which='major', linestyle='-', linewidth=0.5, color='black',)\n# ax.grid(which='minor', linestyle=':', linewidth=0.5, color='black', alpha=0.7)\n\n# plt.gca().xaxis.set_major_locator(plt.MultipleLocator(0.1))\n\n# print(results_test[results_test.index(min(results_test))])\n# print(features[results_test.index(min(results_test))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=39, max_features=24, min_samples_split=2, random_state=15, n_jobs=-1)\nrf.fit(X_train_sc, y_train)\n\nrf_predict = rf.predict(X_test_sc)\nprint(mean_squared_error(y_test, rf_predict, squared=False))\nprint(\"max_error: \", max_error(y_test, rf_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance = rf.feature_importances_[:30]\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfig = plt.figure(figsize=(17, 6))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.array(X_train.columns)[sorted_idx])\nplt.title('Feature Importance')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_predict = rf.predict(X_sub_sc)\n\noutput = pd.DataFrame({'id': test_data['id'], 'target': rf_predict})\noutput.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gb = GradientBoostingRegressor(n_estimators=71, max_features=23, random_state=0)\ngb.fit(X_train_sc, y_train)\n\ngb_predict = gb.predict(X_test_sc)\nprint(mean_squared_error(y_test, gb_predict, squared=False))\nprint(\"max_error: \", max_error(y_test, gb_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance = gb.feature_importances_[:30]\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfig = plt.figure(figsize=(17, 6))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.array(X_train.columns)[sorted_idx])\nplt.title('Feature Importance')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kn = KNeighborsRegressor(n_neighbors=10, n_jobs=-1)\nkn.fit(X_train_sc, y_train)\n\nkn_predict = kn.predict(X_test_sc)\nprint(mean_squared_error(y_test, kn_predict, squared=False))\nprint(\"max_error: \", max_error(y_test, kn_predict))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}