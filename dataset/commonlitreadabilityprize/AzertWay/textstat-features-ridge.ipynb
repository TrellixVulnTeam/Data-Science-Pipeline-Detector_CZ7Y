{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p>Based on the following kernels:\n<p>I.CommonLit: Explore + XGBRF&RepeatedFold Model</p>\n<p>https://www.kaggle.com/andradaolteanu/i-commonlit-explore-xgbrf-repeatedfold-model</p>\n<br>\n<p>CommonLit Readability Prize: EDA + Baseline</p>\n<p>https://www.kaggle.com/ruchi798/commonlit-readability-prize-eda-baseline</p>","metadata":{}},{"cell_type":"markdown","source":"<h1>Import libraries üìö</h1>","metadata":{}},{"cell_type":"code","source":"! pip install textstat\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport time\nimport string\nimport pickle\nimport textstat\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import spearmanr\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import Ridge\nfrom xgboost import XGBRFRegressor\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:11:28.166025Z","iopub.execute_input":"2021-07-07T16:11:28.166373Z","iopub.status.idle":"2021-07-07T16:11:34.748153Z","shell.execute_reply.started":"2021-07-07T16:11:28.166339Z","shell.execute_reply":"2021-07-07T16:11:34.747025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read train and test datasets\ntrain_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:11:47.41495Z","iopub.execute_input":"2021-07-07T16:11:47.41534Z","iopub.status.idle":"2021-07-07T16:11:47.524122Z","shell.execute_reply.started":"2021-07-07T16:11:47.415307Z","shell.execute_reply":"2021-07-07T16:11:47.523256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Preprocessing</h1>","metadata":{}},{"cell_type":"code","source":"class color:\n    BOLD = '\\033[1m' + '\\033[93m'\n    END = '\\033[0m'\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:11:49.942407Z","iopub.execute_input":"2021-07-07T16:11:49.942924Z","iopub.status.idle":"2021-07-07T16:11:49.947049Z","shell.execute_reply.started":"2021-07-07T16:11:49.942892Z","shell.execute_reply":"2021-07-07T16:11:49.945989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_paragraph(paragraph, verbose=False):\n    '''Cleans paragraph before tokenization'''\n\n    # Tokenize & convert to lower case\n    tokens = word_tokenize(paragraph)\n    tokens = [t.lower() for t in tokens]\n\n    # Remove punctuation & non alphabetic characters from each word\n    table = str.maketrans('', '', string.punctuation)\n    tokens = [t.translate(table) for t in tokens]\n    tokens = [t for t in tokens if t.isalpha()]\n\n    # Filter out stopwords\n    stop_words = stopwords.words('english')\n    tokens = [t for t in tokens if t not in stop_words]\n\n    # Lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    tokens_lemm = [lemmatizer.lemmatize(t) for t in tokens]\n\n    if verbose:\n        print(color.BOLD +\n              \"Show difference between original and lemmatized token:\" +\n              color.END)\n        for a, b, in zip(tokens, tokens_lemm):\n            if a != b:\n                print(a, \" | \", b)\n\n    return \" \".join(tokens_lemm)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:11:50.234842Z","iopub.execute_input":"2021-07-07T16:11:50.235438Z","iopub.status.idle":"2021-07-07T16:11:50.244349Z","shell.execute_reply.started":"2021-07-07T16:11:50.235404Z","shell.execute_reply":"2021-07-07T16:11:50.243406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example\ncleaned_paragraph = clean_paragraph(\n    paragraph=train_df[\"excerpt\"][1], verbose=True)\n\nprint(\"\\n\" +\n      color.BOLD + \"Original Text:\" + color.END, \"\\n\" +\n      train_df[\"excerpt\"][1], \"\\n\"*2 +\n      color.BOLD + \"After Cleaning:\" + color.END, \"\\n\" +\n      cleaned_paragraph)\n\n# Apply to the entire text\ntrain_df[\"text\"] = train_df[\"excerpt\"].apply(lambda x: clean_paragraph(x))\ntest_df[\"text\"] = test_df[\"excerpt\"].apply(lambda x: clean_paragraph(x))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:11:50.635845Z","iopub.execute_input":"2021-07-07T16:11:50.636297Z","iopub.status.idle":"2021-07-07T16:12:03.246767Z","shell.execute_reply.started":"2021-07-07T16:11:50.636247Z","shell.execute_reply":"2021-07-07T16:12:03.245764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>XGBRFRegressor model ‚öôÔ∏è</h1>","metadata":{}},{"cell_type":"code","source":"X = train_df[\"text\"]\ny = train_df['target']\n\nrkf = RepeatedKFold(n_repeats=5, n_splits=5, random_state=47)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:12:03.248846Z","iopub.execute_input":"2021-07-07T16:12:03.249533Z","iopub.status.idle":"2021-07-07T16:12:03.255906Z","shell.execute_reply.started":"2021-07-07T16:12:03.249481Z","shell.execute_reply":"2021-07-07T16:12:03.254674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbrf_model = XGBRFRegressor(n_estimators=120, n_jobs=6)\n\nmodel = make_pipeline(\n    TfidfVectorizer(binary=True, ngram_range=(1, 1)),\n    xgbrf_model,\n)\n\ncv_results = cross_validate(model, X, y,\n                            cv=rkf,\n                            scoring='neg_root_mean_squared_error')\ncv_results = pd.DataFrame(cv_results)\ncv_results\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-07T16:12:03.257645Z","iopub.execute_input":"2021-07-07T16:12:03.258345Z","iopub.status.idle":"2021-07-07T16:14:26.845151Z","shell.execute_reply.started":"2021-07-07T16:12:03.2583Z","shell.execute_reply":"2021-07-07T16:14:26.844116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'test score mean: {np.abs(cv_results.test_score.mean())}')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:14:26.846909Z","iopub.execute_input":"2021-07-07T16:14:26.847351Z","iopub.status.idle":"2021-07-07T16:14:26.853027Z","shell.execute_reply.started":"2021-07-07T16:14:26.847307Z","shell.execute_reply":"2021-07-07T16:14:26.8521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.fit(X, y).predict(X)\nprint(mse(y, y_pred, squared=False))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:14:26.856108Z","iopub.execute_input":"2021-07-07T16:14:26.856458Z","iopub.status.idle":"2021-07-07T16:14:34.19927Z","shell.execute_reply.started":"2021-07-07T16:14:26.856429Z","shell.execute_reply":"2021-07-07T16:14:34.197832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>XGBRFRegressor model (with new features) ‚öôÔ∏è</h1>","metadata":{}},{"cell_type":"markdown","source":"<h3>Features creation</h3>","metadata":{}},{"cell_type":"code","source":"def features_extraction(df, train=True, tfidfv=None):\n    # English Word Frequencies Dataset\n    word_freq = pd.read_csv(\"../input/english-word-frequency/unigram_freq.csv\")\n\n    # Convert it into a dict (i.e. hashmap)\n    word_freq = dict(zip(word_freq[\"word\"], word_freq[\"count\"]))\n    available_words = set(word_freq.keys())\n\n    # Tokenize full text\n    df[\"split_text\"] = df[\"excerpt\"].apply(\n        lambda x: [word for word in x.split(\" \")])\n\n    # Get word count for each word\n    df[\"freq_text\"] = df[\"split_text\"].apply(\n        lambda x: [word_freq.get(word, 0)\n                   for word in x if word in available_words])\n\n    # Get sum, mean, std etc. from the text frequencies\n    df[\"freq_sum\"] = df[\"freq_text\"].apply(lambda x: np.sum(x))\n    df[\"freq_mean\"] = df[\"freq_text\"].apply(lambda x: np.mean(x))\n    df[\"freq_std\"] = df[\"freq_text\"].apply(lambda x: np.std(x))\n    df[\"freq_min\"] = df[\"freq_text\"].apply(lambda x: np.min(x))\n    df[\"freq_max\"] = df[\"freq_text\"].apply(lambda x: np.max(x))\n\n    # Get more info from text itself\n    df[\"no_words\"] = df[\"text\"].apply(lambda x: len(x.split(\" \")))\n    df[\"no_words_paragraph\"] = df[\"excerpt\"].apply(lambda x: len(x.split(\" \")))\n\n    # new features with textstat\n    df['flr'] = df['excerpt'].apply(lambda x: textstat.flesch_reading_ease(x))\n    df['flkg'] = df['excerpt'].apply(\n        lambda x: textstat.flesch_kincaid_grade(x))\n    df['fs'] = df['excerpt'].apply(lambda x: textstat.gunning_fog(x))\n    df['ar'] = df['excerpt'].apply(\n        lambda x: textstat.automated_readability_index(x))\n    df['cole'] = df['excerpt'].apply(lambda x: textstat.coleman_liau_index(x))\n    df['lins'] = df['excerpt'].apply(\n        lambda x: textstat.linsear_write_formula(x))\n\n    # Scale these features (as they are HUGE)\n    X = df[['freq_sum', 'freq_mean', 'freq_std', 'freq_min',\n            'freq_max', 'no_words', 'no_words_paragraph',\n            'flr', 'flkg', 'fs', 'ar', 'cole', 'lins']]\n    if train:\n        y = pd.Series(df[\"target\"])\n    else:\n        y = None\n\n    scaler = StandardScaler()\n    X_scaled = pd.DataFrame(scaler.fit_transform(X))\n    X_scaled.columns = X.columns\n\n    if train:\n        tfidfv.fit(df[\"text\"])\n    else:\n        tfidfv.transform(df[\"text\"])\n    train_tf_matrix = pd.DataFrame.sparse.from_spmatrix(\n        tfv.transform(df[\"text\"]))\n    pickle.dump(tfv.vocabulary_, open(\"tfidfvectorizer.pkl\", \"wb\"))\n\n    # Create final X variable, containing all info\n    X = pd.concat([X_scaled, train_tf_matrix], axis=1)\n\n    return X, y\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:16:12.655097Z","iopub.execute_input":"2021-07-07T16:16:12.655539Z","iopub.status.idle":"2021-07-07T16:16:12.675798Z","shell.execute_reply.started":"2021-07-07T16:16:12.655505Z","shell.execute_reply":"2021-07-07T16:16:12.674625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfv = TfidfVectorizer(min_df=3, max_features=None,\n                      strip_accents='unicode', analyzer='word',\n                      token_pattern=r'\\w{1,}', ngram_range=(1, 3),\n                      use_idf=1, smooth_idf=1, sublinear_tf=1,\n                      stop_words='english')\n\nX_train, y_train = features_extraction(train_df, tfidfv=tfv)\nX_test, y_test = features_extraction(test_df, train=False, tfidfv=tfv)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:16:19.009748Z","iopub.execute_input":"2021-07-07T16:16:19.010177Z","iopub.status.idle":"2021-07-07T16:16:34.110981Z","shell.execute_reply.started":"2021-07-07T16:16:19.010127Z","shell.execute_reply":"2021-07-07T16:16:34.108931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of features: {X_train.shape[1]}')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:16:36.941474Z","iopub.execute_input":"2021-07-07T16:16:36.941982Z","iopub.status.idle":"2021-07-07T16:16:36.948697Z","shell.execute_reply.started":"2021-07-07T16:16:36.941948Z","shell.execute_reply":"2021-07-07T16:16:36.947317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Feature analysis</h3>","metadata":{}},{"cell_type":"code","source":"def correlation_heatmap(df):\n    '''function that prints the correlation matrix of a dataframe'''\n    _, ax = plt.subplots(figsize=(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    _ = sns.heatmap(\n        df.corr(),\n        cmap=colormap,\n        square=True,\n        cbar_kws={'shrink': .9},\n        ax=ax,\n        annot=False,\n        linewidths=0.1,\n        vmax=1.0,\n        vmin=-1.0,\n        linecolor='white',\n        annot_kws={'fontsize': 12}\n    )\n\n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:16:43.812342Z","iopub.execute_input":"2021-07-07T16:16:43.812744Z","iopub.status.idle":"2021-07-07T16:16:43.82074Z","shell.execute_reply.started":"2021-07-07T16:16:43.812711Z","shell.execute_reply":"2021-07-07T16:16:43.819416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation matrix for the first 13 features\n# (not including the tfidf's)\ncorrelation_heatmap(X_train.iloc[:, :13])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:16:45.587354Z","iopub.execute_input":"2021-07-07T16:16:45.587734Z","iopub.status.idle":"2021-07-07T16:16:46.122784Z","shell.execute_reply.started":"2021-07-07T16:16:45.587702Z","shell.execute_reply":"2021-07-07T16:16:46.121948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation between the first 13 features\n# and the target\nspearman_corr_scores = dict()\n\nfor feature in X_train.columns[:13]:\n    corr, p = spearmanr(X_train.loc[:, feature], y_train)\n    spearman_corr_scores[feature] = np.abs(corr)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:19:12.699395Z","iopub.execute_input":"2021-07-07T16:19:12.700072Z","iopub.status.idle":"2021-07-07T16:19:12.734151Z","shell.execute_reply.started":"2021-07-07T16:19:12.700019Z","shell.execute_reply":"2021-07-07T16:19:12.73324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spearman_corr_scores = {k: v for k, v in sorted(\n    spearman_corr_scores.items(), key=lambda item: item[1])}\nplt.figure(figsize=(16, 9))\nplt.barh(\n    list(spearman_corr_scores.keys()), list(spearman_corr_scores.values()))\nplt.title('Correlations between new features and the target')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:19:18.552125Z","iopub.execute_input":"2021-07-07T16:19:18.552537Z","iopub.status.idle":"2021-07-07T16:19:18.77202Z","shell.execute_reply.started":"2021-07-07T16:19:18.552495Z","shell.execute_reply":"2021-07-07T16:19:18.770987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Model Eval (cross validation)</h3>","metadata":{}},{"cell_type":"code","source":"xgbrf_model = XGBRFRegressor(n_estimators=120, n_jobs=6)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    cv_results = cross_validate(xgbrf_model,\n                                X_train,\n                                y_train,\n                                cv=rkf,\n                                scoring='neg_root_mean_squared_error')\n\ncv_results = pd.DataFrame(cv_results)\ncv_results\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-07T16:19:28.235012Z","iopub.execute_input":"2021-07-07T16:19:28.235429Z","iopub.status.idle":"2021-07-07T16:39:52.854858Z","shell.execute_reply.started":"2021-07-07T16:19:28.235395Z","shell.execute_reply":"2021-07-07T16:39:52.854012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 6))\nplt.bar(x=list(cv_results.index), height=list(np.abs(cv_results.test_score)))\nplt.axhline(y=np.abs(np.mean(cv_results.test_score)), color='k')\nplt.text(x=21.5, y=0.84, s=np.abs(np.mean(cv_results.test_score)))\nplt.title('Test scores (cross validation)')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:39:52.856707Z","iopub.execute_input":"2021-07-07T16:39:52.857014Z","iopub.status.idle":"2021-07-07T16:39:53.142511Z","shell.execute_reply.started":"2021-07-07T16:39:52.856984Z","shell.execute_reply":"2021-07-07T16:39:53.141622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = xgbrf_model.fit(X_train, y_train).predict(X_train)\nprint(mse(y_train, y_pred, squared=False))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:39:53.144116Z","iopub.execute_input":"2021-07-07T16:39:53.144806Z","iopub.status.idle":"2021-07-07T16:40:49.046099Z","shell.execute_reply.started":"2021-07-07T16:39:53.144759Z","shell.execute_reply":"2021-07-07T16:40:49.044075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fe_dict = xgbrf_model.get_booster().get_score(importance_type='weight')\nfe_dict = pd.DataFrame({\"feature\": fe_dict.keys(),\n                        \"weight\": fe_dict.values()})\\\n            .sort_values(\"weight\", ascending=False).head(10)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:40:49.047549Z","iopub.execute_input":"2021-07-07T16:40:49.047826Z","iopub.status.idle":"2021-07-07T16:40:49.157697Z","shell.execute_reply.started":"2021-07-07T16:40:49.047799Z","shell.execute_reply":"2021-07-07T16:40:49.156506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot\nplt.figure(figsize=(16, 9))\nax = sns.barplot(data=fe_dict, x=\"feature\", y=\"weight\", palette=\"ocean\")\nfor i, v in enumerate(fe_dict.values):\n    plt.text(i, v[1]+10, str(v[1]), color='k', fontweight='bold')\nplt.title(f\"XGBRF: Feature Importance\", size=25)\nplt.xlabel(\"Features\", size=20)\nplt.ylabel(\"Weight\", size=20)\nplt.yticks([])\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:40:49.160379Z","iopub.execute_input":"2021-07-07T16:40:49.160744Z","iopub.status.idle":"2021-07-07T16:40:49.390396Z","shell.execute_reply.started":"2021-07-07T16:40:49.160701Z","shell.execute_reply":"2021-07-07T16:40:49.389175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>Using the Ridge with the same new features</h1>","metadata":{}},{"cell_type":"code","source":"ridge_model = Ridge(fit_intercept=True, normalize=False)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    cv_results = cross_validate(ridge_model,\n                                X_train,\n                                y_train,\n                                cv=rkf,\n                                scoring='neg_root_mean_squared_error')\n\ncv_results = pd.DataFrame(cv_results)\ncv_results\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-07T16:41:18.859493Z","iopub.execute_input":"2021-07-07T16:41:18.859892Z","iopub.status.idle":"2021-07-07T16:44:30.568041Z","shell.execute_reply.started":"2021-07-07T16:41:18.85986Z","shell.execute_reply":"2021-07-07T16:44:30.567042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 6))\nplt.bar(x=list(cv_results.index), height=list(np.abs(cv_results.test_score)))\nplt.axhline(y=np.abs(np.mean(cv_results.test_score)), color='k')\nplt.text(x=21.5, y=0.71, s=np.abs(np.mean(cv_results.test_score)))\nplt.title('Test scores (cross validation)')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:44:30.571599Z","iopub.execute_input":"2021-07-07T16:44:30.574432Z","iopub.status.idle":"2021-07-07T16:44:30.852811Z","shell.execute_reply.started":"2021-07-07T16:44:30.574376Z","shell.execute_reply":"2021-07-07T16:44:30.852096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = ridge_model.fit(X_train, y_train).predict(X_train)\nprint(mse(y_train, y_pred, squared=False))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:44:30.853935Z","iopub.execute_input":"2021-07-07T16:44:30.854402Z","iopub.status.idle":"2021-07-07T16:44:34.785987Z","shell.execute_reply.started":"2021-07-07T16:44:30.854359Z","shell.execute_reply":"2021-07-07T16:44:34.784861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    y_pred = ridge_model.predict(X_test)\n\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = y_pred\npredictions.to_csv(\"submission.csv\", index=False)\npredictions\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:44:34.787602Z","iopub.execute_input":"2021-07-07T16:44:34.788232Z","iopub.status.idle":"2021-07-07T16:44:35.405555Z","shell.execute_reply.started":"2021-07-07T16:44:34.78818Z","shell.execute_reply":"2021-07-07T16:44:35.404558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>PCA</h1>","metadata":{}},{"cell_type":"code","source":"# function that will print the scree\ndef display_scree_plot(pca):\n    plt.figure(figsize=(12, 9))\n    scree = pca.explained_variance_ratio_ * 100\n    plt.bar(np.arange(len(scree))+1, scree)\n    plt.plot(np.arange(len(scree))+1, scree.cumsum(), c=\"red\", marker='o')\n    plt.xlabel(\"explained variance rank\")\n    plt.ylabel(\"explained variance percentage\")\n    plt.title(\"Scree of eigenvalues\")\n    plt.show(block=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:46:00.089168Z","iopub.execute_input":"2021-07-07T16:46:00.089574Z","iopub.status.idle":"2021-07-07T16:46:00.096916Z","shell.execute_reply.started":"2021-07-07T16:46:00.08954Z","shell.execute_reply":"2021-07-07T16:46:00.096024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# conputing of the principal components\npca = PCA(n_components=8)\nt0 = time.time()\npca.fit(X_train)\nprint(\"fit time: %.2fs\" % (time.time() - t0))\n\n# scree of eigenvalues\ndisplay_scree_plot(pca)\n\nprint((pca.explained_variance_ratio_ * 100))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:46:03.539343Z","iopub.execute_input":"2021-07-07T16:46:03.539743Z","iopub.status.idle":"2021-07-07T16:46:08.1242Z","shell.execute_reply.started":"2021-07-07T16:46:03.53971Z","shell.execute_reply":"2021-07-07T16:46:08.122781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_pca = pca.transform(X_train)\n\n# predictions using the Ridge model\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    cv_results = cross_validate(ridge_model,\n                                X_train_pca,\n                                y_train,\n                                cv=rkf,\n                                scoring='neg_root_mean_squared_error')\n\ncv_results = pd.DataFrame(cv_results)\n\nprint(f'test score mean: {np.abs(cv_results.test_score.mean())}')\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-07T16:46:16.082015Z","iopub.execute_input":"2021-07-07T16:46:16.082397Z","iopub.status.idle":"2021-07-07T16:46:17.045429Z","shell.execute_reply.started":"2021-07-07T16:46:16.082366Z","shell.execute_reply":"2021-07-07T16:46:17.044329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = ridge_model.fit(X_train_pca, y_train).predict(X_train_pca)\nprint(mse(y_train, y_pred, squared=False))\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:46:22.38118Z","iopub.execute_input":"2021-07-07T16:46:22.381513Z","iopub.status.idle":"2021-07-07T16:46:22.392437Z","shell.execute_reply.started":"2021-07-07T16:46:22.381484Z","shell.execute_reply":"2021-07-07T16:46:22.391292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions using the XGBRF model\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    cv_results = cross_validate(xgbrf_model,\n                                X_train_pca,\n                                y_train,\n                                cv=rkf,\n                                scoring='neg_root_mean_squared_error')\n\ncv_results = pd.DataFrame(cv_results)\n\nprint(f'test score mean: {np.abs(cv_results.test_score.mean())}')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:46:47.420085Z","iopub.execute_input":"2021-07-07T16:46:47.420483Z","iopub.status.idle":"2021-07-07T16:47:04.230806Z","shell.execute_reply.started":"2021-07-07T16:46:47.420451Z","shell.execute_reply":"2021-07-07T16:47:04.22968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = xgbrf_model.fit(X_train_pca, y_train).predict(X_train_pca)\nprint(mse(y_train, y_pred, squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-07-07T16:47:50.226432Z","iopub.execute_input":"2021-07-07T16:47:50.226799Z","iopub.status.idle":"2021-07-07T16:47:50.948315Z","shell.execute_reply.started":"2021-07-07T16:47:50.226768Z","shell.execute_reply":"2021-07-07T16:47:50.947264Z"},"trusted":true},"execution_count":null,"outputs":[]}]}