{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport time\nimport string\nimport pickle\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\n\nimport torch\nimport torch.nn as nn\nimport seaborn as sns\n\nfrom spacy.lang.en import English\nfrom collections import defaultdict\nfrom nltk.stem import WordNetLemmatizer\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nfrom torch.nn.functional import softmax\nfrom torch.optim.swa_utils import AveragedModel, SWALR\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nfrom scipy.stats import skew","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-12T09:10:38.620985Z","iopub.execute_input":"2021-07-12T09:10:38.621353Z","iopub.status.idle":"2021-07-12T09:10:38.628291Z","shell.execute_reply.started":"2021-07-12T09:10:38.62132Z","shell.execute_reply":"2021-07-12T09:10:38.627332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(s):\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n    \nseed_everything(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:10:38.629765Z","iopub.execute_input":"2021-07-12T09:10:38.630113Z","iopub.status.idle":"2021-07-12T09:10:38.643909Z","shell.execute_reply.started":"2021-07-12T09:10:38.630079Z","shell.execute_reply":"2021-07-12T09:10:38.64311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CONFIG:\n    glove_path='../input/glove6b100dtxt/glove.6B.100d.txt'\n    glove_dim=100\n    gru_hdim=128\n    dmodel=512\n    \n    folds=5\n    batch_size=128\n    max_seq_len=150\n    eval_every=10\n    \n    learning_rate=1e-4\n    weight_decay=1e-4\n    optimizer='AdamW'\n    epochs=30\n    clip_gradient_norm=1.0\n    \n    device=torch.device( 'cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:10:38.645547Z","iopub.execute_input":"2021-07-12T09:10:38.646114Z","iopub.status.idle":"2021-07-12T09:10:38.651881Z","shell.execute_reply.started":"2021-07-12T09:10:38.646085Z","shell.execute_reply":"2021-07-12T09:10:38.651072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Normalization","metadata":{}},{"cell_type":"code","source":"train_df=pd.read_csv('../input/commonlit-kfold-dataset/fold_train.csv')\n\ntarget_mean=train_df.target.mean()\ntarget_std=train_df.target.std()\n\nprint(\"Taget Mean:\", target_mean)\nprint(\"Taget Std:\", target_std)\n\ntrain_df['normalized_target']=(train_df.target - target_mean)/target_std\nsns.histplot(data=train_df, x='normalized_target')","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:10:38.653228Z","iopub.execute_input":"2021-07-12T09:10:38.653613Z","iopub.status.idle":"2021-07-12T09:10:39.003438Z","shell.execute_reply.started":"2021-07-12T09:10:38.653588Z","shell.execute_reply":"2021-07-12T09:10:39.002722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"qs=[]\nfor i in np.arange(0.1, 1.1, 0.1):\n    q=train_df.normalized_target.quantile(i)\n    qs.append(q)\n\ndef get_quantile(target):\n    for i,q in enumerate(qs):\n        if target<=q:\n            return i\n\ntrain_df['q']=train_df.normalized_target.apply(get_quantile)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:10:39.00447Z","iopub.execute_input":"2021-07-12T09:10:39.004875Z","iopub.status.idle":"2021-07-12T09:10:39.173419Z","shell.execute_reply.started":"2021-07-12T09:10:39.004836Z","shell.execute_reply":"2021-07-12T09:10:39.172645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Glove 100-d vectors","metadata":{}},{"cell_type":"code","source":"%%time\nglove_embeddings={}\nwith open(CONFIG.glove_path) as file:\n    for line in file:\n        line=line.split()\n        word=line[0]\n        v=np.array(line[1:]).astype(np.float)\n        glove_embeddings[word]=v\nprint(len(glove_embeddings))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:10:39.174504Z","iopub.execute_input":"2021-07-12T09:10:39.174923Z","iopub.status.idle":"2021-07-12T09:11:22.470087Z","shell.execute_reply.started":"2021-07-12T09:10:39.174883Z","shell.execute_reply":"2021-07-12T09:11:22.46924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"class Tokenizer:\n    def __init__(self):\n        self.lemmatizer=WordNetLemmatizer()\n        self.nlp=English()\n    def __call__(self, doc):\n        tokens=[]\n        for token in self.nlp(doc):\n            if token.like_num or token.text=='' or (not token.is_ascii):\n                continue\n            token=token.lower_.strip()\n            for p in string.punctuation:\n                token=token.replace(p, ' ')\n            token=token.split(' ')\n            token=[w for w in token if w!='']\n            tokens+=token\n        return tokens","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:11:22.471416Z","iopub.execute_input":"2021-07-12T09:11:22.471807Z","iopub.status.idle":"2021-07-12T09:11:22.479785Z","shell.execute_reply.started":"2021-07-12T09:11:22.471767Z","shell.execute_reply":"2021-07-12T09:11:22.47863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=Tokenizer()\ntrain_df['doc']=train_df.excerpt.apply(tokenizer)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:11:22.482751Z","iopub.execute_input":"2021-07-12T09:11:22.4832Z","iopub.status.idle":"2021-07-12T09:11:31.706114Z","shell.execute_reply.started":"2021-07-12T09:11:22.483138Z","shell.execute_reply":"2021-07-12T09:11:31.705206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset and DataSamplers","metadata":{}},{"cell_type":"code","source":"class TrainDataSampler:\n    def __init__(self, batch_size, df):\n        self.qmap={}\n        self.batch_size=batch_size\n        self.batch_fraction=1.0\n        self.df=df.copy()\n        \n        for i in range(10):\n            ids=self.df[self.df.q==i].id.values\n            np.random.shuffle(ids)\n            self.qmap[i]=ids\n        \n    def convert_sentences(self, num_samples, sentences):\n        X=torch.zeros((num_samples, CONFIG.max_seq_len, CONFIG.glove_dim), dtype=torch.float32)\n        for i, doc in enumerate(sentences):\n            for j, word in enumerate(doc[:CONFIG.max_seq_len]):\n                if word in glove_embeddings:\n                    X[i][j]=torch.tensor(glove_embeddings[word])\n        return X\n    \n    def get_mbs(self):\n        y=[]\n        sentences=[]\n        for i in range(10):\n            if i not in self.qmap:\n                continue\n            yids=self.qmap[i][-12:]\n            y+=list(self.df[self.df.id.isin(yids)].normalized_target.values)\n            sentences+=list(self.df[self.df.id.isin(yids)].doc.values)\n            \n            self.qmap[i]=self.qmap[i][:-12]\n            if len(self.qmap[i]) == 0:\n                self.qmap.pop(i)\n        \n        num_samples=len(y)\n        self.batch_fraction=len(y)/self.batch_size\n        \n        X=self.convert_sentences(num_samples, sentences)\n        y=torch.tensor(y, dtype=torch.float32)\n        \n        X=X.to(CONFIG.device)\n        y=y.to(CONFIG.device)\n        return X, y\n    \n    def __iter__(self):\n        while len(self.qmap)>0:\n            X, y=self.get_mbs()\n            if self.batch_fraction < 0.5:\n                break\n            yield X, y\n    def __next__(self):\n        for i in range(10):\n            yield i","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:11:31.707539Z","iopub.execute_input":"2021-07-12T09:11:31.707825Z","iopub.status.idle":"2021-07-12T09:11:31.722507Z","shell.execute_reply.started":"2021-07-12T09:11:31.707796Z","shell.execute_reply":"2021-07-12T09:11:31.721441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, phase):\n        self.df=df\n        self.phase=phase\n    def __getitem__(self, idx):\n        row=self.df.iloc[idx]\n        doc=row.doc\n        \n        X=torch.zeros((CONFIG.max_seq_len, CONFIG.glove_dim), dtype=torch.float32)\n        for i, word in enumerate(doc[:CONFIG.max_seq_len]):\n            if word in glove_embeddings:\n                X[i]=torch.tensor(glove_embeddings[word])\n        \n        if self.phase in ['train', 'val']:\n            y=torch.tensor(row.normalized_target, dtype=torch.float32)\n            return (X.to(CONFIG.device), y.to(CONFIG.device))\n        return X.to(CONFIG.device)\n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:11:31.724326Z","iopub.execute_input":"2021-07-12T09:11:31.724873Z","iopub.status.idle":"2021-07-12T09:11:31.737739Z","shell.execute_reply.started":"2021-07-12T09:11:31.724832Z","shell.execute_reply":"2021-07-12T09:11:31.736904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class ProjectionHead(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bn=nn.BatchNorm1d(CONFIG.dmodel)\n        self.dropout=nn.Dropout()\n        self.relu=nn.ReLU()\n        self.out_layer=nn.Linear(CONFIG.dmodel, 1)\n    def forward(self, x):\n        x=self.bn(x)\n        x=self.dropout(x)\n        x=self.relu(x)\n        x=self.out_layer(x)\n        return x\n\nclass AttentionAggregation(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear=nn.Linear(CONFIG.dmodel, 1)\n    def forward(self, x):\n        w=self.linear(x)\n        w=softmax(w, dim=1)\n        x=x.permute(0, 2, 1)\n        x=torch.matmul(x, w).squeeze(-1)\n        return x\n\nclass FFN(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.linear=nn.Linear(in_dim, out_dim)\n        self.relu=nn.ReLU()\n        self.dropout=nn.Dropout(0.2)\n        self.layer_norm=nn.LayerNorm( (CONFIG.max_seq_len,out_dim) )\n    def forward(self, x):\n        x=self.linear(x)\n        x=self.layer_norm(x)\n        x=self.dropout(x)\n        x=self.relu(x)\n        return x\n    \nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.gru=nn.GRU(CONFIG.glove_dim, CONFIG.gru_hdim, num_layers=2,\n                         bidirectional=True,batch_first=True, dropout=0.25)\n        self.layernorm=nn.LayerNorm((CONFIG.max_seq_len, 2*CONFIG.gru_hdim))\n        self.relu=nn.ReLU()\n        self.ffn=FFN(2*CONFIG.gru_hdim, CONFIG.dmodel)\n        self.attn_agg=AttentionAggregation()\n        self.projection_head=ProjectionHead()\n        \n    def forward(self, x):\n        batch_size=x.shape[0]\n        x,_ = self.gru(x)\n        x=self.layernorm(x)\n        x=self.relu(x)\n        x=self.ffn(x)\n        x=self.attn_agg(x)\n        x=self.projection_head(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:11:31.7395Z","iopub.execute_input":"2021-07-12T09:11:31.740055Z","iopub.status.idle":"2021-07-12T09:11:31.758063Z","shell.execute_reply.started":"2021-07-12T09:11:31.740015Z","shell.execute_reply":"2021-07-12T09:11:31.756864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, fold_train_df, train_dataloader, val_dataloader):\n        self.model=model\n        self.max_iter_count=500\n        #self.swa_model=AveragedModel(model, device=CONFIG.device)\n        \n        self.criterion=nn.MSELoss(reduction='mean')\n        self.fold_train_df=fold_train_df\n        \n        self.train_dataloader=train_dataloader\n        self.val_dataloader=val_dataloader\n        self.optimizer=torch.optim.AdamW(model.parameters(), lr=CONFIG.learning_rate, weight_decay=CONFIG.weight_decay)\n        self.schedular=torch.optim.lr_scheduler.OneCycleLR(self.optimizer,\n                                                           max_lr=CONFIG.learning_rate,\n                                                           total_steps=self.max_iter_count)\n                                                           #epochs=CONFIG.epochs,\n                                                           #steps_per_epoch=len(self.train_dataloader))\n        self.iter_count=0\n        self.best_loss=None\n        self.best_iteration=None\n        self.train_loss_=[]\n        self.train_batch_stdv_=[]\n        self.batch_skew=[]\n        self.val_loss_=[]\n        #self.swa_start=500\n        \n    def evaluate(self):\n        self.model.eval()\n        ytrue=[]; ypred=[];\n        for X, y in self.val_dataloader:\n            X=X.to(CONFIG.device)\n            ytrue+=y.view(-1).tolist()\n            with torch.no_grad():\n                yhat=self.model(X).view(-1).detach().cpu()\n                ypred+=yhat.tolist()\n        ytrue=torch.tensor(ytrue, dtype=torch.float32)\n        ypred=torch.tensor(ypred, dtype=torch.float32)\n        return self.criterion(ypred, ytrue).item()\n    \n    def checkpoint(self, val_loss):\n        if self.best_loss is None or self.best_loss > val_loss:\n            torch.save(self.model, 'best_model.pt')\n            #torch.save(model, 'model_{}'.format(self.iter_count))\n            self.best_loss=val_loss\n            self.best_iteration=self.iter_count\n    \n    def train_ops(self, X, y):\n        self.model.train()\n        \n        self.optimizer.zero_grad()\n        y_hat=self.model(X).view(-1)\n        \n        loss=self.criterion(y_hat, y)\n        loss.backward()\n        \n        self.optimizer.step()\n        self.schedular.step()\n        \n        return loss.item()\n    \n    def train(self):\n        \n        t1=time.time()\n        for _ in range(CONFIG.epochs):\n            if self.iter_count > self.max_iter_count:\n                break\n            for mbs in TrainDataSampler(120, self.fold_train_df):\n                X, y=mbs\n                self.iter_count+=1\n                if self.iter_count > self.max_iter_count:\n                    break\n                X=X.to(CONFIG.device)\n                y=y.to(CONFIG.device)\n                \n                self.train_batch_stdv_.append(np.std(y.view(-1).tolist()))\n                self.batch_skew.append(skew(y.view(-1).tolist()))\n                \n                train_loss=self.train_ops(X, y)\n                self.train_loss_.append(train_loss)\n                \n                if self.iter_count%CONFIG.eval_every==0:\n                    val_loss=self.evaluate()\n                    self.val_loss_.append(val_loss)\n                    self.checkpoint(val_loss)\n                    t2=time.time()\n                    print(\"======\"*10)\n                    print()\n                    print(\"Iteration:{} | Time Taken: {:.2f} | Train Loss:{:.3f}\".format(self.iter_count, (t2-t1)/60, self.train_loss_[-1]))\n                    print(\"Val Loss:{:.4f} | Best Loss:{:.4f} | Best Iteration:{}\".format(val_loss,self.best_loss,self.best_iteration))\n                    \n                    #print()\n                    #print(\"======\"*10)\n                    #print(\"Batch Stats:\")\n                    #print(\"Last 3 batch std: \", self.train_batch_stdv_[-3:])\n                    #print(\"Last 3 batch skew: \", self.batch_skew[-3:])\n                    \n                    #print(\"Last 6 Avg std:\", np.mean(self.train_batch_stdv_[-6:]))\n                    #print(\"Last 6 Avg skew:\", np.mean(self.batch_skew[-6:]))\n                    \n                    #print(\"Val Loss:{:.3f}\".format(val_loss))\n                    t1=time.time()\n        #torch.optim.swa_utils.update_bn(self.train_dataloader, self.swa_model)\n        #torch.save(self.swa_model, 'swa_model.pt')\n                    \n    def lr_range_test(self):\n        self.model.train()\n        lrs=[];losses=[]\n        min_lr=5e-5;max_lr=1e-3;\n        optimizer=torch.optim.AdamW(model.parameters(), lr=min_lr, weight_decay=CONFIG.weight_decay)\n        mse_loss=nn.MSELoss(reduction='mean')\n        schedular=torch.optim.lr_scheduler.StepLR(optimizer, 1, 1.05)\n\n        lrs=[]\n        losses=[]\n\n        for i in range(10):\n            print('Epoch:', i+1, schedular.get_last_lr())\n            #for j, (X, y) in enumerate(self.train_dataloader):\n            for X, y in TrainDataSampler(120, self.fold_train_df):\n                y_hat=self.model(X).view(-1)\n                loss=mse_loss(y_hat, y)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                schedular.step()\n\n                lrs+=schedular.get_last_lr()\n                losses.append(loss.item())\n                if lrs[-1]>max_lr:\n                    break\n            if lrs[-1]>max_lr:\n                break\n        return lrs, losses","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:23:02.051137Z","iopub.execute_input":"2021-07-12T09:23:02.051664Z","iopub.status.idle":"2021-07-12T09:23:02.085199Z","shell.execute_reply.started":"2021-07-12T09:23:02.05162Z","shell.execute_reply":"2021-07-12T09:23:02.084009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model=Model()\n#model=model.to(CONFIG.device)\n#trainer=Trainer(model, fold_train_df, train_dataloader, val_dataloader)\n\n#lrs, losses=trainer.lr_range_test()\n#plt.plot(lrs, losses)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:11:31.789235Z","iopub.execute_input":"2021-07-12T09:11:31.789615Z","iopub.status.idle":"2021-07-12T09:11:31.797978Z","shell.execute_reply.started":"2021-07-12T09:11:31.789576Z","shell.execute_reply":"2021-07-12T09:11:31.797228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models=[]\nfor i in range(5):\n    print()\n    print()\n    print(\"===\"*10)\n    print(\"Fold:{}\".format(i))\n    print()\n    fold_train_df=train_df[train_df.kfold!=i].copy()\n    fold_val_df=train_df[train_df.kfold==i].copy()\n\n    train_dataset=Dataset(fold_train_df, 'train')\n    train_dataloader=torch.utils.data.DataLoader(train_dataset, \n                                                 #batch_size=CONFIG.batch_size,\n                                                 batch_size=120,\n                                                 shuffle=True)\n\n    val_dataset=Dataset(fold_val_df, 'val')\n    val_dataloader=torch.utils.data.DataLoader(val_dataset, batch_size=CONFIG.batch_size, shuffle=False)\n\n\n    #Model Instance \n    model=Model()\n    model=model.to(CONFIG.device)\n    \n    #Trainer Instance\n    trainer=Trainer(model, fold_train_df, train_dataloader, val_dataloader)\n    trainer.train()\n    \n    model=torch.load('./best_model.pt')\n    models.append(model)\n    \n    plt.title(\"Train Loss- Fold:{}\".format(i))\n    plt.plot(trainer.train_loss_)\n    plt.show()\n    \n    plt.title(\"Val Loss- Fold:{}\".format(i))\n    plt.plot(trainer.val_loss_)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:23:04.934501Z","iopub.execute_input":"2021-07-12T09:23:04.934847Z","iopub.status.idle":"2021-07-12T09:24:48.937Z","shell.execute_reply.started":"2021-07-12T09:23:04.934816Z","shell.execute_reply":"2021-07-12T09:24:48.936072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, model in enumerate(models):\n    torch.save(model, \"model_{}.pt\".format(i))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:31:05.422766Z","iopub.execute_input":"2021-07-12T09:31:05.423156Z","iopub.status.idle":"2021-07-12T09:31:05.468011Z","shell.execute_reply.started":"2021-07-12T09:31:05.423108Z","shell.execute_reply":"2021-07-12T09:31:05.467131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ytrue=[]; ypred=[];\n\nfor X, y in val_dataloader:\n    X=X.to(CONFIG.device)\n    ytrue+=y.view(-1).tolist()\n    \n    yhat=np.zeros(y.size(0))\n    for model in models:\n        model.eval()\n        with torch.no_grad():\n            yhat+=model(X).view(-1).detach().cpu().numpy()\n    \n    yhat/=len(models)\n    ypred+=list(yhat)\n\nytrue=torch.tensor(ytrue, dtype=torch.float32)\nypred=torch.tensor(ypred, dtype=torch.float32)\nprint(nn.MSELoss(reduction='mean')(ypred, ytrue).item())","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:40:10.954219Z","iopub.execute_input":"2021-07-12T09:40:10.954677Z","iopub.status.idle":"2021-07-12T09:40:20.574414Z","shell.execute_reply.started":"2021-07-12T09:40:10.954647Z","shell.execute_reply":"2021-07-12T09:40:20.573737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def infer(models, dataloader):\n    preds=[]\n    for X in dataloader:\n        X=X.to(CONFIG.device)\n        y_hat=torch.zeros(X.shape[0])\n        for model in models:\n            model.eval()\n            with torch.no_grad():\n                y=model(X).view(-1).detach().cpu()\n                y_hat+=(target_std*y) + target_mean\n        preds+=list(y_hat.numpy()/len(models))\n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:41:05.049226Z","iopub.execute_input":"2021-07-12T09:41:05.049583Z","iopub.status.idle":"2021-07-12T09:41:05.056323Z","shell.execute_reply.started":"2021-07-12T09:41:05.049555Z","shell.execute_reply":"2021-07-12T09:41:05.055346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_train_dataset=Dataset(train_df, 'test')\ninfer_train_dataloader=torch.utils.data.DataLoader(infer_train_dataset, batch_size=200, shuffle=False)\ntrain_df['preds'] = infer(models, infer_train_dataloader)\ntrain_df[['id', 'target', 'normalized_target', 'preds']].head()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:41:07.793415Z","iopub.execute_input":"2021-07-12T09:41:07.793745Z","iopub.status.idle":"2021-07-12T09:41:59.328235Z","shell.execute_reply.started":"2021-07-12T09:41:07.793717Z","shell.execute_reply":"2021-07-12T09:41:59.327235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(np.sqrt((train_df.preds-train_df.target)**2)).mean()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:41:59.329713Z","iopub.execute_input":"2021-07-12T09:41:59.330267Z","iopub.status.idle":"2021-07-12T09:41:59.338189Z","shell.execute_reply.started":"2021-07-12T09:41:59.330225Z","shell.execute_reply":"2021-07-12T09:41:59.337342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, ax=plt.subplots(2, 1)\nsns.boxplot(data=train_df, x='target', ax=ax[0])\nsns.boxplot(data=train_df, x='preds', ax=ax[1])","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:41:59.339881Z","iopub.execute_input":"2021-07-12T09:41:59.340291Z","iopub.status.idle":"2021-07-12T09:41:59.534646Z","shell.execute_reply.started":"2021-07-12T09:41:59.340251Z","shell.execute_reply":"2021-07-12T09:41:59.533508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.histplot(train_df, x='target', bins=100, color='red')\nsns.histplot(train_df, x='preds', bins=100,)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:41:59.537088Z","iopub.execute_input":"2021-07-12T09:41:59.537634Z","iopub.status.idle":"2021-07-12T09:42:00.063599Z","shell.execute_reply.started":"2021-07-12T09:41:59.537603Z","shell.execute_reply":"2021-07-12T09:42:00.062868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv('train_with_preds.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:00.433716Z","iopub.execute_input":"2021-07-12T09:42:00.433982Z","iopub.status.idle":"2021-07-12T09:42:00.782374Z","shell.execute_reply.started":"2021-07-12T09:42:00.433958Z","shell.execute_reply":"2021-07-12T09:42:00.78123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test_df=pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ntest_df['doc']=test_df.excerpt.apply(tokenizer)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:00.783932Z","iopub.execute_input":"2021-07-12T09:42:00.784391Z","iopub.status.idle":"2021-07-12T09:42:00.83946Z","shell.execute_reply.started":"2021-07-12T09:42:00.784344Z","shell.execute_reply":"2021-07-12T09:42:00.838488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_test_dataset=Dataset(test_df, 'test')\ninfer_test_dataloader=torch.utils.data.DataLoader(infer_test_dataset, batch_size=200, shuffle=False)\ntest_df['target'] = infer(models, infer_test_dataloader)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:00.841363Z","iopub.execute_input":"2021-07-12T09:42:00.841635Z","iopub.status.idle":"2021-07-12T09:42:01.116136Z","shell.execute_reply.started":"2021-07-12T09:42:00.841608Z","shell.execute_reply":"2021-07-12T09:42:01.11517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df=test_df[['id', 'target']].copy()\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:01.117467Z","iopub.execute_input":"2021-07-12T09:42:01.117749Z","iopub.status.idle":"2021-07-12T09:42:01.128157Z","shell.execute_reply.started":"2021-07-12T09:42:01.117721Z","shell.execute_reply":"2021-07-12T09:42:01.127177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.tail()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:01.129366Z","iopub.execute_input":"2021-07-12T09:42:01.129647Z","iopub.status.idle":"2021-07-12T09:42:01.144378Z","shell.execute_reply.started":"2021-07-12T09:42:01.12962Z","shell.execute_reply":"2021-07-12T09:42:01.143254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T09:42:01.145614Z","iopub.execute_input":"2021-07-12T09:42:01.146006Z","iopub.status.idle":"2021-07-12T09:42:01.154111Z","shell.execute_reply.started":"2021-07-12T09:42:01.14597Z","shell.execute_reply":"2021-07-12T09:42:01.153179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}