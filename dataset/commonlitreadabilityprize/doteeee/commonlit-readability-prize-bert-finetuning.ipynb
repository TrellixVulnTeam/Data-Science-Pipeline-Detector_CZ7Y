{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport torch\nimport torch.nn as nn\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-10T16:09:10.748357Z","iopub.execute_input":"2021-07-10T16:09:10.748923Z","iopub.status.idle":"2021-07-10T16:09:13.946248Z","shell.execute_reply.started":"2021-07-10T16:09:10.748815Z","shell.execute_reply":"2021-07-10T16:09:13.945369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything():\n    np.random.seed(42)\n    torch.manual_seed(42)\n    torch.cuda.manual_seed(42)\n    torch.cuda.manual_seed_all(42)\n    \nseed_everything()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T16:12:32.880624Z","iopub.execute_input":"2021-07-10T16:12:32.881163Z","iopub.status.idle":"2021-07-10T16:12:32.889955Z","shell.execute_reply.started":"2021-07-10T16:12:32.881119Z","shell.execute_reply":"2021-07-10T16:12:32.88893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of the ideas are taken from the below notebook\n\nhttps://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit\n","metadata":{}},{"cell_type":"code","source":"class CONFIG:\n    checkpoint='bert-base-uncased'\n    tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n    bert_config=AutoConfig.from_pretrained(checkpoint)\n    bert_model=AutoModel.from_pretrained(checkpoint)\n    \n    hidden_size=bert_config.hidden_size\n    pad_token_id=tokenizer.pad_token_id\n    max_seq_len=tokenizer.model_max_length\n    \n    batch_size=16\n    folds=5\n    learning_rate=1e-5\n    weight_decay=1e-2\n    optimizer='AdamW'\n    epochs=8\n    clip_gradient_norm=1.0\n    eval_every=60\n    \n    device=torch.device( 'cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T16:12:34.248935Z","iopub.execute_input":"2021-07-10T16:12:34.24929Z","iopub.status.idle":"2021-07-10T16:13:09.510833Z","shell.execute_reply.started":"2021-07-10T16:12:34.249261Z","shell.execute_reply":"2021-07-10T16:13:09.509704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('../input/commonlit-kfold-dataset/fold_train.csv')\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T09:22:45.31305Z","iopub.execute_input":"2021-07-10T09:22:45.313446Z","iopub.status.idle":"2021-07-10T09:22:45.459702Z","shell.execute_reply.started":"2021-07-10T09:22:45.313413Z","shell.execute_reply":"2021-07-10T09:22:45.458493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG.tokenizer.save_pretrained('bert_tokenizer')","metadata":{"execution":{"iopub.status.busy":"2021-06-28T05:28:36.692861Z","iopub.execute_input":"2021-06-28T05:28:36.693147Z","iopub.status.idle":"2021-06-28T05:28:36.737802Z","shell.execute_reply.started":"2021-06-28T05:28:36.69311Z","shell.execute_reply":"2021-06-28T05:28:36.736577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.excerpts=df.excerpt.values\n        self.targets=df.target.values\n        self.tokenizer=CONFIG.tokenizer\n        self.pad_token_id=CONFIG.pad_token_id\n        self.max_seq_len=CONFIG.max_seq_len\n    \n    def get_tokenized_features(self, excerpt):\n        inputs=self.tokenizer(excerpt, truncation=True)\n        \n        input_ids=inputs['input_ids']\n        attention_mask=inputs['attention_mask']\n        token_type_ids=inputs['token_type_ids']\n        \n        input_len=len(input_ids)\n        pad_len=self.max_seq_len-input_len\n        input_ids+=[self.pad_token_id]*pad_len\n        attention_mask+=[0]*pad_len\n        token_type_ids+=[0]*pad_len\n        \n        return {\n            'seq_len': input_len,\n            'input_ids': input_ids,\n            'token_type_ids': token_type_ids,\n            'attention_mask': attention_mask\n        }\n        \n    def __getitem__(self, idx):\n        excerpt=self.excerpts[idx]\n        target=self.targets[idx]\n        features=self.get_tokenized_features(excerpt)\n        \n        return {\n            'seq_len': features['seq_len'],\n            'input_ids': torch.tensor(features['input_ids'], dtype=torch.long),\n            'token_type_ids': torch.tensor(features['token_type_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(features['attention_mask'], dtype=torch.long),\n            'labels': torch.tensor(target, dtype=torch.float32)\n        }\n    \n    def __len__(self):\n        return len(self.targets)","metadata":{"execution":{"iopub.status.busy":"2021-06-28T05:28:36.73948Z","iopub.execute_input":"2021-06-28T05:28:36.739975Z","iopub.status.idle":"2021-06-28T05:28:36.751274Z","shell.execute_reply.started":"2021-06-28T05:28:36.739932Z","shell.execute_reply":"2021-06-28T05:28:36.74997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def freeze_bert_layers(bert_model):\n    max_freeze_layer=5\n    for n,p in bert_model.named_parameters():\n        if ('embedding' in n) or ('layer' in n and int(n.split('.')[2]) <= max_freeze_layer):\n            p.requires_grad=False","metadata":{"execution":{"iopub.status.busy":"2021-07-10T16:14:26.679481Z","iopub.execute_input":"2021-07-10T16:14:26.680145Z","iopub.status.idle":"2021-07-10T16:14:26.686519Z","shell.execute_reply.started":"2021-07-10T16:14:26.680089Z","shell.execute_reply":"2021-07-10T16:14:26.685474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class ProjectionHead(nn.Module):\n    def __init__(self):\n        super(ProjectionHead, self).__init__()\n        self.linear1=nn.Linear(CONFIG.hidden_size, 2*CONFIG.hidden_size)\n        self.bn=nn.BatchNorm1d(2*CONFIG.hidden_size)\n        self.relu=nn.ReLU()\n        self.dropout=nn.Dropout(0.3)\n        self.out=nn.Linear(2*CONFIG.hidden_size, 1)\n    def forward(self, x):\n        x=self.linear1(x)\n        x=self.bn(x)\n        x=self.relu(x)\n        x=self.dropout(x)\n        x=self.out(x)\n        return x\n    \nclass CommonLitModel(nn.Module):\n    def __init__(self):\n        super(CommonLitModel, self).__init__()\n        self.bert=CONFIG.bert_model\n        freeze_bert_layers(self.bert)\n        \n        self.layer_norm=nn.LayerNorm(CONFIG.hidden_size)\n        self.relu=nn.ReLU()\n        self.dropout=nn.Dropout(0.4)\n        \n        self.proj_head=ProjectionHead()\n    def forward(self, input_ids, attention_mask, token_type_ids,\n                output_hidden_states=False):\n        bert_output=self.bert(input_ids,\n                              attention_mask=attention_mask,\n                              token_type_ids=token_type_ids,\n                              output_hidden_states=output_hidden_states)\n        \n        bert_pooler_output=bert_output.pooler_output\n        bert_pooler_output=self.layer_norm(bert_pooler_output)\n        bert_pooler_output=self.relu(bert_pooler_output)\n        bert_pooler_output=self.dropout(bert_pooler_output)\n        \n        y=self.proj_head(bert_pooler_output)\n        return y","metadata":{"execution":{"iopub.status.busy":"2021-07-10T16:15:50.795092Z","iopub.execute_input":"2021-07-10T16:15:50.79577Z","iopub.status.idle":"2021-07-10T16:15:50.808812Z","shell.execute_reply.started":"2021-07-10T16:15:50.79571Z","shell.execute_reply":"2021-07-10T16:15:50.807991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, train_dataloader, test_dataloader):\n        self.model=model\n        self.optimizer=torch.optim.AdamW(model.parameters(),\n                                         lr=CONFIG.learning_rate,\n                                         weight_decay=CONFIG.weight_decay)\n        self.schedular=torch.optim.lr_scheduler.OneCycleLR(self.optimizer, \n                                                           max_lr=CONFIG.learning_rate,\n                                                           epochs=CONFIG.epochs,\n                                                           steps_per_epoch=len(train_dataloader))\n        self.criterion=nn.MSELoss()\n        self.train_dataloader=train_dataloader\n        self.test_dataloader=test_dataloader\n        self.train_loss_=[]\n        self.val_loss_=[]\n        self.best_loss=None\n        self.iter_count=0\n        self.best_iter=0\n        \n    def train_ops(self, y, ypred):\n        self.optimizer.zero_grad()\n        loss=self.criterion(ypred, y)\n        loss.backward()\n        self.optimizer.step()\n        self.schedular.step()\n        return loss\n    \n    \n    def evaluate(self):\n        ytrue=[]\n        ypred=[]\n        self.model.eval()\n        for batch in self.test_dataloader:\n            batch_seq_lens=batch['seq_len']\n            batch_max_seq_len=torch.max(batch['seq_len'])\n            \n            input_ids=batch['input_ids'][:, :batch_max_seq_len].to(CONFIG.device)\n            attention_mask=batch['attention_mask'][:, :batch_max_seq_len].to(CONFIG.device)\n            token_type_ids=batch['token_type_ids'][:, :batch_max_seq_len].to(CONFIG.device)\n            labels=batch['labels']\n            \n            ytrue+=labels.tolist()\n            with torch.no_grad():\n                yhat=self.model(input_ids, attention_mask, token_type_ids).view(-1).detach().cpu()\n                ypred+=yhat.tolist()\n        ytrue=torch.tensor(ytrue, dtype=torch.float32)\n        ypred=torch.tensor(ypred, dtype=torch.float32)\n        val_loss=self.criterion(ypred, ytrue)\n        return val_loss.item()\n    \n    def train_epoch(self):\n        t1=time.time()\n        self.model.train()\n        for batch_id, batch in enumerate(self.train_dataloader):\n            self.iter_count+=1\n            if self.iter_count%CONFIG.eval_every==0:\n                val_loss=self.evaluate()\n                self.val_loss_.append(val_loss)\n                if (self.best_loss is None) or (self.best_loss > val_loss):\n                    self.best_loss=val_loss\n                    self.best_iter=self.iter_count\n                    torch.save(self.model, \"best_model.pt\")\n                    \n                torch.save(self.model, \"model_{}.pt\".format(self.iter_count))\n                print(\"===\"*10)\n                print(\"Iteration:{} | ValLoss:{:.4f} | BestIteration:{}\".format(self.iter_count, val_loss, self.best_iter))\n            \n            batch_seq_lens=batch['seq_len']\n            batch_max_seq_len=torch.max(batch['seq_len'])\n            \n            input_ids=batch['input_ids'][:, :batch_max_seq_len].to(CONFIG.device)\n            attention_mask=batch['attention_mask'][:, :batch_max_seq_len].to(CONFIG.device)\n            token_type_ids=batch['token_type_ids'][:, :batch_max_seq_len].to(CONFIG.device)\n            labels=batch['labels'].to(CONFIG.device)\n            \n            ypred=self.model(input_ids, attention_mask, token_type_ids).view(-1)\n            loss=self.train_ops(labels, ypred)\n            self.train_loss_.append(loss.item())\n            \n    def train(self):\n        for e in range(CONFIG.epochs):\n            self.train_epoch()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T05:28:36.783455Z","iopub.execute_input":"2021-06-28T05:28:36.783845Z","iopub.status.idle":"2021-06-28T05:28:36.802536Z","shell.execute_reply.started":"2021-06-28T05:28:36.783809Z","shell.execute_reply":"2021-06-28T05:28:36.801702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=CommonLitModel()\nmodel=model.to(CONFIG.device)\n\n\nfold_train_df=train_df[train_df.kfold!=0].copy()\nfold_val_df  =train_df[train_df.kfold==0].copy()\n\ntrain_dataset=CommonLitDataset(fold_train_df)\nval_dataset=CommonLitDataset(fold_val_df)\n\ntrain_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=CONFIG.batch_size,\n                                             shuffle=True, pin_memory=True, drop_last=False)\n\n\nval_dataloader=torch.utils.data.DataLoader(val_dataset, batch_size=CONFIG.batch_size,\n                                             shuffle=False, pin_memory=True, drop_last=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-28T05:28:36.803888Z","iopub.execute_input":"2021-06-28T05:28:36.804184Z","iopub.status.idle":"2021-06-28T05:28:36.828435Z","shell.execute_reply.started":"2021-06-28T05:28:36.80416Z","shell.execute_reply":"2021-06-28T05:28:36.827735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer=Trainer(model, train_dataloader, val_dataloader)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-06-28T05:28:36.829715Z","iopub.execute_input":"2021-06-28T05:28:36.83007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.train_loss_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.val_loss_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}