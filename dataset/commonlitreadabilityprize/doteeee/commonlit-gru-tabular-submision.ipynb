{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \"../input/textstat/Pyphen-0.10.0-py3-none-any.whl\"\n!pip install \"../input/textstat/textstat-0.7.0-py3-none-any.whl\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-27T11:28:30.876019Z","iopub.execute_input":"2021-05-27T11:28:30.876778Z","iopub.status.idle":"2021-05-27T11:29:25.510508Z","shell.execute_reply.started":"2021-05-27T11:28:30.876643Z","shell.execute_reply":"2021-05-27T11:29:25.509505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport time\nimport string\nimport pickle\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\nimport textstat\n\nimport torch\nimport torch.nn as nn\n\nimport seaborn as sns\n\nfrom nltk.stem import WordNetLemmatizer\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.preprocessing import StandardScaler\n\nfrom collections import defaultdict","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:29:25.512275Z","iopub.execute_input":"2021-05-27T11:29:25.51257Z","iopub.status.idle":"2021-05-27T11:29:28.923536Z","shell.execute_reply.started":"2021-05-27T11:29:25.51254Z","shell.execute_reply":"2021-05-27T11:29:28.922732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp=English()\nstop_words=nlp.Defaults.stop_words\n\nnlp.add_pipe(nlp.create_pipe('sentencizer'))\nprint(nlp.pipe_names)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:29:28.925029Z","iopub.execute_input":"2021-05-27T11:29:28.925474Z","iopub.status.idle":"2021-05-27T11:29:29.376941Z","shell.execute_reply.started":"2021-05-27T11:29:28.925443Z","shell.execute_reply":"2021-05-27T11:29:29.375897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:29:29.378522Z","iopub.execute_input":"2021-05-27T11:29:29.378859Z","iopub.status.idle":"2021-05-27T11:29:29.507598Z","shell.execute_reply.started":"2021-05-27T11:29:29.378828Z","shell.execute_reply":"2021-05-27T11:29:29.506565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_feature_columns=[\n    'sentence_count', 'word_count', 'distinct_word_count', 'words_per_sentence',\n    'stopword_count', 'word_count_without_stopword', 'distinct_word_count_without_stopword',\n    'title_words_count', 'distinct_title_words_count','title_word_per_sentence',\n    'word_redundancy', 'stopword_redundancy', 'word_redundancy_witout_stopwords','words_per_punctuation',\n    '0syllable_no_stop', '0syllable_no_stop_proportion','1syllable_no_stop', '1syllable_no_stop_proportion', \n    '2syllable_no_stop', '2syllable_no_stop_proportion','3syllable_no_stop','3syllable_no_stop_proportion', \n    '4syllable_no_stop','4syllable_no_stop_proportion','>=5syllable_no_stop', '>=5syllable_no_stop_proportion'\n]","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:34:15.703022Z","iopub.execute_input":"2021-05-27T11:34:15.703411Z","iopub.status.idle":"2021-05-27T11:34:15.709655Z","shell.execute_reply.started":"2021-05-27T11:34:15.703377Z","shell.execute_reply":"2021-05-27T11:34:15.70813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Functions","metadata":{}},{"cell_type":"code","source":"def get_sentence_count(excerpt):\n    return len( list(nlp(excerpt).sents) )\n\ndef get_word_count(excerpt):\n    cnt=0\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct:\n            continue\n        cnt+=1\n    return cnt\n    \ndef get_unique_word_count(excerpt):\n    word_set=set()\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct:\n            continue\n        word_set.add(token.lower_)\n    return len(word_set)\n\ndef get_word_count_without_stopword(excerpt):\n    cnt=0\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct or token.is_stop:\n            continue\n        cnt+=1\n    return cnt\n\ndef get_distinct_word_count_without_stopword(excerpt):\n    words=set()\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct or token.is_stop:\n            continue\n        words.add(token.lower_)\n    return len(words)\n\n\ndef get_stopword_count(excerpt):\n    cnt=0\n    for word in nlp(excerpt):\n        if word.is_stop:\n            cnt+=1\n    return cnt\n\ndef get_unique_stopword_count(excerpt):\n    word_set=set()\n    for word in nlp(excerpt):\n        if word.is_stop:\n            word_set.add(word)\n    return len(word_set)\n    \ndef get_punctuation_count(excerpt):\n    cnt=0\n    for word in nlp(excerpt):\n        if word.is_punct:\n            cnt+=1\n    return cnt\n\ndef get_title_word_count(excerpt):\n    cnt=0\n    for word in nlp(excerpt):\n        if word.is_title:\n            cnt+=1\n    return cnt\n\ndef get_unique_title_word_count(excerpt):\n    words=set()\n    for word in nlp(excerpt):\n        if word.is_title:\n            words.add(word.text)\n    return len(words)\n\ndef get_capital_word_count(excerpt):\n    cnt=0\n    for word in nlp(excerpt):\n        if word.is_upper:\n            cnt+=1\n    return cnt\n\ndef get_unique_capital_word_count(excerpt):\n    words=set()\n    for word in nlp(excerpt):\n        if word.is_upper:\n            words.add(word)\n    return len(words)\n\ndef get_syllable_counts(excerpt):\n    syllabel_freq=defaultdict(int)\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct:\n            continue\n        syllabel_freq[ textstat.syllable_count(token.text) ]+=1\n    return syllabel_freq\n\ndef get_syllable_count_without_stop(excerpt):\n    syllabel_freq=defaultdict(int)\n    for token in nlp(excerpt):\n        if token.like_num or token.is_punct or token.is_stop:\n            continue\n        syllabel_freq[ textstat.syllable_count(token.text) ]+=1\n    return syllabel_freq","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:29:29.516311Z","iopub.execute_input":"2021-05-27T11:29:29.516609Z","iopub.status.idle":"2021-05-27T11:29:29.537454Z","shell.execute_reply.started":"2021-05-27T11:29:29.51658Z","shell.execute_reply":"2021-05-27T11:29:29.53606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_hand_engineered_feautures(df):\n    df['sentence_count']=df.excerpt.apply(get_sentence_count)\n    \n    df['word_count']=df.excerpt.apply(get_word_count)\n    df['distinct_word_count']=df.excerpt.apply(get_unique_word_count)\n    \n    df['stopword_count']=df.excerpt.apply(get_stopword_count)\n    df['distinct_stopword_count']=df.excerpt.apply(get_unique_stopword_count)\n\n    df['word_count_without_stopword']=df.excerpt.apply(get_word_count_without_stopword)\n    df['distinct_word_count_without_stopword']=df.excerpt.apply(get_distinct_word_count_without_stopword)\n\n    df['punctuation_counts']=df.excerpt.apply(get_punctuation_count)\n    df['title_words_count']=df.excerpt.apply(get_title_word_count)\n    df['distinct_title_words_count']=df.excerpt.apply(get_unique_title_word_count)\n\n    df['capital_word_count']=df.excerpt.apply(get_capital_word_count)\n    df['distinct_capital_word_count']=df.excerpt.apply(get_unique_capital_word_count)\n\n    df['word_redundancy']=1-(df['distinct_word_count'].div(df['word_count']))\n    df['stopword_redundancy']=1-(df['stopword_count'].div(df['word_count']))\n    df['word_redundancy_witout_stopwords']=1-(df['distinct_word_count_without_stopword'].div(df['word_count_without_stopword']))\n\n    df['title_word_proportion']=df['title_words_count'].div(df['word_count'])\n    df['title_word_per_sentence']=df['title_words_count'].div(df['sentence_count'])\n\n    df['capital_word_proportaion']=df['capital_word_count'].div(df['word_count'])\n\n    df['words_per_punctuation']=df['word_count'].div(df.punctuation_counts)\n    df['words_per_sentence']=df['word_count'].div(df['sentence_count'])\n\n    df['syllable_freq']=df.excerpt.apply(get_syllable_counts)\n    df['syllable_without_stop_freq']=df.excerpt.apply(get_syllable_count_without_stop)\n\n    df['0syllable']=df.syllable_freq.apply(lambda x: x[0])\n    df['0syllable_proportion']=df['0syllable'].div(df['word_count'])\n\n    df['1syllable']=df.syllable_freq.apply(lambda x: x[1])\n    df['1syllable_proportion']=df['1syllable'].div(df['word_count'])\n\n    df['2syllable']=df.syllable_freq.apply(lambda x: x[2])\n    df['2syllable_proportion']=df['2syllable'].div(df['word_count'])\n\n    df['3syllable']=df.syllable_freq.apply(lambda x: x[3])\n    df['3syllable_proportion']=df['3syllable'].div(df['word_count'])\n\n    df['4syllable']=df.syllable_freq.apply(lambda x: x[4])\n    df['4syllable_proportion']=df['4syllable'].div(df['word_count'])\n\n    df['>=5syllable']=df.syllable_freq.apply(lambda x: sum(x.values()) - x[0]- x[1]- x[2]- x[3]- x[4] )\n    df['>=5syllable_proportion']=df['>=5syllable'].div(df['word_count'])\n\n    df['0syllable_no_stop']=df.syllable_without_stop_freq.apply(lambda x: x[0])\n    df['0syllable_no_stop_proportion']=df['0syllable_no_stop'].div(df['word_count_without_stopword'])\n\n    df['1syllable_no_stop']=df.syllable_without_stop_freq.apply(lambda x: x[1])\n    df['1syllable_no_stop_proportion']=df['1syllable_no_stop'].div(df['word_count_without_stopword'])\n\n    df['2syllable_no_stop']=df.syllable_without_stop_freq.apply(lambda x: x[2])\n    df['2syllable_no_stop_proportion']=df['2syllable_no_stop'].div(df['word_count_without_stopword'])\n\n    df['3syllable_no_stop']=df.syllable_without_stop_freq.apply(lambda x: x[3])\n    df['3syllable_no_stop_proportion']=df['3syllable_no_stop'].div(df['word_count_without_stopword'])\n\n    df['4syllable_no_stop']=df.syllable_without_stop_freq.apply(lambda x: x[4])\n    df['4syllable_no_stop_proportion']=df['4syllable_no_stop'].div(df['word_count_without_stopword'])\n\n    df['>=5syllable_no_stop']=df.syllable_without_stop_freq.apply(lambda x: sum(x.values()) - x[0]- x[1]- x[2]- x[3]- x[4] )\n    df['>=5syllable_no_stop_proportion']=df['>=5syllable_no_stop'].div(df['word_count_without_stopword'])\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:29:29.539115Z","iopub.execute_input":"2021-05-27T11:29:29.539633Z","iopub.status.idle":"2021-05-27T11:29:29.565882Z","shell.execute_reply.started":"2021-05-27T11:29:29.539588Z","shell.execute_reply":"2021-05-27T11:29:29.564757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain_df=get_hand_engineered_feautures(train_df)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:29:29.569797Z","iopub.execute_input":"2021-05-27T11:29:29.570105Z","iopub.status.idle":"2021-05-27T11:30:35.432304Z","shell.execute_reply.started":"2021-05-27T11:29:29.570075Z","shell.execute_reply":"2021-05-27T11:30:35.431292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler=StandardScaler()\nX_train=scaler.fit_transform(train_df[train_feature_columns])\nprint(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:34:21.717067Z","iopub.execute_input":"2021-05-27T11:34:21.717453Z","iopub.status.idle":"2021-05-27T11:34:21.73306Z","shell.execute_reply.started":"2021-05-27T11:34:21.71742Z","shell.execute_reply":"2021-05-27T11:34:21.731981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Glove 100-d vectors","metadata":{}},{"cell_type":"code","source":"glove_path='../input/glove6b100dtxt/glove.6B.100d.txt'\nglove_embeddings={}\nwith open(glove_path) as file:\n    for line in file:\n        line=line.split()\n        word=line[0]\n        v=np.array(line[1:]).astype(np.float)\n        glove_embeddings[word]=v\nprint(len(glove_embeddings))","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:30:35.452039Z","iopub.execute_input":"2021-05-27T11:30:35.452362Z","iopub.status.idle":"2021-05-27T11:31:17.813316Z","shell.execute_reply.started":"2021-05-27T11:30:35.452335Z","shell.execute_reply":"2021-05-27T11:31:17.812457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"class Tokenizer:\n    def __init__(self):\n        self.lemmatizer=WordNetLemmatizer()\n        self.nlp=English()\n    def __call__(self, doc):\n        tokens=[]\n        for token in self.nlp(doc):\n            if token.like_num or token.text=='':\n                continue\n            token=token.lower_.strip()\n            for p in string.punctuation:\n                token=token.replace(p, ' ')\n            token=token.split(' ')\n            token=[w for w in token if w!='']\n            tokens+=token\n        return tokens","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:31:17.814404Z","iopub.execute_input":"2021-05-27T11:31:17.814834Z","iopub.status.idle":"2021-05-27T11:31:17.821443Z","shell.execute_reply.started":"2021-05-27T11:31:17.814805Z","shell.execute_reply":"2021-05-27T11:31:17.820695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer=Tokenizer()\ntrain_df['doc']=train_df.excerpt.apply(tokenizer)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:31:17.822416Z","iopub.execute_input":"2021-05-27T11:31:17.822846Z","iopub.status.idle":"2021-05-27T11:31:26.724989Z","shell.execute_reply.started":"2021-05-27T11:31:17.822815Z","shell.execute_reply":"2021-05-27T11:31:26.724207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_mean=-0.9625387984618096#train_df.target.mean()\ntarget_std=1.0382744351056232#train_df.target.std()\n\nprint(\"Taget Mean:\", target_mean)\nprint(\"Taget Std:\", target_std)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:31:26.725962Z","iopub.execute_input":"2021-05-27T11:31:26.726358Z","iopub.status.idle":"2021-05-27T11:31:26.732006Z","shell.execute_reply.started":"2021-05-27T11:31:26.726328Z","shell.execute_reply":"2021-05-27T11:31:26.731016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sequence Dataset","metadata":{"execution":{"iopub.status.busy":"2021-05-24T06:32:08.728978Z","iopub.execute_input":"2021-05-24T06:32:08.729194Z","iopub.status.idle":"2021-05-24T06:32:08.741099Z","shell.execute_reply.started":"2021-05-24T06:32:08.729171Z","shell.execute_reply":"2021-05-24T06:32:08.739923Z"}}},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_seq_len, phase):\n        self.df=df\n        self.MAX_SEQ_LEN=max_seq_len\n        self.phase=phase\n    def __getitem__(self, idx):\n        row=self.df.iloc[idx]\n        doc=row.doc\n        \n        X1=torch.zeros((self.MAX_SEQ_LEN, 100), dtype=torch.float32)\n        X2=scaler.transform(row[train_feature_columns].values.reshape(1, -1))\n        X2=torch.tensor(X2,dtype=torch.float32)\n        X2=X2.view(-1)\n        \n        for i, word in enumerate(doc):\n            if i >= self.MAX_SEQ_LEN:\n                break\n            if word in glove_embeddings:\n                X1[i]=torch.tensor(glove_embeddings[word])\n        \n        if self.phase=='train' or self.phase=='val':\n            y=torch.tensor(row.normalized_target, dtype=torch.float32)\n            return (X1, X2, y)\n        return (X1, X2)\n    def __len__(self):\n        return len(self.df)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:33:57.157148Z","iopub.execute_input":"2021-05-27T11:33:57.157685Z","iopub.status.idle":"2021-05-27T11:33:57.167553Z","shell.execute_reply.started":"2021-05-27T11:33:57.15765Z","shell.execute_reply":"2021-05-27T11:33:57.166673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sequence Model","metadata":{}},{"cell_type":"code","source":"class SequenceEmbedding(nn.Module):\n    def __init__(self, embedd_size, hidden_size, projection_size):\n        super().__init__()\n        self.hidden_size=hidden_size\n        self.gru=nn.GRU(embedd_size, hidden_size, num_layers=2, \n                        dropout=0.2, bidirectional=True,batch_first=True)\n        self.bn=nn.BatchNorm1d(2*hidden_size)\n        self.relu=nn.ReLU()\n        self.dropout=nn.Dropout(0.2)\n        self.projection_layer=nn.Linear(2*hidden_size, projection_size)\n        \n    def forward(self, x):\n        batch_size=x.shape[0]\n        (_, h_n)=self.gru(x)\n        h_n=h_n.view(2, 2, batch_size, self.hidden_size)\n        h_n=h_n[1, :, :, :].permute(1, 0, 2)\n        h_n1=h_n[:, 0, :]\n        h_n2=h_n[:, 1, :]\n        h=torch.cat([h_n1, h_n2], dim=1)\n        \n        h=self.bn(h)\n        h=self.relu(h)\n        h=self.dropout(h)\n        \n        h=self.projection_layer(h)\n        return h\n\nclass SequenceModel(nn.Module):\n    def __init__(self, embedd_size, hidden_size, projection_size):\n        super().__init__()\n        self.sequence_embedding=SequenceEmbedding(embedd_size, hidden_size, projection_size)\n        self.bn=nn.BatchNorm1d(projection_size)\n        self.dropout=nn.Dropout(0.2)\n        self.relu=nn.ReLU()\n        self.out_layer=nn.Linear(projection_size, 1)\n    def forward(self, x):\n        h=self.sequence_embedding(x)\n        h=self.bn(h)\n        h=self.dropout(h)\n        h=self.relu(h)\n        y=self.out_layer(h)\n        return y","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:33:57.168913Z","iopub.execute_input":"2021-05-27T11:33:57.16942Z","iopub.status.idle":"2021-05-27T11:33:57.185138Z","shell.execute_reply.started":"2021-05-27T11:33:57.16939Z","shell.execute_reply":"2021-05-27T11:33:57.184249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tabular Model","metadata":{}},{"cell_type":"code","source":"class TabularEmbedding(nn.Module):\n    def __init__(self, in_feat):\n        super().__init__()\n        self.linear1=nn.Linear(in_feat, 128)\n        self.bn1=nn.BatchNorm1d(128)\n        self.dropout1=nn.Dropout(0.4)\n        self.relu1=nn.ReLU()\n        self.linear2=nn.Linear(128, 64)\n\n    def forward(self, x):\n        x=self.linear1(x)\n        x=self.bn1(x)\n        x=self.dropout1(x)\n        x=self.relu1(x)\n        x=self.linear2(x)\n        return x\n\nclass TabularModel(nn.Module):\n    def __init__(self, in_feat):\n        super().__init__()\n        self.tabular_embedding=TabularEmbedding(in_feat)\n        self.bn=nn.BatchNorm1d(64)\n        self.dropout=nn.Dropout(0.5)\n        self.relu=nn.ReLU()\n        self.out=nn.Linear(64, 1)\n        \n    def forward(self, x):\n        x=self.tabular_embedding(x)\n        x=self.bn(x)\n        x=self.dropout(x)\n        x=self.relu(x)\n        x=self.out(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:33:57.186849Z","iopub.execute_input":"2021-05-27T11:33:57.187398Z","iopub.status.idle":"2021-05-27T11:33:57.209206Z","shell.execute_reply.started":"2021-05-27T11:33:57.187364Z","shell.execute_reply":"2021-05-27T11:33:57.208126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, sequence_embedding,tabular_embedding, seq_embedd_size, tab_embedd_size):\n        super().__init__()\n        self.sequence_embedding=sequence_embedding\n        self.tabular_embedding=tabular_embedding\n        \n        self.bn1=nn.BatchNorm1d(seq_embedd_size + tab_embedd_size)\n        self.dropout1=nn.Dropout(0.5)\n        self.relu1=nn.ReLU()\n        self.linear1=nn.Linear(seq_embedd_size + tab_embedd_size, 784)\n        \n        self.bn2=nn.BatchNorm1d(784)\n        self.dropout2=nn.Dropout(0.5)\n        self.relu2=nn.ReLU()\n        self.linear2=nn.Linear(784, 1)\n    def forward(self, x1, x2):\n        self.sequence_embedding.eval()\n        self.tabular_embedding.eval()\n        with torch.no_grad():\n            h1=self.sequence_embedding(x1)\n        with torch.no_grad():\n            h2=self.tabular_embedding(x2)\n        h =torch.cat([h1, h2], dim=1)\n        \n        h = self.bn1(h)\n        h = self.dropout1(h)\n        h = self.relu1(h)\n        h = self.linear1(h)\n        \n        h = self.bn2(h)\n        h = self.dropout2(h)\n        h = self.relu2(h)\n        \n        y = self.linear2(h)\n        return y","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:33:57.210999Z","iopub.execute_input":"2021-05-27T11:33:57.211448Z","iopub.status.idle":"2021-05-27T11:33:57.228899Z","shell.execute_reply.started":"2021-05-27T11:33:57.211408Z","shell.execute_reply":"2021-05-27T11:33:57.227703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def infer(models, dataloader):\n    preds=[]\n    for (X1, X2) in dataloader:\n        y_hat=torch.zeros(X1.shape[0])\n        for model in models:\n            model.eval()\n            with torch.no_grad():\n                y=model(X1, X2).view(-1)\n                y_hat+=(target_std*y) + target_mean\n        preds+=list(y_hat.numpy()/len(models))\n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:33:57.230514Z","iopub.execute_input":"2021-05-27T11:33:57.230855Z","iopub.status.idle":"2021-05-27T11:33:57.246002Z","shell.execute_reply.started":"2021-05-27T11:33:57.230825Z","shell.execute_reply":"2021-05-27T11:33:57.244988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models=[\n    torch.load('../input/gru-tabular-data/model_0.pt'),\n    torch.load('../input/gru-tabular-data/model_1.pt'),\n    torch.load('../input/gru-tabular-data/model_2.pt'),\n    torch.load('../input/gru-tabular-data/model_3.pt'),\n    torch.load('../input/gru-tabular-data/model_4.pt')\n]\nmodels[0]","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:33:57.24774Z","iopub.execute_input":"2021-05-27T11:33:57.248146Z","iopub.status.idle":"2021-05-27T11:33:57.323326Z","shell.execute_reply.started":"2021-05-27T11:33:57.248112Z","shell.execute_reply":"2021-05-27T11:33:57.322319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"MAX_SEQ_LEN=150\nBATCH_SIZE=128","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:33:57.324689Z","iopub.execute_input":"2021-05-27T11:33:57.324961Z","iopub.status.idle":"2021-05-27T11:33:57.329218Z","shell.execute_reply.started":"2021-05-27T11:33:57.324935Z","shell.execute_reply":"2021-05-27T11:33:57.328101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df=pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ntest_df=get_hand_engineered_feautures(test_df)\ntest_df['doc']=test_df.excerpt.apply(tokenizer)\n\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:33:57.330967Z","iopub.execute_input":"2021-05-27T11:33:57.331262Z","iopub.status.idle":"2021-05-27T11:33:57.617023Z","shell.execute_reply.started":"2021-05-27T11:33:57.331233Z","shell.execute_reply":"2021-05-27T11:33:57.615785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_test_dataset=Dataset(test_df, MAX_SEQ_LEN, 'test')\ninfer_test_dataloader=torch.utils.data.DataLoader(infer_test_dataset, batch_size=200, shuffle=False)\ntest_df['target'] = infer(models, infer_test_dataloader)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:34:26.946815Z","iopub.execute_input":"2021-05-27T11:34:26.947242Z","iopub.status.idle":"2021-05-27T11:34:27.238964Z","shell.execute_reply.started":"2021-05-27T11:34:26.947205Z","shell.execute_reply":"2021-05-27T11:34:27.237887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df=test_df[['id', 'target']].copy()\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:34:29.363334Z","iopub.execute_input":"2021-05-27T11:34:29.363721Z","iopub.status.idle":"2021-05-27T11:34:29.377238Z","shell.execute_reply.started":"2021-05-27T11:34:29.363684Z","shell.execute_reply":"2021-05-27T11:34:29.375946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-27T11:34:31.389299Z","iopub.execute_input":"2021-05-27T11:34:31.389707Z","iopub.status.idle":"2021-05-27T11:34:31.397764Z","shell.execute_reply.started":"2021-05-27T11:34:31.389673Z","shell.execute_reply":"2021-05-27T11:34:31.396698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}