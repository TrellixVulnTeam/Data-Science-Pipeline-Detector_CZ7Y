{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\n\nimport numpy as np\nfrom scipy.stats import norm\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.distributions.normal import Normal\nfrom torch.distributions.kl import kl_divergence\n\nfrom transformers import (AutoTokenizer, AutoModel, AutoConfig, \n                          AdamW,\n                          get_linear_schedule_with_warmup,\n                          get_cosine_schedule_with_warmup)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T04:45:24.544374Z","iopub.execute_input":"2021-07-29T04:45:24.544927Z","iopub.status.idle":"2021-07-29T04:45:24.552099Z","shell.execute_reply.started":"2021-07-29T04:45:24.54489Z","shell.execute_reply":"2021-07-29T04:45:24.550975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(s):\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n    \nseed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:05.409008Z","iopub.execute_input":"2021-07-29T04:34:05.409336Z","iopub.status.idle":"2021-07-29T04:34:05.417991Z","shell.execute_reply.started":"2021-07-29T04:34:05.409303Z","shell.execute_reply":"2021-07-29T04:34:05.416928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3\n\n","metadata":{}},{"cell_type":"code","source":"class CONFIG:\n    env='prod' # Set test for Testing.\n    checkpoint='roberta-base'\n    pretrain_path='../input/clrp-roberta-base-pretrain/clrp_roberta_base'\n    tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n    base_config=AutoConfig.from_pretrained(checkpoint)\n    \n    hidden_size=base_config.hidden_size\n    pad_token_id=tokenizer.pad_token_id\n    max_seq_len=tokenizer.model_max_length\n    \n    FREEZE_LAYERS_START=0\n    TRAIN_MAX_ITERS=680\n    TRAIN_WARMUP_STEPS=204\n    TRAIN_SAMPLES_PER_BATCH=20\n    \n    batch_size=20\n    folds=5\n    bins=9\n    train_sample_bins=10\n    \n    learning_rate=2e-5\n    weight_decay=0.01\n    optimizer='AdamW'\n    epochs=8\n    clip_gradient_norm=1.0\n    eval_every=10\n    \n    device=torch.device( 'cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:05.420635Z","iopub.execute_input":"2021-07-29T04:34:05.421206Z","iopub.status.idle":"2021-07-29T04:34:18.439955Z","shell.execute_reply.started":"2021-07-29T04:34:05.421169Z","shell.execute_reply":"2021-07-29T04:34:18.439066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.env=='test':\n    CONFIG.TRAIN_SAMPLES_PER_BATCH=4\n    CONFIG.TRAIN_MAX_ITERS=3\n    CONFIG.epochs=1\n    CONFIG.eval_every=1\n    \nprint(\"Device:\", CONFIG.device)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:18.441538Z","iopub.execute_input":"2021-07-29T04:34:18.442169Z","iopub.status.idle":"2021-07-29T04:34:18.450232Z","shell.execute_reply.started":"2021-07-29T04:34:18.442131Z","shell.execute_reply":"2021-07-29T04:34:18.449299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_qunatile_boundaries(df, bins=10):\n    df=df.copy()\n    qs=[]\n    for i in np.arange(1/bins, 1.1, 1/bins):\n        q=train_df.target.quantile(i)\n        qs.append(q)\n    return qs\n\ndef get_quantile(target, qs):\n    for i,q in enumerate(qs):\n        if target<=q:\n            return i\n\n        \ndef get_bin_ranges(df):\n    df=df.copy()\n    bin_ranges=[]\n    \n    min_target=train_df.target.min()\n    max_target=train_df.target.max()\n    \n    min_std=train_df[train_df.target==min_target].standard_error.min()\n    max_std=train_df[train_df.target==max_target].standard_error.max()\n\n    min_val=min_target-min_std\n    max_val=max_target+max_std\n    \n    bin_values=np.arange(min_val, max_val, 0.5)\n    start_bin=(-1e9, bin_values[0])\n    end_bin=(bin_values[-1], 1e9)\n    \n    bin_ranges.append(start_bin)\n    for i in range(1, len(bin_values)):\n        bin_ranges.append( (bin_values[i-1], bin_values[i]) )\n    bin_ranges.append(end_bin)\n    return bin_ranges\n\ndef get_bin_distribution(row, bin_ranges):\n    mu=row.target\n    scale=0.2\n    bins=[]\n    \n    for bin_range in bin_ranges:\n        s=bin_range[0]\n        e=bin_range[1]\n        \n        cdf1=norm.cdf(s, mu, scale)\n        cdf2=norm.cdf(e, mu, scale)\n        \n        cdf=cdf2-cdf1\n        \n        bins.append(cdf)\n    return bins","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:18.452301Z","iopub.execute_input":"2021-07-29T04:34:18.452586Z","iopub.status.idle":"2021-07-29T04:34:18.467911Z","shell.execute_reply.started":"2021-07-29T04:34:18.45256Z","shell.execute_reply":"2021-07-29T04:34:18.466949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('../input/commonlit-kfold-dataset/fold_train.csv')\nbin_ranges=get_bin_ranges(train_df)\n#Update Bins in the configuration\nCONFIG.bins=len(bin_ranges)\n\nprint(bin_ranges)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:18.469484Z","iopub.execute_input":"2021-07-29T04:34:18.47004Z","iopub.status.idle":"2021-07-29T04:34:18.599744Z","shell.execute_reply.started":"2021-07-29T04:34:18.469998Z","shell.execute_reply":"2021-07-29T04:34:18.598797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_qs=get_qunatile_boundaries(train_df, CONFIG.train_sample_bins)\ntrain_df['q']=train_df.target.apply(get_quantile, args=(train_qs, ))\ntrain_df['ybin']=train_df.apply(get_bin_distribution, args=(bin_ranges, ), axis=1)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:18.601304Z","iopub.execute_input":"2021-07-29T04:34:18.601935Z","iopub.status.idle":"2021-07-29T04:34:28.405785Z","shell.execute_reply.started":"2021-07-29T04:34:18.60189Z","shell.execute_reply":"2021-07-29T04:34:28.404929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"class CommonLitDataset(torch.utils.data.Dataset):\n    def __init__(self, df, phase='train'):\n        self.excerpts=df.excerpt.values\n        self.targets=df.target.values\n        self.standard_errors=df.standard_error.values\n        self.ybin=df.ybin.values\n        \n        self.tokenizer=CONFIG.tokenizer\n        self.pad_token_id=CONFIG.pad_token_id\n        self.max_seq_len=CONFIG.max_seq_len\n    \n    def get_tokenized_features(self, excerpt):\n        inputs=self.tokenizer(excerpt, truncation=True)\n        \n        input_ids=inputs['input_ids']\n        attention_mask=inputs['attention_mask']\n        \n        input_len=len(input_ids)\n        pad_len=self.max_seq_len-input_len\n        input_ids+=[self.pad_token_id]*pad_len\n        attention_mask+=[0]*pad_len\n        \n        return {\n            'seq_len': input_len,\n            'input_ids': input_ids,\n            'attention_mask': attention_mask\n        }\n        \n    def __getitem__(self, idx):\n        excerpt=self.excerpts[idx]\n        target=self.targets[idx]\n        ybin=self.ybin[idx]\n        sigma=self.standard_errors[idx]\n        features=self.get_tokenized_features(excerpt)\n        return {\n            'seq_len': features['seq_len'],\n            'input_ids': torch.tensor(features['input_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(features['attention_mask'], dtype=torch.long),\n            'yreg': torch.tensor(target, dtype=torch.float32),\n            'ybin': torch.tensor(ybin, dtype=torch.float32),\n            'sigmas': torch.tensor(sigma, dtype=torch.float32)\n        }\n    \n    def __len__(self):\n        return len(self.targets)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:28.408653Z","iopub.execute_input":"2021-07-29T04:34:28.409007Z","iopub.status.idle":"2021-07-29T04:34:28.419461Z","shell.execute_reply.started":"2021-07-29T04:34:28.408978Z","shell.execute_reply":"2021-07-29T04:34:28.418618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Data Sampler","metadata":{}},{"cell_type":"code","source":"class TrainDataSampler:\n    def __init__(self, batch_size, df):\n        self.qmap={}\n        self.batch_size=batch_size\n        self.batch_fraction=1.0\n        self.df=df.copy()\n        \n        self.tokenizer=CONFIG.tokenizer\n        self.pad_token_id=CONFIG.pad_token_id\n        self.max_seq_len=CONFIG.max_seq_len\n        \n        for i in range(CONFIG.train_sample_bins):\n            ids=self.df[self.df.q==i].id.values\n            np.random.shuffle(ids)\n            self.qmap[i]=ids\n    \n    def get_tokenized_features(self, excerpt):\n        inputs=self.tokenizer(excerpt, truncation=True)\n        \n        input_ids=inputs['input_ids']\n        attention_mask=inputs['attention_mask']\n        \n        input_len=len(input_ids)\n        pad_len=self.max_seq_len-input_len\n        input_ids+=[self.pad_token_id]*pad_len\n        attention_mask+=[0]*pad_len\n        \n        return {\n            'seq_len': input_len,\n            'input_ids': input_ids,\n            'attention_mask': attention_mask\n        }\n    \n    def get_mbs(self):\n        sentences=[]\n        yreg=[]; ybin=[];sigmas=[]\n        for i in range(CONFIG.train_sample_bins):\n            if i not in self.qmap:\n                continue\n            yids=self.qmap[i][-2:]\n            \n            sentences+=list(self.df[self.df.id.isin(yids)].excerpt.values)\n            yreg+=list(self.df[self.df.id.isin(yids)].target.values)\n            ybin+=list(self.df[self.df.id.isin(yids)].ybin.values)\n            sigmas+=list( self.df[self.df.id.isin(yids)].standard_error.values )\n            \n            self.qmap[i]=self.qmap[i][:-2]\n            if len(self.qmap[i]) == 0:\n                self.qmap.pop(i)\n        \n        num_samples=len(yreg)\n        self.batch_fraction=len(yreg)/self.batch_size\n        features={\n            'seq_len': [],\n            'input_ids': [],\n            'attention_mask': [],\n            'yreg': [],\n            'ybin':[],\n            'sigmas': []\n        }\n        \n        for i, sentence in enumerate(sentences):\n            data=self.get_tokenized_features(sentence)\n            \n            seq_len=data['seq_len']\n            input_ids=data['input_ids']\n            attention_mask=data['attention_mask']\n            \n            features['seq_len'].append(seq_len)\n            features['input_ids'].append(input_ids)\n            features['attention_mask'].append(attention_mask)\n            features['yreg'].append(yreg[i]+np.random.uniform(-0.1, 0.1))\n            features['ybin'].append(ybin[i])\n            features['sigmas'].append(sigmas[i])\n            \n        features['seq_len']=torch.tensor(features['seq_len'], dtype=torch.long)\n        features['input_ids']=torch.tensor(features['input_ids'], dtype=torch.long)\n        features['attention_mask']=torch.tensor(features['attention_mask'], dtype=torch.long)\n        features['yreg']=torch.tensor(features['yreg'], dtype=torch.float32)\n        features['ybin']=torch.tensor(features['ybin'], dtype=torch.float32)\n        features['sigmas']=torch.tensor(features['sigmas'], dtype=torch.float32)\n        return features\n    \n    def __iter__(self):\n        while len(self.qmap)>0:\n            mbs=self.get_mbs()\n            if self.batch_fraction < 0.5:\n                break\n            yield mbs\n    def __next__(self):\n        for i in range(10):\n            yield i","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:28.421439Z","iopub.execute_input":"2021-07-29T04:34:28.422074Z","iopub.status.idle":"2021-07-29T04:34:28.44717Z","shell.execute_reply.started":"2021-07-29T04:34:28.422034Z","shell.execute_reply":"2021-07-29T04:34:28.446232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def freeze_roberta_layers(roberta):\n    max_freeze_layer=CONFIG.FREEZE_LAYERS_START\n    for n,p in roberta.named_parameters():\n        if ('embedding' in n): #or ('layer' in n and int(n.split('.')[2]) <= max_freeze_layer):\n            p.requires_grad=False\n            \n            \nclass TextRegressor(nn.Module):\n    def __init__(self):\n        super(TextRegressor, self).__init__()\n        self.linear=nn.Linear(CONFIG.hidden_size, 1024)\n        self.layer_norm=nn.LayerNorm(1024)\n        self.relu=nn.ReLU()\n        self.dropout=nn.Dropout(0.5)\n        self.regressor=nn.Linear(1024, 1)\n        \n        nn.init.uniform_(self.linear.weight, -0.02, 0.02)\n        nn.init.uniform_(self.regressor.weight, -0.02, 0.02)\n        \n    def forward(self, x):\n        x=self.linear(x)\n        x=self.layer_norm(x)\n        x=self.relu(x)\n        x=self.dropout(x)\n        x=self.regressor(x)\n        return x\n    \nclass BinEstimator(nn.Module):\n    def __init__(self):\n        super(BinEstimator,self).__init__()\n        self.linear=nn.Linear(CONFIG.hidden_size, 1024)\n        self.layer_norm=nn.LayerNorm(1024)\n        self.relu=nn.ReLU()\n        self.dropout=nn.Dropout(0.5)\n        self.logits=nn.Linear(1024, CONFIG.bins)\n        \n        nn.init.uniform_(self.linear.weight, -0.025, 0.025)\n        nn.init.uniform_(self.logits.weight, -0.02, 0.02)\n        \n    def forward(self, x):    \n        x=self.linear(x)\n        x=self.layer_norm(x)\n        x=self.relu(x)\n        x=self.dropout(x)\n        x=self.logits(x)\n        x=torch.softmax(x, dim=1)\n        return x\n\nclass AttentionHead(nn.Module):\n    def __init__(self):\n        super(AttentionHead, self).__init__()\n        self.W=nn.Linear(CONFIG.hidden_size, CONFIG.hidden_size)\n        self.V=nn.Linear(CONFIG.hidden_size, 1)\n    def forward(self, x):\n        attn=torch.tanh(self.W(x))\n        score=self.V(attn)\n        attention_weights=torch.softmax(score, dim=1)\n        \n        context_vector=attention_weights * x\n        context_vector=torch.sum(context_vector, dim=1)\n        return context_vector\n    \nclass CommonLitModel(nn.Module):\n    def __init__(self):\n        super(CommonLitModel, self).__init__()\n        self.roberta=AutoModel.from_pretrained(CONFIG.pretrain_path)\n        \n        self.attention_head=AttentionHead()\n        self.dropout=nn.Dropout(0.25)\n        self.layer_norm=nn.LayerNorm(CONFIG.hidden_size)\n\n        self.regressor=TextRegressor()\n        self.bin_estimator=BinEstimator()\n        \n        freeze_roberta_layers(self.roberta)\n        \n    def forward(self, input_ids, attention_mask,output_hidden_states=False):\n        roberta_output=self.roberta(input_ids,\n                                 attention_mask=attention_mask,\n                                 output_hidden_states=True)\n        \n        last_hidden_state=roberta_output.last_hidden_state\n        cls_pool=roberta_output.pooler_output\n        \n        attention_pool=self.attention_head(last_hidden_state)\n        x_pool=(cls_pool+attention_pool)/2\n        x_pool=self.dropout(x_pool)\n        x_pool=self.layer_norm(x_pool)\n        \n        yhat_reg=self.regressor(x_pool)\n        yhat_bin=self.bin_estimator(x_pool)\n        \n        return yhat_reg, yhat_bin","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:28.4488Z","iopub.execute_input":"2021-07-29T04:34:28.449223Z","iopub.status.idle":"2021-07-29T04:34:28.472729Z","shell.execute_reply.started":"2021-07-29T04:34:28.449172Z","shell.execute_reply":"2021-07-29T04:34:28.471365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Loss","metadata":{}},{"cell_type":"code","source":"class CustomLoss:\n    def __init__(self):\n        self.criterion=nn.MSELoss()\n        self.kl_divergence=nn.KLDivLoss(reduction='batchmean', log_target=True)\n        \n    def get_reg_loss(self, y, yreg, phase, iter_count):\n        if phase=='val':\n            reg_loss=self.criterion(yreg, y)\n        else:\n            reg_loss=torch.tensor(0.0, device=CONFIG.device)\n            reg_loss1=torch.tensor(0.0, device=CONFIG.device)\n            reg_loss2=torch.tensor(0.0, device=CONFIG.device)\n            reg_loss3=torch.tensor(0.0, device=CONFIG.device)\n            \n            ydiff=torch.abs(yreg-y)\n            \n            ydiff1=ydiff[ydiff<=0.1]\n            ydiff2=ydiff[(ydiff>0.1) & (ydiff<=0.5)]\n            ydiff3=ydiff[(ydiff>0.5)]\n            \n            batch_size=len(ydiff)\n            alpha1=0\n            alpha2=0\n            alpha3=0\n            \n            reg_loss=torch.tensor(0.0, device=CONFIG.device)\n            if len(ydiff1)>0:\n                reg_loss1+=( (ydiff1**2).mean())\n                alpha1=0.1\n            if len(ydiff2)>0:\n                reg_loss2+=( (ydiff2**2).mean())\n                alpha2=0.5\n            if len(ydiff3) >0:\n                alpha3=0.4\n                reg_loss3+=(ydiff3**2).mean()\n                \n            reg_loss=(alpha1 * reg_loss1)+(alpha2 * reg_loss2)+(alpha3 * reg_loss3)\n            reg_loss/=(alpha1+alpha2+alpha3)\n            \n        return reg_loss\n    \n    def get_bin_loss(self, ybin, yhat_bin, phase):\n        if phase == 'val':\n            ybin=ybin.view(-1, CONFIG.bins)\n            yhat_bin=yhat_bin.view(-1, CONFIG.bins)\n            \n        yerr=torch.abs(ybin-yhat_bin)\n        yerr=yerr.sum(dim=1)\n        loss=yerr.mean()\n        return loss\n    \n    def get_distribution_loss(self, y_mus, y_sigmas, yhat_mus):\n        P=Normal(y_mus, y_sigmas)\n        Q=Normal(yhat_mus, y_sigmas)\n        \n        loss=(kl_divergence(P, Q)+kl_divergence(Q, P))/2\n        loss=loss.mean()\n        loss=loss.to(CONFIG.device)\n        return loss\n    \n    \n    def get_consistency_kl_div_loss(self, ybin, yhat_bin, phase):\n        if phase == 'val':\n            ybin=ybin.view(-1, 1+CONFIG.bins)\n            yhat_bin=yhat_bin.view(-1, 1+CONFIG.bins)\n        \n        loss1=self.kl_divergence(ybin, yhat_bin)\n        loss2=self.kl_divergence(yhat_bin, ybin)\n        loss = (loss1+loss2)/2\n        return loss\n    \n    def get_consistency_loss(self, sigmas, yhat_mus, yhat_bin, phase):\n        num_samples=yhat_mus.size(0)\n        yreg_dist=torch.zeros(num_samples, 1+CONFIG.bins)\n        yreg_dist[:, 0]=Normal(yhat_mus, sigmas).cdf(qs_bins[0])\n        \n        for i in range(1, CONFIG.bins):\n            yreg_dist[:, i]=Normal(yhat_mus, sigmas).cdf(qs_bins[i])-Normal(yhat_mus, sigmas).cdf(qs_bins[i-1])\n        yreg_dist[:, CONFIG.bins]=1-Normal(yhat_mus, sigmas).cdf(qs_bins[CONFIG.bins-1])\n        \n        if phase=='train':\n            yreg_dist=yreg_dist.to(CONFIG.device)\n        loss=self.get_bin_loss(yreg_dist, yhat_bin, phase)\n        #loss=self.get_consistency_kl_div_loss(yreg_dist, yhat_bin, phase)\n        return loss\n    \n    def get_bin_cross_entropy_loss(self, ybin, yhat_bin, phase):\n        if phase == 'val':\n            ybin=ybin.view(-1, CONFIG.bins)\n            yhat_bin=yhat_bin.view(-1, CONFIG.bins)\n        loss=torch.tensor(0.0)\n        loss=torch.zeros(ybin.shape)\n        for i in range(ybin.shape[1]):\n            loss[:, i] = (-ybin[:, i]) * torch.log(yhat_bin[:, i] + 1e-9)\n        loss=loss.sum(dim=1).mean()\n        return loss\n    \n    # This loss combines the cumulative distributions of the top-2 bins\n    def get_bin_cum_loss(self, ybin, yhat_bin, phase):\n        loss=0.0\n        if phase == 'val':\n            ybin=ybin.view(-1, CONFIG.bins)\n            yhat_bin=yhat_bin.view(-1, CONFIG.bins)\n        \n        topk=torch.topk(ybin,k=2, dim=1)\n        topk_values=topk.values.sum(dim=1)\n        topk_indices=topk.indices\n        batch_size=ybin.size(0)\n        \n        for i in range(batch_size):\n            ind=topk_indices[i]\n            loss+=torch.abs(yhat_bin[i][ind].sum() - topk_values[i])\n        loss/=max(1, batch_size)\n        return loss\n        \n    \n    def get_loss(self, inputs, phase, iter_count=0):\n        yreg=inputs['yreg']\n        ybin=inputs['ybin']\n        \n        yhat_reg=inputs['yhat_reg']\n        yhat_bin=inputs['yhat_bin']\n        \n        reg_loss=self.get_reg_loss(yreg, yhat_reg, phase, iter_count)\n        loss=reg_loss#+0.4*(bin_loss+bin_cum_loss)\n        return {\n            'loss': loss,\n            'reg_loss': reg_loss.item(),\n            'bin_loss': 0,\n            'bin_cum_loss': 0\n        }","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:28.474484Z","iopub.execute_input":"2021-07-29T04:34:28.47493Z","iopub.status.idle":"2021-07-29T04:34:28.503793Z","shell.execute_reply.started":"2021-07-29T04:34:28.474889Z","shell.execute_reply":"2021-07-29T04:34:28.502989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluator","metadata":{}},{"cell_type":"code","source":"class CustomEvaluator:\n    def __init__(self, val_dataloader):\n        self.val_dataloader=val_dataloader\n        self.criterion=nn.MSELoss()\n        self.custom_loss=CustomLoss()\n        \n    def evaluate(self, model):\n        model.eval()\n        all_yreg=[]\n        all_ybin=[]\n        \n        all_yhatreg=[]\n        all_yhatbin=[]\n        \n        for batch in self.val_dataloader:\n            batch_max_seq_len=torch.max(batch['seq_len'])\n            \n            input_ids=batch['input_ids'][:, :batch_max_seq_len].to(CONFIG.device)\n            attention_mask=batch['attention_mask'][:, :batch_max_seq_len].to(CONFIG.device)\n            yreg=batch['yreg'].view(-1)\n            ybin=batch['ybin'].view(-1)\n            \n            \n            all_yreg+=yreg.tolist()\n            all_ybin+=ybin.tolist()\n            \n            with torch.no_grad():\n                yhat_reg, yhat_bin=model(input_ids, attention_mask)\n                yhat_reg=yhat_reg.view(-1).detach().cpu()\n                yhat_bin=yhat_bin.view(-1).detach().cpu()\n                \n                all_yhatreg+=yhat_reg.tolist()\n                all_yhatbin+=yhat_bin.tolist()\n                \n        all_yreg=torch.tensor(all_yreg, dtype=torch.float32)\n        all_ybin=torch.tensor(all_ybin, dtype=torch.float32)\n        \n        all_yhatreg=torch.tensor(all_yhatreg, dtype=torch.float32)\n        all_yhatbin=torch.tensor(all_yhatbin, dtype=torch.float32)\n        \n        ydiff=torch.abs(all_yhatreg - all_yreg).numpy()\n        \n        print(\"ydiff Variance:\", np.std(ydiff))\n        \n        print('Quantiles---')\n        print(np.quantile(ydiff, 0.7))\n        print(np.quantile(ydiff, 0.8))\n        print(np.quantile(ydiff, 0.9))\n        print(np.quantile(ydiff, 0.95))\n        model_losses=self.custom_loss.get_loss({\n            'yreg': all_yreg,\n            'ybin': all_ybin,\n            \n            'yhat_reg': all_yhatreg,\n            'yhat_bin': all_yhatbin\n        }, 'val', 0)\n        return model_losses","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:28.506585Z","iopub.execute_input":"2021-07-29T04:34:28.506871Z","iopub.status.idle":"2021-07-29T04:34:28.522551Z","shell.execute_reply.started":"2021-07-29T04:34:28.506848Z","shell.execute_reply":"2021-07-29T04:34:28.521785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"markdown","source":"def get_optimizer_params(model):\n    optimizer_parameters=[\n        {\n            'params': [p for n, p in model.named_parameters() if (p.requires_grad and ('roberta' in n) and ('LayerNorm' not in n) )],\n        },\n        {\n            'params': [p for n, p in model.named_parameters() if (p.requires_grad and ('roberta' in n) and 'LayerNorm' in n )],\n            'weight_decay':0 \n        },\n        \n        \n        \n        \n        \n        \n        {\n            'params': [p for n, p in model.named_parameters() if (p.requires_grad and 'roberta' not in n)],\n            'lr': 1e-3\n        }\n    ]\n    return optimizer_parameters","metadata":{"execution":{"iopub.status.busy":"2021-07-28T17:41:03.496056Z","iopub.execute_input":"2021-07-28T17:41:03.496394Z","iopub.status.idle":"2021-07-28T17:41:03.507046Z","shell.execute_reply.started":"2021-07-28T17:41:03.49636Z","shell.execute_reply":"2021-07-28T17:41:03.506229Z"}}},{"cell_type":"code","source":"def get_optimizer_params(model):\n    optimizer_parameters=[\n        {\n            'params': [p for n, p in model.named_parameters() if (p.requires_grad and ('roberta' in n) and ('LayerNorm' not in n) )],\n        },\n        {\n            'params': [p for n, p in model.named_parameters() if (p.requires_grad and ('roberta' in n) and 'LayerNorm' in n )],\n            'weight_decay':0 \n        },\n        {\n            'params': [p for n, p in model.named_parameters() if (p.requires_grad and 'roberta' not in n)],\n            'lr': 1e-3\n        }\n    ]\n    return optimizer_parameters","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:28.525644Z","iopub.execute_input":"2021-07-29T04:34:28.525993Z","iopub.status.idle":"2021-07-29T04:34:28.534131Z","shell.execute_reply.started":"2021-07-29T04:34:28.525969Z","shell.execute_reply":"2021-07-29T04:34:28.533377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit","metadata":{}},{"cell_type":"markdown","source":"def get_optimizer_params(model):\n    no_decay=['LayerNorm', 'bias', 'beta', 'gamma']\n    group1=['layer.8', 'layer.9', 'layer.10', 'layer.11']\n    group2=['layer.4', 'layer.5', 'layer.6', 'layer.7']\n    group3=['layer.1', 'layer.2', 'layer.3']\n    \n    lr_group1=5e-5\n    lr_group2=3e-5\n    lr_group3=2e-5\n    \n    optimizer_parameters=[\n        {\n            'params': [p for n, p in model.roberta.named_parameters() if (p.requires_grad) and \n                       (n in group1) and \n                       any([n not in nd for nd in no_decay]) \n                      ],\n            'lr': lr_group1,\n            'weight_decay': 0.01\n        },\n        \n        {\n            'params': [p for n, p in model.roberta.named_parameters() if (p.requires_grad) and \n                       (n in group1) and \n                       any([n in nd for nd in no_decay]) \n                      ],\n            'lr': lr_group1,\n            'weight_decay': 0\n        },\n        \n        {\n            'params': [p for n, p in model.roberta.named_parameters() if (p.requires_grad) and \n                       (n in group2) and \n                       any([n not in nd for nd in no_decay]) \n                      ],\n            'lr': lr_group2,\n            'weight_decay': 0.01\n        },\n        \n        {\n            'params': [p for n, p in model.roberta.named_parameters() if (p.requires_grad) and \n                       (n in group2) and \n                       any([n in nd for nd in no_decay]) \n                      ],\n            'lr': lr_group2,\n            'weight_decay': 0.0\n        },\n        \n        #Group-3\n        {\n            'params': [p for n, p in model.roberta.named_parameters() if (p.requires_grad) and \n                       (n in group3) and \n                       any([n not in nd for nd in no_decay]) \n                      ],\n            'lr': lr_group3,\n            'weight_decay': 0.01\n        },\n        \n        {\n            'params': [p for n, p in model.roberta.named_parameters() if (p.requires_grad) and \n                       (n in group3) and \n                       any([n in nd for nd in no_decay]) \n                      ],\n            'lr': lr_group3,\n            'weight_decay': 0.01\n        },\n        \n        {\n            'params': [p for n, p in model.named_parameters() if ('roberta' not in n) and any([n not in nd for nd in no_decay])],\n            'lr': 1e-3,\n            'weight_decay': 0.01\n        },\n        {\n            'params': [p for n, p in model.named_parameters() if ('roberta' not in n) and any([n in nd for nd in no_decay])],\n            'lr': 1e-3,\n            'weight_decay': 0.0\n        }\n    ]\n    return optimizer_parameters","metadata":{"execution":{"iopub.status.busy":"2021-07-28T17:37:09.732955Z","iopub.execute_input":"2021-07-28T17:37:09.733372Z","iopub.status.idle":"2021-07-28T17:37:09.748867Z","shell.execute_reply.started":"2021-07-28T17:37:09.733276Z","shell.execute_reply":"2021-07-28T17:37:09.747996Z"}}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, fold_train_df,  val_dataloader):\n        self.df=fold_train_df.copy()\n        self.val_dataloader=val_dataloader\n        \n        self.model=model\n        self.optimizer=AdamW(get_optimizer_params(model),\n                             lr=CONFIG.learning_rate,\n                             weight_decay=CONFIG.weight_decay)\n        \n        self.schedular=torch.optim.lr_scheduler.OneCycleLR(self.optimizer,\n                                                           max_lr=CONFIG.learning_rate,\n                                                           total_steps=CONFIG.TRAIN_MAX_ITERS,\n                                                           pct_start=0.25\n                                                          )\n                                              #CONFIG.TRAIN_WARMUP_STEPS,\n                                              #CONFIG.TRAIN_MAX_ITERS)\n        \n        self.custom_loss=CustomLoss()\n        self.custom_evaluator=CustomEvaluator(val_dataloader)\n        \n        \n        self.train_loss=[]\n        self.train_reg_loss=[]\n        self.train_bin_loss=[]\n        self.train_bin_cum_loss=[]\n        \n        self.val_loss=[]\n        self.val_reg_loss=[]\n        self.val_bin_loss=[]\n        self.val_bin_cum_loss=[]\n        \n        self.iter_count=0\n        self.best_iter=0\n        self.best_reg_iter=0\n        self.best_bin_iter=0\n        \n        self.best_loss=None\n        self.best_reg_loss=None\n        self.best_bin_loss=None\n        \n        \n    def checkpoint(self, model_losses):\n        val_loss=model_losses['loss'].item()\n        val_reg_loss=model_losses['reg_loss']\n        val_bin_loss=model_losses['bin_loss']\n        \n        if (self.best_loss is None) or (self.best_loss > val_loss):\n            self.best_loss=val_loss\n            self.best_iter=self.iter_count\n            torch.save(self.model, \"best_model.pt\")\n        \n        if (self.best_reg_loss is None) or (self.best_reg_loss > val_reg_loss):\n            self.best_reg_loss=val_reg_loss\n            self.best_reg_iter=self.iter_count\n            torch.save(self.model, \"best_reg_model.pt\")\n            \n        if (self.best_bin_loss is None) or (self.best_bin_loss > val_bin_loss):\n            self.best_bin_loss=val_bin_loss\n            self.best_bin_iter=self.iter_count\n            torch.save(self.model, \"best_bin_model.pt\")\n\n        print(\"===\"*10)\n        print(\"Iter:{} | BestIter:{} | Best Reg Iter:{} | Best Bin Iter:{} \".format(\n            self.iter_count,self.best_iter, self.best_reg_iter, self.best_bin_iter\n        ))\n        \n        print(\"Training Losses:\")\n        print(\"Total: {:.3f} | Reg Loss:{:.3f} | Bin Loss:{:.3f} | Bin cumloss:{:.3f}\".format(\n            self.train_loss[-1], self.train_reg_loss[-1], self.train_bin_loss[-1],\n            self.train_bin_cum_loss[-1]\n        ))\n        print()\n        print(\"Val Losses\")\n        print(\"Total: {:.3f} | Reg Loss:{:.3f} | Bin Loss:{:.3f} | Bin cumloss:{:.3f}\".format(\n            val_loss, val_reg_loss, val_bin_loss,\n            self.val_bin_cum_loss[-1]\n        ))\n    \n    def train_ops(self, inputs):\n        self.optimizer.zero_grad()\n        model_losses=self.custom_loss.get_loss(inputs, 'train', self.iter_count)\n        model_losses['loss'].backward()\n        self.optimizer.step()\n        self.schedular.step()\n        return model_losses\n    \n    def train_epoch(self):\n        t1=time.time()\n        self.model.train()\n        for batch in TrainDataSampler(CONFIG.TRAIN_SAMPLES_PER_BATCH, self.df):\n            self.iter_count+=1\n            if self.iter_count > CONFIG.TRAIN_MAX_ITERS:\n                break\n            \n            batch_seq_lens=batch['seq_len']\n            batch_max_seq_len=torch.max(batch['seq_len'])\n            \n            input_ids=batch['input_ids'][:, :batch_max_seq_len].to(CONFIG.device)\n            attention_mask=batch['attention_mask'][:, :batch_max_seq_len].to(CONFIG.device)\n            yreg=batch['yreg'].to(CONFIG.device)\n            ybin=batch['ybin'].to(CONFIG.device)\n            \n            yhat_reg, yhat_bin=self.model(input_ids, attention_mask)\n            yhat_reg=yhat_reg.view(-1)\n            \n            model_losses=self.train_ops({\n                'yreg': yreg,\n                'ybin': ybin,\n                \n                'yhat_reg': yhat_reg,\n                'yhat_bin': yhat_bin\n            })\n            \n            self.train_loss.append(model_losses['loss'].item())\n            self.train_reg_loss.append(model_losses['reg_loss'])\n            self.train_bin_loss.append(model_losses['bin_loss'])\n            self.train_bin_cum_loss.append(model_losses['bin_cum_loss'])\n            \n            \n            if self.iter_count%CONFIG.eval_every==0:\n                model_losses=self.custom_evaluator.evaluate(model)\n                self.val_loss.append(model_losses['loss'].item())\n                self.val_reg_loss.append(model_losses['reg_loss'])\n                self.val_bin_loss.append(model_losses['bin_loss'])\n                self.val_bin_cum_loss.append(model_losses['bin_cum_loss'])\n                self.checkpoint(model_losses)\n\n            \n    def train(self):\n        while True:\n            if self.iter_count > CONFIG.TRAIN_MAX_ITERS:\n                break\n            self.train_epoch()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:28.536801Z","iopub.execute_input":"2021-07-29T04:34:28.537071Z","iopub.status.idle":"2021-07-29T04:34:28.563109Z","shell.execute_reply.started":"2021-07-29T04:34:28.537047Z","shell.execute_reply":"2021-07-29T04:34:28.562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k in range(CONFIG.folds):\n    print(\"===\"*10)\n    print()\n    print(\"Fold: ==> \", k+1)\n    fold_train_df=train_df[train_df.kfold!=k].copy()\n    fold_val_df  =train_df[train_df.kfold==k].copy()\n\n    if CONFIG.env=='test':\n        fold_train_df=fold_train_df.head(4)\n        fold_val_df=fold_val_df.head(4)\n        \n    val_dataset=CommonLitDataset(fold_val_df)\n    val_dataloader=torch.utils.data.DataLoader(val_dataset, batch_size=CONFIG.batch_size,\n                                                 shuffle=False, pin_memory=True, drop_last=False)\n    \n    \n    model=CommonLitModel()\n    model=model.to(CONFIG.device)\n    trainer=Trainer(model, fold_train_df, val_dataloader)\n    trainer.train()\n        \n    best_model=torch.load('best_model.pt')\n    best_reg_model=torch.load('best_reg_model.pt')\n    best_bin_model=torch.load('best_bin_model.pt')\n    \n    torch.save(best_model, \"best_model{}.pt\".format(k+1))\n    torch.save(best_reg_model, \"best_reg_model{}.pt\".format(k+1))\n    torch.save(best_bin_model, \"best_bin_model{}.pt\".format(k+1))\n    \n    print(\"Best Iteration:\", trainer.best_iter)\n    print(\"Best Reg Iteration:\", trainer.best_reg_iter)\n    print(\"Best Bin Iteration:\", trainer.best_bin_iter)\n    \n    \n    print(\"Best Loss:{}\".format(trainer.best_loss))\n    print(\"Best Reg Loss:{}\".format(trainer.best_reg_loss))\n    print(\"Best Bin Loss:{}\".format(trainer.best_bin_loss))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:34:28.564809Z","iopub.execute_input":"2021-07-29T04:34:28.56526Z","iopub.status.idle":"2021-07-29T04:44:45.320407Z","shell.execute_reply.started":"2021-07-29T04:34:28.565219Z","shell.execute_reply":"2021-07-29T04:44:45.319429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.train_loss)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:45:47.862548Z","iopub.execute_input":"2021-07-29T04:45:47.862882Z","iopub.status.idle":"2021-07-29T04:45:48.010654Z","shell.execute_reply.started":"2021-07-29T04:45:47.862852Z","shell.execute_reply":"2021-07-29T04:45:48.009863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.train_reg_loss)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:44:45.974442Z","iopub.execute_input":"2021-07-29T04:44:45.976481Z","iopub.status.idle":"2021-07-29T04:44:46.148045Z","shell.execute_reply.started":"2021-07-29T04:44:45.976436Z","shell.execute_reply":"2021-07-29T04:44:46.147247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.train_bin_loss)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:44:46.149857Z","iopub.execute_input":"2021-07-29T04:44:46.150107Z","iopub.status.idle":"2021-07-29T04:44:46.289019Z","shell.execute_reply.started":"2021-07-29T04:44:46.150082Z","shell.execute_reply":"2021-07-29T04:44:46.28809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.train_bin_cum_loss)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:44:46.290446Z","iopub.execute_input":"2021-07-29T04:44:46.290827Z","iopub.status.idle":"2021-07-29T04:44:46.427379Z","shell.execute_reply.started":"2021-07-29T04:44:46.290789Z","shell.execute_reply":"2021-07-29T04:44:46.426336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.val_loss)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:44:46.428788Z","iopub.execute_input":"2021-07-29T04:44:46.429343Z","iopub.status.idle":"2021-07-29T04:44:46.578814Z","shell.execute_reply.started":"2021-07-29T04:44:46.429306Z","shell.execute_reply":"2021-07-29T04:44:46.578076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.val_reg_loss)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:44:46.580565Z","iopub.execute_input":"2021-07-29T04:44:46.581177Z","iopub.status.idle":"2021-07-29T04:44:46.713108Z","shell.execute_reply.started":"2021-07-29T04:44:46.581075Z","shell.execute_reply":"2021-07-29T04:44:46.712098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.val_bin_loss)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:44:46.714472Z","iopub.execute_input":"2021-07-29T04:44:46.714837Z","iopub.status.idle":"2021-07-29T04:44:46.838775Z","shell.execute_reply.started":"2021-07-29T04:44:46.7148Z","shell.execute_reply":"2021-07-29T04:44:46.837837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(trainer.val_bin_cum_loss)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:44:46.842215Z","iopub.execute_input":"2021-07-29T04:44:46.842461Z","iopub.status.idle":"2021-07-29T04:44:46.96877Z","shell.execute_reply.started":"2021-07-29T04:44:46.842436Z","shell.execute_reply":"2021-07-29T04:44:46.967947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"train_df=pd.read_csv('../input/commonlit-kfold-dataset/fold_train.csv')\ntrain_df['q']=train_df.target.apply(get_quantile, args=(train_qs, ))\ntrain_df['ybin']=train_df.apply(get_bin_distribution, args=(bin_ranges, ), axis=1)\n\nif CONFIG.env=='test':\n    train_df=train_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:44:46.970159Z","iopub.execute_input":"2021-07-29T04:44:46.970411Z","iopub.status.idle":"2021-07-29T04:44:56.659111Z","shell.execute_reply.started":"2021-07-29T04:44:46.970388Z","shell.execute_reply":"2021-07-29T04:44:56.658233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=32\n\ntest_dataset=CommonLitDataset(train_df)\ntest_dataloader=torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n                                            shuffle=False, \n                                            pin_memory=True, drop_last=False)\n\nprint(len(test_dataloader))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:44:56.660492Z","iopub.execute_input":"2021-07-29T04:44:56.660845Z","iopub.status.idle":"2021-07-29T04:44:56.667654Z","shell.execute_reply.started":"2021-07-29T04:44:56.660808Z","shell.execute_reply":"2021-07-29T04:44:56.666152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models=[\n    torch.load('./best_reg_model1.pt')\n]\n\n\n\nypreds=[]\nypred_bins=[]#np.zeros(len(train_df), CONFIG.bins)\n\nfor batch in test_dataloader:\n    batch_seq_lens=batch['seq_len']\n    batch_max_seq_len=torch.max(batch['seq_len'])\n\n    input_ids=batch['input_ids'][:, :batch_max_seq_len].to(CONFIG.device)\n    attention_mask=batch['attention_mask'][:, :batch_max_seq_len].to(CONFIG.device)\n    batch_size=input_ids.size(0)\n    \n    with torch.no_grad():\n        batch_yhat=np.zeros(batch_size)\n        batch_yhat_bin=np.zeros( (batch_size, CONFIG.bins))\n        for model in models:\n            model.eval()\n            yhat, yhat_bin=model(input_ids, attention_mask)\n            yhat=yhat.view(-1).detach().cpu()\n            yhat_bin=yhat_bin.detach().cpu().numpy()\n            \n            batch_yhat+=yhat.numpy()\n            batch_yhat_bin+=yhat_bin\n            \n        batch_yhat/=len(models)\n        batch_yhat_bin/=len(models)\n        \n        ypreds+=batch_yhat.tolist()\n        ypred_bins+=batch_yhat_bin.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:44:56.66922Z","iopub.execute_input":"2021-07-29T04:44:56.669599Z","iopub.status.idle":"2021-07-29T04:45:24.231599Z","shell.execute_reply.started":"2021-07-29T04:44:56.669559Z","shell.execute_reply":"2021-07-29T04:45:24.230744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['yhat_target']=ypreds\ntrain_df['yhat_bins']=ypred_bins\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:45:24.232842Z","iopub.execute_input":"2021-07-29T04:45:24.233259Z","iopub.status.idle":"2021-07-29T04:45:24.264441Z","shell.execute_reply.started":"2021-07-29T04:45:24.233225Z","shell.execute_reply":"2021-07-29T04:45:24.263382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.to_csv('Train Inference.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T04:45:24.265972Z","iopub.execute_input":"2021-07-29T04:45:24.266608Z","iopub.status.idle":"2021-07-29T04:45:24.543112Z","shell.execute_reply.started":"2021-07-29T04:45:24.266558Z","shell.execute_reply":"2021-07-29T04:45:24.54233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}