{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-21T14:34:11.728233Z","iopub.execute_input":"2021-05-21T14:34:11.728714Z","iopub.status.idle":"2021-05-21T14:34:11.743997Z","shell.execute_reply.started":"2021-05-21T14:34:11.728611Z","shell.execute_reply":"2021-05-21T14:34:11.742455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport nltk\n\nfrom torchtext.vocab import GloVe","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:34:12.21935Z","iopub.execute_input":"2021-05-21T14:34:12.219811Z","iopub.status.idle":"2021-05-21T14:34:15.626524Z","shell.execute_reply.started":"2021-05-21T14:34:12.219767Z","shell.execute_reply":"2021-05-21T14:34:15.625414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# In this Notebook, I will preprocess the texts with GloVe and store the resulting vectors for usage in other Notebooks.\n(with and without stopword removal)","metadata":{}},{"cell_type":"markdown","source":"## Load train and test data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:34:15.628381Z","iopub.execute_input":"2021-05-21T14:34:15.628759Z","iopub.status.idle":"2021-05-21T14:34:15.780888Z","shell.execute_reply.started":"2021-05-21T14:34:15.628721Z","shell.execute_reply":"2021-05-21T14:34:15.779756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the GloVe pretrained embedding\n\n## Options:\n\n* Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, ca. 822 MB download):\n  * EMBEDDING_NAME = \"6B\"\n  * EMBEDDING_DIM = [50, 100, 200, 300]\n* Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, ca. 1.75 GB download): \n  * EMBEDDING_NAME = \"42B\"\n  * EMBEDDING_DIM = 300\n* Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, ca. 2.03 GB download): \n  * EMBEDDING_NAME = \"840B\"\n  * EMBEDDING_DIM = 300\n* Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, ca. 1.42 GB download):\n  * EMBEDDING_NAME = \"twitter.27B\"\n  * EMBEDDING_DIM = [25, 50, 100, 200]","metadata":{}},{"cell_type":"code","source":"EMBEDDING_DIM = 300\nEMBEDDING_NAME = \"6B\"","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:34:23.524592Z","iopub.execute_input":"2021-05-21T14:34:23.52521Z","iopub.status.idle":"2021-05-21T14:34:23.53017Z","shell.execute_reply.started":"2021-05-21T14:34:23.525153Z","shell.execute_reply":"2021-05-21T14:34:23.528663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove = GloVe(name=EMBEDDING_NAME, dim=EMBEDDING_DIM)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:34:23.725437Z","iopub.execute_input":"2021-05-21T14:34:23.725853Z","iopub.status.idle":"2021-05-21T14:38:29.471809Z","shell.execute_reply.started":"2021-05-21T14:34:23.725804Z","shell.execute_reply":"2021-05-21T14:38:29.470642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_words = nltk.corpus.stopwords.words('english') # Load standard english stopwords\nremove_words.extend([\".\", \",\", \"!\", \"?\", \"'\", \":\", \";\", '\"', \"-\", \"''\", '``']) # The nltk tokenizer will also use these as tokens, but I want to remove them\n\n\ndef get_vocab(text, stopword_removal=False):\n    words = nltk.tokenize.word_tokenize(text.lower())\n    words_clean = [word for word in words]\n    if stopword_removal:\n        words_clean = [word for word in words_clean if word not in remove_words]\n    return words_clean","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:38:29.473872Z","iopub.execute_input":"2021-05-21T14:38:29.474313Z","iopub.status.idle":"2021-05-21T14:38:29.504555Z","shell.execute_reply.started":"2021-05-21T14:38:29.474265Z","shell.execute_reply":"2021-05-21T14:38:29.50327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The magic happens here, extract GloVe representations for each word\n\ndef get_word_vectors(text, stopword_removal=False, text_len=270):\n    words = get_vocab(text, stopword_removal=stopword_removal)\n    result = np.zeros((text_len, EMBEDDING_DIM))\n    for i, word in enumerate(words):\n        result[i,:] = glove[word]\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:38:29.506427Z","iopub.execute_input":"2021-05-21T14:38:29.506741Z","iopub.status.idle":"2021-05-21T14:38:29.512548Z","shell.execute_reply.started":"2021-05-21T14:38:29.506708Z","shell.execute_reply":"2021-05-21T14:38:29.511569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Look for the maximum sentence length","metadata":{}},{"cell_type":"code","source":"print(\"Stentence length\\n\\t* with stopword removal: {}\\n\\t* without stopword removal: {}\".format(train.excerpt.apply(lambda x: len(get_vocab(x, stopword_removal=True))).max(), train.excerpt.apply(lambda x: len(get_vocab(x, stopword_removal=False))).max()))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:32:48.109298Z","iopub.execute_input":"2021-05-21T14:32:48.109723Z","iopub.status.idle":"2021-05-21T14:33:01.735846Z","shell.execute_reply.started":"2021-05-21T14:32:48.109682Z","shell.execute_reply":"2021-05-21T14:33:01.734899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First without stopword removal","metadata":{}},{"cell_type":"code","source":"def extract_and_save_representation(data, stopword_removal, text_len, filename):\n    # extract\n    get_word_vectors_lambda = lambda x: get_word_vectors(x, stopword_removal=stopword_removal, text_len=text_len)\n    emb_text = data.excerpt.apply(get_word_vectors_lambda)\n    emb_text = np.stack(emb_text)\n    \n    # save\n    if \"target\" in data.columns: # Test data has no target column\n        pickle.dump({\"id\": data.id.values, \"target\": data.target.values, \"wordvectors\": emb_text}, open(filename, \"wb\"))\n    else:\n        pickle.dump({\"id\": data.id.values, \"wordvectors\": emb_text}, open(filename, \"wb\"))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:41:56.708345Z","iopub.execute_input":"2021-05-21T14:41:56.708814Z","iopub.status.idle":"2021-05-21T14:41:56.717801Z","shell.execute_reply.started":"2021-05-21T14:41:56.708777Z","shell.execute_reply":"2021-05-21T14:41:56.716089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# without stopword removal\nextract_and_save_representation(train, False, 270, \"train_with_stopwords_glove_{}_{}.pkl\".format(EMBEDDING_NAME, EMBEDDING_DIM))\nextract_and_save_representation(test, False, 270, \"test_with_stopwords_glove_{}_{}.pkl\".format(EMBEDDING_NAME, EMBEDDING_DIM))\n\n# with stopword removal\nextract_and_save_representation(train, True, 143, \"train_without_stopwords_glove_{}_{}.pkl\".format(EMBEDDING_NAME, EMBEDDING_DIM))\nextract_and_save_representation(test, True, 143, \"test_without_stopwords_glove_{}_{}.pkl\".format(EMBEDDING_NAME, EMBEDDING_DIM))\n\n# would be more clean as loop over configurations","metadata":{"execution":{"iopub.status.busy":"2021-05-21T14:45:05.401646Z","iopub.execute_input":"2021-05-21T14:45:05.402173Z","iopub.status.idle":"2021-05-21T14:45:40.790876Z","shell.execute_reply.started":"2021-05-21T14:45:05.402123Z","shell.execute_reply":"2021-05-21T14:45:40.789887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The output now contains 4 files: Train and test data with and without stopwords","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}