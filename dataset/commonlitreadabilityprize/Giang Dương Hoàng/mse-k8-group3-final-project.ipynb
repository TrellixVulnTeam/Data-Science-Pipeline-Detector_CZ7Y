{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-01T09:24:26.512595Z","iopub.execute_input":"2021-08-01T09:24:26.512969Z","iopub.status.idle":"2021-08-01T09:24:26.525172Z","shell.execute_reply.started":"2021-08-01T09:24:26.512938Z","shell.execute_reply":"2021-08-01T09:24:26.524201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data pre-process","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ntest  = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\nsubmission = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:24:26.526592Z","iopub.execute_input":"2021-08-01T09:24:26.527123Z","iopub.status.idle":"2021-08-01T09:24:26.594247Z","shell.execute_reply.started":"2021-08-01T09:24:26.527089Z","shell.execute_reply":"2021-08-01T09:24:26.593355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['processed_text'] = train['excerpt']\ntest['processed_text'] =  test['excerpt']","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:24:26.596129Z","iopub.execute_input":"2021-08-01T09:24:26.596814Z","iopub.status.idle":"2021-08-01T09:24:26.604248Z","shell.execute_reply.started":"2021-08-01T09:24:26.596776Z","shell.execute_reply":"2021-08-01T09:24:26.603002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apostrophe Dictionary(Từ điển viết tắt)\napostrophe_dict = {\n\"ain't\": \"am not / are not\",\n\"aren't\": \"are not / am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had / he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall / he will\",\n\"he'll've\": \"he shall have / he will have\",\n\"he's\": \"he has / he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has / how is\",\n\"i'd\": \"I had / I would\",\n\"i'd've\": \"I would have\",\n\"i'll\": \"I shall / I will\",\n\"i'll've\": \"I shall have / I will have\",\n\"i'm\": \"I am\",\n\"i've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had / it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall / it will\",\n\"it'll've\": \"it shall have / it will have\",\n\"it's\": \"it has / it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had / she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall / she will\",\n\"she'll've\": \"she shall have / she will have\",\n\"she's\": \"she has / she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as / so is\",\n\"that'd\": \"that would / that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has / that is\",\n\"there'd\": \"there had / there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has / there is\",\n\"they'd\": \"they had / they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall / they will\",\n\"they'll've\": \"they shall have / they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had / we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall / what will\",\n\"what'll've\": \"what shall have / what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has / what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has / when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has / where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall / who will\",\n\"who'll've\": \"who shall have / who will have\",\n\"who's\": \"who has / who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has / why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had / you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall / you will\",\n\"you'll've\": \"you shall have / you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\n\n# Function replace acronyms (Hàm xử lý từ viết tắt -> từ đầy đủ)\ndef lookup_dict(text, dictionary):\n    for word in text.split():\n        if word.lower() in dictionary:\n            if word.lower() in text.split():\n                text = text.replace(word, dictionary[word.lower()])\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:24:26.606391Z","iopub.execute_input":"2021-08-01T09:24:26.60678Z","iopub.status.idle":"2021-08-01T09:24:26.627048Z","shell.execute_reply.started":"2021-08-01T09:24:26.606747Z","shell.execute_reply":"2021-08-01T09:24:26.625809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using lambda function to handles predefined dictionary abbreviations and assign value to new column\n# Sử dụng lambda function để tiền xử lý dữ liệu thay thế các từ viết tắt bằng các từ đầy đủ của chúng và gán giá trị đã xử lý vào cột mới\ntrain['processed_text'] = train['processed_text'].apply(lambda x: lookup_dict(x,apostrophe_dict))\ntest['processed_text'] = test['processed_text'].apply(lambda x: lookup_dict(x,apostrophe_dict))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:24:26.628526Z","iopub.execute_input":"2021-08-01T09:24:26.628899Z","iopub.status.idle":"2021-08-01T09:24:26.783119Z","shell.execute_reply.started":"2021-08-01T09:24:26.628862Z","shell.execute_reply":"2021-08-01T09:24:26.782068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nnltk.download('stopwords') \nps = PorterStemmer()\nwords = stopwords.words(\"english\")\n\ntrain['processed_text'] = train['excerpt'].apply(lambda x: \" \".join([ps.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\ntest['processed_text'] = test['excerpt'].apply(lambda x: \" \".join([ps.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:24:26.784522Z","iopub.execute_input":"2021-08-01T09:24:26.78481Z","iopub.status.idle":"2021-08-01T09:24:36.283536Z","shell.execute_reply.started":"2021-08-01T09:24:26.784782Z","shell.execute_reply":"2021-08-01T09:24:36.282386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\n\n# Make all words lower case\ntrain['processed_text'] = train['processed_text'].str.lower()\ntest['processed_text'] = test['processed_text'].str.lower()\n\n# Remove punctuation\ntable = str.maketrans('', '', string.punctuation)\ntrain['processed_text'] = [train['processed_text'][row].translate(table) for row in range(len(train['processed_text']))]\ntest['processed_text'] = [test['processed_text'][row].translate(table) for row in range(len(test['processed_text']))]\n\n# Remove hash tags\ntrain['processed_text'] = train['processed_text'].str.replace(\"#\", \" \")\ntest['processed_text'] = test['processed_text'].str.replace(\"#\", \" \")\n\n# Remove words less than 1 character\ntrain['processed_text'] = train['processed_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\ntest['processed_text'] = test['processed_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:24:36.285309Z","iopub.execute_input":"2021-08-01T09:24:36.28561Z","iopub.status.idle":"2021-08-01T09:24:36.402167Z","shell.execute_reply.started":"2021-08-01T09:24:36.285583Z","shell.execute_reply":"2021-08-01T09:24:36.401496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove rare words (Xóa những từ hiếm gặp)\nfrom collections import Counter\nfrom itertools import chain\n\n# split words into lists\nv = train['processed_text'].str.split().tolist() \n# compute global word frequency\nc = Counter(chain.from_iterable(v))\n# filter, join, and re-assign\ntrain['processed_text'] = [' '.join([j for j in i if c[j] > 1]) for i in v]\n\n# split words into lists\nv = test['processed_text'].str.split().tolist() \n# compute global word frequency\nc = Counter(chain.from_iterable(v))\n# filter, join, and re-assign\ntest['processed_text'] = [' '.join([j for j in i if c[j] > 1]) for i in v]","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:24:36.403478Z","iopub.execute_input":"2021-08-01T09:24:36.40393Z","iopub.status.idle":"2021-08-01T09:24:36.555639Z","shell.execute_reply.started":"2021-08-01T09:24:36.4039Z","shell.execute_reply":"2021-08-01T09:24:36.55482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=train['target']\nX=train['processed_text']\nX_test=test['processed_text']","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:24:36.556682Z","iopub.execute_input":"2021-08-01T09:24:36.557102Z","iopub.status.idle":"2021-08-01T09:24:36.561126Z","shell.execute_reply.started":"2021-08-01T09:24:36.557072Z","shell.execute_reply":"2021-08-01T09:24:36.560251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.10, random_state=42, shuffle=True)\nX_train.shape, X_val.shape, y_train.shape,y_val.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:24:36.56236Z","iopub.execute_input":"2021-08-01T09:24:36.562676Z","iopub.status.idle":"2021-08-01T09:24:36.581327Z","shell.execute_reply.started":"2021-08-01T09:24:36.562648Z","shell.execute_reply":"2021-08-01T09:24:36.580163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Feature extraction\n# Using stop_word into TfidfVectorizer (Term-Frequency Inverse Document-Frequency)\nvectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\ntrain_tfIdf = vectorizer_tfidf.fit_transform(X_train.values.astype('U'))\nval_tfIdf = vectorizer_tfidf.transform(X_val.values.astype('U'))\nX_test_tfIdf = vectorizer_tfidf.transform(X_test.values.astype('U'))\nprint(vectorizer_tfidf.get_feature_names()[:5])","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:24:36.582659Z","iopub.execute_input":"2021-08-01T09:24:36.58297Z","iopub.status.idle":"2021-08-01T09:24:37.061372Z","shell.execute_reply.started":"2021-08-01T09:24:36.582941Z","shell.execute_reply":"2021-08-01T09:24:37.060298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define model","metadata":{}},{"cell_type":"code","source":"from sklearn.gaussian_process import GaussianProcessRegressor as GP\n\n# Build model using GaussianProcessRegressor \n# Using GaussianProcessRegressor because GP regression works well on small datasets by assuming the data points lie on a multi-dimensional distribution (Multivariate Normal))\nmodel = GP(random_state=1).fit(train_tfIdf.todense(), y_train)\nprint(model.score(train_tfIdf.todense(), y_train))\ny_pred = model.predict(val_tfIdf.todense())\nmodel.score(val_tfIdf.todense(), y_val)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:24:37.062557Z","iopub.execute_input":"2021-08-01T09:24:37.062854Z","iopub.status.idle":"2021-08-01T09:27:05.150971Z","shell.execute_reply.started":"2021-08-01T09:24:37.062825Z","shell.execute_reply":"2021-08-01T09:27:05.146516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(X_test_tfIdf.todense())\nsubmission.target = predictions\nsubmission.to_csv('submission.csv', index=False)\nsubmission = pd.read_csv(\"submission.csv\")\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:27:48.591005Z","iopub.execute_input":"2021-08-01T09:27:48.591823Z","iopub.status.idle":"2021-08-01T09:27:49.129493Z","shell.execute_reply.started":"2021-08-01T09:27:48.591779Z","shell.execute_reply":"2021-08-01T09:27:49.128813Z"},"trusted":true},"execution_count":null,"outputs":[]}]}