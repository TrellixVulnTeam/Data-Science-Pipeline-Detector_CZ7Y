{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n# from wtfml.utils import EarlyStopping\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\nimport transformers\nfrom transformers import AutoModel, BertTokenizerFast\nfrom transformers import RobertaTokenizer, RobertaModel\nfrom transformers import AdamW\nfrom transformers import AutoConfig\n\nfrom tqdm import tqdm\n","metadata":{"_uuid":"6f407fea-9df7-4010-a978-dd510bb9f143","_cell_guid":"79d31724-0ee9-4691-bbeb-1dc268b783b6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-06T02:22:41.161742Z","iopub.execute_input":"2021-08-06T02:22:41.162181Z","iopub.status.idle":"2021-08-06T02:22:41.173688Z","shell.execute_reply.started":"2021-08-06T02:22:41.162113Z","shell.execute_reply":"2021-08-06T02:22:41.172451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reading the data\ntrain = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('/kaggle/input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"_uuid":"5caeb7a3-8aef-4536-a5b3-d99507edcc5d","_cell_guid":"f6497c9a-c3e1-4809-b8e3-42a7089c84fc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-06T02:22:41.399088Z","iopub.execute_input":"2021-08-06T02:22:41.399442Z","iopub.status.idle":"2021-08-06T02:22:41.447615Z","shell.execute_reply.started":"2021-08-06T02:22:41.399414Z","shell.execute_reply":"2021-08-06T02:22:41.446751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parameters\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMAX_LENGTH = 311\nBATCH_SIZE = 16\nNUM_EPOCHS = 40\nNUM_WORKERS = 6\nCHECKPOINT_FILE = ''\nPIN_MEMORY = True\nSAVE_MODEL = True\nLOAD_MODEL = False\nPRETRAINED_MODEL = 'roberta-base'\nFREEZE = False\nPATIENCE = 5\nN_FOLDS = 5\ntokenizer = RobertaTokenizer.from_pretrained('../input/roberta-transformers-pytorch/roberta-base')","metadata":{"_uuid":"f355c8e3-de50-4c8b-a977-1034116eaa76","_cell_guid":"cb67d347-d6b2-47c9-91ef-6d1006b57959","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-08-06T02:22:42.536967Z","iopub.execute_input":"2021-08-06T02:22:42.537312Z","iopub.status.idle":"2021-08-06T02:22:42.63008Z","shell.execute_reply.started":"2021-08-06T02:22:42.537282Z","shell.execute_reply":"2021-08-06T02:22:42.629213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating our dataset\nclass DRDataset(Dataset):\n    def __init__(self, text_col, label_col, train=True):\n        super().__init__()\n        self.text_col = text_col\n        self.label_col = label_col\n        self.train = train\n\n    def __len__(self):\n        return self.text_col.shape[0] \n\n    def __getitem__(self, index):\n        # get text and label if test use -1 for label\n        text, label = (self.text_col.iloc[index], self.label_col.iloc[index]) if self.train else \\\n                      (self.text_col.iloc[index], -1)\n        \n        # tokenize and encode\n        tokens = tokenizer.encode_plus(\n                    text,\n                    padding='max_length',\n                    max_length=MAX_LENGTH,\n                    truncation=True,\n                    return_token_type_ids=False\n            )\n        \n        # seq, mask, and label to tensor\n        seq = torch.tensor(tokens['input_ids'])\n        mask = torch.tensor(tokens['attention_mask'])\n        y = torch.tensor(label) if self.train else torch.tensor(-1)  \n        \n        return seq, mask, y\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-06T02:22:43.663463Z","iopub.execute_input":"2021-08-06T02:22:43.663796Z","iopub.status.idle":"2021-08-06T02:22:43.67223Z","shell.execute_reply.started":"2021-08-06T02:22:43.663765Z","shell.execute_reply":"2021-08-06T02:22:43.670986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make prediction\ndef make_prediction(model, loader, output_csv=\"submission.csv\"):\n    preds = []\n    model.eval()\n\n    for batch in tqdm(loader):\n        batch = [b.to(device=DEVICE) for b in batch]\n        seq, mask, _ = batch\n        \n        with torch.no_grad():\n            pred = model(seq, mask)\n            preds.extend(pred.squeeze(1).cpu().numpy())\n        \n    sample['target'] = preds\n    print(sample.head())\n #   sample.to_csv(\"submission.csv\", index=False)\n    \n    model.train()\n    print(\"Done with predictions\")\n    return preds\n\n# load model    \ndef load_checkpoint(checkpoint, model):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-06T02:24:46.999504Z","iopub.execute_input":"2021-08-06T02:24:46.99984Z","iopub.status.idle":"2021-08-06T02:24:47.006833Z","shell.execute_reply.started":"2021-08-06T02:24:46.999811Z","shell.execute_reply":"2021-08-06T02:24:47.005705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model architecture\nclass Roberta_Arch(nn.Module):\n    \n    def __init__(self, roberta):\n        super(Roberta_Arch, self).__init__()\n        self.roberta = roberta \n        self.dropout = nn.Dropout(0.1)\n        self.relu =  nn.ReLU()\n        self.fc1 = nn.Linear(768, 512)\n        self.fc2 = nn.Linear(512, 1)\n\n    def forward(self, sent_id, mask):\n        _, cls_hs = self.roberta(sent_id, attention_mask=mask, return_dict=False)\n        x = self.fc1(cls_hs)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-06T02:24:47.277305Z","iopub.execute_input":"2021-08-06T02:24:47.277687Z","iopub.status.idle":"2021-08-06T02:24:47.284939Z","shell.execute_reply.started":"2021-08-06T02:24:47.277656Z","shell.execute_reply":"2021-08-06T02:24:47.283663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# main function\ndef main_test(test_loader, model_path):\n    \n    roberta = AutoModel.from_pretrained('../input/roberta-transformers-pytorch/roberta-base')\n    model = Roberta_Arch(roberta)\n    model = model.to(DEVICE)\n    \n    # loading the model\n    load_checkpoint(torch.load(model_path), model)\n    print(f\"model {model_path} loaded successfully\")\n\n    return make_prediction(model, test_loader)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-06T02:25:28.985339Z","iopub.execute_input":"2021-08-06T02:25:28.985674Z","iopub.status.idle":"2021-08-06T02:25:28.992007Z","shell.execute_reply.started":"2021-08-06T02:25:28.985644Z","shell.execute_reply":"2021-08-06T02:25:28.990867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the data test\ndef loading_data_test(data_text):\n    data_ds = DRDataset(\n            text_col=data_text,\n            label_col=-1,\n            train=False,\n    )\n    \n    data_loader = DataLoader(\n        data_ds,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n    )\n   \n    return data_loader\n","metadata":{"execution":{"iopub.status.busy":"2021-08-06T02:25:31.129551Z","iopub.execute_input":"2021-08-06T02:25:31.129875Z","iopub.status.idle":"2021-08-06T02:25:31.134523Z","shell.execute_reply.started":"2021-08-06T02:25:31.129845Z","shell.execute_reply":"2021-08-06T02:25:31.1335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred for each kfold\ndef test_pred_kfold():\n    kfold_preds = []\n    test_text = test['excerpt']\n    test_loader = loading_data_test(test_text)\n    for fold in range(1):\n        model_path = f'../input/commonlitkfoldmodels/model.roberta-base.fold.{fold}.lr.3e-05.wd.0.01'\n        pred = main_test(test_loader, model_path)\n        kfold_preds.append(pred)\n    \n    return kfold_preds","metadata":{"execution":{"iopub.status.busy":"2021-08-06T02:26:55.537866Z","iopub.execute_input":"2021-08-06T02:26:55.538273Z","iopub.status.idle":"2021-08-06T02:26:55.543689Z","shell.execute_reply.started":"2021-08-06T02:26:55.538235Z","shell.execute_reply":"2021-08-06T02:26:55.542587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction for each fold\npred_k = test_pred_kfold()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T02:26:55.824891Z","iopub.execute_input":"2021-08-06T02:26:55.825279Z","iopub.status.idle":"2021-08-06T02:26:57.812556Z","shell.execute_reply.started":"2021-08-06T02:26:55.825243Z","shell.execute_reply":"2021-08-06T02:26:57.809409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # final pred for submission\n# def final_mean_pred_weighted():\n# #    pred_k = test_pred_kfold()\n#     # auc of each model\n#     auc_kfold = torch.tensor([0.49808, 0.56205, 0.50327, 0.51976, 0.51563])    \n#     m = nn.Softmax(dim=0)  \n#     weights = m(1/auc_kfold)\n    \n#     final_pred = torch.tensor(pred_k[0])*weights[0] \\\n#                + torch.tensor(pred_k[1])*weights[1] \\\n#                + torch.tensor(pred_k[2])*weights[2] \\\n#                + torch.tensor(pred_k[3])*weights[3] \\\n#                + torch.tensor(pred_k[4])*weights[4]\n                                \n#     sample['target'] = final_pred\n#     print(sample.head())\n#   #  sample.to_csv(\"submission.csv\", index=False)\n\n# final_mean_pred_weighted()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T02:25:34.713101Z","iopub.execute_input":"2021-08-06T02:25:34.713478Z","iopub.status.idle":"2021-08-06T02:25:34.720207Z","shell.execute_reply.started":"2021-08-06T02:25:34.713443Z","shell.execute_reply":"2021-08-06T02:25:34.719151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final pred for submission\ndef final_best_pred_auc():\n    # pred_k = test_pred_kfold()\n    # auc of each model\n    final_pred = pred_k[0]\n                                \n    sample['target'] = final_pred\n    print(sample.head())\n    sample.to_csv(\"submission.csv\", index=False)\n\nfinal_best_pred_auc()","metadata":{"execution":{"iopub.status.busy":"2021-08-06T02:29:37.099672Z","iopub.execute_input":"2021-08-06T02:29:37.100021Z","iopub.status.idle":"2021-08-06T02:29:37.109957Z","shell.execute_reply.started":"2021-08-06T02:29:37.099983Z","shell.execute_reply":"2021-08-06T02:29:37.108888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\npatience = 5\nusing kfold \nfold 0: val auc 0.49808 attained in epoch 13\nfold 1: val auc 0.56205 attained in epoch 1\nfold 2: val auc 0.50327 attained in epoch 9\nfold 3: val auc 0.51976 attained in epoch 0\nfold 4: val auc 0.51563 attained in epoch 3\n\ngot 0.499 auc on test set when using mean of kfold preds\ngot 0.498 auc on test set when using weighted average (1/softmax(auc)) of kfold preds\n\n\n\n\n\n\n'''","metadata":{"execution":{"iopub.status.busy":"2021-08-05T21:01:41.436621Z","iopub.execute_input":"2021-08-05T21:01:41.43724Z","iopub.status.idle":"2021-08-05T21:01:41.443448Z","shell.execute_reply.started":"2021-08-05T21:01:41.4372Z","shell.execute_reply":"2021-08-05T21:01:41.442546Z"},"trusted":true},"execution_count":null,"outputs":[]}]}