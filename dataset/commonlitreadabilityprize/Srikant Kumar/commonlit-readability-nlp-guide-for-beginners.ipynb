{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-22T12:16:56.905241Z","iopub.execute_input":"2021-06-22T12:16:56.905602Z","iopub.status.idle":"2021-06-22T12:16:56.918528Z","shell.execute_reply.started":"2021-06-22T12:16:56.905525Z","shell.execute_reply":"2021-06-22T12:16:56.917622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Packages ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport string\nimport re\nimport math\n\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, TweedieRegressor,HuberRegressor\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import mean_squared_error as mse\n\n\nimport xgboost as xgb\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n\nimport kerastuner as kt\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.io import curdoc, show, output_notebook\noutput_notebook()\n\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nstop_words = stopwords.words('english')\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')\nfrom spacy import displacy\n\nimport transformers\nfrom transformers import BertTokenizer, TFBertModel, RobertaTokenizer, TFRobertaModel\n","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:16:56.921336Z","iopub.execute_input":"2021-06-22T12:16:56.92201Z","iopub.status.idle":"2021-06-22T12:17:14.032693Z","shell.execute_reply.started":"2021-06-22T12:16:56.921907Z","shell.execute_reply":"2021-06-22T12:17:14.03186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ndf_test = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\ndf_submission = pd.read_csv('/kaggle/input/commonlitreadabilityprize/sample_submission.csv')\n\nprint(\" Training dataset shape : \" + str(df_train.shape))\nprint(\" Testing dataset shape : \" + str(df_test.shape))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:14.034155Z","iopub.execute_input":"2021-06-22T12:17:14.034485Z","iopub.status.idle":"2021-06-22T12:17:14.132003Z","shell.execute_reply.started":"2021-06-22T12:17:14.034449Z","shell.execute_reply":"2021-06-22T12:17:14.131103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:14.133761Z","iopub.execute_input":"2021-06-22T12:17:14.134257Z","iopub.status.idle":"2021-06-22T12:17:14.161948Z","shell.execute_reply.started":"2021-06-22T12:17:14.134219Z","shell.execute_reply":"2021-06-22T12:17:14.161109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:14.165075Z","iopub.execute_input":"2021-06-22T12:17:14.165344Z","iopub.status.idle":"2021-06-22T12:17:14.178649Z","shell.execute_reply.started":"2021-06-22T12:17:14.165311Z","shell.execute_reply":"2021-06-22T12:17:14.177878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:14.180149Z","iopub.execute_input":"2021-06-22T12:17:14.180683Z","iopub.status.idle":"2021-06-22T12:17:14.191248Z","shell.execute_reply.started":"2021-06-22T12:17:14.180626Z","shell.execute_reply":"2021-06-22T12:17:14.189991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['excerpt'][3]","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:14.193205Z","iopub.execute_input":"2021-06-22T12:17:14.193683Z","iopub.status.idle":"2021-06-22T12:17:14.202125Z","shell.execute_reply.started":"2021-06-22T12:17:14.193621Z","shell.execute_reply":"2021-06-22T12:17:14.200987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n\nLets start with null values ","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:14.20366Z","iopub.execute_input":"2021-06-22T12:17:14.204053Z","iopub.status.idle":"2021-06-22T12:17:14.215385Z","shell.execute_reply.started":"2021-06-22T12:17:14.204016Z","shell.execute_reply":"2021-06-22T12:17:14.214524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:14.218924Z","iopub.execute_input":"2021-06-22T12:17:14.21924Z","iopub.status.idle":"2021-06-22T12:17:14.226894Z","shell.execute_reply.started":"2021-06-22T12:17:14.219212Z","shell.execute_reply":"2021-06-22T12:17:14.225911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above results we can say that only url_legal and license columns appear to be having missing values","metadata":{}},{"cell_type":"markdown","source":"### Target Distribution and std_error distribution","metadata":{}},{"cell_type":"code","source":"# Train\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(df_train['target'], ax=ax, color ='green')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:14.229196Z","iopub.execute_input":"2021-06-22T12:17:14.229585Z","iopub.status.idle":"2021-06-22T12:17:14.522873Z","shell.execute_reply.started":"2021-06-22T12:17:14.22955Z","shell.execute_reply":"2021-06-22T12:17:14.522009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(df_train['standard_error'], ax=ax, color ='green')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:14.524265Z","iopub.execute_input":"2021-06-22T12:17:14.524639Z","iopub.status.idle":"2021-06-22T12:17:14.802767Z","shell.execute_reply.started":"2021-06-22T12:17:14.524599Z","shell.execute_reply":"2021-06-22T12:17:14.801851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets Find Length of the excerpt column and Count of number of words in the excerpt column","metadata":{}},{"cell_type":"code","source":"df_train['excerpt_len'] = df_train['excerpt'].apply(\n    lambda x : len(x)\n)\ndf_train['excerpt_word_count'] = df_train['excerpt'].apply(\n    lambda x : len(x.split(' '))\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:14.804272Z","iopub.execute_input":"2021-06-22T12:17:14.804657Z","iopub.status.idle":"2021-06-22T12:17:14.841196Z","shell.execute_reply.started":"2021-06-22T12:17:14.804601Z","shell.execute_reply":"2021-06-22T12:17:14.840171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(df_train['excerpt_len'], ax=ax, color ='green')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:14.84246Z","iopub.execute_input":"2021-06-22T12:17:14.842833Z","iopub.status.idle":"2021-06-22T12:17:15.070086Z","shell.execute_reply.started":"2021-06-22T12:17:14.842798Z","shell.execute_reply":"2021-06-22T12:17:15.069203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(df_train['excerpt_word_count'], ax=ax, color ='green')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:15.071495Z","iopub.execute_input":"2021-06-22T12:17:15.071874Z","iopub.status.idle":"2021-06-22T12:17:15.26611Z","shell.execute_reply.started":"2021-06-22T12:17:15.071837Z","shell.execute_reply":"2021-06-22T12:17:15.26516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Vocabulary and its frequency before performing and data preprocessing ","metadata":{}},{"cell_type":"code","source":"results = Counter()\ndf_train['excerpt'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:15.267395Z","iopub.execute_input":"2021-06-22T12:17:15.267885Z","iopub.status.idle":"2021-06-22T12:17:15.407652Z","shell.execute_reply.started":"2021-06-22T12:17:15.267847Z","shell.execute_reply":"2021-06-22T12:17:15.406852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Word_Freq=df_train.excerpt.str.split(expand=True).stack().value_counts().reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:15.408804Z","iopub.execute_input":"2021-06-22T12:17:15.40913Z","iopub.status.idle":"2021-06-22T12:17:15.858411Z","shell.execute_reply.started":"2021-06-22T12:17:15.40909Z","shell.execute_reply":"2021-06-22T12:17:15.85757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Word_Freq.to_csv('Train_Word_Freq.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:15.859657Z","iopub.execute_input":"2021-06-22T12:17:15.860016Z","iopub.status.idle":"2021-06-22T12:17:16.082059Z","shell.execute_reply.started":"2021-06-22T12:17:15.859982Z","shell.execute_reply":"2021-06-22T12:17:16.081226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Word_Freq_Test=df_test.excerpt.str.split(expand=True).stack().value_counts().reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:16.08337Z","iopub.execute_input":"2021-06-22T12:17:16.083716Z","iopub.status.idle":"2021-06-22T12:17:16.107963Z","shell.execute_reply.started":"2021-06-22T12:17:16.08368Z","shell.execute_reply":"2021-06-22T12:17:16.107118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:16.109371Z","iopub.execute_input":"2021-06-22T12:17:16.109732Z","iopub.status.idle":"2021-06-22T12:17:16.120687Z","shell.execute_reply.started":"2021-06-22T12:17:16.109697Z","shell.execute_reply":"2021-06-22T12:17:16.119607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Word_Freq_Test['index']","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:16.12222Z","iopub.execute_input":"2021-06-22T12:17:16.122647Z","iopub.status.idle":"2021-06-22T12:17:16.132738Z","shell.execute_reply.started":"2021-06-22T12:17:16.122596Z","shell.execute_reply":"2021-06-22T12:17:16.131838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_Words=list(set(Word_Freq_Test['index']) & set(Word_Freq['index']))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:16.134052Z","iopub.execute_input":"2021-06-22T12:17:16.134401Z","iopub.status.idle":"2021-06-22T12:17:16.162162Z","shell.execute_reply.started":"2021-06-22T12:17:16.134365Z","shell.execute_reply":"2021-06-22T12:17:16.161271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(common_Words)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:16.163524Z","iopub.execute_input":"2021-06-22T12:17:16.163909Z","iopub.status.idle":"2021-06-22T12:17:16.174591Z","shell.execute_reply.started":"2021-06-22T12:17:16.163875Z","shell.execute_reply":"2021-06-22T12:17:16.173735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top N-Grams and Visualization","metadata":{}},{"cell_type":"code","source":"def get_top_n_words(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' unigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus) \n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] \n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return freq_sorted[:n]\n\ndef get_top_n_bigram(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' bigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer(ngram_range = (2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return freq_sorted[:n]\n\ndef get_top_n_trigram(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' bigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer(ngram_range = (3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return freq_sorted[:n]","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:16.176072Z","iopub.execute_input":"2021-06-22T12:17:16.17673Z","iopub.status.idle":"2021-06-22T12:17:16.189262Z","shell.execute_reply.started":"2021-06-22T12:17:16.176685Z","shell.execute_reply":"2021-06-22T12:17:16.188408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_unigram = get_top_n_words(df_train['excerpt'], 20)\nwords = [i[0] for i in top_unigram]\ncount = [i[1] for i in top_unigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#6baed6'] * 20))\n\np = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Unigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\ncurdoc().theme = 'dark_minimal'\np.xgrid.grid_line_color = None\np.y_range.start = 0\np.title.align = 'center'\np.xaxis.major_label_orientation = \"vertical\"\nshow(p)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:16.194247Z","iopub.execute_input":"2021-06-22T12:17:16.194501Z","iopub.status.idle":"2021-06-22T12:17:17.236681Z","shell.execute_reply.started":"2021-06-22T12:17:16.194462Z","shell.execute_reply":"2021-06-22T12:17:17.235871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_bigram = get_top_n_bigram(df_train['excerpt'], 20)\nwords = [i[0] for i in top_bigram]\ncount = [i[1] for i in top_bigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#a1dab4'] * 20))\n\np1 = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Bigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np1.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\n# curdoc().theme = 'dark_minimal'\np1.xgrid.grid_line_color = None\np1.title.align = 'center'\np1.y_range.start = 0\np1.xaxis.major_label_orientation = \"vertical\"\nshow(p1)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:17.239517Z","iopub.execute_input":"2021-06-22T12:17:17.239791Z","iopub.status.idle":"2021-06-22T12:17:19.700853Z","shell.execute_reply.started":"2021-06-22T12:17:17.239763Z","shell.execute_reply":"2021-06-22T12:17:19.699882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_bigram = get_top_n_trigram(df_train['excerpt'], 20)\nwords = [i[0] for i in top_bigram]\ncount = [i[1] for i in top_bigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#a1dab4'] * 20))\n\np1 = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Bigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np1.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\n# curdoc().theme = 'dark_minimal'\np1.xgrid.grid_line_color = None\np1.title.align = 'center'\np1.y_range.start = 0\np1.xaxis.major_label_orientation = \"vertical\"\nshow(p1)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:19.702166Z","iopub.execute_input":"2021-06-22T12:17:19.702601Z","iopub.status.idle":"2021-06-22T12:17:23.425014Z","shell.execute_reply.started":"2021-06-22T12:17:19.702562Z","shell.execute_reply":"2021-06-22T12:17:23.424235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the plots we have so many unwanted tokens. We need to remove them and clean the data","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing\n\nData preprocessing is the process of converting raw data into a well-readable format to be used by a machine learning model.\n\n### Data Cleaning\nHere We will be doing list of things:\n* Stop Word Removal: While doing Vectorization, we assign some number or we transform it into a vector, extremely common words like ‘the’, ‘and’, etc. will become very important features while they add little meaning to the text. Your model can often be improved if you don’t take those words into account. Stop words are just a list of words you don’t want to use as features. \n* Punctuation Removal\n* URL/ Links Removal\n* Removal of Numbers \n* Non- Ascii Characters removal\n* Lemmatization","metadata":{}},{"cell_type":"code","source":"def removeStopwords(text):\n    doc = nlp(text)\n    clean_text = ' '\n    for txt in doc:\n        if (txt.is_stop == False):\n            clean_text = clean_text + \" \" + str(txt)        \n    \n    return clean_text\n\nprint(\"\\033[1mText before removeStopwords function: \\033[0m\" + df_train['excerpt'][20])\nprint(\"\\033[1mText after removeStopwords function: \\033[0m\" + removeStopwords(df_train['excerpt'][20]))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:23.426233Z","iopub.execute_input":"2021-06-22T12:17:23.426583Z","iopub.status.idle":"2021-06-22T12:17:23.485998Z","shell.execute_reply.started":"2021-06-22T12:17:23.426546Z","shell.execute_reply":"2021-06-22T12:17:23.485002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removePunctuations(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\nprint(\"\\033[1mText before removePunctuations function: \\033[0m\" + df_train['excerpt'][20])\nprint(\"\\n\")\nprint(\"\\033[1mText after removePunctuations function: \\033[0m\" + removePunctuations(df_train['excerpt'][20]))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:23.487313Z","iopub.execute_input":"2021-06-22T12:17:23.487677Z","iopub.status.idle":"2021-06-22T12:17:23.494007Z","shell.execute_reply.started":"2021-06-22T12:17:23.487625Z","shell.execute_reply":"2021-06-22T12:17:23.492931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeLinks(text):\n    clean_text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    #https? will match both http and https\n    #A|B, where A and B can be arbitrary REs, creates a regular expression that will match either A or B.\n    #\\S Matches any character which is not a whitespace character.\n    #+ Causes the resulting RE to match 1 or more repetitions of the preceding RE. ab+ will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’.\n    return clean_text","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:23.495349Z","iopub.execute_input":"2021-06-22T12:17:23.495739Z","iopub.status.idle":"2021-06-22T12:17:23.506865Z","shell.execute_reply.started":"2021-06-22T12:17:23.495702Z","shell.execute_reply":"2021-06-22T12:17:23.505966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeNumbers(text):\n    clean_text = re.sub(r'\\d+', '', text)\n    return clean_text\n\ntest_string = \"CR 7\"\n(test_string,removeNumbers(test_string))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:23.508002Z","iopub.execute_input":"2021-06-22T12:17:23.508505Z","iopub.status.idle":"2021-06-22T12:17:23.524943Z","shell.execute_reply.started":"2021-06-22T12:17:23.508467Z","shell.execute_reply":"2021-06-22T12:17:23.524122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(text):\n    text = text.lower() #Lets make it lowercase\n    text = removeStopwords(text)\n    text = removePunctuations(text)\n    text = removeNumbers(text)\n    text = removeLinks(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:23.528018Z","iopub.execute_input":"2021-06-22T12:17:23.528278Z","iopub.status.idle":"2021-06-22T12:17:23.534759Z","shell.execute_reply.started":"2021-06-22T12:17:23.528253Z","shell.execute_reply":"2021-06-22T12:17:23.533798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['excerpt_clean'] = df_train['excerpt'].apply(clean)\ndf_test['excerpt_clean'] = df_test['excerpt'].apply(clean)\n\ndf_train['excerpt_clean'] = df_train['excerpt_clean'].str.encode('ascii', 'ignore').str.decode('ascii')\ndf_test['excerpt_clean'] = df_test['excerpt_clean'].str.encode('ascii', 'ignore').str.decode('ascii')\n\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:17:23.537617Z","iopub.execute_input":"2021-06-22T12:17:23.537947Z","iopub.status.idle":"2021-06-22T12:18:40.620467Z","shell.execute_reply.started":"2021-06-22T12:17:23.537919Z","shell.execute_reply":"2021-06-22T12:18:40.619707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:25:31.440549Z","iopub.execute_input":"2021-06-22T12:25:31.440935Z","iopub.status.idle":"2021-06-22T12:25:31.456908Z","shell.execute_reply.started":"2021-06-22T12:25:31.440902Z","shell.execute_reply":"2021-06-22T12:25:31.455838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = Counter()\ndf_train['excerpt_clean'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:18:40.621665Z","iopub.execute_input":"2021-06-22T12:18:40.622002Z","iopub.status.idle":"2021-06-22T12:18:40.691833Z","shell.execute_reply.started":"2021-06-22T12:18:40.621966Z","shell.execute_reply":"2021-06-22T12:18:40.69093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_unigram = get_top_n_words(df_train['excerpt_clean'], 20)\nwords = [i[0] for i in top_unigram]\ncount = [i[1] for i in top_unigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#6baed6'] * 20))\n\np = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Unigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\ncurdoc().theme = 'dark_minimal'\np.xgrid.grid_line_color = None\np.y_range.start = 0\np.title.align = 'center'\np.xaxis.major_label_orientation = \"vertical\"\nshow(p)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:18:40.693115Z","iopub.execute_input":"2021-06-22T12:18:40.693599Z","iopub.status.idle":"2021-06-22T12:18:41.270759Z","shell.execute_reply.started":"2021-06-22T12:18:40.693558Z","shell.execute_reply":"2021-06-22T12:18:41.269847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_bigram = get_top_n_bigram(df_train['excerpt_clean'], 20)\nwords = [i[0] for i in top_bigram]\ncount = [i[1] for i in top_bigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#a1dab4'] * 20))\n\np1 = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Bigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np1.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\n# curdoc().theme = 'dark_minimal'\np1.xgrid.grid_line_color = None\np1.title.align = 'center'\np1.y_range.start = 0\np1.xaxis.major_label_orientation = \"vertical\"\nshow(p1)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:18:41.272089Z","iopub.execute_input":"2021-06-22T12:18:41.272435Z","iopub.status.idle":"2021-06-22T12:18:42.748088Z","shell.execute_reply.started":"2021-06-22T12:18:41.272399Z","shell.execute_reply":"2021-06-22T12:18:42.747149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_bigram = get_top_n_trigram(df_train['excerpt_clean'], 20)\nwords = [i[0] for i in top_bigram]\ncount = [i[1] for i in top_bigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#a1dab4'] * 20))\n\np1 = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Bigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np1.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\n# curdoc().theme = 'dark_minimal'\np1.xgrid.grid_line_color = None\np1.title.align = 'center'\np1.y_range.start = 0\np1.xaxis.major_label_orientation = \"vertical\"\nshow(p1)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:18:42.749375Z","iopub.execute_input":"2021-06-22T12:18:42.749725Z","iopub.status.idle":"2021-06-22T12:18:44.302353Z","shell.execute_reply.started":"2021-06-22T12:18:42.749688Z","shell.execute_reply":"2021-06-22T12:18:44.301436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Stemming","metadata":{}},{"cell_type":"code","source":"stemmer = SnowballStemmer(language='english')\n\ntokens = df_train['excerpt'][1].split()\nclean_text = ' '\n\nfor token in tokens:\n    print(token + ' --> ' + stemmer.stem(token))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:18:44.303683Z","iopub.execute_input":"2021-06-22T12:18:44.304035Z","iopub.status.idle":"2021-06-22T12:18:44.334858Z","shell.execute_reply.started":"2021-06-22T12:18:44.303998Z","shell.execute_reply":"2021-06-22T12:18:44.334047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def stemWord(text):\n    stemmer = SnowballStemmer(language='english')\n    tokens = text.split()\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text + \" \" + stemmer.stem(token)      \n    \n    return clean_text\n\nprint(\"\\033[1mText before stemWord function: \\033[0m\" + df_train['excerpt'][1])\nprint(\"\\033[1mText after stemWord function: \\033[0m\" + stemWord(df_train['excerpt'][1]))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:18:44.335946Z","iopub.execute_input":"2021-06-22T12:18:44.336429Z","iopub.status.idle":"2021-06-22T12:18:44.34571Z","shell.execute_reply.started":"2021-06-22T12:18:44.336392Z","shell.execute_reply":"2021-06-22T12:18:44.344702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['excerpt_clean_Stem'] = df_train['excerpt_clean'].apply(stemWord)\ndf_test['excerpt_clean_Stem'] = df_test['excerpt_clean'].apply(stemWord)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:18:44.346791Z","iopub.execute_input":"2021-06-22T12:18:44.347133Z","iopub.status.idle":"2021-06-22T12:18:48.344598Z","shell.execute_reply.started":"2021-06-22T12:18:44.347096Z","shell.execute_reply":"2021-06-22T12:18:48.343473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lemmatization","metadata":{}},{"cell_type":"code","source":"def lemmatizeWord(text):\n    tokens=nlp(text)\n    clean_text = ' '\n    for token in tokens:\n        clean_text = clean_text + \" \" + token.lemma_      \n    \n    return clean_text","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:18:48.348948Z","iopub.execute_input":"2021-06-22T12:18:48.349295Z","iopub.status.idle":"2021-06-22T12:18:48.357287Z","shell.execute_reply.started":"2021-06-22T12:18:48.349258Z","shell.execute_reply":"2021-06-22T12:18:48.356396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['excerpt_clean_Lemm'] = df_train['excerpt_clean'].apply(lemmatizeWord)\ndf_test['excerpt_clean_Lemm'] = df_test['excerpt_clean'].apply(lemmatizeWord)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:18:48.361681Z","iopub.execute_input":"2021-06-22T12:18:48.36239Z","iopub.status.idle":"2021-06-22T12:19:32.562176Z","shell.execute_reply.started":"2021-06-22T12:18:48.362351Z","shell.execute_reply":"2021-06-22T12:19:32.561329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = Counter()\ndf_train['excerpt_clean'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:32.563358Z","iopub.execute_input":"2021-06-22T12:19:32.563704Z","iopub.status.idle":"2021-06-22T12:19:32.63134Z","shell.execute_reply.started":"2021-06-22T12:19:32.563668Z","shell.execute_reply":"2021-06-22T12:19:32.630344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = Counter()\ndf_train['excerpt_clean_Lemm'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:32.632672Z","iopub.execute_input":"2021-06-22T12:19:32.633012Z","iopub.status.idle":"2021-06-22T12:19:32.697858Z","shell.execute_reply.started":"2021-06-22T12:19:32.632973Z","shell.execute_reply":"2021-06-22T12:19:32.696954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = Counter()\ndf_train['excerpt_clean_Stem'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:32.699039Z","iopub.execute_input":"2021-06-22T12:19:32.699527Z","iopub.status.idle":"2021-06-22T12:19:32.76121Z","shell.execute_reply.started":"2021-06-22T12:19:32.699489Z","shell.execute_reply":"2021-06-22T12:19:32.760373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Vectorization\n* Bag of Words (BoW)\n* TD IDF\n* Embeddings","metadata":{}},{"cell_type":"markdown","source":"## Bag of Words\nA bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n\nA vocabulary of known words.\nA measure of the presence of known words. It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\nAn n-gram is a contiguous sequence of n items from a given sample of text or speech\n\nAn n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\".","metadata":{}},{"cell_type":"markdown","source":"## Machine Learning\n### Unigram","metadata":{}},{"cell_type":"code","source":"rmse = lambda y_true, y_pred: np.sqrt(mse(y_true, y_pred))\nrmse_loss = lambda Estimator, X, y: rmse(y, Estimator.predict(X))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:32.762296Z","iopub.execute_input":"2021-06-22T12:19:32.762623Z","iopub.status.idle":"2021-06-22T12:19:32.768015Z","shell.execute_reply.started":"2021-06-22T12:19:32.762582Z","shell.execute_reply":"2021-06-22T12:19:32.766877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into train and test sets\n\n\nx = df_train['excerpt_clean']\ny = df_train['target']\n\nprint(len(x), len(y))\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\nprint(len(x_train), len(y_train))\nprint(len(x_test), len(y_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:32.769384Z","iopub.execute_input":"2021-06-22T12:19:32.76974Z","iopub.status.idle":"2021-06-22T12:19:32.781057Z","shell.execute_reply.started":"2021-06-22T12:19:32.769705Z","shell.execute_reply":"2021-06-22T12:19:32.780059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into train and test sets\n\n\nx_Lemm = df_train['excerpt_clean_Lemm']\ny_Lemm = df_train['target']\n\nprint(len(x_Lemm), len(y_Lemm))\n\nx_train_Lemm, x_test_Lemm, y_train_Lemm, y_test_Lemm = train_test_split(x_Lemm, y_Lemm, random_state=42)\nprint(len(x_train_Lemm), len(y_train_Lemm))\nprint(len(x_test_Lemm), len(y_test_Lemm))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:27:16.893184Z","iopub.execute_input":"2021-06-22T12:27:16.893513Z","iopub.status.idle":"2021-06-22T12:27:16.902865Z","shell.execute_reply.started":"2021-06-22T12:27:16.893484Z","shell.execute_reply":"2021-06-22T12:27:16.901758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into train and test sets\n\n\nx_Stem = df_train['excerpt_clean_Stem']\ny_Stem = df_train['target']\n\nprint(len(x_Stem), len(y_Stem))\n\nx_train_Stem, x_test_Stem, y_train_Stem, y_test_Stem = train_test_split(x_Stem, y_Stem, random_state=42)\nprint(len(x_train_Stem), len(y_train_Stem))\nprint(len(x_test_Stem), len(y_test_Stem))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:27:18.167004Z","iopub.execute_input":"2021-06-22T12:27:18.167357Z","iopub.status.idle":"2021-06-22T12:27:18.17711Z","shell.execute_reply.started":"2021-06-22T12:27:18.16732Z","shell.execute_reply":"2021-06-22T12:27:18.175201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:27:19.192889Z","iopub.execute_input":"2021-06-22T12:27:19.19323Z","iopub.status.idle":"2021-06-22T12:27:19.200357Z","shell.execute_reply.started":"2021-06-22T12:27:19.193201Z","shell.execute_reply":"2021-06-22T12:27:19.199194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,1)),\n    LinearRegression(),\n)\n\nmodel.fit(x_train,y_train)\n\nval_score = cross_val_score(\n    model, \n    x_test, \n    y_test, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,1): {val_score}')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:32.791437Z","iopub.execute_input":"2021-06-22T12:19:32.791899Z","iopub.status.idle":"2021-06-22T12:19:33.974494Z","shell.execute_reply.started":"2021-06-22T12:19:32.791862Z","shell.execute_reply":"2021-06-22T12:19:33.973603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predict=model.predict(df_test['excerpt_clean'])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:33.975911Z","iopub.execute_input":"2021-06-22T12:19:33.976489Z","iopub.status.idle":"2021-06-22T12:19:33.982403Z","shell.execute_reply.started":"2021-06-22T12:19:33.976452Z","shell.execute_reply":"2021-06-22T12:19:33.981436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predict","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:33.984001Z","iopub.execute_input":"2021-06-22T12:19:33.984675Z","iopub.status.idle":"2021-06-22T12:19:33.994469Z","shell.execute_reply.started":"2021-06-22T12:19:33.984609Z","shell.execute_reply":"2021-06-22T12:19:33.993359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:33.996181Z","iopub.execute_input":"2021-06-22T12:19:33.997049Z","iopub.status.idle":"2021-06-22T12:19:34.018515Z","shell.execute_reply.started":"2021-06-22T12:19:33.996936Z","shell.execute_reply":"2021-06-22T12:19:34.017585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission['target']=test_predict","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:34.019923Z","iopub.execute_input":"2021-06-22T12:19:34.020505Z","iopub.status.idle":"2021-06-22T12:19:34.025621Z","shell.execute_reply.started":"2021-06-22T12:19:34.020466Z","shell.execute_reply":"2021-06-22T12:19:34.024521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:34.02725Z","iopub.execute_input":"2021-06-22T12:19:34.027954Z","iopub.status.idle":"2021-06-22T12:19:34.039847Z","shell.execute_reply.started":"2021-06-22T12:19:34.027913Z","shell.execute_reply":"2021-06-22T12:19:34.038661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bigram","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(2,2)),\n    LinearRegression(),\n)\n\nmodel.fit(x_train,y_train)\n\nval_score = cross_val_score(\n    model, \n    x_test, \n    y_test, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(2,2): {val_score}')\n\nmodel_Stem = make_pipeline(\n    CountVectorizer(ngram_range=(2,2)),\n    LinearRegression(),\n)\n\nmodel_Stem.fit(x_train_Stem,y_train_Stem)\n\nval_score_Stem = cross_val_score(\n    model, \n    x_test_Stem, \n    y_test_Stem, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(2,2): {val_score_Stem}')\n\nmodel_Lemm = make_pipeline(\n    CountVectorizer(ngram_range=(2,2)),\n    LinearRegression(),\n)\n\nmodel_Lemm.fit(x_train_Lemm,y_train_Lemm)\n\nval_score_Lemm = cross_val_score(\n    model, \n    x_test_Lemm, \n    y_test_Lemm, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(2,2): {val_score_Lemm}')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:34.04147Z","iopub.execute_input":"2021-06-22T12:19:34.042087Z","iopub.status.idle":"2021-06-22T12:19:36.402054Z","shell.execute_reply.started":"2021-06-22T12:19:34.042048Z","shell.execute_reply":"2021-06-22T12:19:36.400232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trigram","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(3,3)),\n    LinearRegression(),\n)\n\nmodel.fit(x_train,y_train)\n\nval_score = cross_val_score(\n    model, \n    x_test, \n    y_test, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(3,3): {val_score}')\n\nmodel_Stem = make_pipeline(\n    CountVectorizer(ngram_range=(3,3)),\n    LinearRegression(),\n)\n\nmodel_Stem.fit(x_train_Stem,y_train_Stem)\n\nval_score_Stem = cross_val_score(\n    model, \n    x_test_Stem, \n    y_test_Stem, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(3,3): {val_score_Stem}')\n\nmodel_Lemm = make_pipeline(\n    CountVectorizer(ngram_range=(3,3)),\n    LinearRegression(),\n)\n\nmodel_Lemm.fit(x_train_Lemm,y_train_Lemm)\n\nval_score_Lemm = cross_val_score(\n    model, \n    x_test_Lemm, \n    y_test_Lemm, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(3,3): {val_score_Lemm}')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:36.402931Z","iopub.status.idle":"2021-06-22T12:19:36.403298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Uni + Bi-Gram","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,2)),\n    LinearRegression(),\n)\n\nmodel.fit(x_train,y_train)\n\nval_score = cross_val_score(\n    model, \n    x_test, \n    y_test, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,2): {val_score}')\n\nmodel_Stem = make_pipeline(\n    CountVectorizer(ngram_range=(1,2)),\n    LinearRegression(),\n)\n\nmodel_Stem.fit(x_train_Stem,y_train_Stem)\n\nval_score_Stem = cross_val_score(\n    model, \n    x_test_Stem, \n    y_test_Stem, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,2): {val_score_Stem}')\n\nmodel_Lemm = make_pipeline(\n    CountVectorizer(ngram_range=(1,2)),\n    LinearRegression(),\n)\n\nmodel_Lemm.fit(x_train_Lemm,y_train_Lemm)\n\nval_score_Lemm = cross_val_score(\n    model, \n    x_test_Lemm, \n    y_test_Lemm, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,2): {val_score_Lemm}')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:36.404275Z","iopub.status.idle":"2021-06-22T12:19:36.404848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1+2+3-Gram","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,3)),\n    LinearRegression(),\n)\n\nmodel.fit(x_train,y_train)\n\nval_score = cross_val_score(\n    model, \n    x_test, \n    y_test, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,3): {val_score}')\n\nmodel_Stem = make_pipeline(\n    CountVectorizer(ngram_range=(1,3)),\n    LinearRegression(),\n)\n\nmodel_Stem.fit(x_train_Stem,y_train_Stem)\n\nval_score_Stem = cross_val_score(\n    model, \n    x_test_Stem, \n    y_test_Stem, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,3): {val_score_Stem}')\n\nmodel_Lemm = make_pipeline(\n    CountVectorizer(ngram_range=(1,3)),\n    LinearRegression(),\n)\n\nmodel_Lemm.fit(x_train_Lemm,y_train_Lemm)\n\nval_score_Lemm = cross_val_score(\n    model, \n    x_test_Lemm, \n    y_test_Lemm, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,3): {val_score_Lemm}')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:36.406001Z","iopub.status.idle":"2021-06-22T12:19:36.406549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bi-Gram + Tri-Gram","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(2,3)),\n    LinearRegression(),\n)\n\nmodel.fit(x_train,y_train)\n\nval_score = cross_val_score(\n    model, \n    x_test, \n    y_test, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(2,3): {val_score}')\n\nmodel_Stem = make_pipeline(\n    CountVectorizer(ngram_range=(2,3)),\n    LinearRegression(),\n)\n\nmodel_Stem.fit(x_train_Stem,y_train_Stem)\n\nval_score_Stem = cross_val_score(\n    model, \n    x_test_Stem, \n    y_test_Stem, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(2,3): {val_score_Stem}')\n\nmodel_Lemm = make_pipeline(\n    CountVectorizer(ngram_range=(2,3)),\n    LinearRegression(),\n)\n\nmodel_Lemm.fit(x_train_Lemm,y_train_Lemm)\n\nval_score_Lemm = cross_val_score(\n    model, \n    x_test_Lemm, \n    y_test_Lemm, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(2,3): {val_score_Lemm}')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:36.407706Z","iopub.status.idle":"2021-06-22T12:19:36.408262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,1)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train,y_train)\n\nval_score = cross_val_score(\n    model, \n    x_test, \n    y_test, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,1): {val_score}')\n\nmodel = make_pipeline(\n    CountVectorizer(ngram_range=(1,1)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train_Stem,y_train_Stem)\n\nval_score = cross_val_score(\n    model, \n    x_test_Stem, \n    y_test_Stem, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,1): {val_score}')\n\nmodel = make_pipeline(\n    CountVectorizer(ngram_range=(1,1)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train_Lemm,y_train_Lemm)\n\nval_score = cross_val_score(\n    model, \n    x_test_Lemm, \n    y_test_Lemm, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,1): {val_score}')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:36.409457Z","iopub.status.idle":"2021-06-22T12:19:36.410042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,2)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train,y_train)\n\nval_score = cross_val_score(\n    model, \n    x_test, \n    y_test, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,2): {val_score}')\n\nmodel = make_pipeline(\n    CountVectorizer(ngram_range=(1,2)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train_Stem,y_train_Stem)\n\nval_score = cross_val_score(\n    model, \n    x_test_Stem, \n    y_test_Stem, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,2): {val_score}')\n\nmodel = make_pipeline(\n    CountVectorizer(ngram_range=(1,2)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train_Lemm,y_train_Lemm)\n\nval_score = cross_val_score(\n    model, \n    x_test_Lemm, \n    y_test_Lemm, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,2): {val_score}')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:36.411129Z","iopub.status.idle":"2021-06-22T12:19:36.411683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(1,3)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train,y_train)\n\nval_score = cross_val_score(\n    model, \n    x_test, \n    y_test, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,3): {val_score}')\n\nmodel = make_pipeline(\n    CountVectorizer(ngram_range=(1,3)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train_Stem,y_train_Stem)\n\nval_score = cross_val_score(\n    model, \n    x_test_Stem, \n    y_test_Stem, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,3): {val_score}')\n\nmodel = make_pipeline(\n    CountVectorizer(ngram_range=(1,3)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train_Lemm,y_train_Lemm)\n\nval_score = cross_val_score(\n    model, \n    x_test_Lemm, \n    y_test_Lemm, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(1,3): {val_score}')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:36.412792Z","iopub.status.idle":"2021-06-22T12:19:36.413343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = make_pipeline(\n    CountVectorizer(ngram_range=(2,3)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train,y_train)\n\nval_score = cross_val_score(\n    model, \n    x_test, \n    y_test, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(2,3): {val_score}')\n\nmodel = make_pipeline(\n    CountVectorizer(ngram_range=(2,3)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train_Stem,y_train_Stem)\n\nval_score = cross_val_score(\n    model, \n    x_test_Stem, \n    y_test_Stem, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(2,3): {val_score}')\n\nmodel = make_pipeline(\n    CountVectorizer(ngram_range=(2,3)),\n    xgb.XGBRegressor() ,\n)\n\nmodel.fit(x_train_Lemm,y_train_Lemm)\n\nval_score = cross_val_score(\n    model, \n    x_test_Lemm, \n    y_test_Lemm, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Validation Score for CountVectorizer(2,3): {val_score}')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:19:36.414557Z","iopub.status.idle":"2021-06-22T12:19:36.415188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_BOW(X_train, y_train, X_test, y_test, model_name, m,n):\n    t1 = time.time()\n    \n    model = make_pipeline(CountVectorizer(ngram_range=(m,n)),model_name)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    MSE = mse(y_test, y_pred)\n    rmse =np.sqrt(MSE)\n    t2 = time.time()\n    training_time = t2-t1 \n    \n    print(\"--- Model:\", model_name,\"---\")\n    print(\"MSE: \",MSE)\n    print(\"RMSE: \",rmse)\n    print(\"Training time:\",training_time)\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:29:49.654251Z","iopub.execute_input":"2021-06-22T12:29:49.654575Z","iopub.status.idle":"2021-06-22T12:29:49.661931Z","shell.execute_reply.started":"2021-06-22T12:29:49.654546Z","shell.execute_reply":"2021-06-22T12:29:49.660915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_BOW(x_train_Lemm,y_train_Lemm,x_test_Lemm,y_test_Lemm,xgb.XGBRegressor(),1,2)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:30:48.751305Z","iopub.execute_input":"2021-06-22T12:30:48.751658Z","iopub.status.idle":"2021-06-22T12:31:00.64152Z","shell.execute_reply.started":"2021-06-22T12:30:48.751612Z","shell.execute_reply":"2021-06-22T12:31:00.639993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_BOW(x_train_Lemm,y_train_Lemm,x_test_Lemm,y_test_Lemm,xgb.XGBRegressor(),1,1)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:31:00.642857Z","iopub.execute_input":"2021-06-22T12:31:00.643141Z","iopub.status.idle":"2021-06-22T12:31:03.189631Z","shell.execute_reply.started":"2021-06-22T12:31:00.643115Z","shell.execute_reply":"2021-06-22T12:31:03.188943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_BOW(x_train_Lemm,y_train_Lemm,x_test_Lemm,y_test_Lemm,xgb.XGBRegressor(),1,3)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:29:50.277034Z","iopub.execute_input":"2021-06-22T12:29:50.277359Z","iopub.status.idle":"2021-06-22T12:30:16.970948Z","shell.execute_reply.started":"2021-06-22T12:29:50.27733Z","shell.execute_reply":"2021-06-22T12:30:16.970058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF\n\nTF-IDF (stands for Term-Frequency-Inverse-Document Frequency) weights down the common words occuring in almost all the documents and give more importance to the words that appear in a subset of documents. TF-IDF works by penalising these common words by assigning them lower weights while giving importance to some rare words in a particular document.","metadata":{}},{"cell_type":"code","source":"def training_TFIDF(X_train, y_train, X_test, y_test, model_name,ngram_range):\n    t1 = time.time()\n    \n    model = make_pipeline(TfidfVectorizer(binary=True, ngram_range=ngram_range),model_name)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    MSE = mse(y_test, y_pred)\n    rmse =np.sqrt(MSE)\n    t2 = time.time()\n    training_time = t2-t1 \n    \n    print(\"--- Model:\", model_name,\"---\")\n    print(\"MSE: \",MSE)\n    print(\"RMSE: \",rmse)\n    print(\"Training time:\",training_time)\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:32:36.475715Z","iopub.execute_input":"2021-06-22T12:32:36.47604Z","iopub.status.idle":"2021-06-22T12:32:36.482396Z","shell.execute_reply.started":"2021-06-22T12:32:36.476012Z","shell.execute_reply":"2021-06-22T12:32:36.481198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_gram_dict = { \"Unigram\" : (1,1), \"Unigrams + Bigrams\": (1,2), \"Bigrams alone\": (2,2), \"Unigrams + Bigrams + Trigrams\": (1,3)}","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:33:27.61385Z","iopub.execute_input":"2021-06-22T12:33:27.614175Z","iopub.status.idle":"2021-06-22T12:33:27.618676Z","shell.execute_reply.started":"2021-06-22T12:33:27.614144Z","shell.execute_reply":"2021-06-22T12:33:27.61748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_TFIDF(x_train_Lemm,y_train_Lemm,x_test_Lemm,y_test_Lemm,xgb.XGBRegressor(),n_gram_dict['Unigram'])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:35:05.024304Z","iopub.execute_input":"2021-06-22T12:35:05.024623Z","iopub.status.idle":"2021-06-22T12:35:11.058254Z","shell.execute_reply.started":"2021-06-22T12:35:05.024593Z","shell.execute_reply":"2021-06-22T12:35:11.057397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_TFIDF(x_train_Lemm,y_train_Lemm,x_test_Lemm,y_test_Lemm,LinearRegression(),n_gram_dict['Unigram'])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:38:04.35007Z","iopub.execute_input":"2021-06-22T12:38:04.350388Z","iopub.status.idle":"2021-06-22T12:38:04.798083Z","shell.execute_reply.started":"2021-06-22T12:38:04.350353Z","shell.execute_reply":"2021-06-22T12:38:04.796567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_TFIDF(x_train_Lemm,y_train_Lemm,x_test_Lemm,y_test_Lemm,xgb.XGBRegressor(),n_gram_dict['Unigrams + Bigrams'])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:35:11.061355Z","iopub.execute_input":"2021-06-22T12:35:11.061627Z","iopub.status.idle":"2021-06-22T12:35:27.800876Z","shell.execute_reply.started":"2021-06-22T12:35:11.0616Z","shell.execute_reply":"2021-06-22T12:35:27.799973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_TFIDF(x_train_Lemm,y_train_Lemm,x_test_Lemm,y_test_Lemm,LinearRegression(),n_gram_dict['Unigrams + Bigrams'])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:38:17.25478Z","iopub.execute_input":"2021-06-22T12:38:17.255096Z","iopub.status.idle":"2021-06-22T12:38:18.302436Z","shell.execute_reply.started":"2021-06-22T12:38:17.255067Z","shell.execute_reply":"2021-06-22T12:38:18.301556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_TFIDF(x_train_Lemm,y_train_Lemm,x_test_Lemm,y_test_Lemm,xgb.XGBRegressor(),n_gram_dict['Bigrams alone'])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:35:27.80243Z","iopub.execute_input":"2021-06-22T12:35:27.802918Z","iopub.status.idle":"2021-06-22T12:35:37.743738Z","shell.execute_reply.started":"2021-06-22T12:35:27.802875Z","shell.execute_reply":"2021-06-22T12:35:37.742833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_TFIDF(x_train_Lemm,y_train_Lemm,x_test_Lemm,y_test_Lemm,LinearRegression(),n_gram_dict['Bigrams alone'])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:38:28.016191Z","iopub.execute_input":"2021-06-22T12:38:28.016503Z","iopub.status.idle":"2021-06-22T12:38:28.829294Z","shell.execute_reply.started":"2021-06-22T12:38:28.016474Z","shell.execute_reply":"2021-06-22T12:38:28.8285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_TFIDF(x_train_Lemm,y_train_Lemm,x_test_Lemm,y_test_Lemm,xgb.XGBRegressor(),n_gram_dict['Unigrams + Bigrams + Trigrams'])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:35:52.122196Z","iopub.execute_input":"2021-06-22T12:35:52.122512Z","iopub.status.idle":"2021-06-22T12:36:23.470437Z","shell.execute_reply.started":"2021-06-22T12:35:52.122483Z","shell.execute_reply":"2021-06-22T12:36:23.469582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_TFIDF(x_train_Lemm,y_train_Lemm,x_test_Lemm,y_test_Lemm,LinearRegression(),n_gram_dict['Unigrams + Bigrams + Trigrams'])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:38:35.349389Z","iopub.execute_input":"2021-06-22T12:38:35.349723Z","iopub.status.idle":"2021-06-22T12:38:37.788219Z","shell.execute_reply.started":"2021-06-22T12:38:35.349693Z","shell.execute_reply":"2021-06-22T12:38:37.787382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Deep Learning\n#### Keras","metadata":{}},{"cell_type":"code","source":"x_train_Lemm.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:06:07.326494Z","iopub.execute_input":"2021-06-22T13:06:07.326824Z","iopub.status.idle":"2021-06-22T13:06:07.331847Z","shell.execute_reply.started":"2021-06-22T13:06:07.326795Z","shell.execute_reply":"2021-06-22T13:06:07.331026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\n\n#input_dim = x_train_Lemm.shape[1]  # Number of features\n\nmodel = Sequential()\nmodel.add(layers.Dense(10, input_dim=1, activation='relu'))\nmodel.add(layers.Dense(1))\nmodel.compile(loss='mean_squared_error', metrics=[RootMeanSquaredError()])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:06:37.181294Z","iopub.execute_input":"2021-06-22T13:06:37.181666Z","iopub.status.idle":"2021-06-22T13:06:39.420614Z","shell.execute_reply.started":"2021-06-22T13:06:37.181614Z","shell.execute_reply":"2021-06-22T13:06:39.419754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=26118)\ntokenizer.fit_on_texts(df_train['excerpt_clean_Lemm'].to_list())\n\nX_train = tokenizer.texts_to_sequences(df_train['excerpt_clean_Lemm'].to_list())\nX_test = tokenizer.texts_to_sequences(df_test['excerpt_clean_Lemm'].to_list())\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nprint(df_train['excerpt_clean_Lemm'][2])\nprint(X_train[2])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:41:25.707099Z","iopub.execute_input":"2021-06-22T13:41:25.707414Z","iopub.status.idle":"2021-06-22T13:41:26.052048Z","shell.execute_reply.started":"2021-06-22T13:41:25.707384Z","shell.execute_reply":"2021-06-22T13:41:26.051116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:20:42.980473Z","iopub.execute_input":"2021-06-22T13:20:42.980837Z","iopub.status.idle":"2021-06-22T13:20:42.985538Z","shell.execute_reply.started":"2021-06-22T13:20:42.980795Z","shell.execute_reply":"2021-06-22T13:20:42.984691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 800\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n\nprint(X_train[0, :])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:12:59.743136Z","iopub.execute_input":"2021-06-22T13:12:59.743474Z","iopub.status.idle":"2021-06-22T13:12:59.809726Z","shell.execute_reply.started":"2021-06-22T13:12:59.743442Z","shell.execute_reply":"2021-06-22T13:12:59.808779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=maxlen))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dropout(0.5))\n#model.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='linear'))\nmodel.compile(loss='mean_squared_error', metrics=[RootMeanSquaredError()])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:24:36.13681Z","iopub.execute_input":"2021-06-22T13:24:36.137137Z","iopub.status.idle":"2021-06-22T13:24:36.198061Z","shell.execute_reply.started":"2021-06-22T13:24:36.137107Z","shell.execute_reply":"2021-06-22T13:24:36.196723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_labels_final = np.array(df_train.target)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:24:37.906121Z","iopub.execute_input":"2021-06-22T13:24:37.906476Z","iopub.status.idle":"2021-06-22T13:24:37.910328Z","shell.execute_reply.started":"2021-06-22T13:24:37.906447Z","shell.execute_reply":"2021-06-22T13:24:37.909384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, training_labels_final,\n                    epochs=20,\n                    validation_split=0.1,\n                    batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:24:39.940393Z","iopub.execute_input":"2021-06-22T13:24:39.940726Z","iopub.status.idle":"2021-06-22T13:24:52.229793Z","shell.execute_reply.started":"2021-06-22T13:24:39.940693Z","shell.execute_reply":"2021-06-22T13:24:52.22891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Glove Embeddings\n\nWhat if, instead of learning the embeddings for yourself, you could instead use prelearned embeddings, where researchers have already done the hard work of turning words into vectors and those vectors are proven? One example of this is the GloVe (Global Vectors for WordRepresentation) model developed by Jeffrey Pennington, Richard Socher, and Christopher Manning at Stanford\n\nPretrained Word Embeddings are the embeddings learned in one task that are used for solving another similar task. These embeddings are trained on large datasets, saved, and then used for solving other tasks. That’s why pretrained word embeddings are a form of Transfer Learning.","metadata":{}},{"cell_type":"code","source":"#text = df_train.excerpt\ntext = df_train.excerpt_clean_Lemm","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:46:50.970679Z","iopub.execute_input":"2021-06-22T13:46:50.97105Z","iopub.status.idle":"2021-06-22T13:46:50.974561Z","shell.execute_reply.started":"2021-06-22T13:46:50.971019Z","shell.execute_reply":"2021-06-22T13:46:50.973729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = 26118\nembedding_dim = 512\nmax_length = 500\ntrunc_type='post'\npad_type='post'\noov_tok = \"<OOV>\"","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:48:26.207408Z","iopub.execute_input":"2021-06-22T13:48:26.207751Z","iopub.status.idle":"2021-06-22T13:48:26.212277Z","shell.execute_reply.started":"2021-06-22T13:48:26.207719Z","shell.execute_reply":"2021-06-22T13:48:26.21105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(text)\nword_index = tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:48:36.147896Z","iopub.execute_input":"2021-06-22T13:48:36.148212Z","iopub.status.idle":"2021-06-22T13:48:36.350705Z","shell.execute_reply.started":"2021-06-22T13:48:36.148183Z","shell.execute_reply":"2021-06-22T13:48:36.349853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_Seq = tokenizer.texts_to_sequences(text)\nX_train = pad_sequences(X_train_Seq,maxlen=max_length, \n                                truncating=trunc_type, padding=pad_type)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:49:26.188295Z","iopub.execute_input":"2021-06-22T13:49:26.188682Z","iopub.status.idle":"2021-06-22T13:49:26.383388Z","shell.execute_reply.started":"2021-06-22T13:49:26.188624Z","shell.execute_reply":"2021-06-22T13:49:26.382489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove_embeddings = dict()\nf = open('../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    glove_embeddings[word] = coefs\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:49:33.427837Z","iopub.execute_input":"2021-06-22T13:49:33.428187Z","iopub.status.idle":"2021-06-22T13:49:41.388045Z","shell.execute_reply.started":"2021-06-22T13:49:33.428159Z","shell.execute_reply":"2021-06-22T13:49:41.387175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glove_embeddings['study']","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:49:41.389462Z","iopub.execute_input":"2021-06-22T13:49:41.389812Z","iopub.status.idle":"2021-06-22T13:49:41.397002Z","shell.execute_reply.started":"2021-06-22T13:49:41.389776Z","shell.execute_reply":"2021-06-22T13:49:41.396071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(glove_embeddings[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(50)\n    return v / np.sqrt((v ** 2).sum())","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:49:41.625006Z","iopub.execute_input":"2021-06-22T13:49:41.62532Z","iopub.status.idle":"2021-06-22T13:49:41.632844Z","shell.execute_reply.started":"2021-06-22T13:49:41.625291Z","shell.execute_reply":"2021-06-22T13:49:41.631691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 50\nvocab_size = 26118","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:50:31.713757Z","iopub.execute_input":"2021-06-22T13:50:31.714093Z","iopub.status.idle":"2021-06-22T13:50:31.718404Z","shell.execute_reply.started":"2021-06-22T13:50:31.714066Z","shell.execute_reply":"2021-06-22T13:50:31.717534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n    if index > vocab_size - 1:\n         break\n    else:\n        embedding_vector = glove_embeddings.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:50:33.315242Z","iopub.execute_input":"2021-06-22T13:50:33.315565Z","iopub.status.idle":"2021-06-22T13:50:33.357557Z","shell.execute_reply.started":"2021-06-22T13:50:33.315535Z","shell.execute_reply":"2021-06-22T13:50:33.356729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.Sequential([\n tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False),\n tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim,\n return_sequences=True)),\n tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n tf.keras.layers.Dense(24, activation='relu'),\n tf.keras.layers.Dense(1)\n])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:50:37.719264Z","iopub.execute_input":"2021-06-22T13:50:37.71958Z","iopub.status.idle":"2021-06-22T13:50:38.531924Z","shell.execute_reply.started":"2021-06-22T13:50:37.71955Z","shell.execute_reply":"2021-06-22T13:50:38.531102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.00003\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:50:42.167017Z","iopub.execute_input":"2021-06-22T13:50:42.167329Z","iopub.status.idle":"2021-06-22T13:50:42.190417Z","shell.execute_reply.started":"2021-06-22T13:50:42.167301Z","shell.execute_reply":"2021-06-22T13:50:42.189663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_root_mean_squared_error', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\n\nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=5, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:58:26.237683Z","iopub.execute_input":"2021-06-22T13:58:26.238054Z","iopub.status.idle":"2021-06-22T13:58:26.24318Z","shell.execute_reply.started":"2021-06-22T13:58:26.238022Z","shell.execute_reply":"2021-06-22T13:58:26.241903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 100\nhistory = model.fit(X_train, training_labels_final, epochs=num_epochs, \n                    validation_split=0.3,\n                    batch_size=32,callbacks=[early_stopping,learning_rate_reduction])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T13:58:27.93059Z","iopub.execute_input":"2021-06-22T13:58:27.930953Z","iopub.status.idle":"2021-06-22T13:59:23.05908Z","shell.execute_reply.started":"2021-06-22T13:58:27.930922Z","shell.execute_reply":"2021-06-22T13:59:23.058222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padding_type='post'\nsample_sequences = tokenizer.texts_to_sequences(df_test['excerpt'])\nexcerpt_padded = pad_sequences(sample_sequences, padding=padding_type, \n                             maxlen=max_length) \nvalues = model.predict(excerpt_padded)\nsubmission_file= \"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\"\nsample_submission = pd.read_csv(submission_file)\nsample_submission[\"target\"] = values\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T14:00:49.557801Z","iopub.execute_input":"2021-06-22T14:00:49.558144Z","iopub.status.idle":"2021-06-22T14:00:49.647745Z","shell.execute_reply.started":"2021-06-22T14:00:49.558115Z","shell.execute_reply":"2021-06-22T14:00:49.646876Z"},"trusted":true},"execution_count":null,"outputs":[]}]}