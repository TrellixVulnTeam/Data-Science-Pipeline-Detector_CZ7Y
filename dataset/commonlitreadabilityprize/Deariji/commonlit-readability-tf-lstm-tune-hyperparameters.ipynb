{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Introduction\n\n**Project Goal:** Using machine learning to identify the appropriate reading level of a passage of text for grades 3-12 students.\n\n**Data:** \n- train.csv size: 2834 rows, 6 columns:\n    - id\n    - url_legal\n    - license\n    - excerpt (feature)\n    - target (** dependent variable)\n    - standard_error\n- test.csv size: 7 rows, 4 columns\n\n#### In this project, I did not use any pre-trained models. Therefore, there is no need to turn on the Internet toggle in the kernel in order to download anything.","metadata":{}},{"cell_type":"markdown","source":"# Loading Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# import spacy\nimport string\nfrom collections import Counter\n\nimport os\nimport shutil\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow import keras\nimport kerastuner as kt\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Datasets into Memory","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ndf_topredict = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ndf_sample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n- Add some columns\n    - text length\n    - length of the longest word in the text\n    - length of the longest sentence in the text","metadata":{}},{"cell_type":"markdown","source":"## Add a text_length column","metadata":{}},{"cell_type":"code","source":"# df.apply(lambda row: row.name, axis=1)\ndf['text_length'] = df.apply(lambda row: len(df.loc[row.name, 'excerpt'].split()), axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add a longest word in the text as a column","metadata":{}},{"cell_type":"code","source":"def maxword_len(row_idx):\n    words = df.loc[row_idx, 'excerpt'].split()\n    max_len = len(max(words, key=len))\n    return max_len","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['maxword_length'] = df.index.map(lambda row_idx: maxword_len(row_idx))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Add length of the longest sentence in the text as a column","metadata":{}},{"cell_type":"code","source":"def maxsent_len(row_idx):\n    paragraph = df.loc[row_idx, 'excerpt']\n    num_words = [len(sentence.split()) for sentence in paragraph.split('.')]\n    return max(num_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['maxsent_length'] = df.index.map(lambda row_idx: maxsent_len(row_idx))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalize the 3 created columns using min/max normalization\nformula:  (df-df.min())/(df.max()-df.min())","metadata":{}},{"cell_type":"code","source":"df['ntext_length'] = (df.text_length - df.text_length.min()) / (df.text_length.max()-df.text_length.min())\ndf['nmaxword_length'] = (df.maxword_length - df.maxword_length.min()) / (df.maxword_length.max()-df.maxword_length.min())\ndf['nmaxsent_length'] = (df.maxsent_length - df.maxsent_length.min()) / (df.maxsent_length.max()-df.maxsent_length.min())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Take a look at the engineered df:","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA \n\n## Check the target and standard_error distribution:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\n\nplt.subplot(1,2,1)\nplt.hist(x=df.target, color='tab:cyan', bins=40, edgecolor='k')\nplt.xlabel('Text Difficulty Score(target)')\nplt.ylabel('Count')\nplt.title('Distribution of Target Score')\n\nplt.subplot(1,2,2)\nplt.hist(x=df.standard_error, color='tab:purple',bins=40, edgecolor='k')\nplt.xlabel('Standard Error')\nplt.ylabel('Count')\nplt.title('Distribution of Error')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check the text with lowest and highest target score:","metadata":{}},{"cell_type":"code","source":"# set the display to show more text\npd.options.display.max_colwidth = 100\n\n# print out the text to exam the difference between high score and low score\nmin_target = df.loc[df.target==df.target.min(),['excerpt','target','text_length','maxword_length','maxsent_length']]\nprint('Min Target Score:',min_target.target, '-'*20, 'TEXT', '-'*20)\nprint(f'\"{min_target.excerpt}\"')\nprint(f'Text Length: {min_target.text_length}')\nprint(f'Longest Word Length: {min_target.maxword_length}')\nprint(f'Longest Sentence Length: {min_target.maxsent_length}')\n\nprint()\n\nmax_target = df.loc[df.target==df.target.max(),['excerpt','target','text_length','maxword_length','maxsent_length']]\nprint('Max Target Score:',max_target.target, '-'*20, 'TEXT', '-'*20)\nprint(f'\"{max_target.excerpt}\"')\nprint(f'Text Length: {max_target.text_length}')\nprint(f'Longest Word Length: {max_target.maxword_length}')\nprint(f'Longest Sentence Length: {max_target.maxsent_length}')\n\n# shorten the text display\npd.options.display.max_colwidth = 50\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the relationship between target and \n- text_length\n- maxword_length\n- maxsent_length","metadata":{}},{"cell_type":"code","source":"corr_list = ['text_length','maxword_length','maxsent_length']\n\nsns.set_theme(style=\"white\", color_codes=True)\n\nplt.figure(figsize=(15,5))\nfor i in range(len(corr_list)):\n    plt.subplot(1,3,i+1)\n    sns.regplot(x=df[corr_list[i]] ,y=df.target, marker='+')\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN Model\n## Preprocessing: convert the dataframe into tf dataset","metadata":{}},{"cell_type":"code","source":"SEED = 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices(\n            (tf.cast(df['excerpt'].values, tf.string),\n             tf.cast(df['target'].values, tf.float16)))\ndataset.shuffle(SEED)\n\nprint(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's print out an instance in the dataset\nfor example, label in dataset.take(1):\n    print('Text: ', example.numpy(), sep='\\n')\n    print()\n    print('Label: ', label.numpy(),sep='\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train test split","metadata":{}},{"cell_type":"code","source":"TRAIN_SIZE = int(len(dataset)*0.7)\n\ntrain_dataset = dataset.take(TRAIN_SIZE)\ntest_dataset = dataset.skip(TRAIN_SIZE) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tuning the train, test dataset to feed into tensorflow","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 4\n\ntrain_dataset = train_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create tokenize(encoder) and vectorize(embedding) layers","metadata":{}},{"cell_type":"code","source":"total_words = df['excerpt'].str.split()\ntotal_word_set = set()\ntotal_words.apply(total_word_set.update)\ncount_dict = Counter(total_word_set)\nVOCAB_SIZE = len(count_dict)\n\nprint('total unique words number:', VOCAB_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n    max_tokens=VOCAB_SIZE)\nencoder.adapt(train_dataset.map(lambda text, label: text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_layer = Embedding(\n    input_dim=len(encoder.get_vocabulary()),\n    output_dim=128,\n    mask_zero=True\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tune the LSTM Model\n### Define the model","metadata":{}},{"cell_type":"code","source":"def model_builder(hp):\n    model = Sequential()\n    model.add(encoder)\n    model.add(embedding_layer)\n    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)))\n    \n    hp_units = hp.Int('units', min_value=96, max_value=512, step=32)\n    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    \n    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    \n    model.add(tf.keras.layers.Dense(1))\n    \n    hp_learning_rate = hp.Choice('learning_rate', values = [1e-4, 1e-5])\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                  loss=tf.keras.losses.MeanSquaredError())\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Instantiate the tuner and perform hypertuning","metadata":{}},{"cell_type":"code","source":"tuner = kt.Hyperband(model_builder,\n                     objective='val_loss',\n                     max_epochs=10,\n                     factor=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\ntuner.search(train_dataset, validation_data=test_dataset,\n             epochs=30, callbacks=[stop_early])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete... \nThe optimal number of neurons in the dense layers is {best_hps.get('units')};\nThe optimal learning rate is {best_hps.get('learning_rate')}.\n\"\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the model with the optimal hyperparameters and train it on the data for 50 epochs","metadata":{}},{"cell_type":"code","source":"model = tuner.hypermodel.build(best_hps)\nhistory = model.fit(train_dataset, epochs=50,\n                    validation_data=test_dataset, \n                    callbacks = [stop_early])\n\nval_loss_per_epoch = history.history['val_loss']\nbest_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\nprint(f'Best epoch: {best_epoch}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Re-instantiate the hypermodel and train it with the optimal number of epochs from above.","metadata":{}},{"cell_type":"code","source":"hypermodel = tuner.hypermodel.build(best_hps)\n\n# Retrain the model\nhypermodel.fit(train_dataset, validation_data=test_dataset,\n             epochs=best_epoch, callbacks=[stop_early])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Re-train the model with the entire dataset","metadata":{}},{"cell_type":"code","source":"dataset = dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\nhypermodel.fit(dataset, epochs=best_epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Prediction","metadata":{}},{"cell_type":"code","source":"def make_prediction(row_idx):\n    result = hypermodel.predict(np.array([df_topredict.excerpt[row_idx]]))\n    return result[0][0]\n\ndf_topredict['target'] = df_topredict.index.map(lambda row_idx: make_prediction(row_idx))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub = df_topredict.loc[:, ['id','target']]\ndf_sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}