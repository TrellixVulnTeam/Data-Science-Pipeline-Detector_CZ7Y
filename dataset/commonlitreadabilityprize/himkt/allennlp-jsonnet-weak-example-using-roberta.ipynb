{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## What is this\n\nThis notebook shows an example of [AllenNLP](https://github.com/allenai/allennlp) Jsonnet API for training/inference.\n\nI publish this example because I couldn't find an example using AllenNLP with Jsonnet.\n(Examples using AllenNLP I found didn't use Jsonnet config file)\n\n\nIn notebook only competition, it is relatively hard to use AllenNLP with Jsonnet config file,\nbecause training is launched by running a command `allennlp train` on the shell and it loads\nmodules in a repository. (it means that we have to write scripts outside the notebook).\n\nIn this example, I invoke `allennlp.commands.train` on the notebook to mitigate this limitation.\nEach modules are defined in the notebook.\n\n\nUnfortunately, the submission using this notebook failed with the error `Notebook Exceeded Allowed Compute`.\nAlthough I was trying to figure out the cause of memory/disk problem, I couldn't solve it.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T13:15:35.192289Z","iopub.execute_input":"2021-07-27T13:15:35.192606Z","iopub.status.idle":"2021-07-27T13:15:35.305402Z","shell.execute_reply.started":"2021-07-27T13:15:35.192572Z","shell.execute_reply":"2021-07-27T13:15:35.304257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas\nimport numpy\n\n\ndf = pandas.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\nnum_records = len(df)\n\nids = numpy.arange(num_records)\nids = numpy.random.permutation(ids)\n\ntrain_size = 0.8\npartition = int(num_records * train_size)\n\ntrain_ids, valid_ids = ids[:partition], ids[partition:]\n\ndf.loc[train_ids].to_csv(\"./processed_train.csv\", index=False)\ndf.loc[valid_ids].to_csv(\"./processed_valid.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:15:35.307285Z","iopub.execute_input":"2021-07-27T13:15:35.307703Z","iopub.status.idle":"2021-07-27T13:15:35.543184Z","shell.execute_reply.started":"2021-07-27T13:15:35.307654Z","shell.execute_reply":"2021-07-27T13:15:35.542272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modules: Dataset reader, Model, Predictor\n\nModel is simple that consists of two types of features: 1 categorical feature (hostname) and 1 text feature (excerpt).\n\nA hostname is embedded into 50 dimensional and excerpt is fed into RoBERTa (base).\n\nEach representation are concatenated and projected to a scalar.","metadata":{}},{"cell_type":"code","source":"from typing import Any, Dict, Iterable, MutableMapping, Optional\nfrom urllib.parse import urlparse\n\nfrom allennlp.data import DatasetReader\nfrom allennlp.data import Tokenizer\nfrom allennlp.data.fields.field import Field\nfrom allennlp.data.fields import ArrayField, TextField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer\nfrom allennlp.data.tokenizers.token_class import Token\nimport pandas\nimport numpy\nfrom overrides import overrides\n\n\n@DatasetReader.register(\"commonlit_reader\")\nclass CommonlitDatasetReader(DatasetReader):\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        excerpt_token_indexers: Optional[Dict[str, TokenIndexer]] = None,\n        hostname_token_indexers: Optional[Dict[str, TokenIndexer]] = None,\n    ) -> None:\n\n        super().__init__()\n\n        self.tokenizer = tokenizer\n        self.excerpt_token_indexers: Dict[str, TokenIndexer] = excerpt_token_indexers or {\n            \"tokens\": SingleIdTokenIndexer(),\n        }\n        self.hostname_token_indexers: Dict[str, TokenIndexer] = hostname_token_indexers or {\n            \"tokens\": SingleIdTokenIndexer(),\n        }\n\n    def _read(self, file_path: str) -> Iterable[Instance]:\n        instances = []\n\n        dataframe = pandas.read_csv(file_path)\n        dataframe[\"hostname\"] = dataframe \\\n            .url_legal \\\n            .apply(lambda url: urlparse(url).hostname if isinstance(url, str) else \"EMPTY_HOSTNAME\")\n\n        for _, row in dataframe.iterrows():\n            excerpt = row.excerpt\n            hostname = row.hostname\n            target = row.target if hasattr(row, \"target\") else None\n            instances.append(self.text_to_instance(excerpt, hostname, target))\n\n        return instances\n\n    @overrides\n    def text_to_instance(self, excerpt: str, hostname: str, target: Optional[float] = None) -> Instance:\n        excerpt_tokens = self.tokenizer.tokenize(excerpt)\n        hostname_tokens = [Token(text=hostname)]\n        fields: MutableMapping[str, Field[Any]] = {\n            \"excerpt\": TextField(excerpt_tokens),\n            \"hostname\": TextField(hostname_tokens),\n        }\n        if target is not None:\n            fields[\"target\"] = ArrayField(numpy.asarray(target, dtype=numpy.float32))\n        return Instance(fields=fields)\n\n    def apply_token_indexers(self, instance: Instance) -> None:\n        assert isinstance(instance.fields[\"excerpt\"], TextField)\n        instance.fields[\"excerpt\"].token_indexers = self.excerpt_token_indexers\n        assert isinstance(instance.fields[\"hostname\"], TextField)\n        instance.fields[\"hostname\"].token_indexers = self.hostname_token_indexers\n\n\nfrom typing import Dict, Optional\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules import TextFieldEmbedder\nfrom allennlp.modules import Seq2VecEncoder\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.data.fields.text_field import TextFieldTensors\nfrom overrides.overrides import overrides\nfrom torch import FloatTensor\nfrom torch.functional import Tensor\nfrom torch.nn.functional import mse_loss\nfrom torch import cat\nfrom torch import sqrt\nfrom torch.nn import Linear\n\n\nEPS = 1e-8\n\n\n@Model.register(\"naive\")\nclass NaiveRegressor(Model):\n\n    def __init__(\n        self,\n        vocab: Vocabulary,\n        excerpt_embedder: TextFieldEmbedder,\n        excerpt_encoder: Seq2VecEncoder,\n        hostname_embedder: Optional[TextFieldEmbedder] = None,\n    ) -> None:\n\n        super().__init__(vocab)\n\n        self.vocab = vocab\n        self.excerpt_embedder = excerpt_embedder\n        self.excerpt_encoder = excerpt_encoder\n        self.hostname_embedder = hostname_embedder\n\n        in_features = self.excerpt_encoder.get_output_dim()\n        if hostname_embedder is not None:\n            in_features += hostname_embedder.get_output_dim()\n\n        self.classification_layer = Linear(\n            in_features=in_features,\n            out_features=1,\n        )\n\n    @overrides\n    def forward(\n        self,\n        excerpt: TextFieldTensors,\n        hostname: Optional[TextFieldTensors] = None,\n        target: Optional[FloatTensor] = None,\n    ) -> Dict[str, Tensor]:\n\n        mask = get_text_field_mask(excerpt)\n        excerpt_emb = self.excerpt_embedder(excerpt)\n        hidden_state = self.excerpt_encoder(excerpt_emb, mask=mask)\n\n        if self.hostname_embedder is not None and hostname is not None:\n            hostname_emb = self.hostname_embedder(hostname)\n            hidden_state = cat((hidden_state, hostname_emb.squeeze(dim=1)), dim=1)\n\n        logit = self.classification_layer(hidden_state)\n\n        output_dict = {\"logit\": logit}\n        if target is not None:\n            output_dict[\"loss\"] = sqrt(mse_loss(logit.view(-1), target) + EPS)\n\n        return output_dict\n\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {}\n\n\nfrom allennlp.common.util import JsonDict\nfrom allennlp.data.instance import Instance\nfrom allennlp.predictors import Predictor\n\n\n@Predictor.register(\"regressor_predictor\")\nclass RegressorPredictor(Predictor):\n    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n        return self._dataset_reader.text_to_instance(**json_dict)  # type: ignore\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:15:35.545678Z","iopub.execute_input":"2021-07-27T13:15:35.546052Z","iopub.status.idle":"2021-07-27T13:15:44.563021Z","shell.execute_reply.started":"2021-07-27T13:15:35.546014Z","shell.execute_reply":"2021-07-27T13:15:44.562198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config file\n\nI'm not sure if this is the best way but I create a config file in this notebook.\n\nExecuting the following cell creates a Jsonnet config.","metadata":{}},{"cell_type":"code","source":"jsonnet_text = \"\"\"\\\n{\n    dataset_reader: {\n        type: \"commonlit_reader\",\n        tokenizer: {\n            type: \"pretrained_transformer\",\n            model_name: \"../input/roberta-base\",\n        },\n        excerpt_token_indexers: {\n            tokens: {\n                type: \"pretrained_transformer\",\n                model_name: \"../input/roberta-base\",\n            },\n        },\n    },\n    train_data_path: \"./processed_train.csv\",\n    validation_data_path: \"./processed_valid.csv\",\n    model: {\n        type: \"naive\",\n        excerpt_embedder: {\n            type: \"basic\",\n            token_embedders: {\n                tokens: {\n                    type: \"pretrained_transformer\",\n                    model_name: \"../input/roberta-base\",\n                },\n            },\n        },\n        excerpt_encoder: {\n            type: \"bert_pooler\",\n            pretrained_model: \"../input/roberta-base\",\n        },\n        hostname_embedder: {\n            type: \"basic\",\n            token_embedders: {\n                tokens: {\n                    embedding_dim: 50,\n                },\n            },\n        },\n    },\n    trainer: {\n        num_epochs: 15,\n        learning_rate_scheduler: {\n            type: \"slanted_triangular\",\n            num_epochs: 10,\n            num_steps_per_epoch: 3088,\n            cut_frac: 0.06\n        },\n        optimizer: {\n            type: \"huggingface_adamw\",\n            lr: 5e-7,\n            weight_decay: 0.05,\n        },\n        validation_metric: \"-loss\"\n    },\n    data_loader: {\n        batch_size: 8,\n        shuffle: true\n    }\n}\n\"\"\"\n\nf = open(\"baseline.jsonnet\", \"w\")\nf.write(jsonnet_text)\nf.close()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:15:44.564718Z","iopub.execute_input":"2021-07-27T13:15:44.565047Z","iopub.status.idle":"2021-07-27T13:15:44.572803Z","shell.execute_reply.started":"2021-07-27T13:15:44.565013Z","shell.execute_reply":"2021-07-27T13:15:44.571834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nInstead of running `allennlp train` on a shell,\nwe can directly invoke `allennlp.commands.train.train_model_from_file`.","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:15:03.783348Z","iopub.execute_input":"2021-07-27T13:15:03.783725Z","iopub.status.idle":"2021-07-27T13:15:03.787624Z","shell.execute_reply.started":"2021-07-27T13:15:03.783638Z","shell.execute_reply":"2021-07-27T13:15:03.786807Z"}}},{"cell_type":"code","source":"import allennlp.commands\n\nallennlp.commands.train.train_model_from_file(\n    parameter_filename=\"./baseline.jsonnet\",\n    serialization_dir=\"./serialization/1\",\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:15:44.574221Z","iopub.execute_input":"2021-07-27T13:15:44.57476Z","iopub.status.idle":"2021-07-27T13:38:41.010297Z","shell.execute_reply.started":"2021-07-27T13:15:44.574719Z","shell.execute_reply":"2021-07-27T13:38:41.0093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"from allennlp.models.archival import load_archive\n\narchive = load_archive(\"serialization/1/model.tar.gz\")\npredictor = RegressorPredictor.from_archive(archive)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:38:41.01168Z","iopub.execute_input":"2021-07-27T13:38:41.012024Z","iopub.status.idle":"2021-07-27T13:38:46.919149Z","shell.execute_reply.started":"2021-07-27T13:38:41.011987Z","shell.execute_reply":"2021-07-27T13:38:46.918197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from urllib.parse import urlparse\n\n\ntest_df = pandas.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nprint(test_df.head())\n\ntest_df[\"hostname\"] = test_df \\\n    .url_legal \\\n    .apply(lambda url: urlparse(url).hostname if isinstance(url, str) else \"EMPTY_HOSTNAME\")\n\nbatch_json = test_df.apply(lambda row: {\"excerpt\": row.excerpt, \"hostname\": row.hostname}, axis=1).tolist()\npredictor.predict_batch_json(batch_json)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:38:46.920531Z","iopub.execute_input":"2021-07-27T13:38:46.921109Z","iopub.status.idle":"2021-07-27T13:38:50.186304Z","shell.execute_reply.started":"2021-07-27T13:38:46.921071Z","shell.execute_reply":"2021-07-27T13:38:50.185295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BatchIterator:\n        def __init__(self, data, batch_size):\n                self.data = data\n                self.batch_size = batch_size\n                self.cur = 0\n            \n        def __iter__(self):\n                return self\n            \n        def __next__(self):\n                batch = self.data[self.cur:self.cur+self.batch_size]\n                self.cur += self.batch_size\n                if len(batch) == 0:\n                    raise StopIteration\n                return batch\n\n\npredictions = []\nbatch_iterator = BatchIterator(batch_json, batch_size=1)\n\nfor batch in batch_iterator:\n    predictions += predictor.predict_batch_json(batch)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:38:50.187656Z","iopub.execute_input":"2021-07-27T13:38:50.188031Z","iopub.status.idle":"2021-07-27T13:38:53.211872Z","shell.execute_reply.started":"2021-07-27T13:38:50.187995Z","shell.execute_reply":"2021-07-27T13:38:53.210947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"target\"] = list(map(lambda p: p[\"logit\"][0], predictions))\ntest_df[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-07-27T13:38:53.214323Z","iopub.execute_input":"2021-07-27T13:38:53.214726Z","iopub.status.idle":"2021-07-27T13:38:53.24413Z","shell.execute_reply.started":"2021-07-27T13:38:53.214672Z","shell.execute_reply":"2021-07-27T13:38:53.242956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}