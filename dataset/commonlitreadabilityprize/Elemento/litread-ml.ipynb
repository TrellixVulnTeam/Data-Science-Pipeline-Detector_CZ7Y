{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CommonLit Readability Prize\n- This notebook covers some of the most basic ML models and pre-processing technqiues that a beginner can approach easily\n- Some of the basic pre-processing includes removing stop-words, converting all the text to lower-case, removing links and converting short representations like won't, couldn't, etc.\n- For converting text to vectors, the notebook includes Bag of words, Binary Bag of words, TF-IDF, Average Word2Vec and TF-IDF weighted Word2Vec.\n- In terms of the ML models, the notebook covers Linear Regression, AdaBoost (with RandomizedSearchCV), Bagging Regressor, Extra Trees Regressor, Gradient Boosting Regressor, Random Forest, Histogram Gradient Boosting Regressor.\n- Also, the notebook uses LazyPredict just to see the performance of different regression models.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T18:11:22.022429Z","iopub.execute_input":"2021-06-02T18:11:22.022893Z","iopub.status.idle":"2021-06-02T18:11:22.042062Z","shell.execute_reply.started":"2021-06-02T18:11:22.0228Z","shell.execute_reply":"2021-06-02T18:11:22.04074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install bs4","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T18:11:22.943726Z","iopub.execute_input":"2021-06-02T18:11:22.944516Z","iopub.status.idle":"2021-06-02T18:11:34.75151Z","shell.execute_reply.started":"2021-06-02T18:11:22.94446Z","shell.execute_reply":"2021-06-02T18:11:34.75034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom gensim.models import Word2Vec\nfrom scipy import sparse","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:34.753898Z","iopub.execute_input":"2021-06-02T18:11:34.754298Z","iopub.status.idle":"2021-06-02T18:11:36.520867Z","shell.execute_reply.started":"2021-06-02T18:11:34.754257Z","shell.execute_reply":"2021-06-02T18:11:36.519414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:36.523046Z","iopub.execute_input":"2021-06-02T18:11:36.523405Z","iopub.status.idle":"2021-06-02T18:11:36.657503Z","shell.execute_reply.started":"2021-06-02T18:11:36.523372Z","shell.execute_reply":"2021-06-02T18:11:36.656352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop([\"url_legal\", \"license\", \"standard_error\", \"id\"], axis=1, inplace=True)\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:36.659586Z","iopub.execute_input":"2021-06-02T18:11:36.660082Z","iopub.status.idle":"2021-06-02T18:11:36.674789Z","shell.execute_reply.started":"2021-06-02T18:11:36.660031Z","shell.execute_reply":"2021-06-02T18:11:36.673515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop_duplicates(subset={\"excerpt\"}, keep='first', inplace=True)\nexc = df[\"excerpt\"]\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:36.676705Z","iopub.execute_input":"2021-06-02T18:11:36.67718Z","iopub.status.idle":"2021-06-02T18:11:36.698878Z","shell.execute_reply.started":"2021-06-02T18:11:36.677135Z","shell.execute_reply":"2021-06-02T18:11:36.697463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Processing","metadata":{}},{"cell_type":"code","source":"# Printing some random excerpts\nprint(exc[0])\nprint(\"=\"*50)\nprint(exc[5])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:36.700925Z","iopub.execute_input":"2021-06-02T18:11:36.701428Z","iopub.status.idle":"2021-06-02T18:11:36.710965Z","shell.execute_reply.started":"2021-06-02T18:11:36.70138Z","shell.execute_reply":"2021-06-02T18:11:36.709842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef decontracted(phrase):\n    # Specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # General\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:36.712879Z","iopub.execute_input":"2021-06-02T18:11:36.713668Z","iopub.status.idle":"2021-06-02T18:11:36.726865Z","shell.execute_reply.started":"2021-06-02T18:11:36.713461Z","shell.execute_reply":"2021-06-02T18:11:36.725314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://gist.github.com/sebleier/554280\n# We are removing the words from the stop words list: 'no', 'nor', 'not'\n\nstopwords = set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:36.730224Z","iopub.execute_input":"2021-06-02T18:11:36.730998Z","iopub.status.idle":"2021-06-02T18:11:36.74985Z","shell.execute_reply.started":"2021-06-02T18:11:36.730929Z","shell.execute_reply":"2021-06-02T18:11:36.748126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combining all the above steps\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup\npreprocessed_excerpts = []\n\n# tqdm is for printing the status bar\nfor sen in tqdm(exc):\n    sen = re.sub(r\"http\\S+\", \"\", sen)\n    sen = BeautifulSoup(sen, 'lxml').get_text()\n    sen = decontracted(sen)\n    sen = re.sub(\"\\S*\\d\\S*\", \"\", sen).strip()\n    sen = re.sub('[^A-Za-z]+', ' ', sen)\n    sen = ' '.join(e.lower() for e in sen.split() if e.lower() not in stopwords)\n    preprocessed_excerpts.append(sen.strip())","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:36.752104Z","iopub.execute_input":"2021-06-02T18:11:36.752908Z","iopub.status.idle":"2021-06-02T18:11:39.117466Z","shell.execute_reply.started":"2021-06-02T18:11:36.752849Z","shell.execute_reply":"2021-06-02T18:11:39.115936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the Dataset","metadata":{}},{"cell_type":"code","source":"df[\"excerpt\"] = preprocessed_excerpts\nX = df.drop([\"target\"], axis=1, inplace=False)\ny = df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:39.119135Z","iopub.execute_input":"2021-06-02T18:11:39.119445Z","iopub.status.idle":"2021-06-02T18:11:39.135943Z","shell.execute_reply.started":"2021-06-02T18:11:39.119417Z","shell.execute_reply":"2021-06-02T18:11:39.134418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Featurization (Bag of Words)","metadata":{}},{"cell_type":"code","source":"# count_vect = CountVectorizer() \n# exc_train = X_train[\"excerpt\"]\n# exc_train = count_vect.fit_transform(exc_train)\n# print(type(exc_train), exc_train.shape)\n\n# exc_test = X_test[\"excerpt\"]\n# exc_test = count_vect.transform(exc_test)\n# print(type(exc_test), exc_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:39.13801Z","iopub.execute_input":"2021-06-02T18:11:39.138528Z","iopub.status.idle":"2021-06-02T18:11:39.144743Z","shell.execute_reply.started":"2021-06-02T18:11:39.138479Z","shell.execute_reply":"2021-06-02T18:11:39.143063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Featurization (Binary Bag of Words)","metadata":{}},{"cell_type":"code","source":"count_vect = CountVectorizer(binary=True) \nexc_train = X_train[\"excerpt\"]\nexc_train = count_vect.fit_transform(exc_train)\nprint(type(exc_train), exc_train.shape)\n\nexc_test = X_test[\"excerpt\"]\nexc_test = count_vect.transform(exc_test)\nprint(type(exc_test), exc_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:39.147463Z","iopub.execute_input":"2021-06-02T18:11:39.147968Z","iopub.status.idle":"2021-06-02T18:11:39.582615Z","shell.execute_reply.started":"2021-06-02T18:11:39.147905Z","shell.execute_reply":"2021-06-02T18:11:39.581382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Featurization (TF-IDF)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:58:13.19102Z","iopub.execute_input":"2021-06-01T16:58:13.191396Z","iopub.status.idle":"2021-06-01T16:58:13.218456Z","shell.execute_reply.started":"2021-06-01T16:58:13.191365Z","shell.execute_reply":"2021-06-01T16:58:13.217443Z"}}},{"cell_type":"code","source":"# tf_vect = TfidfVectorizer() \n# exc_train = X_train[\"excerpt\"]\n# exc_train = tf_vect.fit_transform(exc_train)\n# print(type(exc_train), exc_train.shape)\n\n# exc_train = exc_train.todense()\n# print(type(exc_train), exc_train.shape)\n\n# exc_test = X_test[\"excerpt\"]\n# exc_test = tf_vect.transform(exc_test)\n# print(type(exc_test), exc_test.shape)\n\n# exc_test = exc_test.todense()\n# print(type(exc_test), exc_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:39.584474Z","iopub.execute_input":"2021-06-02T18:11:39.58477Z","iopub.status.idle":"2021-06-02T18:11:39.590906Z","shell.execute_reply.started":"2021-06-02T18:11:39.584741Z","shell.execute_reply":"2021-06-02T18:11:39.589711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Featurization (Word2Vec)","metadata":{}},{"cell_type":"code","source":"# list_of_exc_train = []\n# exc_train = X_train[\"excerpt\"]\n# for exc in exc_train:\n#     list_of_exc_train.append(exc.split())\n    \n# list_of_exc_test = []\n# exc_test = X_test[\"excerpt\"]\n# for exc in exc_test:\n#     list_of_exc_test.append(exc.split())\n    \n# # Training W2V model\n# w2v_model = Word2Vec(list_of_exc_train, min_count=5, vector_size=300, workers=4, epochs=50)\n# w2v_words = list(w2v_model.wv.key_to_index)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:39.592289Z","iopub.execute_input":"2021-06-02T18:11:39.592591Z","iopub.status.idle":"2021-06-02T18:11:39.603966Z","shell.execute_reply.started":"2021-06-02T18:11:39.592561Z","shell.execute_reply":"2021-06-02T18:11:39.602658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Average W2V","metadata":{}},{"cell_type":"code","source":"# # Converting exc_train from text to vectors\n# sent_vectors = []\n# for sent in tqdm(list_of_exc_train):\n#     sent_vec = np.zeros(300) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n#     cnt_words =0; # num of words with a valid vector in the sentence/review\n#     for word in sent: # for each word in a review/sentence\n#         if word in w2v_words:\n#             vec = w2v_model.wv[word]\n#             sent_vec += vec\n#             cnt_words += 1\n#     if cnt_words != 0:\n#         sent_vec /= cnt_words\n#     sent_vectors.append(sent_vec)\n\n# exc_train = sparse.csr_matrix(sent_vectors).toarray()\n# print(type(exc_train), exc_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:39.605823Z","iopub.execute_input":"2021-06-02T18:11:39.606158Z","iopub.status.idle":"2021-06-02T18:11:39.622063Z","shell.execute_reply.started":"2021-06-02T18:11:39.606129Z","shell.execute_reply":"2021-06-02T18:11:39.620605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Converting exc_test from text to vectors\n# sent_vectors = []\n# for sent in tqdm(list_of_exc_test):\n#     sent_vec = np.zeros(300) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n#     cnt_words =0; # num of words with a valid vector in the sentence/review\n#     for word in sent: # for each word in a review/sentence\n#         if word in w2v_words:\n#             vec = w2v_model.wv[word]\n#             sent_vec += vec\n#             cnt_words += 1\n#     if cnt_words != 0:\n#         sent_vec /= cnt_words\n#     sent_vectors.append(sent_vec)\n\n# exc_test = sparse.csr_matrix(sent_vectors).toarray()\n# print(type(exc_test), exc_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:39.624306Z","iopub.execute_input":"2021-06-02T18:11:39.624772Z","iopub.status.idle":"2021-06-02T18:11:39.634372Z","shell.execute_reply.started":"2021-06-02T18:11:39.624738Z","shell.execute_reply":"2021-06-02T18:11:39.633326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF-IDF Weighted W2V","metadata":{}},{"cell_type":"code","source":"# model = TfidfVectorizer()\n# tf_idf_train_matrix = model.fit_transform(exc_train)\n# tf_idf_test_matrix = model.transform(exc_test)\n\n# # We are converting a dictionary with word as a key, and the idf as a value\n# dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))\n# tfidf_feat = model.get_feature_names()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:39.635904Z","iopub.execute_input":"2021-06-02T18:11:39.636257Z","iopub.status.idle":"2021-06-02T18:11:39.647045Z","shell.execute_reply.started":"2021-06-02T18:11:39.636224Z","shell.execute_reply":"2021-06-02T18:11:39.645865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n# row=0;\n# for sent in tqdm(list_of_exc_train): # for each review/sentence \n#     sent_vec = np.zeros(300) # as word vectors are of zero length\n#     weight_sum =0; # num of words with a valid vector in the sentence/review\n#     for word in sent: # for each word in a review/sentence\n#         if word in w2v_words and word in tfidf_feat:\n#             vec = w2v_model.wv[word]\n#             tf_idf = tf_idf_train_matrix[row, tfidf_feat.index(word)]\n#             # To reduce the computation, we can use the following\n#             # dictionary[word] = idf value of word in whole courpus\n#             # sent.count(word) = tf valeus of word in this review\n#             # tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n#             sent_vec += (vec * tf_idf)\n#             weight_sum += tf_idf\n#     if weight_sum != 0: sent_vec /= weight_sum\n#     tfidf_sent_vectors.append(sent_vec)\n#     row += 1\n\n# exc_train = sparse.csr_matrix(tfidf_sent_vectors).toarray()\n# print(type(exc_train), exc_train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:39.648877Z","iopub.execute_input":"2021-06-02T18:11:39.649438Z","iopub.status.idle":"2021-06-02T18:11:39.66498Z","shell.execute_reply.started":"2021-06-02T18:11:39.64939Z","shell.execute_reply":"2021-06-02T18:11:39.663877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n# row=0;\n# for sent in tqdm(list_of_exc_test): # for each review/sentence \n#     sent_vec = np.zeros(300) # as word vectors are of zero length\n#     weight_sum =0; # num of words with a valid vector in the sentence/review\n#     for word in sent: # for each word in a review/sentence\n#         if word in w2v_words and word in tfidf_feat:\n#             vec = w2v_model.wv[word]\n#             tf_idf = tf_idf_test_matrix[row, tfidf_feat.index(word)]\n#             # To reduce the computation, we can use the following\n#             # dictionary[word] = idf value of word in whole courpus\n#             # sent.count(word) = tf valeus of word in this review\n#             # tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n#             sent_vec += (vec * tf_idf)\n#             weight_sum += tf_idf\n#     if weight_sum != 0: sent_vec /= weight_sum\n#     tfidf_sent_vectors.append(sent_vec)\n#     row += 1\n\n# exc_test = sparse.csr_matrix(tfidf_sent_vectors).toarray()\n# print(type(exc_test), exc_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:39.668307Z","iopub.execute_input":"2021-06-02T18:11:39.669216Z","iopub.status.idle":"2021-06-02T18:11:39.676552Z","shell.execute_reply.started":"2021-06-02T18:11:39.669155Z","shell.execute_reply":"2021-06-02T18:11:39.675525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Linear Regression","metadata":{}},{"cell_type":"code","source":"# # Binary Bag of Words = 0.72\n# lr = LinearRegression(normalize=True)\n# lr.fit(exc_train, y_train)\n# y_pred = lr.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:11:41.477068Z","iopub.execute_input":"2021-06-02T18:11:41.477498Z","iopub.status.idle":"2021-06-02T18:11:41.482084Z","shell.execute_reply.started":"2021-06-02T18:11:41.47746Z","shell.execute_reply":"2021-06-02T18:11:41.481161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Binary Bag of Words = 1.462\n# exc_train_dense = exc_train.todense()\n# exc_test_dense = exc_test.todense()\n# lr = LinearRegression(normalize=True, fit_intercept=False, positive=True)\n# lr.fit(exc_train_dense, y_train)\n# y_pred = lr.predict(exc_test_dense)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:45:52.095391Z","iopub.execute_input":"2021-06-02T18:45:52.098156Z","iopub.status.idle":"2021-06-02T18:47:29.111938Z","shell.execute_reply.started":"2021-06-02T18:45:52.098072Z","shell.execute_reply":"2021-06-02T18:47:29.110795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying AdaBoost Regressor","metadata":{}},{"cell_type":"code","source":"# # Binary Bag of Words = 0.94\n# abr = AdaBoostRegressor(n_estimators=100, learning_rate=0.025, loss='square')\n# abr.fit(exc_train, y_train)\n# y_pred = abr.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:16:48.919618Z","iopub.execute_input":"2021-06-02T18:16:48.920029Z","iopub.status.idle":"2021-06-02T18:16:54.514694Z","shell.execute_reply.started":"2021-06-02T18:16:48.91998Z","shell.execute_reply":"2021-06-02T18:16:54.513774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Bagging Regressor","metadata":{}},{"cell_type":"code","source":"# # Binary Bag of Words = 0.873\n# from sklearn.ensemble import BaggingRegressor\n# br = BaggingRegressor()\n# br.fit(exc_train, y_train)\n# y_pred = br.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:18:46.008376Z","iopub.execute_input":"2021-06-02T18:18:46.009028Z","iopub.status.idle":"2021-06-02T18:19:01.561212Z","shell.execute_reply.started":"2021-06-02T18:18:46.00896Z","shell.execute_reply":"2021-06-02T18:19:01.560139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Extra Trees Regressor","metadata":{}},{"cell_type":"code","source":"# # Binary Bag of Words = 1.1583\n# from sklearn.ensemble import ExtraTreesRegressor\n# etr = ExtraTreesRegressor()\n# etr.fit(exc_train, y_train)\n# y_pred = etr.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:20:10.650787Z","iopub.execute_input":"2021-06-02T18:20:10.651436Z","iopub.status.idle":"2021-06-02T18:23:59.674783Z","shell.execute_reply.started":"2021-06-02T18:20:10.651397Z","shell.execute_reply":"2021-06-02T18:23:59.673499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Gradient Boosting Regressor","metadata":{}},{"cell_type":"code","source":"# # Binary Bag of Words = 0.806\n# from sklearn.ensemble import GradientBoostingRegressor\n# gbr = GradientBoostingRegressor()\n# gbr.fit(exc_train, y_train)\n# y_pred = gbr.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:23:59.676542Z","iopub.execute_input":"2021-06-02T18:23:59.676861Z","iopub.status.idle":"2021-06-02T18:24:02.767953Z","shell.execute_reply.started":"2021-06-02T18:23:59.676829Z","shell.execute_reply":"2021-06-02T18:24:02.7672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"# # Binary Bag of Words = 0.813\n# from sklearn.ensemble import RandomForestRegressor\n# rfr = RandomForestRegressor()\n# rfr.fit(exc_train, y_train)\n# y_pred = rfr.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:31:44.086382Z","iopub.execute_input":"2021-06-02T18:31:44.087939Z","iopub.status.idle":"2021-06-02T18:34:16.183849Z","shell.execute_reply.started":"2021-06-02T18:31:44.08783Z","shell.execute_reply":"2021-06-02T18:34:16.182256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Histogram Gradient Boosting Regressor","metadata":{}},{"cell_type":"code","source":"# # Binary Bag of Words = 0.785\n# from sklearn.experimental import enable_hist_gradient_boosting\n# from sklearn.ensemble import HistGradientBoostingRegressor\n# exc_train_dense = exc_train.todense()\n# exc_test_dense = exc_test.todense()\n# hgbr = HistGradientBoostingRegressor()\n# hgbr.fit(exc_train_dense, y_train)\n# y_pred = hgbr.predict(exc_test_dense)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T18:42:03.442461Z","iopub.execute_input":"2021-06-02T18:42:03.442886Z","iopub.status.idle":"2021-06-02T18:45:52.089662Z","shell.execute_reply.started":"2021-06-02T18:42:03.442848Z","shell.execute_reply":"2021-06-02T18:45:52.088441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"# parameters = {\n#     'n_estimators': [25, 50, 75, 100],\n#     'learning_rate': [0.001, 0.01, 0.1, 1, 5],\n#     'loss': ['linear', 'square', 'exponential']\n# }\n# abr = AdaBoostRegressor()\n# sco = make_scorer(mean_squared_error)\n# reg = RandomizedSearchCV(abr, parameters, scoring = sco)\n# reg.fit(exc_train, y_train)\n# print(reg.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Applying Lazy Predict","metadata":{}},{"cell_type":"code","source":"# pip install lazypredict","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T17:27:17.420758Z","iopub.execute_input":"2021-06-02T17:27:17.422698Z","iopub.status.idle":"2021-06-02T17:27:17.434389Z","shell.execute_reply.started":"2021-06-02T17:27:17.42265Z","shell.execute_reply":"2021-06-02T17:27:17.433308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from lazypredict.Supervised import LazyRegressor\n\n# reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n# models, predictions = reg.fit(exc_train, exc_test, y_train, y_test)\n# print(models)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T17:27:17.436291Z","iopub.execute_input":"2021-06-02T17:27:17.436942Z","iopub.status.idle":"2021-06-02T17:27:17.448895Z","shell.execute_reply.started":"2021-06-02T17:27:17.436896Z","shell.execute_reply":"2021-06-02T17:27:17.447782Z"},"trusted":true},"execution_count":null,"outputs":[]}]}