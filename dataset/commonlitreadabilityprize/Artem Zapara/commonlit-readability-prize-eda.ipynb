{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CommonLit Readability Prize EDA\n<img src=\"https://library.ucf.edu/wp-content/uploads/sites/5/2016/03/sort-by-color.jpg\" alt=\"title\">","metadata":{}},{"cell_type":"markdown","source":"## Introduction\nThe goal of this competition is to build a model that is able to estimate the readability score of short text excerpts. Traditionally, this can be achieved by applying various readability formulas that depend on linguistic features (e.g. average number of syllables per word, averane sentence length, etc.). Often these features are indeed highly correlated with the reading difficulty, however general measure of the readability score is more complex, as it involves level of abstraction, the use of images and difficult concepts, active and passive voice and so on. Therefore, a data-driven approach together with recent developments in the field of Natural Language Processing can be the next step to improve the current estimations.\n\nThe good models will significantly help teachers, administrators, students, and literacy curriculum developers. ","metadata":{}},{"cell_type":"markdown","source":"# Importing packages and loading data","metadata":{}},{"cell_type":"code","source":"!pip install syllables\n!pip install rich\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\npd.set_option(\"display.max_colwidth\", None)  # setting the maximum width in characters when displaying pandas column. \"None\" value means unlimited.\n\nimport os      \nimport syllables\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom wordcloud import WordCloud\nfrom rich import theme, console","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:23:58.705407Z","iopub.execute_input":"2021-06-02T13:23:58.706109Z","iopub.status.idle":"2021-06-02T13:24:12.636245Z","shell.execute_reply.started":"2021-06-02T13:23:58.705993Z","shell.execute_reply":"2021-06-02T13:24:12.635151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:12.638002Z","iopub.execute_input":"2021-06-02T13:24:12.638363Z","iopub.status.idle":"2021-06-02T13:24:12.644711Z","shell.execute_reply.started":"2021-06-02T13:24:12.638315Z","shell.execute_reply":"2021-06-02T13:24:12.643685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:12.646805Z","iopub.execute_input":"2021-06-02T13:24:12.647183Z","iopub.status.idle":"2021-06-02T13:24:12.755325Z","shell.execute_reply.started":"2021-06-02T13:24:12.647141Z","shell.execute_reply":"2021-06-02T13:24:12.754385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"print(f\"Total entries in train.csv: {len(train_data)}\")\nprint(f\"Total entries in test.csv: {len(test_data)}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:12.756639Z","iopub.execute_input":"2021-06-02T13:24:12.756925Z","iopub.status.idle":"2021-06-02T13:24:12.761548Z","shell.execute_reply.started":"2021-06-02T13:24:12.756899Z","shell.execute_reply":"2021-06-02T13:24:12.760607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:12.762872Z","iopub.execute_input":"2021-06-02T13:24:12.763137Z","iopub.status.idle":"2021-06-02T13:24:12.797069Z","shell.execute_reply.started":"2021-06-02T13:24:12.763112Z","shell.execute_reply":"2021-06-02T13:24:12.79612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:12.79831Z","iopub.execute_input":"2021-06-02T13:24:12.798563Z","iopub.status.idle":"2021-06-02T13:24:12.810348Z","shell.execute_reply.started":"2021-06-02T13:24:12.798537Z","shell.execute_reply":"2021-06-02T13:24:12.809342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:12.811616Z","iopub.execute_input":"2021-06-02T13:24:12.811904Z","iopub.status.idle":"2021-06-02T13:24:12.833577Z","shell.execute_reply.started":"2021-06-02T13:24:12.811876Z","shell.execute_reply":"2021-06-02T13:24:12.832456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:12.835525Z","iopub.execute_input":"2021-06-02T13:24:12.835818Z","iopub.status.idle":"2021-06-02T13:24:12.847312Z","shell.execute_reply.started":"2021-06-02T13:24:12.835787Z","shell.execute_reply":"2021-06-02T13:24:12.846577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For having clearer and styled output in the terminal, we can configure the print output by using *console* and *theme* classes from the Python **rich** library (check out [the official link](https://github.com/willmcgugan/rich)).","metadata":{}},{"cell_type":"code","source":"custom_theme = theme.Theme({\n    \"info\" : \"cyan\",\n    \"warning\": \"magenta\",\n    \"danger\": \"bold red\"\n})\n\nconsole = console.Console(theme=custom_theme)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:12.848411Z","iopub.execute_input":"2021-06-02T13:24:12.848649Z","iopub.status.idle":"2021-06-02T13:24:12.860883Z","shell.execute_reply.started":"2021-06-02T13:24:12.848624Z","shell.execute_reply":"2021-06-02T13:24:12.859898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can print out a random text passage together with the corresponding target and the standard error.","metadata":{}},{"cell_type":"code","source":"i = random.randrange(train_data.shape[0])   # selecting a random row from the dataframe\n\nconsole.print(train_data.iloc[i].excerpt+'\\n',style='warning')\nconsole.print(f\"Target (standard error) --> {train_data.iloc[i].target} ({train_data.iloc[i].standard_error})\", style = 'info')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:12.862188Z","iopub.execute_input":"2021-06-02T13:24:12.862541Z","iopub.status.idle":"2021-06-02T13:24:12.882913Z","shell.execute_reply.started":"2021-06-02T13:24:12.862499Z","shell.execute_reply":"2021-06-02T13:24:12.882023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Advanced exploratory data analysis\nIn this section, we are going to display some graphs and figures using the raw data in *train.csv*.\n\nFirst, we can visualize the distribution of target values and standard errors using *seaborn* and *matplotlib*.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_target = train_data['target'].values\ntrain_se = train_data['standard_error'].values\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,6))\n\n# hardcoding the appearance of the plots\n\nsns.kdeplot(train_target, ax=ax1, label='Train', lw=5, alpha=0.6)\nax1.axvline(train_target.mean(),linestyle='--', linewidth=2)\nax1.annotate('mean', fontsize=18, xy=(train_target.mean(), 0.2), \n            xytext=(0.8, 0.9), textcoords='axes fraction',\n            arrowprops=dict(facecolor='black', shrink=0.02, connectionstyle=\"arc3,rad=-0.3\",),\n             bbox=dict(boxstyle=\"square\", fc=\"w\", ec=\"k\")\n            )\nax1.tick_params(axis='both', which='major', labelsize=18)\nax1.set_xlabel('target', fontsize=18)\nax1.set_ylabel('density', fontsize=18)\n\n\nsns.kdeplot(train_se, ax=ax2, label='Train', lw=5, alpha=0.6)\nax2.axvline(train_se.mean(),linestyle='--', linewidth=2)\nax2.annotate('mean', fontsize=18, xy=(train_se.mean(), 6),  \n            xytext=(0.4, 0.9), textcoords='axes fraction',\n            arrowprops=dict(facecolor='black', shrink=0.02, connectionstyle=\"arc3,rad=0.3\",),\n             bbox=dict(boxstyle=\"square\", fc=\"w\", ec=\"k\")\n            )\nax2.tick_params(axis='both', which='major', labelsize=18)\nax2.set_xlabel('standard_error', fontsize=18)\nax2.set_ylabel('density', fontsize=18)\n\n\nplt.suptitle('Distribution of targets and standard errors in train.csv', fontsize = 24)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:12.884012Z","iopub.execute_input":"2021-06-02T13:24:12.884486Z","iopub.status.idle":"2021-06-02T13:24:13.443028Z","shell.execute_reply.started":"2021-06-02T13:24:12.884455Z","shell.execute_reply":"2021-06-02T13:24:13.442072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we render a scatter plot to reveal the relationship between targets and standard errors.","metadata":{}},{"cell_type":"code","source":"sns.set_theme(style=\"whitegrid\")\n\nfig, ax = plt.subplots(figsize=(8,6))\nsns.scatterplot(y=train_target, x=train_se, color = 'blue', alpha = 0.3, s=50)\nax.tick_params(axis='both', which='major', labelsize=18)\nax.set_xlabel('standard error', fontsize=18)\nax.set_ylabel('target', fontsize=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:13.444166Z","iopub.execute_input":"2021-06-02T13:24:13.444427Z","iopub.status.idle":"2021-06-02T13:24:13.646441Z","shell.execute_reply.started":"2021-06-02T13:24:13.4444Z","shell.execute_reply":"2021-06-02T13:24:13.645493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like one entry has weird numeric values. Let's inspect the raw dataframe for confirmation.","metadata":{}},{"cell_type":"code","source":"train_data.sort_values(by='standard_error').head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:13.647553Z","iopub.execute_input":"2021-06-02T13:24:13.647816Z","iopub.status.idle":"2021-06-02T13:24:13.661376Z","shell.execute_reply.started":"2021-06-02T13:24:13.647789Z","shell.execute_reply":"2021-06-02T13:24:13.660404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, the excerpt with id=*436ce79fe* has both target and standard error equal to zero. However, as mentioned in [this discussion](http://https://www.kaggle.com/c/commonlitreadabilityprize/discussion/236403), this sample is the baseline for all other samples.\n\nWe can also combine results from two previous figures into one using *jointplot* from the **seaborn** package.","metadata":{}},{"cell_type":"code","source":"sns.jointplot(y=train_data['target'], x=train_data['standard_error'], \n              kind='hex', \n              edgecolor='tab:blue', \n              marginal_kws=dict(bins=25), \n              height=8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:13.662638Z","iopub.execute_input":"2021-06-02T13:24:13.662941Z","iopub.status.idle":"2021-06-02T13:24:14.303121Z","shell.execute_reply.started":"2021-06-02T13:24:13.662914Z","shell.execute_reply":"2021-06-02T13:24:14.30224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's display the wordclouds. The WordCloud is a technique for showing which words are the most frequent in a given text. ","metadata":{}},{"cell_type":"code","source":"console.print(f\"Word cloud generated from the whole dataset:\", style = \"info\")\n\nplt.figure(figsize=(10,10))\nwordcloud = WordCloud( background_color='white',\n                        width=600,\n                        height=500).generate(\" \".join(train_data['excerpt']))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:14.30411Z","iopub.execute_input":"2021-06-02T13:24:14.304467Z","iopub.status.idle":"2021-06-02T13:24:16.775083Z","shell.execute_reply.started":"2021-06-02T13:24:14.304439Z","shell.execute_reply":"2021-06-02T13:24:16.774216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = random.randrange(train_data.shape[0])\n\nconsole.print(f\"Word cloud generated from the {i}-th excerpt:\", style = \"info\")\n\nplt.figure(figsize=(10,10))\nwordcloud = WordCloud( background_color='white',\n                        width=600,\n                        height=500).generate(train_data.iloc[i].excerpt)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:16.77656Z","iopub.execute_input":"2021-06-02T13:24:16.776986Z","iopub.status.idle":"2021-06-02T13:24:17.37292Z","shell.execute_reply.started":"2021-06-02T13:24:16.776945Z","shell.execute_reply":"2021-06-02T13:24:17.372016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\nIn the final section, we will construct some extra linguistic features on top of the raw data:\n- letter_count - the total number of letters\n- word_count - the total number of words \n- sentence_count - the total number of sentences\n- syllable_count - the total number of syllables (for this, we can use the Python **syllables** library, see [this reference](https://github.com/prosegrinder/python-syllables))\n- avg_word_len - the average length of words\n- avg_sentence_len - the average length of sentences\n- avg_word_syll - the average number of syllables per word\n- avg_sen_syll - the average number of syllables per sentence","metadata":{}},{"cell_type":"code","source":"train_data['letter_count'] = train_data['excerpt'].str.len()\ntrain_data['word_count'] = train_data['excerpt'].str.split().apply(len)\ntrain_data['sentence_count'] = train_data['excerpt'].str.split(pat='[.!?]+').str.len()-1\ntrain_data['syllable_count'] = train_data['excerpt'].apply(lambda x: syllables.estimate(x.lower()))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:17.374408Z","iopub.execute_input":"2021-06-02T13:24:17.374814Z","iopub.status.idle":"2021-06-02T13:24:17.793443Z","shell.execute_reply.started":"2021-06-02T13:24:17.374769Z","shell.execute_reply":"2021-06-02T13:24:17.792676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['avg_word_len'] = train_data['letter_count']/train_data['word_count']\ntrain_data['avg_sentence_len'] = train_data['word_count']/train_data['sentence_count']\ntrain_data['avg_word_syll'] = train_data['syllable_count']/train_data['word_count']\ntrain_data['avg_sen_syll'] = train_data['syllable_count']/train_data['sentence_count']","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:17.794546Z","iopub.execute_input":"2021-06-02T13:24:17.795099Z","iopub.status.idle":"2021-06-02T13:24:17.804838Z","shell.execute_reply.started":"2021-06-02T13:24:17.795046Z","shell.execute_reply":"2021-06-02T13:24:17.803804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This time, we print out a random text passage together with the corresponding target, the standard error, and some newly generated linguistic features.","metadata":{}},{"cell_type":"code","source":"i = random.randrange(train_data.shape[0])\n\nconsole.print(train_data.iloc[i].excerpt+'\\n',style='warning')\nconsole.print(f\"Target (standard error) --> {train_data.iloc[i].target} ({train_data.iloc[i].standard_error})\", style = 'info')\nconsole.print(f\"Total sentences: {train_data.iloc[i].sentence_count}\", style = 'info')\nconsole.print(f\"Total words: {train_data.iloc[i].word_count}\", style = 'info')\nconsole.print(f\"Total syllables: {train_data.iloc[i].syllable_count}\", style = 'info')\nconsole.print(f\"Total letters: {train_data.iloc[i].letter_count}\", style = 'info')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:17.806251Z","iopub.execute_input":"2021-06-02T13:24:17.806539Z","iopub.status.idle":"2021-06-02T13:24:17.836239Z","shell.execute_reply.started":"2021-06-02T13:24:17.806513Z","shell.execute_reply":"2021-06-02T13:24:17.83525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To be able to see the relationship between numeric data in the training dataset, we display the correlation matrix.","metadata":{}},{"cell_type":"code","source":"corr = train_data.corr()\n\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True  # so that we do not duplicate values in the correlation matrix\n\nfig, ax = plt.subplots(figsize=(8,8))\nax = sns.heatmap(corr, ax=ax, annot=True, mask=mask, square=True, fmt = '.2f', annot_kws={\"fontsize\":15}, cbar=False)\nax.set_xticklabels(ax.get_xmajorticklabels(), fontsize = 18)\nax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 18)\nplt.title('Correlation matrix', fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:17.837508Z","iopub.execute_input":"2021-06-02T13:24:17.837799Z","iopub.status.idle":"2021-06-02T13:24:18.286337Z","shell.execute_reply.started":"2021-06-02T13:24:17.837769Z","shell.execute_reply":"2021-06-02T13:24:18.285636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For example, as one can see, there is a strong correlation between the target and the average number of syllables per word (which is not surprise). So, let's plot the scatter plot for these two columns.","metadata":{}},{"cell_type":"code","source":"sns.jointplot(y=train_data['target'], x=train_data['avg_word_syll'], \n              kind='hex', \n              edgecolor='tab:blue', \n              marginal_kws=dict(bins=25), \n              height=8)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:18.287334Z","iopub.execute_input":"2021-06-02T13:24:18.287738Z","iopub.status.idle":"2021-06-02T13:24:18.814111Z","shell.execute_reply.started":"2021-06-02T13:24:18.287683Z","shell.execute_reply":"2021-06-02T13:24:18.813437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is also possible to display all correlations and distributions using *pairplot* from the Python **seaborn** library.","metadata":{}},{"cell_type":"code","source":"sns.pairplot(train_data, diag_kind='kde')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T13:24:39.091614Z","iopub.execute_input":"2021-06-02T13:24:39.092157Z","iopub.status.idle":"2021-06-02T13:24:54.344646Z","shell.execute_reply.started":"2021-06-02T13:24:39.092111Z","shell.execute_reply":"2021-06-02T13:24:54.343842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To conclude, we have performed exploratory data analysis on our training dataset. This helps us to understand what kind of input we will be working with when building a readability model.","metadata":{}}]}