{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.magic import register_cell_magic\nimport os\nfrom pathlib import Path\n\n## define custom magic to save most useful classes and use them in inference notebook \n## instead of copying the code every time you have changes in the classes\n@register_cell_magic\ndef write_and_run(line, cell):\n    argz = line.split()\n    file = argz[-1]\n    mode = 'w'\n    if len(argz) == 2 and argz[0] == '-a':\n        mode = 'a'\n    with open(file, mode) as f:\n        f.write(cell)\n    get_ipython().run_cell(cell)\n    \nPath('/kaggle/working/scripts').mkdir(exist_ok=True)\nmodels_dir = Path('/kaggle/working/models')\nmodels_dir.mkdir(exist_ok=True)","metadata":{"_uuid":"9c1fe2bd-292e-4ea4-b189-d592be698eaf","_cell_guid":"1f1cbb11-d737-4d9f-8e80-68b3b1fdd9e7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T02:37:40.488932Z","iopub.execute_input":"2021-07-26T02:37:40.489255Z","iopub.status.idle":"2021-07-26T02:37:40.496299Z","shell.execute_reply.started":"2021-07-26T02:37:40.489224Z","shell.execute_reply":"2021-07-26T02:37:40.495413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%write_and_run scripts/imports.py\n\nimport os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import RandomSampler, SequentialSampler, Sampler\nfrom torch.nn.functional import mse_loss\nfrom transformers import AutoModel,AutoTokenizer,get_cosine_schedule_with_warmup, AutoConfig, AdamW\nfrom torch.cuda.amp import autocast, GradScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-talk')\n# print(plt.style.available)\nfrom time import time\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nw_ = Fore.WHITE\nbb_ = Back.BLACK\nsr_ = Style.RESET_ALL","metadata":{"_uuid":"54b8d31f-38bb-4edc-ba8d-3f66788f5f14","_cell_guid":"0a18fcad-d066-48b1-b4d3-edfb1d832d1d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T02:37:40.497856Z","iopub.execute_input":"2021-07-26T02:37:40.498443Z","iopub.status.idle":"2021-07-26T02:37:40.51626Z","shell.execute_reply.started":"2021-07-26T02:37:40.498404Z","shell.execute_reply":"2021-07-26T02:37:40.51541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%write_and_run scripts/config.py\n\nclass Config:\n    model_name = 'roberta-large'\n    output_hidden_states = True\n    epochs = 3\n#     evaluate_interval = 40\n    batch_size = 8\n    device = 'cuda'\n    seed = 2021\n    max_len = 248\n    lr = 1e-5\n    wd = 0.01\n#     eval_schedule = [(float('inf'), 40), (0.5, 30), (0.49, 20), (0.48, 10), (0.47, 3), (0, 0)]\n    eval_schedule = [(float('inf'), 40), (0.47, 20), (0.46, 10), (0, 0)]\n\n    gradient_accumulation = 2","metadata":{"_uuid":"554b3b6d-bae9-4f93-8ddd-67e06d60cf6c","_cell_guid":"b78a2953-0a78-4006-bd2b-5383df67a262","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T02:37:40.518049Z","iopub.execute_input":"2021-07-26T02:37:40.51842Z","iopub.status.idle":"2021-07-26T02:37:40.527038Z","shell.execute_reply.started":"2021-07-26T02:37:40.518386Z","shell.execute_reply":"2021-07-26T02:37:40.526263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=2021):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=Config.seed)","metadata":{"_uuid":"31de144a-ca76-4beb-8263-12ef7fa6518e","_cell_guid":"cb7af884-04a8-409d-9dc6-e837a4a21d7e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T02:37:40.528589Z","iopub.execute_input":"2021-07-26T02:37:40.529003Z","iopub.status.idle":"2021-07-26T02:37:40.540478Z","shell.execute_reply.started":"2021-07-26T02:37:40.528969Z","shell.execute_reply":"2021-07-26T02:37:40.53966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = pd.read_csv('../input/k/chamecall/train-val-split/train.csv')\n# val_df = pd.read_csv('../input/k/chamecall/train-val-split/val.csv')\n\nkfold_df = pd.read_csv('../input/train-val-split/kfold.csv')","metadata":{"_uuid":"e027803b-674b-4b1d-b096-938396516e6a","_cell_guid":"1b998d65-4aeb-4215-b780-627c17cd025e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T02:37:40.541729Z","iopub.execute_input":"2021-07-26T02:37:40.542318Z","iopub.status.idle":"2021-07-26T02:37:40.651884Z","shell.execute_reply.started":"2021-07-26T02:37:40.542266Z","shell.execute_reply":"2021-07-26T02:37:40.651092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%write_and_run scripts/dataset.py\n\nfrom torch.utils.data import Dataset\nimport torch\n\ndef convert_examples_to_features(text, tokenizer, max_len):\n\n    tok = tokenizer.encode_plus(\n        text, \n        max_length=max_len, \n        truncation=True,\n        padding='max_length',\n    )\n    return tok\n\n\nclass CLRPDataset(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.tolist()\n        if not is_test:\n            self.targets = self.data.target.tolist()\n            \n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt = self.excerpts[item]\n            label = self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.float),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","metadata":{"_uuid":"5f7b7454-4455-4386-9b6a-2c8f12bdf533","_cell_guid":"a2f53d03-92d8-4d27-9563-aecdbb600337","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T02:37:40.65322Z","iopub.execute_input":"2021-07-26T02:37:40.653608Z","iopub.status.idle":"2021-07-26T02:37:40.663713Z","shell.execute_reply.started":"2021-07-26T02:37:40.65357Z","shell.execute_reply":"2021-07-26T02:37:40.66268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%write_and_run scripts/model.py\n\nimport torch\nimport torch.nn as nn\n\nclass AttentionHead(nn.Module):\n    def __init__(self, h_size, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(h_size, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(CLRPModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size*4)\n        self.linear = nn.Linear(self.h_size*2, 1)\n        self.linear_out = nn.Linear(self.h_size*8, 1)\n\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n       \n        all_hidden_states = torch.stack(transformer_out.hidden_states)\n        cat_over_last_layers = torch.cat(\n            (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n        )\n        \n        cls_pooling = cat_over_last_layers[:, 0]   \n        head_logits = self.head(cat_over_last_layers)\n        y_hat = self.linear_out(torch.cat([head_logits, cls_pooling], -1))\n        \n        return y_hat","metadata":{"_uuid":"862cc096-8c67-4cd3-8857-3284b97afd17","_cell_guid":"c5aea7bb-0657-4456-937b-529cf3dfa31c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T02:37:40.665717Z","iopub.execute_input":"2021-07-26T02:37:40.666156Z","iopub.status.idle":"2021-07-26T02:37:40.681329Z","shell.execute_reply.started":"2021-07-26T02:37:40.666117Z","shell.execute_reply":"2021-07-26T02:37:40.680427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:389]    \n    attention_parameters = named_parameters[391:395]\n    regressor_parameters = named_parameters[395:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n    \n    # increase lr every second layer\n    increase_lr_every_k_layer = 1\n    lrs = np.linspace(1, 5, 24 // increase_lr_every_k_layer)\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n        splitted_name = name.split('.')\n        lr = Config.lr\n        if len(splitted_name) >= 4 and str.isdigit(splitted_name[3]):\n            layer_num = int(splitted_name[3])\n            lr = lrs[layer_num // increase_lr_every_k_layer] * Config.lr \n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return optim.AdamW(parameters)","metadata":{"_uuid":"e14e602c-b634-4bd1-8fbe-20a46f383648","_cell_guid":"34ebe01b-e208-4705-acd7-465524475975","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T02:37:40.684611Z","iopub.execute_input":"2021-07-26T02:37:40.684913Z","iopub.status.idle":"2021-07-26T02:37:40.695908Z","shell.execute_reply.started":"2021-07-26T02:37:40.684862Z","shell.execute_reply":"2021-07-26T02:37:40.695146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"   \n    \nclass DynamicPadCollate:\n    def __call__(self,batch):\n                \n        out = {'input_ids' :[],\n               'attention_mask':[],\n                'label':[]\n        }\n        \n        for i in batch:\n            for k,v in i.items():\n                out[k].append(v)\n                \n        max_pad =0\n\n        for p in out['input_ids']:\n            if max_pad < len(p):\n                max_pad = len(p)\n                    \n\n        for i in range(len(batch)):\n            \n            input_id = out['input_ids'][i]\n            att_mask = out['attention_mask'][i]\n            text_len = len(input_id)\n            \n            out['input_ids'][i] = (out['input_ids'][i].tolist() + [1] * (max_pad - text_len))[:max_pad]\n            out['attention_mask'][i] = (out['attention_mask'][i].tolist() + [0] * (max_pad - text_len))[:max_pad]\n        \n        out['input_ids'] = torch.tensor(out['input_ids'],dtype=torch.long)\n        out['attention_mask'] = torch.tensor(out['attention_mask'],dtype=torch.long)\n        out['label'] = torch.tensor(out['label'],dtype=torch.float)\n        \n        return out\n\n\nclass AvgCounter:\n    def __init__(self):\n        self.reset()\n        \n    def update(self, loss, n_samples):\n        self.loss += loss * n_samples\n        self.n_samples += n_samples\n        \n    def avg(self):\n        return self.loss / self.n_samples\n    \n    def reset(self):\n        self.loss = 0\n        self.n_samples = 0\n\nclass EvaluationScheduler:\n    def __init__(self, evaluation_schedule, penalize_factor=1, max_penalty=8):\n        self.evaluation_schedule = evaluation_schedule\n        self.evaluation_interval = self.evaluation_schedule[0][1]\n        self.last_evaluation_step = 0\n        self.prev_loss = float('inf')\n        self.penalize_factor = penalize_factor\n        self.penalty = 0\n        self.prev_interval = -1\n        self.max_penalty = max_penalty\n\n    def step(self, step):\n        # should we to make evaluation right now\n        if step >= self.last_evaluation_step + self.evaluation_interval:\n            self.last_evaluation_step = step\n            return True\n        else:\n            return False\n        \n            \n    def update_evaluation_interval(self, last_loss):\n        # set up evaluation_interval depending on loss value\n        cur_interval = -1\n        for i, (loss, interval) in enumerate(self.evaluation_schedule[:-1]):\n            if self.evaluation_schedule[i+1][0] < last_loss < loss:\n                self.evaluation_interval = interval\n                cur_interval = i\n                break\n#         if last_loss > self.prev_loss and self.prev_interval == cur_interval:\n#             self.penalty += self.penalize_factor\n#             self.penalty = min(self.penalty, self.max_penalty)\n#             self.evaluation_interval += self.penalty\n#         else:\n#             self.penalty = 0\n            \n        self.prev_loss = last_loss\n        self.prev_interval = cur_interval\n        \n          \n        \ndef make_dataloader(data, tokenizer, is_train=True):\n    dataset = CLRPDataset(data, tokenizer=tokenizer, max_len=Config.max_len)\n    if is_train:\n        sampler = RandomSampler(dataset)\n    else:\n        sampler = SequentialSampler(dataset)\n\n    batch_dataloader = DataLoader(dataset, sampler=sampler, batch_size=Config.batch_size, pin_memory=True, collate_fn=DynamicPadCollate())\n    return batch_dataloader\n                   \n            \nclass Trainer:\n    def __init__(self, train_dl, val_dl, model, optimizer, scheduler, scaler, criterion, model_num):\n        self.train_dl = train_dl\n        self.val_dl = val_dl\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = Config.device\n        self.batches_per_epoch = len(self.train_dl)\n        self.total_batch_steps = self.batches_per_epoch * Config.epochs\n        self.criterion = criterion\n        self.model_num = model_num\n        \n        self.scaler = scaler\n                \n    def run(self):\n        record_info = {\n            'train_loss': [],\n            'val_loss': [],\n        }\n        \n        best_val_loss = float('inf')\n        evaluation_scheduler = EvaluationScheduler(Config.eval_schedule)\n        train_loss_counter = AvgCounter()\n        step = 0\n        \n        for epoch in range(Config.epochs):\n            \n            print(f'{r_}Epoch: {epoch+1}/{Config.epochs}{sr_}')\n            start_epoch_time = time()\n            \n            for batch_num, batch in enumerate(self.train_dl):\n                train_loss = self.train(batch, step)\n#                 print(f'{epoch+1}#[{step+1}/{len(self.train_dl)}]: train loss - {train_loss.item()}')\n\n                train_loss_counter.update(train_loss, len(batch))\n                record_info['train_loss'].append((step, train_loss.item()))\n\n                if evaluation_scheduler.step(step):\n                    val_loss = self.evaluate()\n                    \n                    record_info['val_loss'].append((step, val_loss.item()))        \n                    print(f'\\t\\t{epoch+1}#[{batch_num+1}/{self.batches_per_epoch}]: train loss - {train_loss_counter.avg()} | val loss - {val_loss}',)\n                    train_loss_counter.reset()\n\n                    if val_loss < best_val_loss:\n                        best_val_loss = val_loss.item()\n                        print(f\"\\t\\t{g_}Val loss decreased from {best_val_loss} to {val_loss}{sr_}\")\n                        torch.save(self.model, models_dir / f'best_model_{self.model_num}.pt')\n                        \n                    evaluation_scheduler.update_evaluation_interval(val_loss.item())\n                        \n            \n                step += 1\n            end_epoch_time = time()\n            print(f'{bb_}{y_}The epoch took {end_epoch_time - start_epoch_time} sec..{sr_}')\n\n        return record_info, best_val_loss\n            \n\n    def train(self, batch, batch_step):\n        self.model.train()\n        sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n        with autocast():\n            preds = self.model(sent_id, mask)\n            train_loss = self.criterion(preds, labels.unsqueeze(1))\n        \n        self.scaler.scale(train_loss).backward()\n#         train_loss.backward()\n        \n        if (batch_step + 1) % Config.gradient_accumulation or batch_step+1 == self.total_batch_steps:\n            self.scaler.step(self.optimizer)\n            self.scaler.update()\n#             self.optimizer.step()\n            self.model.zero_grad() \n        self.scheduler.step()\n        return torch.sqrt(train_loss)\n\n    def evaluate(self):\n        self.model.eval()\n        val_loss_counter = AvgCounter()\n\n        for step,batch in enumerate(self.val_dl):\n            sent_id, mask, labels = batch['input_ids'].to(self.device), batch['attention_mask'].to(self.device), batch['label'].to(self.device)\n            with torch.no_grad():\n                with autocast():\n                    preds = self.model(sent_id, mask)\n                    loss = self.criterion(preds,labels.unsqueeze(1))\n                val_loss_counter.update(torch.sqrt(loss), len(labels))\n        return val_loss_counter.avg()\n    \n    \ndef mse_loss(y_true,y_pred):\n    return nn.functional.mse_loss(y_true,y_pred)","metadata":{"_uuid":"9f29ed45-f81a-4c05-bb82-12069c3dfe08","_cell_guid":"2afd04f1-4d4a-4309-8bb4-91e6713b72a2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T02:37:40.698808Z","iopub.execute_input":"2021-07-26T02:37:40.699096Z","iopub.status.idle":"2021-07-26T02:37:40.733135Z","shell.execute_reply.started":"2021-07-26T02:37:40.69907Z","shell.execute_reply":"2021-07-26T02:37:40.732305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_scores = []\n\nfor model_num in range(10): \n    print(f'{bb_}{w_}  Model#{model_num+1}  {sr_}')\n\n    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n    config = AutoConfig.from_pretrained(Config.model_name)\n    config.update({\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7,\n            \"output_hidden_states\": True\n            }) \n\n    train_dl = make_dataloader(kfold_df[kfold_df.fold!=model_num], tokenizer)\n    val_dl = make_dataloader(kfold_df[kfold_df.fold==model_num], tokenizer, is_train=False)\n\n#     train_dl = make_dataloader(train_df, tokenizer)\n#     val_dl = make_dataloader(val_df, tokenizer, is_train=False)\n\n    transformer = AutoModel.from_pretrained(Config.model_name, config=config)  \n\n    model = CLRPModel(transformer, config)\n    \n    model = model.to(Config.device)\n    optimizer = create_optimizer(model)\n    scaler = GradScaler()\n#     optimizer = optim.AdamW(model.parameters(), lr=Config.lr)\n    scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_training_steps=Config.epochs * len(train_dl),\n            num_warmup_steps=len(train_dl) * Config.epochs * 0.11)  \n\n    criterion = mse_loss\n\n    trainer = Trainer(train_dl, val_dl, model, optimizer, scheduler, scaler, criterion, model_num)\n    record_info, best_val_loss = trainer.run()\n    best_scores.append(best_val_loss)    \n    \n    steps, train_losses = list(zip(*record_info['train_loss']))\n    plt.plot(steps, train_losses, label='train_loss')\n    steps, val_losses = list(zip(*record_info['val_loss']))\n    plt.plot(steps, val_losses, label='val_loss')\n    plt.legend()\n    plt.show()\n    \nprint('Best val losses:', best_scores)\nprint('Avg val loss:', np.array(best_scores).mean())\n!date '+%A %W %Y %X' > execution_time","metadata":{"_uuid":"ee8a7d03-b2ce-49d3-82f3-02460da8ab2a","_cell_guid":"de47bb94-3715-4a8b-a371-ac5a60a378c1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T02:37:40.736275Z","iopub.execute_input":"2021-07-26T02:37:40.73661Z"},"trusted":true},"execution_count":null,"outputs":[]}]}