{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id='introduction'></a>\n# Introduction\n\nI used Maunish's ideas from [this notebook](https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference).\n\nThis offline notebook uses [my other notebook](https://www.kaggle.com/angyalfold/roberta-large-k-fold-models/)'s model and tokenizer to make predictions. The idea here is to use models in a k-fold manner which use my fine-tuned Roberta-large model ([from here](https://www.kaggle.com/angyalfold/pretrain-roberta-large-on-clrp-data/)) with an attention head and a custom regressor on top.\n\nThis notebook is part of a series:\n1. Pretrain roberta large on the CommonLit dataset [here](https://www.kaggle.com/angyalfold/pretrain-roberta-large-on-clrp-data/).\n2. Produce k models which can later be used for determining the readability of texts [here](https://www.kaggle.com/angyalfold/roberta-large-k-fold-models/).\n3. Make predictions with a custom NN regressor (this notebook).\n4. Ensemble (Roberta large + SVR, Roberta large + Ridge, Roberta large + custom NN head) [here](https://www.kaggle.com/angyalfold/ensemble-for-commonlit/).","metadata":{}},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n# Table of contents\n* [Introduction](#introduction)\n* [Classes & configs](#classes)\n    * [Configs](#classes_config)\n    * [Data set](#classes_data_set)\n    * [Model](#classes_model)\n* [Make predictions](#make_predictions)\n    * [Read test data](#make_predictions_read_test_data)\n    * [Make predictions with pretrained models](#make_predictions_make_predictions)\n    * [Save results](#make_predictions_save_results)","metadata":{}},{"cell_type":"markdown","source":"<a id='classes'></a>\n# Classes & configs\n[[back to top]](#toc)","metadata":{}},{"cell_type":"markdown","source":"<a id='classes_config'></a>\n## Config\n[[back to top]](#toc)","metadata":{}},{"cell_type":"code","source":"import torch\n\nconfig = {\n    'batch_size': 8,\n    'best_pretrained_roberta_folder': '../input/pretrain-roberta-large-on-clrp-data/clrp_roberta_large/best_model/',\n    'num_of_models': 5,\n    'sentence_max_length' : 256\n}\n\nfor (k, v) in config.items():\n    print(f\"The value for {k}: {v}\")\n    \ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nprint(f\"Device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-31T06:09:08.440728Z","iopub.execute_input":"2021-07-31T06:09:08.441106Z","iopub.status.idle":"2021-07-31T06:09:09.779652Z","shell.execute_reply.started":"2021-07-31T06:09:08.441019Z","shell.execute_reply":"2021-07-31T06:09:09.778803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=classes_data_set></a>\n## Data set\n[[back to top]](#toc)","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass ReadabilityDataset(torch.utils.data.Dataset):\n    \"\"\"Custom dataset for the Readability task\"\"\"\n    def __init__(self, encodings):\n        self.encodings = encodings\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        return item\n        \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n    \n\nprint(ReadabilityDataset.__doc__)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T06:09:09.78135Z","iopub.execute_input":"2021-07-31T06:09:09.781681Z","iopub.status.idle":"2021-07-31T06:09:09.790054Z","shell.execute_reply.started":"2021-07-31T06:09:09.781645Z","shell.execute_reply":"2021-07-31T06:09:09.789102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=classes_model></a>\n## Model\n[[back to top]](#toc)","metadata":{}},{"cell_type":"markdown","source":"Note, that the concept of attention is awesomely explained in [Lena Voita](https://lena-voita.github.io/)'s excellent notebook [here](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html).","metadata":{}},{"cell_type":"code","source":"from torch import nn\n\nclass AttentionHead(nn.Module):\n    \"\"\"Class implementing the attention head of the model.\"\"\"\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n       \n    \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        \n        return torch.sum(context_vector, dim=1)\n\n\nprint(AttentionHead.__doc__)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T06:09:09.79235Z","iopub.execute_input":"2021-07-31T06:09:09.792906Z","iopub.status.idle":"2021-07-31T06:09:09.801447Z","shell.execute_reply.started":"2021-07-31T06:09:09.79287Z","shell.execute_reply":"2021-07-31T06:09:09.800514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom transformers import RobertaModel\nfrom transformers import RobertaConfig\n\nclass ReadabilityRobertaModel(nn.Module):\n    \"\"\"Custom model for the Readability task containing a Roberta layer and a custom NN head.\"\"\"\n        \n    def __init__(self):\n        super(ReadabilityRobertaModel, self).__init__()\n        \n        self.model_config = RobertaConfig.from_pretrained(config['best_pretrained_roberta_folder'])\n        self.model_config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        \n        self.roberta = RobertaModel.from_pretrained(config['best_pretrained_roberta_folder'],\n                                                    config=self.model_config)\n        self.attention_head = AttentionHead(self.model_config.hidden_size, \n                                            self.model_config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.regressor = nn.Linear(self.model_config.hidden_size, 1)\n        \n        \n    def forward(self, tokens, attention_mask):\n        x = self.roberta(input_ids=tokens, attention_mask=attention_mask)[0]\n        x = self.attention_head(x)\n        x = self.dropout(x)\n        x = self.regressor(x)\n        return x\n    \n    \n    def freeze_roberta(self):\n        \"\"\"\n        Freezes the parameters of the Roberta model so when ReadabilityRobertaModel is \n        trained only the wieghts of the custom regressor are modified.\n        \"\"\"\n        for param in self.roberta.named_parameters():\n            param[1].requires_grad=False\n    \n    def unfreeze_roberta(self):\n        \"\"\"\n        Unfreezes the parameters of the Roberta model so when ReadabilityRobertaModel is \n        trained both the wieghts of the custom regressor and of the underlying Roberta\n        model are modified.\n        \"\"\"\n        for param in self.roberta.named_parameters():\n            param[1].requires_grad=True\n\n    \nprint(ReadabilityRobertaModel.__doc__)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T06:09:09.803152Z","iopub.execute_input":"2021-07-31T06:09:09.803661Z","iopub.status.idle":"2021-07-31T06:09:14.616108Z","shell.execute_reply.started":"2021-07-31T06:09:09.803624Z","shell.execute_reply":"2021-07-31T06:09:14.615124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='make_predictions'></a>\n# Make predictions\n[[back to top]](#toc)","metadata":{}},{"cell_type":"markdown","source":"<a id='make_predictions_read_test_data'></a>\n## Read test data\n[[back to top]](#toc)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ntest_csv_path = '/kaggle/input/commonlitreadabilityprize/test.csv'\ntest_data = pd.read_csv(test_csv_path)\n\nprint('The total # of samples is {}.'.format(test_data.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T06:09:14.617586Z","iopub.execute_input":"2021-07-31T06:09:14.617926Z","iopub.status.idle":"2021-07-31T06:09:14.634839Z","shell.execute_reply.started":"2021-07-31T06:09:14.617889Z","shell.execute_reply":"2021-07-31T06:09:14.633698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='make_predictions_make_predictions'></a>\n## Make predictions with pretrained models\n[[back to top]](#toc)","metadata":{}},{"cell_type":"markdown","source":"Used [this notebook](https://www.kaggle.com/maunish/clrp-pytorch-roberta-inference) as a resource.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef get_dataloader_from_dataframes(df, tokenizer):\n    \"\"\"Converts a complete dataframe (with all columns included) into a dataloader.\"\"\"\n    texts = df['excerpt'].values.tolist()\n    data_encodings = tokenizer(texts, max_length=config['sentence_max_length'],\n                              truncation=True, padding=True)\n    dataset = ReadabilityDataset(data_encodings)\n    dataloader = DataLoader(dataset, batch_size=config['batch_size'])\n    \n    return dataloader\n\nprint(get_dataloader_from_dataframes.__doc__)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T06:09:14.636277Z","iopub.execute_input":"2021-07-31T06:09:14.636617Z","iopub.status.idle":"2021-07-31T06:09:14.643301Z","shell.execute_reply.started":"2021-07-31T06:09:14.636582Z","shell.execute_reply":"2021-07-31T06:09:14.642329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\nfrom tqdm.auto import tqdm\n\ntqdm.pandas()\n\n\ndef get_predictions(data, model_path, tokenizer):\n    \"\"\"Method which makes a prediction based on the provided model.\"\"\"\n    \n    # setup model\n    model = ReadabilityRobertaModel()\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n    \n    # convert data into dataloader\n    dataloader = get_dataloader_from_dataframes(data, tokenizer)\n    \n    # iteration for predictions\n    predictions = list()\n    for i, batch in enumerate(tqdm(dataloader)):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        output = torch.flatten(model(tokens=input_ids, attention_mask=attention_mask))\n        output = output.cpu().detach().numpy().tolist()\n        \n        predictions.extend(output)\n        \n    torch.cuda.empty_cache()\n    return predictions\n\nprint(get_predictions.__doc__)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T06:09:14.644898Z","iopub.execute_input":"2021-07-31T06:09:14.645285Z","iopub.status.idle":"2021-07-31T06:09:14.656704Z","shell.execute_reply.started":"2021-07-31T06:09:14.645239Z","shell.execute_reply":"2021-07-31T06:09:14.655528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom transformers import RobertaTokenizer\n\npredictions = np.zeros(test_data.shape[0])\n\nfor i in range(config['num_of_models']):\n    model_path = f'../input/roberta-large-k-fold-models/model{i}/model{i}.bin'\n    tokenizer_path = f'../input/roberta-large-k-fold-models/model{i}/'\n    tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n    \n    pred = get_predictions(test_data, model_path, tokenizer)\n    print(f\"The predictions from model {i}:\")\n    print(pred)\n    predictions = predictions + pred\n    \n    \npredictions = predictions / config['num_of_models']\npredictions = predictions.tolist()\nprint(\"Overall predictions:\")\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T06:09:14.659469Z","iopub.execute_input":"2021-07-31T06:09:14.660675Z","iopub.status.idle":"2021-07-31T06:11:05.643436Z","shell.execute_reply.started":"2021-07-31T06:09:14.660645Z","shell.execute_reply":"2021-07-31T06:11:05.642421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='make_predictions_save_results'></a>\n## Save results\n[[back to top]](#toc)","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = test_data['id']\nsubmission['target'] = predictions\nsubmission.to_csv('submission.csv', index=False)\n\nprint('Saved predictions.')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T06:11:05.645003Z","iopub.execute_input":"2021-07-31T06:11:05.645373Z","iopub.status.idle":"2021-07-31T06:11:05.665207Z","shell.execute_reply.started":"2021-07-31T06:11:05.645336Z","shell.execute_reply":"2021-07-31T06:11:05.664249Z"},"trusted":true},"execution_count":null,"outputs":[]}]}