{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id='introduction'></a>\n# Introduction\n\nIn this notebook I'm fine-tuning Hugging Face's Roberta Large model with the [CommonLit Readability Prize](https://www.kaggle.com/c/commonlitreadabilityprize) dataset. I follow Hugging Face's [relevant colab page](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling.ipynb) and Maunish's [guide](https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain/) for the process.\n\nThis notebook is part of a series:\n1. Pretrain roberta large on the CommonLit dataset. (this notebook)\n2. Produce k models which can later be used for determining the readability of texts [here](https://www.kaggle.com/angyalfold/roberta-large-k-fold-models).\n3. Make predictions with a custom NN regressor [here](https://www.kaggle.com/angyalfold/roberta-large-with-custom-regressor-pytorch/).\n4. Ensemble (Roberta large + SVR, Roberta large + Ridge, Roberta large + custom NN head) [here](https://www.kaggle.com/angyalfold/ensemble-for-commonlit/).\n\nTo run this notebook the datasets package needs to be installed.","metadata":{}},{"cell_type":"code","source":"!conda install -c huggingface -c conda-forge datasets -y","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:37:02.016529Z","iopub.execute_input":"2021-07-26T03:37:02.016872Z","iopub.status.idle":"2021-07-26T03:39:23.298614Z","shell.execute_reply.started":"2021-07-26T03:37:02.016821Z","shell.execute_reply":"2021-07-26T03:39:23.297512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='toc'></a>\n# Table of contents\n* [Introduction](#introduction)\n* [Overview](#overview)\n    * [The idea explained](#overview_idea)\n    * [Technicalities](#overview_technicalities)\n* [Parameters](#parameters)\n* [Prepare data](#prepare_data)\n* [Setup training](#setup_training)\n    * [Setup tokenzier](#setup_training_tokenizer)\n    * [Setup model](#setup_training_model)\n    * [Create datasets](#setup_training_datasets)\n    * [Create DataCollator](#setup_training_datacollator)\n    * [Training arguments](#setup_training_arguments)\n    * [Create trainer](#setup_training_trainer)\n* [Train](#train)","metadata":{}},{"cell_type":"markdown","source":"<a id='overview'></a>\n# Overview\n[[back to top]](#toc)\n\n<a id='overview_idea'></a>\n## The idea explained\n[[back to top]](#toc)\n\nThe aim of this notebook is to fine-tune Hugging Face's Roberta-large model on the dataset provided for the [CommonLit Readability Prize](https://www.kaggle.com/c/commonlitreadabilityprize) competition.\n\nEach language model is trained on a large amount of text data. For example, Roberta-large is among others trained on thousands of English language books and the English language Wikipedia (see Hugging Face's [related descreption](https://huggingface.co/roberta-large#training-data) for more details). This vast amount of data however, doesn't contain the texts which we want to work with (e.g.: the data for CommonLit Readability Prize in this case).\n\nThe expectation is that a model which is fine-tuned with the data on which the given NLP-task needs to be performed could yield better results compared to a model which previously didn't encounter with that data.\n\n<a id='overview_technicalities'></a>\n## Technicalities\n[[back to top]](#toc)\n\nFine-tuning can happen either by using *causal language modeling* (when the model needs to generate the next tokens provided the begining of a sentence for example) or by *masked language modelling* (when some tokens are missing from a text and the model needs to predict those missing tokens). These ideas are discussed in details on Hugging Face's relevant [collab page](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling.ipynb#scrollTo=JEA1ju653l-p).\n\nIn this notebook I'm using *masked language modelling*.","metadata":{}},{"cell_type":"markdown","source":"<a id='parameters'></a>\n# Parameters\n[[back to top]](#toc)","metadata":{}},{"cell_type":"code","source":"import transformers\n\nmodel_name = 'roberta-large' # the name of the model in Hugging Face\ncheckpoint_output = './clrp_roberta_large_chk'\noutput_path = './clrp_roberta_large' # the folder to which the tokenizer & models are saved","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:36:32.363295Z","iopub.execute_input":"2021-07-26T03:36:32.363552Z","iopub.status.idle":"2021-07-26T03:36:33.445467Z","shell.execute_reply.started":"2021-07-26T03:36:32.363524Z","shell.execute_reply":"2021-07-26T03:36:33.440783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='prepare_data'></a>\n# Prepare data\n[[back to top]](#toc)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ndf_test =  pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ndf_data = pd.concat([df_train, df_test])\n\ndf_data['excerpt'] = df_data['excerpt'].apply(lambda x: x.replace('\\n', ' '))\ntext_data = df_data['excerpt'].to_frame('excerpt')\n\nprint('Loaded and preprocessd {} entries.'.format(text_data.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:36:33.487166Z","iopub.status.idle":"2021-07-26T03:36:33.487706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='setup_training'></a>\n# Setup training\n[[back to top]](#toc)","metadata":{}},{"cell_type":"markdown","source":"<a id='setup_training_tokenizer'></a>\n## Setup tokenizer\n[[back to top]](#toc)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.save_pretrained(output_path)\n\nprint('Tokenizer for {} has successfully been saved.'.format(model_name))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:36:33.489364Z","iopub.status.idle":"2021-07-26T03:36:33.489921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='setup_training_model'></a>\n## Setup model\n[[back to top]](#toc)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForMaskedLM\n\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\n\nprint('Model {} has been intialized.'.format(model_name))","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:36:33.491326Z","iopub.status.idle":"2021-07-26T03:36:33.491949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='setup_training_datasets'></a>\n## Create datasets\n[[back to top]](#toc)\n\nUsed [this description](https://huggingface.co/docs/datasets/loading_datasets.html) from Hugging Face to create a dataset from pandas. (Note, that I needed to install the datasets package using conda because I encountered errors when I tried to install it with pip. I used this command: `conda install -c huggingface -c conda-forge datasets -y`)\n\n[This page](https://huggingface.co/docs/datasets/processing.html) describes how `train_test_split()` works.\n\nThe following snippet encodes the content of the dataset as described [here](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling.ipynb#scrollTo=5io6fY_d3l-u).","metadata":{}},{"cell_type":"code","source":"def encode_text(text_data):\n    return tokenizer(text_data['excerpt'])","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:36:33.493479Z","iopub.status.idle":"2021-07-26T03:36:33.494035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ndataset = Dataset.from_pandas(text_data)\ntokenized_dataset = dataset.map(encode_text, batched=True)\ntokenized_datasets = tokenized_dataset.train_test_split()\n\nprint('Setup dataset & performed train/test split.')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:36:33.495428Z","iopub.status.idle":"2021-07-26T03:36:33.495982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='setup_training_datacollator'></a>\n## Create DataCollator\n[[back to top]](#toc)\n\nData collators can create batches from the data. In the case of masking language, they are important, because they perform random masking on the created batches. See Hugging Face's related documentation [here](https://huggingface.co/transformers/main_classes/trainer.html#id1) & [here](https://huggingface.co/transformers/main_classes/data_collator.html).","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer)\n\nprint('Initialized data collator.')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:36:33.497368Z","iopub.status.idle":"2021-07-26T03:36:33.497928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='setup_training_arguments'></a>\n## Setup training arguments\n[[back to top]](#toc)","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=checkpoint_output,\n    overwrite_output_dir=True,\n    num_train_epochs=8,\n    evaluation_strategy='epoch',\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    report_to='none',\n    learning_rate=2e-5,\n    weight_decay=0.01\n)\n\nprint('Created training arguments.')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:36:33.499304Z","iopub.status.idle":"2021-07-26T03:36:33.49986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='setup_training_trainer'></a>\n## Create trainer\n[[back to top]](#toc)","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test']\n)\n\nprint('Setup trainer.')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:36:33.501255Z","iopub.status.idle":"2021-07-26T03:36:33.501863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='train'></a>\n# Train\n[[back to top]](#toc)","metadata":{}},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model(output_path + '/best_model/')\n\nprint('Trained & saved model.')","metadata":{"execution":{"iopub.status.busy":"2021-07-26T03:36:33.503727Z","iopub.status.idle":"2021-07-26T03:36:33.504416Z"},"trusted":true},"execution_count":null,"outputs":[]}]}