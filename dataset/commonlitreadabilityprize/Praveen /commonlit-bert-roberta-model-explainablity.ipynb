{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f7 ; color : #fe346e; text-align: center; border-radius: 100px 85;\">CommonLit Readability<br> BERT + Roberta + Model Explainabilty</h1>\n<br>","metadata":{}},{"cell_type":"code","source":"!pip install -q nlpretext loguru","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Required Data Loading and Util libraries ","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport copy\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\nfrom loguru import logger\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Required Modeling Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\nimport transformers\nfrom transformers import BertTokenizer, BertModel, BertConfig\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom scipy.stats import ks_2samp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLP Preprocessing Libraries","metadata":{}},{"cell_type":"code","source":"from nlpretext import Preprocessor\nfrom nlpretext.basic.preprocess import (normalize_whitespace, remove_punct, \n                                        remove_eol_characters, remove_stopwords, \n                                        lower_text, unpack_english_contractions)\n\nfrom nlpretext.social.preprocess import remove_html_tags","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Loading","metadata":{}},{"cell_type":"code","source":"%%time\ndf_train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ndf_test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Wrangling or Preprocessing","metadata":{}},{"cell_type":"code","source":"# Sample Text before preprocessing\n\ndf_train['excerpt'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## To know more details about nlpretext library \n\n[Link](https://medium.com/artefact-engineering-and-data-science/introducing-nlpretext-a8bb7c03df89)","metadata":{}},{"cell_type":"code","source":"preprocessor = Preprocessor()\npreprocessor.pipe(unpack_english_contractions)\npreprocessor.pipe(remove_eol_characters)\npreprocessor.pipe(lower_text)\npreprocessor.pipe(normalize_whitespace)\npreprocessor.pipe(remove_punct)\n#preprocessor.pipe(remove_stopwords(lang='en'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['excerpt'] = df_train['excerpt'].apply(preprocessor.run)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['excerpt'] = df_test['excerpt'].apply(preprocessor.run)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample Text after preprocessing\n\ndf_train['excerpt'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def max_words(df):\n    lengths = df['excerpt'].apply(lambda x: len(x.split()))\n    return max(lengths)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"excerpt_lenghts = df_train['excerpt'].apply(lambda x: len(x.split()))\nsns.histplot(excerpt_lenghts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max(excerpt_lenghts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation :- \n    For BERT , Its required to share the maximum words in the text ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    \n    max_len = 218\n    \n    train_batch_size = 8\n    valid_batch_size = 16\n    \n    epochs = 5\n    learning_rate = 1e-5\n    n_accumulate = 1\n    folds = 5\n    \n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sampling Strategy - KFOLD","metadata":{}},{"cell_type":"code","source":"def create_folds(df, n_s=5, n_grp=None):\n    df['Fold'] = -1\n    \n    if n_grp is None:\n        skf = KFold(n_splits=n_s)\n        target = df.target\n    else:\n        skf = StratifiedKFold(n_splits=n_s)\n        df['grp'] = pd.cut(df.target, n_grp, labels=False)\n        target = df.grp\n    \n    for fold_no, (t, v) in enumerate(skf.split(target, target)):\n        df.loc[v, 'Fold'] = fold_no\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = create_folds(df_train, n_s=CFG.folds, n_grp=100)\ndf_train['Fold'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let See How the Taget is distribured accross folds","metadata":{}},{"cell_type":"code","source":"def viz_tgt_folds(df):\n    fig, axs = plt.subplots(1, 5, sharex=True, sharey=True, figsize=(10,4))\n    for i, ax in enumerate(axs):\n        ax.hist(df[df.Fold == i]['target'], bins=100, density=True, label=f'Fold-{i}')\n        if i == 0:\n            ax.set_ylabel('Frequency')\n        if i == 2:\n            ax.set_xlabel(\"Target\")\n        ax.legend(frameon=False, handlelength=0)\n    plt.tight_layout()\n    plt.show()\n    \n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"viz_tgt_folds(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation :- \n#### Target distribution accross the folds looks normal and evenly distributed","metadata":{}},{"cell_type":"markdown","source":"## Ensuring the distribution of Folds\n\nWe can compare any two folds with the **Kolmogorov-Smirnov** test to examine if the folds come from the same distribution. Let's compare all folds with the 1st fold for simplicity. The test results are given below. Indeed, the low KS (~0.0008) and high probability (1.0) values confirm that all folds come from the same distribution.","metadata":{}},{"cell_type":"code","source":"for fold in np.sort(df.Fold.unique())[1:]:\n    print(f'Fold 0 vs {fold}:', ks_2samp(df.loc[df.Fold==0,'target'], df.loc[df.Fold==fold,'target']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observation :- \n#### We Can Statisticaly say that target distribution even where as PValue approaching 1 and Statistic is less than zero","metadata":{}},{"cell_type":"markdown","source":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f7 ; color : #fe346e; text-align: center; border-radius: 100px 85;\">CommonLit Readability<br> BERT Model</h1>\n<br>","metadata":{}},{"cell_type":"markdown","source":"## Preparing Torch dataset for BERT ","metadata":{}},{"cell_type":"code","source":"class BERTDset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.text = df['excerpt'].values\n        self.target = df['target'].values\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'target': torch.tensor(self.target[index], dtype=torch.float)\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss Function","metadata":{}},{"cell_type":"code","source":"def eval_metrics(outputs, targets):\n    return torch.sqrt(nn.MSELoss()(outputs, targets))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTClass(nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.bert(ids, attention_mask = mask, \n                              token_type_ids = token_type_ids, \n                              return_dict=False)\n        output = self.dropout(output)\n        output = self.fc(output)\n        return output\n\nmodel = BERTClass()\nmodel.to(CFG.device);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f7 ; color : #fe346e; text-align: center; border-radius: 100px 85;\">BERT Model - Training & Validation </h1>\n<br>","metadata":{}},{"cell_type":"code","source":"def training(model,optimizer,scheduler,dataloader,device,epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype = torch.float)\n        \n        batch_size = ids.size(0)\n        \n        with amp.autocast(enabled=True):\n            outputs = model(ids, mask, token_type_ids)\n            loss = eval_metrics(outputs, targets)\n            loss = loss / CFG.n_accumulate\n            \n        scaler.scale(loss).backward()\n        \n        if (step + 1) % CFG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            \n            # zero the parameter gradients\n            optimizer.zero_grad()\n            \n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss/dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inferance(model,optimizer,scheduler,dataloader,device,epoch):\n    \n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype = torch.float)\n        \n        batch_size = ids.size(0)\n        \n        outputs = model(ids, mask, token_type_ids)\n        loss = eval_metrics(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss/dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    gc.collect()\n    \n    return epoch_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"@logger.catch\ndef run(model, optimizer, scheduler, device, num_epochs):    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = training(model, optimizer, scheduler, dataloader=train_loader, \n                                           device=CFG.device, epoch=epoch)\n        \n        valid_epoch_loss = inferance(model, optimizer, scheduler, dataloader=valid_loader, \n                                           device=CFG.device, epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(valid_epoch_loss)\n        \n        # deep copy the model\n        if valid_epoch_loss <= best_loss:\n            print(f\"Validation Loss Improved ({best_loss} ---> {valid_epoch_loss})\")\n            best_loss = valid_epoch_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"Loss{:.4f}_epoch{:.0f}.bin\".format(best_loss, epoch)\n            torch.save(model.state_dict(), PATH)\n            print(\"Model Saved\")\n    \n    print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_loss))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(df,fold):\n    \n    df_train = df[df.Fold == fold].reset_index(drop=True)\n    df_valid = df[df.Fold == fold].reset_index(drop=True)\n    \n    train_dataset = BERTDset(df_train, CFG.tokenizer, CFG.max_len)\n    valid_dataset = BERTDset(df_valid, CFG.tokenizer, CFG.max_len)\n\n    train_loader = DataLoader(train_dataset, batch_size=CFG.train_batch_size, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_batch_size, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader, valid_loader = prepare_data(df_train,fold=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in valid_loader:\n    print(x)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining Optimizer with weight decay to params other than bias and layer norms\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0}\n    ]  \n\noptimizer = AdamW(optimizer_parameters, lr=CFG.learning_rate)\n\n#Defining LR Scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=len(train_loader)*CFG.epochs\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lrs = []\nfor epoch in range(1, CFG.epochs + 1):\n    scheduler.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nplt.plot(lrs);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, history = run(model, optimizer, scheduler, device=CFG.device, num_epochs=CFG.epochs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(22,8))\nplt.plot(history['Train Loss'], label='Train Loss')\nplt.plot(history['Valid Loss'], label='Valid Loss')\nplt.legend()\nplt.title('Loss Curve');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ... In progress ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}