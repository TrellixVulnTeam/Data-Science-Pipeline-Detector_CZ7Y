{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-25T17:40:42.506235Z","iopub.execute_input":"2021-07-25T17:40:42.506614Z","iopub.status.idle":"2021-07-25T17:40:42.529641Z","shell.execute_reply.started":"2021-07-25T17:40:42.506582Z","shell.execute_reply":"2021-07-25T17:40:42.528235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:40:44.783747Z","iopub.execute_input":"2021-07-25T17:40:44.78412Z","iopub.status.idle":"2021-07-25T17:40:45.033195Z","shell.execute_reply.started":"2021-07-25T17:40:44.784087Z","shell.execute_reply":"2021-07-25T17:40:45.032364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Null check\ndata.isnull().sum()/len(data) * 100","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:40:46.637719Z","iopub.execute_input":"2021-07-25T17:40:46.638405Z","iopub.status.idle":"2021-07-25T17:40:46.670273Z","shell.execute_reply.started":"2021-07-25T17:40:46.638347Z","shell.execute_reply":"2021-07-25T17:40:46.669154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(['url_legal', 'license'], 1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:40:47.04771Z","iopub.execute_input":"2021-07-25T17:40:47.048385Z","iopub.status.idle":"2021-07-25T17:40:47.058649Z","shell.execute_reply.started":"2021-07-25T17:40:47.048337Z","shell.execute_reply":"2021-07-25T17:40:47.057209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.parsing.preprocessing import remove_stopwords\n\ndocs = data['excerpt'].str.lower().str.replace('[^a-z\\s]', '')\n\ndocs = docs.apply(remove_stopwords)\ndocs[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:40:49.555779Z","iopub.execute_input":"2021-07-25T17:40:49.556247Z","iopub.status.idle":"2021-07-25T17:40:51.212811Z","shell.execute_reply.started":"2021-07-25T17:40:49.556211Z","shell.execute_reply":"2021-07-25T17:40:51.211814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer()\n\ntokenizer.fit_on_texts(docs)\nvocab = list(tokenizer.word_index)\n\nprint('Total number of unique tokens in corpus: %d' % len(vocab))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:40:52.526722Z","iopub.execute_input":"2021-07-25T17:40:52.527121Z","iopub.status.idle":"2021-07-25T17:40:59.592708Z","shell.execute_reply.started":"2021-07-25T17:40:52.52708Z","shell.execute_reply":"2021-07-25T17:40:59.591538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zip_path = '/kaggle/input/quora-insincere-questions-classification/embeddings.zip'\nfrom zipfile import ZipFile\nzf = ZipFile(zip_path)\nzf.filelist","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:34:53.351785Z","iopub.execute_input":"2021-07-25T15:34:53.352228Z","iopub.status.idle":"2021-07-25T15:34:53.369563Z","shell.execute_reply.started":"2021-07-25T15:34:53.352194Z","shell.execute_reply":"2021-07-25T15:34:53.368485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Glove Embedding Layer","metadata":{}},{"cell_type":"code","source":"glove_path = 'glove.840B.300d/glove.840B.300d.txt'\ncount = 0\nwith zf.open(glove_path) as file:\n    embeddings_glove = {}\n    for line in file:\n        line = line.decode('utf-8').replace('\\n', '').split(' ')\n        curr_word = line[0]\n        if curr_word in vocab:\n            vector = line[1:]\n            vector = np.array(vector).astype(float)\n            embeddings_glove[curr_word] = vector","metadata":{"execution":{"iopub.status.busy":"2021-07-25T15:35:13.104901Z","iopub.execute_input":"2021-07-25T15:35:13.105548Z","iopub.status.idle":"2021-07-25T15:57:03.857684Z","shell.execute_reply.started":"2021-07-25T15:35:13.105498Z","shell.execute_reply":"2021-07-25T15:57:03.856657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocab) + 1\nembedding_dim = 300\nwords_not_available = []\n\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, wid in tokenizer.word_index.items():\n    if word in embeddings_glove:\n        embedding_matrix[wid] = embeddings_glove[word]\n    else:\n        words_not_available.append(word)\n        \nprint('Percentage of words not avaialable %.2f%%' % (len(words_not_available)/len(vocab)*100))\nprint('Percentage of words avaialable %.2f%%' % (100 - len(words_not_available)/len(vocab)*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T16:03:11.823493Z","iopub.execute_input":"2021-07-25T16:03:11.823928Z","iopub.status.idle":"2021-07-25T16:03:11.974707Z","shell.execute_reply.started":"2021-07-25T16:03:11.823888Z","shell.execute_reply":"2021-07-25T16:03:11.973207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x_seq = tokenizer.texts_to_sequences(docs)\n\nmax_doc_len = 115\ntrain_x_padded = pad_sequences(train_x_seq, padding='post', maxlen=max_doc_len)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:42:15.966908Z","iopub.execute_input":"2021-07-25T17:42:15.96758Z","iopub.status.idle":"2021-07-25T17:42:16.233139Z","shell.execute_reply.started":"2021-07-25T17:42:15.967527Z","shell.execute_reply":"2021-07-25T17:42:16.232154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using the word embeddings which has the maximum wordâ€™s coverage, create a regressor using simple neural network","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n\nmodel = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim,\n                          weights = [embedding_matrix],\n                          input_length=max_doc_len,\n                          trainable=False))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer='sgd', loss='mse', metrics=[\"mae\"])\nhistory = model.fit(train_x_padded, data['target'], epochs=10, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T16:31:46.215117Z","iopub.execute_input":"2021-07-25T16:31:46.215471Z","iopub.status.idle":"2021-07-25T16:31:54.690978Z","shell.execute_reply.started":"2021-07-25T16:31:46.215439Z","shell.execute_reply":"2021-07-25T16:31:54.689967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\ntest_docs = test_df['excerpt'].str.lower().str.replace('[^a-z\\s]', '')\n\ntest_docs = test_docs.apply(remove_stopwords)\n\ntest_x_seq = tokenizer.texts_to_sequences(test_docs)\n\ntest_x_padded = pad_sequences(test_x_seq, padding='post', maxlen=max_doc_len)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:42:20.496892Z","iopub.execute_input":"2021-07-25T17:42:20.497719Z","iopub.status.idle":"2021-07-25T17:42:20.524267Z","shell.execute_reply.started":"2021-07-25T17:42:20.497658Z","shell.execute_reply":"2021-07-25T17:42:20.523115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_y_pred = model.predict(test_x_padded)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T16:32:54.027166Z","iopub.execute_input":"2021-07-25T16:32:54.027558Z","iopub.status.idle":"2021-07-25T16:32:54.151699Z","shell.execute_reply.started":"2021-07-25T16:32:54.027523Z","shell.execute_reply":"2021-07-25T16:32:54.150707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/sample_submission.csv')\nsubmission_df['target'] = test_y_pred\nsubmission_df.to_csv(\"submission_glove_embedding.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-25T16:34:13.959388Z","iopub.execute_input":"2021-07-25T16:34:13.959737Z","iopub.status.idle":"2021-07-25T16:34:13.979199Z","shell.execute_reply.started":"2021-07-25T16:34:13.959708Z","shell.execute_reply":"2021-07-25T16:34:13.97767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Google News Embedding Layer","metadata":{}},{"cell_type":"code","source":"from gensim.models import KeyedVectors\n\nembedding_file = 'GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\nembeddings = KeyedVectors.load_word2vec_format(zf.open(embedding_file), binary=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T16:35:46.388052Z","iopub.execute_input":"2021-07-25T16:35:46.388556Z","iopub.status.idle":"2021-07-25T16:37:15.216056Z","shell.execute_reply.started":"2021-07-25T16:35:46.388521Z","shell.execute_reply":"2021-07-25T16:37:15.214843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocab) + 1\nembedding_dim = 300\nwords_not_available = []\n\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, wid in tokenizer.word_index.items():\n    if word in embeddings:\n        embedding_matrix[wid] = embeddings[word]\n    else:\n        words_not_available.append(word)\n        \nprint('Percentage of words not avaialable %.2f%%' % (len(words_not_available)/len(vocab)*100))\nprint('Percentage of words avaialable %.2f%%' % (100 - len(words_not_available)/len(vocab)*100))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T16:37:15.218136Z","iopub.execute_input":"2021-07-25T16:37:15.218579Z","iopub.status.idle":"2021-07-25T16:37:15.461195Z","shell.execute_reply.started":"2021-07-25T16:37:15.218543Z","shell.execute_reply":"2021-07-25T16:37:15.460137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim,\n                          weights = [embedding_matrix],\n                          input_length=max_doc_len,\n                          trainable=False))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer='sgd', loss='mse', metrics=[\"mae\"])\nhistory = model.fit(train_x_padded, data['target'], epochs=10, verbose=1)\n\ntest_y_pred = model.predict(test_x_padded)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T16:37:49.082373Z","iopub.execute_input":"2021-07-25T16:37:49.082951Z","iopub.status.idle":"2021-07-25T16:37:56.564516Z","shell.execute_reply.started":"2021-07-25T16:37:49.082904Z","shell.execute_reply":"2021-07-25T16:37:56.563581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['target'] = test_y_pred\nsubmission_df.to_csv(\"submission_google_embedding.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T16:38:19.886704Z","iopub.execute_input":"2021-07-25T16:38:19.88707Z","iopub.status.idle":"2021-07-25T16:38:19.893326Z","shell.execute_reply.started":"2021-07-25T16:38:19.887039Z","shell.execute_reply":"2021-07-25T16:38:19.892546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build custom word embeddings using genism word2vec model (with window size=5) and retrain the neural network","metadata":{}},{"cell_type":"code","source":"from gensim.models import word2vec\n\ndocs_words = [doc.split(' ') for doc in docs]\nlen(docs_words)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 100 \nmodel = word2vec.Word2Vec(sentences =docs_words, vector_size=embedding_dim, min_count=50, window=5, sg=1)\nvocab = model.wv.index_to_key\ndf_embedding_matrix = pd.DataFrame(model.wv[vocab], index=vocab)\ndf_embedding_matrix.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim,\n                          input_length=max_doc_len,\n                          trainable=True))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer='adam', loss='mse', metrics=[\"mae\"])\nhistory = model.fit(train_x_padded, data['target'], epochs=25, verbose=1)\n\ntest_y_pred = model.predict(test_x_padded)\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keras Embedding Layer","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n\nvocab_size = len(vocab) + 1\nembedding_dim = 300\n\nmodel = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim,\n                          input_length=max_doc_len,\n                          trainable=True))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer='adam', loss='mse', metrics=[\"mae\"])\nhistory = model.fit(train_x_padded, data['target'], epochs=10, verbose=1)\n\ntest_y_pred = model.predict(test_x_padded)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-25T17:42:52.339705Z","iopub.execute_input":"2021-07-25T17:42:52.340355Z","iopub.status.idle":"2021-07-25T17:44:55.44963Z","shell.execute_reply.started":"2021-07-25T17:42:52.340295Z","shell.execute_reply":"2021-07-25T17:44:55.448303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['target'] = test_y_pred\nsubmission_df.to_csv(\"submission_keras_embedding.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x_seq = tokenizer.texts_to_sequences(docs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs_size = []\nfor doc in train_x_seq:\n    size = len(doc)\n    docs_size.append(size)\npd.Series(docs_size).plot.box()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_doc_len = 115\n\ntrain_x_padded = pad_sequences(train_x_seq, padding='post', maxlen=max_doc_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(train_x_padded[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n\nvocab_size = len(vocab) + 1\nembedding_dim = 300\n\nmodel = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim,\n                          input_length=max_doc_len,\n                          trainable=True))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer='adam', loss='mse', metrics=[\"mae\"])\nhistory = model.fit(train_x_padded, data['target'], epochs=25, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sgd optimizer\n# loss: 0.3717 - mae: 0.5002\n\n# rmsprop\n# loss: 0.0658 - mae: 0.2122\n\n# adam\n# loss: 0.0594 - mae: 0.1468  # Finalized","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\ntest_data = test_data[['id', 'excerpt']]\n\ntest_docs = test_data['excerpt'].str.lower().str.replace('[^a-z\\s]', '')\n\ntest_docs = test_docs.apply(remove_stopwords)\n\ntest_x_seq = tokenizer.texts_to_sequences(test_docs)\ntest_x_padded = pad_sequences(test_x_seq, padding='post', maxlen=max_doc_len)\ntest_docs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(test_x_padded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_y_pred = model.predict(test_x_padded)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/sample_submission.csv')\nsubmission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df['target'] = test_y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}