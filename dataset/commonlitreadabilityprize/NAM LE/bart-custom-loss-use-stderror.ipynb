{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os, random, sys, time, re\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.utils.data as D\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"597eea9d6295671bd36a809b579d12777738a392","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import *\n# from transformers import (DistilBertTokenizer, DistilBertForSequenceClassification,\n#                           AlbertTokenizer, AlbertForSequenceClassification,\n#                           RobertaTokenizer, RobertaForSequenceClassification,\n#                           ElectraTokenizer, ElectraForSequenceClassification,\n#                          CamembertTokenizer, CamembertForSequenceClassification)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"../input/commonlitreadabilityprize/\"\n\n# MODEL_PATH = '../input/distilbertbaseuncased'\n# MODEL_PATH = '../input/pretrained-albert-pytorch/albert-base-v2'\n# MODEL_PATH = '../input/camembertbasesquadfrfquadpiaf/camembert-base-squadFR-fquad-piaf'\n# MODEL_PATH = '../input/roberta-transformers-pytorch/distilroberta-base'\n# MODEL_PATH = '../input/roberta-transformers-pytorch/roberta-base'\nMODEL_PATH = '../input/bart-models-hugging-face-model-repository/bart-base'\n# MODEL_PATH = '../input/electra-base'\nVOCAB_PATH = '../input/roberta-transformers-pytorch/roberta-base' \n# MODEL_PATH\n\nN_FOLDS = 4\nEPOCHES = 2\nBATCH_SIZE = 12\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nMAX_SEQUENCE_LENGTH = 256\nLR = 2.5e-5\nget_tokenizer = RobertaTokenizer\nget_model = BartForSequenceClassification\n\n\n# error log\nsys.stderr = open('err.txt', 'w')","metadata":{"_uuid":"b7c04a817b5ba68498dc9b30638605da891ba6c4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 7117\nrandom.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"_uuid":"6ddd98901408dbe7c7fb9efdb0ec17cecd511864","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'), index_col='id')\ntest_csv = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'), index_col='id')\n# print(\"before filter\")\n# print(len(train_csv))\n# train_csv['standard_error'].hist(bins=12)\n# train_csv = train_csv[abs(train_csv['standard_error'])<0.6]\n# print(\"after filter\")\n# print(len(train_csv))\n\nsubm = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'), index_col='id')\n\ny = (train_csv.target.values > 0).astype(int)\ncv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = get_tokenizer.from_pretrained(VOCAB_PATH,\n                   model_max_length=MAX_SEQUENCE_LENGTH)\ntrain_csv['token'] = train_csv.excerpt.apply(tokenizer)\ntest_csv['token'] = test_csv.excerpt.apply(tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitDataset(D.Dataset):\n    \n    def __init__(self, token, target,standard_error = None ,training = False):\n        self.token = token\n        self.target = target\n        self.training = training\n        if(training):\n            self.standard_error = standard_error\n    def __len__(self):\n        return self.token.shape[0]\n\n    def __getitem__(self, idx):\n        if self.training:\n            return torch.tensor(self.token[idx].input_ids), \\\n                torch.tensor(self.token[idx].attention_mask),\\\n                self.target[idx], self.standard_error[idx]\n    \n        return torch.tensor(self.token[idx].input_ids), \\\n                torch.tensor(self.token[idx].attention_mask), self.target[idx]\n    \ndef collate_fn(batch):\n    ids, attns, targets, errors = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True).to(DEVICE)\n    attns = pad_sequence(attns, batch_first=True).to(DEVICE)\n    targets = torch.tensor(targets).float().to(DEVICE)\n    errors = torch.tensor(errors).float().to(DEVICE)\n    return ids, attns, targets, errors\n\ndef collate_fn_test(batch):\n    ids, attns, idxs = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True).to(DEVICE)\n    attns = pad_sequence(attns, batch_first=True).to(DEVICE)\n    return idxs, ids, attns","metadata":{"_uuid":"92ea577a18faedfdccf5198d626c5c27861133ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = LitDataset(train_csv.token, train_csv.target,\n                standard_error = train_csv.standard_error,training = True)\ntest_ds = LitDataset(test_csv.token, test_csv.index)\n\ntloader = D.DataLoader(test_ds, batch_size=BATCH_SIZE,\n                       shuffle=False, collate_fn = collate_fn_test, num_workers=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Table for results\nheader = r'''\n            Train         Validation\nEpoch |  MSE  |  RMSE |  MSE  |  RMSE | Time, m\n'''\n#          Epoch         metrics            time\nraw_line = '{:6d}' + '\\u2502{:7.3f}'*4 + '\\u2502{:6.2f}'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l_params = 0.1\nleaky_func = nn.LeakyReLU(l_params)\ndef loss_with_error(output,target,error):\n    \n    return torch.mean(leaky_func((output-target)**2-error**2/4)+l_params*(error**2/4))/2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n@torch.no_grad()\ndef validation_fn(model, loader, loss_fn):\n    tloss = []\n    for texts, attns, target, error in loader:\n        outputs = model(texts, attention_mask=attns)\n        loss = loss_fn(outputs.logits.squeeze(-1), target)\n        tloss.append(loss.item())\n    tloss = np.array(tloss).mean()\n    return tloss\n\ndef oof_preds(ds, tloader, cv, y, epochs = EPOCHES):\n    \n    loss_fn = loss_with_error\n    display_loss = torch.nn.MSELoss()\n    model_id = 0\n    for train_idx, valid_idx in cv.split(range(len(ds)), y):\n        \n        train_ds = D.Subset(ds, train_idx)\n        loader = D.DataLoader(train_ds, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn = collate_fn,num_workers=0)\n        \n        valid_ds = D.Subset(ds, valid_idx)\n        vloader = D.DataLoader(valid_ds, batch_size=BATCH_SIZE,\n                      shuffle=False, collate_fn = collate_fn,num_workers=0)\n        \n        model = get_model.from_pretrained( \n                          MODEL_PATH, num_labels=1).to(DEVICE);\n       \n        optimizer = optim.AdamW(model.parameters(), LR,\n                                betas=(0.9, 0.999), weight_decay=1e-1)\n        scheduler = get_constant_schedule_with_warmup(optimizer, 35)\n        print(header)\n        for epoch in range(1, epochs+1):      \n            start_time = time.time()\n            tloss = []          \n            model.train()\n            \n            for texts, attns, target,error in loader:\n                optimizer.zero_grad()\n                outputs = model(texts, attention_mask=attns)\n                gloss = loss_fn(outputs.logits.squeeze(-1), target, error)\n                loss = display_loss(outputs.logits.squeeze(-1), target)\n                tloss.append(loss.item())\n                gloss.backward()\n                optimizer.step()\n                scheduler.step()\n            tloss = np.array(tloss).mean()\n            vloss = validation_fn(model, vloader, display_loss)\n            tmetric = tloss**.5\n            vmetric = vloss**.5\n            print(raw_line.format(epoch,tloss,tmetric,vloss,vmetric,(time.time()-start_time)/60**1))\n            del loss, outputs\n            \n        model.eval();\n        # Get prediction for test set\n        ids, preds = [], [] \n        with torch.no_grad():\n            for batch_ids, texts, attn in tloader:\n                outputs = model(texts, attention_mask=attn)\n                ids += batch_ids\n                preds.append(outputs.logits.detach().squeeze(-1).cpu().numpy())\n            \n        # Save prediction of test set\n        preds = np.concatenate(preds)\n        subm.loc[ids, 'target']  =  subm.loc[ids, 'target'].values + preds / N_FOLDS\n        \n        output_dir = \"./pretrained_bart_{}/\".format(model_id)\n        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(output_dir)\n        tokenizer.save_pretrained(output_dir)\n        model_id += 1\n        del model, vloader, loader, train_ds, valid_ds\n        torch.cuda.empty_cache()","metadata":{"_uuid":"e566ecdb5f44cf32af12127f073021566d6e66dd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_preds(ds, tloader, cv, y, epochs = EPOCHES)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm.to_csv('submission.csv')","metadata":{"_uuid":"7c6a41bd21bf783455fd11b63745ed454e0413f0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}