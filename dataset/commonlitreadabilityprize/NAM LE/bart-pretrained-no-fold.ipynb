{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os, random, sys, time, re\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.utils.data as D\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom sklearn.model_selection import StratifiedKFold, KFold,train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"597eea9d6295671bd36a809b579d12777738a392","execution":{"iopub.status.busy":"2021-05-23T09:05:38.903573Z","iopub.execute_input":"2021-05-23T09:05:38.90396Z","iopub.status.idle":"2021-05-23T09:05:40.769783Z","shell.execute_reply.started":"2021-05-23T09:05:38.90388Z","shell.execute_reply":"2021-05-23T09:05:40.768933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import *\n# from transformers import (DistilBertTokenizer, DistilBertForSequenceClassification,\n#                           AlbertTokenizer, AlbertForSequenceClassification,\n#                           RobertaTokenizer, RobertaForSequenceClassification,\n#                           ElectraTokenizer, ElectraForSequenceClassification,\n#                          CamembertTokenizer, CamembertForSequenceClassification)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:05:40.771272Z","iopub.execute_input":"2021-05-23T09:05:40.771612Z","iopub.status.idle":"2021-05-23T09:05:46.821291Z","shell.execute_reply.started":"2021-05-23T09:05:40.771574Z","shell.execute_reply":"2021-05-23T09:05:46.820269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"../input/commonlitreadabilityprize/\"\n\n# MODEL_PATH = '../input/distilbertbaseuncased'\n# MODEL_PATH = '../input/pretrained-albert-pytorch/albert-base-v2'\n# MODEL_PATH = '../input/camembertbasesquadfrfquadpiaf/camembert-base-squadFR-fquad-piaf'\n# MODEL_PATH = '../input/roberta-transformers-pytorch/distilroberta-base'\n# MODEL_PATH = '../input/roberta-transformers-pytorch/roberta-base'\nMODEL_PATH = '../input/bart-models-hugging-face-model-repository/bart-base'\n# MODEL_PATH = '../input/electra-base'\nVOCAB_PATH = '../input/roberta-transformers-pytorch/roberta-base' \n# MODEL_PATH\n\nN_FOLDS = 4\nEPOCHES = 4\nBATCH_SIZE = 12\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nMAX_SEQUENCE_LENGTH = 256\nLR = 2.5e-5\nget_tokenizer = RobertaTokenizer\nget_model = BartForSequenceClassification\n\n\n# error log\nsys.stderr = open('err.txt', 'w')","metadata":{"_uuid":"b7c04a817b5ba68498dc9b30638605da891ba6c4","execution":{"iopub.status.busy":"2021-05-23T09:05:46.824987Z","iopub.execute_input":"2021-05-23T09:05:46.825257Z","iopub.status.idle":"2021-05-23T09:05:46.889342Z","shell.execute_reply.started":"2021-05-23T09:05:46.825227Z","shell.execute_reply":"2021-05-23T09:05:46.888664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 7117\nrandom.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"_uuid":"6ddd98901408dbe7c7fb9efdb0ec17cecd511864","execution":{"iopub.status.busy":"2021-05-23T09:05:46.891732Z","iopub.execute_input":"2021-05-23T09:05:46.8923Z","iopub.status.idle":"2021-05-23T09:05:46.900256Z","shell.execute_reply.started":"2021-05-23T09:05:46.892259Z","shell.execute_reply":"2021-05-23T09:05:46.899658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'), index_col='id')\ntest_csv = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'), index_col='id')\n# print(\"before filter\")\n# print(len(train_csv))\n# train_csv['standard_error'].hist(bins=12)\n# train_csv = train_csv[abs(train_csv['standard_error'])<0.6]\n# print(\"after filter\")\n# print(len(train_csv))\n\nsubm = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'), index_col='id')\n\ny = (train_csv.target.values > 0).astype(int)\ncv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:05:46.901636Z","iopub.execute_input":"2021-05-23T09:05:46.902226Z","iopub.status.idle":"2021-05-23T09:05:47.022391Z","shell.execute_reply.started":"2021-05-23T09:05:46.902186Z","shell.execute_reply":"2021-05-23T09:05:47.021757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = get_tokenizer.from_pretrained(VOCAB_PATH,\n                   model_max_length=MAX_SEQUENCE_LENGTH)\ntrain_csv['token'] = train_csv.excerpt.apply(tokenizer)\ntest_csv['token'] = test_csv.excerpt.apply(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:05:47.023645Z","iopub.execute_input":"2021-05-23T09:05:47.024Z","iopub.status.idle":"2021-05-23T09:05:52.355944Z","shell.execute_reply.started":"2021-05-23T09:05:47.023962Z","shell.execute_reply":"2021-05-23T09:05:52.35527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitDataset(D.Dataset):\n    \n    def __init__(self, token, target,standard_error = None ,training = False):\n        self.token = token\n        self.target = target\n        self.training = training\n        if(training):\n            self.standard_error = standard_error\n    def __len__(self):\n        return self.token.shape[0]\n\n    def __getitem__(self, idx):\n        if self.training:\n            return torch.tensor(self.token[idx].input_ids), \\\n                torch.tensor(self.token[idx].attention_mask),\\\n                self.target[idx], self.standard_error[idx]\n    \n        return torch.tensor(self.token[idx].input_ids), \\\n                torch.tensor(self.token[idx].attention_mask), self.target[idx]\n    \ndef collate_fn(batch):\n    ids, attns, targets, errors = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True).to(DEVICE)\n    attns = pad_sequence(attns, batch_first=True).to(DEVICE)\n    targets = torch.tensor(targets).float().to(DEVICE)\n    errors = torch.tensor(errors).float().to(DEVICE)\n    return ids, attns, targets, errors\n\ndef collate_fn_test(batch):\n    ids, attns, idxs = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True).to(DEVICE)\n    attns = pad_sequence(attns, batch_first=True).to(DEVICE)\n    return idxs, ids, attns","metadata":{"_uuid":"92ea577a18faedfdccf5198d626c5c27861133ee","execution":{"iopub.status.busy":"2021-05-23T09:05:52.357189Z","iopub.execute_input":"2021-05-23T09:05:52.357528Z","iopub.status.idle":"2021-05-23T09:05:52.368381Z","shell.execute_reply.started":"2021-05-23T09:05:52.357494Z","shell.execute_reply":"2021-05-23T09:05:52.367603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = LitDataset(train_csv.token, train_csv.target,\n                standard_error = train_csv.standard_error,training = True)\ntest_ds = LitDataset(test_csv.token, test_csv.index)\n\ntloader = D.DataLoader(test_ds, batch_size=BATCH_SIZE,\n                       shuffle=False, collate_fn = collate_fn_test, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:05:52.370987Z","iopub.execute_input":"2021-05-23T09:05:52.371523Z","iopub.status.idle":"2021-05-23T09:05:52.384305Z","shell.execute_reply.started":"2021-05-23T09:05:52.371483Z","shell.execute_reply":"2021-05-23T09:05:52.383726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Table for results\nheader = r'''\n            Train         Validation\nEpoch |  MSE  |  RMSE |  MSE  |  RMSE | Time, m\n'''\n#          Epoch         metrics            time\nraw_line = '{:6d}' + '\\u2502{:7.3f}'*4 + '\\u2502{:6.2f}'","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:05:52.386043Z","iopub.execute_input":"2021-05-23T09:05:52.386391Z","iopub.status.idle":"2021-05-23T09:05:52.393717Z","shell.execute_reply.started":"2021-05-23T09:05:52.386355Z","shell.execute_reply":"2021-05-23T09:05:52.393024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l_params = 0.1\nleaky_func = nn.LeakyReLU(l_params)\ndef loss_with_error(output,target,error):\n    \n    return torch.mean(leaky_func((output-target)**2-error**2/4)+l_params*(error**2/4))/2\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:05:52.396092Z","iopub.execute_input":"2021-05-23T09:05:52.396337Z","iopub.status.idle":"2021-05-23T09:05:52.403619Z","shell.execute_reply.started":"2021-05-23T09:05:52.396313Z","shell.execute_reply":"2021-05-23T09:05:52.403014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n@torch.no_grad()\ndef validation_fn(model, loader, loss_fn):\n    tloss = []\n    for texts, attns, target, error in loader:\n        outputs = model(texts, attention_mask=attns)\n        loss = loss_fn(outputs.logits.squeeze(-1), target)\n        tloss.append(loss.item())\n    tloss = np.array(tloss).mean()\n    return tloss\n\ndef oof_vals(ds, tloader, cv, y, epochs = EPOCHES):\n    \n    loss_fn = loss_with_error\n    display_loss = torch.nn.MSELoss()\n    model_id = 0\n    train_idx,val_idx = train_test_split(range(len(ds)),test_size=0.2)\n        \n    train_ds = D.Subset(ds, train_idx)\n    loader = D.DataLoader(train_ds, batch_size=BATCH_SIZE,\n                          shuffle=True, collate_fn = collate_fn,num_workers=0)\n\n    valid_ds = D.Subset(ds, val_idx)\n    vloader = D.DataLoader(valid_ds, batch_size=BATCH_SIZE,\n                  shuffle=False, collate_fn = collate_fn,num_workers=0)\n\n    model = get_model.from_pretrained( \n                      MODEL_PATH, num_labels=1).to(DEVICE);\n    for name,param in model.named_parameters():\n#         print(name)\n        arr = name.split(\".\")\n        if(name==\"model.encoder.embed_positions.weight\"):\n            param.requires_grad = False\n        if(len(arr)>3 and arr[2]==\"layers\" and int(arr[3])<3):\n            param.requires_grad = False\n    \n    optimizer = optim.AdamW(model.parameters(), LR,\n                            betas=(0.9, 0.999), weight_decay=1e-1)\n    scheduler = get_constant_schedule_with_warmup(optimizer, 35)\n    print(header)\n    for epoch in range(1, epochs+1):      \n        start_time = time.time()\n        tloss = []          \n        model.train()\n\n        for texts, attns, target,error in loader:\n            optimizer.zero_grad()\n            outputs = model(texts, attention_mask=attns)\n            gloss = loss_fn(outputs.logits.squeeze(-1), target, error)\n            loss = display_loss(outputs.logits.squeeze(-1), target)\n            tloss.append(loss.item())\n            gloss.backward()\n            optimizer.step()\n            scheduler.step()\n        tloss = validation_fn(model,loader,display_loss)\n        vloss = validation_fn(model, vloader, display_loss)\n        tmetric = tloss**.5\n        vmetric = vloss**.5\n        print(raw_line.format(epoch,tloss,tmetric,vloss,vmetric,(time.time()-start_time)/60**1))\n        del loss, outputs\n\n    model.eval();\n    # Get prediction for test set\n    ids, preds = [], [] \n    with torch.no_grad():\n        for batch_ids, texts, attn in tloader:\n            outputs = model(texts, attention_mask=attn)\n            ids += batch_ids\n            preds.append(outputs.logits.detach().squeeze(-1).cpu().numpy())\n\n    # Save prediction of test set\n    preds = np.concatenate(preds)\n    subm.loc[ids, 'target']  =  preds \n\n#     output_dir = \"./pretrained_bart_{}/\".format(model_id)\n#     model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n#     model_to_save.save_pretrained(output_dir)\n#     tokenizer.save_pretrained(output_dir)\n#     model_id += 1\n    del model, vloader, loader, train_ds, valid_ds\n    torch.cuda.empty_cache()","metadata":{"_uuid":"e566ecdb5f44cf32af12127f073021566d6e66dd","execution":{"iopub.status.busy":"2021-05-23T09:05:52.406485Z","iopub.execute_input":"2021-05-23T09:05:52.406737Z","iopub.status.idle":"2021-05-23T09:05:52.430453Z","shell.execute_reply.started":"2021-05-23T09:05:52.406704Z","shell.execute_reply":"2021-05-23T09:05:52.42977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def oof_preds(ds, tloader, cv, y, epochs = EPOCHES):\n    \n    loss_fn = loss_with_error\n    display_loss = torch.nn.MSELoss()\n    model_id = 0\n#     train_idx,val_idx = train_test_split(range(len(ds)), y)\n        \n    train_ds = ds\n    loader = D.DataLoader(train_ds, batch_size=BATCH_SIZE,\n                          shuffle=True, collate_fn = collate_fn,num_workers=0)\n\n    valid_ds = ds\n    vloader = D.DataLoader(valid_ds, batch_size=BATCH_SIZE,\n                  shuffle=False, collate_fn = collate_fn,num_workers=0)\n\n    model = get_model.from_pretrained( \n                      MODEL_PATH, num_labels=1).to(DEVICE);\n\n    optimizer = optim.AdamW(model.parameters(), LR,\n                            betas=(0.9, 0.999), weight_decay=1e-3)\n    scheduler = get_constant_schedule_with_warmup(optimizer, 35)\n    print(header)\n    for epoch in range(1, epochs+1):      \n        start_time = time.time()\n        tloss = []          \n        model.train()\n\n        for texts, attns, target,error in loader:\n            optimizer.zero_grad()\n            outputs = model(texts, attention_mask=attns)\n            gloss = loss_fn(outputs.logits.squeeze(-1), target, error)\n            loss = display_loss(outputs.logits.squeeze(-1), target)\n            tloss.append(loss.item())\n            gloss.backward()\n            optimizer.step()\n            scheduler.step()\n        tloss = validation_fn(model,loader,display_loss)\n        vloss = validation_fn(model, vloader, display_loss)\n        tmetric = tloss**.5\n        vmetric = vloss**.5\n        print(raw_line.format(epoch,tloss,tmetric,vloss,vmetric,(time.time()-start_time)/60**1))\n        del loss, outputs\n\n    model.eval();\n    # Get prediction for test set\n    ids, preds = [], [] \n    with torch.no_grad():\n        for batch_ids, texts, attn in tloader:\n            outputs = model(texts, attention_mask=attn)\n            ids += batch_ids\n            preds.append(outputs.logits.detach().squeeze(-1).cpu().numpy())\n\n    # Save prediction of test set\n    preds = np.concatenate(preds)\n    subm.loc[ids, 'target']  =  preds\n\n    output_dir = \"./pretrained_bart_{}/\".format(model_id)\n    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n    model_to_save.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    model_id += 1\n    del model, vloader, loader, train_ds, valid_ds\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:05:52.431789Z","iopub.execute_input":"2021-05-23T09:05:52.432432Z","iopub.status.idle":"2021-05-23T09:05:52.447997Z","shell.execute_reply.started":"2021-05-23T09:05:52.432393Z","shell.execute_reply":"2021-05-23T09:05:52.447403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = get_model.from_pretrained( \n#                       MODEL_PATH, num_labels=1).to(DEVICE)\n# # model_to_exe = model.module if hasattr(model, 'module') else model\n# for name, param in model.named_parameters():\n#     if param.requires_grad:\n#         print (\"{} \".format(name))\n# del(model)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:05:52.449284Z","iopub.execute_input":"2021-05-23T09:05:52.449663Z","iopub.status.idle":"2021-05-23T09:05:52.458224Z","shell.execute_reply.started":"2021-05-23T09:05:52.449627Z","shell.execute_reply":"2021-05-23T09:05:52.45766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_vals(ds, tloader, cv, y, epochs = EPOCHES)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:05:52.460624Z","iopub.execute_input":"2021-05-23T09:05:52.461007Z","iopub.status.idle":"2021-05-23T09:09:24.635168Z","shell.execute_reply.started":"2021-05-23T09:05:52.460972Z","shell.execute_reply":"2021-05-23T09:09:24.634411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# oof_preds(ds, tloader, cv, y, epochs = EPOCHES)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:09:24.636628Z","iopub.execute_input":"2021-05-23T09:09:24.637125Z","iopub.status.idle":"2021-05-23T09:09:24.640172Z","shell.execute_reply.started":"2021-05-23T09:09:24.637076Z","shell.execute_reply":"2021-05-23T09:09:24.639538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm.to_csv('submission.csv')","metadata":{"_uuid":"7c6a41bd21bf783455fd11b63745ed454e0413f0","execution":{"iopub.status.busy":"2021-05-23T09:09:24.641488Z","iopub.execute_input":"2021-05-23T09:09:24.64198Z","iopub.status.idle":"2021-05-23T09:09:24.888344Z","shell.execute_reply.started":"2021-05-23T09:09:24.641941Z","shell.execute_reply":"2021-05-23T09:09:24.887686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T09:09:24.889575Z","iopub.execute_input":"2021-05-23T09:09:24.889946Z","iopub.status.idle":"2021-05-23T09:09:24.904716Z","shell.execute_reply.started":"2021-05-23T09:09:24.889906Z","shell.execute_reply":"2021-05-23T09:09:24.903931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}