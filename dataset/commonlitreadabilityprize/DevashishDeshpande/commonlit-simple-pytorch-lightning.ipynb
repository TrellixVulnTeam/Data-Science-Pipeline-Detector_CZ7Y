{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nimport torchtext\nimport pandas as pd\nimport numpy as np\nimport nltk.data\nimport torch.nn.functional as F\nimport math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = torchtext.data.utils.get_tokenizer('spacy', language='en')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Loading glove embeddings #####\nembed_dim = 300\nf = open('../input/glove-embeds/glove.6B.'+str(embed_dim)+'d.txt', 'r', encoding='utf-8')\n\nword_to_idx = {'<pad>': 0}\nidx_to_embed = {}\nidx = 1\nfor l in f:\n    values = l.split()\n    word = values[0]\n    word_to_idx[word] = idx\n    embed = np.array([float(v) for v in values[1:]])\n    idx_to_embed[idx] = embed\n    idx += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx_to_word = {idx: w for w, idx in word_to_idx.items()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_matrix = np.random.randn(len(word_to_idx), embed_dim)\nfor idx, embed in idx_to_embed.items():\n    embed_matrix[idx, :] = embed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_paragraph(para, tokenizer, word_to_idx):\n    para_encoded = []\n    para = para.lower()\n    for word in tokenizer(para):\n        try:\n            para_encoded.append(word_to_idx[word])\n        except KeyError as e:\n            continue  # skip words not in vocab\n    return para_encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"para_encodings = []\ntargets = []\nfor idx, row in df.iterrows():\n    para_enc = encode_paragraph(row.excerpt, tokenizer, word_to_idx)\n    target = row.target\n    para_enc = torch.tensor(para_enc, dtype=torch.long)\n    para_encodings.append(para_enc)\n    targets.append(target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = list(zip(para_encodings, targets))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\n# class ParaDataset(Dataset):\n#     def __init__(self, data, stage):\n#         self.data = data\n#         self.stage = stage\n        \n#     def __len__(self):\n#         return len(self.data)\n    \n#     def __getitem__(self, idx):\n#         if self.stage == 'train' or self.stage == 'val':\n#             word_enc, syll_enc, target = self.data[idx]\n#             return word_enc, syll_enc, target\n#         if self.stage == 'test':\n#             word_enc, syll_enc = self.data[idx]\n#             return word_enc, syll_enc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\nword_pad_idx = word_to_idx['<pad>']\n\ndef generate_batch(data_batch):\n    word_batch = [d[0] for d in data_batch]\n    word_batch = pad_sequence(word_batch, batch_first=True, padding_value=word_pad_idx)\n    try:\n        target_batch = torch.tensor([d[1] for d in data_batch], dtype=torch.float32)\n        return word_batch, target_batch\n    except IndexError as e:\n        return word_batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build Model","metadata":{}},{"cell_type":"code","source":"class Model(pl.LightningModule):\n    def __init__(self, word_embed_weights, embed_dim, dropout=0.2, lr=1e-3):\n        super(Model, self).__init__()\n        self.word_embeddings = nn.Embedding.from_pretrained(word_embed_weights, freeze=False)\n        self.embed_dim = embed_dim\n        self.dropout1 = nn.Dropout(p=dropout)\n        self.linear1 = nn.Linear(embed_dim, 150)\n        self.dropout2 = nn.Dropout(p=dropout)\n        self.linear2 = nn.Linear(150, 75)\n        self.dropout3 = nn.Dropout(p=dropout)\n        self.linear3 = nn.Linear(75, 25)\n        self.dropout4 = nn.Dropout(p=dropout)\n        self.linear4 = nn.Linear(25, 1)\n        self.lr = lr\n        self.test_preds = []\n        \n    def forward(self, x, mask):\n        \"\"\"\n        x is of size (bsz, seq_len)\n        mask is of size (bsz, seq_len)\n        \"\"\"\n        x = self.word_embeddings(x)  # (bsz, seq_len, embed_dim)\n        n_non_zero = mask.sum(-1)\n        n_non_zero = n_non_zero.unsqueeze(-1)\n        x = x.sum(1) / n_non_zero  # take average embedding for sentence\n        x = self.dropout1(x)\n        x = F.tanh(self.linear1(x))\n        x = self.dropout2(x)\n        x = F.tanh(self.linear2(x))\n        x = self.dropout3(x)\n        x = F.tanh(self.linear3(x))\n        x = self.dropout4(x)\n        pred = self.linear4(x)\n        return pred\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n\n    def shared_step(self, batch, stage):\n        if stage in ['train', 'val']:\n            x, y = batch\n        elif stage == 'test':\n            x = batch\n        mask = x > 0\n        y_hat = self(x, mask)\n        y_hat = y_hat.flatten()\n        if stage == 'train' or stage == 'val':\n            loss = F.mse_loss(y_hat, y)\n            return loss\n        if stage == 'test':\n            return y_hat\n    \n    def training_step(self, train_batch, batch_idx):\n        loss = self.shared_step(train_batch, 'train')\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, val_batch, batch_idx):\n        loss = self.shared_step(val_batch, 'val')\n        self.log('val_loss', loss)\n    \n    def test_step(self, test_batch, batch_idx):\n        preds = self.shared_step(test_batch, 'test')\n        self.test_preds.extend(preds.flatten().cpu().numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 256\ntrain_data_split, val_data_split = torch.utils.data.random_split(train_data, [2734, 100])\ntrain_dataloader = DataLoader(train_data_split, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\nval_dataloader = DataLoader(val_data_split, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batch)\nembed_matrix = torch.tensor(embed_matrix, dtype=torch.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    monitor='val_loss',\n    dirpath='./saved_models/',\n    filename='model-{epoch:02d}-{val_loss:.2f}',\n    save_top_k=3,\n    mode='min'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(\n    word_embed_weights=embed_matrix,\n    embed_dim=embed_dim, \n    dropout=0.2)\ntrainer = pl.Trainer(\n    gpus=1, \n    fast_dev_run=False, \n    auto_lr_find=True, \n    max_epochs=1500,\n    progress_bar_refresh_rate=0,\n    callbacks=[checkpoint_callback])\ntrainer.tune(model, train_dataloader, val_dataloader)\ntrainer.fit(model, train_dataloader, val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:02:28.297122Z","iopub.execute_input":"2021-06-09T19:02:28.297666Z","iopub.status.idle":"2021-06-09T19:02:56.642865Z","shell.execute_reply.started":"2021-06-09T19:02:28.297588Z","shell.execute_reply":"2021-06-09T19:02:56.641202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\npara_encodings_test = []\nfor idx, row in test_df.iterrows():\n    para_enc = encode_paragraph(row.excerpt, tokenizer, word_to_idx)\n    para_enc = torch.tensor(para_enc, dtype=torch.long)\n    para_encodings_test.append([para_enc])\ntest_data = para_encodings_test","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:07:54.566701Z","iopub.execute_input":"2021-06-09T19:07:54.567138Z","iopub.status.idle":"2021-06-09T19:07:54.597324Z","shell.execute_reply.started":"2021-06-09T19:07:54.567106Z","shell.execute_reply":"2021-06-09T19:07:54.596133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataloader = DataLoader(test_data, batch_size=10, shuffle=False, collate_fn=generate_batch)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:07:56.55254Z","iopub.execute_input":"2021-06-09T19:07:56.553048Z","iopub.status.idle":"2021-06-09T19:07:56.559533Z","shell.execute_reply.started":"2021-06-09T19:07:56.552999Z","shell.execute_reply":"2021-06-09T19:07:56.557757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model.load_from_checkpoint(\n    checkpoint_callback.best_model_path,\n    word_embed_weights=embed_matrix,\n    embed_dim=embed_dim, \n    dropout=0.2)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:08:10.385443Z","iopub.execute_input":"2021-06-09T19:08:10.38591Z","iopub.status.idle":"2021-06-09T19:08:10.898875Z","shell.execute_reply.started":"2021-06-09T19:08:10.385864Z","shell.execute_reply":"2021-06-09T19:08:10.897695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.test(model, test_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:08:13.718673Z","iopub.execute_input":"2021-06-09T19:08:13.719125Z","iopub.status.idle":"2021-06-09T19:08:13.802001Z","shell.execute_reply.started":"2021-06-09T19:08:13.719094Z","shell.execute_reply":"2021-06-09T19:08:13.800546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(zip(list(test_df.id), model.test_preds), columns=['id', 'target'])\nsubmission.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-09T19:08:32.246902Z","iopub.execute_input":"2021-06-09T19:08:32.24733Z","iopub.status.idle":"2021-06-09T19:08:32.265638Z","shell.execute_reply.started":"2021-06-09T19:08:32.247284Z","shell.execute_reply":"2021-06-09T19:08:32.26419Z"},"trusted":true},"execution_count":null,"outputs":[]}]}