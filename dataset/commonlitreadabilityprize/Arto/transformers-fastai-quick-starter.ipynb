{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q --no-deps ../input/fasthugs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.text.all import *\nfrom fastai.callback.wandb import *\nfrom fasthugs.data import TransformersTextBlock, TextGetter\nfrom fasthugs.learner import TransLearner\n\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport gc\nimport wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nwandb_key = UserSecretsClient().get_secret(\"wandb_api_key\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env WANDB_ENTITY=arampacha\n%env WANDB_PROJECT=commonlit\n%env WANDB_SILENT=true","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login(key=wandb_key)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"code","source":"path = Path('../input/commonlitreadabilityprize')\noutput_path = Path('./')\npath.ls()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(path/'train.csv')\ntrain_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = KFold(n_splits=5, shuffle=True, random_state=8)\nvalid_idxs = []\nfor _, valid_idx in cv.split(np.arange(len(train_df))):\n    valid_idxs += [valid_idx]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training on first fold","metadata":{}},{"cell_type":"markdown","source":"For demonstration purposes I'm using `distilroberta-base`. It is a lightweight distilled vertion of RoBERTa, which performes considerably worse. You can easily switch to other models by changing `model_name`. The `TransformersTextBlock` uses pretrained huggingface tokenizer internally and is set up by providing path to pretrained model. ","metadata":{}},{"cell_type":"code","source":"model_name = '../input/roberta-transformers-pytorch/distilroberta-base'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dblock = DataBlock(blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock()],\n                   get_x=TextGetter('excerpt'),\n                   get_y=ItemGetter('target'),\n                   splitter=IndexSplitter(valid_idxs[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = dblock.dataloaders(train_df, bs=16, val_bs=32, num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch(max_n=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\nlearn = TransLearner(dls, model, metrics=rmse, path=output_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 2e-5\nwd = 0.05","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm going to log runs to Weights&Biases. This may be of greate use for further analysis of the results. As you shell see with fastai it doesn't require much extra work at all.","metadata":{}},{"cell_type":"code","source":"name = f\"{model_name}_lr{lr:.0e}-fold{0}\"\ngroup = f\"{model_name}_lr{lr:.0e}\"\nrun = wandb.init(name=name, group=group)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cbs=[WandbCallback(log_preds=False, log_model=False),\n     SaveModelCallback(monitor='_rmse', fname='model_0', comp=np.less)]\nlearn.fit_one_cycle(4, lr, wd=wd, cbs=cbs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best performing model is stored and loaded at the end of the training by `SaveModelCallback`:","metadata":{}},{"cell_type":"code","source":"(output_path/'models').ls()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.validate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_preds = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(path/'test.csv')\ntest_dl = dls.test_dl(test_df)\ntest_dl.show_batch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds, _ = learn.get_preds(dl=test_dl)\nall_preds += [preds]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross validation","metadata":{}},{"cell_type":"markdown","source":"Let's fit models on remaining folds and save all the prediction.","metadata":{}},{"cell_type":"code","source":"for i in range (1, len(valid_idxs)):\n    name = f\"{model_name}_lr{lr:.0e}-fold{i}\"\n    group = f\"{model_name}_lr{lr:.0e}\"\n    with wandb.init(name=name, group=group) as run:\n        dblock = DataBlock(blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock()],\n                       get_x=TextGetter('excerpt'),\n                       get_y=ItemGetter('target'),\n                       splitter=IndexSplitter(valid_idxs[i]))\n        dls = dblock.dataloaders(train_df, bs=16, val_bs=32, num_workers=2)\n        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n        learn = TransLearner(dls, model, metrics=rmse)\n        cbs=[WandbCallback(log_preds=False, log_model=False),\n             SaveModelCallback(monitor='_rmse', fname=f'model_{i}', comp=np.less)]\n        learn.fit_one_cycle(4, 2e-5, wd=wd, cbs=cbs)\n        preds, _ = learn.get_preds(dl=test_dl)\n        all_preds += [preds]\n        del learn; gc.collect()\n        torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission\nFinally we can average the predictions from all models and submit:","metadata":{}},{"cell_type":"code","source":"preds = torch.cat(all_preds, dim=1).mean(dim=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(path/'sample_submission.csv', index_col='id')\nsubmission['target'] = preds.numpy()\nsubmission.to_csv('submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}