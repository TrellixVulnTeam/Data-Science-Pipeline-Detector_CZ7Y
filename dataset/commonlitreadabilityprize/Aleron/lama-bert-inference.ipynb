{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ../input/lama-whl/efficientnet_pytorch-0.7.0/dist/efficientnet_pytorch-0.7.0.tar ../input/lama-whl/log_calls-0.3.2/log_calls-0.3.2/ ../input/lama-whl/sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_htmlhelp-1.0.3-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl ../input/lama-whl/sphinxcontrib_serializinghtml-1.1.4-py2.py3-none-any.whl ../input/lama-whl/importlib_metadata-1.7.0-py2.py3-none-any.whl ../input/lama-whl/poetry_core-1.0.3-py2.py3-none-any.whl ../input/lama-whl/imagesize-1.2.0-py2.py3-none-any.whl ../input/lama-whl/docutils-0.16-py2.py3-none-any.whl ../input/lama-whl/alabaster-0.7.12-py2.py3-none-any.whl ../input/lama-whl/snowballstemmer-2.1.0-py2.py3-none-any.whl ../input/lama-whl/Sphinx-3.5.4-py3-none-any.whl ../input/lama-whl/sphinx_autodoc_typehints-1.11.1-py3-none-any.whl ../input/lama-whl/nbsphinx-0.8.0-py3-none-any.whl ../input/lama-whl/nbsphinx_link-1.3.0-py2.py3-none-any.whl ../input/lama-whl/cssselect-1.1.0-py2.py3-none-any.whl ../input/lama-whl/pyquery-1.4.3-py3-none-any.whl ../input/lama-whl/chuanconggao-html2json-0.2.4.1-0-g99d7fbb/chuanconggao-html2json-99d7fbb/ ../input/lama-whl/json2html-1.3.0/json2html-1.3.0 ../input/lama-whl/lightgbm-2.3.1-py2.py3-none-manylinux1_x86_64.whl ../input/lama-whl/AutoWoE-1.2.1-py3-none-any.whl ../input/lama-whl/LightAutoML-0.2.14-py3-none-any.whl > /dev/null","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-27T08:15:03.166516Z","iopub.execute_input":"2021-07-27T08:15:03.167274Z","iopub.status.idle":"2021-07-27T08:15:53.543979Z","shell.execute_reply.started":"2021-07-27T08:15:03.167153Z","shell.execute_reply":"2021-07-27T08:15:53.543015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/packages-for-creating-text-features/*.whl > /dev/null","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:15:53.546884Z","iopub.execute_input":"2021-07-27T08:15:53.547244Z","iopub.status.idle":"2021-07-27T08:16:25.864554Z","shell.execute_reply.started":"2021-07-27T08:15:53.547205Z","shell.execute_reply":"2021-07-27T08:16:25.863504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/packages-for-creating-text-features/ReadabilityCalculator-0.2.37/ReadabilityCalculator-0.2.37 > /dev/null","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:16:25.866874Z","iopub.execute_input":"2021-07-27T08:16:25.867259Z","iopub.status.idle":"2021-07-27T08:16:53.758211Z","shell.execute_reply.started":"2021-07-27T08:16:25.867208Z","shell.execute_reply":"2021-07-27T08:16:53.756988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp ../input/textstat-pre/dist/textstat-0.7.1.tar . > /dev/null\n!cp ../input/pyphen-gz/pyphen-0.11.0.tar . > /dev/null\n!tar -xvf textstat-0.7.1.tar > /dev/null\n!tar -xvf pyphen-0.11.0.tar > /dev/null\n\n!cd pyphen-0.11.0 && python setup.py build > /dev/null && python setup.py install > /dev/null\n!cd textstat-0.7.1 && python setup.py build > /dev/null && python setup.py install > /dev/null","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:16:53.760315Z","iopub.execute_input":"2021-07-27T08:16:53.760692Z","iopub.status.idle":"2021-07-27T08:16:57.711211Z","shell.execute_reply.started":"2021-07-27T08:16:53.76065Z","shell.execute_reply":"2021-07-27T08:16:57.710305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python setup.py build > /dev/null\n# !python setup.py install > /dev/null","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:16:57.712836Z","iopub.execute_input":"2021-07-27T08:16:57.713176Z","iopub.status.idle":"2021-07-27T08:16:57.718892Z","shell.execute_reply.started":"2021-07-27T08:16:57.71314Z","shell.execute_reply":"2021-07-27T08:16:57.718059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd textstat-0.7.1/","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:16:57.720381Z","iopub.execute_input":"2021-07-27T08:16:57.72082Z","iopub.status.idle":"2021-07-27T08:16:57.72948Z","shell.execute_reply.started":"2021-07-27T08:16:57.720711Z","shell.execute_reply":"2021-07-27T08:16:57.728633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import textstat","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:16:57.731025Z","iopub.execute_input":"2021-07-27T08:16:57.731371Z","iopub.status.idle":"2021-07-27T08:16:57.852262Z","shell.execute_reply.started":"2021-07-27T08:16:57.73134Z","shell.execute_reply":"2021-07-27T08:16:57.851485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T08:16:57.853428Z","iopub.execute_input":"2021-07-27T08:16:57.853777Z","iopub.status.idle":"2021-07-27T08:16:57.857625Z","shell.execute_reply.started":"2021-07-27T08:16:57.853742Z","shell.execute_reply":"2021-07-27T08:16:57.856809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from shutil import copyfile\nimport os\nfrom sklearn.metrics import mean_squared_error\nfrom lightautoml.automl.presets.text_presets import TabularNLPAutoML\nfrom lightautoml.tasks import Task","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T08:16:57.860797Z","iopub.execute_input":"2021-07-27T08:16:57.86145Z","iopub.status.idle":"2021-07-27T08:17:03.848138Z","shell.execute_reply.started":"2021-07-27T08:16:57.861392Z","shell.execute_reply":"2021-07-27T08:17:03.847353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom fastprogress.fastprogress import  progress_bar\n \nfrom sklearn.metrics import mean_squared_error\nfrom lightautoml.automl.presets.text_presets import TabularNLPAutoML\nfrom lightautoml.tasks import Task","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T08:17:03.850099Z","iopub.execute_input":"2021-07-27T08:17:03.850361Z","iopub.status.idle":"2021-07-27T08:17:03.859692Z","shell.execute_reply.started":"2021-07-27T08:17:03.850336Z","shell.execute_reply":"2021-07-27T08:17:03.858825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../input/k/aleron751/k/aleron751/","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:17:03.861231Z","iopub.execute_input":"2021-07-27T08:17:03.86199Z","iopub.status.idle":"2021-07-27T08:17:04.540487Z","shell.execute_reply.started":"2021-07-27T08:17:03.861954Z","shell.execute_reply":"2021-07-27T08:17:04.53957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir_path = './models'\nsaved_model_path = '../input/k/aleron751/k/aleron751/lama-bert-starter/models/'\n\nif not os.path.exists(model_dir_path):\n    os.makedirs(model_dir_path)\n\nfor file in os.listdir(saved_model_path):\n    src = os.path.join(saved_model_path, file)\n    dst = os.path.join(model_dir_path, file)\n    copyfile(src, dst)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:17:04.542271Z","iopub.execute_input":"2021-07-27T08:17:04.542632Z","iopub.status.idle":"2021-07-27T08:18:36.181329Z","shell.execute_reply.started":"2021-07-27T08:17:04.542593Z","shell.execute_reply":"2021-07-27T08:18:36.180462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:18:36.182637Z","iopub.execute_input":"2021-07-27T08:18:36.183Z","iopub.status.idle":"2021-07-27T08:18:37.150624Z","shell.execute_reply.started":"2021-07-27T08:18:36.182963Z","shell.execute_reply":"2021-07-27T08:18:37.149352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse(x, y): return np.sqrt(mean_squared_error(x, y))\nwith open('../input/k/aleron751/k/aleron751/lama-bert-starter/LAMA_model.pkl', 'rb') as f:\n    automl = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:18:37.155212Z","iopub.execute_input":"2021-07-27T08:18:37.155613Z","iopub.status.idle":"2021-07-27T08:18:47.901562Z","shell.execute_reply.started":"2021-07-27T08:18:37.15557Z","shell.execute_reply":"2021-07-27T08:18:47.900713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ntrain_df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:18:47.902894Z","iopub.execute_input":"2021-07-27T08:18:47.903226Z","iopub.status.idle":"2021-07-27T08:18:47.973456Z","shell.execute_reply.started":"2021-07-27T08:18:47.903192Z","shell.execute_reply":"2021-07-27T08:18:47.972654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"def preprocess(data):\n    excerpt_processed=[]\n    for e in progress_bar(data['excerpt']):\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed\n\ntest_df[\"excerpt_preprocessed\"] = preprocess(test_df)\ntrain_df[\"excerpt_preprocessed\"] = preprocess(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:18:47.974632Z","iopub.execute_input":"2021-07-27T08:18:47.974963Z","iopub.status.idle":"2021-07-27T08:19:53.419384Z","shell.execute_reply.started":"2021-07-27T08:18:47.974925Z","shell.execute_reply":"2021-07-27T08:19:53.418391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handcrafted features from Kaggle notebooks","metadata":{}},{"cell_type":"code","source":"from textblob.tokenizers import SentenceTokenizer, WordTokenizer\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport os \nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n#import textstat\nplt.style.use('seaborn-talk')\nfrom readcalc import readcalc\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\nimport spacy\nsp = spacy.load('en_core_web_sm')\n\ndef pos_to_id(pos_name):\n    return sp.vocab[pos_name].orth\n\ncontent_poss = ['ADJ', 'NOUN', 'VERB', 'ADV']\n\ndef count_poss(text, poss_names):\n    text = sp(text)\n    poss_ids = [pos_to_id(pos_name) for pos_name in poss_names]\n    pos_freq_dict = text.count_by(spacy.attrs.POS)\n    poss_sum = sum([pos_freq_dict.get(pos_id, 0) for pos_id in poss_ids])\n    return poss_sum\n\n\ncount_poss('my name is', ['PRON', 'NOUN'])\n\n# !pip download textstat ReadabilityCalculator \n# !pip install *.whl\n\nsent_tokenizer = SentenceTokenizer()\nword_tokenizer = WordTokenizer()\n\n# with open('../input/clrauxdata/dale-chall-3000-words.txt') as f:\n#     words = f.readlines()[0].split()\n    \n# common_words = dict(zip(words, [True] * len(words)))\n# # df.sent_cnt.plot(kind='kde')\n\nfeats_to_drop = ['sents_n', 'words_n', 'long_words_n',\n                 #'difficult_words_n',\n                 'content_words_n', 'prons_n', 'chars_n', 'syllables_n']\n\n\ndoc_feats = ['chars_per_word', 'chars_per_sent', 'syllables_per_word',\n       'syllables_per_sent', 'words_per_sent', 'long_words_doc_ratio',\n       'difficult_words_doc_ratio', 'prons_doc_ratio', 'flesch_reading_ease',\n       'flesch_kincaid_grade', 'ari', 'cli', 'gunning_fog', 'lix', 'rix',\n       'smog', 'dcrs', 'lexical_diversity', 'content_diversity', 'lwf']\n\ndef create_handcrafted_features(df):\n    df['sents_n'] = df.excerpt.apply(textstat.sentence_count)\n    df['words_n'] = df.excerpt.apply(textstat.lexicon_count)\n    df['long_words_n'] = df.excerpt.apply(lambda t: readcalc.ReadCalc(t).get_words_longer_than_X(6))\n    #df['difficult_words_n'] = df.excerpt.apply(lambda t: sum([bool(common_words.get(word)) for word in word_tokenizer.tokenize(t, include_punc=False)]))\n    df['content_words_n'] = df.excerpt.apply(lambda t: count_poss(t, content_poss))\n    df['prons_n'] = df.excerpt.apply(lambda t: count_poss(t, ['PRON']))\n    df['chars_n'] = df.excerpt.str.len()\n    df['syllables_n'] = df.excerpt.apply(textstat.syllable_count)\n    print('\\tstage 1 finished..')\n\n    df['chars_per_word_'] = df.chars_n / df.words_n\n    df['chars_per_sent_'] = df.chars_n / df.sents_n\n    df['syllables_per_word_'] = df.syllables_n / df.words_n\n    df['syllables_per_sent_'] = df.syllables_n / df.sents_n\n\n    df['words_per_sent_'] = df.words_n / df.sents_n\n    df['long_words_doc_ratio_'] = df.long_words_n / df.words_n\n    #df['difficult_words_doc_ratio'] = df.difficult_words_n / df.words_n\n    df['prons_doc_ratio'] = df.prons_n / df.words_n\n\n    print('\\tstage 2 finished..')\n\n    df['flesch_reading_ease_'] = df.excerpt.apply(textstat.flesch_reading_ease)\n    df['flesch_kincaid_grade_'] = df.excerpt.apply(textstat.flesch_kincaid_grade)\n    df['ari_'] = df.excerpt.apply(textstat.automated_readability_index)\n    df['cli_'] = df.excerpt.apply(textstat.coleman_liau_index)\n    df['gunning_fog'] = df.excerpt.apply(textstat.gunning_fog)\n\n    df['lix_'] = df.excerpt.apply(lambda t: readcalc.ReadCalc(t).get_lix_index())\n    df['rix_'] = df.long_words_n / df.sents_n\n    df['smog_'] = df.excerpt.apply(lambda t: readcalc.ReadCalc(t).get_smog_index())\n    df['dcrs_'] = df.excerpt.apply(textstat.dale_chall_readability_score)\n\n    df['lexical_diversity_'] = len(set(df.words_n)) / df.words_n\n    df['content_diversity_'] = df.content_words_n / df.words_n\n    df['lwf_'] = df.excerpt.apply(textstat.linsear_write_formula)\n\n    print('\\tstage 3 finished..')\n    return df\n\n\n#test_df = create_handcrafted_features(test_df)\n#test_df.drop(feats_to_drop, inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:19:53.420926Z","iopub.execute_input":"2021-07-27T08:19:53.42129Z","iopub.status.idle":"2021-07-27T08:19:55.838206Z","shell.execute_reply.started":"2021-07-27T08:19:53.421245Z","shell.execute_reply":"2021-07-27T08:19:55.83736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TextStat","metadata":{}},{"cell_type":"code","source":"def text_2_statistics(data):\n    flesch_reading_ease_list, smog_index_list = [], []\n    flesch_kincaid_grade_list, coleman_liau_index_list = [], []\n    automated_readability_index_list, dale_chall_readability_score_list = [], []\n    difficult_words_list, linsear_write_formula_list = [], []\n    gunning_fog_list, text_standard_list = [], []\n    fernandez_huerta_list, szigriszt_pazos_list = [], []\n    gutierrez_polini_list, crawford_list = [], []\n    \n     \n    for sentence in progress_bar(data['excerpt']):\n        flesch_reading_ease_list.append(textstat.flesch_reading_ease(sentence))\n        smog_index_list.append(textstat.smog_index(sentence))\n        flesch_kincaid_grade_list.append(textstat.flesch_kincaid_grade(sentence))\n        coleman_liau_index_list.append(textstat.coleman_liau_index(sentence))\n        automated_readability_index_list.append(textstat.automated_readability_index(sentence))\n        dale_chall_readability_score_list.append(textstat.dale_chall_readability_score(sentence))\n        difficult_words_list.append(textstat.difficult_words(sentence))\n        linsear_write_formula_list.append(textstat.linsear_write_formula(sentence))\n        gunning_fog_list.append(textstat.gunning_fog(sentence))\n        text_standard_list.append(textstat.text_standard(sentence, float_output=True))\n        fernandez_huerta_list.append(textstat.fernandez_huerta(sentence))\n        szigriszt_pazos_list.append(textstat.szigriszt_pazos(sentence))\n        gutierrez_polini_list.append(textstat.gutierrez_polini(sentence))\n        crawford_list.append(textstat.crawford(sentence))\n        \n    statistics_dict = {'flesch_reading_ease':flesch_reading_ease_list,\n                       'smog_index' : smog_index_list,\n                       'flesch_kincaid_grade' : flesch_kincaid_grade_list,\n                       'coleman_liau_index' : coleman_liau_index_list,\n                       'automated_readability_index' : automated_readability_index_list, \n                       'dale_chall_readability_score' : dale_chall_readability_score_list, \n                       'difficult_words' : difficult_words_list,\n                       'linsear_write_formula' : linsear_write_formula_list,\n                       'gunning_fog' : gunning_fog_list,\n                       'text_standard' : text_standard_list,\n                       'fernandez_huerta' : fernandez_huerta_list,\n                       'szigriszt_pazos' : szigriszt_pazos_list,\n                       'gutierrez_polini' : gutierrez_polini_list,\n                       'crawford' : crawford_list\n                      }\n    return statistics_dict\n\n\nstatistics_dict = text_2_statistics(test_df)\nfor k,v in statistics_dict.items():\n    test_df[k] = v\n            ","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:19:55.8396Z","iopub.execute_input":"2021-07-27T08:19:55.839949Z","iopub.status.idle":"2021-07-27T08:19:55.910101Z","shell.execute_reply.started":"2021-07-27T08:19:55.839913Z","shell.execute_reply":"2021-07-27T08:19:55.909159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=1000)\n\ntrain_bags = vectorizer.fit_transform(train_df['excerpt_preprocessed'].values).toarray()\ntrain_bag_of_words_df = pd.DataFrame(train_bags)\ntrain_bag_of_words_df.columns = vectorizer.get_feature_names()\n\n\ntest_bags = vectorizer.fit(train_df['excerpt_preprocessed'].values).transform(test_df['excerpt_preprocessed'].values).toarray()\ntest_bag_of_words_df = pd.DataFrame(test_bags)\ntest_bag_of_words_df.columns = vectorizer.get_feature_names()\n \n\n        \nfor col in test_bag_of_words_df.columns:\n    test_df[col] = test_bag_of_words_df[col].values\n\ndel test_bag_of_words_df\n# train_df.head()\n\n    \n#  ----------------\n\ndef count_words_in_sentences(data):\n    counts = []\n    for sentence in progress_bar(data['excerpt_preprocessed']):\n        words = sentence.split()\n        counts.append(len(words))\n        \n    return counts\n\ntest_df['excerpt_word_counts_by_preprocessed'] = count_words_in_sentences(test_df)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:19:55.911368Z","iopub.execute_input":"2021-07-27T08:19:55.911742Z","iopub.status.idle":"2021-07-27T08:19:56.991665Z","shell.execute_reply.started":"2021-07-27T08:19:55.911695Z","shell.execute_reply":"2021-07-27T08:19:56.990719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLTK features","metadata":{}},{"cell_type":"code","source":"!ls ../input/k/aleron751/lama-bert-starter/nltk_tmp/","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:19:56.993133Z","iopub.execute_input":"2021-07-27T08:19:56.993503Z","iopub.status.idle":"2021-07-27T08:19:57.66837Z","shell.execute_reply.started":"2021-07-27T08:19:56.993465Z","shell.execute_reply":"2021-07-27T08:19:57.667402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List, Dict, Union\n\nimport nltk\nimport numpy as np\nfrom nltk import ne_chunk, pos_tag, word_tokenize\nfrom nltk.tree import Tree\n\n\ndef get_named_entities(text: str) -> List[str]:\n    continuous_chunk = []\n    current_chunk = []\n\n    for i in ne_chunk(pos_tag(word_tokenize(text))):\n        if isinstance(i, Tree):\n            current_chunk.append(\" \".join(token for token, pos in i.leaves()))\n        elif current_chunk:\n            named_entity = \" \".join(current_chunk)\n            continuous_chunk.append(named_entity)\n            current_chunk = []\n\n    named_entity = \" \".join(current_chunk)\n    continuous_chunk.append(named_entity)\n\n    return continuous_chunk\n\n\n_raw_tags = frozenset(\n    {\n        'LS', 'TO', 'VBN', \"''\",\n        'WP', 'UH', 'VBG', 'JJ',\n        'VBZ', '--', 'VBP', 'NN',\n        'DT', 'PRP', ':', 'WP$',\n        'NNPS', 'PRP$', 'WDT',\n        '(', ')', '.', ',', '``',\n        '$', 'RB', 'RBR', 'RBS',\n        'VBD', 'IN', 'FW', 'RP',\n        'JJR', 'JJS', 'PDT', 'MD',\n        'VB', 'WRB', 'NNP', 'EX',\n        'NNS', 'SYM', 'CC', 'CD', 'POS'\n    }\n)\n\n_general_tags = frozenset(\n    {'gVB', 'gNN', 'gPR', 'gWP', 'gRB', 'gJJ'}\n)\n\n_tagset = (\n    *_raw_tags,\n    *_general_tags\n)\n\n\ndef generate_text_features(text: str) -> Dict[str, Union[int, float]]:\n    total_count = dict.fromkeys(_tagset, 0)\n    tokenized_text = nltk.word_tokenize(text)\n    inv_text_len = 1 / len(tokenized_text)\n    for word, pos in nltk.pos_tag(tokenized_text):\n        total_count[pos] += inv_text_len\n        general_tag = f'g{pos[:2]}'\n        if general_tag in _general_tags:\n            total_count[general_tag] += inv_text_len\n\n    max_in_sent = dict.fromkeys(_tagset, 0)\n    min_in_sent = dict.fromkeys(_tagset, 0)\n    mean_in_sent = dict.fromkeys(_tagset, 0)\n    general_tags = set()\n    tags = set()\n\n    sentences = nltk.sent_tokenize(text)\n    num_sentences = len(sentences)\n    num_words = []\n    words_len = []\n\n    for sentence in map(nltk.word_tokenize, sentences):\n        cur_sentence_stat = dict.fromkeys(_tagset, 0)\n        num_words.append(len(sentence))\n        inv_sent_len = 1 / len(sentence)\n        for word, pos in nltk.pos_tag(sentence):\n            words_len.append(len(word))\n            cur_sentence_stat[pos] += inv_sent_len\n            tags.add(pos)\n            general_tag = f'g{pos[:2]}'\n            if general_tag in _general_tags:\n                general_tags.add(general_tag)\n                cur_sentence_stat[general_tag] += inv_sent_len\n        for tag in _tagset:\n            max_in_sent[tag] = max(max_in_sent[tag], cur_sentence_stat[tag])\n            min_in_sent[tag] = min(min_in_sent[tag], cur_sentence_stat[tag])\n            mean_in_sent[tag] += cur_sentence_stat[tag] / num_sentences\n\n    res = {}\n    for k, v in total_count.items():\n        res[f'TOTAL_{k}'] = v\n    for k, v in max_in_sent.items():\n        res[f'MAX_{k}'] = v\n    for k, v in min_in_sent.items():\n        res[f'MIN_{k}'] = v\n    for k, v in mean_in_sent.items():\n        res[f'MEAN_{k}'] = v\n\n    num_words = np.array(num_words)\n    words_len = np.array(words_len)\n    res['NUM_SENTENCES'] = len(num_words)\n    res['MEAN_NUM_WORDS'] = num_words.mean()\n    res['STD_NUM_WORDS'] = num_words.std()\n    res['NUM_WORDS'] = len(words_len)\n    res['MEAN_WORD_LEN'] = words_len.mean()\n    res['STD_WORD_LEN'] = words_len.std()\n    res['TAGS_UNIQUE'] = len(tags)\n    res['GENERAL_TAGS_UNIQUE'] = len(general_tags)\n\n    named_entities = get_named_entities(text)\n    res['NAMED_ENTITIES_PER_SENTENCE'] = len(named_entities) / num_sentences\n    res['UNIQUE_NAMED_ENTITIES_PER_SENTENCE'] = len(set(named_entities)) / num_sentences\n    return res\n\n\ndef max_word_lenght(sentence):\n    words = sentence.split()\n    average = max(len(word) for word in words)\n    return average\n\n\ndef get_all_nltk_feats(text):\n    res = generate_text_features(text)\n    res['number_get_named_entities'] = len(get_named_entities(text))\n    res['max_word_lenght'] = max_word_lenght(text)\n    new_res = {}\n    for k, v in res.items():\n        new_res[k] = [v]\n    \n    return new_res\n    \n# txt = 'Say hello to my little friend, Bro! I love you, Sarra!'\n# nltk_feats = get_all_nltk_feats(txt)\n# nltk_feats\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:19:57.671783Z","iopub.execute_input":"2021-07-27T08:19:57.672064Z","iopub.status.idle":"2021-07-27T08:19:57.69808Z","shell.execute_reply.started":"2021-07-27T08:19:57.672034Z","shell.execute_reply":"2021-07-27T08:19:57.697123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk_feats_df = pd.DataFrame()\nfor txt in progress_bar(test_df['excerpt']):\n    nltk_feats_dict = get_all_nltk_feats(txt)\n    nltk_feats_df = nltk_feats_df.append(pd.DataFrame(nltk_feats_dict))\n     \n        \nfor col in nltk_feats_df.columns:\n    test_df[col] = nltk_feats_df[col].values\n\n# del nltk_feats_df\ntest_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:19:57.701081Z","iopub.execute_input":"2021-07-27T08:19:57.701367Z","iopub.status.idle":"2021-07-27T08:19:59.481297Z","shell.execute_reply.started":"2021-07-27T08:19:57.70134Z","shell.execute_reply":"2021-07-27T08:19:59.480256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(df):\n    df['len_tokens'] = df['excerpt'].str.strip().str.split(' ').apply(len)\n    df['len'] = df['excerpt'].str.strip().apply(len)\n    df['len_sent'] = df['excerpt'].str.strip().str.split('.').apply(len)\n    df['n_comm'] = df['excerpt'].str.strip().str.split(',').apply(len)\n    _t = df['excerpt'].str.strip().str.split(' ').values\n    df['d_mean'] = [np.sum([j.isdigit() for j in i]) for i in _t]\n    df['u_mean'] = [np.sum([j.isupper() for j in i]) for i in _t]\n    \npreprocess_text(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:19:59.482808Z","iopub.execute_input":"2021-07-27T08:19:59.48316Z","iopub.status.idle":"2021-07-27T08:19:59.500228Z","shell.execute_reply.started":"2021-07-27T08:19:59.483124Z","shell.execute_reply":"2021-07-27T08:19:59.499249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Важно проверить число вот тут!\nprint(test_df.shape)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:19:59.503471Z","iopub.execute_input":"2021-07-27T08:19:59.503736Z","iopub.status.idle":"2021-07-27T08:19:59.532239Z","shell.execute_reply.started":"2021-07-27T08:19:59.503711Z","shell.execute_reply":"2021-07-27T08:19:59.531299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_test = automl.predict(test_df).data[:, 0]\nsubmission = pd.DataFrame({'id': test_df.id, 'target': np.clip(preds_test,-3.8, 1.8)})  # TODO добавил clip\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:19:59.533555Z","iopub.execute_input":"2021-07-27T08:19:59.533916Z","iopub.status.idle":"2021-07-27T08:20:42.863943Z","shell.execute_reply.started":"2021-07-27T08:19:59.533879Z","shell.execute_reply":"2021-07-27T08:20:42.860847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:20:42.865218Z","iopub.status.idle":"2021-07-27T08:20:42.865961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm text* -r\n!rm pyp* -r\n!rm mode* -r","metadata":{"execution":{"iopub.status.busy":"2021-07-27T08:20:42.867235Z","iopub.status.idle":"2021-07-27T08:20:42.867943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}