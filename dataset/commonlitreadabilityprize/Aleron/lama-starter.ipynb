{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ../input/lama-whl/LightAutoML-0.2.14-py3-none-any.whl > /dev/null\n!pip install ../input/packages-for-creating-text-features/*.whl > /dev/null\n!pip install ../input/packages-for-creating-text-features/ReadabilityCalculator-0.2.37/ReadabilityCalculator-0.2.37 > /dev/null\n!cp ../input/textstat-pre/dist/textstat-0.7.1.tar . > /dev/null\n!cp ../input/pyphen-gz/pyphen-0.11.0.tar . > /dev/null\n!tar -xvf textstat-0.7.1.tar > /dev/null\n!tar -xvf pyphen-0.11.0.tar > /dev/null\n!cd pyphen-0.11.0 && python setup.py build > /dev/null && python setup.py install > /dev/null\n!cd textstat-0.7.1 && python setup.py build > /dev/null && python setup.py install > /dev/null","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-28T19:02:36.695718Z","iopub.execute_input":"2021-07-28T19:02:36.696135Z","iopub.status.idle":"2021-07-28T19:03:38.210933Z","shell.execute_reply.started":"2021-07-28T19:02:36.696051Z","shell.execute_reply":"2021-07-28T19:03:38.209866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !python setup.py build > /dev/null\n# !python setup.py install > /dev/null","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:03:38.21483Z","iopub.execute_input":"2021-07-28T19:03:38.21515Z","iopub.status.idle":"2021-07-28T19:03:38.222326Z","shell.execute_reply.started":"2021-07-28T19:03:38.215117Z","shell.execute_reply":"2021-07-28T19:03:38.22128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import textstat\nimport numpy as np\nimport pandas as pd\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport transformers\nimport torch\nfrom transformers import BertTokenizer\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\nfrom fastprogress.fastprogress import  progress_bar\n \nfrom sklearn.metrics import mean_squared_error\nfrom lightautoml.automl.presets.text_presets import TabularNLPAutoML\nfrom lightautoml.tasks import Task","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:03:38.226475Z","iopub.execute_input":"2021-07-28T19:03:38.226828Z","iopub.status.idle":"2021-07-28T19:03:44.88789Z","shell.execute_reply.started":"2021-07-28T19:03:38.226792Z","shell.execute_reply":"2021-07-28T19:03:44.886935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\ntrain_df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:03:44.889748Z","iopub.execute_input":"2021-07-28T19:03:44.890132Z","iopub.status.idle":"2021-07-28T19:03:44.975164Z","shell.execute_reply.started":"2021-07-28T19:03:44.890088Z","shell.execute_reply":"2021-07-28T19:03:44.974159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.target.min(), train_df.target.max()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:03:44.976563Z","iopub.execute_input":"2021-07-28T19:03:44.976965Z","iopub.status.idle":"2021-07-28T19:03:44.992375Z","shell.execute_reply.started":"2021-07-28T19:03:44.976922Z","shell.execute_reply":"2021-07-28T19:03:44.990946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TIMEOUT = 15_000 # Time in seconds for automl run\nTARGET_NAME = 'target' # Target column name","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:03:44.994271Z","iopub.execute_input":"2021-07-28T19:03:44.995118Z","iopub.status.idle":"2021-07-28T19:03:44.999507Z","shell.execute_reply.started":"2021-07-28T19:03:44.995072Z","shell.execute_reply":"2021-07-28T19:03:44.998398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse(x, y): return np.sqrt(mean_squared_error(x, y))\ntask = Task('reg', metric=rmse)\nroles = {'target': TARGET_NAME,\n         'text': ['excerpt'],\n         'drop': ['id', 'standard_error', 'url_legal', 'license']}","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:03:45.001242Z","iopub.execute_input":"2021-07-28T19:03:45.001822Z","iopub.status.idle":"2021-07-28T19:03:45.014154Z","shell.execute_reply.started":"2021-07-28T19:03:45.001775Z","shell.execute_reply":"2021-07-28T19:03:45.012951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# preprocess","metadata":{}},{"cell_type":"code","source":"def preprocess(data):\n    excerpt_processed=[]\n    for e in progress_bar(data['excerpt']):\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed\n\ntrain_df['excerpt_preprocessed'] = preprocess(train_df)\n#test_df[\"excerpt_preprocessed\"] = preprocess(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:03:45.018327Z","iopub.execute_input":"2021-07-28T19:03:45.019045Z","iopub.status.idle":"2021-07-28T19:05:00.785021Z","shell.execute_reply.started":"2021-07-28T19:03:45.018993Z","shell.execute_reply":"2021-07-28T19:05:00.783842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Handcrafted features from Kaggle notebooks","metadata":{}},{"cell_type":"code","source":"from textblob.tokenizers import SentenceTokenizer, WordTokenizer\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport os \nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n#import textstat\nplt.style.use('seaborn-talk')\nfrom readcalc import readcalc\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\n\nimport spacy\nsp = spacy.load('en_core_web_sm')\n\ndef pos_to_id(pos_name):\n    return sp.vocab[pos_name].orth\n\ncontent_poss = ['ADJ', 'NOUN', 'VERB', 'ADV']\n\ndef count_poss(text, poss_names):\n    text = sp(text)\n    poss_ids = [pos_to_id(pos_name) for pos_name in poss_names]\n    pos_freq_dict = text.count_by(spacy.attrs.POS)\n    poss_sum = sum([pos_freq_dict.get(pos_id, 0) for pos_id in poss_ids])\n    return poss_sum\n\n\ncount_poss('my name is', ['PRON', 'NOUN'])\n\n# !pip download textstat ReadabilityCalculator \n# !pip install *.whl\n\nsent_tokenizer = SentenceTokenizer()\nword_tokenizer = WordTokenizer()\n\n# with open('../input/clrauxdata/dale-chall-3000-words.txt') as f:\n#     words = f.readlines()[0].split()\n    \n# common_words = dict(zip(words, [True] * len(words)))\n# # df.sent_cnt.plot(kind='kde')\n\nfeats_to_drop = ['sents_n', 'words_n', 'long_words_n',\n                 #'difficult_words_n',\n                 'content_words_n', 'prons_n', 'chars_n', 'syllables_n']\n\n\ndoc_feats = ['chars_per_word', 'chars_per_sent', 'syllables_per_word',\n       'syllables_per_sent', 'words_per_sent', 'long_words_doc_ratio',\n       'difficult_words_doc_ratio', 'prons_doc_ratio', 'flesch_reading_ease',\n       'flesch_kincaid_grade', 'ari', 'cli', 'gunning_fog', 'lix', 'rix',\n       'smog', 'dcrs', 'lexical_diversity', 'content_diversity', 'lwf']\n\ndef create_handcrafted_features(df):\n    df['sents_n'] = df.excerpt.apply(textstat.sentence_count)\n    df['words_n'] = df.excerpt.apply(textstat.lexicon_count)\n    df['long_words_n'] = df.excerpt.apply(lambda t: readcalc.ReadCalc(t).get_words_longer_than_X(6))\n    #df['difficult_words_n'] = df.excerpt.apply(lambda t: sum([bool(common_words.get(word)) for word in word_tokenizer.tokenize(t, include_punc=False)]))\n    df['content_words_n'] = df.excerpt.apply(lambda t: count_poss(t, content_poss))\n    df['prons_n'] = df.excerpt.apply(lambda t: count_poss(t, ['PRON']))\n    df['chars_n'] = df.excerpt.str.len()\n    df['syllables_n'] = df.excerpt.apply(textstat.syllable_count)\n    print('\\tstage 1 finished..')\n\n    df['chars_per_word_'] = df.chars_n / df.words_n\n    df['chars_per_sent_'] = df.chars_n / df.sents_n\n    df['syllables_per_word_'] = df.syllables_n / df.words_n\n    df['syllables_per_sent_'] = df.syllables_n / df.sents_n\n\n    df['words_per_sent_'] = df.words_n / df.sents_n\n    df['long_words_doc_ratio_'] = df.long_words_n / df.words_n\n    #df['difficult_words_doc_ratio'] = df.difficult_words_n / df.words_n\n    df['prons_doc_ratio'] = df.prons_n / df.words_n\n\n    print('\\tstage 2 finished..')\n\n    df['flesch_reading_ease_'] = df.excerpt.apply(textstat.flesch_reading_ease)\n    df['flesch_kincaid_grade_'] = df.excerpt.apply(textstat.flesch_kincaid_grade)\n    df['ari_'] = df.excerpt.apply(textstat.automated_readability_index)\n    df['cli_'] = df.excerpt.apply(textstat.coleman_liau_index)\n    df['gunning_fog'] = df.excerpt.apply(textstat.gunning_fog)\n\n    df['lix_'] = df.excerpt.apply(lambda t: readcalc.ReadCalc(t).get_lix_index())\n    df['rix_'] = df.long_words_n / df.sents_n\n    df['smog_'] = df.excerpt.apply(lambda t: readcalc.ReadCalc(t).get_smog_index())\n    df['dcrs_'] = df.excerpt.apply(textstat.dale_chall_readability_score)\n\n    df['lexical_diversity_'] = len(set(df.words_n)) / df.words_n\n    df['content_diversity_'] = df.content_words_n / df.words_n\n    df['lwf_'] = df.excerpt.apply(textstat.linsear_write_formula)\n\n    print('\\tstage 3 finished..')\n    return df\n\n\n# train_df = create_handcrafted_features(train_df)\n# train_df.drop(feats_to_drop, inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:05:00.787537Z","iopub.execute_input":"2021-07-28T19:05:00.787954Z","iopub.status.idle":"2021-07-28T19:05:03.570374Z","shell.execute_reply.started":"2021-07-28T19:05:00.78791Z","shell.execute_reply":"2021-07-28T19:05:03.569367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TextStat","metadata":{}},{"cell_type":"code","source":"def text_2_statistics(data):\n    flesch_reading_ease_list, smog_index_list = [], []\n    flesch_kincaid_grade_list, coleman_liau_index_list = [], []\n    automated_readability_index_list, dale_chall_readability_score_list = [], []\n    difficult_words_list, linsear_write_formula_list = [], []\n    gunning_fog_list, text_standard_list = [], []\n    fernandez_huerta_list, szigriszt_pazos_list = [], []\n    gutierrez_polini_list, crawford_list = [], []\n    \n     \n    for sentence in progress_bar(data['excerpt']):\n        flesch_reading_ease_list.append(textstat.flesch_reading_ease(sentence))\n        smog_index_list.append(textstat.smog_index(sentence))\n        flesch_kincaid_grade_list.append(textstat.flesch_kincaid_grade(sentence))\n        coleman_liau_index_list.append(textstat.coleman_liau_index(sentence))\n        automated_readability_index_list.append(textstat.automated_readability_index(sentence))\n        dale_chall_readability_score_list.append(textstat.dale_chall_readability_score(sentence))\n        difficult_words_list.append(textstat.difficult_words(sentence))\n        linsear_write_formula_list.append(textstat.linsear_write_formula(sentence))\n        gunning_fog_list.append(textstat.gunning_fog(sentence))\n        text_standard_list.append(textstat.text_standard(sentence, float_output=True))\n        fernandez_huerta_list.append(textstat.fernandez_huerta(sentence))\n        szigriszt_pazos_list.append(textstat.szigriszt_pazos(sentence))\n        gutierrez_polini_list.append(textstat.gutierrez_polini(sentence))\n        crawford_list.append(textstat.crawford(sentence))\n        \n    statistics_dict = {'flesch_reading_ease':flesch_reading_ease_list,\n                       'smog_index' : smog_index_list,\n                       'flesch_kincaid_grade' : flesch_kincaid_grade_list,\n                       'coleman_liau_index' : coleman_liau_index_list,\n                       'automated_readability_index' : automated_readability_index_list, \n                       'dale_chall_readability_score' : dale_chall_readability_score_list, \n                       'difficult_words' : difficult_words_list,\n                       'linsear_write_formula' : linsear_write_formula_list,\n                       'gunning_fog' : gunning_fog_list,\n                       'text_standard' : text_standard_list,\n                       'fernandez_huerta' : fernandez_huerta_list,\n                       'szigriszt_pazos' : szigriszt_pazos_list,\n                       'gutierrez_polini' : gutierrez_polini_list,\n                       'crawford' : crawford_list\n                      }\n    return statistics_dict\n\n\nstatistics_dict = text_2_statistics(train_df)\nfor k,v in statistics_dict.items():\n    train_df[k] = v\n            \n    \n\n#train_txt_stat = pd.DataFrame(statistics_dict)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:05:03.571887Z","iopub.execute_input":"2021-07-28T19:05:03.572286Z","iopub.status.idle":"2021-07-28T19:05:11.364345Z","shell.execute_reply.started":"2021-07-28T19:05:03.572244Z","shell.execute_reply":"2021-07-28T19:05:11.363278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TF-IDF","metadata":{}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=1000)\n\ntrain_bags = vectorizer.fit_transform(train_df['excerpt_preprocessed'].values).toarray()\ntrain_bag_of_words_df = pd.DataFrame(train_bags)\ntrain_bag_of_words_df.columns = vectorizer.get_feature_names()\n \n\n        \nfor col in train_bag_of_words_df.columns:\n    train_df[col] = train_bag_of_words_df[col].values\n\ndel train_bag_of_words_df\n# train_df.head()\n\n    \n#  -------------------\n\ndef count_words_in_sentences(data):\n    counts = []\n    for sentence in progress_bar(data['excerpt_preprocessed']):\n        words = sentence.split()\n        counts.append(len(words))\n        \n    return counts\n\ntrain_df['excerpt_word_counts_by_preprocessed'] = count_words_in_sentences(train_df)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:05:11.366007Z","iopub.execute_input":"2021-07-28T19:05:11.366441Z","iopub.status.idle":"2021-07-28T19:05:12.483062Z","shell.execute_reply.started":"2021-07-28T19:05:11.366391Z","shell.execute_reply":"2021-07-28T19:05:12.481945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NLTK features","metadata":{}},{"cell_type":"code","source":"from typing import List, Dict, Union\n\nimport nltk\nimport numpy as np\nfrom nltk import ne_chunk, pos_tag, word_tokenize\nfrom nltk.tree import Tree\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:05:12.48456Z","iopub.execute_input":"2021-07-28T19:05:12.484965Z","iopub.status.idle":"2021-07-28T19:05:12.490204Z","shell.execute_reply.started":"2021-07-28T19:05:12.484924Z","shell.execute_reply":"2021-07-28T19:05:12.489053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:05:12.492019Z","iopub.execute_input":"2021-07-28T19:05:12.492489Z","iopub.status.idle":"2021-07-28T19:05:13.246724Z","shell.execute_reply.started":"2021-07-28T19:05:12.492425Z","shell.execute_reply":"2021-07-28T19:05:13.2456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef get_named_entities(text: str) -> List[str]:\n    continuous_chunk = []\n    current_chunk = []\n\n    for i in ne_chunk(pos_tag(word_tokenize(text))):\n        if isinstance(i, Tree):\n            current_chunk.append(\" \".join(token for token, pos in i.leaves()))\n        elif current_chunk:\n            named_entity = \" \".join(current_chunk)\n            continuous_chunk.append(named_entity)\n            current_chunk = []\n\n    named_entity = \" \".join(current_chunk)\n    continuous_chunk.append(named_entity)\n\n    return continuous_chunk\n\n\n_raw_tags = frozenset(\n    {\n        'LS', 'TO', 'VBN', \"''\",\n        'WP', 'UH', 'VBG', 'JJ',\n        'VBZ', '--', 'VBP', 'NN',\n        'DT', 'PRP', ':', 'WP$',\n        'NNPS', 'PRP$', 'WDT',\n        '(', ')', '.', ',', '``',\n        '$', 'RB', 'RBR', 'RBS',\n        'VBD', 'IN', 'FW', 'RP',\n        'JJR', 'JJS', 'PDT', 'MD',\n        'VB', 'WRB', 'NNP', 'EX',\n        'NNS', 'SYM', 'CC', 'CD', 'POS'\n    }\n)\n\n_general_tags = frozenset(\n    {'gVB', 'gNN', 'gPR', 'gWP', 'gRB', 'gJJ'}\n)\n\n_tagset = (\n    *_raw_tags,\n    *_general_tags\n)\n\n\ndef generate_text_features(text: str) -> Dict[str, Union[int, float]]:\n    total_count = dict.fromkeys(_tagset, 0)\n    tokenized_text = nltk.word_tokenize(text)\n    inv_text_len = 1 / len(tokenized_text)\n    for word, pos in nltk.pos_tag(tokenized_text):\n        total_count[pos] += inv_text_len\n        general_tag = f'g{pos[:2]}'\n        if general_tag in _general_tags:\n            total_count[general_tag] += inv_text_len\n\n    max_in_sent = dict.fromkeys(_tagset, 0)\n    min_in_sent = dict.fromkeys(_tagset, 0)\n    mean_in_sent = dict.fromkeys(_tagset, 0)\n    general_tags = set()\n    tags = set()\n\n    sentences = nltk.sent_tokenize(text)\n    num_sentences = len(sentences)\n    num_words = []\n    words_len = []\n\n    for sentence in map(nltk.word_tokenize, sentences):\n        cur_sentence_stat = dict.fromkeys(_tagset, 0)\n        num_words.append(len(sentence))\n        inv_sent_len = 1 / len(sentence)\n        for word, pos in nltk.pos_tag(sentence):\n            words_len.append(len(word))\n            cur_sentence_stat[pos] += inv_sent_len\n            tags.add(pos)\n            general_tag = f'g{pos[:2]}'\n            if general_tag in _general_tags:\n                general_tags.add(general_tag)\n                cur_sentence_stat[general_tag] += inv_sent_len\n        for tag in _tagset:\n            max_in_sent[tag] = max(max_in_sent[tag], cur_sentence_stat[tag])\n            min_in_sent[tag] = min(min_in_sent[tag], cur_sentence_stat[tag])\n            mean_in_sent[tag] += cur_sentence_stat[tag] / num_sentences\n\n    res = {}\n    for k, v in total_count.items():\n        res[f'TOTAL_{k}'] = v\n    for k, v in max_in_sent.items():\n        res[f'MAX_{k}'] = v\n    for k, v in min_in_sent.items():\n        res[f'MIN_{k}'] = v\n    for k, v in mean_in_sent.items():\n        res[f'MEAN_{k}'] = v\n\n    num_words = np.array(num_words)\n    words_len = np.array(words_len)\n    res['NUM_SENTENCES'] = len(num_words)\n    res['MEAN_NUM_WORDS'] = num_words.mean()\n    res['STD_NUM_WORDS'] = num_words.std()\n    res['NUM_WORDS'] = len(words_len)\n    res['MEAN_WORD_LEN'] = words_len.mean()\n    res['STD_WORD_LEN'] = words_len.std()\n    res['TAGS_UNIQUE'] = len(tags)\n    res['GENERAL_TAGS_UNIQUE'] = len(general_tags)\n\n    named_entities = get_named_entities(text)\n    res['NAMED_ENTITIES_PER_SENTENCE'] = len(named_entities) / num_sentences\n    res['UNIQUE_NAMED_ENTITIES_PER_SENTENCE'] = len(set(named_entities)) / num_sentences\n    return res\n\n\ndef max_word_lenght(sentence):\n    words = sentence.split()\n    average = max(len(word) for word in words)\n    return average\n\n\ndef get_all_nltk_feats(text):\n    res = generate_text_features(text)\n    res['number_get_named_entities'] = len(get_named_entities(text))\n    res['max_word_lenght'] = max_word_lenght(text)\n    new_res = {}\n    for k, v in res.items():\n        new_res[k] = [v]\n    \n    return new_res\n    \n# txt = 'Say hello to my little friend, Bro! I love you, Sarra!'\n# nltk_feats = get_all_nltk_feats(txt)\n# nltk_feats\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:05:13.250667Z","iopub.execute_input":"2021-07-28T19:05:13.251008Z","iopub.status.idle":"2021-07-28T19:05:13.280698Z","shell.execute_reply.started":"2021-07-28T19:05:13.250974Z","shell.execute_reply":"2021-07-28T19:05:13.279572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#txt = 'Say hello to my little friend, Bro! I love you, Sarra!'\n#nltk_feats = count_part_of_speechs(txt)\n\nnltk_feats_df = pd.DataFrame()\nfor txt in progress_bar(train_df['excerpt']):\n    nltk_feats_dict = get_all_nltk_feats(txt)\n    nltk_feats_df = nltk_feats_df.append(pd.DataFrame(nltk_feats_dict))\n     \n        \n        \nfor col in nltk_feats_df.columns:\n    train_df[col] = nltk_feats_df[col].values\n\n\ntrain_df.head()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:05:13.282335Z","iopub.execute_input":"2021-07-28T19:05:13.282745Z","iopub.status.idle":"2021-07-28T19:17:27.697909Z","shell.execute_reply.started":"2021-07-28T19:05:13.2827Z","shell.execute_reply":"2021-07-28T19:17:27.696674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# K-Best filtering","metadata":{}},{"cell_type":"code","source":"# from sklearn.feature_selection import SelectKBest, f_regression\n\n# nltk_feats_df.fillna(0, inplace=True)\n\n\n# feature_names = list(nltk_feats_df.columns.values)\n\n# kb = SelectKBest(f_regression, k=60)\n# kb.fit(nltk_feats_df, train_df['target'])\n\n\n# mask = kb.get_support() #list of booleans\n# new_features = [] # The list of your K best features\n\n# for bool, feature in zip(mask, feature_names):\n#     if bool:\n#         new_features.append(feature)\n\n# nltk_feats_df = pd.DataFrame(kb.transform(nltk_feats_df), columns=new_features)\n# # nltk_feats_df = pd.DataFrame(kb.transform(X_test))\n\n\n ","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:17:27.699623Z","iopub.execute_input":"2021-07-28T19:17:27.700076Z","iopub.status.idle":"2021-07-28T19:17:27.705761Z","shell.execute_reply.started":"2021-07-28T19:17:27.700029Z","shell.execute_reply":"2021-07-28T19:17:27.70432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk_feats_df['target'] = train_df['target']\n\ncorr = abs(nltk_feats_df.corr())\n \n\nimport seaborn as sns\n%matplotlib inline\n\n# calculate the correlation matrix\n#corr = abs(train_df.corr())\n\n\nfrom matplotlib.pyplot import figure\n\nfigure(figsize=(10, 32), dpi=100)\n\n# plot the heatmap\nsns.heatmap(corr, \n        xticklabels=['target'],\n        yticklabels=corr.columns)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:17:27.707691Z","iopub.execute_input":"2021-07-28T19:17:27.708222Z","iopub.status.idle":"2021-07-28T19:17:32.749635Z","shell.execute_reply.started":"2021-07-28T19:17:27.70815Z","shell.execute_reply":"2021-07-28T19:17:32.748485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(df):\n    df['len_tokens'] = df['excerpt'].str.strip().str.split(' ').apply(len)\n    df['len'] = df['excerpt'].str.strip().apply(len)\n    df['len_sent'] = df['excerpt'].str.strip().str.split('.').apply(len)\n    df['n_comm'] = df['excerpt'].str.strip().str.split(',').apply(len)\n    _t = df['excerpt'].str.strip().str.split(' ').values\n    df['d_mean'] = [np.sum([j.isdigit() for j in i]) for i in _t]\n    df['u_mean'] = [np.sum([j.isupper() for j in i]) for i in _t]\n    \npreprocess_text(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:17:32.750912Z","iopub.execute_input":"2021-07-28T19:17:32.751309Z","iopub.status.idle":"2021-07-28T19:17:33.181933Z","shell.execute_reply.started":"2021-07-28T19:17:32.751268Z","shell.execute_reply":"2021-07-28T19:17:33.180943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Важно проверить число вот тут!\n\nprint(train_df.shape)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:17:33.183463Z","iopub.execute_input":"2021-07-28T19:17:33.183863Z","iopub.status.idle":"2021-07-28T19:17:33.219386Z","shell.execute_reply.started":"2021-07-28T19:17:33.18382Z","shell.execute_reply":"2021-07-28T19:17:33.218133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"automl = TabularNLPAutoML(task=task,\n                          timeout=TIMEOUT,\n                          general_params={'nested_cv': False, 'use_algos': [['linear_l2','nn', 'lgb', 'lgb_tuned', 'cb',]]},\n                          text_params={'lang': 'en', 'bert_model': '../input/roberta-base'},\n                          reader_params={'cv': 5},\n                          selection_params={'mode': 1},\n                          linear_pipeline_params={'text_features': 'embed'},\n                          autonlp_params={'model_name': 'pooled_bert',\n                                          'transformer_params': {'model_params': {'pooling': 'mean'},\n                                                                 'dataset_params': {'max_length': 220}, # поменял max_length. было 220\n                                                                 'loader_params': {'batch_size': 64,\n                                                                                   'shuffle': False,\n                                                                                   'num_workers': 4}\n                                                                 }\n                                          },\n                          nn_params={'opt_params': {'lr': 3e-5},\n                                     'lang': 'en',\n                                     'path_to_save': './models',\n                                     'bert_name': '../input/roberta-base',\n                                     'snap_params': {'k': 1, 'early_stopping': True,\n                                                     'patience': 2, 'swa': False},\n                                     'init_bias': False,\n                                     'pooling': 'mean',\n                                     'max_length': 220, 'bs': 32, 'n_epochs': 20, # поменял max_length. было 220\n                                     'use_cont': False,\n                                     'use_cat': False,\n                                     },\n                          )\n\noof_pred = automl.fit_predict(train_df, roles=roles)\nprint('')\nprint(rmse(train_df[TARGET_NAME], oof_pred.data[:, 0]))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-28T19:17:33.22132Z","iopub.execute_input":"2021-07-28T19:17:33.221759Z","iopub.status.idle":"2021-07-28T19:21:30.720132Z","shell.execute_reply.started":"2021-07-28T19:17:33.221713Z","shell.execute_reply":"2021-07-28T19:21:30.716842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightautoml.addons.interpretation import LimeTextExplainer\nlime = LimeTextExplainer(automl, feature_selection='lasso', force_order=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:30.721597Z","iopub.status.idle":"2021-07-28T19:21:30.722441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train_df.iloc[0]\nexp = lime.explain_instance(df, perturb_column='excerpt')\nexp.visualize_in_notebook()\nprint(df[TARGET_NAME])","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:30.723755Z","iopub.status.idle":"2021-07-28T19:21:30.724427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train_df.iloc[1]\nexp = lime.explain_instance(df, perturb_column='excerpt')\nexp.visualize_in_notebook()\nprint(df[TARGET_NAME])","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:30.725676Z","iopub.status.idle":"2021-07-28T19:21:30.726415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = train_df.iloc[100]\n# exp = lime.explain_instance(df, perturb_column='excerpt')\n# exp.visualize_in_notebook()\n# print(df[TARGET_NAME])","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:30.727791Z","iopub.status.idle":"2021-07-28T19:21:30.728526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train_df.iloc[777]\nexp = lime.explain_instance(df, perturb_column='excerpt')\nexp.visualize_in_notebook()\nprint(df[TARGET_NAME])","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:30.72985Z","iopub.status.idle":"2021-07-28T19:21:30.730597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = train_df.iloc[2222]\n# exp = lime.explain_instance(df, perturb_column='excerpt')\n# exp.visualize_in_notebook()\n# print(df[TARGET_NAME])","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:30.732028Z","iopub.status.idle":"2021-07-28T19:21:30.732763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('LAMA_model.pkl', 'wb') as f:\n    pickle.dump(automl, f)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:30.73415Z","iopub.status.idle":"2021-07-28T19:21:30.734892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm text* -r\n!rm pyp* -r","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:30.736267Z","iopub.status.idle":"2021-07-28T19:21:30.73697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2021-07-28T19:21:30.738307Z","iopub.status.idle":"2021-07-28T19:21:30.739014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}