{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize as wt \nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport gensim\nfrom gensim.models import Word2Vec\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-21T05:19:45.910774Z","iopub.execute_input":"2021-06-21T05:19:45.91116Z","iopub.status.idle":"2021-06-21T05:19:45.929371Z","shell.execute_reply.started":"2021-06-21T05:19:45.911125Z","shell.execute_reply":"2021-06-21T05:19:45.928395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting global variables to input datasets\ndatapathTrain = '/kaggle/input/commonlitreadabilityprize/train.csv'\ndatapathTest = '/kaggle/input/commonlitreadabilityprize/test.csv'\n\nword2vecModel = gensim.models.KeyedVectors.load_word2vec_format(\"/kaggle/input/google-pretrain-model/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n\ncolumns=['id','url','licence','text','score','stdev']\nkeepColumns = ['id','text','score','stdev']","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:19:45.931081Z","iopub.execute_input":"2021-06-21T05:19:45.93146Z","iopub.status.idle":"2021-06-21T05:20:49.663315Z","shell.execute_reply.started":"2021-06-21T05:19:45.931422Z","shell.execute_reply":"2021-06-21T05:20:49.662371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to read csv into dataframe\ndef csv2dataset(datapath,columns=columns,keepColumns=keepColumns):\n    dataset = pd.read_csv(datapath,names=columns,skiprows=[0])\n    return dataset[keepColumns]    ","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:20:49.66746Z","iopub.execute_input":"2021-06-21T05:20:49.667716Z","iopub.status.idle":"2021-06-21T05:20:49.672028Z","shell.execute_reply.started":"2021-06-21T05:20:49.667691Z","shell.execute_reply":"2021-06-21T05:20:49.67116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to cleanup and tokenize the text \ndef datasetTokenizer(dataset):\n    dataset['tokenized_text'] = dataset['text']\n    for i in range(dataset.shape[0]):\n        sms = dataset['text'].iloc[i]\n        # remove non alphabatic characters\n        sms = re.sub('[^A-Za-z]', ' ', sms)\n        # make words lowercase, because Go and go will be considered as two words\n        sms = sms.lower()\n        # tokenising\n        tokenized_sms = wt(sms)\n        # remove stop words and stemming\n        sms_processed = []\n        for word in tokenized_sms:\n            if word not in set(stopwords.words('english')):\n                sms_processed.append(stemmer.stem(word))\n\n        sms_text = \" \".join(sms_processed)\n        dataset['tokenized_text'].iloc[i] = sms_text # update this line to avoid copy warning\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:20:49.673683Z","iopub.execute_input":"2021-06-21T05:20:49.674062Z","iopub.status.idle":"2021-06-21T05:20:49.688418Z","shell.execute_reply.started":"2021-06-21T05:20:49.674025Z","shell.execute_reply":"2021-06-21T05:20:49.687611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating the feature matrix \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef avg_feature_vector(sentence, model, num_features):\n    words = sentence.replace('\\n',\" \").replace(',',' ').replace('.',\" \").split()\n    feature_vec = np.zeros((num_features,),dtype=\"float32\")\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, model[word])\n        except KeyError as error:\n            feature_vec \n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec\n\ndef w2vsequencer(sentence, model, num_features, rating_mean, rating_stdev):\n    words = sentence.replace('\\n',\" \").replace(',',' ').replace('.',\" \").split()\n    feature_vec = np.zeros((num_features,),dtype=\"float32\")\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, model[word])\n        except KeyError as error:\n            feature_vec \n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec\n\ndef datasetVectorizer(dataset):\n    vectorizer = CountVectorizer(max_features=1000) #Bag of words\n    bow = vectorizer.fit_transform(dataset['tokenized_text']).toarray()\n    dataset['BagOfWords_text'] = dataset['tokenized_text']\n\n    vectorizer = TfidfVectorizer(max_features=1000) #TFIDF\n    tfidf = vectorizer.fit_transform(dataset['tokenized_text']).toarray()\n    dataset['TFIDF_text'] = dataset['tokenized_text']\n\n    word2vec_train = np.zeros((dataset.shape[0],300),dtype=\"float32\")\n    dataset['Word2Vec_text'] = dataset['tokenized_text']\n\n    for i in range(dataset.shape[0]):\n        dataset['BagOfWords_text'].iloc[i] = bow[i]\n        dataset['TFIDF_text'].iloc[i] = tfidf[i]\n        dataset['Word2Vec_text'].iloc[i] = avg_feature_vector(dataset['tokenized_text'][i],word2vecModel, 300)\n\n    return dataset\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:20:49.689759Z","iopub.execute_input":"2021-06-21T05:20:49.690063Z","iopub.status.idle":"2021-06-21T05:20:49.706264Z","shell.execute_reply.started":"2021-06-21T05:20:49.690037Z","shell.execute_reply":"2021-06-21T05:20:49.705522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasetTrain = csv2dataset(datapathTrain)\ndatasetTrain = datasetTokenizer(datasetTrain)\ndatasetTrain = datasetVectorizer(datasetTrain)\ndatasetTrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:20:49.70745Z","iopub.execute_input":"2021-06-21T05:20:49.70793Z","iopub.status.idle":"2021-06-21T05:22:10.761777Z","shell.execute_reply.started":"2021-06-21T05:20:49.707874Z","shell.execute_reply":"2021-06-21T05:22:10.760994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datasetTest = csv2dataset(datapathTest)\ndatasetTest = datasetTokenizer(datasetTest)\ndatasetTest = datasetVectorizer(datasetTest)\ndatasetTest.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:22:10.763241Z","iopub.execute_input":"2021-06-21T05:22:10.763635Z","iopub.status.idle":"2021-06-21T05:22:10.995622Z","shell.execute_reply.started":"2021-06-21T05:22:10.763594Z","shell.execute_reply":"2021-06-21T05:22:10.994714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.Dataset(data)\ndf.to_csv(CLds, index = false)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T05:22:10.997706Z","iopub.execute_input":"2021-06-21T05:22:10.998078Z","iopub.status.idle":"2021-06-21T05:22:11.020078Z","shell.execute_reply.started":"2021-06-21T05:22:10.998041Z","shell.execute_reply":"2021-06-21T05:22:11.018601Z"},"trusted":true},"execution_count":null,"outputs":[]}]}