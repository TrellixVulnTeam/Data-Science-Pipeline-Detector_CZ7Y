{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-22T15:41:15.905561Z","iopub.execute_input":"2021-07-22T15:41:15.905898Z","iopub.status.idle":"2021-07-22T15:41:16.013929Z","shell.execute_reply.started":"2021-07-22T15:41:15.905815Z","shell.execute_reply":"2021-07-22T15:41:16.012724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ライブラリのインポート\nimport re\nimport string\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom gensim.models.doc2vec import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument\n\n# データセットの読み込み\ntrain_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsub_data = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\n# train_dataのurl_legalのドメイン抽出とurl_legal&licenseのNaN埋め(blank)\ntrain_data_url = train_data.iloc[:, 0:2]\ntrain_data_url.iloc[:, 1] = train_data_url.iloc[:, 1].fillna('blank')\ntrain_data_url = train_data_url.replace('https://', '', regex=True)\ntrain_data_url = train_data_url.replace('http://', '', regex=True)\n\nfor i in range(len(train_data_url['id'])):\n    if train_data_url.iloc[i, 1] != 'blank':\n        end = train_data_url.iloc[i, 1].index('/')\n        train_data_url.iloc[i, 1] = train_data_url.iloc[i, 1][:end]\n\ntrain_data_url = train_data_url.rename(columns={'url_legal':'url_legal_domain'})\ntrain_data_url.insert(1, 'url_legal', train_data['url_legal'])\ntrain_data_url['license'] = train_data['license']\ntrain_data_url['excerpt'] = train_data['excerpt']\ntrain_data_url['target'] = train_data['target']\ntrain_data_url['standard_error'] = train_data['standard_error']\n\ntrain_data_url.iloc[:, 1] = train_data_url.iloc[:, 1].fillna('blank')\ntrain_data_url.iloc[:, 3] = train_data_url.iloc[:, 3].fillna('blank')\n\n# 学習データの準備 (train_data_url)\ntrain_data_rep = []\ntrain_data_url['taggedDocument'] = \"\"\n\nfor i in range(len(train_data_url['excerpt'])):\n    train_data_rep.append(train_data_url['excerpt'][i].replace('\\n', ' '))\n    w = list(map(str, train_data_rep[i].split(\" \")))\n    s = TaggedDocument(words = w, tags = [train_data_url['id'][i]])\n    train_data_url['taggedDocument'][i] = s\n\n# 学習データの準備 (test_data)\ntest_data_rep = []\ntest_data['taggedDocument'] = \"\"\n\nfor i in range(len(test_data['excerpt'])):\n    test_data_rep.append(test_data['excerpt'][i].replace('\\n', ' '))\n    w = list(map(str, test_data_rep[i].split(\" \")))\n    s = TaggedDocument(words = w, tags = [test_data['id'][i]])\n    test_data['taggedDocument'][i] = s\n\n# 単語のクリーニング (train_data_url)\nfor i in range(len(train_data_url['taggedDocument'])):\n    for j in range(len(train_data_url['taggedDocument'][i][0])):\n        for k in range(3):\n            if train_data_url['taggedDocument'][i][0][j].endswith('.'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            if train_data_url['taggedDocument'][i][0][j].endswith(','):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            if train_data_url['taggedDocument'][i][0][j].endswith(':'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            if train_data_url['taggedDocument'][i][0][j].endswith(';'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            if train_data_url['taggedDocument'][i][0][j].endswith(')'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            if train_data_url['taggedDocument'][i][0][j].endswith(']'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            if train_data_url['taggedDocument'][i][0][j].endswith('}'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            if train_data_url['taggedDocument'][i][0][j].endswith('>'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            if train_data_url['taggedDocument'][i][0][j].endswith('\\''):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            if train_data_url['taggedDocument'][i][0][j].endswith('\\\"'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            if train_data_url['taggedDocument'][i][0][j].endswith('!'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            if train_data_url['taggedDocument'][i][0][j].endswith('?'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][:-1]\n            \n            if train_data_url['taggedDocument'][i][0][j].startswith('.'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n            if train_data_url['taggedDocument'][i][0][j].startswith(','):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n            if train_data_url['taggedDocument'][i][0][j].startswith(':'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n            if train_data_url['taggedDocument'][i][0][j].startswith(';'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n            if train_data_url['taggedDocument'][i][0][j].startswith('('):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n            if train_data_url['taggedDocument'][i][0][j].startswith('['):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n            if train_data_url['taggedDocument'][i][0][j].startswith('{'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n            if train_data_url['taggedDocument'][i][0][j].startswith('<'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n            if train_data_url['taggedDocument'][i][0][j].startswith('\\''):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n            if train_data_url['taggedDocument'][i][0][j].startswith('\\\"'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n            if train_data_url['taggedDocument'][i][0][j].startswith('!'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n            if train_data_url['taggedDocument'][i][0][j].startswith('?'):\n                train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j][1:]\n        \n        train_data_url['taggedDocument'][i][0][j] = train_data_url['taggedDocument'][i][0][j].lower()\n\n# 単語のクリーニング (test_data)\nfor i in range(len(test_data['taggedDocument'])):\n    for j in range(len(test_data['taggedDocument'][i][0])):\n        for k in range(3):\n            if test_data['taggedDocument'][i][0][j].endswith('.'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            if test_data['taggedDocument'][i][0][j].endswith(','):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            if test_data['taggedDocument'][i][0][j].endswith(':'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            if test_data['taggedDocument'][i][0][j].endswith(';'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            if test_data['taggedDocument'][i][0][j].endswith(')'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            if test_data['taggedDocument'][i][0][j].endswith(']'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            if test_data['taggedDocument'][i][0][j].endswith('}'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            if test_data['taggedDocument'][i][0][j].endswith('>'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            if test_data['taggedDocument'][i][0][j].endswith('\\''):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            if test_data['taggedDocument'][i][0][j].endswith('\\\"'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            if test_data['taggedDocument'][i][0][j].endswith('!'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            if test_data['taggedDocument'][i][0][j].endswith('?'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][:-1]\n            \n            if test_data['taggedDocument'][i][0][j].startswith('.'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n            if test_data['taggedDocument'][i][0][j].startswith(','):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n            if test_data['taggedDocument'][i][0][j].startswith(':'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n            if test_data['taggedDocument'][i][0][j].startswith(';'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n            if test_data['taggedDocument'][i][0][j].startswith('('):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n            if test_data['taggedDocument'][i][0][j].startswith('['):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n            if test_data['taggedDocument'][i][0][j].startswith('{'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n            if test_data['taggedDocument'][i][0][j].startswith('<'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n            if test_data['taggedDocument'][i][0][j].startswith('\\''):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n            if test_data['taggedDocument'][i][0][j].startswith('\\\"'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n            if test_data['taggedDocument'][i][0][j].startswith('!'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n            if test_data['taggedDocument'][i][0][j].startswith('?'):\n                test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j][1:]\n        \n        test_data['taggedDocument'][i][0][j] = test_data['taggedDocument'][i][0][j].lower()\n\n# train dataとtest dataのマージ\ntrain_docs = train_data_url['taggedDocument'].values.tolist() + test_data['taggedDocument'].values.tolist()\n\n# 学習の実行\nmodel = Doc2Vec(\n    documents = train_docs,\n    dm = 1,\n    vector_size = 300,\n    window = 15,\n    alpha = 0.0025,\n    workers = 4,\n    epochs = 300,\n    hs = 0,\n    negative = 5,\n    min_count = 1)\n\n# Doc2Vecの学習済みモデルからベクトルを特徴量として抽出\ntrain_docvecs_df = pd.DataFrame()\ntest_docvecs_df = pd.DataFrame()\n\nfor did in train_data_url[\"id\"]:\n    train_docvecs_df[did] = model.dv[did]\n\nfor did in test_data[\"id\"]:\n    test_docvecs_df[did] = model.dv[did]\n\ntrain_docvecs_df = train_docvecs_df.T\ntrain_docvecs_df = train_docvecs_df.rename_axis('id').reset_index()\n\ntest_docvecs_df = test_docvecs_df.T\ntest_docvecs_df = test_docvecs_df.rename_axis('id').reset_index()\n\n# LightGBMでの訓練データと評価データ\ntrain_X, val_X, train_y, val_y = train_test_split(train_docvecs_df.drop('id', axis=1), train_data_url['target'], test_size = 0.33, random_state=42)\nlgb_train = lgb.Dataset(train_X.values, train_y.values)\nlgb_eval = lgb.Dataset(val_X.values, val_y.values, reference=lgb_train)\n\n# LightGBMで学習実行\nparams = {\n    # 回帰問題\n    'objective': 'regression',\n    # RMSEで評価\n    'metric': 'rmse',\n    'max_depth': 3\n}\n\nlgbModel = lgb.train(params, lgb_train, valid_sets = lgb_eval,\n                     verbose_eval = 100,  # 50イテレーション毎に学習結果出力\n                     num_boost_round = 1000,  # 最大イテレーション回数指定\n                     early_stopping_rounds = 500\n                    )\n\n# 評価用データで予測\nfrom sklearn import metrics\ny_pred = lgbModel.predict(val_X.values, num_iteration = lgbModel.best_iteration)\nrmse = np.sqrt(metrics.mean_squared_error(val_y.values, y_pred))\n\n# testデータで予測\npredicted = lgbModel.predict(test_docvecs_df.drop('id', axis = 1).values, num_iteration = lgbModel.best_iteration)\n\n# 提出用データの作成\nsub_data['target'] = predicted\nsub_data.to_csv('submission.csv', index = False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T15:41:16.021652Z","iopub.execute_input":"2021-07-22T15:41:16.021951Z"},"trusted":true},"execution_count":null,"outputs":[]}]}