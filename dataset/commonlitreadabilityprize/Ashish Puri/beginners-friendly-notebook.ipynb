{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Version update: <br>\nAdded Word to Vec <br>\nDetailed Notebook with visualizations: https://www.kaggle.com/getitdone/commonlit-word-to-vec-with-umap","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's use Pandas Profiling library to build an intuition around train and test data","metadata":{}},{"cell_type":"markdown","source":"# **Basic EDA using Pandas Profiling**","metadata":{}},{"cell_type":"code","source":"import pandas_profiling #pre-installed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_prof = pandas_profiling.ProfileReport(df=train)\ntest_prof = pandas_profiling.ProfileReport(df=test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_prof","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notes for train data:\n* id, target, standard_error are all unique","metadata":{}},{"cell_type":"code","source":"# test_prof","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notes for test data:\n* only 7 rows are there\n* standard_error is not provided\n","metadata":{}},{"cell_type":"markdown","source":"**Understanding \"target\"**","metadata":{}},{"cell_type":"markdown","source":"References:\n* https://www.kaggle.com/gunesevitan/commonlit-readability-prize-eda\n* https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model\n* https://www.kaggle.com/andreshg/commonlit-a-complete-analysis","metadata":{}},{"cell_type":"markdown","source":"* Target column is named as target and it is reading ease of the excerpt. The excerpt with *436ce79fe* id is set as baseline for comparisons. That's the reason why its target and standard_error values are 0. \n* Other excerpts are compared with *436ce79fe* and rated by multiple raters based on their ease of read. \n* After that, the excerpts are ranked with Bradley-Terry model. \n* Therefore, every excerpt with target value greater than 0 are easier to read and every excerpt with target value less than 0 are harder to read compared to that particular excerpt. \n* As there were multiple raters, standard_error tells us the measure of spread of scores among the raters for each excerpt.","metadata":{}},{"cell_type":"markdown","source":"**Negative Valu**e_______Zero___________**Positive Value** <br>\n*Difficult Excerpt*____ Base Line_____*_Easy Excerpt*","metadata":{}},{"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nfrom scipy import stats","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_graphs(df,feature):\n    (osm, osr), (slope, intercept, r) = stats.probplot(df[feature], plot=None)\n    fig = make_subplots(\n    rows=1, cols=2,\n\n    subplot_titles=(\n        \"Quantile-Quantile Plot of \" + feature,\n        \"Distribution Plot of \" + feature\n    )\n    )\n\n\n    fig.add_trace(go.Scatter(\n    x=osm,\n    y=slope*osm + intercept,\n    mode='lines',\n    line={\n        'color': '#c81515',\n        'width': 2.5\n    }\n\n    ), row=1, col=1)\n    fig.add_trace(go.Scatter(\n    x=osm,\n    y=osr,\n    mode='markers',\n    marker={\n        'color': '#496595'\n    }\n    ), row=1, col=1)\n    fig1 = ff.create_distplot([df[feature]],['target'], \n                         bin_size=.05, show_rug=False)\n    mean_value = df[feature].mean()\n    median_value = df[feature].median()\n\n    fig.add_trace(go.Scatter(\n    fig1['data'][1],\n    line=dict(\n\n    width=1.5,\n    ),\n    fill='tozeroy'\n    ),row=1,col=2)\n    fig.add_annotation(\n    yref=\"y domain\",\n    x=mean_value,\n    y=0.5,\n    axref=\"x\",\n    ayref=\"y domain\",\n    ax=mean_value + 0.2*mean_value,\n    ay=0.1,\n    text=f\"<span>{feature.capitalize()} mean</span>= {round(mean_value,3)}\",\n    row=1,col=2)\n    fig.add_annotation(\n    yref=\"y domain\",\n    x=median_value,\n    y=0.3,\n    axref=\"x\",\n    ayref=\"y domain\",\n    ax=median_value + 0.2*median_value,\n    ay=0.2,\n    text=f\"<span>{feature.capitalize()} median</span>= {round(median_value,3)}\",\n    row=1,col=2)\n    fig.add_vline(\n    x=mean_value, \n    line_width=2, \n    line_dash=\"dash\",row=1,col=2\n    )\n    fig.add_vline(\n    x=median_value, \n    line_width=2,line_dash=\"dash\",line_color='red' ,row=1,col=2\n\n    )\n\n    fig.update_layout(showlegend=False)\n    fig.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graphs(train,'target')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graphs(train[train['standard_error']!=0],'standard_error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(data= train[train['standard_error']!=0],\n    x='target', \n    y='standard_error', \n    kind='hex',\n    height=8,\n\n)\nplt.suptitle(\"Target vs Standard error \",font=\"Serif\", size=20)\nplt.subplots_adjust(top=0.95)\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* When standard_error is plotted against target without the baseline excerpt, a relationship can be seen. Excerpts with medium ease of read tend to have less spread of scores, however excerpts at both ends have more spread because they are either too easy or too hard for the raters. \n* Raters' subjective opinions vary a lot when they rate those easy and hard excerpts, but they give closer opinions when the excerpts have medium difficulty.","metadata":{}},{"cell_type":"markdown","source":"# Basic Features\n* Excerpt Length\n* Excerpt Word Count\n* Max Length of sentence in excerpt\n* Average Length of Sentences","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize\nfrom statistics import mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://stackoverflow.com/a/55608579\ndef min_max_mean_sentence_length(text):\n\n    tokened_sent = sent_tokenize(text)\n    main_dict = {}\n    for item in tokened_sent:\n        item1 = list(item.split(\" \"))\n        item2 = [' '.join(item1)]\n        Length = []\n        Length.append(len(item1))\n        mydict = dict(zip(item2, Length))\n        main_dict.update(mydict)\n\n    return max(main_dict.values()), min(main_dict.values()), round(mean(main_dict.values()),3)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def basic_features(_):\n    df= _.copy()\n    df['excerpt_len'] = df['excerpt'].apply(lambda x : len(x))\n    df['excerpt_word_count'] = df['excerpt'].apply(lambda x : len(x.split(' ')))\n    df[['max_len_sent','min_len_sent','avg_len_sent']] = df.apply(lambda x: min_max_mean_sentence_length(x['excerpt']),axis=1, result_type='expand')\n    return df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = basic_features(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting Basic Features","metadata":{}},{"cell_type":"code","source":"def plot_feature(feature):\n\n    fig, axes = plt.subplots(ncols=2, figsize=(32, 6))\n\n    sns.regplot(x=train['target'], y=train[feature], line_kws={'color': 'red'}, ax=axes[0])\n    sns.kdeplot(train[feature], fill=True, ax=axes[1])\n\n    axes[0].set_xlabel(f'target', size=18)\n    axes[0].set_ylabel(feature, size=18)\n    axes[1].set_xlabel('')\n    axes[1].set_ylabel('')\n    axes[1].legend(prop={'size': 15})\n    for i in range(2):\n        axes[i].tick_params(axis='x', labelsize=15)\n        axes[i].tick_params(axis='y', labelsize=15)\n    axes[0].set_title(f'target vs {feature}', size=20, pad=20)\n    axes[1].set_title(f'{feature} Distribution', size=20, pad=20)\n\n    plt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in ['excerpt_len', 'excerpt_word_count', 'min_len_sent', 'max_len_sent', 'avg_len_sent']:\n    plot_feature(feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Strong negative relationships can be seen from the scatter plots of max_sent_len, avg_sent_len.\n* Other features don't look very promising from their skewed distributions and weak relationships, but they can be still useful in terms of predictive power.","metadata":{}},{"cell_type":"markdown","source":"# Model on Basic Features ","metadata":{}},{"cell_type":"code","source":"# Split into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(columns='target'), train['target'].values, random_state=42,test_size=0.20)\nprint(len(X_train), len(y_train))\nprint(len(X_test), len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['excerpt_len', 'excerpt_word_count', 'min_len_sent', 'max_len_sent', 'avg_len_sent']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Baseline using mean value of target**","metadata":{}},{"cell_type":"code","source":"pred_y = [train['target'].mean()] * len(y_test)\nprint(f' Test RMSE when we fill predictions with mean value of target in train data is {round(np.sqrt(mean_squared_error(y_test,pred_y)),4)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LightGBM model**","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\ngbm = lgb.LGBMRegressor(random_state=42)\ngbm.fit(X_train[features],y_train,eval_metric='mse')\npred_y = gbm.predict(X_test[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f' Test RMSE using basic features {round(np.sqrt(mean_squared_error(y_test,pred_y)),4)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ~14% improvement from baseline model","metadata":{}},{"cell_type":"code","source":"test = basic_features(test)\npred_y = gbm.predict(test[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_submission(_,predictions):\n    df =_.copy()\n    df['target'] = predictions\n    return df[['id','target']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission = create_submission(test,pred_y)\n# submission.to_csv('./submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Readability Scores","metadata":{}},{"cell_type":"markdown","source":"1. [The Flesch Reading Ease formula](https://www.kaggle.com/prvnkmr/domain-knowledge-readability-score-methods?scriptVersionId=62948009&cellId=10) <br>\nRE = 206.835 – (1.015 x ASL) – (84.6 x ASW)<br>\n*RE = Readability Ease<br>\nASL = Average Sentence Length (i.e., the number of words divided by the number of sentences) <br>\nASW = Average number of syllables per word (i.e., the number of syllables divided by the number of words) <br>\nThe output, i.e., RE is a number ranging from 0 to 100. The higher the number, the easier the text is to read.*\n\n","metadata":{}},{"cell_type":"markdown","source":"2. [The Flesch-Kincaid Grade Level Readability Formula](https://www.kaggle.com/prvnkmr/domain-knowledge-readability-score-methods?scriptVersionId=62948009&cellId=11) <br>\nFKRA = (0.39 x ASL) + (11.8 x ASW) - 15.59<br>\n*FKRA = Flesch-Kincaid Reading Age<br>\nASL = Average Sentence Length (i.e., the number of words divided by the number of sentences) <br>\nASW = Average number of syllables per word (i.e., the number of syllables divided by the number of words) <br>*\n\n\n","metadata":{}},{"cell_type":"markdown","source":"3. [The fog scale](https://www.kaggle.com/prvnkmr/domain-knowledge-readability-score-methods?scriptVersionId=62948009&cellId=12) <br>\nGrade Level = 0.4 (ASL + PHW)<br>\n*ASL = Average Sentence Length (i.e., number of words divided by the number of sentences)<br>\nPHW = Percentage of Hard Words*","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/duboisian/first-draft-model?scriptVersionId=63553418&cellId=2\ndef GrunningFog(excerpt):\n    \"\"\"\n    function takes a passage and determines the grade level based on the Grunning Fog index method\n    \"\"\"\n    document = excerpt\n    document = document.replace('\\n',' ').split('.')\n    document = [x for x in document if len(x)>1]\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    words = []\n    ComplexCount = []\n    for sentence in document:\n        tokens = nltk.word_tokenize(sentence)\n        words.append(len(tokens))\n        tokens = [lemmatizer.lemmatize(x) for x in tokens]\n        Complex = [1 if syllable_count(token) >=3 else 0 for token in tokens]\n        ComplexCount.append(np.sum(Complex))\n    ASL = np.mean(words) #Average words per sentence\n    PropComplex = np.sum(ComplexCount)/np.sum(words) #proprtion of complex words (>= 3 sylables)\n    GrunFog = 0.4*(ASL + (100*PropComplex))\n    return(GrunFog)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. [The SMOG Index](https://www.geeksforgeeks.org/readability-index-pythonnlp/3) <br>","metadata":{}},{"cell_type":"code","source":"def SMOG(excerpt):\n    document = excerpt\n    document = document.replace('\\n',' ').split('.')\n    document = [x for x in document if len(x)>1]\n    words = []\n    ComplexCount = []\n    for sentence in document:\n        tokens = nltk.word_tokenize(sentence)\n        words.append(len(tokens))\n        Complex = [1 if syllable_count(token) >=3 else 0 for token in tokens]\n        ComplexCount.append(np.sum(Complex))\n    SMOGScore = (1.0430 * np.sqrt(np.sum(ComplexCount) * (30/len(words)))) + 3.1291\n    return(SMOGScore)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://stackoverflow.com/a/46759549\ndef syllable_count(word):\n    word = word.lower()\n    count = 0\n    vowels = \"aeiouy\"\n    if word[0] in vowels:\n        count += 1\n    for index in range(1, len(word)):\n        if word[index] in vowels and word[index - 1] not in vowels:\n            count += 1\n    if word.endswith(\"e\"):\n        count -= 1\n    if count == 0:\n        count += 1\n    return count","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def asw_asl(_):\n    df = _.copy()\n    df['ASL'] = df['excerpt'].apply(lambda row: np.sum([len(x.split(' ')) for x in row.replace('\\n','').split('.')])/len([len(x.split(' ')) for x in row.replace('\\n','').split('.')]))\n    df['ASW'] = df['excerpt'].apply(lambda row: np.sum([syllable_count(x) if len(x)>0 else 0 for x in row.replace('\\n','').replace('.','').split(' ')])/len([x for x in row.replace('\\n','').replace('.','').split(' ')]))\n    df['RE'] = df.apply(lambda row: 206.835 - (1.015 * row['ASL']) - (84.6 * row['ASW']),axis = 1)\n    df['FKRA'] = df.apply(lambda row: (0.39 * row['ASL']) + (11.8 * row['ASW']) -15.59 ,axis = 1)\n    df['GrunFog'] = df['excerpt'].apply(lambda row: GrunningFog(row))\n    df['SMOG'] = df['excerpt'].apply(lambda row: SMOG(row))\n    return df","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = asw_asl(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in ['RE','FKRA','GrunFog','SMOG']:\n    plot_feature(feature)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model on Basic Features + Readability Scores","metadata":{}},{"cell_type":"code","source":"# Split into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(columns='target'), train['target'].values, random_state=42,test_size=0.20)\nprint(len(X_train), len(y_train))\nprint(len(X_test), len(y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['excerpt_len', 'excerpt_word_count', 'min_len_sent', 'max_len_sent', 'avg_len_sent','ASL',\n 'ASW',\n 'RE',\n 'FKRA',\n 'GrunFog',\n 'SMOG']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightgbm as lgb\ngbm = lgb.LGBMRegressor(random_state=42)\ngbm.fit(X_train[features],y_train,eval_metric='mse')\npred_y = gbm.predict(X_test[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f' Test RMSE using basic features {round(np.sqrt(mean_squared_error(y_test,pred_y)),4)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ~19.6% improvement from baseline model","metadata":{}},{"cell_type":"code","source":"test = asw_asl(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = basic_features(test)\npred_y = gbm.predict(test[features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission = create_submission(test,pred_y)\n# submission.to_csv('./submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word 2 Vec","metadata":{}},{"cell_type":"code","source":"import spacy\nfrom tqdm.notebook import tqdm\nnlp = spacy.load('en_core_web_lg')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\ndef clean_text(text):\n    text= text.lower() # make text lowercase\n    text = text.replace(\"\\n\",\" \") #remove \\n from text\n#     text = re.sub('[^A-Za-z0-9., ], ' ', text)\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['excerpt'] = train['excerpt'].apply(lambda x: clean_text(x))\ntest['excerpt'] = test['excerpt'].apply(lambda x: clean_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.vstack([nlp(text).vector for text in tqdm(train['excerpt'])])\ny_train = train['target']\nprint(f'Shape of Train vectors: {X_train.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = np.vstack([nlp(text).vector for text in tqdm(test['excerpt'])])\nprint(f'Shape of Test vectors: {X_test.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model on Basic Features + Readability Scores + Word 2 Vec","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_df = pd.concat([pd.DataFrame(X_train),train[features]],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_df = pd.concat([pd.DataFrame(X_test),test[features]],axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X_train_df, train['target'], test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in [1e-5,1e-4,1e-3,1e-2,1e-1,1,10,100]:\n    print(f' aplha {i}')\n    regressor = Ridge(alpha=i,fit_intercept=True, normalize=True)\n    regressor.fit(X_train,y_train)\n    print(f'Train Root mean squared error: {mean_squared_error(y_train,regressor.predict(X_train),squared=False)}')\n    print(f'Validation Root mean squared error: {mean_squared_error(y_val,regressor.predict(X_val),squared=False)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor = Ridge(alpha=0.1,fit_intercept=True, normalize=False) #aplha =0.1\nregressor.fit(X_train, y_train) \ntest['target'] = regressor.predict(X_test_df)\ntest[['id','target']].to_csv('./submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}