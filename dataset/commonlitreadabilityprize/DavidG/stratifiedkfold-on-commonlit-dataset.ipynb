{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport torch\nimport os\nimport random\nfrom matplotlib import pyplot as plt\nimport time\nimport tensorflow as tf\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-25T10:03:27.340971Z","iopub.execute_input":"2021-07-25T10:03:27.341332Z","iopub.status.idle":"2021-07-25T10:03:27.345973Z","shell.execute_reply.started":"2021-07-25T10:03:27.341302Z","shell.execute_reply":"2021-07-25T10:03:27.344981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_FOLD = 5\nROBERTA_PATH = 'roberta-base'\n\ntrain_path = '../input/commonlitreadabilityprize/train.csv'","metadata":{"execution":{"iopub.status.busy":"2021-07-25T10:01:03.279728Z","iopub.execute_input":"2021-07-25T10:01:03.280057Z","iopub.status.idle":"2021-07-25T10:01:03.284337Z","shell.execute_reply.started":"2021-07-25T10:01:03.280028Z","shell.execute_reply":"2021-07-25T10:01:03.283327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import model_selection\nimport seaborn as sns\ndef create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    data = data.sample(frac=1).reset_index(drop=True)\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    data.loc[:, \"bins\"] = pd.cut(data[\"target\"], bins=num_bins, labels=False)\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    return data\n\n\ndf_data_raw =  pd.read_csv(train_path)\ndf_data = create_folds(df_data_raw,N_FOLD)\nfig = plt.figure(figsize=(25,2*N_FOLD))\nfor k in range(N_FOLD):\n    ax = fig.add_subplot(int(N_FOLD/3)+1, 3, k+1)\n    dis_data=df_data.query(f\"kfold != {k}\")\n    sns.histplot(dis_data['target'], kde=True, stat=\"density\", linewidth=0,alpha = 0.5)\n    ax.set_title('fold{} mean: {:.3f}, std: {:.3f}'.format(k, np.mean(dis_data['target']), np.std(dis_data['target'])))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T10:00:36.362228Z","iopub.execute_input":"2021-07-25T10:00:36.362582Z","iopub.status.idle":"2021-07-25T10:00:37.458199Z","shell.execute_reply.started":"2021-07-25T10:00:36.362543Z","shell.execute_reply":"2021-07-25T10:00:37.457503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data","metadata":{"execution":{"iopub.status.busy":"2021-07-25T10:00:42.674814Z","iopub.execute_input":"2021-07-25T10:00:42.675183Z","iopub.status.idle":"2021-07-25T10:00:42.699616Z","shell.execute_reply.started":"2021-07-25T10:00:42.675147Z","shell.execute_reply":"2021-07-25T10:00:42.698691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(ROBERTA_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T10:02:54.229116Z","iopub.execute_input":"2021-07-25T10:02:54.22948Z","iopub.status.idle":"2021-07-25T10:02:55.798942Z","shell.execute_reply.started":"2021-07-25T10:02:54.229442Z","shell.execute_reply":"2021-07-25T10:02:55.797998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For people who use model pretrained with uncased data\ndef processing(text):\n    text = text.replace('\\n','')\n    text = text.lower() # if encoder is uncased\n    text = text.strip()\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-07-25T10:02:58.055677Z","iopub.execute_input":"2021-07-25T10:02:58.055999Z","iopub.status.idle":"2021-07-25T10:02:58.061671Z","shell.execute_reply.started":"2021-07-25T10:02:58.055969Z","shell.execute_reply":"2021-07-25T10:02:58.060609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How to use","metadata":{}},{"cell_type":"markdown","source":"## Pytorch","metadata":{}},{"cell_type":"code","source":"class LitDataset(Dataset):\n    def __init__(self, data_df, tokenizer, is_train=True):\n        super().__init__()\n        self.is_train = is_train\n#         self.text = [processing(text) for text in data_df['excerpt']]\n        self.text = [text.replace(\"\\n\", \" \") for text in data_df['excerpt']]\n        if  self.is_train:\n            self.target = torch.tensor(data_df.target.values, dtype=torch.float32)        \n        self.tokenized = tokenizer(\n            self.text,\n            padding = 'max_length',            \n            max_length = 256,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n    \n    def __len__(self):\n        return len(self.text)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.tokenized['input_ids'][index])\n        attention_mask = torch.tensor(self.tokenized['attention_mask'][index])\n        \n        if self.is_train:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)\n                       \n        else:\n            return (input_ids, attention_mask) ","metadata":{"execution":{"iopub.status.busy":"2021-07-25T10:03:30.758233Z","iopub.execute_input":"2021-07-25T10:03:30.758553Z","iopub.status.idle":"2021-07-25T10:03:30.766008Z","shell.execute_reply.started":"2021-07-25T10:03:30.758527Z","shell.execute_reply":"2021-07-25T10:03:30.764969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = LitDataset(df_data, tokenizer)\nfor fold in range(N_FOLD):\n    print('Fold {}:'.format(fold))\n    #####################################################\n    train_idx = df_data.index[df_data['kfold'] != fold]\n    valid_idx = df_data.index[df_data['kfold'] == fold]\n    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n    val_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n    train_loader = torch.utils.data.DataLoader(\n                      dataset, \n                      batch_size=16, sampler=train_subsampler)\n\n    val_loader = torch.utils.data.DataLoader(\n                      dataset, \n                      batch_size=16, sampler=val_subsampler)\n    #####################################################\n    # model = ...\n    # model.train()\n    # for epoch in range(...):\n    # for i, data in enumerate(train_loader, 0):\n    #      ..............................\n    #####################################################\n    \n    print('Train: {}, Validation: {}'.format(len(train_idx), len(valid_idx)))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T10:04:45.136408Z","iopub.execute_input":"2021-07-25T10:04:45.136747Z","iopub.status.idle":"2021-07-25T10:04:46.021963Z","shell.execute_reply.started":"2021-07-25T10:04:45.136719Z","shell.execute_reply":"2021-07-25T10:04:46.021029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tensorflow","metadata":{}},{"cell_type":"code","source":"def get_dataset(data_df, tokenizer, is_train = True, batch_size=16, seq_len=256,shuffle=True):\n    text = [text.replace(\"\\n\", \" \") for text in data_df['excerpt']]\n    tokenized_inputs =  tokenizer(text=text,max_length = seq_len,truncation = True,padding = 'max_length')\n    if is_train:\n        target_value = tf.cast(data_df.target,dtype=tf.float32)\n        dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']}, \n                                                      tf.expand_dims(target_value, axis=1)))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices({'input_ids': tokenized_inputs['input_ids'], \n                                                  'attention_mask': tokenized_inputs['attention_mask']})\n    if shuffle:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-25T10:03:34.397355Z","iopub.execute_input":"2021-07-25T10:03:34.397702Z","iopub.status.idle":"2021-07-25T10:03:34.404777Z","shell.execute_reply.started":"2021-07-25T10:03:34.397671Z","shell.execute_reply":"2021-07-25T10:03:34.403816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(N_FOLD):\n    print('Fold {}:'.format(fold))\n    #####################################################\n    train_idx = df_data.index[df_data['kfold'] != fold]\n    valid_idx = df_data.index[df_data['kfold'] == fold]\n    df_train,df_val = df_data.iloc[train_idx], df_data.iloc[valid_idx]\n    dataset_train = get_dataset(df_train, tokenizer)\n    dataset_val = get_dataset(df_val, tokenizer,shuffle=False)\n    #####################################################\n    # model = ...\n    # history_train = model.fit(dataset_train, ...)\n    # history_val = model.evaluate(dataset_val, ...)\n    #####################################################\n    \n    print('Train: {}, Validation: {}'.format(len(train_idx), len(valid_idx)))","metadata":{"execution":{"iopub.status.busy":"2021-07-25T10:04:52.318308Z","iopub.execute_input":"2021-07-25T10:04:52.318626Z","iopub.status.idle":"2021-07-25T10:05:12.028662Z","shell.execute_reply.started":"2021-07-25T10:04:52.318599Z","shell.execute_reply":"2021-07-25T10:05:12.027195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}