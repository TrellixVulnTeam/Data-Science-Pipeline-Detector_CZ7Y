{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoModel\nfrom transformers import AutoTokenizer\nfrom transformers import AutoConfig\n\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2021-06-24T13:04:39.413196Z","iopub.execute_input":"2021-06-24T13:04:39.413552Z","iopub.status.idle":"2021-06-24T13:04:42.368164Z","shell.execute_reply.started":"2021-06-24T13:04:39.413476Z","shell.execute_reply":"2021-06-24T13:04:42.367298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Config dict\ncfg = {\n    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    'max_len': 512,\n    'learning_rate': 2e-5,\n    'num_epochs': 3\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-24T13:04:42.371518Z","iopub.execute_input":"2021-06-24T13:04:42.371775Z","iopub.status.idle":"2021-06-24T13:04:42.43867Z","shell.execute_reply.started":"2021-06-24T13:04:42.371749Z","shell.execute_reply":"2021-06-24T13:04:42.437807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg['device']","metadata":{"execution":{"iopub.status.busy":"2021-06-24T13:04:42.440483Z","iopub.execute_input":"2021-06-24T13:04:42.440883Z","iopub.status.idle":"2021-06-24T13:04:42.452125Z","shell.execute_reply.started":"2021-06-24T13:04:42.440807Z","shell.execute_reply":"2021-06-24T13:04:42.451205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read raw csv data to a pandas df\ntrain_df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-24T13:04:42.454224Z","iopub.execute_input":"2021-06-24T13:04:42.454503Z","iopub.status.idle":"2021-06-24T13:04:42.538848Z","shell.execute_reply.started":"2021-06-24T13:04:42.454479Z","shell.execute_reply":"2021-06-24T13:04:42.538124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained('../input/huggingface-bert/bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-06-24T13:04:42.540074Z","iopub.execute_input":"2021-06-24T13:04:42.540439Z","iopub.status.idle":"2021-06-24T13:04:42.609011Z","shell.execute_reply.started":"2021-06-24T13:04:42.540402Z","shell.execute_reply":"2021-06-24T13:04:42.608255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset class","metadata":{}},{"cell_type":"code","source":"class CommonLitDataset(Dataset):\n    \"\"\" Dataset loader class for pytorch \"\"\"\n    \n    def __init__(self, df, tokenizer, max_len, test=False):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.test = test\n        \n        \n    def __len__(self):\n        return len(self.df)\n    \n    \n    def __getitem__(self, idx):\n        text = self.df.loc[idx, 'excerpt']\n        \n        # encode the text and truncate if necessary\n        inputs = self.tokenizer.encode_plus(\n            text,                                 \n            add_special_tokens=True,\n            padding='max_length',\n            max_length=self.max_len,\n            truncation=True\n        )\n        \n        # Define the BERT inputs\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs['token_type_ids']\n        \n        if self.test == False:\n            # Define the BERT outputs\n            target = self.df.loc[idx, ['target']]\n        \n            return {\n                'ids': torch.tensor(ids),\n                'mask': torch.tensor(mask),\n                'token_type_ids': torch.tensor(token_type_ids),\n                'target': torch.torch.FloatTensor(target)\n            }\n        \n        return {\n                'ids': torch.tensor(ids),\n                'mask': torch.tensor(mask),\n                'token_type_ids': torch.tensor(token_type_ids)\n            }","metadata":{"execution":{"iopub.status.busy":"2021-06-24T13:04:42.610261Z","iopub.execute_input":"2021-06-24T13:04:42.610574Z","iopub.status.idle":"2021-06-24T13:04:42.62127Z","shell.execute_reply.started":"2021-06-24T13:04:42.61054Z","shell.execute_reply":"2021-06-24T13:04:42.620433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Criterion (RMSE)","metadata":{}},{"cell_type":"code","source":"def RMSE(y_pred, y_true):\n    metric = nn.MSELoss()\n    return torch.sqrt(metric(y_pred, y_true))","metadata":{"execution":{"iopub.status.busy":"2021-06-24T13:04:42.622943Z","iopub.execute_input":"2021-06-24T13:04:42.623205Z","iopub.status.idle":"2021-06-24T13:04:42.631351Z","shell.execute_reply.started":"2021-06-24T13:04:42.62318Z","shell.execute_reply":"2021-06-24T13:04:42.630412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model class","metadata":{}},{"cell_type":"code","source":"class CommonLitModel(nn.Module):\n    \n    def __init__(self, name):\n        super(CommonLitModel, self).__init__()\n        self.name = name\n        \n        if name == 'BERT':\n            self.bert = AutoModel.from_pretrained('../input/huggingface-bert/bert-base-uncased')\n            # Output from BERT\n            self.in_features = self.bert.pooler.dense.out_features\n        \n        self.dropout = nn.Dropout()\n        self.layer_norm = nn.LayerNorm(self.in_features)\n        self.fc = nn.Linear(self.in_features, 1)\n    \n    \n    def forward(self, ids, mask, token_type_ids):\n        \n        if self.name == 'BERT':\n            _, output = self.bert(ids,\n                                 attention_mask=mask,\n                                 token_type_ids=token_type_ids,\n                                 return_dict=False)\n            \n        output = self.layer_norm(output)\n        output = self.dropout(output)\n        output = self.fc(output)\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2021-06-24T13:04:42.634653Z","iopub.execute_input":"2021-06-24T13:04:42.635298Z","iopub.status.idle":"2021-06-24T13:04:42.643721Z","shell.execute_reply.started":"2021-06-24T13:04:42.635258Z","shell.execute_reply":"2021-06-24T13:04:42.642936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_valid (model, optimizer, criterion, datasets, num_epochs=10):\n    \n    model.to(cfg['device'])\n    \n    for idx, (train, test) in enumerate(datasets):\n        print(f'\\nSPLIT {idx + 1}:')\n        \n        train_dataloader = DataLoader(dataset=train, shuffle=True, batch_size=16)\n        test_dataloader = DataLoader(dataset=test, shuffle=False, batch_size=1)\n        \n        # Train the model\n        model.train()\n        for epoch in range(num_epochs):\n\n            for idx, data in enumerate(train_dataloader):\n\n                optimizer.zero_grad()\n    \n                X_train = {key: value.to(cfg['device']) for (key, value) in data.items() if key != 'target'}\n                y_train = data['target'].to(cfg['device'])\n            \n                output = model(X_train['ids'],\n                               X_train['mask'],\n                               X_train['token_type_ids'])\n\n                torch.cuda.empty_cache()\n                loss = criterion(output, y_train)\n\n                if idx % 140 == 0:\n                    print(f'\\nTRAIN RMSE: {loss}')\n\n                loss.backward()\n                optimizer.step()\n                \n        # Validate the model\n        model.eval()\n        for idx, data in enumerate(test_dataloader):\n            \n            X_test = {key: value.to(cfg['device']) for (key, value) in data.items()}\n            y_test = data['target'].to(cfg['device'])\n\n            with torch.no_grad():\n                output = model(X_test['ids'],\n                                X_test['mask'],\n                                X_test['token_type_ids'])\n                \n            loss = criterion(output, y_test)\n                \n            if idx % 20 == 0:\n                print(f'\\nVALID RMSE: {loss}')\n\n            torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-06-24T13:04:42.645221Z","iopub.execute_input":"2021-06-24T13:04:42.64559Z","iopub.status.idle":"2021-06-24T13:04:42.658391Z","shell.execute_reply.started":"2021-06-24T13:04:42.645532Z","shell.execute_reply":"2021-06-24T13:04:42.657481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the dataframe in a 10 fold cross validation manner\nkf = KFold(n_splits = 10, shuffle = True, random_state = 4)\ndatasets = []\n\nfor train_index, test_index in kf.split(train_df):\n    \n    train = train_df.iloc[train_index].reset_index(drop=True)\n    test =  train_df.iloc[test_index].reset_index(drop=True)\n    \n    datasets.append((CommonLitDataset(train, tokenizer, cfg['max_len'], test=False),\n                    CommonLitDataset(test, tokenizer, cfg['max_len'], test=False)))","metadata":{"execution":{"iopub.status.busy":"2021-06-24T13:04:42.659357Z","iopub.execute_input":"2021-06-24T13:04:42.661156Z","iopub.status.idle":"2021-06-24T13:04:42.687837Z","shell.execute_reply.started":"2021-06-24T13:04:42.66113Z","shell.execute_reply":"2021-06-24T13:04:42.687138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CommonLitModel('BERT')\ncriterion = RMSE\noptimizer = torch.optim.AdamW(model.parameters(), lr=cfg['learning_rate'])\n\ntorch.cuda.empty_cache()\ntrain_valid(model, optimizer, criterion, datasets, num_epochs=cfg['num_epochs'])","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-06-24T13:04:42.688991Z","iopub.execute_input":"2021-06-24T13:04:42.689309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TODO\n- Look into early-stopping","metadata":{}},{"cell_type":"code","source":"def test (model, dataloader):\n    \n    model.eval()\n    model.to(cfg['device'])\n    output_list = []\n    \n    for idx, data in enumerate(dataloader):\n            \n        X_train = {key: value.to(cfg['device']) for (key, value) in data.items()}\n        \n        with torch.no_grad():\n            output = model(X_train['ids'],\n                            X_train['mask'],\n                            X_train['token_type_ids'])\n        \n        output_list.append(output.item())\n        torch.cuda.empty_cache()\n    \n    return output_list\n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CommonLitDataset(test_df, tokenizer, cfg['max_len'], test=True)\ndataloader = DataLoader(dataset=dataset, shuffle=False, batch_size=1)\n\ntorch.cuda.empty_cache()\noutputs = test(model, dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the output\noutput_data = {\n    'id': test_df['id'],\n    'target': outputs\n}\n\noutput_df = pd.DataFrame(output_data, columns=['id', 'target'])\noutput_df.to_csv('./submission.csv', index = False, header=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}