{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CommonLit Readability - RoBerta + XGB Baseline\n\nThanks to [Abhishek](https://www.kaggle.com/abhishek) for the model: [modelf1](https://www.kaggle.com/abhishek/modelf1) discussed in [ðŸš€ AutoNLP to the rescue](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/237795).\n\nFor the embeddings loading code, thanks to [Abhishek](https://www.kaggle.com/abhishek) for the notebook [yum yum yum](https://www.kaggle.com/abhishek/yum-yum-yum) and [Maunish](https://www.kaggle.com/maunish) for the notebook: [CLRP: RoBerta + LGBM](https://www.kaggle.com/maunish/clrp-roberta-lgbm). Also, thanks to [Maunish](https://www.kaggle.com/maunish) for the last notebook for the idea to use boosted trees on RoBerta embeddings. This notebook is basically similar to the previous versions of that notebook except for using XGBoost, ordinary KFold and outputting OOF RMSE.","metadata":{}},{"cell_type":"markdown","source":"# Load Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport time\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport xgboost as xgb\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"data_dir = '../input/commonlitreadabilityprize/'\ntrain = pd.read_csv(data_dir + 'train.csv')\ntest = pd.read_csv(data_dir + 'test.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\ntarget = train['target'].to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Embeddings\nsource: https://www.kaggle.com/maunish/clrp-roberta-lgbm","metadata":{}},{"cell_type":"code","source":"# source: https://www.kaggle.com/maunish/clrp-roberta-lgbm\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\n\nclass CLRPDataset(nn.Module):\n    def __init__(self, df, tokenizer, max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \n\ndef get_embeddings(df, path, plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    MODEL_PATH = path\n    model = AutoModel.from_pretrained(MODEL_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    model.to(device)\n    model.eval()\n\n    ds = CLRPDataset(df, tokenizer, config['max_len'])\n    dl = DataLoader(ds,\n                    batch_size=config[\"batch_size\"],\n                    shuffle=False,\n                    num_workers = 4,\n                    pin_memory=True,\n                    drop_last=False)\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0], -1).to(device) for key, val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs[0][:, 0].detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# source: https://www.kaggle.com/maunish/clrp-roberta-lgbm\n\nconfig = {\n    'batch_size': 128,\n    'max_len': 256,\n    'seed': 42,\n}\nseed_everything(seed=config['seed'])\n\ntrain_embeddings =  get_embeddings(train,'../input/modelf1')\ntest_embeddings = get_embeddings(test,'../input/modelf1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"params = {\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    \n    'eta': 0.05,\n    'max_depth': 3,\n    \n    'gamma': 1,\n    'subsample': 0.8,\n    \n    'nthread': 2\n}\n\nnfolds = 5\nkf = KFold(n_splits=nfolds, shuffle=True, random_state=config['seed'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_iterations = []\noof_rmses = []\npreds = np.zeros(test.shape[0])\n\nfor k, (train_idx, valid_idx) in enumerate(kf.split(train)):    \n    \n    dtrain = xgb.DMatrix(train_embeddings[train_idx], target[train_idx])\n    dvalid = xgb.DMatrix(train_embeddings[valid_idx], target[valid_idx])\n    evals_result = dict()\n    booster = xgb.train(params,\n                        dtrain,\n                        evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                        num_boost_round=300,\n                        early_stopping_rounds=20,\n                        evals_result=evals_result,\n                        verbose_eval=False)\n    \n    best_iteration = np.argmin(evals_result['valid']['rmse'])\n    best_iterations.append(best_iteration)\n    oof_rmse = evals_result['valid']['rmse'][best_iteration]\n    oof_rmses.append(oof_rmse)\n    \n    preds += booster.predict(xgb.DMatrix(test_embeddings), ntree_limit=int(best_iteration+1)) / nfolds\n    \nevals_df = pd.DataFrame()\nevals_df['fold'] = range(1, nfolds+1)\nevals_df['best_iteration'] = best_iterations\nevals_df['oof_rmse'] = oof_rmses\n\ndisplay(evals_df)\nprint('mean oof rmse = {}'.format(np.mean(oof_rmses)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test['prediction'] = preds\nsubmission = pd.DataFrame()\nsubmission['id'] = test['id'].copy()\nsubmission['target'] = test['prediction'].copy()\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}