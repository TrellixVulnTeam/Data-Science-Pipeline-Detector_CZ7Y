{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook presents a first baseline for CommonLit Readability Competition, using a pretrained DistilBert as feature extractor and a standard regression model for prediction.   \nIt can be used on original train text or preprocessed text (stopwords, ponctuation removed).\nSome special features have been added to try to catch the text complexity and add it as an input for regression in case the prepocessed text is used. It should be a way to keep the knowledge of the text structure.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom nltk.corpus import stopwords\nimport torch\nimport transformers as ppb # pytorch transformers\n\nfrom sklearn import metrics\nfrom sklearn import kernel_ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:11:14.948601Z","iopub.execute_input":"2021-07-01T09:11:14.949182Z","iopub.status.idle":"2021-07-01T09:11:17.57826Z","shell.execute_reply.started":"2021-07-01T09:11:14.949098Z","shell.execute_reply":"2021-07-01T09:11:17.577367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get train data\ntrain_file = '../input/commonlitreadabilityprize/train.csv'\ndata_train =  pd.read_csv(train_file)\ndata_train","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:11:22.810519Z","iopub.execute_input":"2021-07-01T09:11:22.810869Z","iopub.status.idle":"2021-07-01T09:11:22.928967Z","shell.execute_reply.started":"2021-07-01T09:11:22.810837Z","shell.execute_reply":"2021-07-01T09:11:22.927908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# explore excerpt\ndata_train['nb_words'] = data_train.excerpt.apply(lambda x: len(x.split()))\ndata_train['nb_chars'] = data_train.excerpt.apply(lambda x: len(x))\n\n# process text : remove ponctuation and stopwords\nlist_sw = stopwords.words(\"english\")\nsw = set(list_sw)\n\ndef remove_stopwords(text):\n    \"\"\"removes ponctuation and stop words\"\"\"\n    text = re.sub(\"[^a-zA-Z0-9]\",   \" \",  text)  \n    words = text.split()\n    interest_words = [w for w in words if not w in sw]\n    return ' '.join(interest_words).lower()\n\ndata_train['interest_words'] = data_train.excerpt.apply(remove_stopwords)\ndata_train['nb_int_words'] = data_train.interest_words.apply(lambda x: len(x))\n\n# plot histograms\nfix, ax = plt.subplots(1, 3, figsize=(15, 5))\nsns.histplot(data=data_train, x=\"nb_chars\", bins=100, ax=ax[0],\n             color='pink', edgecolor=None, stat='density')\nsns.kdeplot(data=data_train, x=\"nb_chars\", color='hotpink', ax=ax[0])\nax[0].axvline(np.median(data_train.nb_chars), 0, np.max(data_train.nb_chars))\nax[0].set_title('Text length', fontsize=10)\n\nsns.histplot(data=data_train, x=\"nb_words\", bins=100, ax=ax[1],\n             color='lightgreen', edgecolor=None, stat='density')\nsns.kdeplot(data=data_train, x=\"nb_words\", color='green', ax=ax[1])\nax[1].axvline(np.median(data_train.nb_words), 0, np.max(data_train.nb_words))\nax[1].set_title('Number of words', fontsize=10)\n\nsns.histplot(data=data_train, x=\"nb_int_words\", bins=100, ax=ax[2],\n             color='aquamarine', edgecolor=None, stat='density')\nsns.kdeplot(data=data_train, x=\"nb_int_words\",\n            color='mediumseagreen', ax=ax[2])\nax[2].axvline(np.median(data_train.nb_int_words), 0,\n              np.max(data_train.nb_int_words))\nax[2].set_title('Number of interesting words', fontsize=10)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:11:42.408556Z","iopub.execute_input":"2021-07-01T09:11:42.409091Z","iopub.status.idle":"2021-07-01T09:11:44.014489Z","shell.execute_reply.started":"2021-07-01T09:11:42.409048Z","shell.execute_reply":"2021-07-01T09:11:44.012741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text complexity analysis\n\ndef get_parts(text, level='sentence'):\n    \"\"\"return parts of a text \n    if level is sentence, returns list of sentences\n    if level is details, uses all signs of ponctuation\"\"\"\n\n    if level == 'sentence':\n        list_replace=['.', '!', '?', '...']\n        sep = '.'  \n    elif level == 'details':\n        list_replace=['.', '!', '?', '...', ';', ':']\n        sep = ','\n    else:\n        return []\n\n    for char in list_replace:\n        text = text.replace(char, sep)\n\n    parts = text.split(sep)\n    parts = [x for x in parts if x]  # cleans empty substrings\n    return parts\n\ndef count_parts(text, level='sentence'):\n    \"\"\"returns the number of substrings from text\n    if the level is 'sentence' (default), counts the number of sentences\n    if the level is 'details', uses every sign of ponctuation\"\"\"\n    parts = get_parts(text, level)\n    return len(parts)\n\ndef length_parts(text, level='sentence', how='mean'):\n    \"\"\"returns the mean or max length of the substrings in a text\n    default level is the sentence analysis\"\"\"\n    parts = get_parts(text, level)\n    len_parts = []\n    for s in parts:\n        len_parts.append(len(s.split()))\n\n    if how == 'mean':\n        return np.mean(len_parts)\n    if how == 'max':\n        return np.max(len_parts)\n\ndef count_parts_in_part(text):\n    \"\"\"for each sentence of the text, find additional ponctuation signs\n    and compute the mean number of 'parts' in the sentences\"\"\"\n    sentences = get_parts(text, level='sentence')\n    nb_parts_by_sentence = []\n    for s in sentences:\n        nb = count_parts(s, level='details')\n        nb_parts_by_sentence.append(nb)\n    return np.mean(nb_parts_by_sentence)\n\ndef types_sentences(text):\n    \"\"\"returns the number of different sentences types in the text\n    from 0 to 4\"\"\"\n    list_sep = ['.', '!', '?', '...']\n    found = []\n    for sep in list_sep:\n        if sep in text:\n            found.append(sep)\n\n    return len(np.unique(found))\n\n# sentences level analysis\ndata_train['nb_sentences'] = data_train.excerpt.apply(count_parts)\ndata_train['mean_len_sentences'] = data_train.excerpt.apply(length_parts)\ndata_train['max_len_sentences'] = data_train.excerpt.apply(length_parts,\n                                                           how='max')\ndata_train['types_sentences'] = data_train.excerpt.apply(types_sentences)\n\nfix, ax = plt.subplots(1, 4, figsize=(20, 5))\nsns.histplot(data=data_train, x=\"nb_sentences\", bins=100, ax=ax[0],\n             color='peachpuff', edgecolor=None, stat='density')\nsns.kdeplot(data=data_train, x=\"nb_sentences\", color='lightsalmon', ax=ax[0])\nax[0].axvline(np.median(data_train.nb_sentences), 0,\n              np.max(data_train.nb_sentences))\nax[0].set_title('Number of sentences in a text', fontsize=10)\n\ndata_types = data_train['types_sentences'].value_counts()\ndata_types.plot(kind='pie', colormap='Set2', autopct=\"%.1f%%\", ax=ax[1])\nax[1].set_title('Number of different types of sentences in a text',\n                fontsize=10)\n\nsns.histplot(data=data_train, x=\"mean_len_sentences\", bins=100, ax=ax[2],\n             color='paleturquoise', edgecolor=None, stat='density')\nsns.kdeplot(data=data_train, x=\"mean_len_sentences\", color='turquoise',\n            ax=ax[2])\nax[2].axvline(np.median(data_train.mean_len_sentences), 0,\n              np.max(data_train.mean_len_sentences))\nax[2].set_title('Mean number of words per sentence', fontsize=10)\n\nsns.histplot(data=data_train, x=\"max_len_sentences\", bins=100, ax=ax[3],\n             color='turquoise', edgecolor=None, stat='density')\nsns.kdeplot(data=data_train, x=\"max_len_sentences\", color='darkturquoise',\n            ax=ax[3])\nax[3].axvline(np.median(data_train.max_len_sentences), 0,\n              np.max(data_train.max_len_sentences))\nax[3].set_title('Max number of words per sentence', fontsize=10)\n\nplt.show()\n\n# sub-sentences (details) level analysis\ndata_train['nb_parts'] = data_train.excerpt.apply(count_parts, level='details')\ndata_train['mean_parts_by_sent'] = data_train.excerpt.apply(count_parts_in_part)\n\nfix, ax = plt.subplots(1, 2, figsize=(12, 5))\nsns.histplot(data=data_train, x=\"nb_parts\", bins=100, ax=ax[0],\n             color='wheat', edgecolor=None, stat='density')\nsns.kdeplot(data=data_train, x=\"nb_parts\", color='darkkhaki', ax=ax[0])\nax[0].axvline(np.median(data_train.nb_parts), 0, np.max(data_train.nb_parts))\nax[0].set_title('Number of sub-parts in a text', fontsize=10)\n\nsns.histplot(data=data_train, x=\"mean_parts_by_sent\", bins=100, ax=ax[1],\n             color='lightcoral', edgecolor=None, stat='density')\nsns.kdeplot(data=data_train, x=\"mean_parts_by_sent\", color='brown', ax=ax[1])\nax[1].axvline(np.median(data_train.mean_parts_by_sent), 0,\n              np.max(data_train.mean_parts_by_sent))\nax[1].set_title('Mean number of sub-parts per sentence', fontsize=10)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:11:56.116396Z","iopub.execute_input":"2021-07-01T09:11:56.116901Z","iopub.status.idle":"2021-07-01T09:11:58.850639Z","shell.execute_reply.started":"2021-07-01T09:11:56.116862Z","shell.execute_reply":"2021-07-01T09:11:58.849775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target labels\nlabels = data_train['target']\n\nfix, ax = plt.subplots(1, 2, figsize=(12, 5))\nsns.histplot(data=data_train, x=\"target\", bins=100, ax=ax[0],\n             color='orange', edgecolor=None, stat='density')\nsns.kdeplot(data=data_train, x=\"target\", ax=ax[0], color='red')\nax[0].axvline(np.median(data_train.target), 0, np.max(data_train.target))\nax[0].set_title('Targets histogram', fontsize=10)\n\nsns.boxplot(x=data_train[\"target\"], ax=ax[1], color='orange')\nax[1].set_title('Targets boxplot', fontsize=10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:12:01.809723Z","iopub.execute_input":"2021-07-01T09:12:01.810256Z","iopub.status.idle":"2021-07-01T09:12:02.307778Z","shell.execute_reply.started":"2021-07-01T09:12:01.81021Z","shell.execute_reply":"2021-07-01T09:12:02.306978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation between target and text complexity features\n\ndata_mat = data_train[['target', 'nb_words', 'nb_int_words',\n                       'nb_sentences', 'max_len_sentences', 'types_sentences',\n                       'mean_parts_by_sent']]\nfig, axes = plt.subplots(figsize= (10,8))\nsns.heatmap(data_mat.corr(), annot=True, cmap='PRGn')\nplt.xticks(rotation=30, ha='right')\nplt.title(\"Matrice de corrélation\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:12:10.746925Z","iopub.execute_input":"2021-07-01T09:12:10.747257Z","iopub.status.idle":"2021-07-01T09:12:11.276562Z","shell.execute_reply.started":"2021-07-01T09:12:10.747216Z","shell.execute_reply":"2021-07-01T09:12:11.274283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distilbert","metadata":{}},{"cell_type":"code","source":"# Load pretrained model/tokenizer Distilbert\nmodel_class = ppb.DistilBertModel\ntokenizer_class = ppb.DistilBertTokenizer\npretrained_weights = \"../input/distilbertbaseuncased/\"\n\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights,\n                                            do_lower_case=True)\nmodel = model_class.from_pretrained(pretrained_weights)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:40:18.65245Z","iopub.execute_input":"2021-07-01T09:40:18.652798Z","iopub.status.idle":"2021-07-01T09:40:20.1822Z","shell.execute_reply.started":"2021-07-01T09:40:18.652764Z","shell.execute_reply":"2021-07-01T09:40:20.18101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# choose here if you want to use :\n# - full original text (-> excerpt)\n# - or preprocessed text (-> interest_words)\nuse_text = data_train.excerpt\ntokenized = use_text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\ntokenized.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:12:18.872812Z","iopub.execute_input":"2021-07-01T09:12:18.873197Z","iopub.status.idle":"2021-07-01T09:12:34.836366Z","shell.execute_reply.started":"2021-07-01T09:12:18.873155Z","shell.execute_reply":"2021-07-01T09:12:34.835403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# padding : add zeros to build same shapes texts\nmax_len = 0\nmin_len = 100000\nfor i in tokenized.values:\n    if len(i) > max_len:\n        max_len = len(i)\n    if len(i) < min_len:\n        min_len = len(i)\nprint('length of texts : {} -> {}'.format(min_len, max_len))\n\npadded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\nprint('texts shape :', padded.shape)\n\n# build attention_mask to focus on real tokens (cf padding)\nattention_mask = np.where(padded != 0, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:12:34.837617Z","iopub.execute_input":"2021-07-01T09:12:34.837908Z","iopub.status.idle":"2021-07-01T09:12:35.053416Z","shell.execute_reply.started":"2021-07-01T09:12:34.837883Z","shell.execute_reply":"2021-07-01T09:12:35.052401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply pretrained distilbert model to extract features\ninput_ids = torch.tensor(padded)\nattention_mask = torch.tensor(attention_mask)\n\nlast_hidden_states = []\nbatch_size = 200\n\nstart = time.time()\nwith torch.no_grad():\n    for i in range(0, len(input_ids), batch_size):\n        print('{} / {}'.format(i, len(input_ids)))\n        ins = input_ids[i: i + batch_size]\n        atts = attention_mask[i: i + batch_size]\n        h = model(ins, attention_mask=atts)\n        last_hidden_states.extend(h[0])\n\nprint('elapsed time : ', time.time() - start)\nprint('hidden states : ', len(last_hidden_states))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:12:35.055164Z","iopub.execute_input":"2021-07-01T09:12:35.055574Z","iopub.status.idle":"2021-07-01T09:26:50.953275Z","shell.execute_reply.started":"2021-07-01T09:12:35.055528Z","shell.execute_reply":"2021-07-01T09:26:50.952228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Regression","metadata":{}},{"cell_type":"code","source":"def show_res(yTest, yPred):\n    \"\"\"plots result curves\"\"\"\n    fig, axes = plt.subplots(figsize=(12, 6), nrows=1, ncols=2)\n    xmin = np.min(yTest) - 1\n    xmax = np.max(yTest) + 1\n    axes[0].scatter(yTest, yPred, color='coral', s=1.5)\n    axes[0].plot([xmin, xmax], [xmin, xmax])\n    axes[0].set_xlabel('Y expected')\n    axes[0].set_ylabel('Y prediction')\n    axes[0].set_title('Prediction / true values')\n\n    residuals = yTest - yPred\n    moy_residuals = np.mean(residuals)\n    lab = 'Mean residuals(' + str(np.round(moy_residuals, decimals=2)) + ')'\n\n    xmin = np.min(yPred) - 1\n    xmax = np.max(yPred) + 1\n    axes[1].scatter(yPred,residuals, color='red', s=1.5)\n    axes[1].plot([xmin, xmax], [0, 0], color='grey', alpha=0.5, linewidth=0.5)\n    axes[1].plot([xmin, xmax], [moy_residuals, moy_residuals],\n                 color='green',linewidth=0.5, label=lab)\n    axes[1].set_xlabel('Y prediction')\n    axes[1].set_ylabel('Residuals')\n    axes[1].legend()\n    axes[1].set_title('Residuals')\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:27:42.011559Z","iopub.execute_input":"2021-07-01T09:27:42.011898Z","iopub.status.idle":"2021-07-01T09:27:42.022207Z","shell.execute_reply.started":"2021-07-01T09:27:42.011871Z","shell.execute_reply":"2021-07-01T09:27:42.021154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get distilbert features (on CLS token)\nfeatures = []\nfor i in range(len(last_hidden_states)):\n    sent = last_hidden_states[i]\n    first_hidden = sent[0]\n    features.append(first_hidden.numpy())\nfeatures = np.array(features)\n\nX = []\nadd_complexity = False\n# add complexity features (cf first analysis) if needed\nif add_complexity:\n    complexity_features = data_train[['nb_sentences',\n                                      'types_sentences',\n                                      'mean_nb_parts_by_sentence']]\n    for i in range(len(data_train)):\n        cur_X = features[i]\n        cur_complexity = complexity_features.iloc[i].to_numpy()\n        cur_X = np.append(cur_X, cur_complexity)\n        X.append(cur_X)\nelse:\n    X = features.copy()\n\nprint('Number of observations :', len(X))\nprint('Number of features :', len(X[0]))\n\n# split into train and validation set for evaluation purpose\nX_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2)\n# do not split for competition\n# X_train = X\n# y_train = labels","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:27:43.076547Z","iopub.execute_input":"2021-07-01T09:27:43.076862Z","iopub.status.idle":"2021-07-01T09:27:43.106165Z","shell.execute_reply.started":"2021-07-01T09:27:43.076834Z","shell.execute_reply":"2021-07-01T09:27:43.105335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ridge polynomial kernel + gridsearch\n\n# parameters\nalpha_range = np.logspace(-2, 2, 5)  # regularization\ngamma_range = np.logspace(-2, 2, 5)  # kernel parameter\nkernel_types = ('polynomial', 'linear', 'rbf', 'laplacian', 'sigmoid', 'cosine')\nparam_grid = {'alpha': alpha_range, 'gamma': gamma_range, 'kernel': kernel_types}\n\nscore = 'neg_mean_squared_error'\nmodel_reg = kernel_ridge.KernelRidge()\n\nkr_model = GridSearchCV(\n    model_reg,\n    param_grid,\n    cv=5,\n    scoring=score,\n    return_train_score=True)\n                                    \n# training\nstart_time = time.time()\nkr_model.fit(X_train, y_train)\nelapsed = time.time() - start_time\n\nprint('time (sec) :', round(elapsed, 2))\nprint('Best model parameters: ', kr_model.best_params_)\n      \n# prediction for evaluation purpose (comment section for competition)\ny_pred = kr_model.predict(X_test)\nrmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n# results\nprint('rmse :', round(rmse, 3))\nshow_res(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:27:46.193756Z","iopub.execute_input":"2021-07-01T09:27:46.194181Z","iopub.status.idle":"2021-07-01T09:36:24.929581Z","shell.execute_reply.started":"2021-07-01T09:27:46.194154Z","shell.execute_reply":"2021-07-01T09:36:24.928577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test set (7 entries)","metadata":{}},{"cell_type":"markdown","source":"Apply the same transformations to test set as the training set (no preprocessing, no complexity features added)","metadata":{}},{"cell_type":"code","source":"test_file = '../input/commonlitreadabilityprize/test.csv'\ndata_test =  pd.read_csv(test_file)\n\n# tokenisation on full text\ntokenized_test = data_test.excerpt.str.lower().apply(\n    (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n\n# padding and attention mask\nmax_len = 0\nfor i in tokenized_test.values:\n    if len(i) > max_len:\n        max_len = len(i)\npadded_test = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test.values])\nattention_mask_test = np.where(padded_test != 0, 1, 0)\n\n# compute features\ninput_ids = torch.tensor(padded_test)  \nattention_mask = torch.tensor(attention_mask_test)\n\nlast_hidden_states_test = []\nbatch_size = 200\n\nstart = time.time()\nwith torch.no_grad():\n    for i in range(0, len(input_ids), batch_size):\n        print('{} / {}'.format(i, len(input_ids)))\n        ins = input_ids[i:i+batch_size]\n        atts = attention_mask[i:i+batch_size]\n        h = model(ins, attention_mask=atts)\n        last_hidden_states_test.extend(h[0])\n          \nprint('elapsed time : ', time.time() - start)\nprint('hidden states : ', len(last_hidden_states_test))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:40:25.229614Z","iopub.execute_input":"2021-07-01T09:40:25.230191Z","iopub.status.idle":"2021-07-01T09:40:26.557242Z","shell.execute_reply.started":"2021-07-01T09:40:25.230159Z","shell.execute_reply":"2021-07-01T09:40:26.556414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get test features\nX_test = []\nfor i in range(len(last_hidden_states_test)):\n    sent = last_hidden_states_test[i]\n    first_hidden = sent[0]\n    X_test.append(first_hidden.numpy())\nX_test = np.array(X_test)\n\n# prediction\ny_test = kr_model.predict(X_test)\n\n# submission file\nsubmission_df = pd.DataFrame({'id': data_test.id, 'target': 0})\nsubmission_df.target = y_test\n\nsubmission_file = 'submission.csv'\nsubmission_df.to_csv(submission_file, index=False)\n\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2021-07-01T09:40:29.401529Z","iopub.execute_input":"2021-07-01T09:40:29.402066Z","iopub.status.idle":"2021-07-01T09:40:29.429891Z","shell.execute_reply.started":"2021-07-01T09:40:29.402019Z","shell.execute_reply":"2021-07-01T09:40:29.428917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}