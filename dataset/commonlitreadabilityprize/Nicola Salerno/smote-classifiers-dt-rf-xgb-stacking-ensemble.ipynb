{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport sys\nimport pickle\nfrom datetime import datetime\n%matplotlib inline\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.utils import class_weight\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's check the test set composition**","metadata":{}},{"cell_type":"code","source":"test_set = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_set.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's start working on the train set**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport sys\nimport pickle\nfrom datetime import datetime\n%matplotlib inline\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.utils import class_weight\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's focus on \"excerpt\" and \"target\"**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[\"excerpt\", \"target\"]].info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[\"excerpt\", \"target\"]].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['target'].hist(bins=40)\nplt.title('Distribuzione variabile target')\nplt.ylabel('Conteggio')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's focus on null values**","metadata":{}},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlation**","metadata":{}},{"cell_type":"code","source":"df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's save standard error for later and drop url_legal and license columns**","metadata":{}},{"cell_type":"code","source":"stde = df['standard_error']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns = ['url_legal','license', 'id', 'standard_error'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's transform continue target in categorical one**","metadata":{}},{"cell_type":"code","source":"print(df['target'].min(),df['target'].max())\nnumberOfStars=6\nmybin=((df['target'].max())-(df['target'].min()-1))/numberOfStars\nbins=[]\nfor i in range(numberOfStars+1):\n    bins.append((df['target'].min()-1)+i*mybin)\nprint (bins)\n\nstars = pd.cut(df.target,bins=6,labels=[0,1,2,3,4,5])#stelle mappate fra 0 e 4 perchè il to_categorical() di keras parte da indice 0\ndf.insert(2,'stars',stars)\ndf[\"stars\"] = pd.to_numeric(df[\"stars\"])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"stars\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.stars.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Dropping target**\n","metadata":{}},{"cell_type":"code","source":"df = df.drop(['target'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.stars.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Unbalanced class! We are going to perform smote**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features engineering**","metadata":{}},{"cell_type":"code","source":"X= df['excerpt']\ny = df['stars']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitto dataset in training e test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,stratify=y)#stratifico rispetto y\n\n\nprint(type(X_train))\nprint (X_train[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2# per features selection\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vect = CountVectorizer(min_df=1,ngram_range=(1,2))  # tokenization and feature extraction (fa tokenizzazione di default conterrà contatori )\n                                  #min_df=5 elimino parole sulla coda lunga (numerose e poco frequenti count<5)\n                                  #ngram_range=(1,2) considero solo le singole parole e  bigrammi, non trigrammi \nvect.fit(X_train)\nX_train = vect.transform(X_train)\nX_test =vect.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sel = SelectKBest(chi2, k=5000)  # feature selection  faccio features selection in maniera supervisionata gli passo y_train\nsel.fit(X_train,y_train)\nX_train = sel.transform(X_train)\nX_test = sel.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfTransformer()  # weighting\ntfidf.fit(X_train)\nX_train = tfidf.transform(X_train)\nX_test =tfidf.transform(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# la classe 4 è sbilanciata, decido quindi di bilanciarla computando la class weight che poi usero come parametro nel fitting della rete neurale\nmy_class_weights = class_weight.compute_class_weight('balanced', # per avere dataset perfettamente bilanciato (per il training) senza però toccare niente sul dataset\n                                                  np.unique(y_train),\n                                                  y_train)\n\nfor _class in [0 ,1,2,3,4]:\n    print(f\"Weight of instances in class {_class:d}: {my_class_weights[_class]:.2f}\")\ntype(my_class_weights)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Smote**","metadata":{}},{"cell_type":"code","source":"print('Before OverSampling, the shape of train_X: {}'.format(X_train.shape))\nprint('Before OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))\n\n\n\nprint(\"Before OverSampling, counts of label '5': {}\".format(sum(y_train == 5)))\nprint(\"Before OverSampling, counts of label '4': {}\".format(sum(y_train == 4)))\nprint(\"Before OverSampling, counts of label '3': {}\".format(sum(y_train == 3)))\nprint(\"Before OverSampling, counts of label '2': {}\".format(sum(y_train == 2)))\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n  \n# import SMOTE module from imblearn library\n# pip install imblearn (if you don't have imblearn in your system)\nfrom imblearn.over_sampling import SMOTE\n\noversample = SMOTE(random_state = 2)\nX_train_res, y_train_res = oversample.fit_resample(X_train, y_train)\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n  \n\nprint(\"After OverSampling, counts of label '5': {}\".format(sum(y_train_res == 5)))\nprint(\"After OverSampling, counts of label '4': {}\".format(sum(y_train_res == 4)))    \nprint(\"After OverSampling, counts of label '3': {}\".format(sum(y_train_res == 3)))\nprint(\"After OverSampling, counts of label '2': {}\".format(sum(y_train_res == 2)))\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Decision Tree**","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\n\n\nclf = DecisionTreeClassifier()\n\n\"\"\"param_list = {\n    'max_depth': [None, 2, 4, 6, 8, 16, 32],\n    'min_samples_leaf': [1, 5, 10, 20, 50, 100],\n    'min_samples_split': [2, 5, 10, 20, 50, 100],\n    'criterion': ['gini', 'entropy']\n}\n\n\ngrid_search = GridSearchCV(clf, param_grid=param_list, cv=5, scoring='accuracy') # ricerca esaustiva fra i parametri\ngrid_search.fit(X_train_res, y_train_res)# (NB: gli passo X_train,y_train ossia solo training perchè poi utilizzo il modello ritornato)\nclf_best_grid = grid_search.best_estimator_ # il miglior calssificatore trovato rispetto all'accuratezza\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"grid_search.best_params_\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"clf_best_grid\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training the model on train_res with best params**","metadata":{}},{"cell_type":"code","source":"clf = DecisionTreeClassifier(criterion= 'gini', max_depth= 32, \n                             min_samples_leaf= 1, min_samples_split= 5) \n\nclf.fit(X_train_res, y_train_res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom scikitplot.metrics import plot_roc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy %s' % accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matrix = plot_confusion_matrix(clf, X_test, y_test,\n                            #display_labels=le.classes_,\n                            cmap=plt.cm.Blues,\n                            xticks_rotation=70)\nmatrix.ax_.set_title('Confusion Matrix')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(y_test, clf.predict_proba(X_test))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Random Forest**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets, metrics, model_selection, svm\nfrom sklearn.metrics import confusion_matrix, accuracy_score \nfrom sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score\nfrom sklearn.metrics import classification_report, precision_recall_curve\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV # ricerca esaustiva fra i parametri\nfrom sklearn.model_selection import RandomizedSearchCV # ricerca randomica fra i parametri con numero prefissato di candidati\n\nclf = RandomForestClassifier()\n\n\"\"\"param_list = {\n    'max_depth': [None, 2, 4, 6, 8, 16, 32],\n    'min_samples_leaf': [1, 5, 10, 20, 50, 100],\n    'min_samples_split': [2, 5, 10, 20, 50, 100],\n    'criterion': ['gini', 'entropy'],\n    'n_estimators': [10,20,50,100,200]\n}\n\n\ngrid_search = GridSearchCV(clf, param_grid=param_list, cv=5, scoring='accuracy') # ricerca esaustiva fra i parametri\ngrid_search.fit(X_train_res, y_train_res)# (NB: gli passo X_train,y_train ossia solo training perchè poi utilizzo il modello ritornato)\nclf_best_grid = grid_search.best_estimator_ # il miglior calssificatore trovato rispetto all'accuratezza\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"grid_search.best_params_\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"clf_best_grid\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(criterion='gini',\n                       max_depth=None, min_samples_leaf=1,\n                       min_samples_split=10,\n                       n_estimators=200) \n\nclf.fit(X_train_res, y_train_res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom scikitplot.metrics import plot_roc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy %s' % accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\nmatrix = plot_confusion_matrix(clf, X_test, y_test,\n                            #display_labels=le.classes_,\n                            cmap=plt.cm.Blues,\n                            xticks_rotation=70)\nmatrix.ax_.set_title('Confusion Matrix')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(y_test, clf.predict_proba(X_test))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Gradient Boosting Classifier**","metadata":{}},{"cell_type":"code","source":"!pip install xgboost","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numpy import loadtxt\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV # ricerca esaustiva fra i parametri\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import *\nclf = xgb.XGBClassifier()\n\n\nclf.fit(X_train_res,y_train_res)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"clf_best_grid\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"grid_search.best_params_\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy %s' % accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\nmatrix = plot_confusion_matrix(clf, X_test, y_test,\n                            #display_labels=le.classes_,\n                            cmap=plt.cm.Blues,\n                            xticks_rotation=70)\nmatrix.ax_.set_title('Confusion Matrix')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(y_test, clf.predict_proba(X_test))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Stacking Ensemble Classifier**","metadata":{}},{"cell_type":"code","source":"!pip install mlxtend","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport itertools\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\n\n# Classifiers\nfrom sklearn.svm import NuSVC, SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingCVClassifier # <- Here is our boy\n\n\n# Used to ignore warnings generated from StackingCVClassifier\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare ensemble to each baseline classifier\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom matplotlib import pyplot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get a stacking ensemble of models\ndef get_stacking():\n    # define the base models\n    level0 = list()\n    #level0.append(('lr', LogisticRegression()))\n    #level0.append(('knn', KNeighborsClassifier()))\n    level0.append(('rf', RandomForestClassifier()))\n    #level0.append(('xgb',  xgb.XGBClassifier()))\n    #level0.append(('gbc',  GradientBoostingClassifier()))\n    #level0.append(('cart', DecisionTreeClassifier()))\n    #level0.append(('linearsvm', LinearSVC()))\n    level0.append(('svm', SVC()))\n    #level0.append(('nusvm', NuSVC()))\n    level0.append(('mlp', MLPClassifier()))\n    #level0.append(('bayes', GaussianNB()))\n    # define meta learner model\n    level1 = LogisticRegression()\n    # define the stacking ensemble\n    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n    return model\n \n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    #models['lr'] = LogisticRegression()\n    #models['knn'] = KNeighborsClassifier()\n    models['rf'] = RandomForestClassifier()\n    #models['xgb'] = xgb.XGBClassifier()\n    #models['gbc'] = GradientBoostingClassifier()\n    #models['cart'] = DecisionTreeClassifier()\n    #models['linearsvm'] = LinearSVC()\n    models['svm'] = SVC()\n    #models['nusvm'] = NuSVC()\n    models['mlp'] = MLPClassifier()\n    #models['bayes'] = GaussianNB()\n    models['stacking'] = get_stacking()\n    return models\n \n# evaluate a give model using cross-validation\ndef evaluate_model(model, X_train_res, y_train_res):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X_train_res, y_train_res, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X_train_res, y_train_res)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n\n    # plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**After running the previous cell to calculate the single models' performances, I received the follwing results.**\n\n* lr 0.708 (0.020)\n* knn 0.664 (0.018)\n* rf 0.773 (0.018)\n* xgb 0.727 (0.018)\n* gbc 0.713 (0.017)\n* cart 0.615 (0.021)\n* linearsvm 0.742 (0.020)\n* svm 0.783 (0.016)\n* mlp 0.779 (0.015)\n\n\n**I decided then to keep the best three models and to run again the cell for getting back the single ones' and the stacking one performances**","metadata":{}},{"cell_type":"code","source":"# make a prediction with a stacking ensemble\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n# define the base models\nlevel0 = list()\nlevel0.append(('rf', RandomForestClassifier()))\nlevel0.append(('svm', SVC()))\nlevel0.append(('mlp', MLPClassifier()))\n# define meta learner model\nlevel1 = LogisticRegression()\n\n# define the stacking ensemble\nmodel = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n\n\n# fit the model on training set\nmodel.fit(X_train_res, y_train_res)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy %s' % accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\n\nmatrix = plot_confusion_matrix(model, X_test, y_test,\n                            #display_labels=le.classes_,\n                            cmap=plt.cm.Blues,\n                            xticks_rotation=70)\nmatrix.ax_.set_title('Confusion Matrix')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(y_test, model.predict_proba(X_test))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## I focus in test set. Need first to convert excerpt with tf/idf and the to get predictions ##","metadata":{}},{"cell_type":"code","source":"test_set.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dati = test_set['excerpt']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dati","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X= dati","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2# per features selection\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vect = CountVectorizer(min_df=1,ngram_range=(1,2))  # tokenization and feature extraction (fa tokenizzazione di default conterrà contatori )\n                                  #min_df=5 elimino parole sulla coda lunga (numerose e poco frequenti count<5)\n                                  #ngram_range=(1,2) considero solo le singole parole e  bigrammi, non trigrammi \nvect.fit(X)\nX = vect.transform(X)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfTransformer()  # weighting\ntfidf.fit(X)\nX= tfidf.transform(X)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}