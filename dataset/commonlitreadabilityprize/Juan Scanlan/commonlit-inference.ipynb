{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nimport logging\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import backend as K\nfrom transformers import RobertaTokenizer, TFRobertaModel, TFAutoModel, AutoTokenizer, AutoConfig\n\ntf.get_logger().setLevel(logging.ERROR)\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-07-15T15:51:22.372569Z","iopub.execute_input":"2021-07-15T15:51:22.373007Z","iopub.status.idle":"2021-07-15T15:51:31.970876Z","shell.execute_reply.started":"2021-07-15T15:51:22.372921Z","shell.execute_reply":"2021-07-15T15:51:31.969746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\n\ntr_dir = '../input/commonlit/submission1/*'\n\ntransformer_list = glob.glob(tr_dir)\ntransformer_list","metadata":{"execution":{"iopub.status.busy":"2021-07-15T15:51:33.564122Z","iopub.execute_input":"2021-07-15T15:51:33.56446Z","iopub.status.idle":"2021-07-15T15:51:33.588657Z","shell.execute_reply.started":"2021-07-15T15:51:33.564428Z","shell.execute_reply":"2021-07-15T15:51:33.587581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MODEL = ['albert-base-v2', 'bert-base-uncased', 'roberta-base', 'distilbert-base-uncased']\nroot = '../input/huggingface-offline-transformers/offline-transformers/'\nMODEL = [root + 'albert-base-v2', root + 'bert-base-uncased',\n         root + 'roberta-base', root + 'distilbert-base-uncased']","metadata":{"execution":{"iopub.status.busy":"2021-07-15T15:51:34.221481Z","iopub.execute_input":"2021-07-15T15:51:34.221862Z","iopub.status.idle":"2021-07-15T15:51:34.227265Z","shell.execute_reply.started":"2021-07-15T15:51:34.221829Z","shell.execute_reply":"2021-07-15T15:51:34.226066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(MODEL[0])\n# #transformer = TFAutoModel.from_pretrained(MODEL[0])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T15:51:34.801341Z","iopub.execute_input":"2021-07-15T15:51:34.801704Z","iopub.status.idle":"2021-07-15T15:51:34.805174Z","shell.execute_reply.started":"2021-07-15T15:51:34.801673Z","shell.execute_reply":"2021-07-15T15:51:34.80439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nx_test = df['excerpt']","metadata":{"tags":[],"execution":{"iopub.status.busy":"2021-07-15T15:51:35.38637Z","iopub.execute_input":"2021-07-15T15:51:35.386891Z","iopub.status.idle":"2021-07-15T15:51:35.407576Z","shell.execute_reply.started":"2021-07-15T15:51:35.386854Z","shell.execute_reply":"2021-07-15T15:51:35.406564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 250","metadata":{"execution":{"iopub.status.busy":"2021-07-15T15:51:35.985968Z","iopub.execute_input":"2021-07-15T15:51:35.986316Z","iopub.status.idle":"2021-07-15T15:51:35.990482Z","shell.execute_reply.started":"2021-07-15T15:51:35.986284Z","shell.execute_reply":"2021-07-15T15:51:35.989692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define function to encode text data in batches\ndef batch_encode(tokenizer, texts, batch_size=200, max_length=MAX_LENGTH):\n    \"\"\"\"\"\"\"\"\"\n    A function that encodes a batch of texts and returns the texts'\n    corresponding encodings and attention masks that are ready to be fed \n    into a pre-trained transformer model.\n    \n    Input:\n        - tokenizer:   Tokenizer object from the PreTrainedTokenizer Class\n        - texts:       List of strings where each string represents a text\n        - batch_size:  Integer controlling number of texts in a batch\n        - max_length:  Integer controlling max number of words to tokenize in a given text\n    Output:\n        - input_ids:       sequence of texts encoded as a tf.Tensor object\n        - attention_mask:  the texts' attention mask encoded as a tf.Tensor object\n    \"\"\"\"\"\"\"\"\"\n    \n    input_ids = []\n    attention_mask = []\n    \n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        inputs = tokenizer.batch_encode_plus(texts,\n                                             max_length=MAX_LENGTH,\n                                             padding='max_length', #implements dynamic padding\n                                             truncation=True,\n                                             return_attention_mask=True,\n                                             return_token_type_ids=False\n                                             )\n        input_ids.extend(inputs['input_ids'])\n        attention_mask.extend(inputs['attention_mask'])\n    \n    return tf.convert_to_tensor(input_ids), tf.convert_to_tensor(attention_mask)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T15:51:36.58031Z","iopub.execute_input":"2021-07-15T15:51:36.580663Z","iopub.status.idle":"2021-07-15T15:51:36.589031Z","shell.execute_reply.started":"2021-07-15T15:51:36.580634Z","shell.execute_reply":"2021-07-15T15:51:36.587773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(Model, max_length=MAX_LENGTH):\n    \"\"\"\n    Template for building a model off of the BERT or DistilBERT architecture\n    for a binary classification task.\n    \n    Input:\n      - transformer:  a base Hugging Face transformer model object (BERT or DistilBERT)\n                      with no added classification head attached.\n      - max_length:   integer controlling the maximum number of encoded tokens \n                      in a given sequence.\n    \n    Output:\n      - model:        a compiled tf.keras.Model with added classification layers \n                      on top of the base pre-trained model architecture.\n    \"\"\"\n    \n    # Transformer\n    transformer = TFAutoModel.from_pretrained(Model)\n    \n    # Make Transformer layers untrainable\n    for layer in transformer.layers:\n        layer.trainable = False\n    \n    # Define input layers\n    input_ids_layer = tf.keras.layers.Input(shape=(max_length,), \n                                            name='input_ids', \n                                            dtype='int32')\n    input_attention_layer = tf.keras.layers.Input(shape=(max_length,), \n                                                  name='input_attention', \n                                                  dtype='int32')\n    \n    # DistilBERT outputs a tuple where the first element at index 0\n    # represents the hidden-state at the output of the model's last layer.\n    # It is a tf.Tensor of shape (batch_size, sequence_length, hidden_size=768).\n    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n    \n    # We only care about DistilBERT's output for the [CLS] token, \n    # which is located at index 0 of every encoded sequence.  \n    # Splicing out the [CLS] tokens gives us 2D data.\n    cls_token = last_hidden_state[:, 0, :]\n    \n    ##                                                 ##\n    ## Define additional dropout and dense layers here ##\n    ##                                                 ##\n    \n    # Define a single node that makes up the output layer (for binary classification)\n    output = tf.keras.layers.Dense(1, activation='linear')(cls_token)\n    \n    # Define the model\n    model = tf.keras.Model([input_ids_layer, input_attention_layer], output)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-15T15:51:37.631184Z","iopub.execute_input":"2021-07-15T15:51:37.631597Z","iopub.status.idle":"2021-07-15T15:51:37.640419Z","shell.execute_reply.started":"2021-07-15T15:51:37.631557Z","shell.execute_reply":"2021-07-15T15:51:37.63929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initiate an empty vector to store prediction\npredictions = np.zeros(len(df))\n\nfor i, Model in enumerate(MODEL):    \n    print('\\n')\n    print('-'*50)\n    print(f'Predicting with model {Model}')\n    \n    #Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(Model)\n    \n    # Encode X_test\n    X_test_ids, X_test_attention = batch_encode(tokenizer, x_test.tolist())\n    \n    tr_model = build_model(Model, max_length=MAX_LENGTH)\n    \n    tr_model.load_weights(transformer_list[i])\n    \n    # Predict\n    fold_predictions = tr_model.predict([X_test_ids, X_test_attention]).reshape(-1)\n    \n    # Add fold prediction to the global predictions\n    predictions += fold_predictions / len(MODEL)\n        \n# Save submissions\ndf['target'] = predictions\ndf[['id', 'target']].to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T15:51:46.15525Z","iopub.execute_input":"2021-07-15T15:51:46.155658Z","iopub.status.idle":"2021-07-15T15:53:02.411599Z","shell.execute_reply.started":"2021-07-15T15:51:46.155628Z","shell.execute_reply":"2021-07-15T15:53:02.4104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-07-14T19:44:58.215278Z","iopub.status.idle":"2021-07-14T19:44:58.215781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}