{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install textstat\n!pip install rich\n\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport re\nimport nltk\nimport textstat\nimport time\nimport wandb\nimport rich\nimport spacy\n\nfrom pandas import DataFrame\nfrom matplotlib.lines import Line2D\nfrom rich.console import Console\nfrom rich import print\nfrom rich.theme import Theme\nfrom collections import Counter\nfrom wordcloud import WordCloud,STOPWORDS\nfrom spacy import displacy\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error as mse\n\nnltk.download('stopwords')\n\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2021-06-10T14:56:14.287527Z","iopub.execute_input":"2021-06-10T14:56:14.287869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\ndef custom_palette(custom_colors):\n    customPalette = sns.set_palette(sns.color_palette(custom_colors))\n    sns.palplot(sns.color_palette(custom_colors),size=0.8)\n    plt.tick_params(axis='both', labelsize=0, length = 0)\n\npalette = [\"#7209B7\",\"#3F88C5\",\"#136F63\",\"#F72585\",\"#FFBA08\"]\npalette2 = sns.diverging_palette(120, 220, n=20)\ncustom_palette(palette)\n\ncustom_theme = Theme({\n    \"info\" : \"italic bold cyan\",\n    \"warning\": \"italic bold magenta\",\n    \"danger\": \"bold blue\",\n    \"notice\": \"bold underline black\"\n})\n\n\nconsole = Console(theme=custom_theme)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"wandb_api_key\")\n\nos.environ[\"WANDB_SILENT\"] = \"true\"\n! wandb login $api_key\n# wandb.init(project='commonlit', entity='mo-5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nsample_submission_df = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_info(df):\n    console.print(\"HEAD\", style=\"notice\")\n    print(df.head())\n    console.print(\"SHAPE\", style=\"notice\")\n    print(df.shape)\n    console.print(\"UNIQUE\", style=\"notice\")\n    print(train_df.nunique())\n    console.print(\"NOT NULL\", style=\"notice\")\n    msno.bar(train_df,color=palette[2], sort=\"ascending\", figsize=(10,5), fontsize=12)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_info(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_readable = train_df.sort_values(by='target', ascending=False).iloc[0]\nleast_readable = train_df.sort_values(by='target').iloc[0]\n\nconsole.print(\"Most_readable\", style='notice')\nconsole.print(most_readable[\"target\"], style='info')\nconsole.print(most_readable[\"excerpt\"], style='warning')\n\nconsole.print(\"Least_readable\", style='notice')\nconsole.print(least_readable[\"target\"], style='info')\nconsole.print(least_readable[\"excerpt\"], style='warning')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# tagging\n\nhttps://www.guru99.com/pos-tagging-chunking-nltk.html#:~:text=Some%20NLTK%20POS%20tagging%20examples,tagging%20with%20NLTK%20is%20complete.","metadata":{}},{"cell_type":"code","source":"def count_tag(excerpt):\n    e = re.sub(\"[^a-zA-Z]\", \" \", excerpt)\n    e = e.lower()\n    e = nltk.word_tokenize(e)\n    e = nltk.pos_tag(e)\n\n    return DataFrame(e, columns=[\"word\", \"tag\"]).groupby(\"tag\").count().apply(lambda e: e / e.sum())\n\ndef mean_tag_count(excerpts):\n    tag_counts = []\n    for excerpt in excerpts:\n        tag_counts.append(count_tag(excerpt))\n    return pd.concat(tag_counts, axis=1, join=\"outer\").fillna(0).mean(axis=1)\n\ndef plot_tag_count(df,title,p):\n    plt.figure(figsize=(16,8))\n    sns.barplot(x='mean', y='tag', data=df,facecolor=(0, 0, 0, 0),linewidth=3,edgecolor=sns.color_palette(p,20))\n    plt.title(title,font='Serif')\n    plt.xlabel(\"Frequency\", fontsize=14)\n    plt.yticks(fontsize=13)\n    plt.xticks(rotation=45, fontsize=13)\n    plt.ylabel(\"\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag_list = [\n    'CC','CD','DT','IN',\n    'JJ','JJS','MD','NN',\n    'NNS','PRP','RB','TO',\n    'VB','VBD','VBG','VBN',\n    'VBP','VBZ','WP','WRB',\n    'PDT','PRP$','RP','WDT',\n    'RBR','EX','JJR','RBS',\n    'WP$','UH','FW','NNP',\n    'SYM','NNPS']\n\ndef count_tags(excerpt):\n    temp = count_tag(excerpt[\"excerpt\"]).T.reset_index()\n    return {i:temp[i][0] if i in temp.columns else 0 for i in tag_list}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ntag_df = train_df[[\"excerpt\"]].apply(count_tags, axis=1, result_type='expand')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_corr(corr):\n    fig = plt.figure(figsize=(9,9),dpi=80)\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    sns.heatmap(corr, mask=mask, cmap='BuPu', robust=True, center=0,\n                square=True, linewidths=.5)\n    plt.title('Correlation of Pos Tag', fontsize=15,font=\"Serif\")\n    plt.show()\n   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pd.concat([train_df[\"target\"], tag_df], axis=1).corr()\n\nshow_corr(corr)\ncorr[\"target\"].sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.scatterplot(data=train_df_tag,x='IN',y='target',color= palette[4],markers='.',alpha=0.5,label=\"IN\",)\nsns.scatterplot(data=train_df_tag,x='VBN',y='target',color= palette[1],markers='.',alpha=0.5,label=\"VBN\")\nplt.legend(title=\"POS tag\",bbox_to_anchor=(1.4, 1))\nplt.xlabel(\"POS tags count\")\nplt.title(\"POS vs Target (Negative correlation)\")\nplt.show()\n\nplt.figure(figsize=(10,8))\nsns.scatterplot(data=train_df_tag,x='PRP',y='target',color= palette[3],markers='.',alpha=0.5,label=\"PRP\")\nsns.scatterplot(data=train_df_tag,x='VBD',y='target',color= palette[0],markers='.',alpha=0.5,label=\"VBD\")\nplt.legend(title=\"POS tag\",bbox_to_anchor=(1.4, 1))\nplt.xlabel(\"POS tags count\")\nplt.title(\"POS vs Target (Positive correlation)\")\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# textstat","metadata":{}},{"cell_type":"code","source":"def text_stats(df):\n    e = df[\"excerpt\"]\n    return {\n        \"syllable_count\":               textstat.syllable_count(e),\n        \"lexicon_count\":                textstat.lexicon_count(e, removepunct=True),\n        \"sentence_count\":               textstat.sentence_count(e),\n        \"flesch_reading_ease\":          textstat.flesch_reading_ease(e),\n        \"flesch_kincaid_grade\":         textstat.flesch_kincaid_grade(e),\n        \"gunning_fog\":                  textstat.gunning_fog(e),\n        \"smog_index\":                   textstat.smog_index(e),\n        \"automated_readability_index\":  textstat.automated_readability_index(e),\n        \"coleman_liau_index\":           textstat.coleman_liau_index(e),\n        \"linsear_write_formula\":        textstat.linsear_write_formula(e),\n        \"dale_chall_readability_score\": textstat.dale_chall_readability_score(e),\n        \"text_standard\":                textstat.text_standard(e, float_output=True),\n    }\n\n\ntextstat_df = train_df[[\"excerpt\"]].apply(text_stats, axis=1, result_type='expand')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = pd.concat([train_df[\"target\"], textstat_df], axis=1).corr()\nshow_corr(corr)\ncorr[\"target\"].sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# train","metadata":{}},{"cell_type":"code","source":"def training(X, y, model, params, folds=9):\n    mdls = []\n    kf = KFold(n_splits=folds, shuffle=True, random_state=21)\n    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        mdl = model(**params)\n        mdl.fit(X_train, y_train,\n                eval_set=[(X_test, y_test)],\n                early_stopping_rounds=100,\n                verbose=100)\n        mdls.append(mdl)\n\n        pred = mdl.predict(np.ascontiguousarray(X_test))\n        loss = np.sqrt(mean_squared_error(y_test, pred))\n        print(f\" Log loss: {loss}\")\n        print(\"-\"*50)\n    \n    return mdls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.concat([train_df[\"target\"], tag_df, textstat_df], axis=1)\nX = temp.drop(\"target\", axis=1)\ny = temp[\"target\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = { # デフォルト\n    'n_estimators': 3000,\n    'max_depth': 5,\n    'learning_rate': 0.01,\n#     'tree_method': 'gpu_hist',\n}\n\nmodels = training(X, y, XGBRegressor, params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tag_df = test_df[[\"excerpt\"]].apply(count_tags, axis=1, result_type='expand')\ntest_textstat_df = train_df[[\"excerpt\"]].apply(text_stats, axis=1, result_type='expand')\ntest_X = pd.concat([tag_df, textstat_df], axis=1)\ny_targets = []\nfor model in models:\n  y_target = model.predict(np.ascontiguousarray(test_X))\n  y_target = pd.DataFrame(y_target)\n  y_targets.append(y_target)\ny_target = sum(y_targets)/len(y_targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = sample_submission_df.copy()\nsubmission_df[\"target\"] = y_target\nsubmission_df.to_csv(\"/kaggle/working/submission.csv\", index=False)\nsubmission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}