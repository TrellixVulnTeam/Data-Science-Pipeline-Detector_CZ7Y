{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is an inference notebook with the `t5-large` model. You can achieve decent results with this model and choose to use the provided model in an ensemble of yours.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-17T12:44:41.881869Z","iopub.execute_input":"2021-07-17T12:44:41.882222Z","iopub.status.idle":"2021-07-17T12:44:41.938606Z","shell.execute_reply.started":"2021-07-17T12:44:41.882191Z","shell.execute_reply":"2021-07-17T12:44:41.93699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Dict, Any, Union\n\nfrom pathlib import Path\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\n\nimport torch.utils.data as D\nfrom torch.utils.data.dataset import Dataset, IterableDataset\nfrom torch.utils.data.dataloader import DataLoader\n\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom transformers import PreTrainedModel\n\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\n\nimport yaml, gc","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:44:43.02474Z","iopub.execute_input":"2021-07-17T12:44:43.025199Z","iopub.status.idle":"2021-07-17T12:44:45.742242Z","shell.execute_reply.started":"2021-07-17T12:44:43.025155Z","shell.execute_reply":"2021-07-17T12:44:45.74137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Folders and Dataframes","metadata":{}},{"cell_type":"code","source":"BASE_PATH = Path('/kaggle/input/commonlit-t5-large')\nDATA_PATH = Path('/kaggle/input/commonlitreadabilityprize/')\nassert DATA_PATH.exists()\nMODELS_PATH = Path(BASE_PATH/'best_models')\nassert MODELS_PATH.exists()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:44:47.239563Z","iopub.execute_input":"2021-07-17T12:44:47.239877Z","iopub.status.idle":"2021-07-17T12:44:47.244937Z","shell.execute_reply.started":"2021-07-17T12:44:47.239846Z","shell.execute_reply":"2021-07-17T12:44:47.243723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(DATA_PATH/'train.csv')\ntest_df = pd.read_csv(DATA_PATH/'test.csv')\nsample_df = pd.read_csv(DATA_PATH/'sample_submission.csv')\n\ndef remove_unnecessary(df):\n    df.drop(df[df['target'] == 0].index, inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    \nremove_unnecessary(train_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:44:48.234573Z","iopub.execute_input":"2021-07-17T12:44:48.2349Z","iopub.status.idle":"2021-07-17T12:44:48.4651Z","shell.execute_reply.started":"2021-07-17T12:44:48.234868Z","shell.execute_reply":"2021-07-17T12:44:48.464269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configuration","metadata":{}},{"cell_type":"code","source":"class Config(): \n    NUM_FOLDS = 6\n    NUM_EPOCHS = 3\n    BATCH_SIZE = 16\n    MAX_LEN = 248\n    MODEL_PATH = BASE_PATH/'lm'\n#     TOKENIZER_PATH = str(MODELS_PATH/'roberta-base-0')\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    SEED = 1000\n    NUM_WORKERS = 2\n    MODEL_FOLDER = MODELS_PATH\n    model_name = 't5-large'\n    svm_kernels = ['rbf']\n    svm_c = 5\n\ncfg = Config()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:44:49.718096Z","iopub.execute_input":"2021-07-17T12:44:49.718438Z","iopub.status.idle":"2021-07-17T12:44:49.788598Z","shell.execute_reply.started":"2021-07-17T12:44:49.718408Z","shell.execute_reply":"2021-07-17T12:44:49.787598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['normalized_target'] = (train_df['target'] - train_df['target'].mean()) / train_df['target'].std()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:44:50.480166Z","iopub.execute_input":"2021-07-17T12:44:50.480478Z","iopub.status.idle":"2021-07-17T12:44:50.488266Z","shell.execute_reply.started":"2021-07-17T12:44:50.48045Z","shell.execute_reply":"2021-07-17T12:44:50.487342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read Existing Models","metadata":{}},{"cell_type":"code","source":"model_path = MODELS_PATH\nassert model_path.exists()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:44:51.584636Z","iopub.execute_input":"2021-07-17T12:44:51.584954Z","iopub.status.idle":"2021-07-17T12:44:51.588853Z","shell.execute_reply.started":"2021-07-17T12:44:51.584924Z","shell.execute_reply":"2021-07-17T12:44:51.587735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls {MODELS_PATH}","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:44:51.950768Z","iopub.execute_input":"2021-07-17T12:44:51.951099Z","iopub.status.idle":"2021-07-17T12:44:52.596974Z","shell.execute_reply.started":"2021-07-17T12:44:51.951067Z","shell.execute_reply":"2021-07-17T12:44:52.595974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    \n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        \n        self.hidden_layer = nn.Linear(in_features, hidden_dim)\n        self.final_layer = nn.Linear(hidden_dim, num_targets)\n        self.out_features = hidden_dim\n        \n    def forward(self, features):\n        att = torch.tanh(self.hidden_layer(features))\n        score = self.final_layer(att)\n        attention_weights = torch.softmax(score, dim=1)\n        return attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:44:52.598781Z","iopub.execute_input":"2021-07-17T12:44:52.59915Z","iopub.status.idle":"2021-07-17T12:44:52.606189Z","shell.execute_reply.started":"2021-07-17T12:44:52.599111Z","shell.execute_reply":"2021-07-17T12:44:52.605025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5EncoderModel\n\nclass CommonLitModel(nn.Module):\n    def __init__(self):\n        super(CommonLitModel, self).__init__()\n        config = AutoConfig.from_pretrained(cfg.MODEL_PATH)\n        config.update({\n            \"output_hidden_states\": True,\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })\n        self.transformer_model = T5EncoderModel.from_pretrained(cfg.MODEL_PATH, config=config)\n        self.attention = AttentionHead(config.hidden_size, 512, 1)\n        self.regressor = nn.Linear(config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        last_layer_hidden_states = self.transformer_model(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n        return self.regressor(context_vector), context_vector","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:47:04.490833Z","iopub.execute_input":"2021-07-17T12:47:04.491178Z","iopub.status.idle":"2021-07-17T12:47:04.498529Z","shell.execute_reply.started":"2021-07-17T12:47:04.491147Z","shell.execute_reply":"2021-07-17T12:47:04.497416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(i):\n    inference_model = CommonLitModel()\n    inference_model = inference_model.cuda()\n    inference_model.load_state_dict(torch.load(str(model_path/f'{i + 1}_pytorch_model.bin')))\n    inference_model.eval();\n    return inference_model","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:47:04.823342Z","iopub.execute_input":"2021-07-17T12:47:04.823674Z","iopub.status.idle":"2021-07-17T12:47:04.828527Z","shell.execute_reply.started":"2021-07-17T12:47:04.823643Z","shell.execute_reply":"2021-07-17T12:47:04.82749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DataSet and Tokenizers","metadata":{}},{"cell_type":"code","source":"def convert_to_list(t):\n    return t.flatten().long()\n\nclass CommonLitDataset(nn.Module):\n    def __init__(self, text, test_id, tokenizer, max_len=128):\n        self.excerpt = text\n        self.test_id = test_id\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        return {'input_ids': convert_to_list(encode['input_ids']),\n                'attention_mask': convert_to_list(encode['attention_mask']),\n                'id': self.test_id[idx]}\n    \n    def __len__(self):\n        return len(self.excerpt)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:47:06.608811Z","iopub.execute_input":"2021-07-17T12:47:06.609164Z","iopub.status.idle":"2021-07-17T12:47:06.61556Z","shell.execute_reply.started":"2021-07-17T12:47:06.609131Z","shell.execute_reply":"2021-07-17T12:47:06.614708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls {MODELS_PATH}/tokenizer-1","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:47:07.115662Z","iopub.execute_input":"2021-07-17T12:47:07.115975Z","iopub.status.idle":"2021-07-17T12:47:07.759016Z","shell.execute_reply.started":"2021-07-17T12:47:07.115942Z","shell.execute_reply":"2021-07-17T12:47:07.758083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import T5Tokenizer\n\ntokenizers = []\nfor i in range(1, cfg.NUM_FOLDS):\n    tokenizer_path = MODELS_PATH/f\"tokenizer-{i}\"\n    print(tokenizer_path)\n    assert(Path(tokenizer_path).exists())\n    tokenizer = T5Tokenizer.from_pretrained(str(tokenizer_path))\n    tokenizers.append(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:47:07.761275Z","iopub.execute_input":"2021-07-17T12:47:07.761534Z","iopub.status.idle":"2021-07-17T12:47:08.077778Z","shell.execute_reply.started":"2021-07-17T12:47:07.761506Z","shell.execute_reply":"2021-07-17T12:47:08.076874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dl(df, tokenizer):\n    text = df['excerpt'].values\n    ids = df['id'].values\n    ds = CommonLitDataset(text, ids, tokenizer, max_len=cfg.MAX_LEN)\n    return DataLoader(ds, \n                      batch_size = cfg.BATCH_SIZE,\n                      shuffle=False,\n                      num_workers = 1,\n                      pin_memory=True,\n                      drop_last=False\n                     )","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:47:08.079445Z","iopub.execute_input":"2021-07-17T12:47:08.079795Z","iopub.status.idle":"2021-07-17T12:47:08.085134Z","shell.execute_reply.started":"2021-07-17T12:47:08.079757Z","shell.execute_reply":"2021-07-17T12:47:08.084255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Extract Embeddings","metadata":{}},{"cell_type":"code","source":"def get_cls_embeddings(dl, transformer_model):\n    cls_embeddings = []\n    with torch.no_grad():\n        for input_features in tqdm(dl, total=len(dl)):\n            _, context_vector = transformer_model(input_features['input_ids'].cuda(), input_features['attention_mask'].cuda())\n#             cls_embeddings.extend(output['last_hidden_state'][:,0,:].detach().cpu().numpy())\n            embedding_out = context_vector.detach().cpu().numpy()\n            cls_embeddings.extend(embedding_out)\n    return np.array(cls_embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:47:09.262461Z","iopub.execute_input":"2021-07-17T12:47:09.262807Z","iopub.status.idle":"2021-07-17T12:47:09.269925Z","shell.execute_reply.started":"2021-07-17T12:47:09.262764Z","shell.execute_reply":"2021-07-17T12:47:09.267447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Extract Number of Bins","metadata":{"execution":{"iopub.status.busy":"2021-05-27T10:01:49.420156Z","iopub.execute_input":"2021-05-27T10:01:49.420515Z","iopub.status.idle":"2021-05-27T10:01:49.424629Z","shell.execute_reply.started":"2021-05-27T10:01:49.420481Z","shell.execute_reply":"2021-05-27T10:01:49.423522Z"}}},{"cell_type":"code","source":"num_bins = int(np.ceil(np.log2(len(train_df))))\ntrain_df['bins'] = pd.cut(train_df['target'], bins=num_bins, labels=False)\nbins = train_df['bins'].values","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:47:10.323418Z","iopub.execute_input":"2021-07-17T12:47:10.323753Z","iopub.status.idle":"2021-07-17T12:47:10.330818Z","shell.execute_reply.started":"2021-07-17T12:47:10.323726Z","shell.execute_reply":"2021-07-17T12:47:10.330044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training","metadata":{}},{"cell_type":"code","source":"def rmse_score(X, y):\n    return np.sqrt(mean_squared_error(X, y))","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:47:11.324178Z","iopub.execute_input":"2021-07-17T12:47:11.324506Z","iopub.status.idle":"2021-07-17T12:47:11.328667Z","shell.execute_reply.started":"2021-07-17T12:47:11.324477Z","shell.execute_reply":"2021-07-17T12:47:11.327295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain_target = train_df['normalized_target'].values\n\ndef calc_mean(scores):\n    return np.mean(np.array(scores), axis=0)\n\nfinal_scores = []\nfinal_rmse = []\nfor j, tokenizer in enumerate(tokenizers):\n    print('Model', j)\n    test_dl = create_dl(test_df, tokenizer)\n    train_dl = create_dl(train_df, tokenizer)\n    transformer_model = load_model(j)\n    transformer_model.cuda()\n    X = get_cls_embeddings(train_dl, transformer_model)\n    y = train_target\n    X_test = get_cls_embeddings(test_dl, transformer_model)\n    kfold = StratifiedKFold(n_splits=cfg.NUM_FOLDS)\n    scores = []\n    rmse_scores = []\n    for kernel in cfg.svm_kernels:\n        print('Kernel', kernel)\n        kernel_scores = []\n        kernel_rmse_scores = []\n        for k, (train_idx, valid_idx) in enumerate(kfold.split(X, bins)):\n\n            print('Fold', k, train_idx.shape, valid_idx.shape)\n            model = SVR(C=cfg.svm_c, kernel=kernel, gamma='auto')\n\n            X_train, y_train = X[train_idx], y[train_idx]\n            X_valid, y_valid = X[valid_idx], y[valid_idx]\n            model.fit(X_train, y_train)\n            prediction = model.predict(X_valid)\n            kernel_rmse_scores.append(rmse_score(prediction, y_valid))\n            print('rmse_score', kernel_rmse_scores[k])\n            kernel_scores.append(model.predict(X_test))\n        scores.append(calc_mean(kernel_scores))\n        rmse_scores.append(calc_mean(kernel_rmse_scores))\n    final_scores.append(calc_mean(scores))\n    final_rmse.append(calc_mean(rmse_scores))\n    del transformer_model\n    torch.cuda.empty_cache()\n    del tokenizer\n    gc.collect()\nprint('FINAL RMSE score', np.mean(np.array(final_rmse)))","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:47:11.856856Z","iopub.execute_input":"2021-07-17T12:47:11.857227Z","iopub.status.idle":"2021-07-17T12:56:17.010046Z","shell.execute_reply.started":"2021-07-17T12:47:11.857197Z","shell.execute_reply":"2021-07-17T12:56:17.009044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_scores_bck = final_scores","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:57:16.759175Z","iopub.execute_input":"2021-07-17T12:57:16.759515Z","iopub.status.idle":"2021-07-17T12:57:16.763929Z","shell.execute_reply.started":"2021-07-17T12:57:16.759481Z","shell.execute_reply":"2021-07-17T12:57:16.763062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_scores_bck","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:57:17.272631Z","iopub.execute_input":"2021-07-17T12:57:17.272934Z","iopub.status.idle":"2021-07-17T12:57:17.282727Z","shell.execute_reply.started":"2021-07-17T12:57:17.272903Z","shell.execute_reply":"2021-07-17T12:57:17.281754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# (train_df['target'] - cfg.train_target_mean) / cfg.train_target_std\nfinal_scores = np.array(final_scores) * train_df['target'].std() + train_df['target'].mean()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:57:17.659942Z","iopub.execute_input":"2021-07-17T12:57:17.66031Z","iopub.status.idle":"2021-07-17T12:57:17.665929Z","shell.execute_reply.started":"2021-07-17T12:57:17.660282Z","shell.execute_reply":"2021-07-17T12:57:17.664918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_scores","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:57:18.291521Z","iopub.execute_input":"2021-07-17T12:57:18.291868Z","iopub.status.idle":"2021-07-17T12:57:18.2986Z","shell.execute_reply.started":"2021-07-17T12:57:18.291831Z","shell.execute_reply":"2021-07-17T12:57:18.297441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Ensure the mean of the prediction equals the mean of the training data","metadata":{}},{"cell_type":"code","source":"def calc_mean(scores):\n    return np.mean(np.array(scores), axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:57:22.128408Z","iopub.execute_input":"2021-07-17T12:57:22.128738Z","iopub.status.idle":"2021-07-17T12:57:22.132437Z","shell.execute_reply.started":"2021-07-17T12:57:22.128707Z","shell.execute_reply":"2021-07-17T12:57:22.131603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_mean = train_df['target'].mean()\nfinal_scores_flat = calc_mean(final_scores).flatten()\nfinal_scores_mean = final_scores_flat.mean()\ntarget_mean, np.array(final_scores).mean()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:57:22.970322Z","iopub.execute_input":"2021-07-17T12:57:22.970632Z","iopub.status.idle":"2021-07-17T12:57:22.977165Z","shell.execute_reply.started":"2021-07-17T12:57:22.970603Z","shell.execute_reply":"2021-07-17T12:57:22.976287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_diff = target_mean - final_scores_mean\nmean_diff, mean_diff / len(final_scores)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:57:23.748966Z","iopub.execute_input":"2021-07-17T12:57:23.749356Z","iopub.status.idle":"2021-07-17T12:57:23.75536Z","shell.execute_reply.started":"2021-07-17T12:57:23.749316Z","shell.execute_reply":"2021-07-17T12:57:23.754391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_df['target'] = final_scores_flat\n# sample_df['target'] = len(final_scores) / np.sum(1 / np.array(final_scores), axis=0) # harmonic mean\nsample_df","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:57:30.203902Z","iopub.execute_input":"2021-07-17T12:57:30.204259Z","iopub.status.idle":"2021-07-17T12:57:30.219422Z","shell.execute_reply.started":"2021-07-17T12:57:30.204228Z","shell.execute_reply":"2021-07-17T12:57:30.218077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(sample_df).to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:57:30.682681Z","iopub.execute_input":"2021-07-17T12:57:30.68306Z","iopub.status.idle":"2021-07-17T12:57:30.955269Z","shell.execute_reply.started":"2021-07-17T12:57:30.683006Z","shell.execute_reply":"2021-07-17T12:57:30.95442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cat submission.csv","metadata":{"execution":{"iopub.status.busy":"2021-07-12T07:07:21.682038Z","iopub.execute_input":"2021-07-12T07:07:21.682378Z","iopub.status.idle":"2021-07-12T07:07:22.427525Z","shell.execute_reply.started":"2021-07-12T07:07:21.682342Z","shell.execute_reply":"2021-07-12T07:07:22.42655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}