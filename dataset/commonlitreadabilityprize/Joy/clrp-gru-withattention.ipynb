{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T21:27:24.21185Z","iopub.execute_input":"2021-07-30T21:27:24.212231Z","iopub.status.idle":"2021-07-30T21:27:24.222253Z","shell.execute_reply.started":"2021-07-30T21:27:24.212197Z","shell.execute_reply":"2021-07-30T21:27:24.221403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Look at the Features\nhttps://www.kaggle.com/donmarch14/commonlit-detailed-guide-to-learn-nlp","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\n\n\nfrom wordcloud import WordCloud\n\n\nfrom collections import Counter\nimport os\nimport numpy as np\nimport re\nimport string\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.util import ngrams    \n\nimport html\nimport unicodedata\n\nstop_words = stopwords.words('english')\n%config InlineBackend.figure_format = 'retina'","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:24.223392Z","iopub.execute_input":"2021-07-30T21:27:24.223841Z","iopub.status.idle":"2021-07-30T21:27:33.737193Z","shell.execute_reply.started":"2021-07-30T21:27:24.223809Z","shell.execute_reply":"2021-07-30T21:27:33.736001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport string\nimport numpy as np\nimport pandas as pd\nfrom string import digits\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nimport logging\nimport tensorflow as tf\n# tf.enable_eager_execution()\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\nlogging.getLogger('tensorflow').setLevel(logging.FATAL)\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\nimport unicodedata\nimport io\nimport time\nimport warnings\nimport sys\nimport nltk\nfrom nltk.probability import FreqDist","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:33.739101Z","iopub.execute_input":"2021-07-30T21:27:33.739432Z","iopub.status.idle":"2021-07-30T21:27:33.752131Z","shell.execute_reply.started":"2021-07-30T21:27:33.739402Z","shell.execute_reply":"2021-07-30T21:27:33.750892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wordcloud(text,ngram=1):\n    wordcloud = WordCloud(width=1400, \n                            height=800,\n                            random_state=2021,\n                            background_color='black',\n                            )\n    if ngram ==1:\n        wordc = wordcloud.generate(' '.join(text))\n    else:\n        wordc = wordcloud.generate_from_frequencies(text)\n    plt.figure(figsize=(12,6), facecolor='k')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n      \n\ndef get_n_grans_count(text, n_grams, min_freq):\n    output = {}\n    tokens = nltk.word_tokenize(text)\n\n    #Create the n_gram\n    if n_grams == 2:\n        gs = nltk.bigrams(tokens)\n        \n    elif n_grams == 3:\n        gs = nltk.trigrams(tokens)\n\n    else:\n        return 'Only 2_grams and 3_grams are supported'\n    \n    # compute frequency distribution for all the bigrams in the text by threshold with min_freq\n    fdist = nltk.FreqDist(gs)\n    for k,v in fdist.items():\n        if v > min_freq:\n            index = ' '.join(k)\n            output[index] = v\n    \n    return output\n    \ndef remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text, and by defult lemmatize nouns\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    words = text2words(text)\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem or lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:33.753792Z","iopub.execute_input":"2021-07-30T21:27:33.754108Z","iopub.status.idle":"2021-07-30T21:27:33.777605Z","shell.execute_reply.started":"2021-07-30T21:27:33.75408Z","shell.execute_reply":"2021-07-30T21:27:33.776074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_raw_train = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ndf_raw_train.head()\ninput_train, input_val, target_train, target_val = train_test_split(df_raw_train['excerpt'], df_raw_train['target'], test_size=0.2)\ndf_raw_train['excerpt'] = [normalize_text(sent) for sent in df_raw_train['excerpt']]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:33.779715Z","iopub.execute_input":"2021-07-30T21:27:33.780178Z","iopub.status.idle":"2021-07-30T21:27:43.998693Z","shell.execute_reply.started":"2021-07-30T21:27:33.780119Z","shell.execute_reply":"2021-07-30T21:27:43.997763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_val","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44Z","iopub.execute_input":"2021-07-30T21:27:44.000477Z","iopub.status.idle":"2021-07-30T21:27:44.011467Z","shell.execute_reply.started":"2021-07-30T21:27:44.000427Z","shell.execute_reply":"2021-07-30T21:27:44.010486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_raw_train[['excerpt','target']]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.012967Z","iopub.execute_input":"2021-07-30T21:27:44.013506Z","iopub.status.idle":"2021-07-30T21:27:44.025152Z","shell.execute_reply.started":"2021-07-30T21:27:44.013459Z","shell.execute_reply":"2021-07-30T21:27:44.023764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(lang):\n    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',oov_token='<oov>')\n    lang_tokenizer.fit_on_texts(lang)\n    tensor = lang_tokenizer.texts_to_sequences(lang)\n    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n    return tensor, lang_tokenizer\n\ndef load_dataset():\n    #targ_val, inp_lang = df_train['target'],df_train['excerpt']\n    targ_val = df_train['target']\n    inp_lang = get_lang()\n    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n    return input_tensor, inp_lang_tokenizer\n\ndef get_lang():\n    en=[]\n    for i in df_train['excerpt']:\n        en_1 = [w for w in i.split(' ')]\n        en.append(en_1)\n    return en","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.029073Z","iopub.execute_input":"2021-07-30T21:27:44.029596Z","iopub.status.idle":"2021-07-30T21:27:44.040864Z","shell.execute_reply.started":"2021-07-30T21:27:44.02956Z","shell.execute_reply":"2021-07-30T21:27:44.039625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor, inp_lang = load_dataset()\ntarget_tensor = df_train['target']","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.043611Z","iopub.execute_input":"2021-07-30T21:27:44.044158Z","iopub.status.idle":"2021-07-30T21:27:44.530056Z","shell.execute_reply.started":"2021-07-30T21:27:44.044112Z","shell.execute_reply":"2021-07-30T21:27:44.528976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\nprint(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.531319Z","iopub.execute_input":"2021-07-30T21:27:44.531616Z","iopub.status.idle":"2021-07-30T21:27:44.540035Z","shell.execute_reply.started":"2021-07-30T21:27:44.531588Z","shell.execute_reply":"2021-07-30T21:27:44.538984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = len(input_tensor_train)\nBATCH_SIZE = 64\nsteps_per_epoch = len(input_tensor_train)//BATCH_SIZE\nembedding_dim = 128\nunits = 256\nvocab_inp_size = len(inp_lang.word_index)+1\ntar_int_size = 64\n\ndataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.54171Z","iopub.execute_input":"2021-07-30T21:27:44.54245Z","iopub.status.idle":"2021-07-30T21:27:44.591954Z","shell.execute_reply.started":"2021-07-30T21:27:44.542404Z","shell.execute_reply":"2021-07-30T21:27:44.590875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.enc_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform')\n\n    def call(self, x, hidden):\n        x = self.embedding(x)\n        output, state = self.gru(x, initial_state = hidden)\n        return output, state\n\n    def initialize_hidden_state(self):\n        return tf.zeros((self.batch_sz, self.enc_units))\n\nencoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n\nclass BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, query, values):\n        hidden_with_time_axis = tf.expand_dims(query, 1)\n        score = self.V(tf.nn.tanh(\n            self.W1(values) + self.W2(hidden_with_time_axis)))\n        attention_weights = tf.nn.softmax(score, axis=1)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector, attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.593327Z","iopub.execute_input":"2021-07-30T21:27:44.593623Z","iopub.status.idle":"2021-07-30T21:27:44.627109Z","shell.execute_reply.started":"2021-07-30T21:27:44.593593Z","shell.execute_reply":"2021-07-30T21:27:44.626003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, tar_int_size, embedding_dim, dec_units, batch_sz):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.fc = tf.keras.layers.Dense(tar_int_size)\n        self.attention = BahdanauAttention(self.dec_units)\n        self.fc2 = tf.keras.layers.Dense(1)\n\n    def call(self, x, hidden, enc_output):\n        context_vector, attention_weights = self.attention(hidden, enc_output)\n        x = tf.concat([context_vector, x], axis=-1)\n#         print(x.shape)\n#         output, state = self.gru(x)\n#         output = tf.reshape(output, (-1, output.shape[2]))\n        x = self.fc(x)\n        x = self.fc2(x)\n        return x, attention_weights\n\ndecoder = Decoder(tar_int_size, embedding_dim, units, BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.628717Z","iopub.execute_input":"2021-07-30T21:27:44.629129Z","iopub.status.idle":"2021-07-30T21:27:44.649736Z","shell.execute_reply.started":"2021-07-30T21:27:44.62909Z","shell.execute_reply":"2021-07-30T21:27:44.648678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.MeanSquaredError()\n\ndef loss_function(real, pred):\n    print(real,pred)\n    \n    loss_  = loss_object(real, pred)\n    return tf.reduce_mean(loss_)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.651206Z","iopub.execute_input":"2021-07-30T21:27:44.651634Z","iopub.status.idle":"2021-07-30T21:27:44.658282Z","shell.execute_reply.started":"2021-07-30T21:27:44.651593Z","shell.execute_reply":"2021-07-30T21:27:44.657063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.660136Z","iopub.execute_input":"2021-07-30T21:27:44.660574Z","iopub.status.idle":"2021-07-30T21:27:44.671325Z","shell.execute_reply.started":"2021-07-30T21:27:44.660529Z","shell.execute_reply":"2021-07-30T21:27:44.670143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(inp, targ, enc_hidden):\n    loss = 0\n    with tf.GradientTape() as tape:\n        enc_output, enc_hidden = encoder(inp, enc_hidden)\n        dec_hidden = enc_hidden\n#         print(dec_hidden.shape)\n        #dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n        dec_input = enc_hidden\n        # Teacher forcing\n        #for t in range(1, targ.shape[1]):\n        prediction,attention_weights = decoder(dec_input, dec_hidden, enc_output)\n        loss = loss_function(targ, prediction)\n        #dec_input = tf.expand_dims(targ[:, t], 1)\n\n    batch_loss = loss\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))      \n    return batch_loss","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.67374Z","iopub.execute_input":"2021-07-30T21:27:44.674063Z","iopub.status.idle":"2021-07-30T21:27:44.684811Z","shell.execute_reply.started":"2021-07-30T21:27:44.674035Z","shell.execute_reply":"2021-07-30T21:27:44.683777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(epochs):\n    EPOCHS = epochs\n\n    for epoch in range(EPOCHS):\n        start = time.time()\n        enc_hidden = encoder.initialize_hidden_state()\n        total_loss = 0\n        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n            batch_loss = train_step(inp, targ, enc_hidden)\n            total_loss += batch_loss\n            if batch % 100 == 0:\n                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                             batch,\n                                                             batch_loss.numpy()))\n        if (epoch + 1) % 2 == 0:\n            checkpoint.save(file_prefix = checkpoint_prefix)\n\n        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                          total_loss / steps_per_epoch))\n        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n\n        \n# train_model(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.686026Z","iopub.execute_input":"2021-07-30T21:27:44.686312Z","iopub.status.idle":"2021-07-30T21:27:44.702084Z","shell.execute_reply.started":"2021-07-30T21:27:44.686286Z","shell.execute_reply":"2021-07-30T21:27:44.701159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def max_length(tensor):\n    return max(len(t) for t in tensor)\nmax_length_inp =  max_length(input_tensor)\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.703362Z","iopub.execute_input":"2021-07-30T21:27:44.703648Z","iopub.status.idle":"2021-07-30T21:27:44.722441Z","shell.execute_reply.started":"2021-07-30T21:27:44.703622Z","shell.execute_reply":"2021-07-30T21:27:44.721287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = []\nfor key in inp_lang.word_index.items():\n    words.append(key[0])\n\ndef evaluate(sentence):\n    sentence = normalize_text(sentence)\n    inputs = [inp_lang.word_index[i] if i in words else 1 for i in sentence.split(' ')]\n    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp,padding='post')\n    inputs = tf.convert_to_tensor(inputs)\n    hidden = [tf.zeros((1, units))]\n    enc_out, enc_hidden = encoder(inputs, hidden)\n    dec_hidden = enc_hidden\n    dec_input = enc_hidden\n    pred, attention_weights = decoder(dec_input,dec_hidden,enc_out)\n#     print(pred.shape)\n    return pred.numpy()[0][0]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:48:25.166921Z","iopub.execute_input":"2021-07-30T21:48:25.167272Z","iopub.status.idle":"2021-07-30T21:48:25.180212Z","shell.execute_reply.started":"2021-07-30T21:48:25.167243Z","shell.execute_reply":"2021-07-30T21:48:25.179391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(u'politicians do not have permission to do what needs to be done.')","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:44.73304Z","iopub.execute_input":"2021-07-30T21:27:44.733327Z","iopub.status.idle":"2021-07-30T21:27:45.049715Z","shell.execute_reply.started":"2021-07-30T21:27:44.733301Z","shell.execute_reply":"2021-07-30T21:27:45.048764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df = pd.DataFrame(input_val,columns=[\"excerpt\"])\nvalidation_df['target']= target_val\nvalidation_df['prediction'] = validation_df['excerpt'].apply(lambda value : evaluate(value))\nvalidation_df['deviation'] = validation_df['target'] - validation_df['prediction']\nvalidation_df.sort_values('deviation',ascending =False)\nprint('RMSE',np.sqrt(np.sum(np.square(validation_df['deviation']))/validation_df.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:27:45.05116Z","iopub.execute_input":"2021-07-30T21:27:45.051557Z","iopub.status.idle":"2021-07-30T21:28:50.461208Z","shell.execute_reply.started":"2021-07-30T21:27:45.051516Z","shell.execute_reply":"2021-07-30T21:28:50.459819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsubmission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\ntest_data['predict'] = test_data['excerpt'].apply(lambda value : evaluate(value))\n\n\nsubmission = pd.DataFrame()\nsubmission['id'] = test_data['id']\nsubmission['target'] = test_data['predict']\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission\n","metadata":{"execution":{"iopub.status.busy":"2021-07-30T21:48:54.433778Z","iopub.execute_input":"2021-07-30T21:48:54.43414Z","iopub.status.idle":"2021-07-30T21:48:55.313308Z","shell.execute_reply.started":"2021-07-30T21:48:54.434108Z","shell.execute_reply":"2021-07-30T21:48:55.312067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}