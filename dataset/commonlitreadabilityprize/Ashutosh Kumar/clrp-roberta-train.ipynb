{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport gc\nimport sys\nimport time\nimport string\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport transformers\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification,\n                          get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup)\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold, GroupKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts, CosineAnnealingLR, \n                                      ReduceLROnPlateau)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-21T15:28:44.869808Z","iopub.execute_input":"2021-05-21T15:28:44.870349Z","iopub.status.idle":"2021-05-21T15:28:54.050168Z","shell.execute_reply.started":"2021-05-21T15:28:44.870246Z","shell.execute_reply":"2021-05-21T15:28:54.049262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:28:54.051627Z","iopub.execute_input":"2021-05-21T15:28:54.051972Z","iopub.status.idle":"2021-05-21T15:28:54.05814Z","shell.execute_reply.started":"2021-05-21T15:28:54.051936Z","shell.execute_reply":"2021-05-21T15:28:54.056713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config():\n    def __init__(self):\n        # maximum number of tokens in the sentence\n        self.max_len = 256\n\n        # batch size\n        self.batch_size = 8\n        #self.valid_batch_size = 4\n\n        # number of epochs\n        self.num_epochs = 10\n        \n        # number of folds\n        self.num_folds = 5\n\n        # define the tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            'roberta-large'\n        )\n        \n        # define seed\n        self.seed = 23\n        \n        # training hyperparameters\n        self.learning_rate = 5e-5\n        self.weight_decay = 1e-1","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:28:54.060218Z","iopub.execute_input":"2021-05-21T15:28:54.060568Z","iopub.status.idle":"2021-05-21T15:28:54.069306Z","shell.execute_reply.started":"2021-05-21T15:28:54.060535Z","shell.execute_reply":"2021-05-21T15:28:54.068058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/commonlitreadabilityprize/'\ntrain_df = pd.read_csv(data_dir + 'train.csv')\ntest_df = pd.read_csv(data_dir + 'test.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:28:54.072103Z","iopub.execute_input":"2021-05-21T15:28:54.072421Z","iopub.status.idle":"2021-05-21T15:28:54.166783Z","shell.execute_reply.started":"2021-05-21T15:28:54.072393Z","shell.execute_reply":"2021-05-21T15:28:54.166005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:28:54.167901Z","iopub.execute_input":"2021-05-21T15:28:54.16824Z","iopub.status.idle":"2021-05-21T15:28:54.173187Z","shell.execute_reply.started":"2021-05-21T15:28:54.168205Z","shell.execute_reply":"2021-05-21T15:28:54.172403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRPDataset(nn.Module):\n    def __init__(self, df, config):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.target = df['target'].to_numpy()\n        self.tokenizer = config.tokenizer\n        self.max_len = config.max_len\n        \n    def __len__(self)->int:\n        return len(self.excerpt)\n    \n    def __getitem__(self, idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        target = torch.tensor(self.target[idx], dtype=torch.float)\n        \n        return encode, target","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:28:54.174461Z","iopub.execute_input":"2021-05-21T15:28:54.175045Z","iopub.status.idle":"2021-05-21T15:28:54.184632Z","shell.execute_reply.started":"2021-05-21T15:28:54.175005Z","shell.execute_reply":"2021-05-21T15:28:54.18381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\ndef loss_fn(outputs, targets):\n    outputs = outputs.logits.squeeze(-1)\n    return torch.sqrt(nn.MSELoss()(outputs, targets))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:28:54.1859Z","iopub.execute_input":"2021-05-21T15:28:54.18657Z","iopub.status.idle":"2021-05-21T15:28:54.195462Z","shell.execute_reply.started":"2021-05-21T15:28:54.186532Z","shell.execute_reply":"2021-05-21T15:28:54.194567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_loop(dataloader, model, loss_fn, device, optimizer, lr_scheduler = None):\n    model.train()\n    total_loss = 0\n    for i, (inputs, targets) in enumerate(dataloader):\n        # clear previous gradients\n        optimizer.zero_grad()\n\n        # compute model output and loss\n        inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n        targets = targets.to(device)\n        outputs = model(**inputs)\n        loss = loss_fn(outputs,targets)\n\n        # compute gradients of all variables wrt loss\n        loss.backward()\n\n        # performs updates using calculated gradients\n        optimizer.step()\n\n        if lr_scheduler:\n            lr_scheduler.step()\n\n        total_loss += loss.item()\n\n    total_loss /= len(dataloader)\n    return total_loss      ","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:28:54.197948Z","iopub.execute_input":"2021-05-21T15:28:54.198458Z","iopub.status.idle":"2021-05-21T15:28:54.206733Z","shell.execute_reply.started":"2021-05-21T15:28:54.198421Z","shell.execute_reply":"2021-05-21T15:28:54.205904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_loop(dataloader, model, loss_fn, device):\n    model.eval()\n    total_loss = 0\n    valid_predictions = list()\n    with torch.no_grad():\n        for i, (inputs,targets) in enumerate(dataloader):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            targets = targets.to(device)\n\n            outputs = model(**inputs)\n            \n            loss = loss_fn(outputs,targets)\n            total_loss += loss.item()\n            outputs = outputs.logits.squeeze(-1).cpu().detach().numpy().tolist()\n            \n            valid_predictions.extend(outputs)\n        total_loss /= len(dataloader)\n    return total_loss, valid_predictions","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:28:54.208467Z","iopub.execute_input":"2021-05-21T15:28:54.208921Z","iopub.status.idle":"2021-05-21T15:28:54.220848Z","shell.execute_reply.started":"2021-05-21T15:28:54.208882Z","shell.execute_reply":"2021-05-21T15:28:54.220047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for kfold  \nnum_bins = int(np.floor(1 + np.log2(len(train_df))))\ntrain_df.loc[:,'bins'] = pd.cut(train_df['target'],bins=num_bins,labels=False)\nbins = train_df.bins.to_numpy()\n\ntrain_df['is_positive'] = (train_df['target'] >=0)\n\ntrain_df['text_len']= train_df['excerpt'].apply(lambda x: len(x.split()))\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:28:54.222192Z","iopub.execute_input":"2021-05-21T15:28:54.222543Z","iopub.status.idle":"2021-05-21T15:28:54.301455Z","shell.execute_reply.started":"2021-05-21T15:28:54.222509Z","shell.execute_reply":"2021-05-21T15:28:54.300678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(train_df, bins, config, plot_losses=True, verbose=True):\n    fold_train_losses = list()\n    fold_valid_losses = list()\n    fold_valid_predictions = list()\n    fold_valid_targets = list()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n    \n    kfold = StratifiedKFold(n_splits=config.num_folds, shuffle=True, random_state=config.seed)\n    for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_df,y=bins)):\n        x_train, x_valid = train_df.loc[train_idx],train_df.loc[valid_idx]\n        \n        model = AutoModelForSequenceClassification.from_pretrained('roberta-large',num_labels=1)\n        model.to(device)\n        tokenizer = config.tokenizer\n\n        train_ds = CLRPDataset(x_train, config)\n        train_dl = DataLoader(train_ds,\n                              batch_size = config.batch_size,\n                              shuffle=True,\n                              num_workers = 4,\n                              pin_memory=True,\n                              drop_last=False\n                             )\n\n        valid_ds = CLRPDataset(x_valid,config)\n        valid_dl = DataLoader(valid_ds,\n                              batch_size = config.batch_size,\n                              shuffle=False,\n                              num_workers = 4,\n                              pin_memory=True,\n                              drop_last=False,\n                             )\n        \n        optimizer = optim.AdamW(model.parameters(),lr=config.learning_rate, weight_decay=config.weight_decay)\n        #         lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer,max_lr=1e-4,\n        #                                                     steps_per_epoch=len(train_dl), epochs=config['epochs'])\n        \n        lr_scheduler = None\n    \n        print(f\"Fold {k}\")\n        best_loss = 99999\n        \n        train_losses = list()\n        valid_losses = list()\n        best_valid_predictions = list()\n        start = time.time()\n        for i in range(config.num_epochs):\n            train_loss = train_loop(train_dl, model, loss_fn, device, optimizer, lr_scheduler=lr_scheduler)\n            valid_loss, valid_predictions = eval_loop(valid_dl, model, loss_fn, device)\n\n            train_losses.append(train_loss)\n            valid_losses.append(valid_loss)\n            \n            end = time.time()\n            epoch_time = end - start\n            start = end\n            \n            valid_targets = x_valid['target'].to_list()\n                                                  \n            if verbose:\n                print(f\"epoch:{i} Training loss:{train_loss} | Validation loss:{valid_loss} |epoch time {epoch_time:.2f}s \")\n\n            if valid_loss <= best_loss:\n                if verbose:\n                    print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n                    \n                best_loss = valid_loss\n                best_valid_predictions = valid_predictions\n                model.save_pretrained(f'./model{k}')\n                tokenizer.save_pretrained(f'./model{k}')\n                \n        fold_train_losses.append(train_losses)\n        fold_valid_losses.append(valid_losses)\n        fold_valid_predictions.append(best_valid_predictions)\n        fold_valid_targets.append(x_valid['target'].tolist())\n        \n        if k == 0:\n            break\n        \n    if plot_losses == True:\n        plt.figure(figsize=(20,14))\n        for i, (t,v) in enumerate(zip(fold_train_losses,fold_valid_losses)):\n            plt.subplot(2,5,i+1)\n            plt.title(f\"Fold {i}\")\n            plt.plot(t,label=\"train_loss\")\n            plt.plot(v,label=\"valid_loss\")\n            plt.legend()\n        plt.show()\n        \n        plt.figure(figsize=(20,14))\n        for i, (p,t) in enumerate(zip(fold_valid_predictions,fold_valid_targets)):\n            plt.subplot(2,5,i+1)\n            plt.title(f\"Fold {i}\")\n            sns.distplot(p,label=\"predictions\")\n            sns.distplot(t,label=\"targets\")\n            plt.legend()\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:28:54.302795Z","iopub.execute_input":"2021-05-21T15:28:54.303111Z","iopub.status.idle":"2021-05-21T15:28:54.322206Z","shell.execute_reply.started":"2021-05-21T15:28:54.303079Z","shell.execute_reply":"2021-05-21T15:28:54.321078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = Config()\n\ngc.collect()\ntorch.cuda.empty_cache()\nrun(train_df, bins, config)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:28:54.323767Z","iopub.execute_input":"2021-05-21T15:28:54.324155Z","iopub.status.idle":"2021-05-21T16:09:14.655232Z","shell.execute_reply.started":"2021-05-21T15:28:54.324103Z","shell.execute_reply":"2021-05-21T16:09:14.6544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}