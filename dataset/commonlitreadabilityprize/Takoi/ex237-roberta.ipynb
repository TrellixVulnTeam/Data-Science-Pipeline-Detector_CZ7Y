{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ========================================\n# library\n# ========================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer,AutoModel,AutoConfig\nimport transformers\nfrom transformers import RobertaModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import RobertaForSequenceClassification\nfrom transformers import RobertaTokenizer\nimport logging\nimport sys\nfrom contextlib import contextmanager\nimport time\nimport random\nfrom tqdm import tqdm\nimport os\nimport gc","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:59:46.500297Z","iopub.execute_input":"2021-06-30T04:59:46.500661Z","iopub.status.idle":"2021-06-30T04:59:46.510707Z","shell.execute_reply.started":"2021-06-30T04:59:46.500629Z","shell.execute_reply":"2021-06-30T04:59:46.509854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ==================\n# Constant\n# ==================\nex = \"237\"\nTRAIN_PATH = \"../input/commonlitreadabilityprize/train.csvv\"\nLOGGER_PATH = f\"ex{ex}.txt\"\nFOLD_PATH = \"../input/fe001-step-1-create-folds/fe001_train_folds.csv\"\nMODEL_PATH_BASE = f\"ex{ex}\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:43:15.179669Z","iopub.execute_input":"2021-06-30T04:43:15.17999Z","iopub.status.idle":"2021-06-30T04:43:15.226131Z","shell.execute_reply.started":"2021-06-30T04:43:15.179953Z","shell.execute_reply":"2021-06-30T04:43:15.224725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===============\n# Settings\n# ===============\n\nMODEL_PATH = '../input/clrp-pytorch-roberta-pretrain-fold0/clrp_roberta_base'\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n# ===============\n# Settings\n# ===============\nSEED = 0\nN_SPLITS = 5\nSHUFFLE = True\nnum_workers = 4\n\nBATCH_SIZE = 16\nn_epochs = 5\nes_patience = 10\n\nmax_len = 256\nweight_decay = 0.1\nlr = 2e-5\nnum_warmup_steps_rate = 0.1\n\neval_steps = 20","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:45:09.584228Z","iopub.execute_input":"2021-06-30T04:45:09.584583Z","iopub.status.idle":"2021-06-30T04:45:09.764737Z","shell.execute_reply.started":"2021-06-30T04:45:09.584552Z","shell.execute_reply":"2021-06-30T04:45:09.763737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===============\n# Functions\n# ===============\n\nclass CommonLitDataset(Dataset):\n    def __init__(self, excerpt, tokenizer, max_len, target=None):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.target = target\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=True\n        )\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        if self.target is not None:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long),\n                \"target\" : torch.tensor(self.target[item], dtype=torch.float32)\n            }\n        else:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long)\n            }\n    \nclass roberta_model(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(MODEL_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(MODEL_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)\n    \n    \ndef calc_loss(y_true, y_pred):\n    return  np.sqrt(mean_squared_error(y_true, y_pred))\n    \ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\n\ndef setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n    LOGGER.handlers = []\n    LOGGER.setLevel(min(stderr_level, file_level))\n\n    if stderr:\n        handler = logging.StreamHandler(sys.stderr)\n        handler.setFormatter(FORMATTER)\n        handler.setLevel(stderr_level)\n        LOGGER.addHandler(handler)\n\n    if out_file is not None:\n        handler = logging.FileHandler(out_file)\n        handler.setFormatter(FORMATTER)\n        handler.setLevel(file_level)\n        LOGGER.addHandler(handler)\n\n    LOGGER.info(\"logger set up\")\n    return LOGGER\n\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield \n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n    \n    \nLOGGER = logging.getLogger()\nFORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nsetup_logger(out_file=LOGGER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:43:15.459841Z","iopub.execute_input":"2021-06-30T04:43:15.460191Z","iopub.status.idle":"2021-06-30T04:43:15.491081Z","shell.execute_reply.started":"2021-06-30T04:43:15.460154Z","shell.execute_reply":"2021-06-30T04:43:15.49004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# Main\n# ================================\ntrain = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ny = train[\"target\"]\nfold_df = pd.read_csv(FOLD_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:43:15.492427Z","iopub.execute_input":"2021-06-30T04:43:15.492783Z","iopub.status.idle":"2021-06-30T04:43:15.629674Z","shell.execute_reply.started":"2021-06-30T04:43:15.492748Z","shell.execute_reply":"2021-06-30T04:43:15.628818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold_array = fold_df[\"kfold\"].values","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:43:15.631543Z","iopub.execute_input":"2021-06-30T04:43:15.631786Z","iopub.status.idle":"2021-06-30T04:43:15.639012Z","shell.execute_reply.started":"2021-06-30T04:43:15.631762Z","shell.execute_reply":"2021-06-30T04:43:15.638243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# train\n# ================================\nwith timer(\"roberta\"):\n    set_seed(SEED)\n    oof = np.zeros([len(train)])\n    kf = KFold(n_splits=N_SPLITS, shuffle=SHUFFLE, random_state=SEED)\n    for fold in range(5):\n        x_train, y_train = train.iloc[fold_array != fold], y.iloc[fold_array != fold]\n        x_val, y_val =train.iloc[fold_array == fold], y.iloc[fold_array == fold]\n        \n        # dataset\n        train_ = CommonLitDataset(x_train[\"excerpt\"].values, tokenizer, max_len, y_train.values.reshape(-1,1))\n        val_ = CommonLitDataset(x_val[\"excerpt\"].values, tokenizer, max_len, y_val.values.reshape(-1,1))\n        \n        # loader\n        train_loader = DataLoader(dataset=train_, batch_size=BATCH_SIZE, shuffle = True , num_workers=4)\n        val_loader = DataLoader(dataset=val_, batch_size=BATCH_SIZE, shuffle = False , num_workers=4)\n        \n        # model\n        model = roberta_model()\n        model = model.to(device)\n        \n        # optimizer, scheduler\n        param_optimizer = list(model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters,\n                          lr=lr,\n                          betas=(0.9, 0.98),\n                          weight_decay=weight_decay,\n                          )\n        num_train_optimization_steps = int(len(train_loader) * n_epochs)\n        num_warmup_steps = int(num_train_optimization_steps * num_warmup_steps_rate)\n        scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                    num_warmup_steps=num_warmup_steps,\n                                                    num_training_steps=num_train_optimization_steps)\n        \n        criterion = nn.MSELoss()\n        best_val = None\n        patience = es_patience\n        for epoch in tqdm(range(n_epochs)):\n            with timer(f\"model_fold:{epoch}\"):\n                \n                # train\n                model.train() \n                train_losses_batch = []\n                \n                epoch_loss = 0\n                \n                for i, d in enumerate(train_loader):\n                    \n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n                    target = d[\"target\"]\n                    \n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    target = target.to(device)\n                    optimizer.zero_grad()\n                    output = model(input_ids, mask)\n                    loss = criterion(output, target)\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                    optimizer.step()\n                    scheduler.step()\n                    train_losses_batch.append(loss.item())\n                    \n                    if i % eval_steps == 0:\n                            # val\n                        val_losses_batch = []\n                        model.eval()  # switch model to the evaluation mode\n                        val_preds = np.ndarray((0,1))\n                        with torch.no_grad():  \n                            # Predicting on validation set\n                            for d in val_loader:\n                                # =========================\n                                # data loader\n                                # =========================\n                                input_ids = d['input_ids']\n                                mask = d['attention_mask']\n                                token_type_ids = d[\"token_type_ids\"]\n                                target = d[\"target\"]\n\n                                input_ids = input_ids.to(device)\n                                mask = mask.to(device)\n                                token_type_ids = token_type_ids.to(device)\n                                target = target.to(device)\n                                output  = model(input_ids, mask)\n\n                                loss = criterion(output, target)\n                                val_preds = np.concatenate([val_preds, output.detach().cpu().numpy()], axis=0)\n                                val_losses_batch.append(loss.item())\n\n\n\n                        val_loss = np.mean(val_losses_batch)\n                        val_rmse = calc_loss(y_val, val_preds)\n                        LOGGER.info(f'{fold},{epoch}:{i},val_loss:{val_loss},val_rmse:{val_rmse}')\n                        # ===================\n                        # early stop\n                        # ===================\n\n                        if not best_val:\n                            best_val = val_loss  # So any validation roc_auc we have is the best one for now\n                            best_rmse = val_rmse\n                            oof[fold_array == fold] = val_preds.reshape(-1)\n                            torch.save(model.state_dict(), MODEL_PATH_BASE + f\"_{fold}.pth\")  # Saving the model\n                            continue\n\n                        if val_loss <= best_val:\n                            best_val = val_loss\n                            best_rmse = val_rmse\n                            oof[fold_array == fold] = val_preds.reshape(-1)\n                            patience = es_patience  # Resetting patience since we have new best validation accuracy\n                            torch.save(model.state_dict(), MODEL_PATH_BASE + f\"_{fold}.pth\") # Saving current best model\n                        #else:\n                        #    patience -= 1\n                        #    if patience == 0:\n                        #        LOGGER.info(f'Early stopping. Best Val : {best_val} Best Rmse : {best_rmse}')\n                        #        break\n                        model.train() \n\n                train_loss = np.mean(train_losses_batch)\n        del model\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T04:59:54.633705Z","iopub.execute_input":"2021-06-30T04:59:54.63408Z","iopub.status.idle":"2021-06-30T05:44:24.035936Z","shell.execute_reply.started":"2021-06-30T04:59:54.634043Z","shell.execute_reply":"2021-06-30T05:44:24.034745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_rmse = calc_loss(y, oof)\nprint(val_rmse)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:44:52.042061Z","iopub.execute_input":"2021-06-30T05:44:52.042392Z","iopub.status.idle":"2021-06-30T05:44:52.04821Z","shell.execute_reply.started":"2021-06-30T05:44:52.042363Z","shell.execute_reply":"2021-06-30T05:44:52.04727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save(f\"ex{ex}_oof.npy\",oof)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T05:44:53.377649Z","iopub.execute_input":"2021-06-30T05:44:53.378003Z","iopub.status.idle":"2021-06-30T05:44:53.383408Z","shell.execute_reply.started":"2021-06-30T05:44:53.377973Z","shell.execute_reply":"2021-06-30T05:44:53.382229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}