{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ========================================\n# library\n# ========================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\nfrom sklearn.metrics import mean_squared_error\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\nimport transformers\nfrom transformers import RobertaModel,RobertaTokenizer\nfrom transformers import AlbertModel,AlbertTokenizer\nfrom transformers import XLNetModel,XLNetTokenizer,XLNetConfig\nfrom transformers import DebertaModel, DebertaTokenizer\nfrom transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\nfrom transformers import BartModel,BertModel,BertTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import MPNetModel,MPNetTokenizer\nfrom transformers import FunnelBaseModel,FunnelTokenizer,FunnelModel\nfrom transformers import GPT2Model, GPT2Tokenizer, GPT2Config\nfrom transformers import T5EncoderModel,T5Tokenizer\nimport logging\nimport sys\nfrom contextlib import contextmanager\nimport time\nimport random\nfrom tqdm import tqdm\nimport os\nimport pickle\nimport gc","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:39:32.333566Z","iopub.execute_input":"2021-08-03T12:39:32.333979Z","iopub.status.idle":"2021-08-03T12:39:37.004434Z","shell.execute_reply.started":"2021-08-03T12:39:32.333943Z","shell.execute_reply":"2021-08-03T12:39:37.003614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ==================\n# Constant\n# ==================\nex = \"final_lb1\"\nTRAIN_PATH = \"../input/commonlitreadabilityprize/train.csv\"\nLOGGER_PATH = f\"ex{ex}.txt\"\nFOLD_PATH = \"../input/fe001-step-1-create-folds/fe001_train_folds.csv\"\nMODEL_PATH_BASE = f\"ex{ex}\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:39:37.159104Z","iopub.execute_input":"2021-08-03T12:39:37.159389Z","iopub.status.idle":"2021-08-03T12:39:37.202532Z","shell.execute_reply.started":"2021-08-03T12:39:37.159363Z","shell.execute_reply":"2021-08-03T12:39:37.201686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===============\n# Settings\n# ===============\nBATCH_SIZE = 8\nmax_len = 256\n\nroberta_large_MODEL_PATH = '../input/roberta-transformers-pytorch/roberta-large'\nroberta_large_tokenizer = RobertaTokenizer.from_pretrained(roberta_large_MODEL_PATH)\n\nroberta_base_MODEL_PATH = '../input/roberta-transformers-pytorch/roberta-base'\nroberta_base_tokenizer = RobertaTokenizer.from_pretrained(roberta_base_MODEL_PATH)\n\nroberta_base_MODEL_PATH2 = '../input/clrp-pytorch-roberta-pretrain-fold0/clrp_roberta_base'\nroberta_base_tokenizer2 = AutoTokenizer.from_pretrained(roberta_base_MODEL_PATH2)\n\ndeberta_large_MODEL_PATH = \"../input/deberta/large\"\ndeberta_large_tokenizer = DebertaTokenizer.from_pretrained(deberta_large_MODEL_PATH)\n\nelectra_large_MODEL_PATH = \"../input/electra/large-discriminator\"\nelectra_large_tokenizer = ElectraTokenizer.from_pretrained(electra_large_MODEL_PATH)\n\nbart_large_MODEL_PATH = '../input/bart-models-hugging-face-model-repository/bart-large'\nbart_large_tokenizer = RobertaTokenizer.from_pretrained(roberta_large_MODEL_PATH)\n\ndeberta_xlarge_MODEL_PATH = \"../input/deberta/v2-xlarge\"\ndeberta_xlarge_tokenizer = AutoTokenizer.from_pretrained(deberta_xlarge_MODEL_PATH)\n\nmpnet_base_MODEL_PATH = \"../input/mpnet-base\"\nmpnet_base_tokenizer = MPNetTokenizer.from_pretrained(mpnet_base_MODEL_PATH)\n\ndeberta_v2_xxlarge_MODEL_PATH = \"../input/deberta/v2-xxlarge\"\ndeberta_v2_xxlarge_tokenizer = AutoTokenizer.from_pretrained(deberta_v2_xxlarge_MODEL_PATH)\n\nfunnel_large_base_MODEL_PATH = '../input/funnel-large-base-save/funnel-large/'\nfunnel_large_base_tokenizer = FunnelTokenizer.from_pretrained(funnel_large_base_MODEL_PATH )\n\nmuppet_roberta_large_MODEL_PATH = \"../input/muppet-roberta-large/muppet-roberta-large/\"\nmuppet_roberta_large_tokenizer = RobertaTokenizer.from_pretrained(muppet_roberta_large_MODEL_PATH)\n\nfunnel_large_MODEL_PATH = '../input/funnel-large-save/funnel-large/'\nfunnel_large_tokenizer = FunnelTokenizer.from_pretrained(funnel_large_MODEL_PATH )\n\ngpt2_medium_MODEL_PATH = \"../input/gpt-medium-save/gpt-medium/\"\ngpt2_medium_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_medium_MODEL_PATH)\n\nalbert_v2_xxlarge_MODEL_PATH = \"../input/albert-xxlarge-v2-save/albert-xxlarge-v2/\"\nalbert_v2_xxlarge_tokenizer = AlbertTokenizer.from_pretrained(albert_v2_xxlarge_MODEL_PATH)\n\nelectra_base_MODEL_PATH = \"../input/electra/base-discriminator\"\nelectra_base_tokenizer = ElectraTokenizer.from_pretrained(electra_base_MODEL_PATH)\n\nbert_base_uncased_MODEL_PATH = '../input/bert-uncased-base-save/bert-base-uncased/'\nbert_base_uncased_tokenizer = BertTokenizer.from_pretrained(bert_base_uncased_MODEL_PATH)\n\nt5_large_MODEL_PATH = '../input/t5-large-save/t5-large/'\nt5_large_tokenizer = T5Tokenizer.from_pretrained(t5_large_MODEL_PATH)\n\ndistil_bart_MODEL_PATH = '../input/distilbart-save/distil-bart/'\ndistil_bart_tokenizer = RobertaTokenizer.from_pretrained(distil_bart_MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:39:37.319028Z","iopub.execute_input":"2021-08-03T12:39:37.319301Z","iopub.status.idle":"2021-08-03T12:39:40.515004Z","shell.execute_reply.started":"2021-08-03T12:39:37.319274Z","shell.execute_reply":"2021-08-03T12:39:40.514146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===============\n# Functions\n# ===============\n\nclass CommonLitDataset(Dataset):\n    def __init__(self, excerpt, tokenizer, max_len, target=None):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.target = target\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=True\n        )\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        if self.target is not None:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long),\n                \"target\" : torch.tensor(self.target[item], dtype=torch.float32)\n            }\n        else:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long)\n            }\n        \n\nclass roberta_large_model(nn.Module):\n    def __init__(self):\n        super(roberta_large_model, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\n            roberta_large_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.roberta(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n\n    \nclass roberta_base_model(nn.Module):\n    def __init__(self):\n        super(roberta_base_model, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\n            roberta_base_MODEL_PATH, \n        )\n        self.drop = nn.Dropout(0.2)\n        self.fc = nn.Linear(768, 256)\n        self.layernorm = nn.LayerNorm(256)\n        self.drop2 = nn.Dropout(0.2)\n        self.relu = nn.ReLU()\n        self.out = nn.Linear(256, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.roberta(ids, attention_mask=mask,token_type_ids=token_type_ids)['pooler_output']\n        output = self.drop(emb)\n        output = self.fc(output)\n        output = self.layernorm(output)\n        output = self.drop2(output)\n        output = self.relu(output)\n        output = self.out(output)\n        return output,emb\n    \n    \nclass roberta_base_model2(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(roberta_base_MODEL_PATH2)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(roberta_base_MODEL_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        return self.regressor(context_vector)\n    \n    \nclass deberta_large_model(nn.Module):\n    def __init__(self):\n        super(deberta_large_model, self).__init__()\n        self.deberta_model = DebertaModel.from_pretrained(deberta_large_MODEL_PATH, \n                                                        hidden_dropout_prob = 0,\n                                                        attention_probs_dropout_prob = 0,\n                                                        hidden_act = \"gelu_new\")\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.deberta_model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state'][:,0,:]\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass electra_large_model(nn.Module):\n    def __init__(self):\n        super(electra_large_model, self).__init__()\n        self.electra = ElectraForSequenceClassification.from_pretrained(\n            electra_large_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0,\n            summary_last_dropout = 0,\n            num_labels = 1\n        )\n\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        output = self.electra(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"logits\"]\n        return output\n    \n    \nclass bart_large_model(nn.Module):\n    def __init__(self):\n        super(bart_large_model, self).__init__()\n        self.bart = BartModel.from_pretrained(\n            bart_large_MODEL_PATH, \n            dropout=0.0, attention_dropout=0.0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask):\n        # pooler\n        emb = self.bart(ids, attention_mask=mask)['last_hidden_state']\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass deberta_xlarge_model(nn.Module):\n    def __init__(self):\n        super(deberta_xlarge_model, self).__init__()\n        self.deberta_model =  AutoModel.from_pretrained(deberta_xlarge_MODEL_PATH, \n                                                        hidden_dropout_prob = 0,\n                                                        attention_probs_dropout_prob = 0)\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        #self.ln = nn.LayerNorm(1536)\n        self.out = nn.Linear(1536, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.deberta_model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state'][:,0,:]\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \n    \nclass mpnet_base_model(nn.Module):\n    def __init__(self):\n        super(mpnet_base_model, self).__init__()\n        self.mpnet = MPNetModel.from_pretrained(\n            mpnet_base_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(768)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.mpnet(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass deberta_v2_xxlarge_model(nn.Module):\n    def __init__(self):\n        super(deberta_v2_xxlarge_model, self).__init__()\n        self.deberta_model =  AutoModel.from_pretrained(deberta_v2_xxlarge_MODEL_PATH, \n                                                        hidden_dropout_prob = 0,\n                                                        attention_probs_dropout_prob = 0)\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        #self.ln = nn.LayerNorm(1536)\n        self.out = nn.Linear(1536, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.deberta_model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state'][:,0,:]\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \n    \nclass funnel_large_base_model(nn.Module):\n    def __init__(self):\n        super(funnel_large_base_model, self).__init__()\n        self.funnel = FunnelBaseModel.from_pretrained(\n            funnel_large_base_MODEL_PATH, \n            hidden_dropout = 0,\n            attention_dropout = 0,\n            hidden_act = \"gelu\"\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.funnel(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \n    \nclass muppet_roberta_large_model(nn.Module):\n    def __init__(self):\n        super(muppet_roberta_large_model, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\n            muppet_roberta_large_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.roberta(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass funnel_large_model(nn.Module):\n    def __init__(self):\n        super(funnel_large_model, self).__init__()\n        self.funnel = FunnelModel.from_pretrained(\n            funnel_large_MODEL_PATH, \n            hidden_dropout = 0,\n            attention_dropout = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        #self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.funnel(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \n    \nclass gpt2_medium_model(nn.Module):\n    def __init__(self):\n        super(gpt2_medium_model, self).__init__()\n        self.gpt2_model = GPT2Model.from_pretrained(gpt2_medium_MODEL_PATH,\n                                                   attn_pdrop = 0,\n                                                   embd_pdrop = 0,\n                                                   resid_pdrop = 0,\n                                                   summary_first_dropout = 0)\n        self.gpt2_model.resize_token_embeddings(len(gpt2_medium_tokenizer))\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask):\n        # pooler\n        emb = self.gpt2_model(ids, attention_mask=mask)[\"last_hidden_state\"]\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass albert_v2_xxlarge_model(nn.Module):\n    def __init__(self):\n        super(albert_v2_xxlarge_model, self).__init__()\n        self.albert = AlbertModel.from_pretrained(\n            albert_v2_xxlarge_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(4096)\n        self.out = nn.Linear(4096, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.albert(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass electra_base_model(nn.Module):\n    def __init__(self):\n        super(electra_base_model, self).__init__()\n        self.electra = ElectraModel.from_pretrained(\n            electra_base_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(768)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.electra(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass bert_base_uncased_model(nn.Module):\n    def __init__(self):\n        super(bert_base_uncased_model, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(bert_base_uncased_MODEL_PATH,\n                                                          hidden_dropout_prob = 0,\n                                                          attention_probs_dropout_prob = 0)\n        #self.bert = transformers.BertForSequenceClassification.from_pretrained(BERT_MODEL,num_labels=1)\n        self.ln = nn.LayerNorm(768)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb , _= self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        output = self.out(output)\n        return output\n    \n    \nclass t5_large_model(nn.Module):\n    def __init__(self):\n        super(t5_large_model, self).__init__()\n        self.t5 = T5EncoderModel.from_pretrained(t5_large_MODEL_PATH,\n                            dropout_rate = 0)\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask):\n        # pooler\n        emb = self.t5(ids, attention_mask=mask)['last_hidden_state']\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass distil_bart_model(nn.Module):\n    def __init__(self):\n        super(distil_bart_model, self).__init__()\n        self.bart = BartModel.from_pretrained(\n            distil_bart_MODEL_PATH, \n            activation_dropout=0.0, attention_dropout=0.0,\n            classif_dropout = 0, classifier_dropout = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask):\n        # pooler\n        emb = self.bart(ids, attention_mask=mask)['last_hidden_state']\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass CommonLitDataset_gpt(Dataset):\n    def __init__(self, excerpt, tokenizer, max_len, target=None):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.target = target\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer('<|startoftext|>'+ text + '<|endoftext|>', truncation=True, max_length=self.max_len, padding=\"max_length\")\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        #token_type_ids = inputs[\"token_type_ids\"]\n        if self.target is not None:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                #\"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long),\n                \"target\" : torch.tensor(self.target[item], dtype=torch.float32)\n            }\n        else:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                #\"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long)\n            }\n\n    \ndef calc_loss(y_true, y_pred):\n    return  np.sqrt(mean_squared_error(y_true, y_pred))\n\n    \ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n    LOGGER.handlers = []\n    LOGGER.setLevel(min(stderr_level, file_level))\n\n    if stderr:\n        handler = logging.StreamHandler(sys.stderr)\n        handler.setFormatter(FORMATTER)\n        handler.setLevel(stderr_level)\n        LOGGER.addHandler(handler)\n\n    if out_file is not None:\n        handler = logging.FileHandler(out_file)\n        handler.setFormatter(FORMATTER)\n        handler.setLevel(file_level)\n        LOGGER.addHandler(handler)\n\n    LOGGER.info(\"logger set up\")\n    return LOGGER\n\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield \n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n    \n    \nLOGGER = logging.getLogger()\nFORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nsetup_logger(out_file=LOGGER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:39:40.518694Z","iopub.execute_input":"2021-08-03T12:39:40.518952Z","iopub.status.idle":"2021-08-03T12:39:40.62511Z","shell.execute_reply.started":"2021-08-03T12:39:40.518926Z","shell.execute_reply":"2021-08-03T12:39:40.62434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# Main\n# ================================\ntest = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:39:40.626789Z","iopub.execute_input":"2021-08-03T12:39:40.62714Z","iopub.status.idle":"2021-08-03T12:39:40.640936Z","shell.execute_reply.started":"2021-08-03T12:39:40.627104Z","shell.execute_reply":"2021-08-03T12:39:40.640082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# roberta base -> svr + ridge\n# ================================\nif len(test) > 0:\n    with timer(\"roberta base -> svr + ridge\"):\n        y_test_roberta_base = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, roberta_base_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n        for fold in range(5):\n\n            # model\n            model = roberta_base_model()\n            model.load_state_dict(torch.load(f\"../input/ex014-roberta/ex014_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_emb = np.ndarray((0,768))\n\n            # svr\n            svr = pickle.load(open(f\"../input/ex015-lgb/ex015_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            # ridge\n            ridge = pickle.load(open(f\"../input/ex015-lgb/ex015_ridge_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    _,output = model(input_ids, mask,token_type_ids )\n\n                    test_emb = np.concatenate([test_emb, output.detach().cpu().numpy()], axis=0)\n            x_test = pd.DataFrame(test_emb )\n            x_test.columns = [f\"emb_{i}\" for i in range(len(x_test.columns))]\n            test_preds_svr = svr.predict(x_test)\n            test_preds_ridge = ridge.predict(x_test)\n            test_preds = (test_preds_svr + test_preds_ridge)/2\n            y_test_roberta_base.append(test_preds)\n            del x_test,model,test_emb\n            gc.collect()\n\n        y_test_roberta_base = np.mean(y_test_roberta_base,axis=0)\n        del test_,test_loader\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:39:40.642245Z","iopub.execute_input":"2021-08-03T12:39:40.642546Z","iopub.status.idle":"2021-08-03T12:41:02.263005Z","shell.execute_reply.started":"2021-08-03T12:39:40.64252Z","shell.execute_reply":"2021-08-03T12:41:02.262158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del roberta_base_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:41:02.264342Z","iopub.execute_input":"2021-08-03T12:41:02.264716Z","iopub.status.idle":"2021-08-03T12:41:02.459702Z","shell.execute_reply.started":"2021-08-03T12:41:02.264661Z","shell.execute_reply":"2021-08-03T12:41:02.458698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# roberta base\n# ================================\nif len(test) > 0:\n    with timer(\"roberta base\"):\n        y_test_roberta_base2 = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, roberta_base_tokenizer2, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n        for fold in range(5):\n\n            # model\n            model = roberta_base_model2()\n            model.load_state_dict(torch.load(f\"../input/ex237-roberta/ex237_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask)\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)     \n            y_test_roberta_base2.append(test_preds)\n            del model\n            gc.collect()\n\n        y_test_roberta_base2 = np.mean(y_test_roberta_base2,axis=0)\n        del test_, test_loader\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:41:02.461389Z","iopub.execute_input":"2021-08-03T12:41:02.461821Z","iopub.status.idle":"2021-08-03T12:42:05.539009Z","shell.execute_reply.started":"2021-08-03T12:41:02.46177Z","shell.execute_reply":"2021-08-03T12:42:05.538089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del roberta_base_tokenizer2\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:42:05.541081Z","iopub.execute_input":"2021-08-03T12:42:05.541494Z","iopub.status.idle":"2021-08-03T12:42:05.746865Z","shell.execute_reply.started":"2021-08-03T12:42:05.541441Z","shell.execute_reply":"2021-08-03T12:42:05.745894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# roberta_large\n# ================================\nif len(test) > 0:\n    with timer(\"roberta_large\"):\n        y_test_roberta_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, roberta_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = roberta_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex072-2/ex072_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_roberta_large.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_roberta_large = np.mean(y_test_roberta_large,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:42:05.748845Z","iopub.execute_input":"2021-08-03T12:42:05.749434Z","iopub.status.idle":"2021-08-03T12:44:52.045916Z","shell.execute_reply.started":"2021-08-03T12:42:05.749396Z","shell.execute_reply":"2021-08-03T12:44:52.044897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del roberta_large_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:44:52.047271Z","iopub.execute_input":"2021-08-03T12:44:52.047615Z","iopub.status.idle":"2021-08-03T12:44:52.246464Z","shell.execute_reply.started":"2021-08-03T12:44:52.04758Z","shell.execute_reply":"2021-08-03T12:44:52.245463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# deberta_large\n# ================================\nif len(test) > 0:\n    with timer(\"deberta_large\"):\n        y_test_deberta_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, deberta_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = deberta_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex182/ex182_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_deberta_large .append(test_preds)\n            del model\n            gc.collect()\n        del test_,test_loader\n        gc.collect()\n        y_test_deberta_large  = np.mean(y_test_deberta_large ,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:44:52.24794Z","iopub.execute_input":"2021-08-03T12:44:52.248307Z","iopub.status.idle":"2021-08-03T12:48:03.390418Z","shell.execute_reply.started":"2021-08-03T12:44:52.248269Z","shell.execute_reply":"2021-08-03T12:48:03.389636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del deberta_large_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:48:03.391838Z","iopub.execute_input":"2021-08-03T12:48:03.392203Z","iopub.status.idle":"2021-08-03T12:48:03.584759Z","shell.execute_reply.started":"2021-08-03T12:48:03.392166Z","shell.execute_reply":"2021-08-03T12:48:03.583961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# electra_large\n# ================================\nif len(test) > 0:\n    with timer(\"electra_largee\"):\n        y_test_electra_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, electra_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n        for fold in tqdm(range(5)):\n\n\n            # model\n            model = electra_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex190/ex190_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_electra_large.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_electra_large = np.mean(y_test_electra_large,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:48:03.586108Z","iopub.execute_input":"2021-08-03T12:48:03.586432Z","iopub.status.idle":"2021-08-03T12:51:14.291034Z","shell.execute_reply.started":"2021-08-03T12:48:03.586395Z","shell.execute_reply":"2021-08-03T12:51:14.290074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del electra_large_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:51:14.292511Z","iopub.execute_input":"2021-08-03T12:51:14.292969Z","iopub.status.idle":"2021-08-03T12:51:14.482968Z","shell.execute_reply.started":"2021-08-03T12:51:14.292926Z","shell.execute_reply":"2021-08-03T12:51:14.481786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# bart_large\n# ================================\nif len(test) > 0:\n    with timer(\"bart_largee\"):\n        y_test_bart_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, bart_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n        for fold in tqdm(range(5)):\n\n\n            # model\n            model = bart_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex107/ex107_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask)\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_bart_large.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_bart_large = np.mean(y_test_bart_large,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:51:14.486916Z","iopub.execute_input":"2021-08-03T12:51:14.487526Z","iopub.status.idle":"2021-08-03T12:55:07.557486Z","shell.execute_reply.started":"2021-08-03T12:51:14.48748Z","shell.execute_reply":"2021-08-03T12:55:07.556578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del bart_large_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:55:07.562753Z","iopub.execute_input":"2021-08-03T12:55:07.563188Z","iopub.status.idle":"2021-08-03T12:55:07.814813Z","shell.execute_reply.started":"2021-08-03T12:55:07.56315Z","shell.execute_reply":"2021-08-03T12:55:07.81373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# deberta_xlarge\n# ================================\nif len(test) > 0:\n    with timer(\"deberta_xlarge\"):\n        y_test_deberta_xlarge = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, deberta_xlarge_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=4, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = deberta_xlarge_model()\n            model.load_state_dict(torch.load(f\"../input/deberta-v2-xlarge-194-save/ex194_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_deberta_xlarge .append(test_preds)\n            del model\n            gc.collect()\n        del test_,test_loader\n        gc.collect()\n        y_test_deberta_xlarge  = np.mean(y_test_deberta_xlarge ,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T12:55:07.816618Z","iopub.execute_input":"2021-08-03T12:55:07.817334Z","iopub.status.idle":"2021-08-03T13:02:05.210538Z","shell.execute_reply.started":"2021-08-03T12:55:07.81729Z","shell.execute_reply":"2021-08-03T13:02:05.209556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del deberta_xlarge_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:02:05.217512Z","iopub.execute_input":"2021-08-03T13:02:05.219369Z","iopub.status.idle":"2021-08-03T13:02:05.549172Z","shell.execute_reply.started":"2021-08-03T13:02:05.219199Z","shell.execute_reply":"2021-08-03T13:02:05.548271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# mpnet_base\n# ================================\nif len(test) > 0:\n    with timer(\"mpnet_base\"):\n        y_test_mpnet_base = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, mpnet_base_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = mpnet_base_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex292/ex292_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_mpnet_base.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_mpnet_base = np.mean(y_test_mpnet_base,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:02:05.553631Z","iopub.execute_input":"2021-08-03T13:02:05.555907Z","iopub.status.idle":"2021-08-03T13:03:02.19468Z","shell.execute_reply.started":"2021-08-03T13:02:05.555862Z","shell.execute_reply":"2021-08-03T13:03:02.193822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del mpnet_base_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:03:02.196052Z","iopub.execute_input":"2021-08-03T13:03:02.196403Z","iopub.status.idle":"2021-08-03T13:03:02.386984Z","shell.execute_reply.started":"2021-08-03T13:03:02.196367Z","shell.execute_reply":"2021-08-03T13:03:02.385927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# deberta_v2_xxlarge\n# ================================\nif len(test) > 0:\n    with timer(\"deberta_v2_xlarge\"):\n        y_test_deberta_v2_xxlarge = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, deberta_v2_xxlarge_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=4, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = deberta_v2_xxlarge_model()\n            if fold < 3:\n                model.load_state_dict(torch.load(f\"../input/ex216-inference-save-1/ex216_{fold}.pth\"))\n            else:\n                model.load_state_dict(torch.load(f\"../input/ex216-inference-save-2/ex216_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_deberta_v2_xxlarge.append(test_preds)\n            del model\n            gc.collect()\n        del test_,test_loader\n        gc.collect()\n        y_test_deberta_v2_xxlarge  = np.mean(y_test_deberta_v2_xxlarge ,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:03:02.388577Z","iopub.execute_input":"2021-08-03T13:03:02.389002Z","iopub.status.idle":"2021-08-03T13:13:35.063678Z","shell.execute_reply.started":"2021-08-03T13:03:02.38896Z","shell.execute_reply":"2021-08-03T13:13:35.061804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del deberta_v2_xxlarge_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:13:35.069174Z","iopub.execute_input":"2021-08-03T13:13:35.069472Z","iopub.status.idle":"2021-08-03T13:13:35.466749Z","shell.execute_reply.started":"2021-08-03T13:13:35.069445Z","shell.execute_reply":"2021-08-03T13:13:35.465702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# funnel_large_base\n# ================================\nif len(test) > 0:\n    with timer(\"funnel_large_base\"):\n        y_test_funnel_large_base = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, funnel_large_base_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=4, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = funnel_large_base_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex272/ex272_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_funnel_large_base.append(test_preds)\n            del model\n            gc.collect()\n        del test_,test_loader\n        gc.collect()\n        y_test_funnel_large_base  = np.mean(y_test_funnel_large_base ,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:13:35.468291Z","iopub.execute_input":"2021-08-03T13:13:35.468653Z","iopub.status.idle":"2021-08-03T13:16:23.156189Z","shell.execute_reply.started":"2021-08-03T13:13:35.468607Z","shell.execute_reply":"2021-08-03T13:16:23.154878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del funnel_large_base_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:16:23.16243Z","iopub.execute_input":"2021-08-03T13:16:23.164758Z","iopub.status.idle":"2021-08-03T13:16:23.422558Z","shell.execute_reply.started":"2021-08-03T13:16:23.164707Z","shell.execute_reply":"2021-08-03T13:16:23.421721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# muppet_roberta_large\n# ================================\nif len(test) > 0:\n    with timer(\"muppet_roberta_large\"):\n        y_test_muppet_roberta_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, muppet_roberta_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = muppet_roberta_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex384/ex384_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_muppet_roberta_large.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_muppet_roberta_large = np.mean(y_test_muppet_roberta_large,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:16:23.427718Z","iopub.execute_input":"2021-08-03T13:16:23.430023Z","iopub.status.idle":"2021-08-03T13:19:09.666834Z","shell.execute_reply.started":"2021-08-03T13:16:23.42998Z","shell.execute_reply":"2021-08-03T13:19:09.665995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del muppet_roberta_large_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:09.668287Z","iopub.execute_input":"2021-08-03T13:19:09.668648Z","iopub.status.idle":"2021-08-03T13:19:09.859698Z","shell.execute_reply.started":"2021-08-03T13:19:09.668611Z","shell.execute_reply":"2021-08-03T13:19:09.858725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# funnel large\n# ================================\nif len(test) > 0:\n    with timer(\"funnel_model\"):\n        y_test_funnel_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, funnel_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = funnel_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex407/ex407_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_funnel_large.append(test_preds)\n            del model\n            gc.collect()\n        del test_,test_loader\n        gc.collect()\n        y_test_funnel_large  = np.mean(y_test_funnel_large ,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:09.861172Z","iopub.execute_input":"2021-08-03T13:19:09.861587Z","iopub.status.idle":"2021-08-03T13:19:10.301531Z","shell.execute_reply.started":"2021-08-03T13:19:09.861548Z","shell.execute_reply":"2021-08-03T13:19:10.298461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del funnel_large_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.302299Z","iopub.status.idle":"2021-08-03T13:19:10.3027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# gpt_medium\n# ================================\nif len(test) > 0:\n    with timer(\"gpt_medium\"):\n        y_test_gpt2_medium = []\n        # dataset\n        test_ = CommonLitDataset_gpt(test[\"excerpt\"].values, gpt2_medium_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = gpt2_medium_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex429/ex429_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    #token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    #token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_gpt2_medium.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_gpt2_medium= np.mean(y_test_gpt2_medium,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.303788Z","iopub.status.idle":"2021-08-03T13:19:10.304443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del gpt2_medium_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.305662Z","iopub.status.idle":"2021-08-03T13:19:10.306306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# albert_v2_xxlarge_model\n# ================================\nif len(test) > 0:\n    with timer(\"albert_v2_xxlarge_model\"):\n        y_test_albert_v2_xxlarge = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, albert_v2_xxlarge_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = albert_v2_xxlarge_model()\n            if fold == 2:\n                model.load_state_dict(torch.load(f\"../input/commonlit-ex448/ex448_{fold}.pth\"))\n            else:\n                model.load_state_dict(torch.load(f\"../input/commonlit-ex450/ex450_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_albert_v2_xxlarge.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_albert_v2_xxlarge = np.mean(y_test_albert_v2_xxlarge,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.30746Z","iopub.status.idle":"2021-08-03T13:19:10.308197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del albert_v2_xxlarge_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.309362Z","iopub.status.idle":"2021-08-03T13:19:10.310011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# ex465 electra_base_model\n# ================================\nif len(test) > 0:\n    with timer(\"electra_base_model\"):\n        ex465_pred = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, electra_base_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = electra_base_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex465/ex465_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            ex465_pred.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        ex465_pred = np.mean(ex465_pred,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.311159Z","iopub.status.idle":"2021-08-03T13:19:10.311926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del electra_base_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.313094Z","iopub.status.idle":"2021-08-03T13:19:10.313865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# ex497 bert_base_uncased_model\n# ================================\nif len(test) > 0:\n    with timer(\"bert_base_uncased_model\"):\n        ex497_pred = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, bert_base_uncased_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = bert_base_uncased_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex497/ex497_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            ex497_pred.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        ex497_pred = np.mean(ex497_pred,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.314964Z","iopub.status.idle":"2021-08-03T13:19:10.3156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# ex494 t5_large_model\n# ================================\nif len(test) > 0:\n    with timer(\"t5_large\"):\n        ex434_pred = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, t5_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = t5_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex434/ex434_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask)\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            ex434_pred.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        ex434_pred = np.mean(ex434_pred,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.316883Z","iopub.status.idle":"2021-08-03T13:19:10.317506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# distil_bart\n# ================================\nif len(test) > 0:\n    with timer(\"distil_bart\"):\n        ex507_pred = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, distil_bart_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model =  distil_bart_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex507/ex507_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask)\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            ex507_pred.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        ex507_pred = np.mean(ex507_pred,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.318757Z","iopub.status.idle":"2021-08-03T13:19:10.31935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_list = [0.08, 0.00, 0.11, 0.09, 0.10, 0.23, 0.05, 0.13, 0.14, 0.11, 0.17, 0.12, -0.17, -0.14, -0.11, 0.09]            ","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.320503Z","iopub.status.idle":"2021-08-03T13:19:10.321145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(test) > 0:\n    y_test = (y_test_roberta_base + y_test_roberta_base2.reshape(-1)) / 2 * weight_list[0] + \\\n             (y_test_roberta_large.reshape(-1) * 0.8 + y_test_muppet_roberta_large.reshape(-1) * 0.2) * weight_list[2] +\\\n             y_test_bart_large.reshape(-1) *  weight_list[3] +\\\n             (y_test_electra_large.reshape(-1) + y_test_funnel_large_base.reshape(-1))  / 2 * weight_list[4]+\\\n             y_test_deberta_large.reshape(-1) *  weight_list[5]+\\\n             y_test_deberta_xlarge.reshape(-1) *  weight_list[6]+\\\n             y_test_mpnet_base.reshape(-1) *  weight_list[7]+\\\n             y_test_deberta_v2_xxlarge.reshape(-1) *  weight_list[8]+\\\n             y_test_funnel_large.reshape(-1) * weight_list[9]+\\\n             y_test_gpt2_medium.reshape(-1) * weight_list[10]+\\\n             y_test_albert_v2_xxlarge.reshape(-1) * weight_list[11]+\\\n             ex465_pred.reshape(-1) * weight_list[12]+\\\n             ex497_pred.reshape(-1) * weight_list[13]+\\\n             ex434_pred.reshape(-1) * weight_list[14]+\\\n             ex507_pred.reshape(-1) * weight_list[15]\nelse:\n    y_test = np.zeros(len(test))\nsubmission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmission.target = y_test\nsubmission.loc[(y_test >= 0.3), \"target\"] = y_test[(y_test >= 0.3)] * 1.07\nsubmission.loc[(y_test < 0.3) & (y_test >= 0) , \"target\"] = y_test[(y_test < 0.3) & (y_test >= 0)] * 1.2\nsubmission.loc[(y_test < 0) & (y_test >= -0.7), \"target\"] = y_test[(y_test < 0) & (y_test >= -0.7)] * 0.97485814\nsubmission.loc[(y_test < -0.7) & (y_test >= -0.9), \"target\"] = y_test[(y_test < -0.7) & (y_test >= -0.9)] * 1.01\nsubmission.loc[(y_test < -0.9) & (y_test >= -2), \"target\"] = y_test[(y_test < -0.9) & (y_test >= -2)] * 1.02150304\nsubmission.loc[(y_test < -2), \"target\"] = y_test[(y_test < -2)] * 1.02764047\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T13:19:10.322366Z","iopub.status.idle":"2021-08-03T13:19:10.322978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}