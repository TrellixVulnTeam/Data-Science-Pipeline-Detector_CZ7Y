{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ========================================\n# library\n# ========================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport transformers\nfrom transformers import RobertaModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import RobertaForSequenceClassification\nfrom transformers import RobertaTokenizer\nimport logging\nimport sys\nfrom contextlib import contextmanager\nimport time\nimport random\nfrom tqdm import tqdm\nimport os\nimport lightgbm as lgb\nimport pickle\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge,Lasso,ElasticNet","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:42:06.338072Z","iopub.execute_input":"2021-05-26T12:42:06.338511Z","iopub.status.idle":"2021-05-26T12:42:06.354341Z","shell.execute_reply.started":"2021-05-26T12:42:06.338473Z","shell.execute_reply":"2021-05-26T12:42:06.353563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ==================\n# Constant\n# ==================\nex = \"015\"\nTRAIN_PATH = \"../input/commonlitreadabilityprize/train.csvv\"\nLOGGER_PATH = f\"ex{ex}.txt\"\nFOLD_PATH = \"../input/fe001-step-1-create-folds/fe001_train_folds.csv\"\nMODEL_PATH_BASE = f\"ex{ex}\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:42:07.339716Z","iopub.execute_input":"2021-05-26T12:42:07.340048Z","iopub.status.idle":"2021-05-26T12:42:07.409094Z","shell.execute_reply.started":"2021-05-26T12:42:07.340018Z","shell.execute_reply":"2021-05-26T12:42:07.408181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===============\n# Settings\n# ===============\nSEED = 0\nN_SPLITS = 5\nSHUFFLE = True\nnum_workers = 4\n\nBATCH_SIZE = 24\nn_epochs = 5\nes_patience = 3\n\nmax_len = 256\nweight_decay = 0.1\nlr = 5e-5\nnum_warmup_steps = 10\n\nMODEL_PATH = '../input/roberta-transformers-pytorch/roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(MODEL_PATH)\n\nLGBM_PARAMS = {'num_leaves': 64,\n               'min_data_in_leaf': 16,\n               'objective': 'regression',\n               'max_depth': -1,\n               'learning_rate': 0.05,\n               \"boosting\": \"gbdt\",\n               \"bagging_freq\": 1,\n               \"bagging_fraction\": 0.8,\n               \"bagging_seed\": SEED,\n               \"verbosity\": -1,\n              'reg_alpha': 0.1,\n              'reg_lambda': 0.3,\n              'colsample_bytree': 0.7,\n              'metrics':\"rmse\",\n              'num_threads':4,\n         }\n\nLGBM_FIT_PARAMS = {\n    'num_boost_round': 3500,\n    'early_stopping_rounds': 50,\n    'verbose_eval': 50,\n}","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:42:08.314008Z","iopub.execute_input":"2021-05-26T12:42:08.314348Z","iopub.status.idle":"2021-05-26T12:42:08.457631Z","shell.execute_reply.started":"2021-05-26T12:42:08.314318Z","shell.execute_reply":"2021-05-26T12:42:08.456531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===============\n# Functions\n# ===============\n\nclass CommonLitDataset(Dataset):\n    def __init__(self, excerpt, tokenizer, max_len, target=None):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.target = target\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=True\n        )\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        if self.target is not None:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long),\n                \"target\" : torch.tensor(self.target[item], dtype=torch.float32)\n            }\n        else:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long)\n            }\n\nclass roberta_model(nn.Module):\n    def __init__(self):\n        super(roberta_model, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\n            MODEL_PATH, \n        )\n        self.drop = nn.Dropout(0.2)\n        self.fc = nn.Linear(768, 256)\n        self.layernorm = nn.LayerNorm(256)\n        self.drop2 = nn.Dropout(0.2)\n        self.relu = nn.ReLU()\n        self.out = nn.Linear(256, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.roberta(ids, attention_mask=mask,token_type_ids=token_type_ids)['pooler_output']\n        output = self.drop(emb)\n        output = self.fc(output)\n        output = self.layernorm(output)\n        output = self.drop2(output)\n        output = self.relu(output)\n        output = self.out(output)\n        return output,emb\n    \n    \ndef calc_loss(y_true, y_pred):\n    return  np.sqrt(mean_squared_error(y_true, y_pred))\n    \ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\n\ndef setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n    LOGGER.handlers = []\n    LOGGER.setLevel(min(stderr_level, file_level))\n\n    if stderr:\n        handler = logging.StreamHandler(sys.stderr)\n        handler.setFormatter(FORMATTER)\n        handler.setLevel(stderr_level)\n        LOGGER.addHandler(handler)\n\n    if out_file is not None:\n        handler = logging.FileHandler(out_file)\n        handler.setFormatter(FORMATTER)\n        handler.setLevel(file_level)\n        LOGGER.addHandler(handler)\n\n    LOGGER.info(\"logger set up\")\n    return LOGGER\n\ndef train_lgbm(X_train, y_train, X_valid, y_valid, X_test, categorical_features, feature_name, fold_id,lgb_params, fit_params, loss_func, calc_importances=True):\n    \n    train = lgb.Dataset(X_train, y_train,\n                        categorical_feature=categorical_features,\n                        feature_name=feature_name)\n    if X_valid is not None:\n        valid = lgb.Dataset(X_valid, y_valid,\n                            categorical_feature=categorical_features,\n                            feature_name=feature_name)\n   \n    if X_valid is not None:\n        model = lgb.train(\n            lgb_params,\n            train,\n            valid_sets=[train,valid],\n            **fit_params\n        )\n    else:\n        model = lgb.train(\n            lgb_params,\n            train,\n            **fit_params\n        )\n    \n    # train score\n    if X_valid is not None:\n        y_pred_valid = model.predict(X_valid)\n        valid_loss = loss_func(y_valid, y_pred_valid)\n    else:\n        y_pred_valid = None\n        valid_loss = None\n    \n    #test\n    if X_test is not None:\n        y_pred_test = model.predict(X_test)\n    else:\n        y_pred_test = None\n\n    if calc_importances:\n        importances = pd.DataFrame()\n        importances['feature'] = feature_name\n        importances['gain'] = model.feature_importance(importance_type='gain')\n        importances['split'] = model.feature_importance(importance_type='split')\n        importances['fold'] = fold_id\n    else:\n        importances = None\n\n    return y_pred_valid, y_pred_test, valid_loss, importances, model.best_iteration, model\n\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield \n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n    \n    \nLOGGER = logging.getLogger()\nFORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nsetup_logger(out_file=LOGGER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:42:08.905086Z","iopub.execute_input":"2021-05-26T12:42:08.905546Z","iopub.status.idle":"2021-05-26T12:42:08.943108Z","shell.execute_reply.started":"2021-05-26T12:42:08.905508Z","shell.execute_reply":"2021-05-26T12:42:08.942036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# Main\n# ================================\ntrain = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ny = train[\"target\"]\nfold_df = pd.read_csv(FOLD_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:42:09.122788Z","iopub.execute_input":"2021-05-26T12:42:09.123114Z","iopub.status.idle":"2021-05-26T12:42:09.270717Z","shell.execute_reply.started":"2021-05-26T12:42:09.123084Z","shell.execute_reply":"2021-05-26T12:42:09.269796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold_array = fold_df[\"kfold\"].values","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:42:09.33403Z","iopub.execute_input":"2021-05-26T12:42:09.334361Z","iopub.status.idle":"2021-05-26T12:42:09.339128Z","shell.execute_reply.started":"2021-05-26T12:42:09.334332Z","shell.execute_reply":"2021-05-26T12:42:09.338162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# train\n# ================================\nwith timer(\"lgbm\"):\n    set_seed(SEED)\n    oof_lgb = np.zeros([len(train)])\n    oof_svr = np.zeros([len(train)])\n    oof_ridge = np.zeros([len(train)])\n    oof_lasso = np.zeros([len(train)])\n    oof_elastic = np.zeros([len(train)])\n    kf = KFold(n_splits=N_SPLITS, shuffle=SHUFFLE, random_state=SEED)\n    for fold in range(5):\n        x_train, y_train = train.iloc[fold_array != fold], y.iloc[fold_array != fold]\n        x_val, y_val =train.iloc[fold_array == fold], y.iloc[fold_array == fold]\n        \n        # dataset\n        train_ = CommonLitDataset(x_train[\"excerpt\"].values, tokenizer, max_len, y_train.values.reshape(-1,1))\n        val_ = CommonLitDataset(x_val[\"excerpt\"].values, tokenizer, max_len, y_val.values.reshape(-1,1))\n        \n        # loader\n        train_loader = DataLoader(dataset=train_, batch_size=BATCH_SIZE, shuffle = False , num_workers=4)\n        val_loader = DataLoader(dataset=val_, batch_size=BATCH_SIZE, shuffle = False , num_workers=4)\n        \n        # model\n        model = roberta_model()\n        model.load_state_dict(torch.load(f\"../input/ex014-roberta/ex014_{fold}.pth\"))\n        model.to(device)\n        model.eval()\n        \n        # make embedding\n        train_emb = np.ndarray((0,768))\n        val_emb = np.ndarray((0,768))\n        \n        # train\n        with torch.no_grad():  \n            for d in train_loader:\n                # =========================\n                # data loader\n                # =========================\n                input_ids = d['input_ids']\n                mask = d['attention_mask']\n                token_type_ids = d[\"token_type_ids\"]\n                target = d[\"target\"]\n\n                input_ids = input_ids.to(device)\n                mask = mask.to(device)\n                token_type_ids = token_type_ids.to(device)\n                target = target.to(device)\n                _,emb = model(input_ids, mask,token_type_ids )\n                train_emb = np.concatenate([train_emb, emb.detach().cpu().numpy()], axis=0)\n        \n        # val\n        with torch.no_grad():  \n            for d in val_loader:\n                # =========================\n                # data loader\n                # =========================\n                input_ids = d['input_ids']\n                mask = d['attention_mask']\n                token_type_ids = d[\"token_type_ids\"]\n                target = d[\"target\"]\n\n                input_ids = input_ids.to(device)\n                mask = mask.to(device)\n                token_type_ids = token_type_ids.to(device)\n                target = target.to(device)\n                _,emb = model(input_ids, mask,token_type_ids )\n\n                val_emb = np.concatenate([val_emb, emb.detach().cpu().numpy()], axis=0)\n        \n        # lgbm\n        x_train = pd.DataFrame(train_emb)\n        x_val = pd.DataFrame(val_emb)\n        x_train.columns = [f\"emb_{i}\" for i in range(len(x_train.columns))]\n        x_val.columns =  [f\"emb_{i}\" for i in range(len(x_train.columns))]\n        features = list(x_train.columns)\n        categorical_features = []\n        y_pred_valid, y_pred_test, valid_loss, importances, best_iter, model = train_lgbm(\n                        x_train, y_train, x_val, y_val,None,\n                        categorical_features=categorical_features,\n                        feature_name=features,\n                        fold_id=fold,\n                        lgb_params=LGBM_PARAMS,\n                        fit_params=LGBM_FIT_PARAMS,\n                        loss_func=calc_loss,\n                        calc_importances=True\n                    )\n        oof_lgb[fold_array == fold] =  y_pred_valid\n        save_path = f\"ex015_lgb_roberta_emb_{fold}.pkl\"\n        pickle.dump(model, open(save_path, 'wb'))\n        \n        # svr\n        model_svr = SVR(C=10,kernel=\"rbf\",gamma='auto')\n        model_svr.fit(x_train,y_train)\n        pred = model_svr.predict(x_val)\n        oof_svr[fold_array == fold] =  pred\n        score = calc_loss(y_val, pred)\n        print(f\"fold_svr:{fold}:{score}\")\n        save_path = f\"ex015_svr_roberta_emb_{fold}.pkl\"\n        pickle.dump(model_svr, open(save_path, 'wb'))\n        \n        # ridge\n        ridge = Ridge(alpha=1)\n        ridge.fit(x_train,y_train)\n        pred = ridge.predict(x_val)\n        oof_ridge[fold_array == fold] =  pred\n        score = calc_loss(y_val, pred)\n        print(f\"fold_ridge:{fold}:{score}\")\n        save_path = f\"ex015_ridge_roberta_emb_{fold}.pkl\"\n        pickle.dump(ridge, open(save_path, 'wb'))\n        \n        # lasso\n        lasso = Lasso(alpha=0.0001)\n        lasso.fit(x_train,y_train)\n        pred = lasso.predict(x_val)\n        oof_lasso[fold_array == fold] =  pred\n        score = calc_loss(y_val, pred)\n        print(f\"fold_lasso:{fold}:{score}\")\n        save_path = f\"ex015_lasso_roberta_emb_{fold}.pkl\"\n        pickle.dump(lasso, open(save_path, 'wb'))\n        \n        # elastic\n        elastic = ElasticNet(alpha=0.0001)\n        elastic.fit(x_train,y_train)\n        pred = elastic.predict(x_val)\n        oof_elastic[fold_array == fold] =  pred\n        score = calc_loss(y_val, pred)\n        print(f\"fold_elastic:{fold}:{score}\")\n        save_path = f\"ex015_elastic_roberta_emb_{fold}.pkl\"\n        pickle.dump(elastic, open(save_path, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:42:09.943751Z","iopub.execute_input":"2021-05-26T12:42:09.944161Z","iopub.status.idle":"2021-05-26T12:46:40.604311Z","shell.execute_reply.started":"2021-05-26T12:42:09.944123Z","shell.execute_reply":"2021-05-26T12:46:40.602646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save(\"ex015_lgb.npy\",oof_lgb)\nnp.save(\"ex015_svr.npy\",oof_svr)\nnp.save(\"ex015_lasso.npy\",oof_lasso)\nnp.save(\"ex015_ridge.npy\",oof_ridge)\nnp.save(\"ex015_elastic.npy\",oof_elastic)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T12:46:40.606076Z","iopub.execute_input":"2021-05-26T12:46:40.606469Z","iopub.status.idle":"2021-05-26T12:46:40.617851Z","shell.execute_reply.started":"2021-05-26T12:46:40.606427Z","shell.execute_reply":"2021-05-26T12:46:40.616886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}