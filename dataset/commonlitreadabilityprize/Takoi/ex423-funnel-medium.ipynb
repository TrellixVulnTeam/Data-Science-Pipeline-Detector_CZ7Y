{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ========================================\n# library\n# ========================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\nimport transformers\nfrom transformers import RobertaModel,RobertaTokenizer\nfrom transformers import AlbertModel,AlbertTokenizer\nfrom transformers import XLNetModel,XLNetTokenizer,XLNetConfig\nfrom transformers import DebertaModel, DebertaTokenizer\nfrom transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\nfrom transformers import BartModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import MPNetModel,MPNetTokenizer\nfrom transformers import FunnelModel,FunnelTokenizer, FunnelBaseModel\nimport logging\nimport sys\nfrom contextlib import contextmanager\nimport time\nimport random\nfrom tqdm import tqdm\nimport os\nimport pickle\nimport gc","metadata":{"execution":{"iopub.status.busy":"2021-06-27T02:27:33.514425Z","iopub.execute_input":"2021-06-27T02:27:33.514823Z","iopub.status.idle":"2021-06-27T02:27:41.34431Z","shell.execute_reply.started":"2021-06-27T02:27:33.514738Z","shell.execute_reply":"2021-06-27T02:27:41.34341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ==================\n# Constant\n# ==================\nex = \"423\"\nTRAIN_PATH = \"../input/commonlitreadabilityprize/train.csvv\"\nLOGGER_PATH = f\"ex{ex}.txt\"\nFOLD_PATH = \"../input/fe001-step-1-create-folds/fe001_train_folds.csv\"\nMODEL_PATH_BASE = f\"ex{ex}\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-06-27T02:27:41.345963Z","iopub.execute_input":"2021-06-27T02:27:41.346272Z","iopub.status.idle":"2021-06-27T02:27:41.395739Z","shell.execute_reply.started":"2021-06-27T02:27:41.346237Z","shell.execute_reply":"2021-06-27T02:27:41.394634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===============\n# Settings\n# ===============\nBATCH_SIZE = 8\nmax_len = 256\n\nrobeota_large_MODEL_PATH = '../input/roberta-transformers-pytorch/roberta-large'\nrobeota_large_tokenizer = RobertaTokenizer.from_pretrained(robeota_large_MODEL_PATH)\n\nroberta_base_MODEL_PATH = '../input/roberta-transformers-pytorch/roberta-base'\nroberta_base_tokenizer = RobertaTokenizer.from_pretrained(roberta_base_MODEL_PATH)\n\nroberta_base_MODEL_PATH2 = '../input/clrp-pytorch-roberta-pretrain-fold0/clrp_roberta_base'\nroberta_base_tokenizer2 = AutoTokenizer.from_pretrained(roberta_base_MODEL_PATH2)\n\nalbert_large_MODEL_PATH = \"../input/pretrained-albert-pytorch/albert-large-v2\"\nalbert_large_tokenizer = AlbertTokenizer.from_pretrained(albert_large_MODEL_PATH)\n\nxlnet_large_MODEL_PATH = '../input/xlnet-pretrained-models-pytorch/xlnet-large-cased-pytorch_model.bin'\nxlnet_large_tokenizer = XLNetTokenizer.from_pretrained(\"../input/xlnet-pretrained-models-pytorch/xlnet-large-cased-spiece.model\")\n\ndeberta_large_MODEL_PATH = \"../input/deberta/large\"\ndeberta_large_tokenizer = DebertaTokenizer.from_pretrained(deberta_large_MODEL_PATH)\n\nxlnet_base_MODEL_PATH = '../input/xlnet-pretrained-models-pytorch/xlnet-base-cased-pytorch_model.bin'\nxlnet_base_tokenizer = XLNetTokenizer.from_pretrained(\"../input/xlnet-pretrained-models-pytorch/xlnet-base-cased-spiece.model\")\n\nelectra_large_MODEL_PATH = \"../input/electra/large-discriminator\"\nelectra_large_tokenizer = ElectraTokenizer.from_pretrained(electra_large_MODEL_PATH)\n\nbart_large_MODEL_PATH = '../input/bart-models-hugging-face-model-repository/bart-large'\nbart_large_tokenizer = RobertaTokenizer.from_pretrained(robeota_large_MODEL_PATH)\n\ndeberta_xlarge_MODEL_PATH = \"../input/deberta/v2-xlarge\"\ndeberta_xlarge_tokenizer = AutoTokenizer.from_pretrained(deberta_xlarge_MODEL_PATH)\n\nmpnet_base_MODEL_PATH = \"../input/mpnet-base\"\nmpnet_base_tokenizer = MPNetTokenizer.from_pretrained(mpnet_base_MODEL_PATH)\n\ndeberta_v2_xxlarge_MODEL_PATH = \"../input/deberta/v2-xxlarge\"\ndeberta_v2_xxlarge_tokenizer = AutoTokenizer.from_pretrained(deberta_v2_xxlarge_MODEL_PATH)\n\nfunnel_large_MODEL_PATH = '../input/funnel-large-base-save/funnel-large/'\nfunnel_large_tokenizer = FunnelTokenizer.from_pretrained(funnel_large_MODEL_PATH )\n\nmuppet_roberta_large_MODEL_PATH = \"../input/muppet-roberta-large/muppet-roberta-large/\"\nmuppet_roberta_large_tokenizer = RobertaTokenizer.from_pretrained(muppet_roberta_large_MODEL_PATH)\n\nfunnel_medium_MODEL_PATH = '../input/funnel-medium-save/funnel-medium'\nfunnel_medium_tokenizer = FunnelTokenizer.from_pretrained(funnel_medium_MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T02:41:21.179363Z","iopub.execute_input":"2021-06-27T02:41:21.179733Z","iopub.status.idle":"2021-06-27T02:41:23.086147Z","shell.execute_reply.started":"2021-06-27T02:41:21.179701Z","shell.execute_reply":"2021-06-27T02:41:23.085377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===============\n# Functions\n# ===============\n\nclass CommonLitDataset(Dataset):\n    def __init__(self, excerpt, tokenizer, max_len, target=None):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.target = target\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=True\n        )\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        if self.target is not None:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long),\n                \"target\" : torch.tensor(self.target[item], dtype=torch.float32)\n            }\n        else:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long)\n            }\n\n    \nclass roberta_large_model(nn.Module):\n    def __init__(self):\n        super(roberta_large_model, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\n            robeota_large_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.roberta(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n\nclass roberta_base_model(nn.Module):\n    def __init__(self):\n        super(roberta_base_model, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\n            roberta_base_MODEL_PATH, \n        )\n        self.drop = nn.Dropout(0.2)\n        self.fc = nn.Linear(768, 256)\n        self.layernorm = nn.LayerNorm(256)\n        self.drop2 = nn.Dropout(0.2)\n        self.relu = nn.ReLU()\n        self.out = nn.Linear(256, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.roberta(ids, attention_mask=mask,token_type_ids=token_type_ids)['pooler_output']\n        output = self.drop(emb)\n        output = self.fc(output)\n        output = self.layernorm(output)\n        output = self.drop2(output)\n        output = self.relu(output)\n        output = self.out(output)\n        return output,emb\n    \nclass roberta_base_model2(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(roberta_base_MODEL_PATH2)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(roberta_base_MODEL_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        return self.regressor(context_vector)\n    \n    \n\nclass deberta_large_model(nn.Module):\n    def __init__(self):\n        super(deberta_large_model, self).__init__()\n        self.deberta_model = DebertaModel.from_pretrained(deberta_large_MODEL_PATH, \n                                                        hidden_dropout_prob = 0,\n                                                        attention_probs_dropout_prob = 0,\n                                                        hidden_act = \"gelu_new\")\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.deberta_model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state'][:,0,:]\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n\n\nclass xlnet_base_model(nn.Module):\n    def __init__(self):\n        super(xlnet_base_model, self).__init__()\n        xlnet_config = XLNetConfig.from_json_file('../input/xlnet-pretrained-models-pytorch/xlnet-base-cased-config.json')\n        xlnet_config.hidden_dropout_prob = 0\n        xlnet_config.attention_probs_dropout_prob = 0\n        xlnet_config.dropout = 0\n        self.xlnet_model = XLNetModel.from_pretrained(xlnet_base_MODEL_PATH, config=xlnet_config)\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(768)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.xlnet_model(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass electra_large_model(nn.Module):\n    def __init__(self):\n        super(electra_large_model, self).__init__()\n        self.electra = ElectraForSequenceClassification.from_pretrained(\n            electra_large_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0,\n            summary_last_dropout = 0,\n            num_labels = 1\n        )\n\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        output = self.electra(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"logits\"]\n        return output\n    \n    \n    \nclass bart_large_model(nn.Module):\n    def __init__(self):\n        super(bart_large_model, self).__init__()\n        self.bart = BartModel.from_pretrained(\n            bart_large_MODEL_PATH, \n            dropout=0.0, attention_dropout=0.0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask):\n        # pooler\n        emb = self.bart(ids, attention_mask=mask)['last_hidden_state']\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \nclass deberta_xlarge_model(nn.Module):\n    def __init__(self):\n        super(deberta_xlarge_model, self).__init__()\n        self.deberta_model =  AutoModel.from_pretrained(deberta_xlarge_MODEL_PATH, \n                                                        hidden_dropout_prob = 0,\n                                                        attention_probs_dropout_prob = 0)\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        #self.ln = nn.LayerNorm(1536)\n        self.out = nn.Linear(1536, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.deberta_model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state'][:,0,:]\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \nclass mpnet_base_model(nn.Module):\n    def __init__(self):\n        super(mpnet_base_model, self).__init__()\n        self.mpnet = MPNetModel.from_pretrained(\n            mpnet_base_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(768)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.mpnet(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \nclass deberta_v2_xxlarge_model(nn.Module):\n    def __init__(self):\n        super(deberta_v2_xxlarge_model, self).__init__()\n        self.deberta_model =  AutoModel.from_pretrained(deberta_v2_xxlarge_MODEL_PATH, \n                                                        hidden_dropout_prob = 0,\n                                                        attention_probs_dropout_prob = 0)\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        #self.ln = nn.LayerNorm(1536)\n        self.out = nn.Linear(1536, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.deberta_model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state'][:,0,:]\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \nclass funnel_large_model(nn.Module):\n    def __init__(self):\n        super(funnel_large_model, self).__init__()\n        self.funnel = FunnelBaseModel.from_pretrained(\n            funnel_large_MODEL_PATH, \n            hidden_dropout = 0,\n            attention_dropout = 0,\n            hidden_act = \"gelu\"\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.funnel(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \n    \nclass muppet_roberta_large_model(nn.Module):\n    def __init__(self):\n        super(muppet_roberta_large_model, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\n            muppet_roberta_large_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.roberta(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \nclass funnel_medium_model(nn.Module):\n    def __init__(self):\n        super(funnel_medium_model, self).__init__()\n        self.funnel = FunnelModel.from_pretrained(\n            funnel_medium_MODEL_PATH, \n            hidden_dropout = 0,\n            attention_dropout = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        #self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.funnel(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \n    \n    \ndef calc_loss(y_true, y_pred):\n    return  np.sqrt(mean_squared_error(y_true, y_pred))\n    \ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\n\ndef setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n    LOGGER.handlers = []\n    LOGGER.setLevel(min(stderr_level, file_level))\n\n    if stderr:\n        handler = logging.StreamHandler(sys.stderr)\n        handler.setFormatter(FORMATTER)\n        handler.setLevel(stderr_level)\n        LOGGER.addHandler(handler)\n\n    if out_file is not None:\n        handler = logging.FileHandler(out_file)\n        handler.setFormatter(FORMATTER)\n        handler.setLevel(file_level)\n        LOGGER.addHandler(handler)\n\n    LOGGER.info(\"logger set up\")\n    return LOGGER\n\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield \n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n    \n    \nLOGGER = logging.getLogger()\nFORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nsetup_logger(out_file=LOGGER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T02:41:26.319901Z","iopub.execute_input":"2021-06-27T02:41:26.320222Z","iopub.status.idle":"2021-06-27T02:41:26.398008Z","shell.execute_reply.started":"2021-06-27T02:41:26.320192Z","shell.execute_reply":"2021-06-27T02:41:26.397218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# Main\n# ================================\ntest = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-27T02:41:26.465702Z","iopub.execute_input":"2021-06-27T02:41:26.465951Z","iopub.status.idle":"2021-06-27T02:41:26.480086Z","shell.execute_reply.started":"2021-06-27T02:41:26.465926Z","shell.execute_reply":"2021-06-27T02:41:26.479146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# funnel_medium\n# ================================\nif len(test) > 0:\n    with timer(\"funnel_medium\"):\n        y_test_funnel_medium = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, funnel_medium_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = funnel_medium_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex423/ex423_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_funnel_medium.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_funnel_medium = np.mean(y_test_funnel_medium,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T02:42:19.720187Z","iopub.execute_input":"2021-06-27T02:42:19.72056Z","iopub.status.idle":"2021-06-27T02:44:19.262487Z","shell.execute_reply.started":"2021-06-27T02:42:19.720524Z","shell.execute_reply":"2021-06-27T02:44:19.261543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_test_funnel_medium)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmission.target = y_test_funnel_medium.reshape(-1)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}