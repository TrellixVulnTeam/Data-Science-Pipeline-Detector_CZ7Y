{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ========================================\n# library\n# ========================================\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel,AutoConfig\nimport transformers\nfrom transformers import RobertaModel,RobertaTokenizer\n\nfrom transformers import AlbertModel,AlbertTokenizer\nfrom transformers import XLNetModel,XLNetTokenizer,XLNetConfig\nfrom transformers import DebertaModel, DebertaTokenizer\nfrom transformers import ElectraModel, ElectraTokenizer, ElectraForSequenceClassification\nfrom transformers import BartModel,BertModel,BertTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import MPNetModel,MPNetTokenizer\nfrom transformers import FunnelBaseModel,FunnelTokenizer,FunnelModel\nfrom transformers import GPT2Model, GPT2Tokenizer, GPT2Config\nfrom transformers import T5EncoderModel,T5Tokenizer\nimport logging\nimport sys\nfrom contextlib import contextmanager\nimport time\nimport random\nfrom tqdm import tqdm\nimport os\nimport pickle\nimport gc","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:35:33.631446Z","iopub.execute_input":"2021-07-28T05:35:33.63178Z","iopub.status.idle":"2021-07-28T05:35:41.057655Z","shell.execute_reply.started":"2021-07-28T05:35:33.63171Z","shell.execute_reply":"2021-07-28T05:35:41.054821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ==================\n# Constant\n# ==================\nex = \"015_237_064_072_107_190_182_194_292_216_272_384_407_429_450\"\nTRAIN_PATH = \"../input/commonlitreadabilityprize/train.csvv\"\nLOGGER_PATH = f\"ex{ex}.txt\"\nFOLD_PATH = \"../input/fe001-step-1-create-folds/fe001_train_folds.csv\"\nMODEL_PATH_BASE = f\"ex{ex}\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:35:41.059419Z","iopub.execute_input":"2021-07-28T05:35:41.05977Z","iopub.status.idle":"2021-07-28T05:35:41.118836Z","shell.execute_reply.started":"2021-07-28T05:35:41.059733Z","shell.execute_reply":"2021-07-28T05:35:41.117577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===============\n# Settings\n# ===============\nBATCH_SIZE = 8\nmax_len = 256\n\nrobeota_large_MODEL_PATH = '../input/roberta-transformers-pytorch/roberta-large'\nrobeota_large_tokenizer = RobertaTokenizer.from_pretrained(robeota_large_MODEL_PATH)\n\nroberta_base_MODEL_PATH = '../input/roberta-transformers-pytorch/roberta-base'\nroberta_base_tokenizer = RobertaTokenizer.from_pretrained(roberta_base_MODEL_PATH)\n\nroberta_base_MODEL_PATH2 = '../input/clrp-pytorch-roberta-pretrain-fold0/clrp_roberta_base'\nroberta_base_tokenizer2 = AutoTokenizer.from_pretrained(roberta_base_MODEL_PATH2)\n\n#albert_large_MODEL_PATH = \"../input/pretrained-albert-pytorch/albert-large-v2\"\n#albert_large_tokenizer = AlbertTokenizer.from_pretrained(albert_large_MODEL_PATH)\n\ndeberta_large_MODEL_PATH = \"../input/deberta/large\"\ndeberta_large_tokenizer = DebertaTokenizer.from_pretrained(deberta_large_MODEL_PATH)\n\n#xlnet_base_MODEL_PATH = '../input/xlnet-pretrained-models-pytorch/xlnet-base-cased-pytorch_model.bin'\n#xlnet_base_tokenizer = XLNetTokenizer.from_pretrained(\"../input/xlnet-pretrained-models-pytorch/xlnet-base-cased-spiece.model\")\n\nelectra_large_MODEL_PATH = \"../input/electra/large-discriminator\"\nelectra_large_tokenizer = ElectraTokenizer.from_pretrained(electra_large_MODEL_PATH)\n\nbart_large_MODEL_PATH = '../input/bart-models-hugging-face-model-repository/bart-large'\nbart_large_tokenizer = RobertaTokenizer.from_pretrained(robeota_large_MODEL_PATH)\n\ndeberta_xlarge_MODEL_PATH = \"../input/deberta/v2-xlarge\"\ndeberta_xlarge_tokenizer = AutoTokenizer.from_pretrained(deberta_xlarge_MODEL_PATH)\n\nmpnet_base_MODEL_PATH = \"../input/mpnet-base\"\nmpnet_base_tokenizer = MPNetTokenizer.from_pretrained(mpnet_base_MODEL_PATH)\n\ndeberta_v2_xxlarge_MODEL_PATH = \"../input/deberta/v2-xxlarge\"\ndeberta_v2_xxlarge_tokenizer = AutoTokenizer.from_pretrained(deberta_v2_xxlarge_MODEL_PATH)\n\nfunnel_large_MODEL_PATH = '../input/funnel-large-base-save/funnel-large/'\nfunnel_large_tokenizer = FunnelTokenizer.from_pretrained(funnel_large_MODEL_PATH )\n\nmuppet_roberta_large_MODEL_PATH = \"../input/muppet-roberta-large/muppet-roberta-large/\"\nmuppet_roberta_large_tokenizer = RobertaTokenizer.from_pretrained(muppet_roberta_large_MODEL_PATH)\n\nfunnel_large_full_MODEL_PATH = '../input/funnel-large-save/funnel-large/'\nfunnel_large_full_tokenizer = FunnelTokenizer.from_pretrained(funnel_large_full_MODEL_PATH )\n\nfunnel_medium_MODEL_PATH = '../input/funnel-medium-save/funnel-medium'\nfunnel_medium_tokenizer = FunnelTokenizer.from_pretrained(funnel_medium_MODEL_PATH)\n\ngpt2_medium_MODEL_PATH = \"../input/gpt-medium-save/gpt-medium/\"\ngpt2_medium_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_medium_MODEL_PATH)\n\nalbert_v2_xxlarge_MODEL_PATH = \"../input/albert-xxlarge-v2-save/albert-xxlarge-v2/\"\nalbert_v2_xxlarge_tokenizer = AlbertTokenizer.from_pretrained(albert_v2_xxlarge_MODEL_PATH)\n\nelectra_base_MODEL_PATH = \"../input/electra/base-discriminator\"\nelectra_base_tokenizer = ElectraTokenizer.from_pretrained(electra_base_MODEL_PATH)\n\nbert_base_uncased_MODEL_PATH = '../input/bert-uncased-base-save/bert-base-uncased/'\nbert_base_uncased_tokenizer = BertTokenizer.from_pretrained(bert_base_uncased_MODEL_PATH)\n\nt5_large_MODEL_PATH = '../input/t5-large-save/t5-large/'\nt5_large_tokenizer = T5Tokenizer.from_pretrained(t5_large_MODEL_PATH)\n\ndistil_bart_MODEL_PATH = '../input/distilbart-save/distil-bart/'\ndistil_bart_tokenizer = RobertaTokenizer.from_pretrained(distil_bart_MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:35:41.121346Z","iopub.execute_input":"2021-07-28T05:35:41.121871Z","iopub.status.idle":"2021-07-28T05:35:44.368318Z","shell.execute_reply.started":"2021-07-28T05:35:41.12183Z","shell.execute_reply":"2021-07-28T05:35:44.367443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ===============\n# Functions\n# ===============\n\nclass CommonLitDataset(Dataset):\n    def __init__(self, excerpt, tokenizer, max_len, target=None):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.target = target\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=True\n        )\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        if self.target is not None:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long),\n                \"target\" : torch.tensor(self.target[item], dtype=torch.float32)\n            }\n        else:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long)\n            }\n\nclass roberta_large_model(nn.Module):\n    def __init__(self):\n        super(roberta_large_model, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\n            robeota_large_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.roberta(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n\nclass roberta_base_model(nn.Module):\n    def __init__(self):\n        super(roberta_base_model, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\n            roberta_base_MODEL_PATH, \n        )\n        self.drop = nn.Dropout(0.2)\n        self.fc = nn.Linear(768, 256)\n        self.layernorm = nn.LayerNorm(256)\n        self.drop2 = nn.Dropout(0.2)\n        self.relu = nn.ReLU()\n        self.out = nn.Linear(256, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.roberta(ids, attention_mask=mask,token_type_ids=token_type_ids)['pooler_output']\n        output = self.drop(emb)\n        output = self.fc(output)\n        output = self.layernorm(output)\n        output = self.drop2(output)\n        output = self.relu(output)\n        output = self.out(output)\n        return output,emb\n    \nclass roberta_base_model2(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(roberta_base_MODEL_PATH2)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(roberta_base_MODEL_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        return self.regressor(context_vector)\n    \n    \n\nclass deberta_large_model(nn.Module):\n    def __init__(self):\n        super(deberta_large_model, self).__init__()\n        self.deberta_model = DebertaModel.from_pretrained(deberta_large_MODEL_PATH, \n                                                        hidden_dropout_prob = 0,\n                                                        attention_probs_dropout_prob = 0,\n                                                        hidden_act = \"gelu_new\")\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.deberta_model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state'][:,0,:]\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n\n\nclass xlnet_base_model(nn.Module):\n    def __init__(self):\n        super(xlnet_base_model, self).__init__()\n        xlnet_config = XLNetConfig.from_json_file('../input/xlnet-pretrained-models-pytorch/xlnet-base-cased-config.json')\n        xlnet_config.hidden_dropout_prob = 0\n        xlnet_config.attention_probs_dropout_prob = 0\n        xlnet_config.dropout = 0\n        self.xlnet_model = XLNetModel.from_pretrained(xlnet_base_MODEL_PATH, config=xlnet_config)\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(768)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.xlnet_model(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass electra_large_model(nn.Module):\n    def __init__(self):\n        super(electra_large_model, self).__init__()\n        self.electra = ElectraForSequenceClassification.from_pretrained(\n            electra_large_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0,\n            summary_last_dropout = 0,\n            num_labels = 1\n        )\n\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        output = self.electra(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"logits\"]\n        return output\n    \n    \n    \nclass bart_large_model(nn.Module):\n    def __init__(self):\n        super(bart_large_model, self).__init__()\n        self.bart = BartModel.from_pretrained(\n            bart_large_MODEL_PATH, \n            dropout=0.0, attention_dropout=0.0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask):\n        # pooler\n        emb = self.bart(ids, attention_mask=mask)['last_hidden_state']\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \nclass deberta_xlarge_model(nn.Module):\n    def __init__(self):\n        super(deberta_xlarge_model, self).__init__()\n        self.deberta_model =  AutoModel.from_pretrained(deberta_xlarge_MODEL_PATH, \n                                                        hidden_dropout_prob = 0,\n                                                        attention_probs_dropout_prob = 0)\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        #self.ln = nn.LayerNorm(1536)\n        self.out = nn.Linear(1536, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.deberta_model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state'][:,0,:]\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \nclass mpnet_base_model(nn.Module):\n    def __init__(self):\n        super(mpnet_base_model, self).__init__()\n        self.mpnet = MPNetModel.from_pretrained(\n            mpnet_base_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(768)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.mpnet(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \nclass deberta_v2_xxlarge_model(nn.Module):\n    def __init__(self):\n        super(deberta_v2_xxlarge_model, self).__init__()\n        self.deberta_model =  AutoModel.from_pretrained(deberta_v2_xxlarge_MODEL_PATH, \n                                                        hidden_dropout_prob = 0,\n                                                        attention_probs_dropout_prob = 0)\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        #self.ln = nn.LayerNorm(1536)\n        self.out = nn.Linear(1536, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.deberta_model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state'][:,0,:]\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \nclass funnel_large_model(nn.Module):\n    def __init__(self):\n        super(funnel_large_model, self).__init__()\n        self.funnel = FunnelBaseModel.from_pretrained(\n            funnel_large_MODEL_PATH, \n            hidden_dropout = 0,\n            attention_dropout = 0,\n            hidden_act = \"gelu\"\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.funnel(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \n    \nclass muppet_roberta_large_model(nn.Module):\n    def __init__(self):\n        super(muppet_roberta_large_model, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(\n            muppet_roberta_large_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.roberta(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \nclass funnel_model(nn.Module):\n    def __init__(self):\n        super(funnel_model, self).__init__()\n        self.funnel = FunnelModel.from_pretrained(\n            funnel_large_full_MODEL_PATH, \n            hidden_dropout = 0,\n            attention_dropout = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        #self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.funnel(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \nclass funnel_medium_model(nn.Module):\n    def __init__(self):\n        super(funnel_medium_model, self).__init__()\n        self.funnel = FunnelModel.from_pretrained(\n            funnel_medium_MODEL_PATH, \n            hidden_dropout = 0,\n            attention_dropout = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        #self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.funnel(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        #output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(emb)\n        return output\n    \nclass gpt2_medium_model(nn.Module):\n    def __init__(self):\n        super(gpt2_medium_model, self).__init__()\n        self.gpt2_model = GPT2Model.from_pretrained(gpt2_medium_MODEL_PATH,\n                                                   attn_pdrop = 0,\n                                                   embd_pdrop = 0,\n                                                   resid_pdrop = 0,\n                                                   summary_first_dropout = 0)\n        self.gpt2_model.resize_token_embeddings(len(gpt2_medium_tokenizer))\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask):\n        # pooler\n        emb = self.gpt2_model(ids, attention_mask=mask)[\"last_hidden_state\"]\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \nclass albert_v2_xxlarge_model(nn.Module):\n    def __init__(self):\n        super(albert_v2_xxlarge_model, self).__init__()\n        self.albert = AlbertModel.from_pretrained(\n            albert_v2_xxlarge_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(4096)\n        self.out = nn.Linear(4096, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.albert(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \nclass gpt2_large_model(nn.Module):\n    def __init__(self):\n        super(gpt2_large_model, self).__init__()\n        self.gpt2_model = GPT2Model.from_pretrained(gpt2_large_MODEL_PATH,\n                                                   attn_pdrop = 0,\n                                                   embd_pdrop = 0,\n                                                   resid_pdrop = 0,\n                                                   summary_first_dropout = 0)\n        self.gpt2_model.resize_token_embeddings(len(gpt2_large_tokenizer))\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1280)\n        self.out = nn.Linear(1280, 1)\n    \n    def forward(self, ids, mask):\n        # pooler\n        emb = self.gpt2_model(ids, attention_mask=mask)[\"last_hidden_state\"]\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass electra_base_model(nn.Module):\n    def __init__(self):\n        super(electra_base_model, self).__init__()\n        self.electra = ElectraModel.from_pretrained(\n            electra_base_MODEL_PATH, \n            hidden_dropout_prob = 0,\n            attention_probs_dropout_prob = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(768)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb = self.electra(ids, attention_mask=mask,token_type_ids=token_type_ids)[\"last_hidden_state\"]\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \nclass bert_base_uncased_model(nn.Module):\n    def __init__(self):\n        super(bert_base_uncased_model, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(bert_base_uncased_MODEL_PATH,\n                                                          hidden_dropout_prob = 0,\n                                                          attention_probs_dropout_prob = 0)\n        #self.bert = transformers.BertForSequenceClassification.from_pretrained(BERT_MODEL,num_labels=1)\n        self.ln = nn.LayerNorm(768)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        # pooler\n        emb , _= self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n        emb = torch.mean(emb, axis=1)\n        output = self.ln(emb)\n        output = self.out(output)\n        return output\n    \nclass t5_large_model(nn.Module):\n    def __init__(self):\n        super(t5_large_model, self).__init__()\n        self.t5 = T5EncoderModel.from_pretrained(t5_large_MODEL_PATH,\n                            dropout_rate = 0)\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask):\n        # pooler\n        emb = self.t5(ids, attention_mask=mask)['last_hidden_state']\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \nclass distil_bart_model(nn.Module):\n    def __init__(self):\n        super(distil_bart_model, self).__init__()\n        self.bart = BartModel.from_pretrained(\n            distil_bart_MODEL_PATH, \n            activation_dropout=0.0, attention_dropout=0.0,\n            classif_dropout = 0, classifier_dropout = 0\n        )\n        \n        #self.dropout = nn.Dropout(p=0.2)\n        self.ln = nn.LayerNorm(1024)\n        self.out = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask):\n        # pooler\n        emb = self.bart(ids, attention_mask=mask)['last_hidden_state']\n        emb = torch.mean(emb,axis=1)\n        output = self.ln(emb)\n        #output = self.dropout(output)\n        output = self.out(output)\n        return output\n    \n    \nclass CommonLitDataset_gpt(Dataset):\n    def __init__(self, excerpt, tokenizer, max_len, target=None):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.target = target\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer('<|startoftext|>'+ text + '<|endoftext|>', truncation=True, max_length=self.max_len, padding=\"max_length\")\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        #token_type_ids = inputs[\"token_type_ids\"]\n        if self.target is not None:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                #\"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long),\n                \"target\" : torch.tensor(self.target[item], dtype=torch.float32)\n            }\n        else:\n            return {\n                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n                #\"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long)\n            }\n\n    \ndef calc_loss(y_true, y_pred):\n    return  np.sqrt(mean_squared_error(y_true, y_pred))\n    \ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\n\ndef setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n    LOGGER.handlers = []\n    LOGGER.setLevel(min(stderr_level, file_level))\n\n    if stderr:\n        handler = logging.StreamHandler(sys.stderr)\n        handler.setFormatter(FORMATTER)\n        handler.setLevel(stderr_level)\n        LOGGER.addHandler(handler)\n\n    if out_file is not None:\n        handler = logging.FileHandler(out_file)\n        handler.setFormatter(FORMATTER)\n        handler.setLevel(file_level)\n        LOGGER.addHandler(handler)\n\n    LOGGER.info(\"logger set up\")\n    return LOGGER\n\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield \n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n    \n    \nLOGGER = logging.getLogger()\nFORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nsetup_logger(out_file=LOGGER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:35:44.371794Z","iopub.execute_input":"2021-07-28T05:35:44.372082Z","iopub.status.idle":"2021-07-28T05:35:44.480624Z","shell.execute_reply.started":"2021-07-28T05:35:44.372049Z","shell.execute_reply":"2021-07-28T05:35:44.479869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# Main\n# ================================\ntest = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:35:44.48255Z","iopub.execute_input":"2021-07-28T05:35:44.482894Z","iopub.status.idle":"2021-07-28T05:35:44.500531Z","shell.execute_reply.started":"2021-07-28T05:35:44.482858Z","shell.execute_reply":"2021-07-28T05:35:44.499694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# roberta base -> svr + ridge\n# ================================\nif len(test) > 100:\n    with timer(\"roberta base -> svr + ridge\"):\n        y_test_roberta_base = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, roberta_base_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n        for fold in range(5):\n\n            # model\n            model = roberta_base_model()\n            model.load_state_dict(torch.load(f\"../input/ex014-roberta/ex014_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_emb = np.ndarray((0,768))\n\n            # svr\n            svr = pickle.load(open(f\"../input/ex015-lgb/ex015_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            # ridge\n            ridge = pickle.load(open(f\"../input/ex015-lgb/ex015_ridge_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    _,output = model(input_ids, mask,token_type_ids )\n\n                    test_emb = np.concatenate([test_emb, output.detach().cpu().numpy()], axis=0)\n            x_test = pd.DataFrame(test_emb )\n            x_test.columns = [f\"emb_{i}\" for i in range(len(x_test.columns))]\n            test_preds_svr = svr.predict(x_test)\n            test_preds_ridge = ridge.predict(x_test)\n            test_preds = (test_preds_svr + test_preds_ridge)/2\n            y_test_roberta_base.append(test_preds)\n            del x_test,model,test_emb\n            gc.collect()\n\n        y_test_roberta_base = np.mean(y_test_roberta_base,axis=0)\n        del test_,test_loader\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:36:08.458658Z","iopub.execute_input":"2021-07-28T05:36:08.458982Z","iopub.status.idle":"2021-07-28T05:37:19.562933Z","shell.execute_reply.started":"2021-07-28T05:36:08.458952Z","shell.execute_reply":"2021-07-28T05:37:19.562098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del roberta_base_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# roberta base\n# ================================\nif len(test) > 100:\n    with timer(\"roberta base\"):\n        y_test_roberta_base2 = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, roberta_base_tokenizer2, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n        for fold in range(5):\n\n            # model\n            model = roberta_base_model2()\n            model.load_state_dict(torch.load(f\"../input/ex237-roberta/ex237_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask)\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)     \n            y_test_roberta_base2.append(test_preds)\n            del model\n            gc.collect()\n\n        y_test_roberta_base2 = np.mean(y_test_roberta_base2,axis=0)\n        del test_, test_loader\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:24:52.566701Z","iopub.execute_input":"2021-07-28T05:24:52.567166Z","iopub.status.idle":"2021-07-28T05:25:54.27109Z","shell.execute_reply.started":"2021-07-28T05:24:52.56712Z","shell.execute_reply":"2021-07-28T05:25:54.269835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del roberta_base_tokenizer2\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:53:39.646702Z","iopub.execute_input":"2021-07-28T05:53:39.647224Z","iopub.status.idle":"2021-07-28T05:53:39.835034Z","shell.execute_reply.started":"2021-07-28T05:53:39.647158Z","shell.execute_reply":"2021-07-28T05:53:39.833613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# roberta_larege\n# ================================\nif len(test) > 100:\n    with timer(\"roberta_larege\"):\n        y_test_roberta_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, robeota_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = roberta_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex072-2/ex072_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_roberta_large.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_roberta_large = np.mean(y_test_roberta_large,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:25:54.275137Z","iopub.execute_input":"2021-07-28T05:25:54.275475Z","iopub.status.idle":"2021-07-28T05:28:39.999026Z","shell.execute_reply.started":"2021-07-28T05:25:54.275441Z","shell.execute_reply":"2021-07-28T05:28:39.997926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del robeota_large_tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:53:54.234726Z","iopub.execute_input":"2021-07-28T05:53:54.235194Z","iopub.status.idle":"2021-07-28T05:53:54.422671Z","shell.execute_reply.started":"2021-07-28T05:53:54.235141Z","shell.execute_reply":"2021-07-28T05:53:54.421446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# deberta_large\n# ================================\nif len(test) > 100:\n    with timer(\"deberta_large\"):\n        y_test_deberta_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, deberta_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = deberta_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex182/ex182_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_deberta_large .append(test_preds)\n            del model\n            gc.collect()\n        del test_,test_loader\n        gc.collect()\n        y_test_deberta_large  = np.mean(y_test_deberta_large ,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T05:28:40.002174Z","iopub.execute_input":"2021-07-28T05:28:40.002674Z","iopub.status.idle":"2021-07-28T05:31:16.837955Z","shell.execute_reply.started":"2021-07-28T05:28:40.002623Z","shell.execute_reply":"2021-07-28T05:31:16.83639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# electra_large\n# ================================\nif len(test) > 100:\n    with timer(\"electra_largee\"):\n        y_test_electra_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, electra_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n        for fold in tqdm(range(5)):\n\n\n            # model\n            model = electra_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex190/ex190_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_electra_large.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_electra_large = np.mean(y_test_electra_large,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T02:47:10.030149Z","iopub.execute_input":"2021-06-27T02:47:10.030486Z","iopub.status.idle":"2021-06-27T02:49:32.886887Z","shell.execute_reply.started":"2021-06-27T02:47:10.030451Z","shell.execute_reply":"2021-06-27T02:49:32.886026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del electra_large_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# bart_large\n# ================================\nif len(test) > 100:\n    with timer(\"bart_largee\"):\n        y_test_bart_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, bart_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n        for fold in tqdm(range(5)):\n\n\n            # model\n            model = bart_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex107/ex107_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask)\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_bart_large.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_bart_large = np.mean(y_test_bart_large,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T02:49:32.889357Z","iopub.execute_input":"2021-06-27T02:49:32.889724Z","iopub.status.idle":"2021-06-27T02:52:26.341365Z","shell.execute_reply.started":"2021-06-27T02:49:32.889686Z","shell.execute_reply":"2021-06-27T02:52:26.340545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del bart_large_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# deberta_xlarge\n# ================================\nif len(test) > 100:\n    with timer(\"deberta_xlarge\"):\n        y_test_deberta_xlarge = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, deberta_xlarge_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=4, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = deberta_xlarge_model()\n            model.load_state_dict(torch.load(f\"../input/deberta-v2-xlarge-194-save/ex194_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_deberta_xlarge .append(test_preds)\n            del model\n            gc.collect()\n        del test_,test_loader\n        gc.collect()\n        y_test_deberta_xlarge  = np.mean(y_test_deberta_xlarge ,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T02:52:26.343893Z","iopub.execute_input":"2021-06-27T02:52:26.344252Z","iopub.status.idle":"2021-06-27T02:58:45.463574Z","shell.execute_reply.started":"2021-06-27T02:52:26.344213Z","shell.execute_reply":"2021-06-27T02:58:45.462614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del deberta_xlarge_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# mpnet_base\n# ================================\nif len(test) > 100:\n    with timer(\"mpnet_base\"):\n        y_test_mpnet_base = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, mpnet_base_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = mpnet_base_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex292/ex292_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n\n            with torch.no_grad():  \n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_mpnet_base.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_mpnet_base = np.mean(y_test_mpnet_base,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T02:58:45.464942Z","iopub.execute_input":"2021-06-27T02:58:45.465432Z","iopub.status.idle":"2021-06-27T02:59:29.401523Z","shell.execute_reply.started":"2021-06-27T02:58:45.465394Z","shell.execute_reply":"2021-06-27T02:59:29.400678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del mpnet_base_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# deberta_v2_xxlarge\n# ================================\nif len(test) > 100:\n    with timer(\"deberta_v2_xlarge\"):\n        y_test_deberta_v2_xxlarge = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, deberta_v2_xxlarge_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=4, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = deberta_v2_xxlarge_model()\n            if fold < 3:\n                model.load_state_dict(torch.load(f\"../input/ex216-inference-save-1/ex216_{fold}.pth\"))\n            else:\n                model.load_state_dict(torch.load(f\"../input/ex216-inference-save-2/ex216_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_deberta_v2_xxlarge.append(test_preds)\n            del model\n            gc.collect()\n        del test_,test_loader\n        gc.collect()\n        y_test_deberta_v2_xxlarge  = np.mean(y_test_deberta_v2_xxlarge ,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T02:59:29.402873Z","iopub.execute_input":"2021-06-27T02:59:29.403204Z","iopub.status.idle":"2021-06-27T03:08:16.853038Z","shell.execute_reply.started":"2021-06-27T02:59:29.403168Z","shell.execute_reply":"2021-06-27T03:08:16.851296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del deberta_v2_xxlarge_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# funnel_large\n# ================================\nif len(test) > 100:\n    with timer(\"funnel_large\"):\n        y_test_funnel_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, funnel_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=4, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = funnel_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex272/ex272_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_funnel_large.append(test_preds)\n            del model\n            gc.collect()\n        del test_,test_loader\n        gc.collect()\n        y_test_funnel_large  = np.mean(y_test_funnel_large ,axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T03:08:16.858747Z","iopub.execute_input":"2021-06-27T03:08:16.859006Z","iopub.status.idle":"2021-06-27T03:08:16.874547Z","shell.execute_reply.started":"2021-06-27T03:08:16.858979Z","shell.execute_reply":"2021-06-27T03:08:16.873749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del funnel_large_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(test) > 100:\n    with timer(\"muppet_roberta_larege\"):\n        y_test_muppet_roberta_large = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, muppet_roberta_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = muppet_roberta_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex384/ex384_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_muppet_roberta_large.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_muppet_roberta_large = np.mean(y_test_muppet_roberta_large,axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del muppet_roberta_large_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# funnel large\n# ================================\nif len(test) > 100:\n    with timer(\"funnel_model\"):\n        y_test_funnel_large_full = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, funnel_large_full_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = funnel_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex407/ex407_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_funnel_large_full.append(test_preds)\n            del model\n            gc.collect()\n        del test_,test_loader\n        gc.collect()\n        y_test_funnel_large_full  = np.mean(y_test_funnel_large_full ,axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del funnel_large_full_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# funnel_medium\n# ================================\nif len(test) > 100:\n    with timer(\"gpt_medium\"):\n        y_test_gpt2_medium = []\n        # dataset\n        test_ = CommonLitDataset_gpt(test[\"excerpt\"].values, gpt2_medium_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = gpt2_medium_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex429/ex429_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    #token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    #token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_gpt2_medium.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_gpt2_medium= np.mean(y_test_gpt2_medium,axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del gpt2_medium_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# albert_v2_xxlarge_model\n# ================================\nif len(test) > 100:\n    with timer(\"albert_v2_xxlarge_model\"):\n        y_test_albert_v2_xxlarge = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, albert_v2_xxlarge_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = albert_v2_xxlarge_model()\n            if fold == 2:\n                model.load_state_dict(torch.load(f\"../input/commonlit-ex448/ex448_{fold}.pth\"))\n            else:\n                model.load_state_dict(torch.load(f\"../input/commonlit-ex450/ex450_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            y_test_albert_v2_xxlarge.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        y_test_albert_v2_xxlarge = np.mean(y_test_albert_v2_xxlarge,axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del albert_v2_xxlarge_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del deberta_large_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# ex465 electra_base_model\n# ================================\nif len(test) > 100:\n    with timer(\"electra_base_model\"):\n        ex465_pred = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, electra_base_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = electra_base_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex465/ex465_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            ex465_pred.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        ex465_pred = np.mean(ex465_pred,axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del electra_base_tokenizer\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# ex497 bert_base_uncased_model\n# ================================\nif len(test) > 100:\n    with timer(\"bert_base_uncased_model\"):\n        ex497_pred = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, bert_base_uncased_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = bert_base_uncased_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex497/ex497_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask,token_type_ids )\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            ex497_pred.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        ex497_pred = np.mean(ex497_pred,axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# ex494 t5_large_model\n# ================================\nif len(test) > 100:\n    with timer(\"t5_large\"):\n        ex434_pred = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, t5_large_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model = t5_large_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex434/ex434_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask)\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            ex434_pred.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        ex434_pred = np.mean(ex434_pred,axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ================================\n# distil_bart\n# ================================\nif len(test) > 100:\n    with timer(\"distil_bart\"):\n        ex507_pred = []\n        # dataset\n        test_ = CommonLitDataset(test[\"excerpt\"].values, distil_bart_tokenizer, max_len, None)\n\n        # loader\n        test_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n\n        for fold in tqdm(range(5)):\n\n            # model\n            model =  distil_bart_model()\n            model.load_state_dict(torch.load(f\"../input/commonlit-ex507/ex507_{fold}.pth\"))\n            model.to(device)\n            model.eval()\n            test_preds = np.ndarray((0,1))\n\n            # svr\n            #svr = pickle.load(open(f\"../input/ex040-svr/ex040_svr_roberta_emb_{fold}.pkl\",\"rb\"))\n\n            with torch.no_grad():  \n                # Predicting on validation set\n                for d in test_loader:\n                    # =========================\n                    # data loader\n                    # =========================\n                    input_ids = d['input_ids']\n                    mask = d['attention_mask']\n                    token_type_ids = d[\"token_type_ids\"]\n\n                    input_ids = input_ids.to(device)\n                    mask = mask.to(device)\n                    token_type_ids = token_type_ids.to(device)\n                    output = model(input_ids, mask)\n\n                    test_preds = np.concatenate([test_preds, output.detach().cpu().numpy()], axis=0)        \n            ex507_pred.append(test_preds)\n            del model\n            gc.collect()\n        del test_, test_loader\n        gc.collect()\n        ex507_pred = np.mean(ex507_pred,axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_list = [0.08,        0.00,        0.11,        0.09,      0.10,      0.23,      0.05 ,    0.13   , 0.14 ,  0.11,  0.17, 0.12,-0.17,-0.14,-0.11, 0.09]\n            # ro ba 15    xl ba 64     ro la 72     ba la 107  el la 190    de la 182  de xla 194 mp ba 207  de xxla    funel large  gpt_medium  albert v2 xx\n            #CV  0.499    0.491        0.483        0.476      0.483           0.481     0.486       0.482     0.482      0.475         0.498      0.486\n            #LB  0.475    0.480        0.463        0.469      0.470           0.460     0.466       0.470     0.465      0.464         0.479      0.467\n            #ro ba 237                 ro mu la                funel -base\n             ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(test) > 100:\n    y_test = (y_test_roberta_base + y_test_roberta_base2.reshape(-1)) / 2 * weight_list[0] + \\\n             (y_test_roberta_large.reshape(-1) * 0.8 + y_test_muppet_roberta_large.reshape(-1) * 0.2) * weight_list[2] +\\\n             y_test_bart_large.reshape(-1) *  weight_list[3] +\\\n             (y_test_electra_large.reshape(-1) + y_test_funnel_large.reshape(-1))  / 2 * weight_list[4]+\\\n             y_test_deberta_large.reshape(-1) *  weight_list[5]+\\\n              y_test_deberta_xlarge.reshape(-1) *  weight_list[6]+\\\n             y_test_mpnet_base.reshape(-1) *  weight_list[7]+\\\n             y_test_deberta_v2_xxlarge.reshape(-1) *  weight_list[8]+\\\n             y_test_funnel_large_full.reshape(-1) * weight_list[9]+\\\n             y_test_gpt2_medium.reshape(-1) * weight_list[10]+\\\n             y_test_albert_v2_xxlarge.reshape(-1) * weight_list[11]+\\\n             ex465_pred.reshape(-1) * weight_list[12]+\\\n             ex497_pred.reshape(-1) * weight_list[13]+\\\n             ex434_pred.reshape(-1) * weight_list[14]+\\\n             ex507_pred.reshape(-1) * weight_list[15]\nelse:\n    y_test = np.zeros(len(test))\nsubmission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmission.target = y_test\nsubmission.loc[(y_test >= 0.3), \"target\"] = y_test[(y_test >= 0.3)] * 1.07\nsubmission.loc[(y_test < 0.3) & (y_test >= 0) , \"target\"] = y_test[(y_test < 0.3) & (y_test >= 0)] * 1.2\nsubmission.loc[(y_test < 0) & (y_test >= -0.7), \"target\"] = y_test[(y_test < 0) & (y_test >= -0.7)] * 0.97485814\nsubmission.loc[(y_test < -0.7) & (y_test >= -0.9), \"target\"] = y_test[(y_test < -0.7) & (y_test >= -0.9)] * 1.01\nsubmission.loc[(y_test < -0.9) & (y_test >= -2), \"target\"] = y_test[(y_test < -0.9) & (y_test >= -2)] * 1.02150304\nsubmission.loc[(y_test < -2), \"target\"] = y_test[(y_test < -2)] * 1.02764047\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sum(weight_list))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}