{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src = \"https://uploads-ssl.webflow.com/5a0a62705f389a000169e9bb/5a0a62705f389a000169effa_869061bd245d56abe0d8ca3fe83885d71cb26a8d8b7d0f64ade725cd4fc86592.jpg\" width=1000, align='center'>\nsrc = https://uploads-ssl.webflow.com/5a0a62705f389a000169e9bb/5a0a62705f389a000169effa_869061bd245d56abe0d8ca3fe83885d71cb26a8d8b7d0f64ade725cd4fc86592.jpg","metadata":{}},{"cell_type":"markdown","source":"## Understanding:\nCan machine learning identify the appropriate reading level of a passage of text, and help inspire learning? Reading is an essential skill for academic success. When students have access to engaging passages offering the right level of challenge, they naturally develop reading skills.\n\nCurrently, most educational texts are matched to readers using traditional readability methods or commercially available formulas. However, each has its issues. Tools like Flesch-Kincaid Grade Level are based on weak proxies of text decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number or words per sentence). As a result, they lack construct and theoretical validity. At the same time, commercially available formulas, such as Lexile, can be cost-prohibitive, lack suitable validation studies, and suffer from transparency issues when the formula's features aren't publicly available.\n\nCommonLit, Inc., is a nonprofit education technology organization serving over 20 million teachers and students with free digital reading and writing lessons for grades 3-12. Together with Georgia State University, an R1 public research university in Atlanta, they are challenging Kagglers to improve readability rating methods.\n\nIn this competition, youâ€™ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.\n\nIf successful, you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these formulas will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.\n\n### Data: \n#### Columns\n- id - unique ID for excerpt\n- url_legal - URL of source - this is blank in the test set.\n- license - license of source material - this is blank in the test set.\n- excerpt - text to predict reading ease of\n- target - reading ease\n- standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n\n# AIM:\nMy basic understanding is that most of the big words are not easy to read atleast for me ;p. So in this notebook I tried to find the bigwords from the text. and surprisingly I found 12k bigwords. \n- How I decided the bigwords? I Simply find the words which has more than or equal to 10 characters present including '/' & '-'\n\n\n<h2 style=\"color:red;\">Please upvote if you like itðŸ¥ºðŸ¥ºðŸ¥º<h2>\n","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imoprting the dataset \ntrain = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's have a glimpse of the dataset \ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's print the info as well as description of the dataset \ntrain.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe(include='all').T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target","metadata":{}},{"cell_type":"code","source":"# let's check the distribution of the dataset \nfig, ax = plt.subplots(1, 1, figsize=(20, 8))\nsns.distplot(train['target'], bins=7, ax=ax)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"The mean for the distribution is {np.mean(train['target'])}\")\nprint(f\"The standard deviation is {np.std(train['target'])}\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This looks like **Normal Distribution** around -1 and ranging from -3.5 to 1.5 \n\nWith **mean** and std is -0.95 and **std** is 1.033\n","metadata":{}},{"cell_type":"code","source":"# max and minimum value \nprint(f\"max is {max(train['target'])} and minimum is {min(train['target'])}\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## standard_error","metadata":{}},{"cell_type":"code","source":"# let's check the distribution of the dataset \nfig, ax = plt.subplots(1, 1, figsize=(20, 8))\nsns.distplot(train['standard_error'], bins=100, ax=ax)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- It is heavily skewed. \n- I guess people disagreed with the target value \n- or maybe just like sentiment classifier, depending on the circumstances One won't be able to classify the category. ","metadata":{}},{"cell_type":"code","source":"print(f\"The mean for the distribution is {np.mean(train['standard_error'])}\")\nprint(f\"The standard deviation is {np.std(train['standard_error'])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Excerpt \n## Try first wordcloud","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n# Define a function to plot word cloud\ndef plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(40, 30))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \" \".join(train['excerpt'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import package\nfrom wordcloud import WordCloud, STOPWORDS\n# Generate word cloud\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(text)\n# Plot\nplot_cloud(wordcloud)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's divide the dataset by target which has greater value than 0 and lesser value than 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pos = train[train.target>=0]\ntrain_neg = train[train.target<0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos = \" \".join(train_pos['excerpt'])\nneg = \" \".join(train_neg['excerpt'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(pos)\n# Plot\nplot_cloud(wordcloud)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(neg)\n# Plot\nplot_cloud(wordcloud)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's see which are the words bigger \ntext = train.excerpt\ntext = \" \".join(text)\n#text = [len(x) for x in text.split()]\nbigwords = []\ntext = text.split()\n# let's print the word with length more than 10 characters \nfor i, t in enumerate(text):\n    if len(t)>9 and t not in bigwords:\n        bigwords.append(text[i])\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- so my basic understanding was if the text is diffivcult to read i.e because the most of the words hasn't seen by the audience \n- so i tried to get the bigwords which maybe the reason people are having the difficulty \n- let's now will see what this **bigwords causing to the target variables** ","metadata":{}},{"cell_type":"code","source":"len(bigwords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(bigwords[:50])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DAMN! we have 12642 bigwords**\n","metadata":{}},{"cell_type":"code","source":"#cobra':'viper', 'max_speed'\ntrain['bigword?']=0\nl= []\nfor i in range(2834):\n    for word in bigwords:\n        if word in train.loc[i, 'excerpt']:\n            train.loc[i,'bigword?']=1\n            break\n        \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['bigword?'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos = train[train['target']>=0]\nneg = train[train['target']<0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Positive Target value with bigwords ')\n\npos['bigword?'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Negative Target value with bigwords ')\nneg['bigword?'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(20, 8))\n#sns.distplot(pos['bigword?'], ax=ax[0], kde=True, bins=3)\n#sns.distplot(neg['bigword?'], ax=ax[1],kde=True, bins=3)\n#df.letters.value_counts().sort_values().plot(kind = 'barh')\n\npos['bigword?'].value_counts().plot(kind='bar', ax=ax[0])\nneg['bigword?'].value_counts().plot(kind='bar', ax=ax[1])\nax[0].set_title('Positve')\nax[1].set_title('Negative')\n \n\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- So here I got more than 12k bigwords \n- most of the bigwords are coming from the positve side of the targets \n\n- I don't know how will this helpful for anyone but I love to find it. \n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}