{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom transformers import RobertaConfig, RobertaModel, RobertaTokenizer\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, excerpt, tokenizer, max_len, numerical_features, tfidf):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.numerical_features = numerical_features\n        self.tfidf_df = tfidf\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        numerical_features = self.numerical_features[item]\n        tfidf = self.tfidf_df.values[item]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n            \"numerical_features\" : torch.tensor(numerical_features, dtype=torch.float32),\n            \"tfidf\" : torch.tensor(tfidf, dtype=torch.float32),\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\n\nclass RoBERTaLarge(nn.Module):\n    def __init__(self, model_path):\n        super(RoBERTaLarge, self).__init__()\n        self.in_features = 1024\n        self.roberta = RobertaModel.from_pretrained(model_path)\n        self.head = AttentionHead(self.in_features,self.in_features,1)\n        self.dropout = nn.Dropout(0.1)\n        self.process_num = nn.Sequential(\n            nn.Linear(10, 8),\n            nn.BatchNorm1d(8),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.process_tfidf = nn.Sequential(\n            nn.Linear(100, 32),\n            nn.BatchNorm1d(32),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.l0 = nn.Linear(self.in_features + 8 + 32, 1)\n        self.l1 = nn.Linear(self.in_features + 8 + 32, 12)\n\n    def forward(self, ids, mask, numerical_features, tfidf):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask\n        )\n\n        x1 = self.head(roberta_outputs[0]) # bs, 1024\n\n        x2 = self.process_num(numerical_features) # bs, 8\n\n        x3 = self.process_tfidf(tfidf) # bs, 32\n\n        x = torch.cat([x1, x2, x3], 1) # bs, 1024 + 8 + 32\n\n        logits = self.l0(self.dropout(x))\n        aux_logits = torch.sigmoid(self.l1(self.dropout(x)))\n        return logits.squeeze(-1), aux_logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport re\nimport scipy as sp\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.feature_extraction.text import _document_frequency\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n\nclass BM25Transformer(BaseEstimator, TransformerMixin):\n    def __init__(self, use_idf=True, k1=2.0, b=0.75):\n        self.use_idf = use_idf\n        self.k1 = k1\n        self.b = b\n\n    def fit(self, X):\n        if not sp.sparse.issparse(X):\n            X = sp.sparse.csc_matrix(X)\n        if self.use_idf:\n            n_samples, n_features = X.shape\n            df = _document_frequency(X)\n            idf = np.log((n_samples - df + 0.5) / (df + 0.5))\n            self._idf_diag = sp.sparse.spdiags(idf, diags=0, m=n_features, n=n_features)\n\n        doc_len = X.sum(axis=1)\n        self._average_document_len = np.average(doc_len)\n\n        return self\n\n    def transform(self, X, copy=True):\n        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n            X = sp.sparse.csr_matrix(X, copy=copy)\n        else:\n            X = sp.sparse.csr_matrix(X, dtype=np.float, copy=copy)\n\n        n_samples, n_features = X.shape\n        doc_len = X.sum(axis=1)\n        sz = X.indptr[1:] - X.indptr[0:-1]\n        rep = np.repeat(np.asarray(doc_len), sz)\n\n        nom = self.k1 + 1\n        denom = X.data + self.k1 * (1 - self.b + self.b * rep / self._average_document_len)\n        data = X.data * nom / denom\n\n        X = sp.sparse.csr_matrix((data, X.indices, X.indptr), shape=X.shape)\n\n        if self.use_idf:\n            check_is_fitted(self, '_idf_diag', 'idf vector is not fitted')\n\n            expected_n_features = self._idf_diag.shape[0]\n            if n_features != expected_n_features:\n                raise ValueError(\"Input has n_features=%d while the model\"\n                                 \" has been trained with n_features=%d\" % (\n                                     n_features, expected_n_features))\n            X = X * self._idf_diag\n\n        return X \n\n\nclass TextPreprocessor(object):\n    def __init__(self):\n        self.puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n                       '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…',\n                       '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─',\n                       '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n                       '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '（', '）', '～',\n                       '➡', '％', '⇒', '▶', '「', '➄', '➆',  '➊', '➋', '➌', '➍', '⓪', '①', '②', '③', '④', '⑤', '⑰', '❶', '❷', '❸', '❹', '❺', '❻', '❼', '❽',  \n                       '＝', '※', '㈱', '､', '△', '℮', 'ⅼ', '‐', '｣', '┝', '↳', '◉', '／', '＋', '○',\n                       '【', '】', '✅', '☑', '➤', 'ﾞ', '↳', '〶', '☛', '｢', '⁺', '『', '≫',\n                       ]\n\n        self.numbers = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"０\",\"１\",\"２\",\"３\",\"４\",\"５\",\"６\",\"７\",\"８\",\"９\"]\n        self.stopwords = nltk.corpus.stopwords.words('english')\n\n    def _pre_preprocess(self, x):\n        return str(x).lower() \n\n    def rm_num(self, x, use_num=True):\n        x = re.sub('[0-9]{5,}', '', x)\n        x = re.sub('[0-9]{4}', '', x)\n        x = re.sub('[0-9]{3}', '', x)\n        x = re.sub('[0-9]{2}', '', x)    \n        for i in self.numbers:\n            x = x.replace(str(i), '')        \n        return x\n\n    def clean_puncts(self, x):\n        for punct in self.puncts:\n            x = x.replace(punct, '')\n        return x\n    \n    def clean_stopwords(self, x):\n        list_x = x.split()\n        res = []\n        for w in list_x:\n            if w not in self.stopwords:\n                res.append(w)\n        return ' '.join(res)\n\n    def preprocess(self, sentence):\n        sentence = sentence.fillna(\" \")\n        sentence = sentence.map(lambda x: self._pre_preprocess(x))\n        sentence = sentence.map(lambda x: self.clean_puncts(x))\n        sentence = sentence.map(lambda x: self.clean_stopwords(x))\n        sentence = sentence.map(lambda x: self.rm_num(x))\n        return sentence\n\n\ndef get_sentence_features(train, col):\n    train[col + '_num_chars'] = train[col].apply(len)\n    train[col + '_num_capitals'] = train[col].apply(lambda x: sum(1 for c in x if c.isupper()))\n    train[col + '_caps_vs_length'] = train.apply(lambda row: row[col + '_num_chars'] / (row[col + '_num_capitals']+1e-5), axis=1)\n    train[col + '_num_exclamation_marks'] = train[col].apply(lambda x: x.count('!'))\n    train[col + '_num_question_marks'] = train[col].apply(lambda x: x.count('?'))\n    train[col + '_num_punctuation'] = train[col].apply(lambda x: sum(x.count(w) for w in '.,;:'))\n    train[col + '_num_symbols'] = train[col].apply(lambda x: sum(x.count(w) for w in '*&$%'))\n    train[col + '_num_words'] = train[col].apply(lambda x: len(x.split()))\n    train[col + '_num_unique_words'] = train[col].apply(lambda comment: len(set(w for w in comment.split())))\n    train[col + '_words_vs_unique'] = train[col + '_num_unique_words'] / train[col + '_num_words'] \n    return train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_predictions(pretrain_path, max_len):\n    device = \"cuda\"\n    model_path = '../input/robertalarge/'\n    model = RoBERTaLarge(model_path)\n    model.to(device)\n    model.load_state_dict(torch.load(pretrain_path))\n    model.eval()\n    tokenizer = RobertaTokenizer.from_pretrained(model_path)\n    \n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    df = get_sentence_features(df, 'excerpt')\n    \n    train = pd.read_csv(\"../input/step-1-create-folds/train_folds.csv\")\n    train = pd.concat([train, df]).reset_index(drop=True)\n    \n    TP = TextPreprocessor()\n    preprocessed_text = TP.preprocess(train['excerpt'])\n\n    pipeline = make_pipeline(\n                TfidfVectorizer(max_features=100000),\n                make_union(\n                    TruncatedSVD(n_components=50, random_state=42),\n                    make_pipeline(\n                        BM25Transformer(use_idf=True, k1=2.0, b=0.75),\n                        TruncatedSVD(n_components=50, random_state=42)\n                    ),\n                    n_jobs=1,\n                ),\n             )\n\n    z = pipeline.fit_transform(preprocessed_text)\n    tfidf_df = pd.DataFrame(z, columns=[f'cleaned_excerpt_tf_idf_svd_{i}' for i in range(50*2)])\n    \n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len, numerical_features=df[numerical_cols].values, tfidf=tfidf_df)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=0, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n    for b_idx, data in tqdm(enumerate(data_loader)):\n        with torch.no_grad():\n            inputs = data['input_ids'].to(device)\n            masks = data['attention_mask'].to(device)\n            numerical_features = data['numerical_features'].to(device)\n            tfidf = data['tfidf'].to(device)\n            output, _ = model(inputs, masks, numerical_features, tfidf)\n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n    \n    torch.cuda.empty_cache()\n    return np.array(final_output)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_cols = [\n       'excerpt_num_chars', 'excerpt_num_capitals', 'excerpt_caps_vs_length',\n       'excerpt_num_exclamation_marks', 'excerpt_num_question_marks',\n       'excerpt_num_punctuation', 'excerpt_num_symbols', 'excerpt_num_words',\n       'excerpt_num_unique_words', 'excerpt_words_vs_unique'\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://huggingface.co/phiyodr/roberta-large-finetuned-squad2, MSE","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds0 = generate_predictions(\"../input/kaerururu-commonlit-110/fold-0.bin\", max_len=256)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-110/fold-1.bin\", max_len=256)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-110/fold-2.bin\", max_len=256)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-110/fold-3.bin\", max_len=256)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-110/fold-4.bin\", max_len=256)\n\npreds = (preds0 + preds1 + preds2 + preds3 + preds4) / 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmission.target = preds\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}