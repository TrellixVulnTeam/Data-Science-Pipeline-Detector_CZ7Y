{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\nimport re\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nimport logging\nfrom sklearn.metrics import mean_squared_error\nfrom tensorflow.keras import backend as K\nfrom transformers import TFRobertaModel, RobertaConfig, RobertaModel, RobertaTokenizer\nfrom kaggle_datasets import KaggleDatasets\ntf.get_logger().setLevel(logging.ERROR)\n\n\nimport sys\nimport time\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport xgboost as xgb\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport json\nfrom tensorflow.keras.models import load_model\nimport string\nimport keras\nfrom sklearn.svm import SVR\n\nimport pickle\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Input,LSTM,Bidirectional,Embedding,Dense, Conv1D, Dropout , MaxPool1D , MaxPooling1D, GlobalAveragePooling2D , GlobalAveragePooling1D , GlobalMaxPooling1D , concatenate , Flatten\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.models import Model,load_model,save_model , model_from_json\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint, EarlyStopping ,LearningRateScheduler\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\n\nfrom transformers import RobertaConfig, RobertaModel, RobertaTokenizer, AutoConfig, AutoModel, AutoTokenizer, AutoModelForSequenceClassification, TFBertModel, BertTokenizerFast, BertTokenizer, RobertaTokenizerFast, TFRobertaModel, TFAutoModel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 108, gpt2, 0.48\n# 115, roberta-base, 0.478\n# 121, bart-large, 0.474","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, excerpt, tokenizer, max_len):\n        self.excerpt = excerpt\n        tokenizer.add_special_tokens({'pad_token': '0'})\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \n    \nclass CLRPModel(nn.Module):\n    def __init__(self, configuration):\n        super(CLRPModel, self).__init__()\n        self.in_features = 1280\n        self.auto_model = AutoModel.from_config(configuration)\n        self.head = AttentionHead(self.in_features,self.in_features,1)\n        self.dropout = nn.Dropout(0.1)\n        self.l0 = nn.Linear(self.in_features, 1)\n        self.l1 = nn.Linear(self.in_features, 7)\n\n    def forward(self, ids, mask):\n        outputs = self.auto_model(\n            ids,\n            attention_mask=mask\n        )\n\n        x = self.head(outputs[0]) # bs, 1024\n\n        logits = self.l0(self.dropout(x))\n        aux_logits = torch.sigmoid(self.l1(self.dropout(x)))\n        return logits.squeeze(-1), aux_logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_predictions(pretrain_path, max_len):\n    device = \"cuda\"\n    model_path = AutoConfig.from_pretrained('../input/gpt2-pytorch/gpt2-large-config.json')\n    model = CLRPModel(model_path)\n    model.to(device)\n    model.load_state_dict(torch.load(pretrain_path))\n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained('../input/gpt2large345m/gpt2-345M/')\n    \n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n\n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=0, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n    for b_idx, data in tqdm(enumerate(data_loader)):\n        with torch.no_grad():\n            inputs = data['input_ids'].to(device)\n            masks = data['attention_mask'].to(device)\n            output, _ = model(inputs, masks)\n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n    \n    del model, tokenizer, df, dataset, data_loader\n    del inputs, masks, output\n    gc.collect()\n    torch.cuda.empty_cache()\n    return np.array(final_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds0 = generate_predictions(\"../input/kaerururu-commonlit-108/fold-0.bin\", max_len=256)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-108/fold-1.bin\", max_len=256)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-108/fold-2.bin\", max_len=256)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-108/fold-3.bin\", max_len=256)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-108/fold-4.bin\", max_len=256)\n\npreds_gpt2 = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    for gpu in gpus:\n        tf.config.experimental.set_memory_growth(gpu,True)\n        \nmax_len = 250\nbatch_size = 32\nAUTOTUNE = tf.data.AUTOTUNE\n\nMODEL=['bert-base-uncased' , 'roberta-base']\n\nmodel_name = MODEL[1]\n\npath=[\n    \"../input/commonlitreadabilityprize/sample_submission.csv\",\n    \"../input/commonlitreadabilityprize/test.csv\",\n    \"../input/commonlitreadabilityprize/train.csv\"\n]\n\ndf_train = pd.read_csv(path[2])\ndf_test = pd.read_csv(path[1])\ndf_ss = pd.read_csv(path[0])\n                         \ndf_train = df_train.drop(['url_legal','license','standard_error'],axis='columns')\ndf_test = df_test.drop(['url_legal','license'],axis='columns')\nX= df_train['excerpt']\ny=df_train['target'].values\n\nX_test = df_test['excerpt']\n\ntokenizer1 = AutoTokenizer.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\")\n\nprint('tokenization')\ntrain_embeddings = tokenizer1(X.to_list(), truncation = True , padding = 'max_length' , max_length=max_len)\ntest_embeddings = tokenizer1(X_test.to_list() , truncation = True , padding = 'max_length' , max_length = max_len)\n                         \n@tf.function\ndef map_function(encodings):\n    input_ids = encodings['input_ids']\n    \n    return {'input_word_ids': input_ids}\n\nprint(\"generating train and test\")    \ntrain = tf.data.Dataset.from_tensor_slices((train_embeddings))\ntrain = (\n            train\n            .map(map_function, num_parallel_calls=AUTOTUNE)\n            .batch(16)\n            .prefetch(AUTOTUNE)\n        )\n\n\ntest = tf.data.Dataset.from_tensor_slices((test_embeddings))\ntest = (\n        test\n        .map(map_function, num_parallel_calls = AUTOTUNE)\n        .batch(16)\n        .prefetch(AUTOTUNE)\n    )\n                         \n                         \ndef build_roberta_base_model(max_len=max_len ):\n    \n    transformer = TFAutoModel.from_pretrained(\"../input/huggingface-roberta-variants/roberta-base/roberta-base\")\n    \n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    \n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    \n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = output)\n    \n    return model\n                         \nragnar_model = build_roberta_base_model()\ndef feature_extractor(path):\n    print(\"loading weights\")\n    ragnar_model.load_weights(path)\n    x= ragnar_model.layers[-3].output\n    model = Model(inputs = ragnar_model.inputs , outputs = x)\n    return model\n                         \ndef get_preds(model,train,test):\n    print(\"Extracting Features from train data\")\n    train_features = model.predict( train , verbose =1)\n    train_features = train_features.last_hidden_state\n    train_features = train_features[: , 0 , :]\n    print(\"Extracting Features from train data\")\n    test_features = model.predict( test , verbose =1)\n    test_features = test_features.last_hidden_state\n    test_features = test_features[: , 0 , :]\n    \n    return np.array(train_features , dtype= np.float16) , np.array(test_features , dtype= np.float16) \n                         \n#model weight paths\npaths=[\"../input/commonlit-readability-roberta-base/Roberta_Base_123_1.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_2.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_3.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_4.h5\",\n       \"../input/commonlit-readability-roberta-base/Roberta_Base_123_5.h5\"\n      ]\n                         \n#1\nextraction_model = feature_extractor(paths[0])\ntrain_embeddings1 , test_embeddings1 = get_preds(extraction_model , train , test)\n                         \n#2\nextraction_model = feature_extractor(paths[1])\ntrain_embeddings2 , test_embeddings2 = get_preds(extraction_model , train , test)\n                         \n#3\nextraction_model = feature_extractor(paths[2])\ntrain_embeddings3 , test_embeddings3 = get_preds(extraction_model , train , test)\n                         \n#4\nextraction_model = feature_extractor(paths[3])\ntrain_embeddings4 , test_embeddings4 = get_preds(extraction_model , train , test)\n                         \n#5\nextraction_model = feature_extractor(paths[4])\ntrain_embeddings5 , test_embeddings5 = get_preds(extraction_model , train , test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('test_embeddings1_Roberta_Base_123_1_h5.npy', test_embeddings1)\nnp.save('test_embeddings2_Roberta_Base_123_2_h5.npy', test_embeddings2)\nnp.save('test_embeddings3_Roberta_Base_123_3_h5.npy', test_embeddings3)\nnp.save('test_embeddings4_Roberta_Base_123_4_h5.npy', test_embeddings4)\nnp.save('test_embeddings5_Roberta_Base_123_5_h5.npy', test_embeddings5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del tokenizer1, train_embeddings, test_embeddings;gc.collect()\ndel extraction_model, train_embeddings1, test_embeddings1, train_embeddings2, test_embeddings2, train_embeddings3, test_embeddings3, train_embeddings4, test_embeddings4, train_embeddings5, test_embeddings5;gc.collect()\n# del preds,preds1,preds2,preds3,preds4,preds5;gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding wordcloud in offline mode\nimport sys\nsys.path.append('../input/sentence-transformers/sentence-transformers-master')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import Ridge\nfrom scipy import stats\nfrom scipy.stats import norm\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# loding train and test data\n\ntrain_df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n\n# dropping some columns\n\ntrain_df=train_df[['id','excerpt','target','standard_error']]\n\n# creating corpus\n\nfrom nltk import FreqDist\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words('english'))\n\nimport sentence_transformers\nfrom sentence_transformers import SentenceTransformer, models\n\nmodel_path = '../input/finetuned-model1/checkpoint-568'\nword_embedding_model = models.Transformer(model_path, max_seq_length=275)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\nX_train = model.encode(train_df.excerpt, device='cuda')\nX_test = model.encode(test_df.excerpt, device='cuda')\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\n\ntrain_df['Character Count'] = train_df['excerpt'].apply(lambda x: len(str(x)))\npreds = []\n\n\ndf_oof=train_df.copy()\ndf_oof['oof'] = 0\n\n\nskf = StratifiedKFold(10, shuffle=True, random_state=42)\n\nsplits = list(skf.split(X=X_train, y=train_df['Character Count']))\nfor i, (train_idx, val_idx) in enumerate(splits):\n    print(f'\\n------------- Training Fold {i + 1} / {10}')\n    print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=30, random_state=43)\n    ridge = Ridge(alpha=50.0)\n    br = BayesianRidge(n_iter=30, verbose=True)\n\n    clf =   BayesianRidge(n_iter=30, verbose=True) #VotingRegressor([('r2', r2), ('br', br)])\n    clf.fit(X_train[train_idx],train_df.target[train_idx])\n    \n    preds.append(clf.predict(X_test))\n    x=clf.predict(X_train[val_idx])\n    df_oof['oof'].iloc[val_idx]+= x\n\nprint(f'Training score: {mean_squared_error(train_df.target, clf.predict(X_train), squared=False)}')\nprint(f'OOF score across folds: {mean_squared_error(df_oof.target, df_oof.oof, squared=False)}')\n\n# getting mean prediction across 5 folds\n# y_pred = np.mean(preds,0)\npreds_sentence_transformer = np.mean(preds,0)\n\n# creating submission csv\n\n# mysub = test_df[[\"id\"]].copy()\n# mysub[\"target\"] = y_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\n\nconfig = {\n    'batch_size':128,\n    'max_len':256,\n    'nfolds':5,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\n\nclass CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \n    \nclass Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained('../input/roberta-base')    \n        self.head = AttentionHead(768,768,1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.head.out_features,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        return x\n    \n    \ndef get_embeddings(df,path,plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    model = Model()\n    model.load_state_dict(torch.load(path))\n    model.to(device)\n    model.eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained('../input/roberta-base')\n    \n    ds = CLRPDataset(df,tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs.detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_embeddings1 = get_embeddings(test_data,'../input/clr-roberta/model0/model0.bin')\ntest_embeddings2 = get_embeddings(test_data,'../input/clr-roberta/model1/model1.bin')\ntest_embeddings3 = get_embeddings(test_data,'../input/clr-roberta/model2/model2.bin')\ntest_embeddings4 = get_embeddings(test_data,'../input/clr-roberta/model3/model3.bin')\ntest_embeddings5 = get_embeddings(test_data,'../input/clr-roberta/model4/model4.bin')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('test_embeddings1_clr_roberta_model0.npy', test_embeddings1)\nnp.save('test_embeddings2_clr_roberta_model1.npy', test_embeddings2)\nnp.save('test_embeddings3_clr_roberta_model2.npy', test_embeddings3)\nnp.save('test_embeddings4_clr_roberta_model3.npy', test_embeddings4)\nnp.save('test_embeddings5_clr_roberta_model4.npy', test_embeddings5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del test_embeddings1, test_embeddings2, test_embeddings3, test_embeddings4, test_embeddings5;gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, excerpt, tokenizer, max_len, numerical_features, tfidf):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.numerical_features = numerical_features\n        self.tfidf_df = tfidf\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        numerical_features = self.numerical_features[item]\n        tfidf = self.tfidf_df.values[item]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n            \"numerical_features\" : torch.tensor(numerical_features, dtype=torch.float32),\n            \"tfidf\" : torch.tensor(tfidf, dtype=torch.float32),\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoBERTaLarge(nn.Module):\n    def __init__(self, model_path):\n        super(RoBERTaLarge, self).__init__()\n        self.in_features = 768 # 1024\n        self.roberta = RobertaModel.from_pretrained(model_path)\n        self.head = AttentionHead(self.in_features,self.in_features,1)\n        self.dropout = nn.Dropout(0.1)\n        self.process_num = nn.Sequential(\n            nn.Linear(10, 8),\n            nn.BatchNorm1d(8),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.process_tfidf = nn.Sequential(\n            nn.Linear(100, 32),\n            nn.BatchNorm1d(32),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.l0 = nn.Linear(self.in_features + 8 + 32, 1)\n        self.l1 = nn.Linear(self.in_features + 8 + 32, 12)\n\n    def forward(self, ids, mask, numerical_features, tfidf):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask\n        )\n\n        x1 = self.head(roberta_outputs[0]) # bs, 1024\n\n        x2 = self.process_num(numerical_features) # bs, 8\n\n        x3 = self.process_tfidf(tfidf) # bs, 32\n\n        x = torch.cat([x1, x2, x3], 1) # bs, 1024 + 8 + 32\n\n        logits = self.l0(self.dropout(x))\n        aux_logits = torch.sigmoid(self.l1(self.dropout(x)))\n        return logits.squeeze(-1), aux_logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport re\nimport scipy as sp\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.feature_extraction.text import _document_frequency\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\n\nclass BM25Transformer(BaseEstimator, TransformerMixin):\n    def __init__(self, use_idf=True, k1=2.0, b=0.75):\n        self.use_idf = use_idf\n        self.k1 = k1\n        self.b = b\n\n    def fit(self, X):\n        if not sp.sparse.issparse(X):\n            X = sp.sparse.csc_matrix(X)\n        if self.use_idf:\n            n_samples, n_features = X.shape\n            df = _document_frequency(X)\n            idf = np.log((n_samples - df + 0.5) / (df + 0.5))\n            self._idf_diag = sp.sparse.spdiags(idf, diags=0, m=n_features, n=n_features)\n\n        doc_len = X.sum(axis=1)\n        self._average_document_len = np.average(doc_len)\n\n        return self\n\n    def transform(self, X, copy=True):\n        if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n            X = sp.sparse.csr_matrix(X, copy=copy)\n        else:\n            X = sp.sparse.csr_matrix(X, dtype=np.float, copy=copy)\n\n        n_samples, n_features = X.shape\n        doc_len = X.sum(axis=1)\n        sz = X.indptr[1:] - X.indptr[0:-1]\n        rep = np.repeat(np.asarray(doc_len), sz)\n\n        nom = self.k1 + 1\n        denom = X.data + self.k1 * (1 - self.b + self.b * rep / self._average_document_len)\n        data = X.data * nom / denom\n\n        X = sp.sparse.csr_matrix((data, X.indices, X.indptr), shape=X.shape)\n\n        if self.use_idf:\n            check_is_fitted(self, '_idf_diag', 'idf vector is not fitted')\n\n            expected_n_features = self._idf_diag.shape[0]\n            if n_features != expected_n_features:\n                raise ValueError(\"Input has n_features=%d while the model\"\n                                 \" has been trained with n_features=%d\" % (\n                                     n_features, expected_n_features))\n            X = X * self._idf_diag\n\n        return X \n\n\nclass TextPreprocessor(object):\n    def __init__(self):\n        self.puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n                       '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…',\n                       '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─',\n                       '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '«',\n                       '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', '（', '）', '～',\n                       '➡', '％', '⇒', '▶', '「', '➄', '➆',  '➊', '➋', '➌', '➍', '⓪', '①', '②', '③', '④', '⑤', '⑰', '❶', '❷', '❸', '❹', '❺', '❻', '❼', '❽',  \n                       '＝', '※', '㈱', '､', '△', '℮', 'ⅼ', '‐', '｣', '┝', '↳', '◉', '／', '＋', '○',\n                       '【', '】', '✅', '☑', '➤', 'ﾞ', '↳', '〶', '☛', '｢', '⁺', '『', '≫',\n                       ]\n\n        self.numbers = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"０\",\"１\",\"２\",\"３\",\"４\",\"５\",\"６\",\"７\",\"８\",\"９\"]\n        self.stopwords = nltk.corpus.stopwords.words('english')\n\n    def _pre_preprocess(self, x):\n        return str(x).lower() \n\n    def rm_num(self, x, use_num=True):\n        x = re.sub('[0-9]{5,}', '', x)\n        x = re.sub('[0-9]{4}', '', x)\n        x = re.sub('[0-9]{3}', '', x)\n        x = re.sub('[0-9]{2}', '', x)    \n        for i in self.numbers:\n            x = x.replace(str(i), '')        \n        return x\n\n    def clean_puncts(self, x):\n        for punct in self.puncts:\n            x = x.replace(punct, '')\n        return x\n    \n    def clean_stopwords(self, x):\n        list_x = x.split()\n        res = []\n        for w in list_x:\n            if w not in self.stopwords:\n                res.append(w)\n        return ' '.join(res)\n\n    def preprocess(self, sentence):\n        sentence = sentence.fillna(\" \")\n        sentence = sentence.map(lambda x: self._pre_preprocess(x))\n        sentence = sentence.map(lambda x: self.clean_puncts(x))\n        sentence = sentence.map(lambda x: self.clean_stopwords(x))\n        sentence = sentence.map(lambda x: self.rm_num(x))\n        return sentence\n\n\ndef get_sentence_features(train, col):\n    train[col + '_num_chars'] = train[col].apply(len)\n    train[col + '_num_capitals'] = train[col].apply(lambda x: sum(1 for c in x if c.isupper()))\n    train[col + '_caps_vs_length'] = train.apply(lambda row: row[col + '_num_chars'] / (row[col + '_num_capitals']+1e-5), axis=1)\n    train[col + '_num_exclamation_marks'] = train[col].apply(lambda x: x.count('!'))\n    train[col + '_num_question_marks'] = train[col].apply(lambda x: x.count('?'))\n    train[col + '_num_punctuation'] = train[col].apply(lambda x: sum(x.count(w) for w in '.,;:'))\n    train[col + '_num_symbols'] = train[col].apply(lambda x: sum(x.count(w) for w in '*&$%'))\n    train[col + '_num_words'] = train[col].apply(lambda x: len(x.split()))\n    train[col + '_num_unique_words'] = train[col].apply(lambda comment: len(set(w for w in comment.split())))\n    train[col + '_words_vs_unique'] = train[col + '_num_unique_words'] / train[col + '_num_words'] \n    return train\n\n\nnumerical_cols = [\n       'excerpt_num_chars', 'excerpt_num_capitals', 'excerpt_caps_vs_length',\n       'excerpt_num_exclamation_marks', 'excerpt_num_question_marks',\n       'excerpt_num_punctuation', 'excerpt_num_symbols', 'excerpt_num_words',\n       'excerpt_num_unique_words', 'excerpt_words_vs_unique'\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_predictions(pretrain_path, max_len):\n    device = \"cuda\"\n    model_path = '../input/roberta-base/'\n    model = RoBERTaLarge(model_path)\n    model.to(device)\n    model.load_state_dict(torch.load(pretrain_path))\n    model.eval()\n    tokenizer = RobertaTokenizer.from_pretrained(model_path)\n    \n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    df = get_sentence_features(df, 'excerpt')\n    \n    train = pd.read_csv(\"../input/step-1-create-folds/train_folds.csv\")\n    train = pd.concat([train, df]).reset_index(drop=True)\n    \n    TP = TextPreprocessor()\n    preprocessed_text = TP.preprocess(train['excerpt'])\n\n    pipeline = make_pipeline(\n                TfidfVectorizer(max_features=100000),\n                make_union(\n                    TruncatedSVD(n_components=50, random_state=42),\n                    make_pipeline(\n                        BM25Transformer(use_idf=True, k1=2.0, b=0.75),\n                        TruncatedSVD(n_components=50, random_state=42)\n                    ),\n                    n_jobs=1,\n                ),\n             )\n\n    z = pipeline.fit_transform(preprocessed_text)\n    tfidf_df = pd.DataFrame(z, columns=[f'cleaned_excerpt_tf_idf_svd_{i}' for i in range(50*2)])\n    \n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len, numerical_features=df[numerical_cols].values, tfidf=tfidf_df)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=0, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n    for b_idx, data in tqdm(enumerate(data_loader)):\n        with torch.no_grad():\n            inputs = data['input_ids'].to(device)\n            masks = data['attention_mask'].to(device)\n            numerical_features = data['numerical_features'].to(device)\n            tfidf = data['tfidf'].to(device)\n            output, _ = model(inputs, masks, numerical_features, tfidf)\n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n\n    del model, tokenizer, train, df, preprocessed_text, pipeline, z, tfidf_df, dataset, data_loader\n    del inputs, masks, numerical_features, tfidf, output\n    gc.collect()\n    torch.cuda.empty_cache()\n    return np.array(final_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds0 = generate_predictions(\"../input/kaerururu-commonlit-115/fold-0.bin\", max_len=256)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-115/fold-1.bin\", max_len=256)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-115/fold-2.bin\", max_len=256)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-115/fold-3.bin\", max_len=256)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-115/fold-4.bin\", max_len=256)\n\npreds_roberta_base = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoBERTaLarge(nn.Module):\n    def __init__(self, model_path):\n        super(RoBERTaLarge, self).__init__()\n        self.in_features = 1024\n        self.roberta = RobertaModel.from_pretrained(model_path)\n        self.head = AttentionHead(self.in_features,self.in_features,1)\n        self.dropout = nn.Dropout(0.1)\n        self.process_num = nn.Sequential(\n            nn.Linear(10, 8),\n            nn.BatchNorm1d(8),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.process_tfidf = nn.Sequential(\n            nn.Linear(100, 32),\n            nn.BatchNorm1d(32),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.l0 = nn.Linear(self.in_features + 8 + 32, 1)\n        self.l1 = nn.Linear(self.in_features + 8 + 32, 7)\n\n    def forward(self, ids, mask, numerical_features, tfidf):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask\n        )\n\n        x1 = self.head(roberta_outputs[0]) # bs, 1024\n\n        x2 = self.process_num(numerical_features) # bs, 8\n\n        x3 = self.process_tfidf(tfidf) # bs, 32\n\n        x = torch.cat([x1, x2, x3], 1) # bs, 1024 + 8 + 32\n\n        logits = self.l0(self.dropout(x))\n        aux_logits = torch.sigmoid(self.l1(self.dropout(x)))\n        return logits.squeeze(-1), aux_logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_predictions(pretrain_path, max_len):\n    device = \"cuda\"\n    model_path = '../input/robertalarge/'\n    model = RoBERTaLarge(model_path)\n    model.to(device)\n    model.load_state_dict(torch.load(pretrain_path))\n    model.eval()\n    tokenizer = RobertaTokenizer.from_pretrained(model_path)\n    \n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    df = get_sentence_features(df, 'excerpt')\n    \n    train = pd.read_csv(\"../input/step-1-create-folds/train_folds.csv\")\n    train = pd.concat([train, df]).reset_index(drop=True)\n    \n    TP = TextPreprocessor()\n    preprocessed_text = TP.preprocess(train['excerpt'])\n\n    pipeline = make_pipeline(\n                TfidfVectorizer(max_features=100000),\n                make_union(\n                    TruncatedSVD(n_components=50, random_state=42),\n                    make_pipeline(\n                        BM25Transformer(use_idf=True, k1=2.0, b=0.75),\n                        TruncatedSVD(n_components=50, random_state=42)\n                    ),\n                    n_jobs=1,\n                ),\n             )\n\n    z = pipeline.fit_transform(preprocessed_text)\n    tfidf_df = pd.DataFrame(z, columns=[f'cleaned_excerpt_tf_idf_svd_{i}' for i in range(50*2)])\n    \n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len, numerical_features=df[numerical_cols].values, tfidf=tfidf_df)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=0, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n    for b_idx, data in tqdm(enumerate(data_loader)):\n        with torch.no_grad():\n            inputs = data['input_ids'].to(device)\n            masks = data['attention_mask'].to(device)\n            numerical_features = data['numerical_features'].to(device)\n            tfidf = data['tfidf'].to(device)\n            output, _ = model(inputs, masks, numerical_features, tfidf)\n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n\n    del model, tokenizer, train, df, preprocessed_text, pipeline, z, tfidf_df, dataset, data_loader\n    del inputs, masks, numerical_features, tfidf, output\n    gc.collect()\n    torch.cuda.empty_cache()\n    return np.array(final_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://huggingface.co/deepset/roberta-large-squad2\npreds0 = generate_predictions(\"../input/kaerururu-commonlit-084/fold-0.bin\", max_len=256)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-084/fold-1.bin\", max_len=256)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-084/fold-2.bin\", max_len=256)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-084/fold-3.bin\", max_len=256)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-084/fold-4.bin\", max_len=256)\n\npreds0462 = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://huggingface.co/phiyodr/roberta-large-finetuned-squad2\npreds0 = generate_predictions(\"../input/kaerururu-commonlit-086/fold-0.bin\", max_len=256)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-086/fold-1.bin\", max_len=256)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-086/fold-2.bin\", max_len=256)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-086/fold-3.bin\", max_len=256)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-086/fold-4.bin\", max_len=256)\n\npreds0461 = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://huggingface.co/tli8hf/unqover-roberta-large-newsqa\npreds0 = generate_predictions(\"../input/kaerururu-commonlit-088/fold-0.bin\", max_len=256)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-088/fold-1.bin\", max_len=256)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-088/fold-2.bin\", max_len=256)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-088/fold-3.bin\", max_len=256)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-088/fold-4.bin\", max_len=256)\n\npreds0463 = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://huggingface.co/phiyodr/roberta-large-finetuned-squad2, MSE ver\npreds0 = generate_predictions(\"../input/kaerururu-commonlit-095/fold-0.bin\", max_len=256)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-095/fold-1.bin\", max_len=256)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-095/fold-2.bin\", max_len=256)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-095/fold-3.bin\", max_len=256)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-095/fold-4.bin\", max_len=256)\n\npreds0459 = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# max_len 512, cv score 0.493933596032441 <-- 0.499383855892775 (076)\n\npreds0 = generate_predictions(\"../input/kaerururu-commonlit-098/fold-0.bin\", max_len=512)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-098/fold-1.bin\", max_len=512)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-098/fold-2.bin\", max_len=512)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-098/fold-3.bin\", max_len=512)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-098/fold-4.bin\", max_len=512)\n\npreds_max_len_512 = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0457","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoBERTaLarge(nn.Module):\n    def __init__(self, model_path):\n        super(RoBERTaLarge, self).__init__()\n        self.in_features = 1024\n        self.roberta = RobertaModel.from_pretrained(model_path)\n        self.head = AttentionHead(self.in_features,self.in_features,1)\n        self.dropout = nn.Dropout(0.1)\n        self.process_num = nn.Sequential(\n            nn.Linear(10, 8),\n            nn.BatchNorm1d(8),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.process_tfidf = nn.Sequential(\n            nn.Linear(100, 32),\n            nn.BatchNorm1d(32),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.l0 = nn.Linear(self.in_features + 8 + 32, 1)\n        self.l1 = nn.Linear(self.in_features + 8 + 32, 12)\n\n    def forward(self, ids, mask, numerical_features, tfidf):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask\n        )\n\n        x1 = self.head(roberta_outputs[0]) # bs, 1024\n\n        x2 = self.process_num(numerical_features) # bs, 8\n\n        x3 = self.process_tfidf(tfidf) # bs, 32\n\n        x = torch.cat([x1, x2, x3], 1) # bs, 1024 + 8 + 32\n\n        logits = self.l0(self.dropout(x))\n        aux_logits = torch.sigmoid(self.l1(self.dropout(x)))\n        return logits.squeeze(-1), aux_logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds0 = generate_predictions(\"../input/kaerururu-commonlit-110/fold-0.bin\", max_len=256)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-110/fold-1.bin\", max_len=256)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-110/fold-2.bin\", max_len=256)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-110/fold-3.bin\", max_len=256)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-110/fold-4.bin\", max_len=256)\n\npreds0457 = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add conv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoBERTaLarge(nn.Module):\n    def __init__(self, model_path):\n        super(RoBERTaLarge, self).__init__()\n        self.in_features = 1024\n        self.roberta = RobertaModel.from_pretrained(model_path)\n\n        lstm_hidden_size = 128 * 2\n        gru_hidden_size = 128 * 2\n        n_channels = 64 * 2\n        self.embedding_dropout = nn.Dropout2d(0.2)\n        self.lstm = nn.LSTM(256, lstm_hidden_size, bidirectional=True, batch_first=True)\n        self.gru = nn.GRU(lstm_hidden_size * 2, gru_hidden_size, bidirectional=True, batch_first=True)\n        self.conv = nn.Conv1d(gru_hidden_size * 2, n_channels, 3, padding=2)\n        nn.init.xavier_uniform_(self.conv.weight)\n\n        self.head = AttentionHead(self.in_features,self.in_features,1)\n        self.dropout = nn.Dropout(0.1)\n        self.process_num = nn.Sequential(\n            nn.Linear(10, 8),\n            nn.BatchNorm1d(8),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.process_tfidf = nn.Sequential(\n            nn.Linear(100, 32),\n            nn.BatchNorm1d(32),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.l0 = nn.Linear(self.in_features + 8 + 32 + 128 * 2, 1)\n        self.l1 = nn.Linear(self.in_features + 8 + 32 + 128 * 2, 7)\n\n    def apply_spatial_dropout(self, h_embedding):\n        h_embedding = h_embedding.transpose(1, 2).unsqueeze(2)\n        h_embedding = self.embedding_dropout(h_embedding).squeeze(2).transpose(1, 2)\n        return h_embedding\n\n    def forward(self, ids, mask, numerical_features, tfidf):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask\n        )\n\n        h_embedding = self.apply_spatial_dropout(roberta_outputs[0]).transpose(2, 1) # bs, 1024, 256\n        h_lstm, _ = self.lstm(h_embedding)\n        h_gru, _ = self.gru(h_lstm)\n        h_gru = h_gru.transpose(2, 1)\n        conv = self.conv(h_gru) # bs, 128, 258\n        conv_avg_pool = torch.mean(conv, 2) # bs, 128\n        conv_max_pool, _ = torch.max(conv, 2) # bs, 128\n\n        x1 = self.head(roberta_outputs[0]) # bs, 1024\n\n        x2 = self.process_num(numerical_features) # bs, 8\n\n        x3 = self.process_tfidf(tfidf) # bs, 32\n\n        x = torch.cat([x1, x2, x3, conv_avg_pool, conv_max_pool], 1) # bs, 1024 + 8 + 32 + 128 * 2\n\n        logits = self.l0(self.dropout(x))\n        aux_logits = torch.sigmoid(self.l1(self.dropout(x)))\n        return logits.squeeze(-1), aux_logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds0 = generate_predictions(\"../input/kaerururu-commonlit-074/fold-0.bin\", max_len=256)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-074/fold-1.bin\", max_len=256)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-074/fold-2.bin\", max_len=256)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-074/fold-3.bin\", max_len=256)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-074/fold-4.bin\", max_len=256)\n\npreds_conv_roberta = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# electra-large","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoBERTaLarge(nn.Module):\n    def __init__(self, model_path):\n        super(RoBERTaLarge, self).__init__()\n        self.in_features = 1024\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.head = AttentionHead(self.in_features,self.in_features,1)\n        self.dropout = nn.Dropout(0.1)\n        self.process_num = nn.Sequential(\n            nn.Linear(10, 8),\n            nn.BatchNorm1d(8),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.process_tfidf = nn.Sequential(\n            nn.Linear(100, 32),\n            nn.BatchNorm1d(32),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.l0 = nn.Linear(self.in_features + 8 + 32, 1)\n        self.l1 = nn.Linear(self.in_features + 8 + 32, 12)\n\n    def forward(self, ids, mask, numerical_features, tfidf):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask\n        )\n\n        x1 = self.head(roberta_outputs[0]) # bs, 1024\n\n        x2 = self.process_num(numerical_features) # bs, 8\n\n        x3 = self.process_tfidf(tfidf) # bs, 32\n\n        x = torch.cat([x1, x2, x3], 1) # bs, 1024 + 8 + 32\n\n        logits = self.l0(self.dropout(x))\n        aux_logits = torch.sigmoid(self.l1(self.dropout(x)))\n        return logits.squeeze(-1), x1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_predictions(pretrain_path, max_len):\n    device = \"cuda\"\n    model_path = '../input/electra/large-discriminator/'\n    model = RoBERTaLarge(model_path)\n    model.to(device)\n    model.load_state_dict(torch.load(pretrain_path))\n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    df = get_sentence_features(df, 'excerpt')\n    \n    train = pd.read_csv(\"../input/step-1-create-folds/train_folds.csv\")\n    train = pd.concat([train, df]).reset_index(drop=True)\n    \n    TP = TextPreprocessor()\n    preprocessed_text = TP.preprocess(train['excerpt'])\n\n    pipeline = make_pipeline(\n                TfidfVectorizer(max_features=100000),\n                make_union(\n                    TruncatedSVD(n_components=50, random_state=42),\n                    make_pipeline(\n                        BM25Transformer(use_idf=True, k1=2.0, b=0.75),\n                        TruncatedSVD(n_components=50, random_state=42)\n                    ),\n                    n_jobs=1,\n                ),\n             )\n\n    z = pipeline.fit_transform(preprocessed_text)\n    tfidf_df = pd.DataFrame(z, columns=[f'cleaned_excerpt_tf_idf_svd_{i}' for i in range(50*2)])\n    \n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len, numerical_features=df[numerical_cols].values, tfidf=tfidf_df)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=0, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n    emb = []\n    for b_idx, data in tqdm(enumerate(data_loader)):\n        with torch.no_grad():\n            inputs = data['input_ids'].to(device)\n            masks = data['attention_mask'].to(device)\n            numerical_features = data['numerical_features'].to(device)\n            tfidf = data['tfidf'].to(device)\n            output, emb_out = model(inputs, masks, numerical_features, tfidf)\n            output = output.detach().cpu().numpy().tolist()\n            emb_out = emb_out.detach().cpu().numpy()\n            final_output.extend(output)\n            emb.append(emb_out)\n\n    del model, tokenizer, train, df, preprocessed_text, pipeline, z, tfidf_df, dataset, data_loader\n    del inputs, masks, numerical_features, tfidf, output, emb_out\n    gc.collect()\n    torch.cuda.empty_cache()\n    return np.array(final_output), np.concatenate(emb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds0, emb_preds0 = generate_predictions(\"../input/kaeruru-commonlit-129/fold-0.bin\", max_len=256)\npreds1, emb_preds1 = generate_predictions(\"../input/kaeruru-commonlit-129/fold-1.bin\", max_len=256)\npreds2, emb_preds2 = generate_predictions(\"../input/kaeruru-commonlit-129/fold-2.bin\", max_len=256)\npreds3, emb_preds3 = generate_predictions(\"../input/kaeruru-commonlit-129/fold-3.bin\", max_len=256)\npreds4, emb_preds4 = generate_predictions(\"../input/kaeruru-commonlit-129/fold-4.bin\", max_len=256)\n\npreds_electra_large = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\nemb_preds_electra_large = (emb_preds0 + emb_preds1 + emb_preds2 + emb_preds3 + emb_preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()\ndel emb_preds0, emb_preds1, emb_preds2, emb_preds3, emb_preds4; gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# distil-roberta-base","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoBERTaLarge(nn.Module):\n    def __init__(self, model_path):\n        super(RoBERTaLarge, self).__init__()\n        self.in_features = 768 # 1024\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.head = AttentionHead(self.in_features,self.in_features,1)\n        self.dropout = nn.Dropout(0.1)\n        self.process_num = nn.Sequential(\n            nn.Linear(10, 8),\n            nn.BatchNorm1d(8),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.process_tfidf = nn.Sequential(\n            nn.Linear(100, 32),\n            nn.BatchNorm1d(32),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.l0 = nn.Linear(self.in_features + 8 + 32, 2)\n        self.l1 = nn.Linear(self.in_features + 8 + 32, 12)\n\n    def forward(self, ids, mask, numerical_features, tfidf):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask\n        )\n\n        x1 = self.head(roberta_outputs[0]) # bs, 1024\n\n        x2 = self.process_num(numerical_features) # bs, 8\n\n        x3 = self.process_tfidf(tfidf) # bs, 32\n\n        x = torch.cat([x1, x2, x3], 1) # bs, 1024 + 8 + 32\n\n        logits = self.l0(self.dropout(x))\n        aux_logits = torch.sigmoid(self.l1(self.dropout(x)))\n\n        return logits[:, 0].squeeze(-1), x1\n    \n    \ndef generate_predictions(pretrain_path, max_len):\n    device = \"cuda\"\n    model_path = '../input/distil-roberta-base/'\n    model = RoBERTaLarge(model_path)\n    model.to(device)\n    model.load_state_dict(torch.load(pretrain_path))\n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    df = get_sentence_features(df, 'excerpt')\n    \n    train = pd.read_csv(\"../input/step-1-create-folds/train_folds.csv\")\n    train = pd.concat([train, df]).reset_index(drop=True)\n    \n    TP = TextPreprocessor()\n    preprocessed_text = TP.preprocess(train['excerpt'])\n\n    pipeline = make_pipeline(\n                TfidfVectorizer(max_features=100000),\n                make_union(\n                    TruncatedSVD(n_components=50, random_state=42),\n                    make_pipeline(\n                        BM25Transformer(use_idf=True, k1=2.0, b=0.75),\n                        TruncatedSVD(n_components=50, random_state=42)\n                    ),\n                    n_jobs=1,\n                ),\n             )\n\n    z = pipeline.fit_transform(preprocessed_text)\n    tfidf_df = pd.DataFrame(z, columns=[f'cleaned_excerpt_tf_idf_svd_{i}' for i in range(50*2)])\n    \n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len, numerical_features=df[numerical_cols].values, tfidf=tfidf_df)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=0, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n    emb = []\n    for b_idx, data in tqdm(enumerate(data_loader)):\n        with torch.no_grad():\n            inputs = data['input_ids'].to(device)\n            masks = data['attention_mask'].to(device)\n            numerical_features = data['numerical_features'].to(device)\n            tfidf = data['tfidf'].to(device)\n            output, emb_out = model(inputs, masks, numerical_features, tfidf)\n            output = output.detach().cpu().numpy().tolist()\n            emb_out = emb_out.detach().cpu().numpy()\n            final_output.extend(output)\n            emb.append(emb_out)\n\n    del model, tokenizer, train, df, preprocessed_text, pipeline, z, tfidf_df, dataset, data_loader\n    del inputs, masks, numerical_features, tfidf, output, emb_out\n    gc.collect()\n    torch.cuda.empty_cache()\n    return np.array(final_output), np.concatenate(emb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds0, emb_preds0 = generate_predictions(\"../input/kaerururu-commonlit-125/fold-0.bin\", max_len=248)\npreds1, emb_preds1 = generate_predictions(\"../input/kaerururu-commonlit-125/fold-1.bin\", max_len=248)\npreds2, emb_preds2 = generate_predictions(\"../input/kaerururu-commonlit-125/fold-2.bin\", max_len=248)\npreds3, emb_preds3 = generate_predictions(\"../input/kaerururu-commonlit-125/fold-3.bin\", max_len=248)\npreds4, emb_preds4 = generate_predictions(\"../input/kaerururu-commonlit-125/fold-4.bin\", max_len=248)\n\npreds_distil_roberta_base = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\nemb_preds_distil_roberta_base = (emb_preds0 + emb_preds1 + emb_preds2 + emb_preds3 + emb_preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()\ndel emb_preds0, emb_preds1, emb_preds2, emb_preds3, emb_preds4; gc.collect()\n\nprint(preds_distil_roberta_base.shape, emb_preds_distil_roberta_base.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# meanpooling\n\n\nclass RoBERTaLarge(nn.Module):\n    def __init__(self, model_path):\n        super(RoBERTaLarge, self).__init__()\n        self.in_features = 1024\n        self.roberta = RobertaModel.from_pretrained(model_path)\n        self.layer_norm = nn.LayerNorm(self.in_features)\n        self.dropout = nn.Dropout(0.1)\n        self.process_num = nn.Sequential(\n            nn.Linear(10, 8),\n            nn.BatchNorm1d(8),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.process_tfidf = nn.Sequential(\n            nn.Linear(100, 32),\n            nn.BatchNorm1d(32),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.l0 = nn.Linear(self.in_features + 8 + 32, 1)\n        self.l1 = nn.Linear(self.in_features + 8 + 32, 12)\n\n    def forward(self, ids, mask, numerical_features, tfidf):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask\n        )\n\n        last_hidden_state = roberta_outputs[0]\n        input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        norm_mean_embeddings = self.layer_norm(mean_embeddings) # bs, 1024\n\n        x2 = self.process_num(numerical_features) # bs, 8\n\n        x3 = self.process_tfidf(tfidf) # bs, 32\n\n        x = torch.cat([norm_mean_embeddings, x2, x3], 1) # bs, 1024 + 8 + 32\n\n        logits = self.l0(self.dropout(x))\n        aux_logits = torch.sigmoid(self.l1(self.dropout(x)))\n        return logits.squeeze(-1), aux_logits\n\n    \ndef generate_predictions(pretrain_path, max_len):\n    device = \"cuda\"\n    model_path = '../input/robertalarge/'\n    model = RoBERTaLarge(model_path)\n    model.to(device)\n    model.load_state_dict(torch.load(pretrain_path))\n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    df = get_sentence_features(df, 'excerpt')\n    \n    train = pd.read_csv(\"../input/step-1-create-folds/train_folds.csv\")\n    train = pd.concat([train, df]).reset_index(drop=True)\n    \n    TP = TextPreprocessor()\n    preprocessed_text = TP.preprocess(train['excerpt'])\n\n    pipeline = make_pipeline(\n                TfidfVectorizer(max_features=100000),\n                make_union(\n                    TruncatedSVD(n_components=50, random_state=42),\n                    make_pipeline(\n                        BM25Transformer(use_idf=True, k1=2.0, b=0.75),\n                        TruncatedSVD(n_components=50, random_state=42)\n                    ),\n                    n_jobs=1,\n                ),\n             )\n\n    z = pipeline.fit_transform(preprocessed_text)\n    tfidf_df = pd.DataFrame(z, columns=[f'cleaned_excerpt_tf_idf_svd_{i}' for i in range(50*2)])\n    \n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len, numerical_features=df[numerical_cols].values, tfidf=tfidf_df)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=0, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n    for b_idx, data in tqdm(enumerate(data_loader)):\n        with torch.no_grad():\n            inputs = data['input_ids'].to(device)\n            masks = data['attention_mask'].to(device)\n            numerical_features = data['numerical_features'].to(device)\n            tfidf = data['tfidf'].to(device)\n            output, _ = model(inputs, masks, numerical_features, tfidf)\n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n\n    del model, tokenizer, train, df, preprocessed_text, pipeline, z, tfidf_df, dataset, data_loader\n    del inputs, masks, numerical_features, tfidf, output\n    gc.collect()\n    torch.cuda.empty_cache()\n    return np.array(final_output)\n\n\npreds0 = generate_predictions(\"../input/kaerururu-commonlit-124/fold-0.bin\", max_len=256)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-124/fold-1.bin\", max_len=256)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-124/fold-2.bin\", max_len=256)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-124/fold-3.bin\", max_len=256)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-124/fold-4.bin\", max_len=256)\n\npreds_roberta_large_meanpooling = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# xlnet-base-cased\n\nclass RoBERTaLarge(nn.Module):\n    def __init__(self, model_path):\n        super(RoBERTaLarge, self).__init__()\n        self.in_features = 768 # 1024\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.head = AttentionHead(self.in_features,self.in_features,1)\n        self.dropout = nn.Dropout(0.1)\n        self.process_num = nn.Sequential(\n            nn.Linear(10, 8),\n            nn.BatchNorm1d(8),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.process_tfidf = nn.Sequential(\n            nn.Linear(100, 32),\n            nn.BatchNorm1d(32),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.l0 = nn.Linear(self.in_features + 8 + 32, 2)\n        self.l1 = nn.Linear(self.in_features + 8 + 32, 12)\n\n    def forward(self, ids, mask, numerical_features, tfidf):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask\n        )\n\n        x1 = self.head(roberta_outputs[0]) # bs, 1024\n\n        x2 = self.process_num(numerical_features) # bs, 8\n\n        x3 = self.process_tfidf(tfidf) # bs, 32\n\n        x = torch.cat([x1, x2, x3], 1) # bs, 1024 + 8 + 32\n\n        logits = self.l0(self.dropout(x))\n        aux_logits = torch.sigmoid(self.l1(self.dropout(x)))\n\n        return logits[:, 0].squeeze(-1), aux_logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_predictions(pretrain_path, max_len):\n    device = \"cuda\"\n    model_path = '../input/xlnetbasecased/'\n    model = RoBERTaLarge(model_path)\n    model.to(device)\n    model.load_state_dict(torch.load(pretrain_path))\n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    df = get_sentence_features(df, 'excerpt')\n    \n    train = pd.read_csv(\"../input/step-1-create-folds/train_folds.csv\")\n    train = pd.concat([train, df]).reset_index(drop=True)\n    \n    TP = TextPreprocessor()\n    preprocessed_text = TP.preprocess(train['excerpt'])\n\n    pipeline = make_pipeline(\n                TfidfVectorizer(max_features=100000),\n                make_union(\n                    TruncatedSVD(n_components=50, random_state=42),\n                    make_pipeline(\n                        BM25Transformer(use_idf=True, k1=2.0, b=0.75),\n                        TruncatedSVD(n_components=50, random_state=42)\n                    ),\n                    n_jobs=1,\n                ),\n             )\n\n    z = pipeline.fit_transform(preprocessed_text)\n    tfidf_df = pd.DataFrame(z, columns=[f'cleaned_excerpt_tf_idf_svd_{i}' for i in range(50*2)])\n    \n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len, numerical_features=df[numerical_cols].values, tfidf=tfidf_df)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=0, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n    for b_idx, data in tqdm(enumerate(data_loader)):\n        with torch.no_grad():\n            inputs = data['input_ids'].to(device)\n            masks = data['attention_mask'].to(device)\n            numerical_features = data['numerical_features'].to(device)\n            tfidf = data['tfidf'].to(device)\n            output, _ = model(inputs, masks, numerical_features, tfidf)\n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n\n    del model, tokenizer, train, df, preprocessed_text, pipeline, z, tfidf_df, dataset, data_loader\n    del inputs, masks, numerical_features, tfidf, output\n    gc.collect()\n    torch.cuda.empty_cache()\n    return np.array(final_output)\n\n\npreds0 = generate_predictions(\"../input/kaerururu-commonlit-126/fold-0.bin\", max_len=275)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-126/fold-1.bin\", max_len=275)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-126/fold-2.bin\", max_len=275)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-126/fold-3.bin\", max_len=275)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-126/fold-4.bin\", max_len=275)\n\npreds_xlnet_base_cased = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bart-large","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BARTLarge(nn.Module):\n    def __init__(self, model_path):\n        super(BARTLarge, self).__init__()\n        self.in_features = 1024\n        self.roberta = AutoModel.from_pretrained(model_path)\n        self.head = AttentionHead(self.in_features,self.in_features,1)\n        self.dropout = nn.Dropout(0.1)\n        self.process_num = nn.Sequential(\n            nn.Linear(10, 8),\n            nn.BatchNorm1d(8),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.process_tfidf = nn.Sequential(\n            nn.Linear(100, 32),\n            nn.BatchNorm1d(32),\n            nn.PReLU(),\n            nn.Dropout(0.1),\n        )\n        self.l0 = nn.Linear(self.in_features + 8 + 32, 1)\n        self.l1 = nn.Linear(self.in_features + 8 + 32, 12)\n\n    def forward(self, ids, mask, numerical_features, tfidf):\n        roberta_outputs = self.roberta(\n            ids,\n            attention_mask=mask\n        )\n\n        x1 = self.head(roberta_outputs[0]) # bs, 1024\n\n        x2 = self.process_num(numerical_features) # bs, 8\n\n        x3 = self.process_tfidf(tfidf) # bs, 32\n\n        x = torch.cat([x1, x2, x3], 1) # bs, 1024 + 8 + 32\n\n        logits = self.l0(self.dropout(x))\n        aux_logits = torch.sigmoid(self.l1(self.dropout(x)))\n        return logits.squeeze(-1), aux_logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_predictions(pretrain_path, max_len):\n    device = \"cuda\"\n    model_path = '../input/bart-models-hugging-face-model-repository/bart-large/'\n    model = BARTLarge(model_path)\n    model.to(device)\n    model.load_state_dict(torch.load(pretrain_path))\n    model.eval()\n    tokenizer = AutoTokenizer.from_pretrained('../input/bartbase/')\n    \n    df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n    df = get_sentence_features(df, 'excerpt')\n    \n    train = pd.read_csv(\"../input/step-1-create-folds/train_folds.csv\")\n    train = pd.concat([train, df]).reset_index(drop=True)\n    \n    TP = TextPreprocessor()\n    preprocessed_text = TP.preprocess(train['excerpt'])\n\n    pipeline = make_pipeline(\n                TfidfVectorizer(max_features=100000),\n                make_union(\n                    TruncatedSVD(n_components=50, random_state=42),\n                    make_pipeline(\n                        BM25Transformer(use_idf=True, k1=2.0, b=0.75),\n                        TruncatedSVD(n_components=50, random_state=42)\n                    ),\n                    n_jobs=1,\n                ),\n             )\n\n    z = pipeline.fit_transform(preprocessed_text)\n    tfidf_df = pd.DataFrame(z, columns=[f'cleaned_excerpt_tf_idf_svd_{i}' for i in range(50*2)])\n    \n    dataset = Dataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len, numerical_features=df[numerical_cols].values, tfidf=tfidf_df)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=32, num_workers=0, pin_memory=True, shuffle=False\n    )\n\n    final_output = []\n    for b_idx, data in tqdm(enumerate(data_loader)):\n        with torch.no_grad():\n            inputs = data['input_ids'].to(device)\n            masks = data['attention_mask'].to(device)\n            numerical_features = data['numerical_features'].to(device)\n            tfidf = data['tfidf'].to(device)\n            output, _ = model(inputs, masks, numerical_features, tfidf)\n            output = output.detach().cpu().numpy().tolist()\n            final_output.extend(output)\n  \n    del model, tokenizer, train, df, preprocessed_text, pipeline, z, tfidf_df, dataset, data_loader\n    del inputs, masks, numerical_features, tfidf, output\n    gc.collect()\n    torch.cuda.empty_cache()\n    return np.array(final_output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds0 = generate_predictions(\"../input/kaerururu-commonlit-121/fold-0.bin\", max_len=256)\npreds1 = generate_predictions(\"../input/kaerururu-commonlit-121/fold-1.bin\", max_len=256)\npreds2 = generate_predictions(\"../input/kaerururu-commonlit-121/fold-2.bin\", max_len=256)\npreds3 = generate_predictions(\"../input/kaerururu-commonlit-121/fold-3.bin\", max_len=256)\npreds4 = generate_predictions(\"../input/kaerururu-commonlit-121/fold-4.bin\", max_len=256)\n\npreds_bart_large = (preds0 + preds1 + preds2 + preds3 + preds4) / 5\n\ndel preds0, preds1, preds2, preds3, preds4; gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_embeddings1 = np.load('../input/commonlit-2nd-stacking-4/train_embeddings1_clr_roberta_model0.npy')\ntrain_embeddings2 = np.load('../input/commonlit-2nd-stacking-4/train_embeddings2_clr_roberta_model1.npy')\ntrain_embeddings3 = np.load('../input/commonlit-2nd-stacking-4/train_embeddings3_clr_roberta_model2.npy')\ntrain_embeddings4 = np.load('../input/commonlit-2nd-stacking-4/train_embeddings4_clr_roberta_model3.npy')\ntrain_embeddings5 = np.load('../input/commonlit-2nd-stacking-4/train_embeddings5_clr_roberta_model4.npy')\n\ntrain_embeddings_clr_roberta = (train_embeddings1+train_embeddings2+train_embeddings3+train_embeddings4+train_embeddings5)/5\ndel train_embeddings1,train_embeddings2,train_embeddings3,train_embeddings4,train_embeddings5\ngc.collect()\n\ntrain_embeddings1 = np.load('../input/commonlit-2nd-stacking-4/train_embeddings1_Roberta_Base_123_1_h5.npy')\ntrain_embeddings2 = np.load('../input/commonlit-2nd-stacking-4/train_embeddings2_Roberta_Base_123_2_h5.npy')\ntrain_embeddings3 = np.load('../input/commonlit-2nd-stacking-4/train_embeddings3_Roberta_Base_123_3_h5.npy')\ntrain_embeddings4 = np.load('../input/commonlit-2nd-stacking-4/train_embeddings4_Roberta_Base_123_4_h5.npy')\ntrain_embeddings5 = np.load('../input/commonlit-2nd-stacking-4/train_embeddings5_Roberta_Base_123_5_h5.npy')\n\ntrain_embeddings_Roberta_Base_123 = (train_embeddings1+train_embeddings2+train_embeddings3+train_embeddings4+train_embeddings5)/5\ndel train_embeddings1,train_embeddings2,train_embeddings3,train_embeddings4,train_embeddings5\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_embeddings1 = np.load('test_embeddings1_clr_roberta_model0.npy')\ntest_embeddings2 = np.load('test_embeddings2_clr_roberta_model1.npy')\ntest_embeddings3 = np.load('test_embeddings3_clr_roberta_model2.npy')\ntest_embeddings4 = np.load('test_embeddings4_clr_roberta_model3.npy')\ntest_embeddings5 = np.load('test_embeddings5_clr_roberta_model4.npy')\n\ntest_embeddings_clr_roberta = (test_embeddings1+test_embeddings2+test_embeddings3+test_embeddings4+test_embeddings5)/5\ndel test_embeddings1,test_embeddings2,test_embeddings3,test_embeddings4,test_embeddings5\ngc.collect()\n\ntest_embeddings1 = np.load('test_embeddings1_Roberta_Base_123_1_h5.npy')\ntest_embeddings2 = np.load('test_embeddings2_Roberta_Base_123_2_h5.npy')\ntest_embeddings3 = np.load('test_embeddings3_Roberta_Base_123_3_h5.npy')\ntest_embeddings4 = np.load('test_embeddings4_Roberta_Base_123_4_h5.npy')\ntest_embeddings5 = np.load('test_embeddings5_Roberta_Base_123_5_h5.npy')\n\ntest_embeddings_Roberta_Base_123 = (test_embeddings1+test_embeddings2+test_embeddings3+test_embeddings4+test_embeddings5)/5\ndel test_embeddings1,test_embeddings2,test_embeddings3,test_embeddings4,test_embeddings5\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oofs_gpt2 = pd.read_csv('../input/commonlit-2nd-stacking-1/oofs_gpt2.csv')\noofs_roberta_base = pd.read_csv('../input/commonlit-2nd-stacking-1/oofs_roberta_base.csv')\noofs_bart_large = pd.read_csv('../input/commonlit-2nd-stacking-1/oofs_bart_large.csv')\noofs0457 = pd.read_csv('../input/commonlit-2nd-stacking-3/oofs0457.csv')\noofs0462 = pd.read_csv('../input/commonlit-2nd-stacking-prepare-oof/oofs0462.csv')\noofs0461 = pd.read_csv('../input/commonlit-2nd-stacking-prepare-oof/oofs0461.csv')\noofs0463 = pd.read_csv('../input/commonlit-2nd-stacking-prepare-oof/oofs0463.csv')\noofs0459 = pd.read_csv('../input/commonlit-2nd-stacking-prepare-oof/oofs0459.csv')\noofs_max_len_512 = pd.read_csv('../input/commonlit-2nd-stacking-prepare-oof/oofs_max_len_512.csv')\noofs_conv_roberta = pd.read_csv('../input/commonlit-2nd-stacking-prepare-oof/oofs_conv_roberta.csv')\noofs_electra_large = pd.read_csv('../input/commonlit-2nd-stacking-prepare-oof-2/oofs_electra_large.csv')\noofs_distil_roberta_base = pd.read_csv('../input/commonlit-2nd-stacking-prepare-oof-2/oofs_distil_roberta_base.csv')\noofs_roberta_large_meanpooling = pd.read_csv('../input/commonlit-2nd-stacking-prepare-oof-3/oofs_roberta_large_meanpooling.csv')\noofs_xlnet_base_cased = pd.read_csv('../input/commonlit-2nd-stacking-prepare-oof-4/oofs_xlnet_base_cased.csv')\noofs_sentence_transformer = pd.read_csv('../input/commonlit-stacking-prepare-ooemb-pub/sentence_transformer_oof_df.csv')[['id', 'oof']]\n\n# emb_distil_roberta_base = pd.read_csv('../input/commonlit-stacking-prepare-ooemb/emb_distil_roberta_base.csv')\nemb_electra_large = pd.read_csv('../input/commonlit-stacking-prepare-ooemb/emb_electra_large.csv')\n\noofs_gpt2.columns = ['id', 'oof0']\noofs_roberta_base.columns = ['id', 'oof1']\noofs_bart_large.columns = ['id', 'oof2']\noofs0457.columns = ['id', 'oof3']\noofs0462.columns = ['id', 'oof4']\noofs0461.columns = ['id', 'oof5']\noofs0463.columns = ['id', 'oof6']\noofs0459.columns = ['id', 'oof7']\noofs_max_len_512.columns = ['id', 'oof8']\noofs_conv_roberta.columns = ['id', 'oof9']\noofs_electra_large.columns = ['id', 'oof10']\noofs_distil_roberta_base.columns = ['id', 'oof11']\noofs_roberta_large_meanpooling.columns = ['id', 'oof12']\noofs_xlnet_base_cased.columns = ['id', 'oof13']\noofs_sentence_transformer.columns = ['id', 'oof14']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# oof_all = pd.read_csv(\"../input/step-1-create-folds/train_folds.csv\")\noof_all = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n\noof_all = pd.merge(oof_all, oofs_gpt2, on='id')\noof_all = pd.merge(oof_all, oofs_roberta_base, on='id')\noof_all = pd.merge(oof_all, oofs_bart_large, on='id')\noof_all = pd.merge(oof_all, oofs0457, on='id')\noof_all = pd.merge(oof_all, oofs0462, on='id')\noof_all = pd.merge(oof_all, oofs0461, on='id')\noof_all = pd.merge(oof_all, oofs0463, on='id')\noof_all = pd.merge(oof_all, oofs0459, on='id')\noof_all = pd.merge(oof_all, oofs_max_len_512, on='id')\noof_all = pd.merge(oof_all, oofs_conv_roberta, on='id')\noof_all = pd.merge(oof_all, oofs_electra_large, on='id')\noof_all = pd.merge(oof_all, oofs_distil_roberta_base, on='id')\noof_all = pd.merge(oof_all, oofs_roberta_large_meanpooling, on='id')\noof_all = pd.merge(oof_all, oofs_xlnet_base_cased, on='id')\noof_all = pd.merge(oof_all, oofs_sentence_transformer, on='id')\n\n# oof_all = pd.merge(oof_all, emb_distil_roberta_base, on='id')\noof_all = pd.merge(oof_all, emb_electra_large, on='id')\n\ndel oofs_gpt2, oofs_roberta_base, oofs_bart_large, oofs0457;gc.collect()\ndel oofs0462, oofs0461, oofs0463;gc.collect()\ndel oofs0459, oofs_max_len_512, oofs_conv_roberta;gc.collect()\ndel oofs_electra_large, oofs_distil_roberta_base;gc.collect()\ndel oofs_roberta_large_meanpooling, oofs_xlnet_base_cased;gc.collect()\n# del emb_distil_roberta_base;gc.collect() \ndel emb_electra_large;gc.collect() \n\noof_all = pd.concat([oof_all, pd.DataFrame(train_embeddings_clr_roberta, columns=[f'clr_{i}' for i in range(768)])], 1).reset_index(drop=True)\noof_all = pd.concat([oof_all, pd.DataFrame(train_embeddings_Roberta_Base_123, columns=[f'roberta_base_{i}' for i in range(768)])], 1).reset_index(drop=True)\n\ndel train_embeddings_clr_roberta, train_embeddings_Roberta_Base_123;gc.collect()\nprint(oof_all.shape)\noof_all.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_all = np.stack([\n    preds_gpt2,preds_roberta_base,preds_bart_large,preds0457,\n    preds0462,preds0461,preds0463,preds0459,\n    preds_max_len_512,preds_conv_roberta,\n    preds_electra_large,preds_distil_roberta_base,\n    preds_roberta_large_meanpooling, preds_xlnet_base_cased,\n    # preds_sentence_transformer,\n], 1)\n\nprint(preds_all.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = np.concatenate([preds_all, emb_preds_electra_large], 1)\nX_test = np.concatenate([X_test, test_embeddings_clr_roberta], 1)\nX_test = np.concatenate([X_test, test_embeddings_Roberta_Base_123], 1)\n\nX_test1 = np.concatenate([preds_all, preds_sentence_transformer.reshape(-1, 1)], 1)\nX_test1 = np.concatenate([X_test1, emb_preds_electra_large], 1)\nX_test1 = np.concatenate([X_test1, test_embeddings_clr_roberta], 1)\nX_test1 = np.concatenate([X_test1, test_embeddings_Roberta_Base_123], 1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds_all = np.concatenate([preds_all, emb_preds_distil_roberta_base], 1)\n# preds_all = np.concatenate([preds_all, emb_preds_electra_large], 1)\n\n# preds_all = np.concatenate([preds_all, test_embeddings_clr_roberta], 1)\n# preds_all = np.concatenate([preds_all, test_embeddings_Roberta_Base_123], 1)\n\ndel test_embeddings_clr_roberta, test_embeddings_Roberta_Base_123;gc.collect()\nprint(preds_all.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = oof_all[[f'oof{i}' for i in range(14)]+[f'emb_electra_large_{i}' for i in range(1024)]+[f'clr_{i}' for i in range(768)]+[f'roberta_base_{i}' for i in range(768)]].values\nX1 = oof_all[[f'oof{i}' for i in range(15)]+[f'emb_electra_large_{i}' for i in range(1024)]+[f'clr_{i}' for i in range(768)]+[f'roberta_base_{i}' for i in range(768)]].values\n\ny = oof_all[['target']].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XX1 = oof_all[[f'oof{i}' for i in range(5)]+['oof10', 'oof12']+[f'clr_{i}' for i in range(768)]].values\n# XX2 = oof_all[[f'oof{i+5}' for i in range(5)]+['oof11', 'oof13']+[f'roberta_base_{i}' for i in range(768)]].values\n# XX_test1 = np.concatenate([X_test[:, [0,1,2,3,4,10,12]], X_test[:, -768*2:-768*1]], 1)\n# XX_test2 = np.concatenate([X_test[:, [5,6,7,8,9,11,13]], X_test[:, -768*1:]], 1)\n\nXX1 = oof_all[[f'oof{i}' for i in range(5)]+[f'clr_{i}' for i in range(768)]].values\nXX2 = oof_all[[f'oof{i+5}' for i in range(5)]+[f'roberta_base_{i}' for i in range(768)]].values\n\nXX_test1 = np.concatenate([X_test[:, [0,1,2,3,4]], X_test[:, -768*2:-768*1]], 1)\nXX_test2 = np.concatenate([X_test[:, [5,6,7,8,9]], X_test[:, -768*1:]], 1)\n\nXX = np.stack([XX1, XX2], 1)\nXX_test = np.stack([XX_test1, XX_test2], 1)\n\nXX.shape, XX_test.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import BayesianRidge, LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import SVR\n\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\ndef get_preds_2nd_stages(X,y,X_test,nfolds=5,C=10,kernel='rbf'):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    new_oof = []\n    new_true = []\n    \n    kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=71)\n    num_bins = int(np.floor(1 + np.log2(len(X)))) # 12\n    y2 = pd.cut(y.reshape(-1),bins=num_bins,labels=False)\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X, y2)):\n    # kfold = KFold(n_splits=5,shuffle=True,random_state=71)\n    # for k, (train_idx,valid_idx) in enumerate(kfold.split(X)):\n        model = BayesianRidge(n_iter=30, verbose=True) \n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        \n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += np.squeeze(model.predict(X_test))\n        new_oof.append(prediction)\n        new_true.append(y_valid)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)/nfolds, np.concatenate(new_oof, 0), np.concatenate(new_true, 0)\n\n\nbr_stacked_preds, br_stacked_oofs, br_stacked_trues = get_preds_2nd_stages(X1,y,X_test1)\nprint(br_stacked_preds.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MLP Stacking","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n\n\ndef calc_loss(y_true, y_pred):\n    return  np.sqrt(mean_squared_error(y_true, y_pred))\n    \n    \nSEED = 71\nseed_everything(SEED)\n\n\nclass MLPDataset:\n    def __init__(self, X, y=None):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, item):\n\n        features = self.X[item]\n\n        if self.y is not None:\n            targets = self.y[item]\n        \n            return {\n                'x': torch.tensor(features, dtype=torch.float32),\n                'y': torch.tensor(targets, dtype=torch.float32),\n            }\n          \n        else:\n            return {\n                'x': torch.tensor(features, dtype=torch.float32),\n            }  \n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass MetricMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.y_true = []\n        self.y_pred = []\n    \n    def update(self, y_true, y_pred):\n        self.y_true.extend(y_true.cpu().detach().numpy().tolist())\n        self.y_pred.extend(y_pred.cpu().detach().numpy().tolist())\n\n    @property\n    def avg(self):\n        self.rmse = calc_loss(self.y_true, self.y_pred)\n       \n        return {\n            \"RMSE\" : self.rmse,\n        }\n    \n        \nclass RMSELoss(torch.nn.Module):\n    def __init__(self):\n        super(RMSELoss,self).__init__()\n\n    def forward(self,x,y):\n        criterion = nn.MSELoss()\n        loss = torch.sqrt(criterion(x, y))\n        return loss\n\n\ndef loss_fn(logits, targets):\n    loss_fct = RMSELoss()\n    loss = loss_fct(logits, targets)\n    return loss\n        \n        \ndef train_fn(model, data_loader, device, optimizer, scheduler):\n    model.train()\n    losses = AverageMeter()\n    scores = MetricMeter()\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    for data in tk0:\n        optimizer.zero_grad()\n        inputs = data['x'].to(device)\n        targets = data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        losses.update(loss.item(), inputs.size(0))\n        scores.update(targets, outputs)\n        tk0.set_postfix(loss=losses.avg)\n    return scores.avg, losses.avg\n\n\ndef valid_fn(model, data_loader, device):\n    model.eval()\n    losses = AverageMeter()\n    scores = MetricMeter()\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    valid_preds = []\n    with torch.no_grad():\n        for data in tk0:\n            inputs = data['x'].to(device)\n            targets = data['y'].to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, targets)\n\n            losses.update(loss.item(), inputs.size(0))\n            scores.update(targets, outputs)\n            tk0.set_postfix(loss=losses.avg)\n    return scores.avg, losses.avg\n\n\ndef run_one_fold(fold, X, y):\n    kf = KFold(n_splits = 5, random_state = SEED, shuffle=True)\n    splits = list(kf.split(X=X))\n    train_idx = splits[fold][0]\n    valid_idx = splits[fold][1]\n\n    train_dataset = MLPDataset(X=X[train_idx], y=y[train_idx])\n    train_loader = torch.utils.data.DataLoader(\n                   train_dataset, shuffle=True, \n                   batch_size=32,\n                   num_workers=0, pin_memory=True)\n\n    val_dataset = MLPDataset(X=X[valid_idx], y=y[valid_idx])\n    val_loader = torch.utils.data.DataLoader(\n                 val_dataset, shuffle=False, \n                 batch_size=32,\n                 num_workers=0, pin_memory=True)\n\n    del train_dataset, val_dataset\n    gc.collect()\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = MLP(X.shape[1])\n    model = model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n\n    patience = 5\n    p = 0\n    min_loss = 999\n    best_score = 999\n\n    for epoch in range(1, 100 + 1):\n\n        print(\"Starting {} epoch...\".format(epoch))\n\n        start_time = time.time()\n        \n        train_avg, train_loss = train_fn(model, train_loader, device, optimizer, scheduler)\n        valid_avg, valid_loss = valid_fn(model, val_loader, device)\n        scheduler.step()\n\n        elapsed = time.time() - start_time\n        \n        print(f'Epoch {epoch+1} - avg_train_loss: {train_loss:.5f}  avg_val_loss: {valid_loss:.5f}  time: {elapsed:.0f}s')\n        print(f\"Epoch {epoch+1} - train_rmse:{train_avg['RMSE']:0.5f}  valid_rmse:{valid_avg['RMSE']:0.5f}\")\n\n        if valid_avg['RMSE'] < best_score:\n            print(f\">>>>>>>> Model Improved From {best_score} ----> {valid_avg['RMSE']}\")\n            torch.save(model.state_dict(), f'fold-{fold}.bin')\n            best_score = valid_avg['RMSE']\n            p = 0 \n\n        p += 1\n        if p > patience:\n            print(f'Early Stopping')\n            break\n            \n            \ndef calc_cv_and_inference(model_paths, X, y, X_test):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    models = []\n    for p in model_paths:\n        model = MLP(X.shape[1])\n        model.to(device)\n        model.load_state_dict(torch.load(p))\n        model.eval()\n        models.append(model)\n    \n    kf = KFold(n_splits = 5, random_state = SEED, shuffle=True)\n    splits = list(kf.split(X=X))\n\n    y_true = []\n    y_pred = []\n    y_test_pred = []\n    for fold, model in enumerate(models):\n        train_idx = splits[fold][0]\n        valid_idx = splits[fold][1]\n    \n        valid_dataset = MLPDataset(X=X[valid_idx], y=y[valid_idx])\n        valid_dataloader = torch.utils.data.DataLoader(\n                 valid_dataset, shuffle=False, \n                 batch_size=32,\n                 num_workers=0, pin_memory=True)\n        \n        test_dataset = MLPDataset(X=X_test)\n        test_dataloader = torch.utils.data.DataLoader(\n                 test_dataset, shuffle=False, \n                 batch_size=32,\n                 num_workers=0, pin_memory=True)\n\n        final_output = []\n        for b_idx, data in tqdm(enumerate(valid_dataloader)):\n            with torch.no_grad():\n                inputs = data['x'].to(device)\n                targets = data['y'].to(device)\n                output = model(inputs)\n                output = output.detach().cpu().numpy().tolist()\n                final_output.extend(output)\n        print(calc_loss(np.array(final_output), y[valid_idx]))\n        y_pred.append(np.array(final_output))\n        y_true.append(y[valid_idx])\n        \n        test_output = []\n        for b_idx, data in tqdm(enumerate(test_dataloader)):\n            with torch.no_grad():\n                inputs = data['x'].to(device)\n                output = model(inputs)\n                output = output.detach().cpu().numpy().tolist()\n                test_output.extend(output)   \n        y_test_pred.append(np.array(test_output))\n        \n    y_pred = np.concatenate(y_pred)\n    y_true = np.concatenate(y_true)\n    y_test_pred = np.squeeze(np.mean(y_test_pred, 0))\n    overall_cv_score = calc_loss(y_true, y_pred)\n    print(f'cv score {overall_cv_score}')\n    return y_test_pred, y_pred, y_true\n\n\nclass MLP(nn.Module):\n    def __init__(self, len_features):\n        super(MLP, self).__init__()    \n        \n        self.conv = nn.Conv1d(773, 2, 3, padding=2)\n        nn.init.xavier_uniform_(self.conv.weight)\n\n        self.regressor = nn.Sequential(\n            nn.Linear(8, 1)\n        )\n\n    def forward(self, features):\n        # bs, 2, 773\n        conv_out = self.conv(features.transpose(1,2))\n        bs, _, _ = conv_out.size()\n        conv_out = conv_out.view(bs, -1)\n        \n        output = self.regressor(conv_out)\n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold in range(5):\n    print(\"Starting fold {} ...\".format(fold))\n    run_one_fold(fold, XX, y)\n    \nmodel_paths = [\n    'fold-0.bin', \n    'fold-1.bin', \n    'fold-2.bin', \n    'fold-3.bin', \n    'fold-4.bin', \n]\n\n# mlp_stacked_preds, mlp_stacked_oofs, mlp_stacked_trues = calc_cv_and_inference(model_paths, X, y, X_test)\nmlp_stacked_preds, mlp_stacked_oofs, mlp_stacked_trues = calc_cv_and_inference(model_paths, XX, y, XX_test)\nprint(rmse_score(mlp_stacked_oofs, mlp_stacked_trues))\nprint(mlp_stacked_preds.shape, mlp_stacked_oofs.shape, mlp_stacked_trues.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, len_features):\n        super(MLP, self).__init__()    \n\n        self.regressor = nn.Sequential(\n            nn.Linear(1550+1024, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, features):\n        # bs, 1550+1024\n        output = self.regressor(features)\n        return output\n    \n    \nfor fold in range(5):\n    print(\"Starting fold {} ...\".format(fold))\n    run_one_fold(fold, X, y)\n    \nmodel_paths = [\n    'fold-0.bin', \n    'fold-1.bin', \n    'fold-2.bin', \n    'fold-3.bin', \n    'fold-4.bin', \n]\n\nmlp_stacked_preds2, mlp_stacked_oofs2, mlp_stacked_trues2 = calc_cv_and_inference(model_paths, X, y, X_test)\nprint(rmse_score(mlp_stacked_oofs2, mlp_stacked_trues2))\nprint(mlp_stacked_preds2.shape, mlp_stacked_oofs2.shape, mlp_stacked_trues2.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make submission","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_oof = (br_stacked_oofs + oof_all['oof3'])/2\nfinal_y = (br_stacked_trues + y)/2\nrmse_score(final_oof, final_y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_oof = (br_stacked_oofs + np.squeeze(mlp_stacked_oofs) + np.squeeze(oof_all['oof3']))/3\nfinal_y = (br_stacked_trues + mlp_stacked_trues + y)/3\nrmse_score(final_oof, final_y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_oof = (br_stacked_oofs + np.squeeze(mlp_stacked_oofs) + np.squeeze(mlp_stacked_oofs2) + np.squeeze(oof_all['oof3']))/4\nfinal_y = (br_stacked_trues + mlp_stacked_trues + mlp_stacked_trues2 + y)/4\nrmse_score(final_oof, final_y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmission.target = (br_stacked_preds + mlp_stacked_preds + mlp_stacked_preds2 + preds0457)/4\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}