{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport torch\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig","metadata":{"execution":{"iopub.status.busy":"2021-08-01T19:01:30.552948Z","iopub.execute_input":"2021-08-01T19:01:30.553325Z","iopub.status.idle":"2021-08-01T19:01:36.09655Z","shell.execute_reply.started":"2021-08-01T19:01:30.553247Z","shell.execute_reply":"2021-08-01T19:01:36.095513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-01T19:01:36.097963Z","iopub.execute_input":"2021-08-01T19:01:36.098294Z","iopub.status.idle":"2021-08-01T19:01:36.112109Z","shell.execute_reply.started":"2021-08-01T19:01:36.098255Z","shell.execute_reply":"2021-08-01T19:01:36.111224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        self.TARGET_COLS = [\"target\"]\n        self.MAX_LEN = 256\n        self.AUTOMODEL_CHECKPOINT = \"../input/roberta-base\"\n        self.TOKENIZER_CHECKPOINT = \"../input/roberta-base\"\n        self.EPOCHS = 3\n        self.TRAIN_BATCH_SIZE = 16\n        self.EVAL_BATCH_SIZE = 16\n        self.LR = 5e-5\n        self.DEVICE = \"cuda\"\n        self.EVAL_INTERVAL = 20\n        self.LOG_INTERVAL = 20\n        self.FOLDS = 5\n\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T19:01:39.937399Z","iopub.execute_input":"2021-08-01T19:01:39.937765Z","iopub.status.idle":"2021-08-01T19:01:39.944798Z","shell.execute_reply.started":"2021-08-01T19:01:39.937729Z","shell.execute_reply":"2021-08-01T19:01:39.94369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRPDataset(Dataset):\n    def __init__(self, data, tokenizer_checkpoint, max_length: int = 256, is_test: bool = False):\n        self.excerpts = data.excerpt.values.tolist()\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n        self.max_len = max_length\n        self.is_test = is_test\n        if not self.is_test:\n            self.targets = data.target.values.tolist()\n        \n    def __getitem__(self, idx):\n        item = self.tokenizer(self.excerpts[idx], max_length=self.max_len,\n                             return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n        if self.is_test:\n            return {\n                \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long).squeeze(0),\n                \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long).squeeze(0)\n            }\n        else:\n            target = self.targets[idx]\n            return {\n                \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long).squeeze(0),\n                \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long).squeeze(0),\n                \"label\": torch.tensor(target, dtype=torch.float).squeeze(0)\n            }\n\n    def __len__(self):\n        return len(self.excerpts)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T19:01:46.496368Z","iopub.execute_input":"2021-08-01T19:01:46.496705Z","iopub.status.idle":"2021-08-01T19:01:46.506645Z","shell.execute_reply.started":"2021-08-01T19:01:46.496654Z","shell.execute_reply":"2021-08-01T19:01:46.505775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \nclass CLRPModel(nn.Module):\n    def __init__(self,path):\n        super(CLRPModel, self).__init__()\n        self.roberta = AutoModel.from_pretrained(path)  \n        self.config = AutoConfig.from_pretrained(path)\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.config.hidden_size,1)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        loss = None\n        if labels is not None:\n            loss = loss_fn(x, labels)\n        return (loss, x) if loss is not None else x","metadata":{"execution":{"iopub.status.busy":"2021-08-01T19:01:51.124617Z","iopub.execute_input":"2021-08-01T19:01:51.124978Z","iopub.status.idle":"2021-08-01T19:01:51.140845Z","shell.execute_reply.started":"2021-08-01T19:01:51.124946Z","shell.execute_reply":"2021-08-01T19:01:51.139232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, test_loader):\n    preds = []\n    for batch_idx, batch in enumerate(test_loader):\n        input_ids = batch[\"input_ids\"].to(config.DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n        model.eval()\n        with torch.no_grad():\n            logits = model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask\n                    )    \n            logits = logits.view(-1).detach().cpu().numpy()\n            preds.extend(logits)\n    del model\n    gc.collect()\n    return preds","metadata":{"execution":{"iopub.status.busy":"2021-08-01T19:01:53.52027Z","iopub.execute_input":"2021-08-01T19:01:53.520612Z","iopub.status.idle":"2021-08-01T19:01:53.527256Z","shell.execute_reply.started":"2021-08-01T19:01:53.52058Z","shell.execute_reply":"2021-08-01T19:01:53.526271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(model_dir, model_prefix, test_data):    \n    all_preds = []\n    dtest = CLRPDataset(\n            data=test_data,\n            tokenizer_checkpoint=config.TOKENIZER_CHECKPOINT,\n            max_length=config.MAX_LEN,\n            is_test=True\n        )\n    test_loader = DataLoader(\n                    dataset=dtest,\n                    batch_size=16,\n                    shuffle=False,\n                    drop_last=False\n                )\n\n    model = CLRPModel(path=config.AUTOMODEL_CHECKPOINT)\n    \n    print(\"Starting inference...\")\n    for dir, _, filenames in os.walk(model_dir):\n        for filename in filenames:\n            if model_prefix in filename:\n                model.load_state_dict(torch.load(os.path.join(dir, filename), map_location=config.DEVICE))\n                model.to(config.DEVICE)\n                print(f\"Predicting using model: {filename}\")\n                preds = predict(model, test_loader)\n                all_preds.append(preds)\n    all_preds = np.array(all_preds)\n    final_preds = np.mean(all_preds, axis=0)\n    print(\"Inference Completed\")\n    return final_preds","metadata":{"execution":{"iopub.status.busy":"2021-08-01T19:01:55.98396Z","iopub.execute_input":"2021-08-01T19:01:55.98427Z","iopub.status.idle":"2021-08-01T19:01:55.992241Z","shell.execute_reply.started":"2021-08-01T19:01:55.98424Z","shell.execute_reply":"2021-08-01T19:01:55.990801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = run(model_dir=\"../input/clrp-roberta-base\",\n                 model_prefix=\"model-fold-\",\n                 test_data=test_df)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T19:02:00.498774Z","iopub.execute_input":"2021-08-01T19:02:00.499136Z","iopub.status.idle":"2021-08-01T19:02:34.544002Z","shell.execute_reply.started":"2021-08-01T19:02:00.499102Z","shell.execute_reply":"2021-08-01T19:02:34.542805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"id\": test_df.id.values,\n    \"target\": predictions\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T19:02:56.489886Z","iopub.execute_input":"2021-08-01T19:02:56.490207Z","iopub.status.idle":"2021-08-01T19:02:56.498354Z","shell.execute_reply.started":"2021-08-01T19:02:56.490177Z","shell.execute_reply":"2021-08-01T19:02:56.497521Z"},"trusted":true},"execution_count":null,"outputs":[]}]}