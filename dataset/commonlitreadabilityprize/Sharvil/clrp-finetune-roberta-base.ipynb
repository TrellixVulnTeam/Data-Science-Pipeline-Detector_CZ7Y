{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport tqdm\nimport torch\nimport joblib\nimport transformers\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import AutoTokenizer, AdamW, get_cosine_schedule_with_warmup\nfrom transformers import AutoModel, AutoConfig\n\nfrom sklearn import model_selection","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:10:48.742391Z","iopub.execute_input":"2021-08-01T18:10:48.742721Z","iopub.status.idle":"2021-08-01T18:10:54.826033Z","shell.execute_reply.started":"2021-08-01T18:10:48.742693Z","shell.execute_reply":"2021-08-01T18:10:54.825067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Config","metadata":{}},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        self.TARGET_COLS = [\"target\"]\n        self.MAX_LEN = 256\n        self.CHECKPOINT = \"../input/clrp-itpt-roberta-base/clrp-itpt-model-roberta-base\"\n        self.TOKENIZER_CHECKPOINT = \"roberta-base\"\n        self.EPOCHS = 3\n        self.TRAIN_BATCH_SIZE = 16\n        self.EVAL_BATCH_SIZE = 16\n        self.LR = 5e-5\n        self.DEVICE = \"cuda\"\n        self.EVAL_INTERVAL = 20\n        self.LOG_INTERVAL = 20\n        self.FOLDS = 5\n        self.WD = 0.01\n\nconfig = Config()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:10:56.878512Z","iopub.execute_input":"2021-08-01T18:10:56.87889Z","iopub.status.idle":"2021-08-01T18:10:56.884677Z","shell.execute_reply.started":"2021-08-01T18:10:56.878858Z","shell.execute_reply":"2021-08-01T18:10:56.883724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Folds","metadata":{}},{"cell_type":"code","source":"raw_train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\nraw_test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n\ntrain_df = raw_train.copy(deep=True)\ntrain_df.loc[:, \"excerpt\"] = train_df.excerpt.map(lambda x: x.replace(\"\\n\", \"\"))\n# sum(train_df.excerpt.str.contains(\"\\n\"))\n\nnum_bins = int(np.floor(np.log2(len(train_df))))\nprint(f\"Num bins : {num_bins}\")\n\ntrain_df.loc[:, \"bins\"] = pd.cut(\n    train_df[\"target\"], bins=num_bins, labels=False\n)\n\nkf = model_selection.StratifiedKFold(n_splits=config.FOLDS)\n\ntrain_df[\"fold\"] = -1\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X=train_df, y=train_df.bins.values)):\n    train_df.loc[valid_idx, \"fold\"] = fold\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:10:59.602887Z","iopub.execute_input":"2021-08-01T18:10:59.603204Z","iopub.status.idle":"2021-08-01T18:10:59.75211Z","shell.execute_reply.started":"2021-08-01T18:10:59.603175Z","shell.execute_reply":"2021-08-01T18:10:59.751298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.bins.hist()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:11:00.071271Z","iopub.execute_input":"2021-08-01T18:11:00.071586Z","iopub.status.idle":"2021-08-01T18:11:00.261829Z","shell.execute_reply.started":"2021-08-01T18:11:00.071558Z","shell.execute_reply":"2021-08-01T18:11:00.260835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"excerpts = train_df[\"excerpt\"].copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:11:03.124837Z","iopub.execute_input":"2021-08-01T18:11:03.125165Z","iopub.status.idle":"2021-08-01T18:11:03.130811Z","shell.execute_reply.started":"2021-08-01T18:11:03.125137Z","shell.execute_reply":"2021-08-01T18:11:03.129933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"class CLRPDataset(Dataset):\n    def __init__(self, data, checkpoint, max_length: int = 256, is_test: bool = False):\n        self.excerpts = data.excerpt.values.tolist()\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.max_len = max_length\n        self.targets = data.target.values.tolist()\n        self.is_test = is_test\n        \n    def __getitem__(self, idx):\n        item = self.tokenizer(self.excerpts[idx], max_length=self.max_len,\n                             return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n        if self.is_test:\n            return {\n                \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long).squeeze(0),\n                \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long).squeeze(0)\n            }\n        else:\n            target = self.targets[idx]\n            return {\n                \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long).squeeze(0),\n                \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long).squeeze(0),\n                \"label\": torch.tensor(target, dtype=torch.float).squeeze(0)\n            }\n\n    def __len__(self):\n        return len(self.targets)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:11:03.851194Z","iopub.execute_input":"2021-08-01T18:11:03.851592Z","iopub.status.idle":"2021-08-01T18:11:03.861426Z","shell.execute_reply.started":"2021-08-01T18:11:03.851563Z","shell.execute_reply":"2021-08-01T18:11:03.860324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model\n","metadata":{}},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n    \nclass CLRPModel(nn.Module):\n    def __init__(self,path):\n        super(CLRPModel, self).__init__()\n        self.roberta = AutoModel.from_pretrained(path)  \n        self.config = AutoConfig.from_pretrained(path)\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.config.hidden_size,1)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        x = self.roberta(input_ids=input_ids, attention_mask=attention_mask)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        loss = None\n        if labels is not None:\n            loss = loss_fn(x, labels)\n        return (loss, x) if loss is not None else x","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:11:04.826894Z","iopub.execute_input":"2021-08-01T18:11:04.827215Z","iopub.status.idle":"2021-08-01T18:11:04.837605Z","shell.execute_reply.started":"2021-08-01T18:11:04.827184Z","shell.execute_reply":"2021-08-01T18:11:04.836567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Metrics","metadata":{}},{"cell_type":"code","source":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.count = 0\n        self.max = 0\n        self.min = 0\n        self.avg = 0\n        self.sum = 0\n    \n    def update(self, val, n=1):\n        self.val = val\n        self.count += n\n        self.sum += val*n\n        self.avg = self.sum / self.count\n        if val > self.max: self.max = val\n        if val < self.min: self.min = val\n\n\ndef loss_fn(outputs, targets):\n    outputs = outputs.view(-1)\n    targets = targets.view(-1)\n    return torch.sqrt(nn.MSELoss()(outputs, targets))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:11:06.449729Z","iopub.execute_input":"2021-08-01T18:11:06.450062Z","iopub.status.idle":"2021-08-01T18:11:06.457995Z","shell.execute_reply.started":"2021-08-01T18:11:06.450032Z","shell.execute_reply":"2021-08-01T18:11:06.45694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trainer","metadata":{}},{"cell_type":"code","source":"class Trainer:\n\n    def __init__(self, model, log_interval, eval_interval, epochs, \n                optimizer, lr_scheduler, model_dir):\n        self.model = model\n        self.optimizer = optimizer\n        self.lr_scheduler = lr_scheduler\n        self.epochs = epochs\n        self.log_interval = log_interval\n        self.eval_interval = eval_interval\n        self.model_dir = model_dir\n        self.evaluator = Evaluator(self.model)\n\n\n    def train(self, train_loader, valid_loader, result_dict, fold):\n        result_dict[\"best_valid_loss\"] = 9999\n        for epoch in range(self.epochs):\n            result_dict[\"epoch\"] = epoch\n            result_dict = self._train_loop_for_one_epoch(\n                epoch=epoch,\n                train_loader=train_loader,\n                valid_loader=valid_loader,\n                result_dict=result_dict\n            )\n        \n        return result_dict\n\n    def _train_loop_for_one_epoch(self, epoch, train_loader, valid_loader, result_dict):\n        losses = AverageMeter()\n        for batch_idx, batch in enumerate(train_loader):\n            input_ids = batch[\"input_ids\"].to(config.DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n            label = batch[\"label\"].to(config.DEVICE)\n            self.model = self.model.to(config.DEVICE)\n\n            loss = self._train_loop_for_one_step(\n                input_ids,\n                attention_mask,\n                label\n            )\n            losses.update(loss.item())\n\n            if batch_idx % self.log_interval == 0:\n                print(f\"Epoch={epoch}, Avg Loss={losses.avg}, Batch Idx={batch_idx}\")\n                if \"train_loss\" not in result_dict.keys(): result_dict[\"train_loss\"] = list()\n                result_dict[\"train_loss\"].append(losses.avg)\n                print(\"--------Training Results Summary------\")\n                print(f\"Epoch: {epoch}, train_loss: {losses.avg}\")\n\n            if batch_idx % self.eval_interval == 0:\n                result_dict = self.evaluator.evaluate(\n                    valid_loader=valid_loader,\n                    result_dict=result_dict,\n                    epoch=epoch\n                )\n                if result_dict[\"valid_loss\"][-1] <= result_dict[\"best_valid_loss\"]:\n                    print(f\"Train loss: {result_dict['train_loss'][-1]}, Valid loss: {result_dict['valid_loss'][-1]}\")\n                    print(f\"Valid loss decreased from {result_dict['best_valid_loss']} to {result_dict['valid_loss'][-1]}\")\n                    result_dict[\"best_valid_loss\"] = result_dict[\"valid_loss\"][-1]\n                    \n                    print(f\"Saving model state dict in {self.model_dir}....\")\n                    torch.save(self.model.state_dict(), f'{self.model_dir}/model-fold-{fold}_dict')\n\n        return result_dict\n    \n    def _train_loop_for_one_step(self, input_ids, attention_mask, label):\n        self.model.train()\n        self.optimizer.zero_grad()\n        loss, logits = self.model(input_ids, attention_mask, label)\n        loss.backward()\n        self.optimizer.step()\n        if self.lr_scheduler:\n            self.lr_scheduler.step()\n        return loss","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:11:07.067279Z","iopub.execute_input":"2021-08-01T18:11:07.067781Z","iopub.status.idle":"2021-08-01T18:11:07.089527Z","shell.execute_reply.started":"2021-08-01T18:11:07.067721Z","shell.execute_reply":"2021-08-01T18:11:07.087349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluator","metadata":{}},{"cell_type":"code","source":"class Evaluator:\n\n    def __init__(self, model):\n        self.model = model\n\n    def evaluate(self, epoch, valid_loader, result_dict):\n        losses = AverageMeter()\n        with torch.no_grad():\n            for batch_idx, batch in enumerate(valid_loader):\n                input_ids = batch[\"input_ids\"].to(config.DEVICE)\n                attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n                label = batch[\"label\"].to(config.DEVICE)\n                self.model = self.model.to(config.DEVICE)\n\n                loss = self._eval_loop_for_one_step(\n                    input_ids,\n                    attention_mask,\n                    label\n                )\n                losses.update(loss.item())\n            print(\"----------Validation Results Summary---------\")\n            print(f\"Epoch: {epoch}, valid_loss: {losses.avg}\")\n            if \"valid_loss\" not in result_dict.keys(): result_dict[\"valid_loss\"] = list()\n            result_dict[\"valid_loss\"].append(losses.avg)\n\n        return result_dict\n\n    def _eval_loop_for_one_step(self, input_ids, attention_mask, label):\n        self.model.eval()\n        loss, logits = self.model(input_ids, attention_mask, label)\n        return loss","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:11:11.426906Z","iopub.execute_input":"2021-08-01T18:11:11.427287Z","iopub.status.idle":"2021-08-01T18:11:11.435733Z","shell.execute_reply.started":"2021-08-01T18:11:11.427246Z","shell.execute_reply":"2021-08-01T18:11:11.434526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Main App","metadata":{}},{"cell_type":"code","source":"def run(df, fold, model_dir):\n\n    xtrain = df[df[\"fold\"] != fold]\n    xvalid = df[df [\"fold\"] == fold]\n    dtrain = CLRPDataset(xtrain, config.TOKENIZER_CHECKPOINT, config.MAX_LEN)\n    dvalid = CLRPDataset(xvalid, config.TOKENIZER_CHECKPOINT, config.MAX_LEN)\n\n    train_loader = DataLoader(\n        dtrain,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        shuffle=True\n    )\n    valid_loader = DataLoader(\n        dvalid,\n        batch_size=config.EVAL_BATCH_SIZE,\n        shuffle=True\n    )\n\n    model = CLRPModel(path=config.CHECKPOINT)\n    \n    optimizer = AdamW(model.parameters(),\n                      lr=config.LR,\n                      weight_decay=config.WD\n                )\n    \n    num_training_steps = config.EPOCHS*len(train_loader)\n    lr_scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n    \n    print(f\"Fold: {fold}\")\n    print(f\"Total Epochs: {config.EPOCHS}, Train Dataset length: {len(dtrain)}, Train Data Loader length: {len(train_loader)}\")\n    print(f\"Total Epochs: {config.EPOCHS}, Valid Dataset length: {len(dvalid)}, Valid Data Loader length: {len(valid_loader)}\")\n\n    print(f\"Num training steps: {num_training_steps}\")\n\n    result_dict = {}\n    trainer = Trainer(\n        model=model,\n        optimizer=optimizer,\n        lr_scheduler=lr_scheduler,\n        epochs=config.EPOCHS,\n        log_interval=config.LOG_INTERVAL,\n        eval_interval=config.EVAL_INTERVAL,\n        model_dir=model_dir\n    )\n    result_dict = trainer.train(\n        train_loader=train_loader,\n        valid_loader=valid_loader,\n        result_dict=result_dict,\n        fold=fold\n    )\n    joblib.dump(result_dict, f\"./result_dict-fold-{fold}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:11:14.138962Z","iopub.execute_input":"2021-08-01T18:11:14.139311Z","iopub.status.idle":"2021-08-01T18:11:14.149203Z","shell.execute_reply.started":"2021-08-01T18:11:14.139278Z","shell.execute_reply":"2021-08-01T18:11:14.148279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport gc\n\nfor fold in range(config.FOLDS):\n    torch.cuda.empty_cache()\n    gc.collect()\n    run(train_df, fold, \".\")","metadata":{"execution":{"iopub.status.busy":"2021-08-01T18:11:17.153171Z","iopub.execute_input":"2021-08-01T18:11:17.153547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = train_df.copy(deep=True)\nxtrain = df[df[\"fold\"] == fold]\nxvalid = df[df [\"fold\"] != fold]\ndtrain = CLRPDataset(xtrain, config.TOKENIZER_CHECKPOINT, config.MAX_LEN)\ndvalid = CLRPDataset(xvalid, config.TOKENIZER_CHECKPOINT, config.MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:19:35.060726Z","iopub.execute_input":"2021-08-01T17:19:35.061074Z","iopub.status.idle":"2021-08-01T17:19:41.487342Z","shell.execute_reply.started":"2021-08-01T17:19:35.061045Z","shell.execute_reply":"2021-08-01T17:19:41.486125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xvalid.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-01T17:19:51.403886Z","iopub.execute_input":"2021-08-01T17:19:51.404321Z","iopub.status.idle":"2021-08-01T17:19:51.411708Z","shell.execute_reply.started":"2021-08-01T17:19:51.40426Z","shell.execute_reply":"2021-08-01T17:19:51.410303Z"},"trusted":true},"execution_count":null,"outputs":[]}]}