{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install datasets > /dev/null","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\n\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer,TrainingArguments, Trainer, LineByLineTextDataset, DataCollatorForLanguageModeling","metadata":{"execution":{"iopub.status.busy":"2021-08-01T13:59:12.56952Z","iopub.execute_input":"2021-08-01T13:59:12.569958Z","iopub.status.idle":"2021-08-01T13:59:16.716209Z","shell.execute_reply.started":"2021-08-01T13:59:12.569891Z","shell.execute_reply":"2021-08-01T13:59:16.715228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create target text corpus\ntrain = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n\ndata = pd.concat([train, test], axis=0)\n\ndata.loc[:, \"excerpt\"] = data.excerpt.apply(lambda x: x.replace(\"\\n\", \"\"))\nexcerpts = \"\\n\".join(data.excerpt.values.tolist())\n\nwith open(\"./pretrain_data.txt\", \"w\") as f:\n    f.write(excerpts)\n    \nprint(\"Pretrain data created\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create model\ncheckpoint = \"roberta-base\"\nmodel = AutoModelForMaskedLM.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\n#create dataset\ndtrain = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=f\"./pretrain_data.txt\",\n    block_size=128\n)\n\ndvalid = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=f\"./pretrain_data.txt\",\n    block_size=128\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True,\n    mlm_probability=0.15\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=f\"./{checkpoint}_chk\", #select model path for checkpoint\n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=8,\n    evaluation_strategy= 'steps',\n    save_total_limit=2,\n    eval_steps=200,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end =True,\n    prediction_loss_only=True,\n    report_to = \"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dtrain,\n    eval_dataset=dvalid\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Starting with training...\")\ntrainer.train()\ntrainer.save_model(f\"./clrp-itpt-model-{checkpoint}\")\ntokenizer.save_pretrained(f\"./clrp-itpt-tokenizer-{checkpoint}\")","metadata":{},"execution_count":null,"outputs":[]}]}