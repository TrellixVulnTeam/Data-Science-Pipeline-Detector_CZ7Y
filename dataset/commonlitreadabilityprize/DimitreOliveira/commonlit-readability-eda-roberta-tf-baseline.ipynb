{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://github.com/dimitreOliveira/MachineLearning/blob/master/Kaggle/CommonLit%20Readability%20Prize/banner.png?raw=true\" width=\"1000\"></center>\n<br>\n<center><h1>CommonLit Readability - EDA & RoBERTa TF baseline</h1></center>\n<br>","metadata":{"papermill":{"duration":0.029498,"end_time":"2020-12-02T01:26:39.919827","exception":false,"start_time":"2020-12-02T01:26:39.890329","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Dependencies","metadata":{"papermill":{"duration":0.027678,"end_time":"2020-12-02T01:26:39.97669","exception":false,"start_time":"2020-12-02T01:26:39.949012","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import random, os, warnings, math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import optimizers, losses, metrics, Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom transformers import TFAutoModelForSequenceClassification, TFAutoModel, AutoTokenizer\n\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 0\nseed_everything(seed)\nsns.set(style='whitegrid')\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', 150)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":7.659947,"end_time":"2020-12-02T01:26:58.496129","exception":false,"start_time":"2020-12-02T01:26:50.836182","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hardware configuration","metadata":{"papermill":{"duration":0.028625,"end_time":"2020-12-02T01:26:58.553448","exception":false,"start_time":"2020-12-02T01:26:58.524823","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# TPU or GPU detection\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","metadata":{"_kg_hide-input":true,"papermill":{"duration":4.058857,"end_time":"2020-12-02T01:27:02.640767","exception":false,"start_time":"2020-12-02T01:26:58.58191","status":"completed"},"tags":[],"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"train_filepath = '/kaggle/input/commonlitreadabilityprize/train.csv'\ntest_filepath = '/kaggle/input/commonlitreadabilityprize/test.csv'\n\ntrain = pd.read_csv(train_filepath)\ntest = pd.read_csv(test_filepath)\n\nprint(f'Train samples: {len(train)}')\ndisplay(train.head())\n\nprint(f'Test samples: {len(test)}')\ndisplay(test.head())\n\n# removing unused columns\ntrain.drop(['url_legal', 'license'], axis=1, inplace=True)\ntest.drop(['url_legal', 'license'], axis=1, inplace=True)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model parameters","metadata":{"papermill":{"duration":0.029341,"end_time":"2020-12-02T01:27:02.69983","exception":false,"start_time":"2020-12-02T01:27:02.670489","status":"completed"},"tags":[]}},{"cell_type":"code","source":"BATCH_SIZE = 8 * REPLICAS\nLEARNING_RATE = 1e-5 * REPLICAS\nEPOCHS = 35\nES_PATIENCE = 7\nPATIENCE = 2\nN_FOLDS = 5\nSEQ_LEN = 256 #300\nBASE_MODEL = '/kaggle/input/huggingface-roberta/roberta-base/'","metadata":{"papermill":{"duration":0.040441,"end_time":"2020-12-02T01:27:02.769991","exception":false,"start_time":"2020-12-02T01:27:02.72955","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Auxiliary functions","metadata":{"papermill":{"duration":0.032096,"end_time":"2020-12-02T01:27:03.666229","exception":false,"start_time":"2020-12-02T01:27:03.634133","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Datasets utility functions\ndef custom_standardization(text):\n    text = text.lower() # if encoder is uncased\n    text = text.strip()\n    return text\n\n\ndef sample_target(features, target):\n    mean, stddev = target\n    sampled_target = tf.random.normal([], mean=tf.cast(mean, dtype=tf.float32), \n                                      stddev=tf.cast(stddev, dtype=tf.float32), dtype=tf.float32)\n    \n    return (features, sampled_target)\n    \n\ndef get_dataset(pandas_df, tokenizer, labeled=True, ordered=False, repeated=False, \n                is_sampled=False, batch_size=32, seq_len=128):\n    \"\"\"\n        Return a Tensorflow dataset ready for training or inference.\n    \"\"\"\n    text = [custom_standardization(text) for text in pandas_df['excerpt']]\n    \n    # Tokenize inputs\n    tokenized_inputs = tokenizer(text, max_length=seq_len, truncation=True, \n                                 padding='max_length', return_tensors='tf')\n    \n    if labeled:\n        dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']}, \n                                                      (pandas_df['target'], pandas_df['standard_error'])))\n        if is_sampled:\n            dataset = dataset.map(sample_target, num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']})\n        \n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset\n\n\ndef plot_metrics(history):\n    metric_list = list(history.keys())\n    size = len(metric_list)//2\n    fig, axes = plt.subplots(size, 1, sharex='col', figsize=(20, size * 5))\n    axes = axes.flatten()\n    \n    for index in range(len(metric_list)//2):\n        metric_name = metric_list[index]\n        val_metric_name = metric_list[index+size]\n        axes[index].plot(history[metric_name], label='Train %s' % metric_name)\n        axes[index].plot(history[val_metric_name], label='Validation %s' % metric_name)\n        axes[index].legend(loc='best', fontsize=16)\n        axes[index].set_title(metric_name)\n\n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":0.059666,"end_time":"2020-12-02T01:27:03.859184","exception":false,"start_time":"2020-12-02T01:27:03.799518","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n\n### Looking at a few examples","metadata":{"papermill":{"duration":0.032458,"end_time":"2020-12-02T01:27:04.047455","exception":false,"start_time":"2020-12-02T01:27:04.014997","status":"completed"},"tags":[]}},{"cell_type":"code","source":"display(train.head())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now the examples with the 5 lowest `target` values","metadata":{}},{"cell_type":"code","source":"display(train.sort_values(by=['target']).head())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now the examples with the 5 highest `target` values","metadata":{}},{"cell_type":"code","source":"display(train.sort_values(by=['target'], ascending=False).head())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I would say that just by simply looking at those samples may be a little hard to decide the `target` score for them, but the samples with the lowest `target` score seems to have a mixture of grammatical, semantics, and punctuation error, that indeed would make them less easy to reading and understanding.","metadata":{}},{"cell_type":"markdown","source":"## Label distribution","metadata":{"papermill":{"duration":0.106953,"end_time":"2020-12-02T01:27:21.185772","exception":false,"start_time":"2020-12-02T01:27:21.078819","status":"completed"},"tags":[]}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['target'], ax=ax)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `target` column follows a distribution close to normal, but we can see that we have much more samples with the negative score than positive, also the negative values get close to `-4` while positive only go as high as `2`.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['standard_error'], ax=ax)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `standard_error` column seems to have some outliers with values lower than `0.4`.","metadata":{}},{"cell_type":"code","source":"print(f\"standard_error values >= than 0.4: {len(train[train['standard_error'] >= 0.4])}\")\nprint(f\"standard_error values < than 0.4: {len(train[train['standard_error'] < 0.4])}\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10, 10))\nsns.scatterplot(x=train['target'], y=train['standard_error'], s=10, color=\".15\")\nsns.kdeplot(x=train['target'], y=train['standard_error'], levels=5, color=\"r\", linewidths=1)\nplt.ylim([0.4, None])\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that samples with extreme `target` values (closer to `-4` and `2`) usually have higher `standard_error`.","metadata":{}},{"cell_type":"markdown","source":"## `excerpt` text distribution\n\n### `excerpt` length","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\ntrain['excerpt_len'] = train['excerpt'].apply(lambda x : len(x))\ntrain['excerpt_wordCnt'] = train['excerpt'].apply(lambda x : len(x.split(' ')))\ntrain['excerpt_tokenCnt'] = train['excerpt'].apply(lambda x : len(tokenizer.encode(x, add_special_tokens=False)))\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['excerpt_len'], ax=ax).set_title('Excerpt length')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `excerpt` word count","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['excerpt_wordCnt'], ax=ax).set_title('Excerpt word count')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### `excerpt` token count (after using the tokenizer)","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(train['excerpt_tokenCnt'], ax=ax).set_title('Excerpt token count')\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.116994,"end_time":"2020-12-02T01:27:23.094852","exception":false,"start_time":"2020-12-02T01:27:22.977858","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def model_fn(encoder, seq_len=256):\n    input_ids = L.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n    input_attention_mask = L.Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    outputs = encoder({'input_ids': input_ids, \n                       'attention_mask': input_attention_mask})\n    \n    model = Model(inputs=[input_ids, input_attention_mask], outputs=outputs)\n\n    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(optimizer=optimizer, \n                  loss=losses.MeanSquaredError(), \n                  metrics=[metrics.RootMeanSquaredError()])\n    \n    return model\n\n\nwith strategy.scope():\n    encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n    model = model_fn(encoder, SEQ_LEN)\n    \nmodel.summary()","metadata":{"papermill":{"duration":0.125571,"end_time":"2020-12-02T01:27:23.333002","exception":false,"start_time":"2020-12-02T01:27:23.207431","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"papermill":{"duration":0.109701,"end_time":"2020-12-02T01:27:23.557148","exception":false,"start_time":"2020-12-02T01:27:23.447447","status":"completed"},"tags":[]}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\nskf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\noof_pred = []; oof_labels = []; history_list = []; test_pred = []\n\nfor fold,(idxT, idxV) in enumerate(skf.split(train)):\n    if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    print(f'\\nFOLD: {fold+1}')\n    print(f'TRAIN: {len(idxT)} VALID: {len(idxV)}')\n\n    # Model\n    K.clear_session()\n    with strategy.scope():\n        encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n        model = model_fn(encoder, SEQ_LEN)\n        \n    model_path = f'model_{fold}.h5'\n    es = EarlyStopping(monitor='val_root_mean_squared_error', mode='min', \n                       patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n    checkpoint = ModelCheckpoint(model_path, monitor='val_root_mean_squared_error', mode='min', \n                                 save_best_only=True, save_weights_only=True)\n\n    # Train\n    history = model.fit(x=get_dataset(train.loc[idxT], tokenizer, repeated=True, is_sampled=True, \n                                      batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        validation_data=get_dataset(train.loc[idxV], tokenizer, ordered=True, \n                                                    batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        steps_per_epoch=50, \n                        callbacks=[es, checkpoint], \n                        epochs=EPOCHS,  \n                        verbose=2).history\n      \n    history_list.append(history)\n    # Save last model weights\n    model.load_weights(model_path)\n    \n    # Results\n    print(f\"#### FOLD {fold+1} OOF RMSE = {np.min(history['val_root_mean_squared_error']):.4f}\")\n\n    # OOF predictions\n    valid_ds = get_dataset(train.loc[idxV], tokenizer, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    oof_labels.append([target[0].numpy() for sample, target in iter(valid_ds.unbatch())])\n    x_oof = valid_ds.map(lambda sample, target: sample)\n    oof_pred.append(model.predict(x_oof)['logits'])\n\n    # Test predictions\n    test_ds = get_dataset(test, tokenizer, labeled=False, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    x_test = test_ds.map(lambda sample: sample)\n    test_pred.append(model.predict(x_test)['logits'])","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"papermill":{"duration":9115.501878,"end_time":"2020-12-02T03:59:19.168964","exception":false,"start_time":"2020-12-02T01:27:23.667086","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model loss and metrics graph","metadata":{"papermill":{"duration":0.212373,"end_time":"2020-12-02T03:59:19.592426","exception":false,"start_time":"2020-12-02T03:59:19.380053","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for fold, history in enumerate(history_list):\n    print(f'\\nFOLD: {fold+1}')\n    plot_metrics(history)","metadata":{"_kg_hide-input":true,"papermill":{"duration":3.521043,"end_time":"2020-12-02T03:59:23.325415","exception":false,"start_time":"2020-12-02T03:59:19.804372","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model evaluation\n\nWe are evaluating the model on the `OOF` predictions, it stands for `Out Of Fold`, since we are training using `K-Fold` our model will see all the data, and the correct way to evaluate each fold is by looking at the predictions that are not from that fold.\n\n## OOF metrics","metadata":{"papermill":{"duration":0.221722,"end_time":"2020-12-02T03:59:23.776701","exception":false,"start_time":"2020-12-02T03:59:23.554979","status":"completed"},"tags":[]}},{"cell_type":"code","source":"y_true = np.concatenate(oof_labels)\ny_preds = np.concatenate(oof_pred)\n\n\nfor fold, history in enumerate(history_list):\n    print(f\"FOLD {fold+1} RMSE: {np.min(history['val_root_mean_squared_error']):.4f}\")\n    \nprint(f'OOF RMSE: {mean_squared_error(y_true, y_preds, squared=False):.4f}')","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.282584,"end_time":"2020-12-02T03:59:24.282463","exception":false,"start_time":"2020-12-02T03:59:23.999879","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Error analysis**, label x prediction distribution\n\nHere we can compare the distribution from the labels and the predicted values, in a perfect scenario they should align.","metadata":{}},{"cell_type":"code","source":"preds_df = pd.DataFrame({'Label': y_true, 'Prediction': y_preds[:,0]})\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(preds_df['Label'], ax=ax, label='Label')\nsns.distplot(preds_df['Prediction'], ax=ax, label='Prediction')\nax.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(data=preds_df, x='Label', y='Prediction', kind='reg', height=10)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test set predictions","metadata":{}},{"cell_type":"code","source":"submission = test[['id']]\nsubmission['target'] = np.mean(test_pred, axis=0)\nsubmission.to_csv('submission.csv', index=False)\ndisplay(submission.head(10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}