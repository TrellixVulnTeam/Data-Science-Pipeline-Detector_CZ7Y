{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom glob import glob\nimport gc\n\nimport random\n\nfrom sklearn.model_selection import KFold, train_test_split, StratifiedKFold\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas()\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nplt.style.use('seaborn-whitegrid')\nsns.set_palette('Set2')\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\n\nimport tensorflow.keras.backend as K\n\nimport os\nprint(os.listdir('../input/'))\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed = 0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 777\nseed_everything(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = '../input/commonlitreadabilityprize/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(base_dir + 'train.csv')\nprint(train.shape)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(base_dir + 'test.csv')\nprint(test.shape)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(base_dir + 'sample_submission.csv')\nprint(sub.shape)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of unique id in trainset: {train['id'].nunique()}\")\nprint(f\"Number of unique id in testset: {test['id'].nunique()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<code>__Distribution of Target__</code>","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(train['target'], shade = True, color = 'green')\nplt.axvline(train['target'].mean(), label = 'Mean', color = 'r', linewidth = 1, linestyle = '--')\nplt.axvline(train['target'].median(), label = 'Median', color = 'b', linewidth = 1, linestyle = '--')\nplt.legend();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<code>__Distribution of Standard Error__</code>\n\n- Not provided for testset","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(train['standard_error'], shade = True, color = 'grey')\nplt.axvline(train['standard_error'].mean(), label = 'Mean', color = 'r', linewidth = 1, linestyle = '--')\nplt.axvline(train['standard_error'].median(), label = 'Median', color = 'b', linewidth = 1, linestyle = '--')\nplt.legend();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['excerpt_len'] = train['excerpt'].apply(lambda x: len(str(x)))\ntrain['excerpt_wordlen'] = train['excerpt'].apply(lambda x: len(str(x).split(' ')))\n\ntest['excerpt_len'] = test['excerpt'].apply(lambda x: len(str(x)))\ntest['excerpt_wordlen'] = test['excerpt'].apply(lambda x: len(str(x).split(' ')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Max. word length in train - Excerpt: {train['excerpt_wordlen'].max()}\")\nprint(f\"Min. word length in train - Excerpt: {train['excerpt_wordlen'].min()}\")\nprint()\nprint(f\"Max. word length in train - Excerpt: {test['excerpt_wordlen'].max()}\")\nprint(f\"Min. word length in train - Excerpt: {test['excerpt_wordlen'].min()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The max word length is useful to determine the tokenizer's max_len ","metadata":{}},{"cell_type":"markdown","source":"<code>__Distribution of Text Lengths__</code>","metadata":{}},{"cell_type":"code","source":"plt.subplot(1, 2, 1)\nsns.distplot(train['excerpt_len'], bins = 50)\nplt.title('Train Character Length')\n\nplt.subplot(1, 2, 2)\nsns.distplot(train['excerpt_wordlen'], bins = 50)\nplt.title('Train Word Length');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(1, 2, 1)\nsns.distplot(test['excerpt_len'], bins = 50)\nplt.title('Test Character Length')\n\nplt.subplot(1, 2, 2)\nsns.distplot(test['excerpt_wordlen'], bins = 50)\nplt.title('Test Word Length');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['folds'] = -1\n\ntrain['bins'] = pd.cut(train['target'], bins = 6, labels = False)\n\nskf = StratifiedKFold(n_splits = 5)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X = train['excerpt'], y = train['bins'].values)):\n    train.loc[val_idx, 'folds'] = fold\n        \ntrain.drop('bins', axis = 1, inplace = True)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RoBERTa","metadata":{}},{"cell_type":"code","source":"import tokenizers\nfrom transformers import RobertaConfig, TFRobertaModel\nfrom transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n\nmax_len = 256\n\nroberta_path = '../input/tf-roberta/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tok = RobertaTokenizer.from_pretrained('../input/roberta-base')\ntok.vocab_size","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def roberta_encode(texts, tokenizer, max_len = max_len):\n    all_tokens = np.ones((len(texts), max_len), dtype = 'int32')\n    all_masks = np.zeros((len(texts), max_len), dtype = 'int32')\n    \n    for k, text in enumerate(texts):\n        encoded = tok.encode_plus(\n            text,                \n            add_special_tokens = True,\n            max_length = max_len,     \n            pad_to_max_length = True,\n            return_attention_mask = True,\n       )\n        #print(encoded['input_ids'])\n        #print(encoded['attention_mask'])\n        #For one sentence as input:\n        # <s> ...word tokens... </s\n        \n        # bos_token_id <s>: 0\n        # eos_token_id </s>: 2\n        # sep_token_id </s>: 2\n        # pad_token_id <pad>: 1\n        \n        #Roberta does not use token_type_ids like BERT does.\n        #So there's no need to create token_type_ids.\n        \n        all_tokens[k, :max_len] = encoded['input_ids']\n        all_masks[k, :max_len] = encoded['attention_mask']\n    return all_tokens, all_masks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_roberta(max_len = max_len):\n    input_word_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_word_ids\")\n    input_mask = Input(shape = (max_len,), dtype = tf.int32, name = \"input_mask\")\n    \n    config = RobertaConfig.from_pretrained(roberta_path + 'config-roberta-base.json')\n    \n    roberta_model = TFRobertaModel.from_pretrained(roberta_path + 'pretrained-roberta-base.h5', \n                                                                       config = config)\n    \n    x = roberta_model([input_word_ids, input_mask])[0]\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    out = tf.keras.layers.Dense(1, activation = 'linear')(x)\n    \n    model = Model(inputs = [input_word_ids, input_mask], outputs = out)\n    \n    model.compile(Adam(lr = 1e-5), loss = tf.keras.losses.MeanSquaredError(), \n                  metrics = tf.keras.metrics.RootMeanSquaredError())\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain = roberta_encode(train['excerpt'].values, tok, max_len = max_len)\nytrain = train['target'].values\n\nprint(Xtrain[0].shape, ytrain.shape)\n\nK.clear_session()\nmodel = build_roberta(max_len = max_len)\n\n#model.trainable = True\n\ncheck = ModelCheckpoint(f'roberta_model.h5', monitor = 'val_loss', verbose = 1, save_best_only = True,\n    save_weights_only = True, mode = 'auto', save_freq = 'epoch')\n\nhistory = model.fit(Xtrain, ytrain, epochs = 4, batch_size = 8, \n          verbose = 1, callbacks = [check], \n          validation_split = 0.2\n         )\n\nprint('Loading model...')\nmodel.load_weights(f'roberta_model.h5')\n\nXtest = roberta_encode(test['excerpt'].values, tok, max_len = max_len)\nprint(Xtest[0].shape)\n\nprint('Predicting Test...')\npreds = model.predict(Xtest, verbose = 1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'] = np.mean(preds, axis = 1)\nsub.to_csv('./submission.csv', index = False)\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(1, 2, 1)\nsns.kdeplot(train['target'], shade = True, color = 'green')\nplt.axvline(train['target'].mean(), label = 'Mean', color = 'r', linewidth = 1, linestyle = '--')\nplt.axvline(train['target'].median(), label = 'Median', color = 'b', linewidth = 1, linestyle = '--')\nplt.legend()\nplt.title('Train Target')\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(sub['target'], shade = True, color = 'blue')\nplt.axvline(sub['target'].mean(), label = 'Mean', color = 'r', linewidth = 1, linestyle = '--')\nplt.axvline(sub['target'].median(), label = 'Median', color = 'b', linewidth = 1, linestyle = '--')\nplt.legend()\nplt.title('Predicted Target');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}