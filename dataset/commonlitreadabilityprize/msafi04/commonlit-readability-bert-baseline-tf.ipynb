{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Competition Challenge\n\n- In this competition, we are required to build models to rate the complexity of reading passages for grade 3 to 12 class students\n\n# Evaluation Metrics\n\n- Root Mean Squared Error (RMSE)","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom glob import glob\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nimport plotly.express as px #Plotly Express\n\nfrom plotly.offline import iplot\n#to link plotly to pandas\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline = False, world_readable = True)\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas()\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nplt.style.use('seaborn-whitegrid')\nsns.set_palette('Set2')\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\n\nimport os\nprint(os.listdir('../input/'))\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = '../input/commonlitreadabilityprize/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(base_dir + 'train.csv')\nprint(train.shape)\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(base_dir + 'test.csv')\nprint(test.shape)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(base_dir + 'sample_submission.csv')\nprint(sub.shape)\nsub.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of unique id in trainset: {train['id'].nunique()}\")\nprint(f\"Number of unique id in testset: {test['id'].nunique()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check for number of missing values","metadata":{}},{"cell_type":"code","source":"missing = train.isna().sum().reset_index()\nmissing.columns = ['features', 'total_missing']\nmissing['percent'] = (missing['total_missing'] / len(train)) * 100\nmissing.index = missing['features']\ndel missing['features']\n\nmissing['total_missing'].iplot(kind = 'bar', \n                               title = 'Missing Values Count in train',\n                               xTitle = 'Features',\n                               colors = 'blue',\n                               yTitle = 'Count')\nmissing.T","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing = test.isna().sum().reset_index()\nmissing.columns = ['features', 'total_missing']\nmissing['percent'] = (missing['total_missing'] / len(test)) * 100\nmissing.index = missing['features']\ndel missing['features']\n\nmissing['total_missing'].iplot(kind = 'bar', \n                               title = 'Missing Values Count in test',\n                               xTitle = 'Features',\n                               colors = 'red',\n                               yTitle = 'Count')\nmissing.T","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As per competition, URL and License are blank in testset","metadata":{}},{"cell_type":"markdown","source":"<code>__Distribution of Target__</code>","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(train['target'], shade = True, color = 'green');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<code>__Distribution of Standard Error__</code>\n\n- Not provided for testset","metadata":{}},{"cell_type":"code","source":"sns.kdeplot(train['standard_error'], shade = True, color = 'grey');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['excerpt_len'] = train['excerpt'].apply(lambda x: len(str(x)))\ntrain['excerpt_wordlen'] = train['excerpt'].apply(lambda x: len(str(x).split(' ')))\n\ntest['excerpt_len'] = test['excerpt'].apply(lambda x: len(str(x)))\ntest['excerpt_wordlen'] = test['excerpt'].apply(lambda x: len(str(x).split(' ')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Max. word length in train - Excerpt: {train['excerpt_wordlen'].max()}\")\nprint(f\"Min. word length in train - Excerpt: {train['excerpt_wordlen'].min()}\")\nprint()\nprint(f\"Max. word length in train - Excerpt: {test['excerpt_wordlen'].max()}\")\nprint(f\"Min. word length in train - Excerpt: {test['excerpt_wordlen'].min()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The max word length is useful to determine the tokenizer's max_len ","metadata":{}},{"cell_type":"markdown","source":"<code>__Distribution of Text Lengths__</code>","metadata":{}},{"cell_type":"code","source":"plt.subplot(1, 2, 1)\nsns.distplot(train['excerpt_len'], bins = 50)\nplt.title('Train Character Length')\n\nplt.subplot(1, 2, 2)\nsns.distplot(train['excerpt_wordlen'], bins = 50)\nplt.title('Train Word Length');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(1, 2, 1)\nsns.distplot(test['excerpt_len'], bins = 50)\nplt.title('Test Character Length')\n\nplt.subplot(1, 2, 2)\nsns.distplot(test['excerpt_wordlen'], bins = 50)\nplt.title('Test Word Length');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nimport collections\nfrom collections import Counter\n\nfrom nltk.corpus import stopwords\n\nimport re\nfrom wordcloud import WordCloud\n\ndef plot_wordcloud(data, col, text = None):\n    stop = stopwords.words('english')\n    all_words = [word for each in data[col] for word in each.split(' ') if word not in stop]\n    word_freq = Counter(all_words)\n\n    wordcloud = WordCloud(width = 900,\n                          height = 500,\n                          max_words = 200,\n                          max_font_size = 100,\n                          relative_scaling = 0.5,\n                          background_color = \"rgba(255, 255, 255, 0)\", \n                          mode = \"RGBA\",\n                          normalize_plurals = True).generate_from_frequencies(word_freq)\n    plt.figure(figsize = (18, 16))\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.title(text, fontsize = 20, color = 'grey', y = 1.05)\n    plt.axis(\"off\")\n    plt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_wordcloud(train, 'excerpt', 'WordCloud of Train - Excerpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_wordcloud(test, 'excerpt', 'WordCloud of Test - Excerpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize","metadata":{}},{"cell_type":"code","source":"from transformers import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'bert-large-uncased'\nbert_url = '../input/bert-base-uncased-huggingface-transformer/'\n\nmax_len = train['excerpt_wordlen'].max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(bert_url + 'bert-base-uncased-vocab.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub\ndef bert_encode(texts, tokenizer, max_len = max_len):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        \n        text = text[:max_len - 2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain = bert_encode(train['excerpt'].values, tokenizer, max_len = max_len)\ntargets = train['target'].values\n\nprint(Xtrain[0].shape, targets.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtest = bert_encode(test['excerpt'].values, tokenizer, max_len = max_len)\nprint(Xtest[0].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# __Train Model__","metadata":{}},{"cell_type":"code","source":"epochs = 10","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(max_len = max_len):\n    input_word_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_word_ids\")\n    input_mask = Input(shape = (max_len,), dtype = tf.int32, name = \"input_mask\")\n    segment_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"segment_ids\")\n    \n    config = BertConfig()\n    config.output_hidden_states = False\n    \n    bert_model = TFBertModel.from_pretrained(\n        bert_url + 'bert-base-uncased-tf_model.h5', config = config)\n\n    sequence_output = bert_model([input_word_ids, input_mask, segment_ids])[0]\n    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    \n    out = tf.keras.layers.Dense(1, activation = 'linear')(x)\n    \n    model = Model(inputs = [input_word_ids, input_mask, segment_ids], outputs = out)\n    \n    model.compile(Adam(lr = 1e-5), loss = tf.keras.losses.MeanSquaredError())\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early = EarlyStopping(monitor = 'val_loss', min_delta = 0., patience = 2,\n                   verbose = 1, mode = 'min', restore_best_weights = True)\ncheck = ModelCheckpoint(filepath = 'commonlit_model.h5', monitor = 'val_loss', verbose = 1, \n                                               ave_weights_only = True)\nreduce = ReduceLROnPlateau(monitor = 'val_loss', patience = 2, verbose = 1, factor = 0.5)\n\nmodel = build_model(max_len = max_len)\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(Xtrain, targets, validation_split = 0.2, epochs = epochs, batch_size = 16, \n                   callbacks = [reduce])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_training_curves(training, validation, title, subplot):\n    \"\"\"\n    Source: https://www.kaggle.com/mgornergoogle/getting-started-with-100-flowers-on-tpu\n    \"\"\"\n    plt.subplots(figsize = (10, 10), facecolor = '#F0F0F0')\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history.history.keys()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_training_curves(\n                    history.history['loss'], \n                    history.history['val_loss'], \n                    'loss', 211)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(Xtest, verbose = 1)\npreds[:10]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'] = preds\nsub.to_csv('./submission.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(1, 2, 1)\nsns.kdeplot(train['target'], shade = True, color = 'green');\nplt.title('Train Target')\n\nplt.subplot(1, 2, 2)\nsns.kdeplot(sub['target'], shade = True, color = 'blue');\nplt.title('Predicted Target');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","metadata":{},"execution_count":null,"outputs":[]}]}