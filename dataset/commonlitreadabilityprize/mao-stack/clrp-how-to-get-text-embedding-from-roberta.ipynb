{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this Notebook,  I introduce how to get text embedding from RoBERTa (/BERT/ALBERT/etc.).  \nThere are 2 methods to get text embedding from RoBERTa.\n1. get CLS Token\n2. pool RoBERTa output (RoBERTa output = word embeddings)  \n  \nIf I make mistakes, please let me know in the comments.","metadata":{}},{"cell_type":"markdown","source":"このnotebookでは、RoBERTa (またはALBERT, BERTなど) を使って文章のベクトル化 (text embedding) を行う方法を紹介する。  \nRoBERTaを使ったtext embeddingには2種類の方法が提案されている。  \n1. CLSトークンを取得してそれを文章の埋め込みベクトルと見做す方法\n2. RoBERTaの出力をプーリングする方法 (ここでいう出力とは単語埋め込みベクトルたちのこと)  \n  \nもし間違えている個所があったらコメントで教えてください","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport transformers\nfrom transformers import RobertaModel, RobertaTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Settings:\n    batch_size=16\n    max_len=350\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    seed = 318","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.backends.cudnn.deterministic = True\n    \nset_seed(Settings.seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class TrainValidDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.text = df[\"excerpt\"].values\n        self.target = df[\"target\"].values\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        texts = self.text[idx]\n        tokenized = self.tokenizer.encode_plus(texts, truncation=True, add_special_tokens=True,\n                                               max_length=self.max_len, padding=\"max_length\")\n        ids = tokenized[\"input_ids\"]\n        mask = tokenized[\"attention_mask\"]\n        targets = self.target[idx]\n        return {\n            \"ids\": torch.LongTensor(ids),\n            \"mask\": torch.LongTensor(mask),\n            \"targets\": torch.tensor(targets, dtype=torch.float32)\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class CommonLitRoBERTa(nn.Module):\n    def __init__(self, pretrained_path):\n        super().__init__()\n        self.roberta = RobertaModel.from_pretrained(pretrained_path)\n        \n    def forward(self, ids, mask):\n        output = self.roberta(ids, attention_mask=mask)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CommonLitRoBERTa(\"../input/roberta-transformers-pytorch/roberta-base\")\nmodel.to(Settings.device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get Text Embeddings","metadata":{}},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(\"../input/roberta-transformers-pytorch/roberta-base\")\ntokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare dataset\n# データセットを準備\ndf_train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\n\ntrain_dataset = TrainValidDataset(df_train, tokenizer, Settings.max_len)\ntrain_loader = DataLoader(train_dataset, batch_size=Settings.batch_size,\n                          shuffle=True, num_workers=8, pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make mini batch data\n# ミニバッチデータを作る\nbatch = next(iter(train_loader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids = batch[\"ids\"].to(Settings.device)\nmask = batch[\"mask\"].to(Settings.device)\ntargets = batch[\"targets\"].to(Settings.device)\n\nprint(ids.shape)\nprint(mask.shape)\nprint(targets.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"16 = num of texts, 350 = num of word tokens in a text  \n16 = 文章の数、350 = 1文の中にある単語tokenの数","metadata":{}},{"cell_type":"code","source":"output = model(ids, mask)\noutput","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2 outputs (last_hidden_state, pooler_output) from RoBERTa  \n2つの出力がRoBERTaから吐き出される","metadata":{}},{"cell_type":"code","source":"# last_hidden_state\nlast_hidden_state = output[0]\nprint(\"shape:\", last_hidden_state.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"16 = num of texts, 350 = num of tokens in a text, 768 = dimension of word embedding  \n16 = 文章の数、350 = 1文中の単語tokenの数、768 = 単語埋め込みの次元数","metadata":{}},{"cell_type":"code","source":"# pooler output\npooler_output = output[1]\nprint(\"shape:\", pooler_output.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"meaning of pooler output will be explained later.  \npooler outputの意味は後で説明する。","metadata":{}},{"cell_type":"markdown","source":"## 1. Get CLS Token","metadata":{}},{"cell_type":"markdown","source":"![get cls token](https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png)","metadata":{}},{"cell_type":"code","source":"# .detach() = make copies and remove gradient information  \n# .detach() = 勾配情報を除外してテンソルをコピー\ncls_embeddings = last_hidden_state[:, 0, :].detach()\n\nprint(\"shape:\", cls_embeddings.shape)\nprint(\"\")\nprint(cls_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"16 = num of texts, 768 = dimension of text embedding  \n16 = 文章の数、768 = text embeddingの次元数","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(cls_embeddings.numpy()).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Pool RoBERTa Output","metadata":{}},{"cell_type":"markdown","source":"use last_hidden_state  \nlast_hidden_stateを使う","metadata":{}},{"cell_type":"code","source":"last_hidden_state.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply avg.pooling to word embeddings\n# 単語埋め込みベクトルにaverage pooling を適用する\npooled_embeddings = last_hidden_state.detach().mean(dim=1)\n\nprint(\"shape:\", pooled_embeddings.shape)\nprint(\"\")\nprint(pooled_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(pooled_embeddings.numpy()).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"note!: pooler output \"not\" equal pooled_embeddings we calculated  \nWhat is pooler output ?  \n-> It takes the representation from the [CLS] token from top layer of RoBERTa encoder, and feed that through another dense layer.  \nreference: https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/modeling.py#L224-L232  \n  \n注： pooler outputは今私たちが計算したpooled_embeddingsとは全くの別物。  \nなら、pooler outputとは何か？\n-> 1. Get CLS Token で取得したCLSトークンを別のdense layerに通したもの。  \n参照: https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/modeling.py#L224-L232","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}