{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### âš™ Basic text pre-processing and new dataset generation\nIn this notebook, we will use the most common methods of text pre-processing to reduce the number of features across the train and test sets and homogenise the input. ","metadata":{}},{"cell_type":"markdown","source":"Boilerpate follows","metadata":{}},{"cell_type":"code","source":"!pip install -q ../input/ftfywhl602/ftfy-6.0.2-py2.py3-none-any.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q ../input/docoptwhl062/docopt-0.6.2-py2.py3-none-any.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q ../input/num2wordswhl0510/num2words-0.5.10-py2.py3-none-any.whl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nimport ftfy\nimport pandas as pd\n\nfrom string import punctuation\nfrom num2words import num2words\nfrom urllib.parse import unquote\nfrom string import punctuation\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim import corpora, models, similarities\nfrom nltk import word_tokenize, sent_tokenize\nfrom wordsegment import load, segment\n\nlemmatizer = WordNetLemmatizer()\nload()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For convenience, combine the two\ntrain_df['is_train'] = True\ntest_df['is_train'] = False\n\ndf = pd.concat([train_df, test_df], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transformations","metadata":{}},{"cell_type":"markdown","source":"Convert to lower case. This is the most common of transformation and is pretty self-explanatory","metadata":{}},{"cell_type":"code","source":"df['excerpt'] = df['excerpt'].apply(lambda x: str(x).lower().replace('\\\\', '').replace('_', ' '))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In case there is unicode text in the input, this will fix inconsistencies and glitches in it, such as mojibake (text that was decoded in the wrong encoding).","metadata":{}},{"cell_type":"code","source":"df['excerpt'] = df['excerpt'].apply(lambda x: ftfy.fix_text(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Double spaces are erroneous in our context. Remove them","metadata":{}},{"cell_type":"code","source":"df['excerpt'][df['excerpt'].str.contains(\"  \")]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_multiple_spaces(text):\n    text = re.sub('\\s+',  ' ', text)\n    return text\n\ndf['excerpt'] = df['excerpt'].apply(lambda x: remove_multiple_spaces(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Contractions. The ability to replace contractions like \"don't\" with \"do not\" and \"we've\" with \"we have\" can help us reveal tokens/features that are hidden in such a way. Normally, I would use the [pycontractions](https://pypi.org/project/pycontractions/) library, but it does not support python 3.7. Regardless, the way to use this goes as follows:\n\n```\nfrom pycontractions import Contractions\n\ncont = Contractions(api_key=\"text8\")\ndf['excerpt'].apply(lambda x: list(cont.expand_texts([x]))[0])\n```\n\nFor the purposes of this demo, we can write our own function like so:","metadata":{}},{"cell_type":"code","source":"def decontraction(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndf['excerpt'] = df['excerpt'].apply(lambda x: decontraction(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Punctuation should also be irrelevant in terms of modelling for readability difficulty. Remove them","metadata":{}},{"cell_type":"code","source":"def remove_punct(text):\n    new_punct = re.sub('\\ |\\!|\\?', '', punctuation)\n    table=str.maketrans('', '', new_punct)\n    return text.translate(table)\n\ndf['excerpt'] = df['excerpt'].apply(lambda x: remove_punct(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Words are typically inflected (e.g., letters suffixed, affixed, etc.) to express their forms (e.g., plural, tense, etc.). Dog -> Dogs is an example of inflection. Usually, words must be compared in their native forms for effective text matching.\n\nLemmatization is one method used to convert a word to a non-inflected form, i.e. reduce a word to its most native form.\n\nIt uses a simple mechanism that removes or modifies inflections to form the root word, but the root word may not be a valid word in the language. Also, it removes or modifies the inflections to form the root word, but the root word needs to be a valid word in the language.","metadata":{}},{"cell_type":"code","source":"def lemma(text):\n    words = word_tokenize(text)\n    return ' '.join([lemmatizer.lemmatize(w, pos='v') for w in words])\n\ndf['excerpt'] = df['excerpt'].apply(lambda x: lemma(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, any numbers should be replaced with their letter equivalents.","metadata":{}},{"cell_type":"code","source":"df['excerpt'] = df['excerpt'].apply(lambda x: re.sub(r\"(\\d+)\", lambda s: num2words(int(s.group(0))), x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save back the datasets","metadata":{}},{"cell_type":"code","source":"df[df['is_train']].iloc[:, :-1].to_csv(\"train.csv\", index=False)\ndf[~df['is_train']].iloc[:, :-3].to_csv(\"test.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Latent semantic analysis. \n\nLet's look at how well our dataset has been separated, based on abstract, high level topics using LDA analysis","metadata":{}},{"cell_type":"markdown","source":"**Workflow**\nCreate a dictionary and corpus\n  * Dictionary: use words from all of the tweets for more tokens\n  * Corpus: composed of the tweets with the greatest certainty in their classification.","metadata":{}},{"cell_type":"code","source":"# Takes as input the tweet dataframe, dictionary, corpus and dimensions for the tweets and returns \n# a new dataframe with each tweet characterized by the new lower dimensional features\n# also returns the topics if desired\ndef latent_semantic_analysis(df, dictionary, corpus_tfidf, dimensions, return_topics = False, n_topics = 10, n_words = 10):\n    # Create a lsi wrapper around the tfidf wrapper\n    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=dimensions, power_iters=10)\n    corpus_lsi = lsi[corpus_tfidf]\n    \n    #create the features for a new dataframe\n    features = []\n    for doc in corpus_lsi:\n        features.append(remove_doc_label(doc))\n        \n    # Create a new dataframe with the features\n    df_features = pd.DataFrame(data = features)\n    \n    #return the new features dataframe devoid of columns that contain nothing\n    if return_topics:\n        return (df_features.fillna(0), lsi.print_topics(n_topics, num_words = n_words), lsi)\n    else:\n        return df_features.fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Makes the gensim dictionary and corpus\ndef make_dictionary_and_corpus(df):\n    # The tokenized and stemmed data form our texts database \n    texts = df.copy()\n    \n    # Check how frequently a given word appears and remove it if only one occurrence\n    frequency = defaultdict(int)\n    for text in texts:\n        for token in text:\n            frequency[token] += 1\n    texts = [[token for token in text if frequency[token] > 1] for text in texts]\n    \n    # Create a gensim dictionary\n    dictionary = corpora.Dictionary(texts)\n    \n    # Create a new texts of only the ones I will analyze\n    texts = df.copy()    \n    \n    # Create the bag of words corpus\n    corpus = [dictionary.doc2bow(text) for text in texts]\n    #corpus = [token_word2vec_map(text, frequency) for text in texts]\n    \n    # Create a tfidf wrapper and convert the corpus to a tfidf format\n    tfidf = models.TfidfModel(corpus)\n    corpus_tfidf = tfidf[corpus]\n    \n    # Return a tuple with the dictionary and corpus\n    return (dictionary, corpus_tfidf, corpus, tfidf)\n\n#clean the features for use in dataframe\ndef remove_doc_label(doc):\n    cleaned_doc = []\n    for element in doc:\n        cleaned_doc.append(element[1])\n    return cleaned_doc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom collections import defaultdict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['excerpt_tokenized'] = df['excerpt'].apply(\n    lambda excerpt: [word for word in word_tokenize(excerpt) if word not in stopwords.words('english')]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate the dictionary and the corpus for our tweets\ntext_col = \"excerpt_tokenized\"\ndictionary, corpus_tfidf, corpus_bow, tfidf = make_dictionary_and_corpus(df[text_col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dimensions = 43\ndf_lsi_features, topics, lsi = latent_semantic_analysis(df, dictionary, corpus_tfidf, dimensions, True, 15, 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check out the \"topics\"**\n\n  1. Print out the top 15 topics with the top 20 tokens they are composed of\n  2. Plot some topics against each other with colors to indicate class","metadata":{}},{"cell_type":"code","source":"for topic in topics:\n    print(\"Topic %d:\" % topic[0])\n    print(topic[1] + \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot topics**\n\nTopic 0 and topic 1 seem to be easily readable, as they contain easier words like old, man and mother, while topic 9 a lot less so, as it includes words such as bacteria, DNA and species. Let's verify this with a plot..","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set the two topics\nfeature_0 = 0\nfeature_1 = 1\nfeature_2 = 9\n\n# Extract the data for plotting\nfeature_0 = df_lsi_features[feature_0]\nfeature_1 = df_lsi_features[feature_1]\nfeature_2 = df_lsi_features[feature_2]\n\n# Things to plot\nplt.scatter(feature_0, feature_1, c=\"b\", s=40, alpha=0.3, linewidths=0.0, label = \"More readable\")\nplt.scatter(feature_0, feature_2, c=\"r\", s=40, alpha=0.3, linewidths=0.0, label = \"Less readable\")\n\n#backround grid details\naxes = plt.gca()\naxes.grid(b = True, which = 'both', axis = 'both', color = 'gray', linestyle = '-', alpha = 0.5, linewidth = 0.5) \n# axes.set_axis_bgcolor('white')  \n\n#font scpecifications\ntitle_font = {'family' : 'arial', 'color'  : 'black', 'weight' : 'heavy','size': 20}\naxis_label_font = {'family' : 'arial', 'color'  : 'black', 'weight' : 'normal','size': 20}\n\n#figure size and tick style\nplt.rcParams[\"figure.figsize\"] = [6,6]\nplt.rc('axes',edgecolor='black',linewidth=1)\nplt.tick_params(which='both', axis='both', color='black', length=4, width=0.5)\nplt.rcParams['xtick.direction'] = 'in'\nplt.rcParams['ytick.direction'] = 'in'\n\n#axis range and labels (also specify if log or not)\nplt.xlim(0.0, 0.4)\n#plt.xscale('log')\nplt.ylim(-0.3, 0.3)\nplt.xlabel(r'Origin topic', y=3, fontsize=20, fontdict = axis_label_font)\nplt.ylabel(r'Target topic', fontsize=20, fontdict = axis_label_font)\n\n#title and axis labels\nplt.tick_params(axis='both', labelsize=20)\nplt.title('Features', y=1.05, fontdict = title_font)\n\n#legend details\nlegend = plt.legend(shadow = True, frameon = True, fancybox = False, ncol = 1, fontsize = 15, loc = 'lower left')\nframe = legend.get_frame()\n#frame.set_width(100)\nframe.set_facecolor('white')\nframe.set_edgecolor('black')\n    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, the topics match our intuition based on what tokens they contain! The readable topics are more spread on the x and y axis in comparison with non-readable topics, which remained \"clumped\" near the origin. With more topics and text augmentations applied to enrich the input dataset, good separation is possible.","metadata":{}},{"cell_type":"markdown","source":"### Modelling","metadata":{}},{"cell_type":"markdown","source":"The following were copied in from https://www.kaggle.com/hengzheng/simpletransformers-regression-starter-less-code. Many thanks @hengzheng ","metadata":{}},{"cell_type":"code","source":"!pip install -q /kaggle/input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install -q simpletransformers==0.51.0 --no-index --find-links=file:///kaggle/input/simpletransformers/simpletransformers-0.51.0\n\nfrom sklearn.model_selection import train_test_split\nfrom simpletransformers.classification import ClassificationModel, ClassificationArgs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = df[df['is_train']][['excerpt', 'target']].copy()\ntrain.columns = ['text', 'labels']\ntrain_df, valid_df = train_test_split(train, test_size=0.01, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_args = ClassificationArgs()\nmodel_args.max_seq_length = 300\nmodel_args.num_train_epochs = 5\nmodel_args.regression = True\nmodel_args.no_save = True\nmodel_args.save_model_every_epoch = False\nmodel_args.save_steps = -1\n\nmodel = ClassificationModel(\n    \"roberta\",\n    \"../input/robertalarge\",\n    num_labels=1,\n    args=model_args\n)\n\nmodel.train_model(train_df)\n\nresult, model_outputs, wrong_predictions = model.eval_model(valid_df)\nprint(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = df[~df['is_train']][['id', 'excerpt']].copy()\ntest.columns = ['id', 'text']\n\npredictions, _ = model.predict(test['text'].values)\ntest['target'] = predictions\n\ntest[['id', 'target']].to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Upvote if it was remotely helpful! More to come :)","metadata":{}}]}