{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a baseline model based on BERT for the CommonLitReadabilityPrize competition. The implementation refers to [BERT beginner](https://www.kaggle.com/chumajin/pytorch-bert-beginner-s-room/notebook).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Table of Contents\n1. [Preparation](#preparation)\n2. [K-fold](#k-fold)\n3. [Model and Training](#model-and-training)\n4. [Testing](#testing)\n5. [Scoring](#scoring)","metadata":{}},{"cell_type":"markdown","source":"# 1. Preparation\n**Import all dependencies**","metadata":{}},{"cell_type":"markdown","source":"We import all dependencies here and define the model path","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport transformers\nfrom transformers import AdamW","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:01.911767Z","iopub.execute_input":"2021-07-12T13:12:01.912126Z","iopub.status.idle":"2021-07-12T13:12:06.951207Z","shell.execute_reply.started":"2021-07-12T13:12:01.912069Z","shell.execute_reply":"2021-07-12T13:12:06.950378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gpu/cpu\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:06.958104Z","iopub.execute_input":"2021-07-12T13:12:06.958464Z","iopub.status.idle":"2021-07-12T13:12:06.995093Z","shell.execute_reply.started":"2021-07-12T13:12:06.958428Z","shell.execute_reply":"2021-07-12T13:12:06.994237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model loading path\nMODEL_PATH = '../input/huggingface-bert/bert-base-uncased'","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:06.997176Z","iopub.execute_input":"2021-07-12T13:12:06.997811Z","iopub.status.idle":"2021-07-12T13:12:07.008423Z","shell.execute_reply.started":"2021-07-12T13:12:06.997774Z","shell.execute_reply":"2021-07-12T13:12:07.007492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading training data**","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntrain","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:07.009826Z","iopub.execute_input":"2021-07-12T13:12:07.010195Z","iopub.status.idle":"2021-07-12T13:12:07.074288Z","shell.execute_reply.started":"2021-07-12T13:12:07.010157Z","shell.execute_reply":"2021-07-12T13:12:07.073406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will only use the \"excerpt\" and \"target\" to train the model. So we merely extract the column \"excerpt\" and \"target\" from raw data.","metadata":{}},{"cell_type":"code","source":"train_data = train.iloc[:,3]\ntrain_target = train.iloc[:,4]\n# for visualization\ntrain_set = pd.concat([train_data,train_target], axis=1)\ntrain_set","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:07.075549Z","iopub.execute_input":"2021-07-12T13:12:07.075907Z","iopub.status.idle":"2021-07-12T13:12:07.093047Z","shell.execute_reply.started":"2021-07-12T13:12:07.075876Z","shell.execute_reply":"2021-07-12T13:12:07.092002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can partition the train data into train set and test set. So we can use the test set to check the performance during implementing the model. The test set is not the data in \"test.csv\".","metadata":{}},{"cell_type":"code","source":"num_data = len(train_set)\nmsk = np.random.rand(num_data)<1 # we can change it to 1 to make all train data as the train set\ntraining = train_data[msk]\ntesting = train_data[~msk]\ntraining_target = train_target[msk]\ntesting_target = train_target[~msk]\ntrain_sample = pd.concat([training, training_target], axis=1)\ntest_sample = pd.concat([testing, testing_target], axis=1)\ntrain_sample, test_sample","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:07.095113Z","iopub.execute_input":"2021-07-12T13:12:07.096043Z","iopub.status.idle":"2021-07-12T13:12:07.113514Z","shell.execute_reply.started":"2021-07-12T13:12:07.095999Z","shell.execute_reply":"2021-07-12T13:12:07.11272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. K-fold","metadata":{}},{"cell_type":"markdown","source":"We implement the k-fold with 5 folds. Each iteration, we use only one fold as the validation data, and the rest as the training data.","metadata":{}},{"cell_type":"code","source":"train_sample = train_sample.values","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:07.116416Z","iopub.execute_input":"2021-07-12T13:12:07.116688Z","iopub.status.idle":"2021-07-12T13:12:07.122168Z","shell.execute_reply.started":"2021-07-12T13:12:07.116662Z","shell.execute_reply":"2021-07-12T13:12:07.121073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# partition data into 5 folds\nkf = KFold(shuffle=True)\n\n# check the data partition\nfor train_index, valid_index in kf.split(train_sample):\n    print(len(train_sample[train_index]))\n    print(len(train_sample[valid_index]))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:07.126286Z","iopub.execute_input":"2021-07-12T13:12:07.126589Z","iopub.status.idle":"2021-07-12T13:12:07.1393Z","shell.execute_reply.started":"2021-07-12T13:12:07.126544Z","shell.execute_reply":"2021-07-12T13:12:07.138458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we will tokenize the data and make a dataloader for mini-batch training.","metadata":{}},{"cell_type":"code","source":"# tokenizer from BERT\ntokenizer = transformers.BertTokenizer.from_pretrained(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:07.14198Z","iopub.execute_input":"2021-07-12T13:12:07.14224Z","iopub.status.idle":"2021-07-12T13:12:07.27438Z","shell.execute_reply.started":"2021-07-12T13:12:07.142215Z","shell.execute_reply":"2021-07-12T13:12:07.273432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a dataset","metadata":{}},{"cell_type":"code","source":"class BERTDataSet(Dataset):\n    \n    def __init__(self,excerpts,targets):\n        \n        self.excerpts = excerpts\n        self.targets = targets\n        \n    def __len__(self):\n        \n        return len(self.excerpts)\n    \n    def __getitem__(self,idx):\n        \n        excerpt = self.excerpts[idx]\n        \n        bert_excerpts = tokenizer.encode_plus(\n                                excerpt,\n                                add_special_tokens = True, \n                                max_length = 314,\n                                # pad_to_max_length = True, \n                                padding='max_length',\n                                return_attention_mask = True,\n                                truncation=True)\n\n        ids = torch.tensor(bert_excerpts['input_ids'], dtype=torch.long)\n        mask = torch.tensor(bert_excerpts['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(bert_excerpts['token_type_ids'], dtype=torch.long)\n     \n            \n        target = torch.tensor(self.targets[idx],dtype=torch.float)\n        \n        return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'targets': target\n            }","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:07.275891Z","iopub.execute_input":"2021-07-12T13:12:07.276259Z","iopub.status.idle":"2021-07-12T13:12:07.284561Z","shell.execute_reply.started":"2021-07-12T13:12:07.276221Z","shell.execute_reply":"2021-07-12T13:12:07.283432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model and Training\n\nDefine the optimizer and the model","metadata":{}},{"cell_type":"code","source":"LR=2e-5\n# model = transformers.BertForSequenceClassification.from_pretrained(MODEL_PATH,num_labels=1)\n# model.to(device)\n# optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2)\n# model_original_stat_dict = model.state_dict() # store the original weight matrix","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:07.286084Z","iopub.execute_input":"2021-07-12T13:12:07.286521Z","iopub.status.idle":"2021-07-12T13:12:07.297471Z","shell.execute_reply.started":"2021-07-12T13:12:07.286485Z","shell.execute_reply":"2021-07-12T13:12:07.296638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# minibatch for dataloader and training epochs\nbatchSize = 16\nepochs = 20","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:07.300433Z","iopub.execute_input":"2021-07-12T13:12:07.300849Z","iopub.status.idle":"2021-07-12T13:12:07.305847Z","shell.execute_reply.started":"2021-07-12T13:12:07.300815Z","shell.execute_reply":"2021-07-12T13:12:07.304853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can implement the training phase.","metadata":{"execution":{"iopub.status.busy":"2021-07-11T07:53:23.591574Z","iopub.execute_input":"2021-07-11T07:53:23.591875Z","iopub.status.idle":"2021-07-11T07:53:23.595853Z","shell.execute_reply.started":"2021-07-11T07:53:23.591848Z","shell.execute_reply":"2021-07-11T07:53:23.595173Z"}}},{"cell_type":"code","source":"All_train_losses = []\nvalidate_losses = []\n# model_matrix = []\nscaler = torch.cuda.amp.GradScaler()\nfold = 0\n# model_original_stat_dict = model.state_dict()\nfor train_index, valid_index in kf.split(train_sample):\n    model = transformers.BertForSequenceClassification.from_pretrained(MODEL_PATH,num_labels=1)\n    optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2)\n    model.to(device)\n    train_input = train_sample[train_index]\n    valid_input = train_sample[valid_index]\n    # print(valid_input.shape)\n    train_input = BERTDataSet(train_input[:,0],train_input[:,1])\n    valid_input = BERTDataSet(valid_input[:,0],valid_input[:,1])\n    # print(train_input)\n    train_dataloader = DataLoader(train_input, batch_size = batchSize,shuffle = True,num_workers=4,pin_memory=True)\n    valid_dataloader = DataLoader(valid_input, batch_size = batchSize,shuffle = True,num_workers=4,pin_memory=True)\n    train_losses = []\n    bestScore = None\n    for epoch in tqdm(range(epochs)):\n        # train phase\n        model.train()\n        batch_pred = []\n        batch_target = []\n        for step, batch in enumerate(train_dataloader):\n            optimizer.zero_grad()\n            ids = batch[\"ids\"].to(device,non_blocking=True)\n            mask = batch[\"mask\"].to(device,non_blocking=True)\n            tokentype = batch[\"token_type_ids\"].to(device,non_blocking=True)\n\n            # print(step)\n            output = model(ids,mask)\n            output = output[\"logits\"].squeeze(-1)\n\n            target = batch[\"targets\"].to(device,non_blocking=True)\n\n            loss = nn.MSELoss()(output,target)\n            batch_pred += list(output.detach().cpu().numpy())\n            batch_target += list(target.detach().cpu().numpy())\n\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        train_losses.append(np.sqrt(mean_squared_error(batch_pred, batch_target)))\n\n        # print(\"train_end\")\n    \n        # validate phase\n#         if epoch+1==epochs:\n        with torch.no_grad():\n            valid_pred = []\n            valid_targets = []\n            model.eval()\n            for valid_step, valid_batch in enumerate(valid_dataloader):\n                valid_ids = valid_batch[\"ids\"].to(device,non_blocking=True)\n                valid_mask = valid_batch[\"mask\"].to(device,non_blocking=True)\n                valid_tokentype = valid_batch[\"token_type_ids\"].to(device,non_blocking=True)\n\n                valid_output = model(valid_ids,valid_mask)\n                valid_output = valid_output[\"logits\"].squeeze(-1)\n\n                valid_target = valid_batch[\"targets\"].to(device,non_blocking=True)\n\n                # v_loss = nn.MSELoss()(output,target)\n                # valid_loss.append(v_loss.item())\n                valid_pred += list(valid_output.detach().cpu().numpy())\n                valid_targets += list(valid_target.detach().cpu().numpy())\n\n            if bestScore is None:\n                bestScore = np.sqrt(mean_squared_error(valid_pred,valid_targets))\n                state = {\n                            'state_dict': model.state_dict(),\n                            'optimizer_dict': optimizer.state_dict(),\n                            \"bestscore\":bestScore\n                        }\n                torch.save(state, \"model\" + str(fold) + \".pth\")\n            elif bestScore > np.sqrt(mean_squared_error(valid_pred,valid_targets)):\n                bestsSore = np.sqrt(mean_squared_error(valid_pred,valid_targets))\n                state = {\n                            'state_dict': model.state_dict(),\n                            'optimizer_dict': optimizer.state_dict(),\n                            \"bestscore\":bestScore\n                        }\n                torch.save(state, \"model\"+ str(fold) + \".pth\")\n            else:\n                pass\n    validate_losses.append(bestScore)\n            \n    All_train_losses.append(train_losses)\n\n    print('Fold [%d/%d] Train Loss: %.4f  Validate Loss: %.4f'\n                  % (fold, 5, All_train_losses[fold-1][-1], validate_losses[fold-1]))\n  #     if not os.path.exists(os.path.join('BERT_pretrained',str(fold))):\n#         os.makedirs(os.path.join('BERT_pretrained',str(fold)))\n#     torch.save(model.state_dict(), os.path.join('BERT_pretrained',str(fold),'baseline3.pth'))\n#     model_matrix.append(model.state_dict())\n    fold += 1\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:12:07.307249Z","iopub.execute_input":"2021-07-12T13:12:07.307689Z","iopub.status.idle":"2021-07-12T15:36:06.887609Z","shell.execute_reply.started":"2021-07-12T13:12:07.307653Z","shell.execute_reply":"2021-07-12T15:36:06.886505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can show the average validation score","metadata":{}},{"cell_type":"code","source":"print(np.mean(validate_losses))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T15:36:06.892321Z","iopub.execute_input":"2021-07-12T15:36:06.898842Z","iopub.status.idle":"2021-07-12T15:36:06.909131Z","shell.execute_reply.started":"2021-07-12T15:36:06.898781Z","shell.execute_reply":"2021-07-12T15:36:06.908204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Testing\nIf we partition train data into train set and test set, we can use the test set to check the performance.","metadata":{}},{"cell_type":"code","source":"model = transformers.BertForSequenceClassification.from_pretrained(MODEL_PATH,num_labels=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T15:36:06.912331Z","iopub.execute_input":"2021-07-12T15:36:06.915422Z","iopub.status.idle":"2021-07-12T15:36:09.132034Z","shell.execute_reply.started":"2021-07-12T15:36:06.915377Z","shell.execute_reply":"2021-07-12T15:36:09.131182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_sample = test_sample.values\n# test_input = BERTDataSet(test_sample[:,0], test_sample[:,1])\n# test_dataloader = DataLoader(test_input,batch_size=int(batchSize),shuffle=True,num_workers=4,pin_memory=True)\n\n# test_scores = []\n\n# for i in range(5):\n#     model_weight = model_matrix[i]\n#     fold = i + 1\n#     model.load_state_dict(model_weight)\n#     model.to(device)\n#     model.eval()\n#     test_pred = []\n#     test_targets = []\n#     with torch.no_grad():\n#         for test_step, test in enumerate(test_dataloader):\n#             test_ids = test[\"ids\"].to(device,non_blocking=True)\n#             test_mask = test[\"mask\"].to(device,non_blocking=True)\n#             test_output = model(test_ids, test_mask)\n#             test_output = test_output[\"logits\"].squeeze(-1)\n#             test_target = test[\"targets\"].to(device,non_blocking=True)\n#             test_pred += list(test_output.detach().cpu().numpy())\n#             test_targets += list(test_target.detach().cpu().numpy())\n\n#     test_scores.append(np.sqrt(mean_squared_error(test_pred,test_targets)))\n#     print(np.sqrt(mean_squared_error(test_pred,test_targets)))\n# print(np.mean(test_scores))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T15:36:09.133446Z","iopub.execute_input":"2021-07-12T15:36:09.133819Z","iopub.status.idle":"2021-07-12T15:36:09.139046Z","shell.execute_reply.started":"2021-07-12T15:36:09.13378Z","shell.execute_reply":"2021-07-12T15:36:09.138095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Scoring\nUse the test data in \"test.csv\" to get the submission file.","metadata":{}},{"cell_type":"code","source":"All_test_preds = []\npathes = [os.path.join(\"./\",s) for s in os.listdir(\"./\") if \".pth\" in s]\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntest_input = test_df[\"excerpt\"]\ntest_input = test_input.values\ntest_target = np.zeros(len(test_input))\n# test_id = test_df[\"id\"]\ntest_input = BERTDataSet(test_input,test_target)\nfor path in pathes:\n    state = torch.load(path)\n    model.load_state_dict(state[\"state_dict\"])    \n\n    model.to(device)\n    model.eval()\n  \n\n    testloader = DataLoader(test_input,batch_size=32,shuffle=False,num_workers=4,pin_memory=True)\n  \n    test_preds = []\n    with torch.no_grad():\n        for test_step, test_batch in enumerate(testloader):\n            test_ids = test_batch[\"ids\"].to(device)\n            test_mask = test_batch[\"mask\"].to(device)\n            test_output = model(test_ids, test_mask)\n            test_output = test_output[\"logits\"].squeeze(-1)\n            test_pred = test_output.detach().cpu().numpy()\n            test_preds.append(test_pred)\n        test_preds = np.concatenate(test_preds)\n        All_test_preds.append(test_preds)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T16:00:31.992556Z","iopub.execute_input":"2021-07-12T16:00:31.992895Z","iopub.status.idle":"2021-07-12T16:01:39.524316Z","shell.execute_reply.started":"2021-07-12T16:00:31.992864Z","shell.execute_reply":"2021-07-12T16:01:39.523259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"All_test_preds","metadata":{"execution":{"iopub.status.busy":"2021-07-12T16:01:39.526188Z","iopub.execute_input":"2021-07-12T16:01:39.526573Z","iopub.status.idle":"2021-07-12T16:01:39.535791Z","shell.execute_reply.started":"2021-07-12T16:01:39.526529Z","shell.execute_reply":"2021-07-12T16:01:39.534795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scoring = pd.DataFrame(All_test_preds)\nscoring = scoring.T","metadata":{"execution":{"iopub.status.busy":"2021-07-12T16:01:39.537896Z","iopub.execute_input":"2021-07-12T16:01:39.538282Z","iopub.status.idle":"2021-07-12T16:01:39.547665Z","shell.execute_reply.started":"2021-07-12T16:01:39.538244Z","shell.execute_reply":"2021-07-12T16:01:39.546812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aver_score = scoring.mean(axis=1)\naver_score","metadata":{"execution":{"iopub.status.busy":"2021-07-12T16:01:39.549317Z","iopub.execute_input":"2021-07-12T16:01:39.549841Z","iopub.status.idle":"2021-07-12T16:01:39.560653Z","shell.execute_reply.started":"2021-07-12T16:01:39.549802Z","shell.execute_reply":"2021-07-12T16:01:39.559842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsample","metadata":{"execution":{"iopub.status.busy":"2021-07-12T16:01:39.56197Z","iopub.execute_input":"2021-07-12T16:01:39.56246Z","iopub.status.idle":"2021-07-12T16:01:39.584277Z","shell.execute_reply.started":"2021-07-12T16:01:39.562297Z","shell.execute_reply":"2021-07-12T16:01:39.583573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample[\"target\"] = aver_score\nsample","metadata":{"execution":{"iopub.status.busy":"2021-07-12T16:01:39.585469Z","iopub.execute_input":"2021-07-12T16:01:39.585808Z","iopub.status.idle":"2021-07-12T16:01:39.59665Z","shell.execute_reply.started":"2021-07-12T16:01:39.585775Z","shell.execute_reply":"2021-07-12T16:01:39.595583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.to_csv(\"submission.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T16:05:06.999379Z","iopub.execute_input":"2021-07-12T16:05:06.999737Z","iopub.status.idle":"2021-07-12T16:05:07.005796Z","shell.execute_reply.started":"2021-07-12T16:05:06.999703Z","shell.execute_reply":"2021-07-12T16:05:07.004827Z"},"trusted":true},"execution_count":null,"outputs":[]}]}