{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data.dataset import Dataset\nimport csv\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-16T19:28:47.689263Z","iopub.execute_input":"2021-06-16T19:28:47.689597Z","iopub.status.idle":"2021-06-16T19:28:50.411946Z","shell.execute_reply.started":"2021-06-16T19:28:47.689501Z","shell.execute_reply":"2021-06-16T19:28:50.411088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loc[:2500, :].to_csv('train.csv')\ndf.loc[2500:, :].to_csv('test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T19:28:52.421066Z","iopub.execute_input":"2021-06-16T19:28:52.421397Z","iopub.status.idle":"2021-06-16T19:28:52.773204Z","shell.execute_reply.started":"2021-06-16T19:28:52.421366Z","shell.execute_reply":"2021-06-16T19:28:52.772401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n\n    def __init__(self, data_path, dict_path, max_length_sentences=30, max_length_word=35):\n        super(MyDataset, self).__init__()\n\n        df = pd.read_csv(data_path) #\"../input/commonlitreadabilityprize/train.csv\"\n\n        self.texts = df['excerpt'].str.lower().values\n        try:\n            self.labels = df['target'].values\n        except:\n            self.labels = list(range(len(self.texts)))\n        self.dict = pd.read_csv(filepath_or_buffer=dict_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE, usecols=[0]).values\n        self.dict = [word[0] for word in self.dict]\n        self.max_length_sentences = max_length_sentences\n        self.max_length_word = max_length_word\n        self.num_classes = 1\n        self.document_encode = [[\n            [self.dict.index(word) if word in self.dict else -1 for word in word_tokenize(text=sentences)] for sentences\n            in\n            sent_tokenize(text=tx)] for tx in self.texts]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        label = self.labels[index]\n        document_encode = self.document_encode[index]\n\n        for sentences in document_encode:\n            if len(sentences) < self.max_length_word:\n                extended_words = [-1 for _ in range(self.max_length_word - len(sentences))]\n                sentences.extend(extended_words)\n\n        if len(document_encode) < self.max_length_sentences:\n            extended_sentences = [[-1 for _ in range(self.max_length_word)] for _ in\n                                  range(self.max_length_sentences - len(document_encode))]\n            document_encode.extend(extended_sentences)\n\n        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n                          :self.max_length_sentences]\n\n        document_encode = np.stack(arrays=document_encode, axis=0)\n        document_encode += 1\n\n        return document_encode.astype(np.int64), label","metadata":{"execution":{"iopub.status.busy":"2021-06-16T19:28:54.161063Z","iopub.execute_input":"2021-06-16T19:28:54.161373Z","iopub.status.idle":"2021-06-16T19:28:54.172845Z","shell.execute_reply.started":"2021-06-16T19:28:54.161344Z","shell.execute_reply":"2021-06-16T19:28:54.172034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nimport csv\ncsv.field_size_limit(sys.maxsize)\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn import metrics\nimport numpy as np\n\n\ndef matrix_mul(input, weight, bias=False):\n    feature_list = []\n    for feature in input:\n        feature = torch.mm(feature, weight)\n        if isinstance(bias, torch.nn.parameter.Parameter):\n            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n        feature = torch.tanh(feature).unsqueeze(0)\n        feature_list.append(feature)\n\n    return torch.cat(feature_list, 0).squeeze()\n\ndef element_wise_mul(input1, input2):\n\n    feature_list = []\n    for feature_1, feature_2 in zip(input1, input2):\n        feature_2 = feature_2.unsqueeze(1).expand_as(feature_1)\n        feature = feature_1 * feature_2\n        feature_list.append(feature.unsqueeze(0))\n    output = torch.cat(feature_list, 0)\n\n    return torch.sum(output, 0).unsqueeze(0)\n\n\nclass SentAttNet(nn.Module):\n    def __init__(self, sent_hidden_size=50, word_hidden_size=50, num_classes=14):\n        super(SentAttNet, self).__init__()\n\n        self.sent_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 2 * sent_hidden_size))\n        self.sent_bias = nn.Parameter(torch.Tensor(1, 2 * sent_hidden_size))\n        self.context_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 1))\n\n        self.gru = nn.GRU(2 * word_hidden_size, sent_hidden_size, bidirectional=True)\n        self.fc = nn.Linear(2 * sent_hidden_size, num_classes)\n        self._create_weights(mean=0.0, std=0.05)\n\n    def _create_weights(self, mean=0.0, std=0.05):\n        self.sent_weight.data.normal_(mean, std)\n        self.context_weight.data.normal_(mean, std)\n\n    def forward(self, input, hidden_state):\n\n        f_output, h_output = self.gru(input, hidden_state)\n        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n        output = matrix_mul(output, self.context_weight).permute(1, 0)\n        output = F.softmax(output)\n        #output = F.tanh(output)\n        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n        output = self.fc(output)\n\n        return output, h_output\n\n\nclass WordAttNet(nn.Module):\n    def __init__(self, word2vec_path, hidden_size=50):\n        super(WordAttNet, self).__init__()\n        dict = pd.read_csv(filepath_or_buffer=word2vec_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE).values[:, 1:]\n        dict_len, embed_size = dict.shape\n        dict_len += 1\n        unknown_word = np.zeros((1, embed_size))\n        dict = torch.from_numpy(np.concatenate([unknown_word, dict], axis=0).astype(np.float))\n\n        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n        self.word_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n\n        self.lookup = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(dict)\n        self.gru = nn.GRU(embed_size, hidden_size, bidirectional=True)\n        self._create_weights(mean=0.0, std=0.05)\n\n    def _create_weights(self, mean=0.0, std=0.05):\n\n        self.word_weight.data.normal_(mean, std)\n        self.context_weight.data.normal_(mean, std)\n\n    def forward(self, input, hidden_state):\n\n        output = self.lookup(input)\n        f_output, h_output = self.gru(output.float(), hidden_state)  # feature output and hidden state output\n        output = matrix_mul(f_output, self.word_weight, self.word_bias)\n        output = matrix_mul(output, self.context_weight).permute(1,0)\n        output = F.softmax(output)\n        #output = F.tanh(output)\n        output = element_wise_mul(f_output,output.permute(1,0))\n\n        return output, h_output\n\n\nclass HierAttNet(nn.Module):\n    def __init__(self, word_hidden_size, sent_hidden_size, batch_size, num_classes, pretrained_word2vec_path,\n                 max_sent_length, max_word_length):\n        super(HierAttNet, self).__init__()\n        self.batch_size = batch_size\n        self.word_hidden_size = word_hidden_size\n        self.sent_hidden_size = sent_hidden_size\n        self.max_sent_length = max_sent_length\n        self.max_word_length = max_word_length\n        self.word_att_net = WordAttNet(pretrained_word2vec_path, word_hidden_size)\n        self.sent_att_net = SentAttNet(sent_hidden_size, word_hidden_size, num_classes)\n        self._init_hidden_state()\n\n    def _init_hidden_state(self, last_batch_size=None):\n        if last_batch_size:\n            batch_size = last_batch_size\n        else:\n            batch_size = self.batch_size\n        self.word_hidden_state = torch.zeros(2, batch_size, self.word_hidden_size)\n        self.sent_hidden_state = torch.zeros(2, batch_size, self.sent_hidden_size)\n        if torch.cuda.is_available():\n            self.word_hidden_state = self.word_hidden_state.cuda()\n            self.sent_hidden_state = self.sent_hidden_state.cuda()\n\n    def forward(self, input):\n\n        output_list = []\n        input = input.permute(1, 0, 2)\n        for i in input:\n            output, self.word_hidden_state = self.word_att_net(i.permute(1, 0), self.word_hidden_state)\n            output_list.append(output)\n        output = torch.cat(output_list, 0)\n        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:11:16.425132Z","iopub.execute_input":"2021-06-16T20:11:16.42546Z","iopub.status.idle":"2021-06-16T20:11:16.456321Z","shell.execute_reply.started":"2021-06-16T20:11:16.425426Z","shell.execute_reply":"2021-06-16T20:11:16.455429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    torch.cuda.manual_seed(123)\nelse:\n    torch.manual_seed(123)\n\ntraining_params = {\"batch_size\": 128*4,\n                   \"shuffle\": True,\n                   \"drop_last\": True}\ntest_params = {\"batch_size\": 128*4,\n               \"shuffle\": False,\n               \"drop_last\": False}","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:11:16.789358Z","iopub.execute_input":"2021-06-16T20:11:16.789679Z","iopub.status.idle":"2021-06-16T20:11:23.420783Z","shell.execute_reply.started":"2021-06-16T20:11:16.78964Z","shell.execute_reply":"2021-06-16T20:11:23.41986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nmax_word_length, max_sent_length = 15, 10\ntraining_set = MyDataset(\"train.csv\", '../input/nlpword2vecembeddingspretrained/glove.6B.50d.txt', max_sent_length, max_word_length)\ntraining_generator = DataLoader(training_set, **training_params)\ntest_set = MyDataset(\"test.csv\", '../input/nlpword2vecembeddingspretrained/glove.6B.50d.txt', max_sent_length, max_word_length)\ntest_generator = DataLoader(test_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:35:51.404406Z","iopub.execute_input":"2021-06-16T20:35:51.404761Z","iopub.status.idle":"2021-06-16T20:37:22.543178Z","shell.execute_reply.started":"2021-06-16T20:35:51.404729Z","shell.execute_reply":"2021-06-16T20:37:22.542309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generator.__iter__().next()[0]#.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:38:31.932435Z","iopub.execute_input":"2021-06-16T20:38:31.932926Z","iopub.status.idle":"2021-06-16T20:38:31.958824Z","shell.execute_reply.started":"2021-06-16T20:38:31.932885Z","shell.execute_reply":"2021-06-16T20:38:31.957788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = HierAttNet(50, 50, 512, 1, '../input/nlpword2vecembeddingspretrained/glove.6B.50d.txt', max_sent_length, max_word_length)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:38:33.65795Z","iopub.execute_input":"2021-06-16T20:38:33.658268Z","iopub.status.idle":"2021-06-16T20:38:39.273193Z","shell.execute_reply.started":"2021-06-16T20:38:33.658239Z","shell.execute_reply":"2021-06-16T20:38:39.272337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    model.cuda()\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001) #, momentum=0.9\nbest_loss = 1e5\nbest_epoch = 0","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:38:40.63291Z","iopub.execute_input":"2021-06-16T20:38:40.633233Z","iopub.status.idle":"2021-06-16T20:38:40.675546Z","shell.execute_reply.started":"2021-06-16T20:38:40.633194Z","shell.execute_reply":"2021-06-16T20:38:40.674792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epoches = 500\ntest_interval = 1\nes_min_delta = 0\nes_patience = 15\n\nmodel.train()\nnum_iter_per_epoch = len(training_generator)\nfor epoch in range(num_epoches):\n    for iter, (feature, label) in enumerate(training_generator):\n        if torch.cuda.is_available():\n            feature = feature.cuda()\n            label = label.cuda()\n        optimizer.zero_grad()\n        model._init_hidden_state()\n        predictions = model(feature)\n        loss = criterion(predictions.view(-1), label.float())\n        loss.backward()\n        optimizer.step()\n        print(\"Epoch: {}/{}, Iteration: {}/{}, Lr: {}, Loss: {}\".format(\n            epoch + 1,\n            num_epoches,\n            iter + 1,\n            num_iter_per_epoch,\n            optimizer.param_groups[0]['lr'],\n            loss**0.5))\n    if epoch % test_interval == 0:\n        model.eval()\n        loss_ls = []\n        te_label_ls = []\n        te_pred_ls = []\n        for te_feature, te_label in test_generator:\n            num_sample = len(te_label)\n            if torch.cuda.is_available():\n                te_feature = te_feature.cuda()\n                te_label = te_label.cuda()\n            with torch.no_grad():\n                model._init_hidden_state(num_sample)\n                te_predictions = model(te_feature)\n            te_loss = criterion(te_predictions.view(-1), te_label)\n            loss_ls.append(te_loss * num_sample)\n            te_label_ls.extend(te_label.clone().cpu())\n            te_pred_ls.append(te_predictions.clone().cpu())\n        te_loss = sum(loss_ls) / test_set.__len__()\n        te_pred = torch.cat(te_pred_ls, 0)\n        te_label = np.array(te_label_ls)\n\n        print(\"Epoch: {}/{}, Lr: {}, Loss: {}\".format(\n            epoch + 1,\n            num_epoches,\n            optimizer.param_groups[0]['lr'],\n            te_loss**0.5))\n        model.train()\n        if te_loss + es_min_delta < best_loss:\n            best_loss = te_loss\n            best_epoch = epoch\n            torch.save(model, \"whole_model_han\")\n\n        # Early stopping\n        if epoch - best_epoch > es_patience > 0:\n            print(\"Stop training at epoch {}. The lowest loss achieved is {}\".format(epoch, te_loss))\n            break","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:38:42.423093Z","iopub.execute_input":"2021-06-16T20:38:42.423417Z","iopub.status.idle":"2021-06-16T20:58:56.546833Z","shell.execute_reply.started":"2021-06-16T20:38:42.423386Z","shell.execute_reply":"2021-06-16T20:58:56.376951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#list(model.parameters())","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:48:49.799988Z","iopub.execute_input":"2021-06-16T13:48:49.800329Z","iopub.status.idle":"2021-06-16T13:48:49.805418Z","shell.execute_reply.started":"2021-06-16T13:48:49.800299Z","shell.execute_reply":"2021-06-16T13:48:49.803012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = MyDataset(\"../input/commonlitreadabilityprize/test.csv\", '../input/nlpword2vecembeddingspretrained/glove.6B.50d.txt', max_sent_length, max_word_length)\nf_generator = DataLoader(f, **test_params)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:29:48.429018Z","iopub.execute_input":"2021-06-16T13:29:48.429335Z","iopub.status.idle":"2021-06-16T13:29:50.241671Z","shell.execute_reply.started":"2021-06-16T13:29:48.429306Z","shell.execute_reply":"2021-06-16T13:29:50.240876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nte_pred_ls = []\n\nfor te_feature, _ in f_generator:\n    if torch.cuda.is_available():\n        te_feature = te_feature.cuda()\n    with torch.no_grad():\n        model._init_hidden_state(te_feature.shape[0])\n        te_predictions = model(te_feature)\n        te_pred_ls.append(te_predictions.view(-1).clone().cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:41:51.23622Z","iopub.execute_input":"2021-06-16T13:41:51.236542Z","iopub.status.idle":"2021-06-16T13:41:51.281165Z","shell.execute_reply.started":"2021-06-16T13:41:51.236512Z","shell.execute_reply":"2021-06-16T13:41:51.280276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")[['id']]","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:44:22.603818Z","iopub.execute_input":"2021-06-16T13:44:22.604189Z","iopub.status.idle":"2021-06-16T13:44:22.620506Z","shell.execute_reply.started":"2021-06-16T13:44:22.604158Z","shell.execute_reply":"2021-06-16T13:44:22.619739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre = pd.DataFrame({\n    'id': pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\").loc[:, 'id'].values,\n    'target': np.concatenate(te_pred_ls)\n})\n\npre","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:48:01.598231Z","iopub.execute_input":"2021-06-16T13:48:01.598565Z","iopub.status.idle":"2021-06-16T13:48:01.62136Z","shell.execute_reply.started":"2021-06-16T13:48:01.598533Z","shell.execute_reply":"2021-06-16T13:48:01.620414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T13:47:17.043705Z","iopub.execute_input":"2021-06-16T13:47:17.044076Z","iopub.status.idle":"2021-06-16T13:47:17.05605Z","shell.execute_reply.started":"2021-06-16T13:47:17.044042Z","shell.execute_reply":"2021-06-16T13:47:17.055183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}