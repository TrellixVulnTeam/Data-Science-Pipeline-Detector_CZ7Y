{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center>CommonLit Readability Prize</center>\n\n\n**To-Do**\n* To predict the complexity of reading passages for grade 3-12 classroom use.\n\n**About data** -\n> * id - unique ID for excerpt\n> * url_legal - URL of source - this is blank in the test set.\n> * license - license of source material - this is blank in the test set.\n> * excerpt - text to predict reading ease of\n> * target - reading ease\n> * standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n\n\n**Special Notes** -\n* url_legal, license and standard error are not available for test data.","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np \nimport pandas as pd\nimport nltk\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading Data","metadata":{}},{"cell_type":"code","source":"## train_data\ntrain_data = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\n\n##test_data\ntest_data = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\n\n\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histogram of token length","metadata":{}},{"cell_type":"code","source":"len_v = train_data['excerpt'].apply(nltk.word_tokenize).apply(lambda x : len(x))\nlen_v.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.figure(figsize=(10,10))\nsns.histplot(len_v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(train_data['excerpt'],train_data['target'],random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Universal Sentence Encoder - \n\n* USE encodes text into some n-dimensional vectors, which then can be used for text classification, clustering etc.\n* It comes with two variations i.e. one trained with Transformer encoder and other trained with Deep Averaging Network (DAN). \n* Tf-Hub provides both versions. In this notebook, I will be comparing both models.\n* Best part about USE is, it can convert paragraphs to embeddings as well.\n\n","metadata":{}},{"cell_type":"markdown","source":"#### DAN model","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential()\nmodel.add(hub.KerasLayer(\"/kaggle/input/universalsentenceencoder/\",input_shape=[],trainable=False,dtype=tf.string))\nmodel.add(tf.keras.layers.Dense(128))\nmodel.add(tf.keras.layers.Dense(64))\nmodel.add(tf.keras.layers.Dense(32))\nmodel.add(tf.keras.layers.Dense(1))\n\nmodel.compile(optimizer='adam',loss = \"mean_squared_error\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Transformer Model","metadata":{}},{"cell_type":"code","source":"model2 = tf.keras.models.Sequential()\nmodel2.add(hub.KerasLayer(\"/kaggle/input/universalsentenceencoderlarge/\",input_shape=[],trainable=False,dtype=tf.string))\nmodel2.add(tf.keras.layers.Dense(128))\nmodel2.add(tf.keras.layers.Dense(64))\nmodel2.add(tf.keras.layers.Dense(32))\nmodel2.add(tf.keras.layers.Dense(1))\nmodel2.compile(optimizer='adam',loss = \"mean_squared_error\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,y_train,epochs=20)\npreds_dan = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.fit(X_train,y_train,epochs=20)\npreds_trans = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"RMSE for DAN Model: \" + str(np.sqrt(mean_squared_error(y_test,preds_dan))))\n\nprint(\"RMSE for Transformer Model: \" + str(np.sqrt(mean_squared_error(y_test,preds_trans))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **We can see that both models have approximately same RMSE. It does not make sense to waste resources on the transformer one.**","metadata":{}},{"cell_type":"code","source":"test_data['target'] = model.predict(test_data['excerpt'])\ntest_data.drop(['url_legal','license','excerpt'],axis=1,inplace=True)\n\ntest_data.to_csv('/kaggle/working/submission.csv',index=False)\ntest_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ### **If you find this useful, please upvote my work.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}