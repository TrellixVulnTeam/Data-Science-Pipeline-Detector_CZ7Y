{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Version 5\n* Ensemble Hiro's 5 models and Tri's 13 models.\n\n## Model list\n\n#### Hiro's models\n* RoBERTa-large version 31;\n* RoBERTa-large version 31-b;\n* kfold: Ver.2, RoBERTa-large Ver.26a;\n* ELECTRA-large Ver.4;\n* kfold: Ver.3, DeBERTa-large Ver.14\n\n#### Tri's models\n* RoBERTa-large version 15-0, 15-3;\n* RoBERTa-large version 16-1;\n* XLNet-large-cased version 2-0, 3-0, 3-1;\n* GPT2-medium version 1-0;\n* ELECTRA-large-discriminator version 1-0, 1-1;\n* DeBERTa-large version 1-0, 1-1;\n* Funnel-large version 1-0;\n* BART-large version 1-0","metadata":{}},{"cell_type":"markdown","source":"# Tri's part\n##################################################################################################################","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport random\nimport gc\n\nimport sys\nsys.path.append('../input/readability-package')\nimport readability\nimport spacy\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, pos_tag_sents\nimport string\nimport re\nimport math\nimport pickle\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n# Autocast\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.swa_utils import AveragedModel\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, AdamW\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":9.557215,"end_time":"2021-06-01T09:59:54.614047","exception":false,"start_time":"2021-06-01T09:59:45.056832","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T15:08:28.828921Z","iopub.execute_input":"2021-07-29T15:08:28.829425Z","iopub.status.idle":"2021-07-29T15:08:38.909112Z","shell.execute_reply.started":"2021-07-29T15:08:28.82929Z","shell.execute_reply":"2021-07-29T15:08:38.908232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r ../input/spacy-readability/spacy_readability-master/* ./\n!cp -r ../input/syllapy/syllapy-master/* ./\nimport spacy\nfrom spacy_readability import Readability\n\nnlp = spacy.load('en')\nnlp.add_pipe(Readability(), last = True)","metadata":{"papermill":{"duration":2.866749,"end_time":"2021-06-01T09:59:57.492869","exception":false,"start_time":"2021-06-01T09:59:54.62612","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T15:08:38.910489Z","iopub.execute_input":"2021-07-29T15:08:38.910799Z","iopub.status.idle":"2021-07-29T15:08:41.369903Z","shell.execute_reply.started":"2021-07-29T15:08:38.910764Z","shell.execute_reply":"2021-07-29T15:08:41.368917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed = 0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed = 0\nseed_everything(seed)","metadata":{"papermill":{"duration":0.022504,"end_time":"2021-06-01T09:59:57.527015","exception":false,"start_time":"2021-06-01T09:59:57.504511","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T15:08:41.371958Z","iopub.execute_input":"2021-07-29T15:08:41.372313Z","iopub.status.idle":"2021-07-29T15:08:41.383775Z","shell.execute_reply.started":"2021-07-29T15:08:41.372269Z","shell.execute_reply":"2021-07-29T15:08:41.382925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import data","metadata":{"papermill":{"duration":0.010926,"end_time":"2021-06-01T09:59:57.549012","exception":false,"start_time":"2021-06-01T09:59:57.538086","status":"completed"},"tags":[]}},{"cell_type":"code","source":"base_dir = '../input/commonlitreadabilityprize'\ntrain_data = pd.read_csv(f'{base_dir}/train.csv')\n# Benchmark text\nbenchmark = train_data[train_data['standard_error'] == 0.]","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:08:41.385968Z","iopub.execute_input":"2021-07-29T15:08:41.386355Z","iopub.status.idle":"2021-07-29T15:08:41.472414Z","shell.execute_reply.started":"2021-07-29T15:08:41.38633Z","shell.execute_reply":"2021-07-29T15:08:41.471595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = '../input/commonlitreadabilityprize'\ndata = pd.read_csv(f'{base_dir}/test.csv')\nss = pd.read_csv(f'{base_dir}/sample_submission.csv')\ndata.head()","metadata":{"papermill":{"duration":0.048563,"end_time":"2021-06-01T09:59:57.608677","exception":false,"start_time":"2021-06-01T09:59:57.560114","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T15:08:41.475214Z","iopub.execute_input":"2021-07-29T15:08:41.475487Z","iopub.status.idle":"2021-07-29T15:08:41.502947Z","shell.execute_reply.started":"2021-07-29T15:08:41.475462Z","shell.execute_reply":"2021-07-29T15:08:41.50223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower().strip()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ndef text_preprocessing(text):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    combined_text = ' '.join(tokenized_text)\n    return combined_text\n\ndef readability_feat(text):\n    text = nlp(text)\n    \n    return np.array([text._.flesch_kincaid_grade_level,\n                     text._.flesch_kincaid_reading_ease,\n                     text._.dale_chall,\n                     text._.coleman_liau_index,\n                     text._.automated_readability_index,\n                     text._.forcast], dtype = np.float)\n\ndef sample_text(targets, num_output = 5):\n    mean, var = targets[0], targets[1]\n    if targets[1] != 0.:\n        sampled_target = torch.normal(mean, var, size = (num_output,))\n    else:\n        sampled_target = torch.tensor([0.] * num_output, dtype = torch.float)\n    return sampled_target\n\ndef convert_examples_to_features(text, tokenizer, max_len, is_test = False, return_tensor = False):\n    # Take from https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit\n    text = text.replace('\\n', '')\n    if return_tensor:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            return_tensors = 'pt',\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = True\n        )\n    else:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = True\n        )\n    return tok\n\ndef form_dataset(token, external_features = None, target = None, bins = None):\n    if target is not None:\n        if bins is not None:\n            if external_features is not None:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'external_features': torch.tensor(external_features, dtype = torch.float),\n                    'target': target,\n                    'bins': bins,\n                }\n            else:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'target': target,\n                    'bins': bins,\n                }\n        else:\n            if external_features is not None:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'external_features': torch.tensor(external_features, dtype = torch.float),\n                    'target': target,\n                }\n            else:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'target': target,\n                }\n    else:\n        if external_features is not None:\n            return {\n                'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                'external_features': torch.tensor(external_features, dtype = torch.float),\n            }\n        else:\n            return {\n                'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n            }","metadata":{"papermill":{"duration":0.030823,"end_time":"2021-06-01T09:59:57.651134","exception":false,"start_time":"2021-06-01T09:59:57.620311","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T15:08:41.505572Z","iopub.execute_input":"2021-07-29T15:08:41.505819Z","iopub.status.idle":"2021-07-29T15:08:41.527722Z","shell.execute_reply.started":"2021-07-29T15:08:41.50579Z","shell.execute_reply":"2021-07-29T15:08:41.526764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"papermill":{"duration":0.011306,"end_time":"2021-06-01T09:59:57.673861","exception":false,"start_time":"2021-06-01T09:59:57.662555","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Dataset(Dataset):\n    def __init__(self, documents, tokenizer, max_len = 300, mode = 'infer'):\n        self.documents = documents\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.documents)\n    \n    def __getitem__(self, idx):\n        sample = self.documents.iloc[idx]\n        document = sample['excerpt']\n        \n        # Tokenize\n        features = convert_examples_to_features(document, self.tokenizer, self.max_len)\n        \n        return form_dataset(features)","metadata":{"papermill":{"duration":0.01933,"end_time":"2021-06-01T09:59:57.704564","exception":false,"start_time":"2021-06-01T09:59:57.685234","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T15:08:41.529045Z","iopub.execute_input":"2021-07-29T15:08:41.529414Z","iopub.status.idle":"2021-07-29T15:08:41.539826Z","shell.execute_reply.started":"2021-07-29T15:08:41.52936Z","shell.execute_reply":"2021-07-29T15:08:41.539072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{"papermill":{"duration":0.011233,"end_time":"2021-06-01T09:59:57.727393","exception":false,"start_time":"2021-06-01T09:59:57.71616","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Utils class","metadata":{}},{"cell_type":"code","source":"class AttentivePooling(nn.Module):\n    def __init__(self, input_dim = 768, attention_dim = 1024):\n        super(AttentivePooling, self).__init__()\n        # Attention pooler\n        self.word_weight = nn.Linear(input_dim, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n    def forward(self, x, mask = None):\n        '''\n        x : Batch_size x Seq_len x input_dim\n        mask: \n        '''\n        # Attention Pooling (over sequence for the first sequence)\n        u_i = torch.tanh(self.word_weight(x))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        \n        if mask is not None:\n            att = att * (1 - mask.unsqueeze(-1))\n            \n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        x = x * att\n        return x.sum(dim = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:08:41.543118Z","iopub.execute_input":"2021-07-29T15:08:41.543737Z","iopub.status.idle":"2021-07-29T15:08:41.551968Z","shell.execute_reply.started":"2021-07-29T15:08:41.543699Z","shell.execute_reply":"2021-07-29T15:08:41.551184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RoBERTa base","metadata":{}},{"cell_type":"markdown","source":"* Version 11","metadata":{}},{"cell_type":"code","source":"class Readability_Model_RoBERTa_base_v11(nn.Module):\n    def __init__(self, backbone, model_config, benchmark_token = None, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True):\n        super(Readability_Model_RoBERTa_base_v11, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size, num_cat)\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.output_cat)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        output = self.layer_norm(output_backbone.pooler_output)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts)\n        cats /= len(self.dropouts)\n\n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:08:41.553549Z","iopub.execute_input":"2021-07-29T15:08:41.55387Z","iopub.status.idle":"2021-07-29T15:08:41.573261Z","shell.execute_reply.started":"2021-07-29T15:08:41.553842Z","shell.execute_reply":"2021-07-29T15:08:41.572282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RoBERTa large","metadata":{}},{"cell_type":"markdown","source":"* Version 15","metadata":{}},{"cell_type":"code","source":"class Readability_Model_RoBERTa_large_v15(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_RoBERTa_large_v15, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:08:41.574607Z","iopub.execute_input":"2021-07-29T15:08:41.575002Z","iopub.status.idle":"2021-07-29T15:08:41.597855Z","shell.execute_reply.started":"2021-07-29T15:08:41.574967Z","shell.execute_reply":"2021-07-29T15:08:41.596957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Version 16","metadata":{}},{"cell_type":"code","source":"class Readability_Model_RoBERTa_large_v16(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_RoBERTa_large_v16, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Weighted mean pooling (over hidden layers)\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        output_backbone = torch.sum(hidden_states * layer_weight, dim = 0)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:08:41.599295Z","iopub.execute_input":"2021-07-29T15:08:41.599778Z","iopub.status.idle":"2021-07-29T15:08:41.622474Z","shell.execute_reply.started":"2021-07-29T15:08:41.599742Z","shell.execute_reply":"2021-07-29T15:08:41.621554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XLNet large cased","metadata":{}},{"cell_type":"markdown","source":"* Version 2","metadata":{}},{"cell_type":"code","source":"class Readability_Model_XLNet_large_cased_v2(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_XLNet_large_cased_v2, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        token_type_ids = token_type_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:08:41.623995Z","iopub.execute_input":"2021-07-29T15:08:41.624598Z","iopub.status.idle":"2021-07-29T15:08:41.647684Z","shell.execute_reply.started":"2021-07-29T15:08:41.62456Z","shell.execute_reply":"2021-07-29T15:08:41.646761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Version 3","metadata":{}},{"cell_type":"code","source":"class Readability_Model_XLNet_large_cased_v3(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_XLNet_large_cased_v3, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        token_type_ids = token_type_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:08:41.648957Z","iopub.execute_input":"2021-07-29T15:08:41.649314Z","iopub.status.idle":"2021-07-29T15:08:41.672946Z","shell.execute_reply.started":"2021-07-29T15:08:41.649279Z","shell.execute_reply":"2021-07-29T15:08:41.671923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GPT2 medium","metadata":{}},{"cell_type":"markdown","source":"* Version 1","metadata":{}},{"cell_type":"code","source":"class Readability_Model_GPT2_medium_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_GPT2_medium_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:08:41.674409Z","iopub.execute_input":"2021-07-29T15:08:41.674748Z","iopub.status.idle":"2021-07-29T15:08:41.697875Z","shell.execute_reply.started":"2021-07-29T15:08:41.674715Z","shell.execute_reply":"2021-07-29T15:08:41.697044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ALBERT xlarge v2","metadata":{}},{"cell_type":"markdown","source":"* Version 1","metadata":{}},{"cell_type":"code","source":"class Readability_Model_ALBERT_xlarge_v2_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_ALBERT_xlarge_v2_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:08:41.699067Z","iopub.execute_input":"2021-07-29T15:08:41.699479Z","iopub.status.idle":"2021-07-29T15:08:41.722392Z","shell.execute_reply.started":"2021-07-29T15:08:41.699443Z","shell.execute_reply":"2021-07-29T15:08:41.721579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ELECTRA large discriminator","metadata":{}},{"cell_type":"markdown","source":"* Version 1","metadata":{}},{"cell_type":"code","source":"class Readability_Model_ELECTRA_large_discriminator_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_ELECTRA_large_discriminator_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    \n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:08:41.723671Z","iopub.execute_input":"2021-07-29T15:08:41.724186Z","iopub.status.idle":"2021-07-29T15:08:41.746715Z","shell.execute_reply.started":"2021-07-29T15:08:41.72415Z","shell.execute_reply":"2021-07-29T15:08:41.745917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DeBERTa large","metadata":{}},{"cell_type":"markdown","source":"* Version 1","metadata":{}},{"cell_type":"code","source":"class Readability_Model_DeBERTa_large_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_DeBERTa_large_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:08:41.748154Z","iopub.execute_input":"2021-07-29T15:08:41.748576Z","iopub.status.idle":"2021-07-29T15:08:41.771782Z","shell.execute_reply.started":"2021-07-29T15:08:41.74854Z","shell.execute_reply":"2021-07-29T15:08:41.770811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Funnel-transformer/large","metadata":{}},{"cell_type":"markdown","source":"* Version 1","metadata":{}},{"cell_type":"code","source":"class Readability_Model_Funnel_large_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_Funnel_large_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        token_type_ids = token_type_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        # Extract output\n        output_backbone = output_backbone.last_hidden_state\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:09:34.3402Z","iopub.execute_input":"2021-07-29T15:09:34.340546Z","iopub.status.idle":"2021-07-29T15:09:34.362664Z","shell.execute_reply.started":"2021-07-29T15:09:34.340515Z","shell.execute_reply":"2021-07-29T15:09:34.360468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BART large","metadata":{}},{"cell_type":"markdown","source":"* Version 1","metadata":{}},{"cell_type":"code","source":"class Readability_Model_BART_large_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_BART_large_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers * 2).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.init_std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.init_std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        decoder_hidden_states = output_backbone.decoder_hidden_states\n        encoder_hidden_states = output_backbone.encoder_hidden_states\n\n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        decoder_hidden_states = tuple(decoder_hidden_states[-i-1] for i in range(self.model_config.num_hidden_layers))\n        encoder_hidden_states = tuple(encoder_hidden_states[-i-1] for i in range(self.model_config.num_hidden_layers))\n        hidden_states = torch.stack(decoder_hidden_states + encoder_hidden_states, dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:10:05.88443Z","iopub.execute_input":"2021-07-29T15:10:05.88477Z","iopub.status.idle":"2021-07-29T15:10:05.906946Z","shell.execute_reply.started":"2021-07-29T15:10:05.884741Z","shell.execute_reply":"2021-07-29T15:10:05.906075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference function","metadata":{"papermill":{"duration":0.01121,"end_time":"2021-06-01T09:59:57.786273","exception":false,"start_time":"2021-06-01T09:59:57.775063","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def infer(model, dataloader, device = 'cpu', use_tqdm = True, benchmark_token = None):\n    model.eval()\n    \n    if use_tqdm:\n        tbar = tqdm(dataloader)\n    else:\n        tbar = dataloader\n        \n    pred = []\n        \n    for item in tbar:\n        input_ids = item['input_ids'].to(device)\n        token_type_ids = item['token_type_ids'].to(device)\n        attention_mask = item['attention_mask'].to(device)\n        \n        if benchmark_token is not None:\n            benchmark_input_ids, benchmark_token_type_ids, benchmark_attention_mask = benchmark_token\n            input_ids = torch.cat((input_ids, benchmark_input_ids), dim = 0)\n            token_type_ids = torch.cat((token_type_ids, benchmark_token_type_ids), dim = 0)\n            attention_mask = torch.cat((attention_mask, benchmark_attention_mask), dim = 0)\n            \n        with torch.no_grad():\n            with autocast():\n                pred_mean, pred_std, pred_bins = model(input_ids = input_ids, \n                                                       attention_mask = attention_mask, \n                                                       token_type_ids = token_type_ids)\n        \n        pred.extend(pred_mean.cpu().detach().numpy())\n        \n    # Stack\n    pred = np.array(pred)\n    \n    return pred","metadata":{"papermill":{"duration":0.019234,"end_time":"2021-06-01T09:59:57.816999","exception":false,"start_time":"2021-06-01T09:59:57.797765","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T15:10:06.545604Z","iopub.execute_input":"2021-07-29T15:10:06.545931Z","iopub.status.idle":"2021-07-29T15:10:06.553552Z","shell.execute_reply.started":"2021-07-29T15:10:06.545898Z","shell.execute_reply":"2021-07-29T15:10:06.552735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{"papermill":{"duration":0.011913,"end_time":"2021-06-01T09:59:57.840919","exception":false,"start_time":"2021-06-01T09:59:57.829006","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class config():\n    # For inference\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    use_tqdm = True\n    model_names = ['roberta_large_v15_0', 'roberta_large_v15_3',\n                   'roberta_large_v16_1', \n                   'gpt2_medium_v1_0', \n                   'xlnet_large_cased_v2_0', 'xlnet_large_cased_v3_0', 'xlnet_large_cased_v3_1', \n                   'electra_large_discriminator_v1_0', 'electra_large_discriminator_v1_1',\n                   'deberta_large_v1_0', 'deberta_large_v1_1',\n                   'funnel_large_v1_0',\n                   'bart_large_v1_0']\n    # For dataloader\n    max_len = [250] * 15\n    batch_size = (8, 8, 8, \n                  8, 8,\n                  6, \n                  6, 6, 6,\n                  8, 8,\n                  4, 4,\n                  8,\n                  8)    # In the same order as the 'model_names' attribute\n    num_workers = 4\n    # For models\n    num_bins = (29, 1, 29, \n                29, 29, \n                29, \n                29, 1, 1, \n                1, 1,\n                29, 29,\n                1,\n                1)    # In the same order as the 'model_names' attribute\n    \ncfg = config()","metadata":{"papermill":{"duration":0.070651,"end_time":"2021-06-01T09:59:57.9232","exception":false,"start_time":"2021-06-01T09:59:57.852549","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T15:10:08.123036Z","iopub.execute_input":"2021-07-29T15:10:08.123392Z","iopub.status.idle":"2021-07-29T15:10:08.175966Z","shell.execute_reply.started":"2021-07-29T15:10:08.12334Z","shell.execute_reply":"2021-07-29T15:10:08.174957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{"papermill":{"duration":0.011621,"end_time":"2021-06-01T09:59:57.946721","exception":false,"start_time":"2021-06-01T09:59:57.9351","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Tokenizer and model configuration\ntokenizer_roberta_large = AutoTokenizer.from_pretrained('../input/robertalarge', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_roberta_large = AutoConfig.from_pretrained('../input/robertalarge', output_hidden_states = True)\n\ntokenizer_gpt2_medium = AutoTokenizer.from_pretrained('../input/gpt2-medium', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\ntokenizer_gpt2_medium.add_special_tokens({'pad_token': '[PAD]'})\n\ntokenizer_xlnet_large_cased = AutoTokenizer.from_pretrained('../input/xlnet-large-cased', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\ntokenizer_xlnet_large_cased.add_special_tokens({'pad_token': '[PAD]'})\n\ntokenizer_electra_large = AutoTokenizer.from_pretrained('../input/electra-large-discriminator', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_electra_large = AutoConfig.from_pretrained('../input/electra-large-discriminator', output_hidden_states = True)\n\ntokenizer_deberta_large = AutoTokenizer.from_pretrained('../input/deberta-large', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_debert_large = AutoConfig.from_pretrained('../input/deberta-large', output_hidden_states = True)\n\ntokenizer_funnel_large = AutoTokenizer.from_pretrained('../input/funnel-transformer-large', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_funnel_large = AutoConfig.from_pretrained('../input/funnel-transformer-large', output_hidden_states = True)\n\ntokenizer_bart_large = AutoTokenizer.from_pretrained('../input/bart-large', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_bart_large = AutoConfig.from_pretrained('../input/bart-large', output_hidden_states = True)\n\n# Dataloader\ninfer_dataset_roberta_large_v15_0 = Readability_Dataset(data, tokenizer_roberta_large, max_len = cfg.max_len[0], mode = 'infer')\ninfer_dataloader_roberta_large_v15_0 = DataLoader(infer_dataset_roberta_large_v15_0, batch_size = cfg.batch_size[0], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_roberta_large_v15_3 = Readability_Dataset(data, tokenizer_roberta_large, max_len = cfg.max_len[2], mode = 'infer')\ninfer_dataloader_roberta_large_v15_3 = DataLoader(infer_dataset_roberta_large_v15_3, batch_size = cfg.batch_size[2], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_roberta_large_v16_1 = Readability_Dataset(data, tokenizer_roberta_large, max_len = cfg.max_len[4], mode = 'infer')\ninfer_dataloader_roberta_large_v16_1 = DataLoader(infer_dataset_roberta_large_v16_1, batch_size = cfg.batch_size[4], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_gpt2_medium_v1_0 = Readability_Dataset(data, tokenizer_gpt2_medium, max_len = cfg.max_len[5], mode = 'infer')\ninfer_dataloader_gpt2_medium_v1_0 = DataLoader(infer_dataset_gpt2_medium_v1_0, batch_size = cfg.batch_size[5], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_xlnet_large_cased_v2_0 = Readability_Dataset(data, tokenizer_xlnet_large_cased, max_len = cfg.max_len[6], mode = 'infer')\ninfer_dataloader_xlnet_large_cased_v2_0 = DataLoader(infer_dataset_xlnet_large_cased_v2_0, batch_size = cfg.batch_size[6], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_xlnet_large_cased_v3_0 = Readability_Dataset(data, tokenizer_xlnet_large_cased, max_len = cfg.max_len[7], mode = 'infer')\ninfer_dataloader_xlnet_large_cased_v3_0 = DataLoader(infer_dataset_xlnet_large_cased_v3_0, batch_size = cfg.batch_size[7], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_xlnet_large_cased_v3_1 = Readability_Dataset(data, tokenizer_xlnet_large_cased, max_len = cfg.max_len[8], mode = 'infer')\ninfer_dataloader_xlnet_large_cased_v3_1 = DataLoader(infer_dataset_xlnet_large_cased_v3_1, batch_size = cfg.batch_size[8], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_electra_large_v1_0 = Readability_Dataset(data, tokenizer_electra_large, max_len = cfg.max_len[9], mode = 'infer')\ninfer_dataloader_electra_large_v1_0 = DataLoader(infer_dataset_electra_large_v1_0, batch_size = cfg.batch_size[9], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_electra_large_v1_1 = Readability_Dataset(data, tokenizer_electra_large, max_len = cfg.max_len[10], mode = 'infer')\ninfer_dataloader_electra_large_v1_1 = DataLoader(infer_dataset_electra_large_v1_1, batch_size = cfg.batch_size[10], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_deberta_large_v1_0 = Readability_Dataset(data, tokenizer_deberta_large, max_len = cfg.max_len[11], mode = 'infer')\ninfer_dataloader_deberta_large_v1_0 = DataLoader(infer_dataset_deberta_large_v1_0, batch_size = cfg.batch_size[11], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_deberta_large_v1_1 = Readability_Dataset(data, tokenizer_deberta_large, max_len = cfg.max_len[12], mode = 'infer')\ninfer_dataloader_deberta_large_v1_1 = DataLoader(infer_dataset_deberta_large_v1_1, batch_size = cfg.batch_size[12], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_funnel_large_v1_0 = Readability_Dataset(data, tokenizer_funnel_large, max_len = cfg.max_len[13], mode = 'infer')\ninfer_dataloader_funnel_large_v1_0 = DataLoader(infer_dataset_funnel_large_v1_0, batch_size = cfg.batch_size[13], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_bart_large_v1_0 = Readability_Dataset(data, tokenizer_bart_large, max_len = cfg.max_len[14], mode = 'infer')\ninfer_dataloader_bart_large_v1_0 = DataLoader(infer_dataset_bart_large_v1_0, batch_size = cfg.batch_size[14], num_workers = cfg.num_workers, shuffle = False)\n\n# Prediction storage\nprediction_roberta_large_v15_0 = np.zeros(data.shape[0])\nprediction_roberta_large_v15_2 = np.zeros(data.shape[0])\nprediction_roberta_large_v15_3 = np.zeros(data.shape[0])\nprediction_roberta_large_v16_0 = np.zeros(data.shape[0])\nprediction_roberta_large_v16_1 = np.zeros(data.shape[0])\nprediction_gpt2_medium_v1_0 = np.zeros(data.shape[0])\nprediction_xlnet_large_cased_v2_0 = np.zeros(data.shape[0])\nprediction_xlnet_large_cased_v3_0 = np.zeros(data.shape[0])\nprediction_xlnet_large_cased_v3_1 = np.zeros(data.shape[0])\nprediction_electra_large_v1_0 = np.zeros(data.shape[0])\nprediction_electra_large_v1_1 = np.zeros(data.shape[0])\nprediction_deberta_large_v1_0 = np.zeros(data.shape[0])\nprediction_deberta_large_v1_1 = np.zeros(data.shape[0])\nprediction_funnel_large_v1_0 = np.zeros(data.shape[0])\nprediction_bart_large_v1_0 = np.zeros(data.shape[0])\n\n# Tokenize the benchmark text\nbenchmark_token_roberta_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_roberta_large, cfg.max_len[0], return_tensor = True)\nbenchmark_token_roberta_large = (benchmark_token_roberta_large['input_ids'].to(cfg.device), \n                                 benchmark_token_roberta_large['token_type_ids'].to(cfg.device), \n                                 benchmark_token_roberta_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_gpt2_medium = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_gpt2_medium, cfg.max_len[5], return_tensor = True)\nbenchmark_token_gpt2_medium = (benchmark_token_gpt2_medium['input_ids'].to(cfg.device), \n                               benchmark_token_gpt2_medium['token_type_ids'].to(cfg.device), \n                               benchmark_token_gpt2_medium['attention_mask'].to(cfg.device))\n\nbenchmark_token_xlnet_large_cased = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_xlnet_large_cased, cfg.max_len[6], return_tensor = True)\nbenchmark_token_xlnet_large_cased = (benchmark_token_xlnet_large_cased['input_ids'].to(cfg.device), \n                                     benchmark_token_xlnet_large_cased['token_type_ids'].to(cfg.device), \n                                     benchmark_token_xlnet_large_cased['attention_mask'].to(cfg.device))\n\nbenchmark_token_electra_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_electra_large, cfg.max_len[9], return_tensor = True)\nbenchmark_token_electra_large = (benchmark_token_electra_large['input_ids'].to(cfg.device), \n                                 benchmark_token_electra_large['token_type_ids'].to(cfg.device), \n                                 benchmark_token_electra_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_deberta_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_deberta_large, cfg.max_len[11], return_tensor = True)\nbenchmark_token_deberta_large = (benchmark_token_deberta_large['input_ids'].to(cfg.device), \n                                 benchmark_token_deberta_large['token_type_ids'].to(cfg.device), \n                                 benchmark_token_deberta_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_funnel_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_funnel_large, cfg.max_len[13], return_tensor = True)\nbenchmark_token_funnel_large = (benchmark_token_funnel_large['input_ids'].to(cfg.device), \n                                benchmark_token_funnel_large['token_type_ids'].to(cfg.device), \n                                benchmark_token_funnel_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_bart_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_bart_large, cfg.max_len[14], return_tensor = True)\nbenchmark_token_bart_large = (benchmark_token_bart_large['input_ids'].to(cfg.device), \n                              benchmark_token_bart_large['token_type_ids'].to(cfg.device), \n                              benchmark_token_bart_large['attention_mask'].to(cfg.device))\n\nfor fold in range(5):\n    print('*' * 50)\n    print(f'Fold: {fold}')\n    \n    # Load pretrained models\n    model_name = 'roberta_large'\n    print(f'Inference model, {model_name} version 15-0...')\n    model_roberta_large_v15_0 = Readability_Model_RoBERTa_large_v15('../input/robertalarge', model_config_roberta_large, num_cat = cfg.num_bins[0], \n                                                                    benchmark_token = benchmark_token_roberta_large).to(cfg.device)\n    model_root_path = '../input/clrroberta-largepretrained-modelsv15/model_best_roberta_large_v15_0'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_roberta_large_v15_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_roberta_large_v15_0 += infer(model_roberta_large_v15_0, infer_dataloader_roberta_large_v15_0, device = cfg.device, \n                                            use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_roberta_large) / 5\n    del model_roberta_large_v15_0; gc.collect()\n    \n    model_name = 'roberta_large'\n    print(f'Inference model, {model_name} version 15-3...')\n    model_roberta_large_v15_3 = Readability_Model_RoBERTa_large_v15('../input/robertalarge', model_config_roberta_large, num_cat = cfg.num_bins[2], \n                                                                    benchmark_token = benchmark_token_roberta_large).to(cfg.device)\n    model_root_path = '../input/clrroberta-largev15/model_best_roberta_large_v15_3'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_roberta_large_v15_3.load_state_dict(ckp['model_state_dict'])    \n    prediction_roberta_large_v15_3 += infer(model_roberta_large_v15_3, infer_dataloader_roberta_large_v15_3, device = cfg.device, \n                                            use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_roberta_large) / 5\n    del model_roberta_large_v15_3; gc.collect()\n    \n    model_name = 'roberta_large'\n    print(f'Inference model, {model_name} version 16-1...')\n    model_roberta_large_v16_1 = Readability_Model_RoBERTa_large_v16('../input/robertalarge', model_config_roberta_large, num_cat = cfg.num_bins[4], \n                                                                    benchmark_token = benchmark_token_roberta_large).to(cfg.device)\n    model_root_path = '../input/clrroberta-largepretrained-modelsv16/model_best_roberta_large_v16_1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_roberta_large_v16_1.load_state_dict(ckp['model_state_dict'])\n    prediction_roberta_large_v16_1 += infer(model_roberta_large_v16_1, infer_dataloader_roberta_large_v16_1, device = cfg.device, \n                                            use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_roberta_large) / 5\n    del model_roberta_large_v16_1; gc.collect()\n    \n    model_name = 'gpt2_medium'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_config_gpt2_medium = AutoConfig.from_pretrained('../input/gpt2-medium', output_hidden_states = True)\n    model_gpt2_medium_v1_0 = Readability_Model_GPT2_medium_v1('../input/gpt2-medium', model_config_gpt2_medium, num_cat = cfg.num_bins[5], \n                                                              benchmark_token = benchmark_token_gpt2_medium).to(cfg.device)\n    model_gpt2_medium_v1_0.backbone.resize_token_embeddings(len(tokenizer_gpt2_medium))\n    model_root_path = '../input/clrgpt2-mediumpretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_gpt2_medium_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_gpt2_medium_v1_0 += infer(model_gpt2_medium_v1_0, infer_dataloader_gpt2_medium_v1_0, device = cfg.device, \n                                         use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_gpt2_medium) / 5\n    del model_gpt2_medium_v1_0; gc.collect()\n    \n    model_name = 'xlnet_large_cased'\n    print(f'Inference model, {model_name} version 2-0...')\n    model_config_xlnet_large_cased = AutoConfig.from_pretrained('../input/xlnet-large-cased', output_hidden_states = True)\n    model_xlnet_large_cased_v2_0 = Readability_Model_XLNet_large_cased_v2('../input/xlnet-large-cased', model_config_xlnet_large_cased, \n                                                                          num_cat = cfg.num_bins[6], benchmark_token = benchmark_token_xlnet_large_cased).to(cfg.device)\n    model_xlnet_large_cased_v2_0.backbone.resize_token_embeddings(len(tokenizer_xlnet_large_cased))\n    model_root_path = '../input/clrxlnet-largepretrained-models/v02'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_xlnet_large_cased_v2_0.load_state_dict(ckp['model_state_dict'])\n    prediction_xlnet_large_cased_v2_0 += infer(model_xlnet_large_cased_v2_0, infer_dataloader_xlnet_large_cased_v2_0, device = cfg.device, \n                                               use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_xlnet_large_cased) / 5\n    del model_xlnet_large_cased_v2_0; gc.collect()\n    \n    model_name = 'xlnet_large_cased'\n    print(f'Inference model, {model_name} version 3-0...')\n    model_config_xlnet_large_cased = AutoConfig.from_pretrained('../input/xlnet-large-cased', output_hidden_states = True)\n    model_xlnet_large_cased_v3_0 = Readability_Model_XLNet_large_cased_v3('../input/xlnet-large-cased', model_config_xlnet_large_cased, \n                                                                          num_cat = cfg.num_bins[7], benchmark_token = benchmark_token_xlnet_large_cased).to(cfg.device)\n    model_xlnet_large_cased_v3_0.backbone.resize_token_embeddings(len(tokenizer_xlnet_large_cased))\n    model_root_path = '../input/clrxlnet-largepretrained-models/v03'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_xlnet_large_cased_v3_0.load_state_dict(ckp['model_state_dict'])\n    prediction_xlnet_large_cased_v3_0 += infer(model_xlnet_large_cased_v3_0, infer_dataloader_xlnet_large_cased_v3_0, device = cfg.device, \n                                               use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_xlnet_large_cased) / 5\n    del model_xlnet_large_cased_v3_0; gc.collect()\n    \n    model_name = 'xlnet_large_cased'\n    print(f'Inference model, {model_name} version 3-1...')\n    model_config_xlnet_large_cased = AutoConfig.from_pretrained('../input/xlnet-large-cased', output_hidden_states = True)\n    model_xlnet_large_cased_v3_1 = Readability_Model_XLNet_large_cased_v3('../input/xlnet-large-cased', model_config_xlnet_large_cased, \n                                                                          num_cat = cfg.num_bins[8], benchmark_token = benchmark_token_xlnet_large_cased).to(cfg.device)\n    model_xlnet_large_cased_v3_1.backbone.resize_token_embeddings(len(tokenizer_xlnet_large_cased))\n    model_root_path = '../input/clrxlnet-large-casedv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_xlnet_large_cased_v3_1.load_state_dict(ckp['model_state_dict'])\n    prediction_xlnet_large_cased_v3_1 += infer(model_xlnet_large_cased_v3_1, infer_dataloader_xlnet_large_cased_v3_1, device = cfg.device, \n                                               use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_xlnet_large_cased) / 5\n    del model_xlnet_large_cased_v3_1; gc.collect()\n    \n    model_name = 'electra_large_discriminator'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_electra_large_v1_0 = Readability_Model_ELECTRA_large_discriminator_v1('../input/electra-large-discriminator', model_config_electra_large, \n                                                                                num_cat = cfg.num_bins[9], benchmark_token = benchmark_token_electra_large).to(cfg.device)\n    model_root_path = '../input/clrelectra-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_electra_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_electra_large_v1_0 += infer(model_electra_large_v1_0, infer_dataloader_electra_large_v1_0, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_electra_large) / 5\n    del model_electra_large_v1_0; gc.collect()\n    \n    model_name = 'electra_large_discriminator'\n    print(f'Inference model, {model_name} version 1-1...')\n    model_electra_large_v1_1 = Readability_Model_ELECTRA_large_discriminator_v1('../input/electra-large-discriminator', model_config_electra_large, \n                                                                                num_cat = cfg.num_bins[10], benchmark_token = benchmark_token_electra_large).to(cfg.device)\n    model_root_path = '../input/clrelectra-largev1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_electra_large_v1_1.load_state_dict(ckp['model_state_dict'])    \n    prediction_electra_large_v1_1 += infer(model_electra_large_v1_1, infer_dataloader_electra_large_v1_1, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_electra_large) / 5\n    del model_electra_large_v1_1; gc.collect()\n    \n    model_name = 'deberta_large'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_deberta_large_v1_0 = Readability_Model_DeBERTa_large_v1('../input/deberta-large', model_config_debert_large, \n                                                                  num_cat = cfg.num_bins[11], benchmark_token = benchmark_token_deberta_large).to(cfg.device)\n    model_root_path = '../input/clrdeberta-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_deberta_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_deberta_large_v1_0 += infer(model_deberta_large_v1_0, infer_dataloader_deberta_large_v1_0, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_deberta_large) / 5\n    del model_deberta_large_v1_0; gc.collect()\n    \n    model_name = 'deberta_large'\n    print(f'Inference model, {model_name} version 1-1...')\n    model_deberta_large_v1_1 = Readability_Model_DeBERTa_large_v1('../input/deberta-large', model_config_debert_large, \n                                                                  num_cat = cfg.num_bins[12], benchmark_token = benchmark_token_deberta_large).to(cfg.device)\n    if fold == 3:\n        model_deberta_large_v1_1 = AveragedModel(model_deberta_large_v1_1)\n        \n    model_root_path = '../input/clrdeberta-largev1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_deberta_large_v1_1.load_state_dict(ckp['model_state_dict'])    \n    prediction_deberta_large_v1_1 += infer(model_deberta_large_v1_1, infer_dataloader_deberta_large_v1_1, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_deberta_large) / 5\n    del model_deberta_large_v1_1; gc.collect()\n    \n    model_name = 'funnel_large'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_funnel_large_v1_0 = Readability_Model_Funnel_large_v1('../input/funnel-transformer-large', model_config_funnel_large, num_cat = cfg.num_bins[7], \n                                                                benchmark_token = benchmark_token_funnel_large).to(cfg.device)\n    model_root_path = '../input/clrfunnel-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_funnel_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_funnel_large_v1_0 += infer(model_funnel_large_v1_0, infer_dataloader_funnel_large_v1_0, device = cfg.device, \n                                          use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_funnel_large) / 5\n    del model_funnel_large_v1_0; gc.collect()\n    \n    model_name = 'bart_large'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_bart_large_v1_0 = Readability_Model_BART_large_v1('../input/bart-large', model_config_bart_large, num_cat = cfg.num_bins[8], \n                                                            benchmark_token = benchmark_token_bart_large).to(cfg.device)\n    model_root_path = '../input/clrbart-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_bart_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_bart_large_v1_0 += infer(model_bart_large_v1_0, infer_dataloader_bart_large_v1_0, device = cfg.device, \n                                        use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_bart_large) / 5\n    del model_bart_large_v1_0; gc.collect()","metadata":{"papermill":{"duration":269.555216,"end_time":"2021-06-01T10:04:27.51343","exception":false,"start_time":"2021-06-01T09:59:57.958214","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T15:10:09.615629Z","iopub.execute_input":"2021-07-29T15:10:09.615957Z","iopub.status.idle":"2021-07-29T15:46:22.950466Z","shell.execute_reply.started":"2021-07-29T15:10:09.615926Z","shell.execute_reply":"2021-07-29T15:46:22.948323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Averaging","metadata":{}},{"cell_type":"code","source":"pred_zoo = np.vstack([prediction_roberta_large_v15_0, prediction_roberta_large_v15_3, \n                      prediction_roberta_large_v16_1, \n                      prediction_gpt2_medium_v1_0, \n                      prediction_xlnet_large_cased_v2_0, prediction_xlnet_large_cased_v3_0, prediction_xlnet_large_cased_v3_1, \n                      prediction_electra_large_v1_0, prediction_electra_large_v1_1, \n                      prediction_deberta_large_v1_0, prediction_deberta_large_v1_1, \n                      prediction_funnel_large_v1_0, \n                      prediction_bart_large_v1_0]).T\n\nss['target'] = np.mean(pred_zoo, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:46:22.957449Z","iopub.execute_input":"2021-07-29T15:46:22.957738Z","iopub.status.idle":"2021-07-29T15:46:22.984582Z","shell.execute_reply.started":"2021-07-29T15:46:22.957707Z","shell.execute_reply":"2021-07-29T15:46:22.983644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hiro's part\n##################################################################################################################","metadata":{}},{"cell_type":"code","source":"ENV = 'kaggle'\nassert ENV in ['colab', 'kaggle']\n \nPHASE = 'inference'\nassert PHASE in ['eval_oof','inference']","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:46:22.987041Z","iopub.execute_input":"2021-07-29T15:46:22.987454Z","iopub.status.idle":"2021-07-29T15:46:22.999747Z","shell.execute_reply.started":"2021-07-29T15:46:22.987418Z","shell.execute_reply":"2021-07-29T15:46:22.998899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if ENV=='colab':\n    from google.colab import drive\n    drive.mount('/content/drive')","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:46:23.001517Z","iopub.execute_input":"2021-07-29T15:46:23.001933Z","iopub.status.idle":"2021-07-29T15:46:23.008587Z","shell.execute_reply.started":"2021-07-29T15:46:23.001897Z","shell.execute_reply":"2021-07-29T15:46:23.007806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport time\n \nimport numpy as np\nimport pandas as pd\n \nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n \nimport transformers\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n \nfrom sklearn import datasets\nfrom sklearn import model_selection\n\nimport gc, json, pickle, shutil\ngc.enable()\n\nfrom tqdm.auto import tqdm\nfrom matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:46:23.009712Z","iopub.execute_input":"2021-07-29T15:46:23.010139Z","iopub.status.idle":"2021-07-29T15:46:23.019407Z","shell.execute_reply.started":"2021-07-29T15:46:23.010112Z","shell.execute_reply":"2021-07-29T15:46:23.018528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_folds(data, num_splits, shuffle=False, random_state=None):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=shuffle, random_state=random_state)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:46:23.020757Z","iopub.execute_input":"2021-07-29T15:46:23.021118Z","iopub.status.idle":"2021-07-29T15:46:23.031327Z","shell.execute_reply.started":"2021-07-29T15:46:23.021082Z","shell.execute_reply":"2021-07-29T15:46:23.030403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2021-07-29T15:46:23.032633Z","iopub.execute_input":"2021-07-29T15:46:23.033117Z","iopub.status.idle":"2021-07-29T15:46:23.041135Z","shell.execute_reply.started":"2021-07-29T15:46:23.03308Z","shell.execute_reply":"2021-07-29T15:46:23.040385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)\n            self.bins = torch.tensor(df.bins.values, dtype=torch.long)\n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        out_dict = {'input_ids':input_ids, 'attention_mask':attention_mask}\n        \n        if not NO_TOKEN_TYPE:\n            out_dict['token_type_ids'] = torch.tensor(self.encoded['token_type_ids'][index])\n        \n        if sa_complex is not None:\n            if sa_complex == 'hdd':\n                with open(f'SelfAttComplex/{str(index).zfill(4)}.pkl','rb') as f:\n                    out_dict['sa_complex'] = pickle.load(f)\n            else:\n                out_dict['sa_complex'] = sa_complex[index]\n\n        if not self.inference_only:\n            out_dict['target'] = self.target[index]\n            out_dict['bins'] = self.bins[index]\n\n        return out_dict","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.336388Z","iopub.execute_input":"2021-07-27T23:03:50.336629Z","iopub.status.idle":"2021-07-27T23:03:50.349461Z","shell.execute_reply.started":"2021-07-27T23:03:50.336604Z","shell.execute_reply":"2021-07-27T23:03:50.348608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Self Attention Complexity in Pretrained Model","metadata":{}},{"cell_type":"code","source":"def SelfAttention_Complexity(df: pd.DataFrame, output_device):\n    pre_dataset = LitDataset(df, inference_only=True)\n    pre_loader = DataLoader(pre_dataset, batch_size=BATCH_SIZE,\n                            drop_last=False, shuffle=False)\n    \n    if output_device == 'hdd':\n        os.makedirs('SelfAttComplex', exist_ok=True)\n\n    cfg_update = {\"output_attentions\":True, \"hidden_dropout_prob\": 0.0,\n                  \"layer_norm_eps\": 1e-7}\n    if PHASE=='train':\n        config = AutoConfig.from_pretrained(MODEL_NAME)\n        config.update(cfg_update)\n        backbone = AutoModel.from_pretrained(MODEL_NAME, config=config).to(DEVICE)\n    elif PHASE=='eval_oof' or PHASE=='inference':\n        config = AutoConfig.from_pretrained(LOAD_BACKBONE_DIR)\n        config.update(cfg_update)\n        backbone = AutoModel.from_pretrained(LOAD_BACKBONE_DIR, config=config).to(DEVICE)\n\n    backbone.resize_token_embeddings(len(tokenizer))\n\n    output_sa_complex = []\n    backbone.eval()\n    idx = 0\n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(tqdm(pre_loader)):\n\n            kwargs = {}\n            kwargs['input_ids'] = dsargs['input_ids'].to(DEVICE)\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = dsargs['token_type_ids'].to(DEVICE)\n            kwargs['attention_mask'] = dsargs['attention_mask'].to(DEVICE)\n\n            if 't5' in MODEL_NAME.lower() and HAS_DECODER:\n                # shift to right\n                kwargs['decoder_input_ids'] = torch.cat([tokenizer.pad_token_id * torch.ones(kwargs['input_ids'].size(0), 1).long().to(DEVICE),\n                                                        kwargs['input_ids'][:,:-1]], dim=1)\n            \n            # self attention\n            output_backbone = backbone(**kwargs)\n            self_att = torch.stack(output_backbone.attentions, dim=1) #[batch, layer, head, seq, seq]\n            seq_len = self_att.size(-1)\n            self_att = self_att.view(self_att.size(0), -1, seq_len, seq_len) #[batch, layer*head, seq, seq]\n            self_att *= kwargs['attention_mask'].unsqueeze(1).unsqueeze(-1)\n\n            # self attention complexity\n            distance_from_diag = (torch.arange(seq_len).view(1, -1) - torch.arange(seq_len).view(-1, 1)) / (seq_len - 1)\n            distance_from_diag = distance_from_diag.to(DEVICE)\n            sa_complex = []\n            temp = self_att * distance_from_diag.unsqueeze(0).unsqueeze(1).clip(min=0)\n            temp = temp.sum(dim=-1) #[batch, layer*head, seq]\n            sa_complex.append(temp)\n            temp = self_att * distance_from_diag.unsqueeze(0).unsqueeze(1).clip(max=0).abs()\n            temp = temp.sum(dim=-1) #[batch, layer*head, seq]\n            sa_complex.append(temp)\n            sa_complex = torch.cat(sa_complex, dim=1).transpose(-2,-1) #[batch, seq, layer*head*2]\n\n            if output_device == 'hdd':\n                for batch_item in sa_complex:\n                    with open(f'SelfAttComplex/{str(idx).zfill(4)}.pkl','wb') as f:\n                        pickle.dump(batch_item, f)\n                    idx += 1\n            else:\n                output_sa_complex.append(sa_complex)\n    \n    if output_device == 'hdd':\n        return 'hdd'\n    else:\n        output_sa_complex = torch.cat(output_sa_complex, dim=0)\n        return output_sa_complex.to(output_device)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.352367Z","iopub.execute_input":"2021-07-27T23:03:50.352629Z","iopub.status.idle":"2021-07-27T23:03:50.370358Z","shell.execute_reply.started":"2021-07-27T23:03:50.352605Z","shell.execute_reply":"2021-07-27T23:03:50.369369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\nThe model is inspired by the one from [Maunish](http://https://www.kaggle.com/maunish/clrp-roberta-svm).","metadata":{}},{"cell_type":"code","source":"class LitModel(nn.Module):\n    def __init__(self, benchmark_token=None, use_max_pooling=False, sa_complex_dim=0):\n        super().__init__()\n \n        self.benchmark_token = benchmark_token\n        self.use_max_pooling = use_max_pooling\n        self.sa_complex_dim = sa_complex_dim\n        \n        cfg_update = {\"output_hidden_states\":True, \"hidden_dropout_prob\": 0.0,\n                      \"layer_norm_eps\": 1e-7}\n        if PHASE=='train':\n            config = AutoConfig.from_pretrained(MODEL_NAME)\n            config.save_pretrained(f'{SAVE_DIR}/backbone')\n            config.update(cfg_update)                       \n            self.backbone = AutoModel.from_pretrained(MODEL_NAME, config=config)\n            self.backbone.save_pretrained(f'{SAVE_DIR}/backbone')\n        elif PHASE=='eval_oof' or PHASE=='inference':\n            config = AutoConfig.from_pretrained(LOAD_BACKBONE_DIR)\n            config.update(cfg_update)                       \n            self.backbone = AutoModel.from_pretrained(LOAD_BACKBONE_DIR, config=config)\n            \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(NUM_HIDDEN_LAYERS).view(-1, 1, 1, 1))\n \n        # Dropout layers\n        self.dropouts_regr = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n        self.dropouts_clsi = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n \n        if self.use_max_pooling:\n            num_pool = 2\n        else:\n            num_pool = 1\n        self.attention_layer_norm = nn.LayerNorm(HIDDEN_SIZE * num_pool + sa_complex_dim)\n        self.attention = nn.Sequential(            \n            nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, 512 * num_pool),            \n            nn.Tanh(),                       \n            nn.Linear(512 * num_pool, 1),\n            nn.Softmax(dim=1)\n            )        \n        self.head_regressor = nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, 1)\n        self.head_classifier = nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, NUM_BINS)                   \n \n    def forward(self, input_ids, token_type_ids, attention_mask, self_att_complex):\n\n        kwargs = {}\n        if self.benchmark_token is None:\n            kwargs['input_ids'] = input_ids\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = token_type_ids\n            kwargs['attention_mask'] = attention_mask\n        else:\n            benchmark_input_ids, benchmark_token_type_ids, benchmark_attention_mask = self.benchmark_token\n            kwargs['input_ids'] = torch.cat((input_ids, benchmark_input_ids), dim = 0)\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = torch.cat((token_type_ids, benchmark_token_type_ids), dim = 0)\n            kwargs['attention_mask'] = torch.cat((attention_mask, benchmark_attention_mask), dim = 0)\n\n        if 't5' in MODEL_NAME.lower() and HAS_DECODER:\n            # shift to right\n            kwargs['decoder_input_ids'] = torch.cat([tokenizer.pad_token_id * torch.ones(kwargs['input_ids'].size(0), 1).long().to(DEVICE),\n                                                     kwargs['input_ids'][:,:-1]], dim=1)\n        output_backbone = self.backbone(**kwargs)\n        \n        # Extract output\n        if HAS_DECODER:\n            hidden_states = output_backbone.encoder_hidden_states + output_backbone.decoder_hidden_states[1:]\n        else:\n            hidden_states = output_backbone.hidden_states\n \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = nn.functional.softmax(self.hidden_layer_weights, dim = 0)\n        output_backbone = torch.sum(hidden_states * layer_weight, dim = 0)\n        if self.use_max_pooling:\n            out_max, _ = torch.max(hidden_states, dim = 0)\n            output_backbone = torch.cat((output_backbone, out_max), dim = -1)\n        if self.sa_complex_dim != 0:\n            self_att_complex = torch.cat((self_att_complex, benchmark_sa_complex), dim = 0)\n            output_backbone = torch.cat((output_backbone, self_att_complex), dim = -1)\n        \n        output_backbone = self.attention_layer_norm(output_backbone)\n \n        # Attention Pooling\n        weights = self.attention(output_backbone)\n        context_vector = torch.sum(weights * output_backbone, dim=1)        \n \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_regr):\n            if i == 0:\n                output_regr = self.head_regressor(dropout(context_vector))\n                output_clsi = self.head_classifier(self.dropouts_clsi[i](context_vector))\n            else:\n                output_regr += self.head_regressor(dropout(context_vector))\n                output_clsi += self.head_classifier(self.dropouts_clsi[i](context_vector))\n \n        output_regr /= len(self.dropouts_regr)\n        output_clsi /= len(self.dropouts_clsi)\n\n        if self.benchmark_token is not None:\n            output_regr = output_regr[:-1] - output_regr[-1]\n            output_clsi = output_clsi[:-1]\n\n        # Now we reduce the context vector to the prediction score.\n        return output_regr, nn.functional.softmax(output_clsi, dim=-1)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.371838Z","iopub.execute_input":"2021-07-27T23:03:50.372284Z","iopub.status.idle":"2021-07-27T23:03:50.39703Z","shell.execute_reply.started":"2021-07-27T23:03:50.372246Z","shell.execute_reply":"2021-07-27T23:03:50.396183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss function","metadata":{}},{"cell_type":"code","source":"class QuadraticWeightedKappaLoss(nn.Module):\n    def __init__(self, num_cat, device = 'cpu'):\n        super(QuadraticWeightedKappaLoss, self).__init__()\n        self.num_cat = num_cat\n        cats = torch.arange(num_cat).to(device)\n        self.weights = (cats.view(-1,1) - cats.view(1,-1)).pow(2) / (num_cat - 1)**2\n        \n    def _confusion_matrix(self, pred_smax, true_cat):\n        confusion_matrix = torch.zeros((self.num_cat, self.num_cat)).to(pred_smax.device)\n        for t, p in zip(true_cat.view(-1), pred_smax):\n            confusion_matrix[t.long()] += p\n        return confusion_matrix\n        \n    def forward(self, pred_smax, true_cat):\n        # Confusion matrix\n        O = self._confusion_matrix(pred_smax, true_cat)\n        \n        # Count elements in each category\n        true_hist = torch.bincount(true_cat, minlength = self.num_cat)\n        pred_hist = pred_smax.sum(dim = 0)\n        \n        # Expected values\n        E = torch.outer(true_hist, pred_hist)\n        \n        # Normlization\n        O = O / torch.sum(O)\n        E = E / torch.sum(E)\n        \n        # Weighted Kappa\n        numerator = torch.sum(self.weights * O)\n        denominator = torch.sum(self.weights * E)\n        \n        return COEF_QWK * numerator / denominator\n    \nclass BradleyTerryLoss(nn.Module):\n    def __init__(self):\n        super(BradleyTerryLoss, self).__init__()\n\n    def forward(self, pred_mean, true_mean):\n        batch_size = len(pred_mean)\n        true_comparison = true_mean.view(-1,1) - true_mean.view(1,-1)\n        pred_comparison = pred_mean.view(-1,1) - pred_mean.view(1,-1)\n        \n        return COEF_BT * (torch.log(1 + torch.tril(torch.exp(-true_comparison * pred_comparison))).sum()\n                          / (batch_size * (batch_size - 1) / 2))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.398484Z","iopub.execute_input":"2021-07-27T23:03:50.398914Z","iopub.status.idle":"2021-07-27T23:03:50.413066Z","shell.execute_reply.started":"2021-07-27T23:03:50.398874Z","shell.execute_reply":"2021-07-27T23:03:50.41219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_mse(model, data_loader):\n    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n    model.eval()            \n    mse_sum = 0\n\n    all_pred_r = []\n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(data_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n            target = dsargs['target'].to(DEVICE)\n            bins = dsargs['bins'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n\n            pred_r, _ = model(input_ids, token_type_ids, attention_mask, self_att_complex)                       \n\n            mse_sum += nn.MSELoss(reduction=\"sum\")(pred_r.flatten(), target).item()\n            all_pred_r.append(pred_r)\n\n    return mse_sum / len(data_loader.dataset), torch.cat(all_pred_r, dim=0).squeeze()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.414334Z","iopub.execute_input":"2021-07-27T23:03:50.41474Z","iopub.status.idle":"2021-07-27T23:03:50.425469Z","shell.execute_reply.started":"2021-07-27T23:03:50.414701Z","shell.execute_reply":"2021-07-27T23:03:50.424533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training, Validation","metadata":{}},{"cell_type":"code","source":"def train(model, model_path, train_loader, val_loader,\n          optimizer, num_epochs, fold, scheduler=None):    \n    best_val_rmse = None\n    best_epoch = 0\n    step = 0\n    last_eval_step = 0\n    eval_period = EVAL_SCHEDULE[0][1]    \n\n    start = time.time()\n\n    history = {'step':[], 'epoch':[], 'batch_num':[], 'val_rmse':[],\n               'trn_rmse':[], 'trn_qwk':[], 'trn_bt':[]}\n    \n    for epoch in range(num_epochs):\n        val_rmse = None         \n\n        epoch_target, epoch_bins, epoch_pred_r, epoch_pred_c = (torch.tensor([]),)*4\n        epoch_bins = epoch_bins.long()\n    \n        for batch_num, dsargs in enumerate(train_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n            target = dsargs['target'].to(DEVICE)\n            bins = dsargs['bins'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n\n            optimizer.zero_grad()\n            \n            model.train()\n\n            pred_r, pred_c = model(input_ids, token_type_ids, attention_mask, self_att_complex)\n                                                        \n            loss = (nn.MSELoss(reduction=\"mean\")(pred_r.flatten(), target)\n                    + QWKloss(pred_c, bins) + BTloss(pred_r.flatten(), target))\n                        \n            loss.backward()\n            \n            epoch_target = torch.cat([epoch_target.to(DEVICE), target.clone().detach()], dim=0)\n            epoch_bins = torch.cat([epoch_bins.to(DEVICE), bins.clone().detach()], dim=0)\n            epoch_pred_r = torch.cat([epoch_pred_r.to(DEVICE), pred_r.clone().detach()], dim=0)\n            epoch_pred_c = torch.cat([epoch_pred_c.to(DEVICE), pred_c.clone().detach()], dim=0)\n\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            \n            if step >= last_eval_step + eval_period:\n                # Evaluate the model on val_loader.\n                elapsed_seconds = time.time() - start\n                num_steps = step - last_eval_step\n                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n                last_eval_step = step\n                \n                mse, _ = eval_mse(model, val_loader)\n                val_rmse = math.sqrt(mse)\n                trn_rmse = nn.MSELoss(reduction=\"mean\")(epoch_pred_r.flatten(), epoch_target).item()\n                trn_qwk  = QWKloss(epoch_pred_c, epoch_bins).item()\n                trn_bt  = BTloss(epoch_pred_r.flatten(), epoch_target).item()\n\n                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n                      f\"val_rmse: {val_rmse:0.4}\", f\"train_rmse: {trn_rmse:0.4}\",\n                      f\"train_qwk: {trn_qwk:0.4}\", f\"train_bt: {trn_bt:0.4}\")\n\n                for rmse, period in EVAL_SCHEDULE:\n                    if val_rmse >= rmse:\n                        eval_period = period\n                        break\n                percent = step / (num_epochs * len(train_loader))\n                if 0.5 <= percent and percent <= 0.8:\n                    eval_period = min([eval_period, 8])\n                \n                if not best_val_rmse or val_rmse < best_val_rmse:                    \n                    best_val_rmse = val_rmse\n                    best_epoch = epoch\n                    torch.save(model.state_dict(), model_path)\n                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n                else:       \n                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n                          f\"(from epoch {best_epoch})\")\n\n                ''' history json dump '''\n                history['step'].append(step)\n                history['epoch'].append(epoch)\n                history['batch_num'].append(batch_num)\n                history['val_rmse'].append(val_rmse)\n                history['trn_rmse'].append(trn_rmse)\n                history['trn_qwk'].append(trn_qwk)\n                history['trn_bt'].append(trn_bt)\n                with open(f'{SAVE_DIR}/{MODEL_VER}_fold{fold+1}_history.json', 'w') as f:\n                    json.dump(history, f, indent=4)\n                    \n                start = time.time()\n                                            \n            step += 1\n\n        del epoch_target, epoch_bins, epoch_pred_r, epoch_pred_c\n        \n        print('\\nHidden Layer Weights:')\n        print(model.hidden_layer_weights.squeeze())\n        print(nn.functional.softmax(model.hidden_layer_weights.squeeze(),dim=0))\n    \n    return best_val_rmse","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.426881Z","iopub.execute_input":"2021-07-27T23:03:50.427315Z","iopub.status.idle":"2021-07-27T23:03:50.449438Z","shell.execute_reply.started":"2021-07-27T23:03:50.427275Z","shell.execute_reply":"2021-07-27T23:03:50.448601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(data_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n                        \n            pred_r, _ = model(input_ids, token_type_ids, attention_mask, self_att_complex)                        \n\n            result[index : index + pred_r.shape[0]] = pred_r.flatten().to(\"cpu\")\n            index += pred_r.shape[0]\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.450869Z","iopub.execute_input":"2021-07-27T23:03:50.451294Z","iopub.status.idle":"2021-07-27T23:03:50.460125Z","shell.execute_reply.started":"2021-07-27T23:03:50.451257Z","shell.execute_reply":"2021-07-27T23:03:50.459116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())\n    \n    backbone_parameters = [(n, p) for n, p in named_parameters if n.startswith('backbone')]\n    attention_parameters = [(n, p) for n, p in named_parameters if n.startswith('attention')]\n    hidden_wts_parameters = [(n, p) for n, p in named_parameters if n.startswith ('hidden_layer_weights')]\n    head_parameters = [(n, p) for n, p in named_parameters if n.startswith('head')]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    hidden_wts_group = [params for (name, params) in hidden_wts_parameters]\n    head_group = [params for (name, params) in head_parameters]\n \n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": hidden_wts_group, 'weight_decay': 0.0, 'lr': HIDDEN_WTS_LR})\n    parameters.append({\"params\": head_group})\n \n    no_decay = ['bias', 'LayerNorm.weight', 'layer_norm']\n \n    if 'roberta' in MODEL_NAME.lower() or 'electra' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').embeddings] + list(getattr(model, 'backbone').encoder.layer)\n    elif 'gpt2' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').wte] + list(getattr(model, 'backbone').h)\n    elif 'xlnet' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').word_embedding] + list(getattr(model, 'backbone').layer)\n    elif 'bart' in MODEL_NAME.lower():\n        enc_layers = ([getattr(model, 'backbone').encoder.embed_positions] +\n                      list(getattr(model, 'backbone').encoder.layers) +\n                      [getattr(model, 'backbone').encoder.layernorm_embedding])\n        dec_layers = ([getattr(model, 'backbone').decoder.embed_positions] +\n                      list(getattr(model, 'backbone').decoder.layers) + \n                      [getattr(model, 'backbone').decoder.layernorm_embedding])\n        assert len(enc_layers)==len(dec_layers)\n        layers = [getattr(model, 'backbone').shared]\n        for e, d in zip(enc_layers, dec_layers):\n            layers += [e, d]\n    elif 't5' in MODEL_NAME.lower():\n        enc_layers = (list(getattr(model, 'backbone').encoder.block) +\n                      [getattr(model, 'backbone').encoder.final_layer_norm])\n        dec_layers = (list(getattr(model, 'backbone').decoder.block) + \n                      [getattr(model, 'backbone').decoder.final_layer_norm])\n        assert len(enc_layers)==len(dec_layers)\n        layers = [getattr(model, 'backbone').shared]\n        for e, d in zip(enc_layers, dec_layers):\n            layers += [e, d]\n    else:\n        raise RuntimeError('specify the parameters for backbone.')\n \n    layers.reverse()\n    layerwise_learning_rate_decay = LAYERWISE_LR_DECAY**(1.0/len(layers))\n    lr = BACKBONE_LR\n    for i, layer in enumerate(layers):\n        lr *= layerwise_learning_rate_decay\n        parameters += [\n            {\n                'params': [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n                'weight_decay': 0.01,\n                'lr': lr,\n            },\n            {\n                'params': [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.0,\n                'lr': lr,\n            },\n        ]\n \n    return AdamW(parameters)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.461419Z","iopub.execute_input":"2021-07-27T23:03:50.461925Z","iopub.status.idle":"2021-07-27T23:03:50.481371Z","shell.execute_reply.started":"2021-07-27T23:03:50.461888Z","shell.execute_reply":"2021-07-27T23:03:50.480571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_examples_to_features(text, tokenizer, max_len, is_test = False, return_tensor = False):\n    # Take from https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit\n    text = text.replace('\\n', '')\n    if return_tensor:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            return_tensors = 'pt',\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n    else:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n    return tok","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.483799Z","iopub.execute_input":"2021-07-27T23:03:50.484201Z","iopub.status.idle":"2021-07-27T23:03:50.49227Z","shell.execute_reply.started":"2021-07-27T23:03:50.484144Z","shell.execute_reply":"2021-07-27T23:03:50.491491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Train_or_Validation():\n    list_val_rmse = []\n \n    oof = []\n    for fold in range(NUM_FOLDS):\n        print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n            \n        set_random_seed(SEED + fold)\n        \n        train_dataset = LitDataset(train_df[train_df['kfold'] != fold])\n        val_dataset = LitDataset(train_df[train_df['kfold'] == fold])\n        val_df = train_df[train_df['kfold'] == fold].copy()\n            \n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                                  drop_last=True, shuffle=True, num_workers=0)    \n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                                drop_last=False, shuffle=False, num_workers=0)    \n        \n        sa_complex_dim = 0\n        if USE_SELF_ATT:\n            sa_complex_dim = benchmark_sa_complex.size(-1)\n        \n        model = LitModel(benchmark_token = benchmark_token, use_max_pooling = USE_MAX_POOLING,\n                         sa_complex_dim = sa_complex_dim).to(DEVICE)\n        \n        # Update vocabulary size\n        model.backbone.resize_token_embeddings(len(tokenizer))\n \n        if PHASE=='train':\n            model_path = f\"{SAVE_DIR}/model_{fold + 1}.bin\"\n            set_random_seed(SEED + fold)    \n \n            optimizer = create_optimizer(model)                        \n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer,\n                num_training_steps = NUM_EPOCHS * len(train_loader) * 11//10,\n                num_warmup_steps = 50)\n            \n            list_val_rmse.append(train(model, model_path, train_loader, val_loader, optimizer, \n                                       num_epochs=NUM_EPOCHS, fold=fold, scheduler=scheduler, ))\n        \n        elif PHASE=='eval_oof':\n            model_path = f\"{MODEL_DIR}/model_{fold + 1}.bin\"\n            model.load_state_dict(torch.load(model_path))\n            model.to(DEVICE)\n            \n            mse, pred_r = eval_mse(model, val_loader)\n            val_df['pred'] = pred_r.to('cpu').detach().numpy().copy()\n            oof.append(val_df)\n            list_val_rmse.append(math.sqrt(mse))\n \n        del model\n        gc.collect()\n        \n        print(\"\\nPerformance estimates:\")\n        print(list_val_rmse)\n        print(\"Mean:\", np.array(list_val_rmse).mean())\n\n    if PHASE=='eval_oof':\n        oof = pd.concat(oof).set_index('id').sort_index()\n\n    return oof","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.493393Z","iopub.execute_input":"2021-07-27T23:03:50.493652Z","iopub.status.idle":"2021-07-27T23:03:50.507917Z","shell.execute_reply.started":"2021-07-27T23:03:50.493627Z","shell.execute_reply":"2021-07-27T23:03:50.506834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Inference():\n    all_predictions = np.zeros((NUM_FOLDS, len(test_df)))\n\n    test_dataset = LitDataset(test_df, inference_only=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                             drop_last=False, shuffle=False, num_workers=0)\n\n    for fold in range(NUM_FOLDS):            \n\n        sa_complex_dim = 0\n        if USE_SELF_ATT:\n            sa_complex_dim = benchmark_sa_complex.size(-1)\n\n        model = LitModel(benchmark_token = benchmark_token, use_max_pooling = USE_MAX_POOLING,\n                         sa_complex_dim = sa_complex_dim).to(DEVICE)\n\n        # Update vocabulary size\n        model.backbone.resize_token_embeddings(len(tokenizer))\n\n        model_path = f\"{MODEL_DIR}/model_{fold + 1}.bin\"\n        print(f\"\\nUsing {model_path}\")\n                            \n        model.load_state_dict(torch.load(model_path))    \n        \n        all_predictions[fold] = predict(model, test_loader)\n        \n        del model\n        gc.collect()\n\n    predictions = all_predictions.mean(axis=0)\n    output_df = submission_df.copy()\n    output_df.target = predictions\n    print(output_df)\n\n    return output_df","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.509512Z","iopub.execute_input":"2021-07-27T23:03:50.510355Z","iopub.status.idle":"2021-07-27T23:03:50.519442Z","shell.execute_reply.started":"2021-07-27T23:03:50.51019Z","shell.execute_reply":"2021-07-27T23:03:50.51862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"### RoBERTa-large Ver.31","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '/content/drive/MyDrive/Colab Notebooks/CLR/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '../input/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = '../input/step-1-create-folds'\n\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}/train_folds.csv')\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.52081Z","iopub.execute_input":"2021-07-27T23:03:50.521196Z","iopub.status.idle":"2021-07-27T23:03:50.636171Z","shell.execute_reply.started":"2021-07-27T23:03:50.521158Z","shell.execute_reply":"2021-07-27T23:03:50.63536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 1000\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 8\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'roberta-large'\nMODEL_VER = 'CLRP_LightBase_031_RoBERTaL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = True\nUSE_SELF_ATT = True\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 2e-5\nHIDDEN_WTS_LR = 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'/content/drive/MyDrive/Colab Notebooks/CLR/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '../input/clrp-lightbase-031-robertal-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '../input/robertalarge'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df_RoBERTaL031 = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df_RoBERTaL031.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df_RoBERTaL031 = Inference()\n    submission_df_RoBERTaL031.to_csv(f\"submission_{MODEL_VER}.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:03:50.637414Z","iopub.execute_input":"2021-07-27T23:03:50.637746Z","iopub.status.idle":"2021-07-27T23:06:01.935145Z","shell.execute_reply.started":"2021-07-27T23:03:50.637708Z","shell.execute_reply":"2021-07-27T23:06:01.934268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RoBERTa-large Ver.31b","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '/content/drive/MyDrive/Colab Notebooks/CLR/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '../input/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = '../input/step-1-create-folds'\n\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}/train_folds.csv')\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:06:01.937357Z","iopub.execute_input":"2021-07-27T23:06:01.937717Z","iopub.status.idle":"2021-07-27T23:06:02.011603Z","shell.execute_reply.started":"2021-07-27T23:06:01.937679Z","shell.execute_reply":"2021-07-27T23:06:02.01084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 2319\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 8\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'roberta-large'\nMODEL_VER = 'CLRP_LightBase_031b_RoBERTaL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = True\nUSE_SELF_ATT = True\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 2e-5\nHIDDEN_WTS_LR = 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'/content/drive/MyDrive/Colab Notebooks/CLR/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '../input/clrp-lightbase-031b-robertal-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '../input/robertalarge'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df_RoBERTaL031b = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df_RoBERTaL031b.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df_RoBERTaL031b = Inference()\n    submission_df_RoBERTaL031b.to_csv(f\"submission_{MODEL_VER}.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:06:02.012939Z","iopub.execute_input":"2021-07-27T23:06:02.013316Z","iopub.status.idle":"2021-07-27T23:08:02.286357Z","shell.execute_reply.started":"2021-07-27T23:06:02.013276Z","shell.execute_reply":"2021-07-27T23:08:02.285513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### kfold: Ver.2, RoBERTa-large Ver.26a","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '/content/drive/MyDrive/Colab Notebooks/CLR/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '../input/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = BASE_DIR\n\n# train_df = pd.read_csv(f'{TRAIN_DATA_DIR}/train_folds.csv')\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}/train.csv')\ntrain_df = create_folds(train_df, num_splits=5, shuffle=True, random_state=1605)\n\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:08:02.288589Z","iopub.execute_input":"2021-07-27T23:08:02.288969Z","iopub.status.idle":"2021-07-27T23:08:02.40789Z","shell.execute_reply.started":"2021-07-27T23:08:02.288928Z","shell.execute_reply":"2021-07-27T23:08:02.407016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 1605\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 8\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'roberta-large'\nMODEL_VER = 'CLRP_LightBase_kfoldv2_026a_RoBERTaL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = True\nUSE_SELF_ATT = False\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 2e-5\nHIDDEN_WTS_LR = 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'/content/drive/MyDrive/Colab Notebooks/CLR/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '../input/clrp-lightbase-kfoldv2-026a-robertal-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '../input/robertalarge'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df_kfv2_RoBERTaL026a = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df_kfv2_RoBERTaL026a.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df_kfv2_RoBERTaL026a = Inference()\n    submission_df_kfv2_RoBERTaL026a.to_csv(f\"submission_{MODEL_VER}.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:08:02.409274Z","iopub.execute_input":"2021-07-27T23:08:02.409619Z","iopub.status.idle":"2021-07-27T23:09:51.275423Z","shell.execute_reply.started":"2021-07-27T23:08:02.409582Z","shell.execute_reply":"2021-07-27T23:09:51.27393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ELECTRA-large Ver.4","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '/content/drive/MyDrive/Colab Notebooks/CLR/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '../input/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = '../input/step-1-create-folds'\n\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}/train_folds.csv')\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:09:51.27752Z","iopub.execute_input":"2021-07-27T23:09:51.277851Z","iopub.status.idle":"2021-07-27T23:09:51.354787Z","shell.execute_reply.started":"2021-07-27T23:09:51.277805Z","shell.execute_reply":"2021-07-27T23:09:51.353976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 1605\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 8\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'google/electra-large-discriminator'\nMODEL_VER = 'CLRP_LightBase_004_ElectraL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = False\nUSE_SELF_ATT = False\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 2e-5\nHIDDEN_WTS_LR = 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'/content/drive/MyDrive/Colab Notebooks/CLR/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '../input/clrp-lightbase-004-electral-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '../input/electra-large-discriminator'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df_ElectraL004 = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df_ElectraL004.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df_ElectraL004 = Inference()\n    submission_df_ElectraL004.to_csv(f\"submission_{MODEL_VER}.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:09:51.356127Z","iopub.execute_input":"2021-07-27T23:09:51.356493Z","iopub.status.idle":"2021-07-27T23:11:34.769409Z","shell.execute_reply.started":"2021-07-27T23:09:51.356453Z","shell.execute_reply":"2021-07-27T23:11:34.768422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### kfold: Ver.3, DeBERTa-large Ver.14","metadata":{}},{"cell_type":"code","source":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '/content/drive/MyDrive/Colab Notebooks/CLR/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '../input/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = BASE_DIR\n\n# train_df = pd.read_csv(f'{TRAIN_DATA_DIR}/train_folds.csv')\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}/train.csv')\ntrain_df = create_folds(train_df, num_splits=5, shuffle=True, random_state=321)\n\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:11:34.770803Z","iopub.execute_input":"2021-07-27T23:11:34.771427Z","iopub.status.idle":"2021-07-27T23:11:34.857163Z","shell.execute_reply.started":"2021-07-27T23:11:34.77138Z","shell.execute_reply":"2021-07-27T23:11:34.856285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 1000\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 5\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'microsoft/deberta-large'\nMODEL_VER = 'CLRP_LightBase_kfoldv3_014_DeBERTaL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = True\nUSE_SELF_ATT = False\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 1e-5 # 2e-5\nHIDDEN_WTS_LR = 5e-3 # 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'/content/drive/MyDrive/Colab Notebooks/CLR/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '../input/clrp-lightbase-kfoldv3-014-debertal-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '../input/deberta-large'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df_kfv3_DeBERTaL014 = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df_kfv3_DeBERTaL014.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df_kfv3_DeBERTaL014 = Inference()\n    submission_df_kfv3_DeBERTaL014.to_csv(f\"submission_{MODEL_VER}.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:11:34.858534Z","iopub.execute_input":"2021-07-27T23:11:34.858898Z","iopub.status.idle":"2021-07-27T23:13:38.920206Z","shell.execute_reply.started":"2021-07-27T23:11:34.858857Z","shell.execute_reply":"2021-07-27T23:13:38.919308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"weight = [0.4057247639 / 3] * 3 + [0.2704413533, 0.3238339126]\n\nif PHASE=='eval_oof':\n    oof_df = oof_df_RoBERTaL031[['pred']].copy()\n    oof_df['pred'] = (weight[0] * oof_df_RoBERTaL031['pred'].values +\n                      weight[1] * oof_df_RoBERTaL031b['pred'].values +\n                      weight[2] * oof_df_kfv2_RoBERTaL026a['pred'].values +\n                      weight[3] * oof_df_ElectraL004['pred'].values+\n                      weight[4] * oof_df_kfv3_DeBERTaL014['pred'].values)\n    oof_df.to_csv(f'oof_ensemble.csv')\n\nif PHASE=='inference':\n    submission_df['target'] = (weight[0] * submission_df_RoBERTaL031['target'].values +\n                               weight[1] * submission_df_RoBERTaL031b['target'].values +\n                               weight[2] * submission_df_kfv2_RoBERTaL026a['target'].values +\n                               weight[3] * submission_df_ElectraL004['target'].values +\n                               weight[4] * submission_df_kfv3_DeBERTaL014['target'].values)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:13:38.921555Z","iopub.execute_input":"2021-07-27T23:13:38.921961Z","iopub.status.idle":"2021-07-27T23:13:38.931906Z","shell.execute_reply.started":"2021-07-27T23:13:38.921917Z","shell.execute_reply":"2021-07-27T23:13:38.931012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meta ensemble\n##################################################################################################################","metadata":{}},{"cell_type":"code","source":"meta_weights = [0.42, 0.58]\nss['target'] = ss['target'].values * meta_weights[0] + submission_df['target'].values * meta_weights[1]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T23:13:38.933326Z","iopub.execute_input":"2021-07-27T23:13:38.93373Z","iopub.status.idle":"2021-07-27T23:13:38.944437Z","shell.execute_reply.started":"2021-07-27T23:13:38.933691Z","shell.execute_reply":"2021-07-27T23:13:38.943577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final submission\n##################################################################################################################","metadata":{"papermill":{"duration":0.014656,"end_time":"2021-06-01T10:04:27.543904","exception":false,"start_time":"2021-06-01T10:04:27.529248","status":"completed"},"tags":[]}},{"cell_type":"code","source":"ss.to_csv('submission.csv', index = None)\nss","metadata":{"papermill":{"duration":0.105064,"end_time":"2021-06-01T10:04:27.663568","exception":false,"start_time":"2021-06-01T10:04:27.558504","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-27T23:13:38.945606Z","iopub.execute_input":"2021-07-27T23:13:38.945984Z","iopub.status.idle":"2021-07-27T23:13:38.960534Z","shell.execute_reply.started":"2021-07-27T23:13:38.945946Z","shell.execute_reply":"2021-07-27T23:13:38.959616Z"},"trusted":true},"execution_count":null,"outputs":[]}]}