{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Version 12\n#### Model list\n* RoBERTa-large version 15-0, 15-3;\n* RoBERTa-large version 16-1;\n* XLNet-large-cased version 2-0, 3-0, 3-1;\n* GPT2-medium version 1-0;\n* ELECTRA-large-discriminator version 1-0, 1-1;\n* DeBERTa-large version 1-0, 1-1;\n* Funnel-large version 1-0;\n* BART-large version 1-0","metadata":{"papermill":{"duration":0.036814,"end_time":"2021-07-27T17:22:54.142989","exception":false,"start_time":"2021-07-27T17:22:54.106175","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport random\nimport gc\n\nimport sys\nsys.path.append('../input/readability-package')\nimport readability\nimport spacy\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, pos_tag_sents\nimport string\nimport re\nimport math\nimport pickle\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n# Autocast\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.swa_utils import AveragedModel\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, AdamW\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":9.471301,"end_time":"2021-07-27T17:23:03.649416","exception":false,"start_time":"2021-07-27T17:22:54.178115","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:27.443741Z","iopub.execute_input":"2021-07-29T10:10:27.444146Z","iopub.status.idle":"2021-07-29T10:10:38.40532Z","shell.execute_reply.started":"2021-07-29T10:10:27.444064Z","shell.execute_reply":"2021-07-29T10:10:38.404386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r ../input/spacy-readability/spacy_readability-master/* ./\n!cp -r ../input/syllapy/syllapy-master/* ./\nimport spacy\nfrom spacy_readability import Readability\n\nnlp = spacy.load('en')\nnlp.add_pipe(Readability(), last = True)","metadata":{"papermill":{"duration":2.626173,"end_time":"2021-07-27T17:23:06.310226","exception":false,"start_time":"2021-07-27T17:23:03.684053","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:38.406849Z","iopub.execute_input":"2021-07-29T10:10:38.407234Z","iopub.status.idle":"2021-07-29T10:10:41.347872Z","shell.execute_reply.started":"2021-07-29T10:10:38.407191Z","shell.execute_reply":"2021-07-29T10:10:41.346811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed = 0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed = 0\nseed_everything(seed)","metadata":{"papermill":{"duration":0.045256,"end_time":"2021-07-27T17:23:06.390392","exception":false,"start_time":"2021-07-27T17:23:06.345136","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.35001Z","iopub.execute_input":"2021-07-29T10:10:41.350421Z","iopub.status.idle":"2021-07-29T10:10:41.363719Z","shell.execute_reply.started":"2021-07-29T10:10:41.350377Z","shell.execute_reply":"2021-07-29T10:10:41.362403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import data","metadata":{"papermill":{"duration":0.034116,"end_time":"2021-07-27T17:23:06.459572","exception":false,"start_time":"2021-07-27T17:23:06.425456","status":"completed"},"tags":[]}},{"cell_type":"code","source":"base_dir = '../input/commonlitreadabilityprize'\ntrain_data = pd.read_csv(f'{base_dir}/train.csv')\n# Benchmark text\nbenchmark = train_data[train_data['standard_error'] == 0.]","metadata":{"papermill":{"duration":0.134846,"end_time":"2021-07-27T17:23:06.628642","exception":false,"start_time":"2021-07-27T17:23:06.493796","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.367251Z","iopub.execute_input":"2021-07-29T10:10:41.367553Z","iopub.status.idle":"2021-07-29T10:10:41.535463Z","shell.execute_reply.started":"2021-07-29T10:10:41.367518Z","shell.execute_reply":"2021-07-29T10:10:41.53439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_dir = '../input/commonlitreadabilityprize'\ndata = pd.read_csv(f'{base_dir}/test.csv')\nss = pd.read_csv(f'{base_dir}/sample_submission.csv')\ndata.head()","metadata":{"papermill":{"duration":0.062258,"end_time":"2021-07-27T17:23:06.725748","exception":false,"start_time":"2021-07-27T17:23:06.66349","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.537067Z","iopub.execute_input":"2021-07-29T10:10:41.537694Z","iopub.status.idle":"2021-07-29T10:10:41.579452Z","shell.execute_reply.started":"2021-07-29T10:10:41.537632Z","shell.execute_reply":"2021-07-29T10:10:41.578604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{"papermill":{"duration":0.035153,"end_time":"2021-07-27T17:23:06.796086","exception":false,"start_time":"2021-07-27T17:23:06.760933","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower().strip()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ndef text_preprocessing(text):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    combined_text = ' '.join(tokenized_text)\n    return combined_text\n\ndef readability_feat(text):\n    text = nlp(text)\n    \n    return np.array([text._.flesch_kincaid_grade_level,\n                     text._.flesch_kincaid_reading_ease,\n                     text._.dale_chall,\n                     text._.coleman_liau_index,\n                     text._.automated_readability_index,\n                     text._.forcast], dtype = np.float)\n\ndef sample_text(targets, num_output = 5):\n    mean, var = targets[0], targets[1]\n    if targets[1] != 0.:\n        sampled_target = torch.normal(mean, var, size = (num_output,))\n    else:\n        sampled_target = torch.tensor([0.] * num_output, dtype = torch.float)\n    return sampled_target\n\ndef convert_examples_to_features(text, tokenizer, max_len, is_test = False, return_tensor = False):\n    # Take from https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-fit\n    text = text.replace('\\n', '')\n    if return_tensor:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            return_tensors = 'pt',\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = True\n        )\n    else:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = True\n        )\n    return tok\n\ndef form_dataset(token, external_features = None, target = None, bins = None):\n    if target is not None:\n        if bins is not None:\n            if external_features is not None:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'external_features': torch.tensor(external_features, dtype = torch.float),\n                    'target': target,\n                    'bins': bins,\n                }\n            else:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'target': target,\n                    'bins': bins,\n                }\n        else:\n            if external_features is not None:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'external_features': torch.tensor(external_features, dtype = torch.float),\n                    'target': target,\n                }\n            else:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'target': target,\n                }\n    else:\n        if external_features is not None:\n            return {\n                'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                'external_features': torch.tensor(external_features, dtype = torch.float),\n            }\n        else:\n            return {\n                'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n            }","metadata":{"papermill":{"duration":0.059709,"end_time":"2021-07-27T17:23:06.890564","exception":false,"start_time":"2021-07-27T17:23:06.830855","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.581043Z","iopub.execute_input":"2021-07-29T10:10:41.581412Z","iopub.status.idle":"2021-07-29T10:10:41.606256Z","shell.execute_reply.started":"2021-07-29T10:10:41.581373Z","shell.execute_reply":"2021-07-29T10:10:41.604916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"papermill":{"duration":0.034507,"end_time":"2021-07-27T17:23:06.959779","exception":false,"start_time":"2021-07-27T17:23:06.925272","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Dataset(Dataset):\n    def __init__(self, documents, tokenizer, max_len = 300, mode = 'infer'):\n        self.documents = documents\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.documents)\n    \n    def __getitem__(self, idx):\n        sample = self.documents.iloc[idx]\n        document = sample['excerpt']\n        \n        # Tokenize\n        features = convert_examples_to_features(document, self.tokenizer, self.max_len)\n        \n        return form_dataset(features)","metadata":{"papermill":{"duration":0.042977,"end_time":"2021-07-27T17:23:07.03752","exception":false,"start_time":"2021-07-27T17:23:06.994543","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.608215Z","iopub.execute_input":"2021-07-29T10:10:41.608695Z","iopub.status.idle":"2021-07-29T10:10:41.619753Z","shell.execute_reply.started":"2021-07-29T10:10:41.608631Z","shell.execute_reply":"2021-07-29T10:10:41.618807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"papermill":{"duration":0.034692,"end_time":"2021-07-27T17:23:07.107376","exception":false,"start_time":"2021-07-27T17:23:07.072684","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Utils class","metadata":{"papermill":{"duration":0.03522,"end_time":"2021-07-27T17:23:07.177404","exception":false,"start_time":"2021-07-27T17:23:07.142184","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class AttentivePooling(nn.Module):\n    def __init__(self, input_dim = 768, attention_dim = 1024):\n        super(AttentivePooling, self).__init__()\n        # Attention pooler\n        self.word_weight = nn.Linear(input_dim, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n    def forward(self, x, mask = None):\n        '''\n        x : Batch_size x Seq_len x input_dim\n        mask: \n        '''\n        # Attention Pooling (over sequence for the first sequence)\n        u_i = torch.tanh(self.word_weight(x))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        \n        if mask is not None:\n            att = att * (1 - mask.unsqueeze(-1))\n            \n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        x = x * att\n        return x.sum(dim = 1)","metadata":{"papermill":{"duration":0.051934,"end_time":"2021-07-27T17:23:07.264087","exception":false,"start_time":"2021-07-27T17:23:07.212153","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.623895Z","iopub.execute_input":"2021-07-29T10:10:41.624285Z","iopub.status.idle":"2021-07-29T10:10:41.635097Z","shell.execute_reply.started":"2021-07-29T10:10:41.624212Z","shell.execute_reply":"2021-07-29T10:10:41.63385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RoBERTa base","metadata":{"papermill":{"duration":0.03771,"end_time":"2021-07-27T17:23:07.339725","exception":false,"start_time":"2021-07-27T17:23:07.302015","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"* Version 11","metadata":{"papermill":{"duration":0.037043,"end_time":"2021-07-27T17:23:07.413696","exception":false,"start_time":"2021-07-27T17:23:07.376653","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Model_RoBERTa_base_v11(nn.Module):\n    def __init__(self, backbone, model_config, benchmark_token = None, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True):\n        super(Readability_Model_RoBERTa_base_v11, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size, num_cat)\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.output_cat)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        output = self.layer_norm(output_backbone.pooler_output)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts)\n        cats /= len(self.dropouts)\n\n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"papermill":{"duration":0.059055,"end_time":"2021-07-27T17:23:07.511607","exception":false,"start_time":"2021-07-27T17:23:07.452552","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.638269Z","iopub.execute_input":"2021-07-29T10:10:41.639059Z","iopub.status.idle":"2021-07-29T10:10:41.660966Z","shell.execute_reply.started":"2021-07-29T10:10:41.638826Z","shell.execute_reply":"2021-07-29T10:10:41.659942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RoBERTa large","metadata":{"papermill":{"duration":0.036307,"end_time":"2021-07-27T17:23:07.584079","exception":false,"start_time":"2021-07-27T17:23:07.547772","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"* Version 15","metadata":{"papermill":{"duration":0.034727,"end_time":"2021-07-27T17:23:07.655242","exception":false,"start_time":"2021-07-27T17:23:07.620515","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Model_RoBERTa_large_v15(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_RoBERTa_large_v15, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"papermill":{"duration":0.058977,"end_time":"2021-07-27T17:23:07.749253","exception":false,"start_time":"2021-07-27T17:23:07.690276","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.662595Z","iopub.execute_input":"2021-07-29T10:10:41.66303Z","iopub.status.idle":"2021-07-29T10:10:41.691495Z","shell.execute_reply.started":"2021-07-29T10:10:41.662988Z","shell.execute_reply":"2021-07-29T10:10:41.688447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Version 16","metadata":{"papermill":{"duration":0.034995,"end_time":"2021-07-27T17:23:07.818757","exception":false,"start_time":"2021-07-27T17:23:07.783762","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Model_RoBERTa_large_v16(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_RoBERTa_large_v16, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Weighted mean pooling (over hidden layers)\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        output_backbone = torch.sum(hidden_states * layer_weight, dim = 0)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"papermill":{"duration":0.057931,"end_time":"2021-07-27T17:23:07.911478","exception":false,"start_time":"2021-07-27T17:23:07.853547","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.693271Z","iopub.execute_input":"2021-07-29T10:10:41.693744Z","iopub.status.idle":"2021-07-29T10:10:41.719289Z","shell.execute_reply.started":"2021-07-29T10:10:41.693698Z","shell.execute_reply":"2021-07-29T10:10:41.718037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XLNet large cased","metadata":{"papermill":{"duration":0.034454,"end_time":"2021-07-27T17:23:07.98064","exception":false,"start_time":"2021-07-27T17:23:07.946186","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"* Version 2","metadata":{"papermill":{"duration":0.034702,"end_time":"2021-07-27T17:23:08.050074","exception":false,"start_time":"2021-07-27T17:23:08.015372","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Model_XLNet_large_cased_v2(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_XLNet_large_cased_v2, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        token_type_ids = token_type_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"papermill":{"duration":0.059289,"end_time":"2021-07-27T17:23:08.144519","exception":false,"start_time":"2021-07-27T17:23:08.08523","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.721269Z","iopub.execute_input":"2021-07-29T10:10:41.721905Z","iopub.status.idle":"2021-07-29T10:10:41.749594Z","shell.execute_reply.started":"2021-07-29T10:10:41.721858Z","shell.execute_reply":"2021-07-29T10:10:41.748462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Version 3","metadata":{"papermill":{"duration":0.034854,"end_time":"2021-07-27T17:23:08.21442","exception":false,"start_time":"2021-07-27T17:23:08.179566","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Model_XLNet_large_cased_v3(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_XLNet_large_cased_v3, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        token_type_ids = token_type_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"papermill":{"duration":0.097192,"end_time":"2021-07-27T17:23:08.346311","exception":false,"start_time":"2021-07-27T17:23:08.249119","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.75113Z","iopub.execute_input":"2021-07-29T10:10:41.751575Z","iopub.status.idle":"2021-07-29T10:10:41.77864Z","shell.execute_reply.started":"2021-07-29T10:10:41.751531Z","shell.execute_reply":"2021-07-29T10:10:41.777415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GPT2 medium","metadata":{"papermill":{"duration":0.059256,"end_time":"2021-07-27T17:23:08.471238","exception":false,"start_time":"2021-07-27T17:23:08.411982","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"* Version 1","metadata":{"papermill":{"duration":0.057702,"end_time":"2021-07-27T17:23:08.58622","exception":false,"start_time":"2021-07-27T17:23:08.528518","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Model_GPT2_medium_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_GPT2_medium_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"papermill":{"duration":0.095418,"end_time":"2021-07-27T17:23:08.739402","exception":false,"start_time":"2021-07-27T17:23:08.643984","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.780273Z","iopub.execute_input":"2021-07-29T10:10:41.780999Z","iopub.status.idle":"2021-07-29T10:10:41.808455Z","shell.execute_reply.started":"2021-07-29T10:10:41.780945Z","shell.execute_reply":"2021-07-29T10:10:41.807259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ALBERT xlarge v2","metadata":{"papermill":{"duration":0.043873,"end_time":"2021-07-27T17:23:08.840816","exception":false,"start_time":"2021-07-27T17:23:08.796943","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"* Version 1","metadata":{"papermill":{"duration":0.034481,"end_time":"2021-07-27T17:23:08.910219","exception":false,"start_time":"2021-07-27T17:23:08.875738","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Model_ALBERT_xlarge_v2_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_ALBERT_xlarge_v2_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"papermill":{"duration":0.059423,"end_time":"2021-07-27T17:23:09.004196","exception":false,"start_time":"2021-07-27T17:23:08.944773","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.810369Z","iopub.execute_input":"2021-07-29T10:10:41.810922Z","iopub.status.idle":"2021-07-29T10:10:41.837775Z","shell.execute_reply.started":"2021-07-29T10:10:41.810876Z","shell.execute_reply":"2021-07-29T10:10:41.836544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ELECTRA large discriminator","metadata":{"papermill":{"duration":0.035054,"end_time":"2021-07-27T17:23:09.073979","exception":false,"start_time":"2021-07-27T17:23:09.038925","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"* Version 1","metadata":{"papermill":{"duration":0.034517,"end_time":"2021-07-27T17:23:09.143737","exception":false,"start_time":"2021-07-27T17:23:09.10922","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Model_ELECTRA_large_discriminator_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_ELECTRA_large_discriminator_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    \n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"papermill":{"duration":0.059234,"end_time":"2021-07-27T17:23:09.238207","exception":false,"start_time":"2021-07-27T17:23:09.178973","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.839716Z","iopub.execute_input":"2021-07-29T10:10:41.840303Z","iopub.status.idle":"2021-07-29T10:10:41.868002Z","shell.execute_reply.started":"2021-07-29T10:10:41.840251Z","shell.execute_reply":"2021-07-29T10:10:41.866982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DeBERTa large","metadata":{}},{"cell_type":"markdown","source":"* Version 1","metadata":{}},{"cell_type":"code","source":"class Readability_Model_DeBERTa_large_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_DeBERTa_large_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T10:10:41.86964Z","iopub.execute_input":"2021-07-29T10:10:41.870121Z","iopub.status.idle":"2021-07-29T10:10:41.896973Z","shell.execute_reply.started":"2021-07-29T10:10:41.870078Z","shell.execute_reply":"2021-07-29T10:10:41.895943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Funnel-transformer/large","metadata":{"papermill":{"duration":0.034588,"end_time":"2021-07-27T17:23:09.307864","exception":false,"start_time":"2021-07-27T17:23:09.273276","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"* Version 1","metadata":{"papermill":{"duration":0.034796,"end_time":"2021-07-27T17:23:09.377531","exception":false,"start_time":"2021-07-27T17:23:09.342735","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Model_Funnel_large_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_Funnel_large_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        token_type_ids = token_type_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        # Extract output\n        output_backbone = output_backbone.last_hidden_state\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"papermill":{"duration":0.057291,"end_time":"2021-07-27T17:23:09.469533","exception":false,"start_time":"2021-07-27T17:23:09.412242","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.898628Z","iopub.execute_input":"2021-07-29T10:10:41.899287Z","iopub.status.idle":"2021-07-29T10:10:41.924409Z","shell.execute_reply.started":"2021-07-29T10:10:41.899244Z","shell.execute_reply":"2021-07-29T10:10:41.92336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BART large","metadata":{"papermill":{"duration":0.034645,"end_time":"2021-07-27T17:23:09.53888","exception":false,"start_time":"2021-07-27T17:23:09.504235","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"* Version 1","metadata":{"papermill":{"duration":0.034892,"end_time":"2021-07-27T17:23:09.608598","exception":false,"start_time":"2021-07-27T17:23:09.573706","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Readability_Model_BART_large_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_BART_large_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers * 2).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.init_std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.init_std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        decoder_hidden_states = output_backbone.decoder_hidden_states\n        encoder_hidden_states = output_backbone.encoder_hidden_states\n\n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        decoder_hidden_states = tuple(decoder_hidden_states[-i-1] for i in range(self.model_config.num_hidden_layers))\n        encoder_hidden_states = tuple(encoder_hidden_states[-i-1] for i in range(self.model_config.num_hidden_layers))\n        hidden_states = torch.stack(decoder_hidden_states + encoder_hidden_states, dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att / torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits /= len(self.dropouts_output)\n        cats /= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","metadata":{"papermill":{"duration":0.05957,"end_time":"2021-07-27T17:23:09.702941","exception":false,"start_time":"2021-07-27T17:23:09.643371","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.92613Z","iopub.execute_input":"2021-07-29T10:10:41.926704Z","iopub.status.idle":"2021-07-29T10:10:41.955523Z","shell.execute_reply.started":"2021-07-29T10:10:41.926642Z","shell.execute_reply":"2021-07-29T10:10:41.954439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference function","metadata":{"papermill":{"duration":0.03456,"end_time":"2021-07-27T17:23:10.003785","exception":false,"start_time":"2021-07-27T17:23:09.969225","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def infer(model, dataloader, device = 'cpu', use_tqdm = True, benchmark_token = None):\n    model.eval()\n    \n    if use_tqdm:\n        tbar = tqdm(dataloader)\n    else:\n        tbar = dataloader\n        \n    pred = []\n        \n    for item in tbar:\n        input_ids = item['input_ids'].to(device)\n        token_type_ids = item['token_type_ids'].to(device)\n        attention_mask = item['attention_mask'].to(device)\n        \n        if benchmark_token is not None:\n            benchmark_input_ids, benchmark_token_type_ids, benchmark_attention_mask = benchmark_token\n            input_ids = torch.cat((input_ids, benchmark_input_ids), dim = 0)\n            token_type_ids = torch.cat((token_type_ids, benchmark_token_type_ids), dim = 0)\n            attention_mask = torch.cat((attention_mask, benchmark_attention_mask), dim = 0)\n            \n        with torch.no_grad():\n            with autocast():\n                pred_mean, pred_std, pred_bins = model(input_ids = input_ids, \n                                                       attention_mask = attention_mask, \n                                                       token_type_ids = token_type_ids)\n        \n        pred.extend(pred_mean.cpu().detach().numpy())\n        \n    # Stack\n    pred = np.array(pred)\n    \n    return pred","metadata":{"papermill":{"duration":0.045093,"end_time":"2021-07-27T17:23:10.083602","exception":false,"start_time":"2021-07-27T17:23:10.038509","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.957223Z","iopub.execute_input":"2021-07-29T10:10:41.957737Z","iopub.status.idle":"2021-07-29T10:10:41.969406Z","shell.execute_reply.started":"2021-07-29T10:10:41.95766Z","shell.execute_reply":"2021-07-29T10:10:41.968486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{"papermill":{"duration":0.034626,"end_time":"2021-07-27T17:23:10.152746","exception":false,"start_time":"2021-07-27T17:23:10.11812","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class config():\n    # For inference\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    use_tqdm = True\n    model_names = ['roberta_large_v15_0', 'roberta_large_v15_3',\n                   'roberta_large_v16_1', \n                   'gpt2_medium_v1_0', \n                   'xlnet_large_cased_v2_0', 'xlnet_large_cased_v3_0', 'xlnet_large_cased_v3_1', \n                   'electra_large_discriminator_v1_0', 'electra_large_discriminator_v1_1',\n                   'deberta_large_v1_0', 'deberta_large_v1_1',\n                   'funnel_large_v1_0',\n                   'bart_large_v1_0']\n    # For dataloader\n    max_len = [250] * 15\n    batch_size = (8, 8, 8, \n                  8, 8,\n                  6, \n                  6, 6, 6,\n                  8, 8,\n                  4, 4,\n                  8,\n                  8)    # In the same order as the 'model_names' attribute\n    num_workers = 4\n    # For models\n    num_bins = (29, 1, 29, \n                29, 29, \n                29, \n                29, 1, 1, \n                1, 1,\n                29, 29,\n                1,\n                1)    # In the same order as the 'model_names' attribute\n    \ncfg = config()","metadata":{"papermill":{"duration":0.084096,"end_time":"2021-07-27T17:23:10.272074","exception":false,"start_time":"2021-07-27T17:23:10.187978","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:41.970715Z","iopub.execute_input":"2021-07-29T10:10:41.971094Z","iopub.status.idle":"2021-07-29T10:10:42.022144Z","shell.execute_reply.started":"2021-07-29T10:10:41.971054Z","shell.execute_reply":"2021-07-29T10:10:42.021102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{"papermill":{"duration":0.034398,"end_time":"2021-07-27T17:23:10.341556","exception":false,"start_time":"2021-07-27T17:23:10.307158","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Tokenizer and model configuration\ntokenizer_roberta_large = AutoTokenizer.from_pretrained('../input/robertalarge', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_roberta_large = AutoConfig.from_pretrained('../input/robertalarge', output_hidden_states = True)\n\ntokenizer_gpt2_medium = AutoTokenizer.from_pretrained('../input/gpt2-medium', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\ntokenizer_gpt2_medium.add_special_tokens({'pad_token': '[PAD]'})\n\ntokenizer_xlnet_large_cased = AutoTokenizer.from_pretrained('../input/xlnet-large-cased', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\ntokenizer_xlnet_large_cased.add_special_tokens({'pad_token': '[PAD]'})\n\ntokenizer_electra_large = AutoTokenizer.from_pretrained('../input/electra-large-discriminator', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_electra_large = AutoConfig.from_pretrained('../input/electra-large-discriminator', output_hidden_states = True)\n\ntokenizer_deberta_large = AutoTokenizer.from_pretrained('../input/deberta-large', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_debert_large = AutoConfig.from_pretrained('../input/deberta-large', output_hidden_states = True)\n\ntokenizer_funnel_large = AutoTokenizer.from_pretrained('../input/funnel-transformer-large', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_funnel_large = AutoConfig.from_pretrained('../input/funnel-transformer-large', output_hidden_states = True)\n\ntokenizer_bart_large = AutoTokenizer.from_pretrained('../input/bart-large', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_bart_large = AutoConfig.from_pretrained('../input/bart-large', output_hidden_states = True)\n\n# Dataloader\ninfer_dataset_roberta_large_v15_0 = Readability_Dataset(data, tokenizer_roberta_large, max_len = cfg.max_len[0], mode = 'infer')\ninfer_dataloader_roberta_large_v15_0 = DataLoader(infer_dataset_roberta_large_v15_0, batch_size = cfg.batch_size[0], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_roberta_large_v15_3 = Readability_Dataset(data, tokenizer_roberta_large, max_len = cfg.max_len[2], mode = 'infer')\ninfer_dataloader_roberta_large_v15_3 = DataLoader(infer_dataset_roberta_large_v15_3, batch_size = cfg.batch_size[2], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_roberta_large_v16_1 = Readability_Dataset(data, tokenizer_roberta_large, max_len = cfg.max_len[4], mode = 'infer')\ninfer_dataloader_roberta_large_v16_1 = DataLoader(infer_dataset_roberta_large_v16_1, batch_size = cfg.batch_size[4], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_gpt2_medium_v1_0 = Readability_Dataset(data, tokenizer_gpt2_medium, max_len = cfg.max_len[5], mode = 'infer')\ninfer_dataloader_gpt2_medium_v1_0 = DataLoader(infer_dataset_gpt2_medium_v1_0, batch_size = cfg.batch_size[5], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_xlnet_large_cased_v2_0 = Readability_Dataset(data, tokenizer_xlnet_large_cased, max_len = cfg.max_len[6], mode = 'infer')\ninfer_dataloader_xlnet_large_cased_v2_0 = DataLoader(infer_dataset_xlnet_large_cased_v2_0, batch_size = cfg.batch_size[6], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_xlnet_large_cased_v3_0 = Readability_Dataset(data, tokenizer_xlnet_large_cased, max_len = cfg.max_len[7], mode = 'infer')\ninfer_dataloader_xlnet_large_cased_v3_0 = DataLoader(infer_dataset_xlnet_large_cased_v3_0, batch_size = cfg.batch_size[7], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_xlnet_large_cased_v3_1 = Readability_Dataset(data, tokenizer_xlnet_large_cased, max_len = cfg.max_len[8], mode = 'infer')\ninfer_dataloader_xlnet_large_cased_v3_1 = DataLoader(infer_dataset_xlnet_large_cased_v3_1, batch_size = cfg.batch_size[8], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_electra_large_v1_0 = Readability_Dataset(data, tokenizer_electra_large, max_len = cfg.max_len[9], mode = 'infer')\ninfer_dataloader_electra_large_v1_0 = DataLoader(infer_dataset_electra_large_v1_0, batch_size = cfg.batch_size[9], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_electra_large_v1_1 = Readability_Dataset(data, tokenizer_electra_large, max_len = cfg.max_len[10], mode = 'infer')\ninfer_dataloader_electra_large_v1_1 = DataLoader(infer_dataset_electra_large_v1_1, batch_size = cfg.batch_size[10], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_deberta_large_v1_0 = Readability_Dataset(data, tokenizer_deberta_large, max_len = cfg.max_len[11], mode = 'infer')\ninfer_dataloader_deberta_large_v1_0 = DataLoader(infer_dataset_deberta_large_v1_0, batch_size = cfg.batch_size[11], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_deberta_large_v1_1 = Readability_Dataset(data, tokenizer_deberta_large, max_len = cfg.max_len[12], mode = 'infer')\ninfer_dataloader_deberta_large_v1_1 = DataLoader(infer_dataset_deberta_large_v1_1, batch_size = cfg.batch_size[12], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_funnel_large_v1_0 = Readability_Dataset(data, tokenizer_funnel_large, max_len = cfg.max_len[13], mode = 'infer')\ninfer_dataloader_funnel_large_v1_0 = DataLoader(infer_dataset_funnel_large_v1_0, batch_size = cfg.batch_size[13], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_bart_large_v1_0 = Readability_Dataset(data, tokenizer_bart_large, max_len = cfg.max_len[14], mode = 'infer')\ninfer_dataloader_bart_large_v1_0 = DataLoader(infer_dataset_bart_large_v1_0, batch_size = cfg.batch_size[14], num_workers = cfg.num_workers, shuffle = False)\n\n# Prediction storage\nprediction_roberta_large_v15_0 = np.zeros(data.shape[0])\nprediction_roberta_large_v15_2 = np.zeros(data.shape[0])\nprediction_roberta_large_v15_3 = np.zeros(data.shape[0])\nprediction_roberta_large_v16_0 = np.zeros(data.shape[0])\nprediction_roberta_large_v16_1 = np.zeros(data.shape[0])\nprediction_gpt2_medium_v1_0 = np.zeros(data.shape[0])\nprediction_xlnet_large_cased_v2_0 = np.zeros(data.shape[0])\nprediction_xlnet_large_cased_v3_0 = np.zeros(data.shape[0])\nprediction_xlnet_large_cased_v3_1 = np.zeros(data.shape[0])\nprediction_electra_large_v1_0 = np.zeros(data.shape[0])\nprediction_electra_large_v1_1 = np.zeros(data.shape[0])\nprediction_deberta_large_v1_0 = np.zeros(data.shape[0])\nprediction_deberta_large_v1_1 = np.zeros(data.shape[0])\nprediction_funnel_large_v1_0 = np.zeros(data.shape[0])\nprediction_bart_large_v1_0 = np.zeros(data.shape[0])\n\n# Tokenize the benchmark text\nbenchmark_token_roberta_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_roberta_large, cfg.max_len[0], return_tensor = True)\nbenchmark_token_roberta_large = (benchmark_token_roberta_large['input_ids'].to(cfg.device), \n                                 benchmark_token_roberta_large['token_type_ids'].to(cfg.device), \n                                 benchmark_token_roberta_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_gpt2_medium = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_gpt2_medium, cfg.max_len[5], return_tensor = True)\nbenchmark_token_gpt2_medium = (benchmark_token_gpt2_medium['input_ids'].to(cfg.device), \n                               benchmark_token_gpt2_medium['token_type_ids'].to(cfg.device), \n                               benchmark_token_gpt2_medium['attention_mask'].to(cfg.device))\n\nbenchmark_token_xlnet_large_cased = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_xlnet_large_cased, cfg.max_len[6], return_tensor = True)\nbenchmark_token_xlnet_large_cased = (benchmark_token_xlnet_large_cased['input_ids'].to(cfg.device), \n                                     benchmark_token_xlnet_large_cased['token_type_ids'].to(cfg.device), \n                                     benchmark_token_xlnet_large_cased['attention_mask'].to(cfg.device))\n\nbenchmark_token_electra_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_electra_large, cfg.max_len[9], return_tensor = True)\nbenchmark_token_electra_large = (benchmark_token_electra_large['input_ids'].to(cfg.device), \n                                 benchmark_token_electra_large['token_type_ids'].to(cfg.device), \n                                 benchmark_token_electra_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_deberta_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_deberta_large, cfg.max_len[11], return_tensor = True)\nbenchmark_token_deberta_large = (benchmark_token_deberta_large['input_ids'].to(cfg.device), \n                                 benchmark_token_deberta_large['token_type_ids'].to(cfg.device), \n                                 benchmark_token_deberta_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_funnel_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_funnel_large, cfg.max_len[13], return_tensor = True)\nbenchmark_token_funnel_large = (benchmark_token_funnel_large['input_ids'].to(cfg.device), \n                                benchmark_token_funnel_large['token_type_ids'].to(cfg.device), \n                                benchmark_token_funnel_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_bart_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_bart_large, cfg.max_len[14], return_tensor = True)\nbenchmark_token_bart_large = (benchmark_token_bart_large['input_ids'].to(cfg.device), \n                              benchmark_token_bart_large['token_type_ids'].to(cfg.device), \n                              benchmark_token_bart_large['attention_mask'].to(cfg.device))\n\nfor fold in range(5):\n    print('*' * 50)\n    print(f'Fold: {fold}')\n    \n    # Load pretrained models\n    model_name = 'roberta_large'\n    print(f'Inference model, {model_name} version 15-0...')\n    model_roberta_large_v15_0 = Readability_Model_RoBERTa_large_v15('../input/robertalarge', model_config_roberta_large, num_cat = cfg.num_bins[0], \n                                                                    benchmark_token = benchmark_token_roberta_large).to(cfg.device)\n    model_root_path = '../input/clrroberta-largepretrained-modelsv15/model_best_roberta_large_v15_0'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_roberta_large_v15_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_roberta_large_v15_0 += infer(model_roberta_large_v15_0, infer_dataloader_roberta_large_v15_0, device = cfg.device, \n                                            use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_roberta_large) / 5\n    del model_roberta_large_v15_0; gc.collect()\n    \n    model_name = 'roberta_large'\n    print(f'Inference model, {model_name} version 15-3...')\n    model_roberta_large_v15_3 = Readability_Model_RoBERTa_large_v15('../input/robertalarge', model_config_roberta_large, num_cat = cfg.num_bins[2], \n                                                                    benchmark_token = benchmark_token_roberta_large).to(cfg.device)\n    model_root_path = '../input/clrroberta-largev15/model_best_roberta_large_v15_3'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_roberta_large_v15_3.load_state_dict(ckp['model_state_dict'])    \n    prediction_roberta_large_v15_3 += infer(model_roberta_large_v15_3, infer_dataloader_roberta_large_v15_3, device = cfg.device, \n                                            use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_roberta_large) / 5\n    del model_roberta_large_v15_3; gc.collect()\n    \n    model_name = 'roberta_large'\n    print(f'Inference model, {model_name} version 16-1...')\n    model_roberta_large_v16_1 = Readability_Model_RoBERTa_large_v16('../input/robertalarge', model_config_roberta_large, num_cat = cfg.num_bins[4], \n                                                                    benchmark_token = benchmark_token_roberta_large).to(cfg.device)\n    model_root_path = '../input/clrroberta-largepretrained-modelsv16/model_best_roberta_large_v16_1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_roberta_large_v16_1.load_state_dict(ckp['model_state_dict'])\n    prediction_roberta_large_v16_1 += infer(model_roberta_large_v16_1, infer_dataloader_roberta_large_v16_1, device = cfg.device, \n                                            use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_roberta_large) / 5\n    del model_roberta_large_v16_1; gc.collect()\n    \n    model_name = 'gpt2_medium'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_config_gpt2_medium = AutoConfig.from_pretrained('../input/gpt2-medium', output_hidden_states = True)\n    model_gpt2_medium_v1_0 = Readability_Model_GPT2_medium_v1('../input/gpt2-medium', model_config_gpt2_medium, num_cat = cfg.num_bins[5], \n                                                              benchmark_token = benchmark_token_gpt2_medium).to(cfg.device)\n    model_gpt2_medium_v1_0.backbone.resize_token_embeddings(len(tokenizer_gpt2_medium))\n    model_root_path = '../input/clrgpt2-mediumpretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_gpt2_medium_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_gpt2_medium_v1_0 += infer(model_gpt2_medium_v1_0, infer_dataloader_gpt2_medium_v1_0, device = cfg.device, \n                                         use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_gpt2_medium) / 5\n    del model_gpt2_medium_v1_0; gc.collect()\n    \n    model_name = 'xlnet_large_cased'\n    print(f'Inference model, {model_name} version 2-0...')\n    model_config_xlnet_large_cased = AutoConfig.from_pretrained('../input/xlnet-large-cased', output_hidden_states = True)\n    model_xlnet_large_cased_v2_0 = Readability_Model_XLNet_large_cased_v2('../input/xlnet-large-cased', model_config_xlnet_large_cased, \n                                                                          num_cat = cfg.num_bins[6], benchmark_token = benchmark_token_xlnet_large_cased).to(cfg.device)\n    model_xlnet_large_cased_v2_0.backbone.resize_token_embeddings(len(tokenizer_xlnet_large_cased))\n    model_root_path = '../input/clrxlnet-largepretrained-models/v02'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_xlnet_large_cased_v2_0.load_state_dict(ckp['model_state_dict'])\n    prediction_xlnet_large_cased_v2_0 += infer(model_xlnet_large_cased_v2_0, infer_dataloader_xlnet_large_cased_v2_0, device = cfg.device, \n                                               use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_xlnet_large_cased) / 5\n    del model_xlnet_large_cased_v2_0; gc.collect()\n    \n    model_name = 'xlnet_large_cased'\n    print(f'Inference model, {model_name} version 3-0...')\n    model_config_xlnet_large_cased = AutoConfig.from_pretrained('../input/xlnet-large-cased', output_hidden_states = True)\n    model_xlnet_large_cased_v3_0 = Readability_Model_XLNet_large_cased_v3('../input/xlnet-large-cased', model_config_xlnet_large_cased, \n                                                                          num_cat = cfg.num_bins[7], benchmark_token = benchmark_token_xlnet_large_cased).to(cfg.device)\n    model_xlnet_large_cased_v3_0.backbone.resize_token_embeddings(len(tokenizer_xlnet_large_cased))\n    model_root_path = '../input/clrxlnet-largepretrained-models/v03'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_xlnet_large_cased_v3_0.load_state_dict(ckp['model_state_dict'])\n    prediction_xlnet_large_cased_v3_0 += infer(model_xlnet_large_cased_v3_0, infer_dataloader_xlnet_large_cased_v3_0, device = cfg.device, \n                                               use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_xlnet_large_cased) / 5\n    del model_xlnet_large_cased_v3_0; gc.collect()\n    \n    model_name = 'xlnet_large_cased'\n    print(f'Inference model, {model_name} version 3-1...')\n    model_config_xlnet_large_cased = AutoConfig.from_pretrained('../input/xlnet-large-cased', output_hidden_states = True)\n    model_xlnet_large_cased_v3_1 = Readability_Model_XLNet_large_cased_v3('../input/xlnet-large-cased', model_config_xlnet_large_cased, \n                                                                          num_cat = cfg.num_bins[8], benchmark_token = benchmark_token_xlnet_large_cased).to(cfg.device)\n    model_xlnet_large_cased_v3_1.backbone.resize_token_embeddings(len(tokenizer_xlnet_large_cased))\n    model_root_path = '../input/clrxlnet-large-casedv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_xlnet_large_cased_v3_1.load_state_dict(ckp['model_state_dict'])\n    prediction_xlnet_large_cased_v3_1 += infer(model_xlnet_large_cased_v3_1, infer_dataloader_xlnet_large_cased_v3_1, device = cfg.device, \n                                               use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_xlnet_large_cased) / 5\n    del model_xlnet_large_cased_v3_1; gc.collect()\n    \n    model_name = 'electra_large_discriminator'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_electra_large_v1_0 = Readability_Model_ELECTRA_large_discriminator_v1('../input/electra-large-discriminator', model_config_electra_large, \n                                                                                num_cat = cfg.num_bins[9], benchmark_token = benchmark_token_electra_large).to(cfg.device)\n    model_root_path = '../input/clrelectra-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_electra_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_electra_large_v1_0 += infer(model_electra_large_v1_0, infer_dataloader_electra_large_v1_0, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_electra_large) / 5\n    del model_electra_large_v1_0; gc.collect()\n    \n    model_name = 'electra_large_discriminator'\n    print(f'Inference model, {model_name} version 1-1...')\n    model_electra_large_v1_1 = Readability_Model_ELECTRA_large_discriminator_v1('../input/electra-large-discriminator', model_config_electra_large, \n                                                                                num_cat = cfg.num_bins[10], benchmark_token = benchmark_token_electra_large).to(cfg.device)\n    model_root_path = '../input/clrelectra-largev1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_electra_large_v1_1.load_state_dict(ckp['model_state_dict'])    \n    prediction_electra_large_v1_1 += infer(model_electra_large_v1_1, infer_dataloader_electra_large_v1_1, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_electra_large) / 5\n    del model_electra_large_v1_1; gc.collect()\n    \n    model_name = 'deberta_large'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_deberta_large_v1_0 = Readability_Model_DeBERTa_large_v1('../input/deberta-large', model_config_debert_large, \n                                                                  num_cat = cfg.num_bins[11], benchmark_token = benchmark_token_deberta_large).to(cfg.device)\n    model_root_path = '../input/clrdeberta-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_deberta_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_deberta_large_v1_0 += infer(model_deberta_large_v1_0, infer_dataloader_deberta_large_v1_0, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_deberta_large) / 5\n    del model_deberta_large_v1_0; gc.collect()\n    \n    model_name = 'deberta_large'\n    print(f'Inference model, {model_name} version 1-1...')\n    model_deberta_large_v1_1 = Readability_Model_DeBERTa_large_v1('../input/deberta-large', model_config_debert_large, \n                                                                  num_cat = cfg.num_bins[12], benchmark_token = benchmark_token_deberta_large).to(cfg.device)\n    if fold == 3:\n        model_deberta_large_v1_1 = AveragedModel(model_deberta_large_v1_1)\n        \n    model_root_path = '../input/clrdeberta-largev1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_deberta_large_v1_1.load_state_dict(ckp['model_state_dict'])    \n    prediction_deberta_large_v1_1 += infer(model_deberta_large_v1_1, infer_dataloader_deberta_large_v1_1, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_deberta_large) / 5\n    del model_deberta_large_v1_1; gc.collect()\n    \n    model_name = 'funnel_large'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_funnel_large_v1_0 = Readability_Model_Funnel_large_v1('../input/funnel-transformer-large', model_config_funnel_large, num_cat = cfg.num_bins[7], \n                                                                benchmark_token = benchmark_token_funnel_large).to(cfg.device)\n    model_root_path = '../input/clrfunnel-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_funnel_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_funnel_large_v1_0 += infer(model_funnel_large_v1_0, infer_dataloader_funnel_large_v1_0, device = cfg.device, \n                                          use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_funnel_large) / 5\n    del model_funnel_large_v1_0; gc.collect()\n    \n    model_name = 'bart_large'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_bart_large_v1_0 = Readability_Model_BART_large_v1('../input/bart-large', model_config_bart_large, num_cat = cfg.num_bins[8], \n                                                            benchmark_token = benchmark_token_bart_large).to(cfg.device)\n    model_root_path = '../input/clrbart-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_bart_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_bart_large_v1_0 += infer(model_bart_large_v1_0, infer_dataloader_bart_large_v1_0, device = cfg.device, \n                                        use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_bart_large) / 5\n    del model_bart_large_v1_0; gc.collect()","metadata":{"papermill":{"duration":1787.606285,"end_time":"2021-07-27T17:52:57.982288","exception":false,"start_time":"2021-07-27T17:23:10.376003","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:10:42.023975Z","iopub.execute_input":"2021-07-29T10:10:42.024565Z","iopub.status.idle":"2021-07-29T10:22:50.331779Z","shell.execute_reply.started":"2021-07-29T10:10:42.02452Z","shell.execute_reply":"2021-07-29T10:22:50.32563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Averaging","metadata":{"papermill":{"duration":0.07569,"end_time":"2021-07-27T17:52:58.611862","exception":false,"start_time":"2021-07-27T17:52:58.536172","status":"completed"},"tags":[]}},{"cell_type":"code","source":"pred_zoo = np.vstack([prediction_roberta_large_v15_0, prediction_roberta_large_v15_3, \n                      prediction_roberta_large_v16_1,\n                      prediction_gpt2_medium_v1_0,\n                      prediction_xlnet_large_cased_v2_0, prediction_xlnet_large_cased_v3_0, prediction_xlnet_large_cased_v3_1, \n                      prediction_electra_large_v1_0, prediction_electra_large_v1_1, \n                      prediction_deberta_large_v1_0, prediction_deberta_large_v1_1, \n                      prediction_funnel_large_v1_0, \n                      prediction_bart_large_v1_0]).T\n\nss['target'] = np.mean(pred_zoo, axis = 1)","metadata":{"papermill":{"duration":0.114989,"end_time":"2021-07-27T17:52:58.952858","exception":false,"start_time":"2021-07-27T17:52:58.837869","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:22:52.465553Z","iopub.execute_input":"2021-07-29T10:22:52.465986Z","iopub.status.idle":"2021-07-29T10:22:52.49181Z","shell.execute_reply.started":"2021-07-29T10:22:52.465942Z","shell.execute_reply":"2021-07-29T10:22:52.49046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":0.104103,"end_time":"2021-07-27T17:53:02.009279","exception":false,"start_time":"2021-07-27T17:53:01.905176","status":"completed"},"tags":[]}},{"cell_type":"code","source":"ss.to_csv('submission.csv', index = None)\nss","metadata":{"papermill":{"duration":0.970773,"end_time":"2021-07-27T17:53:03.058078","exception":false,"start_time":"2021-07-27T17:53:02.087305","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-29T10:22:53.770576Z","iopub.execute_input":"2021-07-29T10:22:53.770942Z","iopub.status.idle":"2021-07-29T10:22:54.749076Z","shell.execute_reply.started":"2021-07-29T10:22:53.770912Z","shell.execute_reply":"2021-07-29T10:22:54.748116Z"},"trusted":true},"execution_count":null,"outputs":[]}]}