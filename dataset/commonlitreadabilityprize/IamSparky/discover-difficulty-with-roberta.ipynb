{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\n\nimport transformers\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\nfrom sklearn import model_selection\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nMAX_LEN = 192\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 8\nEPOCHS = 5\n\nTOKENIZER = transformers.RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-13T17:20:41.431323Z","iopub.execute_input":"2021-07-13T17:20:41.431729Z","iopub.status.idle":"2021-07-13T17:20:53.090532Z","shell.execute_reply.started":"2021-07-13T17:20:41.431644Z","shell.execute_reply":"2021-07-13T17:20:53.089655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd ../input/cpythonlibrary/cpython-master\nfrom Lib import copy\n%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:20:53.091898Z","iopub.execute_input":"2021-07-13T17:20:53.092349Z","iopub.status.idle":"2021-07-13T17:20:53.152437Z","shell.execute_reply.started":"2021-07-13T17:20:53.092314Z","shell.execute_reply":"2021-07-13T17:20:53.151628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport gc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:20:54.986006Z","iopub.execute_input":"2021-07-13T17:20:54.986336Z","iopub.status.idle":"2021-07-13T17:20:54.992824Z","shell.execute_reply.started":"2021-07-13T17:20:54.986304Z","shell.execute_reply":"2021-07-13T17:20:54.991856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\", usecols = ['id','excerpt', 'target'])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:20:55.404173Z","iopub.execute_input":"2021-07-13T17:20:55.404505Z","iopub.status.idle":"2021-07-13T17:20:55.489026Z","shell.execute_reply.started":"2021-07-13T17:20:55.404472Z","shell.execute_reply":"2021-07-13T17:20:55.48808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"kfold\"] = -1    \ndf = df.sample(frac=1).reset_index(drop=True)\ny = df.target.values\nkf = model_selection.KFold(n_splits=5) # KFold for regression problems\n\nfor f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n    df.loc[v_, 'kfold'] = f\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:20:57.300951Z","iopub.execute_input":"2021-07-13T17:20:57.30128Z","iopub.status.idle":"2021-07-13T17:20:57.329006Z","shell.execute_reply.started":"2021-07-13T17:20:57.301247Z","shell.execute_reply":"2021-07-13T17:20:57.3281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoBERTa_Dataset:\n    def __init__(self, excerpt, target):\n        self.excerpt = excerpt\n        self.target = target\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        excerpt = str(self.excerpt[item])\n        excerpt = \" \".join(excerpt.split())\n\n        inputs = self.tokenizer.encode_plus(\n            excerpt,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        padding_length  =  self.max_len - len(ids)\n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"targets\": torch.tensor(self.target[item], dtype=torch.float),\n        }","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:20:57.712081Z","iopub.execute_input":"2021-07-13T17:20:57.712351Z","iopub.status.idle":"2021-07-13T17:20:57.722678Z","shell.execute_reply.started":"2021-07-13T17:20:57.712325Z","shell.execute_reply":"2021-07-13T17:20:57.721822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold = 0\ndf_train = df[df.kfold != fold].reset_index(drop=True)\ndf_valid = df[df.kfold == fold].reset_index(drop=True)\n\ntrain_data = RoBERTa_Dataset(excerpt = df_train.excerpt.values, \n                         target = df_train.target.values)\n\nval_data = RoBERTa_Dataset(excerpt = df_valid.excerpt.values, \n                       target = df_valid.target.values)\n\nidx = 7\n\nprint(val_data[idx]['ids'])\nprint(val_data[idx]['mask'])\nprint(val_data[idx]['targets'])","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:21:01.770018Z","iopub.execute_input":"2021-07-13T17:21:01.770355Z","iopub.status.idle":"2021-07-13T17:21:01.871886Z","shell.execute_reply.started":"2021-07-13T17:21:01.770326Z","shell.execute_reply":"2021-07-13T17:21:01.870282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_dataloader = DataLoader(train_data,\n                        num_workers= 4,\n                        batch_size= TRAIN_BATCH_SIZE,\n                        shuffle=True,\n                        drop_last=True\n                       )\n\nval_dataloader = DataLoader(val_data,\n                        num_workers= 4,\n                        batch_size= VALID_BATCH_SIZE,\n                        shuffle=False,\n                        drop_last=False\n                       )","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:21:02.282404Z","iopub.execute_input":"2021-07-13T17:21:02.282779Z","iopub.status.idle":"2021-07-13T17:21:02.288833Z","shell.execute_reply.started":"2021-07-13T17:21:02.282748Z","shell.execute_reply":"2021-07-13T17:21:02.287728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking if cuda is available\nfrom torch import device as device_\n\ndevice = device_(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:21:06.24688Z","iopub.execute_input":"2021-07-13T17:21:06.247215Z","iopub.status.idle":"2021-07-13T17:21:06.293431Z","shell.execute_reply.started":"2021-07-13T17:21:06.247185Z","shell.execute_reply":"2021-07-13T17:21:06.292533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RoBERTaBase(nn.Module):\n    def __init__(self):\n        super(RoBERTaBase, self).__init__()\n        self.bert = transformers.RobertaModel.from_pretrained('roberta-base', return_dict=False)\n        self.bert_drop = nn.Dropout(0.5)\n        self.out = nn.Linear(768, 1)\n\n    def forward(self, ids, mask):\n        x = self.bert(ids, attention_mask=mask)\n        \n        o1 = x[0]\n        \n        mean_pooling = torch.mean(o1, 1)\n        max_pooling, _ = torch.max(o1, 1) \n        avg_sum = torch.add(mean_pooling, max_pooling)/2\n        \n        output = self.out(avg_sum)\n        return output\n    \nmodel = RoBERTaBase()\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:24:37.832169Z","iopub.execute_input":"2021-07-13T17:24:37.832535Z","iopub.status.idle":"2021-07-13T17:24:42.130581Z","shell.execute_reply.started":"2021-07-13T17:24:37.832491Z","shell.execute_reply":"2021-07-13T17:24:42.129695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\n\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n\noptimizer_parameters = [\n    {\n        \"params\": [\n            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n        ],\n        \"weight_decay\": 0.01,\n    },\n    {\n        \"params\": [\n            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n        ],\n        \"weight_decay\": 0.0,\n    },\n]\n\nnum_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n\noptimizer = AdamW(optimizer_parameters, lr=2e-5)\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:24:42.131972Z","iopub.execute_input":"2021-07-13T17:24:42.132293Z","iopub.status.idle":"2021-07-13T17:24:42.144703Z","shell.execute_reply.started":"2021-07-13T17:24:42.132256Z","shell.execute_reply":"2021-07-13T17:24:42.143623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self,yhat,y):\n        return torch.sqrt(self.mse(yhat,y))\n\nloss_fn = RMSELoss()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:24:42.147576Z","iopub.execute_input":"2021-07-13T17:24:42.148237Z","iopub.status.idle":"2021-07-13T17:24:42.154736Z","shell.execute_reply.started":"2021-07-13T17:24:42.148199Z","shell.execute_reply":"2021-07-13T17:24:42.153754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining the training loop\ndef train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n    running_loss = 0.0\n    all_targets = 0\n    all_predictions = 0\n    \n    model.train()\n    \n    for batch_index,dataset in enumerate(data_loader):\n        ids = dataset['ids']\n        mask = dataset['mask']\n        targets = dataset['targets']\n        \n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        \n        optimizer.zero_grad()\n\n        outputs = model(ids = ids,\n                        mask = mask)\n        \n        loss = loss_fn(outputs, targets)\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        running_loss += loss.item()\n        \n        del ids, mask, targets\n        gc.collect()\n        torch.cuda.empty_cache()\n            \n    train_loss = running_loss / float(len(train_data))\n    \n    return train_loss\n\n\n\ndef eval_loop_fn(data_loader, model, device):\n    running_loss = 0.0\n    all_targets = 0\n    all_predictions = 0\n    \n    model.eval()\n    \n    for batch_index,dataset in enumerate(data_loader):\n        ids = dataset['ids']\n        mask = dataset['mask']\n        targets = dataset['targets']\n        \n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        outputs = model(ids = ids,\n                        mask = mask)\n        \n        loss = loss_fn(outputs, targets)\n        \n        running_loss += loss.item()\n        \n        del ids, mask, targets\n        gc.collect()\n        torch.cuda.empty_cache()\n    \n    valid_loss = running_loss / float(len(val_data))\n    \n    return valid_loss","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:24:42.156277Z","iopub.execute_input":"2021-07-13T17:24:42.156982Z","iopub.status.idle":"2021-07-13T17:24:42.170579Z","shell.execute_reply.started":"2021-07-13T17:24:42.156945Z","shell.execute_reply":"2021-07-13T17:24:42.169766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _run():\n    no_of_folds = 5\n    for i in range(no_of_folds):\n        a_string = \"*\" * 20\n\n        print(a_string, \" FOLD NUMBER \", i, a_string)\n        \n        df_train = df[df.kfold != i].reset_index(drop=True)\n        df_valid = df[df.kfold == i].reset_index(drop=True)\n        \n        all_RMSE = []\n        \n        for epoch in range(EPOCHS):\n            print(f\"Epoch --> {epoch+1} / {EPOCHS}\")\n            print(f\"-------------------------------\")\n\n            train_loss = train_loop_fn(training_dataloader, model, optimizer, device, scheduler)\n            print('RMSE training Loss: {:.4f}'.format(train_loss))\n\n            valid_loss = eval_loop_fn(val_dataloader, model, device)\n            print('RMSE validation Loss: {:.4f}\\n'.format(valid_loss))\n            \n            all_RMSE.append(valid_loss)\n        print('\\n')\n        \n        if i < 1:\n            best_loss = min(all_RMSE)\n            best_model = copy.deepcopy(model)\n        else:\n            if best_loss < min(all_RMSE):\n                continue\n            else:\n                best_loss = min(all_RMSE)\n                best_model = copy.deepcopy(model)\n    \n    torch.save(best_model,'./roberta_model_first.bin')\n    print()\n    print(\"The least loss we got among all the folds is {:.4f}\".format(best_loss))\n        \nif __name__ == \"__main__\":\n    _run()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:24:42.171917Z","iopub.execute_input":"2021-07-13T17:24:42.172253Z","iopub.status.idle":"2021-07-13T17:34:02.633698Z","shell.execute_reply.started":"2021-07-13T17:24:42.17222Z","shell.execute_reply":"2021-07-13T17:34:02.631722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    ","metadata":{},"execution_count":null,"outputs":[]}]}