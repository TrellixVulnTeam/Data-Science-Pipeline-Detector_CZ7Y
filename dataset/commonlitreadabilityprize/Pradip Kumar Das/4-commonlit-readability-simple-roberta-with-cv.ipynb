{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **CommonLit Readability Assessment**\n## Determinining Performance with RoBERTa Base","metadata":{}},{"cell_type":"markdown","source":"**_Sections:_**\n- _Required Packages & Helpers_\n- _Configuration_\n- _Data Preparation_\n- _Modeling_\n- _Evaluation_\n- _Submission_\n\n**_References (My Earlier Related Work):_**\n1. [*Exploratory Data Analysis (EDA)*](https://www.kaggle.com/pradipkumardas/1-commonlit-readability-eda)\n2. [*Baselining Model Performance with 1D ConvNet*](https://www.kaggle.com/pradipkumardas/2-commonlit-readability-baseline-perf-1dconvnet)\n3. [*Simple Model with BERT*](https://www.kaggle.com/pradipkumardas/3-commonlit-readability-simple-model-with-bert)","metadata":{}},{"cell_type":"markdown","source":"_**Note:** This notebook just fine-tunes pretrained RoBERTa (base) model with cross validation, and tries to find if RoBERTa (base) performs better than BERT (base, uncased) where the latter one was experimented in previous notebook (without cross validation through). Other advanced options and techniques based on this findings will be explored and shared soon._","metadata":{}},{"cell_type":"markdown","source":"## Required Packages & Helpers","metadata":{"execution":{"iopub.status.busy":"2021-06-12T05:15:53.734247Z","iopub.execute_input":"2021-06-12T05:15:53.73481Z","iopub.status.idle":"2021-06-12T05:15:53.740068Z","shell.execute_reply.started":"2021-06-12T05:15:53.734744Z","shell.execute_reply":"2021-06-12T05:15:53.738786Z"}}},{"cell_type":"code","source":"# Imports required packages\n\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport gc","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:10:59.58198Z","iopub.execute_input":"2021-06-18T11:10:59.582317Z","iopub.status.idle":"2021-06-18T11:10:59.589084Z","shell.execute_reply.started":"2021-06-18T11:10:59.582285Z","shell.execute_reply":"2021-06-18T11:10:59.588264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"# Sets data configurations\n\ndata_config ={\n    \"n_bins\": 20,\n    \"n_splits\": 5    \n}","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:10:59.590867Z","iopub.execute_input":"2021-06-18T11:10:59.591553Z","iopub.status.idle":"2021-06-18T11:10:59.600121Z","shell.execute_reply.started":"2021-06-18T11:10:59.591513Z","shell.execute_reply":"2021-06-18T11:10:59.599223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sets model specific configurations\n\nmodel_config = {\n    \"model_name\": \"../input/huggingface-roberta-variants/roberta-base/roberta-base\",\n    \"model_path\": \"model.h5\",\n    \"num_labels\": 1,\n    \"learning_rate\": 5e-5,\n    \"batch_size\": 32,\n    \"max_length\": 256,\n    \"epochs\": 30,\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:10:59.602552Z","iopub.execute_input":"2021-06-18T11:10:59.603249Z","iopub.status.idle":"2021-06-18T11:10:59.609881Z","shell.execute_reply.started":"2021-06-18T11:10:59.603166Z","shell.execute_reply":"2021-06-18T11:10:59.608739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seeds to reproduce experiment results\nrandom.seed(42)\nnp.random.seed(42)\n\n# Setting initialization for the theme of the plots\nsns.set_theme(style=\"whitegrid\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:10:59.611766Z","iopub.execute_input":"2021-06-18T11:10:59.612173Z","iopub.status.idle":"2021-06-18T11:10:59.619956Z","shell.execute_reply.started":"2021-06-18T11:10:59.612135Z","shell.execute_reply":"2021-06-18T11:10:59.619005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"# Loads data\n\ntrain = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nsubmission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:10:59.622571Z","iopub.execute_input":"2021-06-18T11:10:59.623248Z","iopub.status.idle":"2021-06-18T11:10:59.666513Z","shell.execute_reply.started":"2021-06-18T11:10:59.623214Z","shell.execute_reply":"2021-06-18T11:10:59.665787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Segmenting Labels (Distributing Lables in Discrete Intervals)**: As target is a interval variable, these labels should be segmented so that nearly equal number of samples from each segment can be selected during training model.","metadata":{}},{"cell_type":"code","source":"# Segments discrete interval of label by marking each sample with a bin number\n\ntrain[\"bin\"] = pd.cut(\n    x=train.target, bins=data_config[\"n_bins\"], \n    labels=[i for i in range(data_config[\"n_bins\"])])","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:10:59.66925Z","iopub.execute_input":"2021-06-18T11:10:59.669488Z","iopub.status.idle":"2021-06-18T11:10:59.677431Z","shell.execute_reply.started":"2021-06-18T11:10:59.669464Z","shell.execute_reply":"2021-06-18T11:10:59.676503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"training_history = []    # Stores the training and validation performance during cross validation\nprediction_history = []  # Stores the prediction against test data to aggregate upon while submission","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:10:59.688426Z","iopub.execute_input":"2021-06-18T11:10:59.688978Z","iopub.status.idle":"2021-06-18T11:10:59.698905Z","shell.execute_reply.started":"2021-06-18T11:10:59.688941Z","shell.execute_reply":"2021-06-18T11:10:59.697825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creates tokenizer to prepare data for model training\n\ntokenizer = AutoTokenizer.from_pretrained(model_config[\"model_name\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:10:59.700345Z","iopub.execute_input":"2021-06-18T11:10:59.700883Z","iopub.status.idle":"2021-06-18T11:10:59.801051Z","shell.execute_reply.started":"2021-06-18T11:10:59.700653Z","shell.execute_reply":"2021-06-18T11:10:59.800138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepares test data in TensorFlow dataset format\n\ntest_encodings = tokenizer(\n    test.excerpt.tolist(), \n    max_length=model_config[\"max_length\"], \n    truncation=True, \n    padding=\"max_length\",\n    return_tensors=\"tf\")\n\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n    {\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]})\ntest_dataset = test_dataset.batch(model_config[\"batch_size\"])\ntest_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:10:59.802682Z","iopub.execute_input":"2021-06-18T11:10:59.803046Z","iopub.status.idle":"2021-06-18T11:10:59.819009Z","shell.execute_reply.started":"2021-06-18T11:10:59.803007Z","shell.execute_reply":"2021-06-18T11:10:59.818056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nPerforms cross validation\n\"\"\"\n\n# Applies stratified cross validation to have equal distribution for different target bins \n\ncv = StratifiedKFold(data_config[\"n_splits\"], shuffle=True)\n\ncv_generator = cv.split(train, y=train.bin)\n\nfor fold, (idx_train, idx_val) in enumerate(cv_generator):\n    \n    print(f\"FOLD {fold+1}...\")\n    \n    # Encodes training data\n    train_encodings = tokenizer(\n        train.excerpt.iloc[idx_train].tolist(), \n        max_length=model_config[\"max_length\"], \n        truncation=True, \n        padding=\"max_length\",\n        return_tensors=\"tf\")\n\n    # Encodes validation data\n    val_encodings = tokenizer(\n        train.excerpt.iloc[idx_val].tolist(), \n        max_length=model_config[\"max_length\"], \n        truncation=True, \n        padding=\"max_length\",\n        return_tensors=\"tf\")\n    \n    # Creates TensorFlow dataset out of training data encodings\n    train_dataset = tf.data.Dataset.from_tensor_slices((\n        {\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"]},\n        train.target.iloc[idx_train]))\n    train_dataset = train_dataset.shuffle(1024)\n    train_dataset = train_dataset.batch(model_config[\"batch_size\"])\n    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n\n    # Creates TensorFlow dataset out of validation data encodings\n    val_dataset = tf.data.Dataset.from_tensor_slices((\n        {\"input_ids\": val_encodings[\"input_ids\"], \"attention_mask\": val_encodings[\"attention_mask\"]},\n        train.target.iloc[idx_val]))\n    val_dataset = val_dataset.batch(model_config[\"batch_size\"])\n    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n    \n    # Creates encoder from Transformer\n    encoder = TFAutoModelForSequenceClassification.from_pretrained(\n    model_config[\"model_name\"], num_labels = model_config[\"num_labels\"])\n\n    # Creates multi inputs for model\n    input_ids = layers.Input(shape=(model_config[\"max_length\"], ), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = layers.Input(shape=(model_config[\"max_length\"]), dtype=tf.int32, name=\"attention_mask\")\n\n    # Sets model output\n    outputs = encoder({\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n\n    # Wraps all layers within a model object\n    model = Model(inputs=[input_ids, attention_mask], outputs=outputs)\n\n    # Compiles and shows the summary to check\n    model.compile(\n        optimizer=keras.optimizers.Adam(model_config[\"learning_rate\"]),\n        loss=keras.losses.MeanSquaredError(),\n        metrics=keras.metrics.RootMeanSquaredError())\n\n    # Configures monitor with rules for model training to stop if criterion match\n    early_stopping_monitor = EarlyStopping(\n        monitor=\"val_root_mean_squared_error\", mode=\"min\", patience=5, restore_best_weights=True, verbose=1)\n\n    # Configures rules to store model parameters (only weights) at its best during training\n    checkpoint = ModelCheckpoint(\n        model_config[\"model_path\"], monitor=\"val_root_mean_squared_error\", mode=\"min\", save_best_only=True, save_weights_only=True)\n\n    # Fits the model\n    history = model.fit(\n        x=train_dataset,\n        validation_data=val_dataset,\n        callbacks=[early_stopping_monitor, checkpoint],\n        epochs=model_config[\"epochs\"],\n        verbose=2).history\n    \n    # Adds the model training history into list for later analysis\n    training_history.append(history)\n    best_epoch = np.argmin(history[\"val_root_mean_squared_error\"])\n    print(f\"\\nBest Validation Performance: {history['val_root_mean_squared_error'][best_epoch]} (RMSE) at epoch {best_epoch + 1}\")\n    \n    # Predicts on test data and appends the prediction into list to average later\n    predictions = model.predict(test_dataset)[\"logits\"]\n    prediction_history.append(predictions)\n    print(\"\\nPerformed predictions using current model on test dataset and values were recorded.\\n\")\n    \n    # Frees resources\n    del checkpoint, early_stopping_monitor, model, outputs, encoder\n    del val_dataset, train_dataset, train_encodings, val_encodings\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-18T11:10:59.821682Z","iopub.execute_input":"2021-06-18T11:10:59.822136Z","iopub.status.idle":"2021-06-18T12:14:47.785409Z","shell.execute_reply.started":"2021-06-18T11:10:59.822099Z","shell.execute_reply":"2021-06-18T12:14:47.784477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"# Plots Model's Cross Validation Performance \n\nfig, axes = plt.subplots(1, 5, sharey=True, figsize=(20,5))\nfig.suptitle(\"Cross Validation Performance\")\nfor ax, history in enumerate(training_history):\n    axes[ax].plot(range(1, len(history[\"root_mean_squared_error\"]) + 1), history[\"root_mean_squared_error\"], \"bo\", label=\"Training Loss\")\n    axes[ax].plot(range(1, len(history[\"val_root_mean_squared_error\"]) + 1), history[\"val_root_mean_squared_error\"], \"b\", label=\"Validation Loss\")\n    axes[ax].set_title(f\"FOLD {ax+1}\")\n    axes[ax].set_xlabel(\"Epoch\")\n    axes[ax].legend()\n    if ax == 0:\n        axes[ax].set_ylabel(\"Loss (RMS)\")","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:15:42.253225Z","iopub.execute_input":"2021-06-18T12:15:42.253538Z","iopub.status.idle":"2021-06-18T12:15:42.954797Z","shell.execute_reply.started":"2021-06-18T12:15:42.253507Z","shell.execute_reply":"2021-06-18T12:15:42.953805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"# Predicts on test data\n\nmean_predictions = np.mean(prediction_history, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:18:04.086786Z","iopub.execute_input":"2021-06-18T12:18:04.087116Z","iopub.status.idle":"2021-06-18T12:18:04.09139Z","shell.execute_reply.started":"2021-06-18T12:18:04.087085Z","shell.execute_reply":"2021-06-18T12:18:04.090299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Averaging predictions across folds\n\nsubmission.target = mean_predictions","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:18:07.881504Z","iopub.execute_input":"2021-06-18T12:18:07.881825Z","iopub.status.idle":"2021-06-18T12:18:07.888477Z","shell.execute_reply.started":"2021-06-18T12:18:07.881793Z","shell.execute_reply":"2021-06-18T12:18:07.88764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submitting by saving predictions into submission file\n\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:18:08.40139Z","iopub.execute_input":"2021-06-18T12:18:08.401666Z","iopub.status.idle":"2021-06-18T12:18:08.411072Z","shell.execute_reply.started":"2021-06-18T12:18:08.401638Z","shell.execute_reply":"2021-06-18T12:18:08.410215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2021-06-18T12:18:09.661277Z","iopub.execute_input":"2021-06-18T12:18:09.661584Z","iopub.status.idle":"2021-06-18T12:18:09.673251Z","shell.execute_reply.started":"2021-06-18T12:18:09.661557Z","shell.execute_reply":"2021-06-18T12:18:09.672336Z"},"trusted":true},"execution_count":null,"outputs":[]}]}