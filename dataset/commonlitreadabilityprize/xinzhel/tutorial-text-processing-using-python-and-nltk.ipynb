{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport string \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-19T04:19:40.637599Z","iopub.execute_input":"2021-06-19T04:19:40.637967Z","iopub.status.idle":"2021-06-19T04:19:40.642885Z","shell.execute_reply.started":"2021-06-19T04:19:40.637934Z","shell.execute_reply":"2021-06-19T04:19:40.641836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read traning data\ndata_dir = '../input/commonlitreadabilityprize/'\ntrain = pd.read_csv(data_dir + 'train.csv')\n\n# 'exerpt' field of each example is textual data which is represented as python `str` class\ntext_example = train.excerpt[0]\nprint(text_example, '\\n\\n type:', type(text_example))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:20:42.486471Z","iopub.execute_input":"2021-06-19T04:20:42.48687Z","iopub.status.idle":"2021-06-19T04:20:42.527601Z","shell.execute_reply.started":"2021-06-19T04:20:42.486838Z","shell.execute_reply":"2021-06-19T04:20:42.526636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize\n# text_example.split() # it only split by space and would not consider punctuations \ntokens = word_tokenize(text_example) \nprint(' '.join(tokens))\n\n# remove punctuation & non alphabetic characters\ntable = str.maketrans('', '', string.punctuation) # define punctuation to remove\ntokens_1 = [t.translate(table) for t in tokens] # punctuation would be \"replaced\" by empty string ''\nprint('\\n', ' '.join(tokens_1))\n\n# filter out stopwords\nstop_words = stopwords.words('english')\ntokens_2 = [token for token in tokens_1 if not token in stop_words + ['']]\nprint('\\n', ' '.join(tokens_2))\n\n# lemmatize\nlemmatizer = WordNetLemmatizer()\ntokens_3 = [lemmatizer.lemmatize(token) for token in tokens_2]\nprint('\\n', ' '.join(tokens_3))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T05:08:33.570735Z","iopub.execute_input":"2021-06-19T05:08:33.571087Z","iopub.status.idle":"2021-06-19T05:08:33.583625Z","shell.execute_reply.started":"2021-06-19T05:08:33.571058Z","shell.execute_reply":"2021-06-19T05:08:33.582614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use `pd.Series().apply()` for predefined text processing pipeline\ntrain.excerpt.apply(lambda x: word_tokenize(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-19T04:15:41.789162Z","iopub.execute_input":"2021-06-19T04:15:41.789513Z","iopub.status.idle":"2021-06-19T04:15:41.794415Z","shell.execute_reply.started":"2021-06-19T04:15:41.789484Z","shell.execute_reply":"2021-06-19T04:15:41.793702Z"},"trusted":true},"execution_count":null,"outputs":[]}]}