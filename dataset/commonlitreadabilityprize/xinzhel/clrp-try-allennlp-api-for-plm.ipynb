{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport gc\nimport sys\nimport time\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom typing import Optional, Dict\nfrom overrides import overrides\n\n\n! pip install --upgrade pip\n! pip install --upgrade allennlp\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport torch.nn.functional as F\nimport allennlp\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data import Vocabulary, Instance, Batch\nfrom allennlp.data.tokenizers import PretrainedTransformerTokenizer\nfrom allennlp.data.token_indexers import PretrainedTransformerIndexer\nfrom allennlp.data.fields import TextField, LabelField, TensorField\nfrom allennlp.modules.token_embedders.pretrained_transformer_embedder import PretrainedTransformerEmbedder\nfrom allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\nfrom allennlp.modules import FeedForward, Seq2VecEncoder, TextFieldEmbedder\nfrom allennlp.models.model import Model\nfrom allennlp.nn import InitializerApplicator, RegularizerApplicator\nfrom allennlp.nn import util\nfrom allennlp.training.metrics import MeanAbsoluteError\n\n\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom sklearn.model_selection import KFold","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-10T02:27:36.928786Z","iopub.execute_input":"2021-07-10T02:27:36.929174Z","iopub.status.idle":"2021-07-10T02:29:01.559918Z","shell.execute_reply.started":"2021-07-10T02:27:36.929143Z","shell.execute_reply":"2021-07-10T02:29:01.558908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from allennlp.modules.token_embedders.pretrained_transformer_embedder import PretrainedTransformerEmbedder\n# import inspect\n# inspect.signature(PretrainedTransformerEmbedder.__init__)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:29:01.561751Z","iopub.execute_input":"2021-07-10T02:29:01.562108Z","iopub.status.idle":"2021-07-10T02:29:20.360767Z","shell.execute_reply.started":"2021-07-10T02:29:01.562069Z","shell.execute_reply":"2021-07-10T02:29:20.359642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/commonlitreadabilityprize/'\ntrain = pd.read_csv(data_dir + 'train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:29:20.363007Z","iopub.execute_input":"2021-07-10T02:29:20.363607Z","iopub.status.idle":"2021-07-10T02:29:20.48346Z","shell.execute_reply.started":"2021-07-10T02:29:20.363556Z","shell.execute_reply":"2021-07-10T02:29:20.479273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate a sequence of word representations (vetors) for model input\n* Huggingface : `Tokenizer` is responsible for all the stuff with some useful methods to probe `covert_tokens_to_ids(self.tokenize())` , `encode`, `encode_plus`\n* AllenNLP has more control for varying Tokenization and Indexing with `Vocabulary`, `Indexer` and `Tokenizer`.\n\n# AllenNLP: Text -> Vectors\n1.`Tokenizers` (Text â†’ Tokens) and `TokenIndexers` (Tokens â†’ Ids):  \n   * Tokenizers: AllenNLP has its own tokenizers and tokenizers built on top of `Spacy` tokenizers. In any way, it outputs a list of `Token` object.\n   * Indexers: This is super convenient when we experiment with multiple indexers. Combined to the above step, the easy coding example (Text -> Ids) could be as this. \n   * work with ðŸ¤—transformers: Since transformers models have a fixed scheme for Tokens â†’ Ids,`Tokenizers` and `TokenIndexers` have to be matched using `PretrainedTransformerTokenizer` and `PretrainedTransformerIndexer`. Underlying them,  ðŸ¤—transformers Auto Classes will be called which\n        > automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary\n        + `AutoModel.from_pretrained('bert-base-cased')`\n        + `AutoTokenizer.from_pretrained('bert-base-cased')`: this will access the vocabulary and output [`PreTrainedTokenizerBase`](https://huggingface.co/transformers/internal/tokenization_utils.html#pretrainedtokenizerbase) object for both tokenization and indexing.\n        + `AutoConfig.from_pretrained('bert-base-cased')`: this is inferred automatically by the above two methods which output Configuration objects like [`DistilBertConfig`](https://huggingface.co/transformers/_modules/transformers/models/distilbert/configuration_distilbert.html#DistilBertConfig)\n        + **Since we need to submit the code without access network, we have to use the directory with `config.json`, e.g., `config_dir = \"../input/roberta-base\"`**\n    \n\n2. `TextField`\n```\n    tokenizer = ...  # Whatever tokenizer you want\n    sentence = \"We are learning about TextFields\"\n    tokens = tokenizer.tokenize(sentence)\n    token_indexers = {\"indexer1\": SingleIdTokenIndexer()} # we'll talk about this in the next section\n    text_field = TextField(tokens, token_indexers)\n    token_tensor = text_field.as_tensor() # The output would be: {\"indexer1\": {\"tokens\": torch.LongTensor([[1, 3, 2, 9, 4, 3]])}}\n    ```\n```\n\n3.`TextFieldEmbedders` (Ids â†’ Vectors):  Embedding those IDs into a vector space which happens to the model side. The names has to align with ones used in TokenIndexers\n```\nembedder = BasicTextFieldEmbedder(token_embedders={\"indexer1\": Embedding(num_embeddings=10, embedding_dim=3)}) # This 'indexer1' key must match the 'indexer1' key in the `token_tensor` above. \n```\n        \n        ","metadata":{}},{"cell_type":"code","source":"backbone = '../input/d/maunish/clr-roberta/model3/model3.bin'\nconfig_dir = \"../input/roberta-base\"","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:24:37.70773Z","iopub.execute_input":"2021-07-10T02:24:37.708206Z","iopub.status.idle":"2021-07-10T02:24:37.718238Z","shell.execute_reply.started":"2021-07-10T02:24:37.708103Z","shell.execute_reply":"2021-07-10T02:24:37.717205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# construct instances\ntrain_instances, test_instances = [], []\ntokenizer = PretrainedTransformerTokenizer(model_name=config_dir, add_special_tokens=True)\nindexer = PretrainedTransformerIndexer(model_name=config_dir) \nvocab = Vocabulary.from_pretrained_transformer(config_dir)\nfor text, y in zip(list(train['excerpt']),list(train['target'])):\n    tokens = tokenizer.tokenize(text) # text -> tokens\n    text_field = TextField(tokens, token_indexers={'tokens': indexer}) # tokens -> ids\n    text_field.index(vocab)\n    # if I want to use Huggingface pipeline, we can use the following code to extract the tokens information as input to the transformers models\n    # However, I still wanna use AllenNLP PretrainedTransformerEmbedder as wrapper for transformers models for brevity\n    # token_tensor = text_field.as_tensor(text_field.get_padding_lengths())\n    # add batch dimension\n    #token_tensor['tokens']['token_ids'] = token_tensor['tokens']['token_ids'].unsqueeze(0)\n    #token_tensor['tokens']['mask'] = token_tensor['tokens']['mask'].unsqueeze(0)\n    #token_tensor['tokens']['type_ids'] = token_tensor['tokens']['type_ids'].unsqueeze(0)\n    fields = {\"tokens\": text_field}\n    fields[\"label\"] = TensorField(torch.FloatTensor([y]))\n    train_instances.append(Instance(fields))\n\n# Unsolved Bug: UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte\ntext_field_embedder = PretrainedTransformerEmbedder(model_name=backbone, train_parameters=False, load_weights=False)\ntext_field_embedder = text_field_embedder.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:41:09.738304Z","iopub.execute_input":"2021-07-10T02:41:09.738678Z","iopub.status.idle":"2021-07-10T02:41:16.166326Z","shell.execute_reply.started":"2021-07-10T02:41:09.738646Z","shell.execute_reply":"2021-07-10T02:41:16.165485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Due to the unsolved bug above, I choose to directly use transformers model\n# text_field_embedder = BasicTextFieldEmbedder(token_embedders={\"tokens\": token_embedder})\n# print(\"Using the TextFieldEmbedder:\", text_field_embedder(token_tensor))\n# print(\"Using the TokenEmbedder:\", token_embedder(token_tensor['tokens']['token_ids'], token_tensor['tokens']['mask']))\n\n\n# # `mask` is needed for `encoder`\n# tokens_mask = token_tensor['tokens']['mask'] #get_text_field_mask(tokens) # shape: (B, T)\n# print(tokens_mask)\n# from allennlp.nn import util\n# tokens_mask = util.get_text_field_mask(token_tensor) # shape: (B, T)\n# print(tokens_mask)\n\n\n# # Even though `TextFieldEmbedders` is the standard way to do embeddings, \n# # we have another option by just using `TokenEmbedder` (since normally we donot have > 1 (indexer, embedder) pair to concatenate).\n# token_ids = tokens['tokens']['token_ids'] # shape: (B, T)\n# embeddings = self.pretrained_embeddings.forward\n\n\n# indexer logic: e.g. the length of: \"[CLS] A B C [SEP] [CLS] D E F [SEP]\" .\n#         (token_ids=token_ids, mask=mask) # shape: (B, T, C)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:46:01.948089Z","iopub.execute_input":"2021-07-10T02:46:01.94846Z","iopub.status.idle":"2021-07-10T02:46:01.953902Z","shell.execute_reply.started":"2021-07-10T02:46:01.948431Z","shell.execute_reply":"2021-07-10T02:46:01.951338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# construct AllenNLP model for this regression task\nclass ClrpModel(Model):\n\n    def __init__(self, vocab: Vocabulary,\n                 text_field_embedder: TextFieldEmbedder,\n                 regressor: nn.Module,\n                 seq2vec_encoder: Seq2VecEncoder = None,\n                 initializer: InitializerApplicator = InitializerApplicator(),\n                 regularizer: Optional[RegularizerApplicator] = None,\n                 remove_cls: bool = False) -> None:\n        super(ClrpModel, self).__init__(vocab, regularizer)\n\n        self.text_field_embedder = text_field_embedder\n        self.seq2vec_encoder = seq2vec_encoder\n        self.regressor = regressor\n        self.metrics = {\n                \"mae\": MeanAbsoluteError()\n        }\n        self.loss = nn.MSELoss()\n        self.remove_cls = remove_cls\n        \n        initializer(self)\n\n    @overrides\n    def forward(self,  \n                tokens: Dict[str, Dict[str, torch.LongTensor]],\n                label: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n\n        \"\"\"\n        Parameters\n        ----------\n        tokens : Dict[str, Dict[str, Variable]], required\n            The output of ``TextField.as_array()``.\n        label : Variable, optional (default = None)\n            A variable representing the label for each instance in the batch.\n        Returns\n        -------\n        An output dictionary consisting of:\n        class_probabilities : torch.FloatTensor\n            A tensor of shape ``(batch_size, num_classes)`` representing a distribution over the\n            label classes for each instance.\n        loss : torch.FloatTensor, optional\n            A scalar loss to be optimised.\n        \"\"\"\n        \n        tokens_mask = util.get_text_field_mask(tokens) # shape: (B, T)\n        torch.cuda.empty_cache() # in case of OOM error\n        embedded_tokens = self.text_field_embedder(tokens) # shape: (B, T, C)\n        if self.seq2vec_encoder is None:\n            encoded_tokens = embedded_tokens[:,0,:]\n        else:\n            if self.remove_cls:\n                embedded_tokens = embedded_tokens[:,1:,:]\n                tokens_mask = tokens_mask[:, 1:]\n            encoded_tokens = self.seq2vec_encoder(embedded_tokens, mask=tokens_mask) # shape: (B, hidden)\n\n        logits = self.regressor(encoded_tokens) # shape: (B, hidden) -> (B, 1)\n        \n        output_dict = {'logits': logits}\n        if label is not None:\n            for metric in self.metrics.values():\n                metric(logits, label)\n            logits = logits.squeeze(-1)\n            label = label.squeeze(-1)\n            output_dict[\"loss\"] = self.loss(logits, label)\n            \n        return output_dict\n\n    @overrides\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {metric_name: metric.get_metric(reset) for metric_name, metric in self.metrics.items()}\n\nclass View(nn.Module):\n    def __init__(self):\n        super(View, self).__init__()\n\n    def forward(self, x):\n        return torch.unsqueeze(x, 1)\n    \n# instantiate the regressor\ndef crt_model(hidden_size=1024):\n#     model = nn.Sequential(\n#             View(), # input: (bsz, input channel = 1, signal length=lm_hidden_units=768)\n#             nn.Conv1d(1, 64, 5,padding=0), # signal legth= emb_size = 768-4\n#             nn.ReLU(), \n#             nn.MaxPool1d(2), # signal legth= 764/2 = 382\n#             nn.Conv1d(64, 128, 5,padding=0), # 378\n#             nn.ReLU(),\n#             nn.MaxPool1d(2), # 189\n#             nn.Conv1d(128, 256, 5,padding=0), # 185\n#             nn.ReLU(),\n#             nn.MaxPool1d(185),  # 1\n#             nn.Flatten(1,-1), # no dim for signal length\n#             nn.Linear(256, 120), nn.ReLU(),\n#             nn.Linear(120, 1),\n#     ) # Training loss:1429762551.7295775\n#     model = nn.Sequential(\n#         View(), # input: (bsz, input channel = 1, signal length=lm_hidden_units=768) | roberta-large: 1024\n#         nn.Conv1d(1, 64, 5,padding=0), # signal legth= emb_size = 768-4  | roberta-large: 1024 - 4\n#         nn.ReLU(), \n#         nn.MaxPool1d(2), # signal legth= 764/2 = 382 | roberta-large: 510\n#         nn.Conv1d(64, 128, 5,padding=0), # 378 | roberta-large: 506\n#         nn.ReLU(),\n#         nn.MaxPool1d(2), # 189  | roberta-large: 253\n#         nn.Conv1d(128, 256, 5,padding=0), # 185  | roberta-large: 249\n#         nn.ReLU(),\n#         nn.Conv1d(256, 512, 5,padding=0), # 181  | roberta-large: 245\n#         nn.ReLU(),\n#         nn.Conv1d(512, 1024, 5,padding=0), # 177  | roberta-large: 241\n#         nn.ReLU(),\n#         nn.Conv1d(1024, 2048, 5,padding=0), # 173  | roberta-large: 237\n#         nn.ReLU(),\n#         nn.Conv1d(2048, 4098, 5,padding=0), # 169  | roberta-large: 233\n#         nn.ReLU(),\n#         nn.Conv1d(4098, 8196, 5,padding=0), # 165 | roberta-large: 229\n#         nn.ReLU(),\n#         nn.MaxPool1d(229),  # 1\n#         nn.Flatten(1,-1), # no dim for signal length\n#         nn.Linear(8196, 120), nn.ReLU(),\n#         nn.Linear(120, 240), nn.ReLU(),\n#         nn.Linear(240, 480), nn.ReLU(),\n#         nn.Linear(480, 980), nn.ReLU(),\n#         nn.Linear(980, 1),\n#         ) # Training loss: 2.057927086896341e+36 (use weight_init); 1.1041184755698057\n\n    model = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.Dropout(0.1), nn.Linear(hidden_size, 1), ) # Training loss:59182.79889552157\n\n#     def weight_init(m):\n#         if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n#             nn.init.normal_(m.weight)\n#             nn.init.zeros_(m.bias) # keras bias in default is initialized into 0\n#     model.apply(weight_init)\n    return model\n\n\n    \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nconfig = {\n#     'lr': 1e-5,\n#     'wd':1e-1,\n#     'batch_size_pretrained_infer': 128,\n    'batch_size_reg_train':8,\n    'max_len':256,\n    'epochs':7,\n    'nfolds':3,\n    'seed':42,\n}\n\n\n\n# construct model\nregressor = crt_model()\nregressor.to(device)\nmodel = ClrpModel(vocab, BasicTextFieldEmbedder(token_embedders={'tokens': text_field_embedder}), regressor)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T13:33:35.298295Z","iopub.execute_input":"2021-06-06T13:33:35.298665Z","iopub.status.idle":"2021-06-06T13:33:35.321836Z","shell.execute_reply.started":"2021-06-06T13:33:35.298634Z","shell.execute_reply":"2021-06-06T13:33:35.320756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train model\nclass CLRPDataset(nn.Module):\n    def __init__(self, instances):\n        self.instances = instances\n    \n    def __getitem__(self,idx):\n        \n        return self.instances[idx]\n    \n    def __len__(self):\n        return len(self.instances)\n    \ndef collate_fn(instances):\n    # instances into a dictinoary of tensor\n    instances = Batch(instances)\n    instances.index_instances(vocab)\n    return instances.as_tensor_dict()\n\n\noptimizer = torch.optim.Adam(model.parameters())\nlr_scheduler = None\n\nbest_loss = 99999\nbest_valid_predictions = list()\n\ntrain_dl = DataLoader(CLRPDataset(train_instances),\n                batch_size=config[\"batch_size_reg_train\"],\n                shuffle=False,\n                collate_fn=collate_fn,\n#                 num_workers = 4,\n                pin_memory=True,\n                drop_last=False)\n\n# for i in range(config['epochs']):\nmodel.train()\ntrain_loss = 0\nfor i, inputs in enumerate(train_dl):\n    inputs = util.move_to_device(inputs, device)\n    optimizer.zero_grad()\n    outputs = model(**inputs)\n    loss = outputs['loss']\n    loss.backward()\n    optimizer.step()\n\n    train_loss += loss.item()\n\n#     print(f\"epoch:{i} \")\n\n#     valid_loss, valid_predictions = valid_loop(valid_dl, model, loss_fn, device)\n#     print(f\" | Validation loss:{valid_loss}  \")\n#     if valid_loss <= best_loss:\n\n#         print(f\"Validation loss Decreased from {best_loss} to {valid_loss}\")\n\n#         best_loss = valid_loss\n#         best_valid_predictions = valid_predictions\n\n# fold_valid_predictions.append(best_valid_predictions)\n# fold_valid_targets.append(target.tolist())\n\n# torch.save(model.state_dict(), './model.th')\ntrain_loss /= len(train_dl)\nprint(f\"Training loss:{train_loss}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-06T13:33:47.537227Z","iopub.execute_input":"2021-06-06T13:33:47.537572Z","iopub.status.idle":"2021-06-06T13:33:59.109672Z","shell.execute_reply.started":"2021-06-06T13:33:47.537542Z","shell.execute_reply":"2021-06-06T13:33:59.108292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict\ntest = pd.read_csv(data_dir + 'test.csv')\ntest_instances = list()\nfor text in list(test['excerpt']):\n    tokens = tokenizer.tokenize(text)\n    text_field = TextField(tokens, token_indexers={'tokens': indexer})\n    fields = {\"tokens\": text_field}\n    test_instances.append(Instance(fields))\ntest_instances = collate_fn(test_instances)\nwith torch.no_grad():\n    test_instances = util.move_to_device(test_instances, device)\n    predictions = model(**test_instances)\nsubmission = pd.DataFrame({'id':test.id,'target':predictions['logits'].squeeze().cpu().detach().numpy()})\nsubmission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T13:26:02.36382Z","iopub.execute_input":"2021-06-06T13:26:02.364139Z","iopub.status.idle":"2021-06-06T13:26:02.728523Z","shell.execute_reply.started":"2021-06-06T13:26:02.364109Z","shell.execute_reply":"2021-06-06T13:26:02.727757Z"},"trusted":true},"execution_count":null,"outputs":[]}]}