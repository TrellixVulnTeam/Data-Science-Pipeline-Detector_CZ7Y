{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n\n\nThis notebook is a LightGBM learning & inference model using Word2vec.  It's a very light model so it can be run on a CPU.\n\nWord2vec represents words in 300 dimensions. By averaging the 300-dimensional vectors of the words in the sentence, the sentence was represented in 300 dimensions.\n\nSince the parameters are hardly changed, there is a possibility of improving the score.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gensim\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/commonlitreadabilityprize/'\ntrain = pd.read_csv(data_dir + 'train.csv')\ntest = pd.read_csv(data_dir + 'test.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\ntarget = train['target'].to_numpy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Embedding by Word2vec","metadata":{}},{"cell_type":"code","source":"word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)\nprint(word2vec_model.vectors.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def avg_feature_vector(sentence, model, num_features):\n    words = sentence.replace('\\n',\" \").replace(',',' ').replace('.',\" \").split()\n    feature_vec = np.zeros((num_features,),dtype=\"float32\")#特徴ベクトルの初期化\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, model[word])\n        except KeyError as error:\n            feature_vec \n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2vec_train = np.zeros((len(train.index),300),dtype=\"float32\")#特徴ベクトルの初期化\nword2vec_test = np.zeros((len(test.index),300),dtype=\"float32\")\n\nfor i in range(len(train.index)):\n    word2vec_train[i] = avg_feature_vector(train[\"excerpt\"][i],word2vec_model, 300)\n    \nfor i in range(len(test.index)):\n    word2vec_test[i] = avg_feature_vector(test[\"excerpt\"][i],word2vec_model, 300) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(word2vec_train.shape)\nprint(target.shape)\nprint(word2vec_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training & Inference\nlightgbm (KFold=5)","metadata":{}},{"cell_type":"code","source":"#parameter settings\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'seed': 42,\n    'learning_rate': 0.01,\n    \"n_jobs\": -1,\n    \"verbose\": -1\n}\n\npred = np.zeros(test.shape[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#KFold 　n_splits=5\nfrom sklearn.model_selection import KFold\nfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncv=list(fold.split(word2vec_train, target))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmses = []\nfor tr_idx, val_idx in cv: \n    x_tr, x_va = word2vec_train[tr_idx], word2vec_train[val_idx]\n    y_tr, y_va = target[tr_idx], target[val_idx]\n        \n    train_set = lgb.Dataset(x_tr, y_tr)\n    val_set = lgb.Dataset(x_va, y_va, reference=train_set)\n        \n    # Training\n    model = lgb.train(params, train_set, num_boost_round=10000, early_stopping_rounds=100,\n                      valid_sets=[train_set, val_set], verbose_eval=-1)\n        \n    y_pred = model.predict(x_va)\n    rmse =  np.sqrt(mean_squared_error(y_va, y_pred))\n    rmses.append(rmse)\n        \n    #Inference\n    test_pred = model.predict(word2vec_test)\n    pred += test_pred / 5  \n        \nprint(\"\\n\", \"Mean Fold RMSE:\", np.mean(rmses))    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.target = pred\nsample_submission.to_csv('submission.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{},"execution_count":null,"outputs":[]}]}