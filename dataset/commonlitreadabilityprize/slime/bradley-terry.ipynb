{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nprint(pd.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:54:49.739867Z","iopub.execute_input":"2021-07-28T15:54:49.740289Z","iopub.status.idle":"2021-07-28T15:54:49.781461Z","shell.execute_reply.started":"2021-07-28T15:54:49.740246Z","shell.execute_reply":"2021-07-28T15:54:49.780713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_sample = pd.read_csv('/kaggle/input/samples-bt/sample_10.csv')\nbase_sample","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:54:49.782779Z","iopub.execute_input":"2021-07-28T15:54:49.783097Z","iopub.status.idle":"2021-07-28T15:54:49.835545Z","shell.execute_reply.started":"2021-07-28T15:54:49.783064Z","shell.execute_reply":"2021-07-28T15:54:49.834767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data and Get Probabilities from Targets\n\nWe are getting probabilities to train a cross-encoder that compares two passages and decides the long-run probability of a reviewer rating one passage as easier than the other.","metadata":{}},{"cell_type":"code","source":"train_path = \"../input/commonlitreadabilityprize/train.csv\"\ndf = pd.read_csv(train_path)\ndf.head(n=3)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:54:49.839458Z","iopub.execute_input":"2021-07-28T15:54:49.841363Z","iopub.status.idle":"2021-07-28T15:54:49.943465Z","shell.execute_reply.started":"2021-07-28T15:54:49.841328Z","shell.execute_reply":"2021-07-28T15:54:49.942636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create all unique pairs of excerpts in the training set\n\nShould be $2 |A|\\cdot|B| - 2|A|$ total samples in the training set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom itertools import combinations, permutations\n\ndef cartesian(df_1, df_2):\n    df_1['key'] = 0\n    df_2['key'] = 0\n\n    df_cartesian = df_1.merge(df_2, how='outer', on='key')\n    return df_cartesian\n\ndef create_training_set(df_train):\n    combination_idx = list(combinations(range(len(df_train)), 2))\n    print(f'number of combination indicies: {len(combination_idx)}')\n\n    left_idx, right_idx = zip(*combination_idx)\n    left, right = df_train.loc[left_idx, :], df_train.loc[right_idx, :]\n    train = pd.concat([left.reset_index(), right.reset_index()], axis=1)\n    train.columns = ['index', 'id', 'url_legal', 'license', 'excerpt_l', 'target_l',\n           'standard_error_l', 'index', 'id', 'url_legal', 'license', 'excerpt_r',\n           'target_r', 'standard_error_r']\n\n    assert len(train) == len(combination_idx)\n    return train\n\n\ndef create_samples_train(raw_df, samples):\n    df = raw_df[~raw_df.id.isin(samples.id)]\n    print(f'number of df rows without samples: {len(df)}')\n    kfold = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True)\n#     skfold = StratifiedKFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True) # TODO: stratify by standard error?\n    splits= kfold.split(df)\n    for i,(train_index, test_index) in enumerate(splits):\n        print(train_index.shape,test_index.shape)\n        df_train, df_test = df.iloc[train_index,:].reset_index(drop=True), df.iloc[test_index,:].reset_index(drop=True)\n        break\n    df_train = pd.concat([df_train, samples])\n    print(f'total shape: {len(df_train)}')\n    ab = cartesian(samples, df_train)\n    ba = cartesian(df_train, samples)\n    print(f'|A||B| = {len(ab)}')\n    print(f'|B||A| = {len(ba)}')\n    return pd.concat([ab, ba]), df_test","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:54:49.945033Z","iopub.execute_input":"2021-07-28T15:54:49.945476Z","iopub.status.idle":"2021-07-28T15:54:50.891932Z","shell.execute_reply.started":"2021-07-28T15:54:49.94544Z","shell.execute_reply":"2021-07-28T15:54:50.891049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_batch_size = 16\ntrain_size = 100000\nRANDOM_STATE = 42","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:54:50.895017Z","iopub.execute_input":"2021-07-28T15:54:50.895279Z","iopub.status.idle":"2021-07-28T15:54:50.901102Z","shell.execute_reply.started":"2021-07-28T15:54:50.895253Z","shell.execute_reply":"2021-07-28T15:54:50.900222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kfold = KFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True)\n#     skfold = StratifiedKFold(n_splits=5, random_state=RANDOM_STATE, shuffle=True) # TODO: stratify by standard error?\nsplits= kfold.split(df)\nfor i,(train_index, test_index) in enumerate(splits):\n    print(train_index.shape,test_index.shape)\n    raw = df.iloc[train_index,:].reset_index(drop=True)\n    break","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:54:50.903985Z","iopub.execute_input":"2021-07-28T15:54:50.90428Z","iopub.status.idle":"2021-07-28T15:54:50.913859Z","shell.execute_reply.started":"2021-07-28T15:54:50.904254Z","shell.execute_reply":"2021-07-28T15:54:50.913127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test = create_samples_train(df, base_sample)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:54:50.916917Z","iopub.execute_input":"2021-07-28T15:54:50.917218Z","iopub.status.idle":"2021-07-28T15:54:50.965743Z","shell.execute_reply.started":"2021-07-28T15:54:50.91719Z","shell.execute_reply":"2021-07-28T15:54:50.964785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train[df_train.id_x != df_train.id_y]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:54:50.968366Z","iopub.execute_input":"2021-07-28T15:54:50.968704Z","iopub.status.idle":"2021-07-28T15:54:50.995234Z","shell.execute_reply.started":"2021-07-28T15:54:50.968676Z","shell.execute_reply":"2021-07-28T15:54:50.994536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df_train), 2269*10*2 - 2*10","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:54:50.997123Z","iopub.execute_input":"2021-07-28T15:54:50.997468Z","iopub.status.idle":"2021-07-28T15:54:51.004854Z","shell.execute_reply.started":"2021-07-28T15:54:50.997433Z","shell.execute_reply":"2021-07-28T15:54:51.003985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:54:51.006109Z","iopub.execute_input":"2021-07-28T15:54:51.006456Z","iopub.status.idle":"2021-07-28T15:55:04.533839Z","shell.execute_reply.started":"2021-07-28T15:54:51.006417Z","shell.execute_reply":"2021-07-28T15:55:04.53292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross Encoder Class","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\nimport numpy as np\nimport logging\nimport os\nfrom typing import Dict, Type, Callable, List\nimport transformers\nimport torch\nfrom torch import nn\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\nfrom tqdm.autonotebook import tqdm, trange\nfrom sentence_transformers import SentenceTransformer, util\nfrom sentence_transformers.evaluation import SentenceEvaluator\n\nlogger = logging.getLogger(__name__)\n\nclass CrossEncoder():\n    def __init__(self, model_name:str, num_labels:int = None, max_length:int = None, device:str = None, tokenizer_args:Dict = {},\n                 default_activation_function = None, kl = False):\n        \"\"\"\n        A CrossEncoder takes exactly two sentences / texts as input and either predicts\n        a score or label for this sentence pair. It can for example predict the similarity of the sentence pair\n        on a scale of 0 ... 1.\n        It does not yield a sentence embedding and does not work for individually sentences.\n        :param model_name: Any model name from Huggingface Models Repository that can be loaded with AutoModel. We provide several pre-trained CrossEncoder models that can be used for common tasks\n        :param num_labels: Number of labels of the classifier. If 1, the CrossEncoder is a regression model that outputs a continous score 0...1. If > 1, it output several scores that can be soft-maxed to get probability scores for the different classes.\n        :param max_length: Max length for input sequences. Longer sequences will be truncated. If None, max length of the model will be used\n        :param device: Device that should be used for the model. If None, it will use CUDA if available.\n        :param tokenizer_args: Arguments passed to AutoTokenizer\n        :param default_activation_function: Callable (like nn.Sigmoid) about the default activation function that should be used on-top of model.predict(). If None. nn.Sigmoid() will be used if num_labels=1, else nn.Identity()\n        \"\"\"\n        self.kl = kl\n        self.config = AutoConfig.from_pretrained(model_name)\n        classifier_trained = True\n        if self.config.architectures is not None:\n            classifier_trained = any([arch.endswith('ForSequenceClassification') for arch in self.config.architectures])\n\n        if num_labels is None and not classifier_trained:\n            num_labels = 1\n\n        if num_labels is not None:\n            self.config.num_labels = num_labels\n\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, config=self.config)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, **tokenizer_args)\n        self.max_length = max_length\n\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            logger.info(\"Use pytorch device: {}\".format(device))\n\n        self._target_device = torch.device(device)\n\n        if default_activation_function is not None:\n            self.default_activation_function = default_activation_function\n            try:\n                self.config.sbert_ce_default_activation_function = util.fullname(self.default_activation_function)\n            except Exception as e:\n                logger.warning(\"Was not able to update config about the default_activation_function: {}\".format(str(e)) )\n        elif hasattr(self.config, 'sbert_ce_default_activation_function') and self.config.sbert_ce_default_activation_function is not None:\n            self.default_activation_function = util.import_from_string(self.config.sbert_ce_default_activation_function)()\n        else:\n            self.default_activation_function = nn.Sigmoid() if self.config.num_labels == 1 else nn.Identity()\n\n    def smart_batching_collate(self, batch):\n        texts = [[] for _ in range(len(batch[0].texts))]\n        labels = []\n\n        for example in batch:\n            for idx, text in enumerate(example.texts):\n                texts[idx].append(text.strip())\n\n            labels.append(example.label)\n\n        tokenized = self.tokenizer(*texts, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_length)\n        labels = torch.tensor(labels, dtype=torch.float if (self.config.num_labels == 1 or self.kl) else torch.long).to(self._target_device)\n\n        for name in tokenized:\n            tokenized[name] = tokenized[name].to(self._target_device)\n\n        return tokenized, labels\n\n    def smart_batching_collate_text_only(self, batch):\n        texts = [[] for _ in range(len(batch[0]))]\n\n        for example in batch:\n            for idx, text in enumerate(example):\n                texts[idx].append(text.strip())\n\n        tokenized = self.tokenizer(*texts, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_length)\n\n        for name in tokenized:\n            tokenized[name] = tokenized[name].to(self._target_device)\n\n        return tokenized\n\n    def fit(self,\n            train_dataloader: DataLoader,\n            evaluator: SentenceEvaluator = None,\n            epochs: int = 1,\n            loss_fct = None,\n            activation_fct = nn.Identity(),\n            scheduler: str = 'WarmupLinear',\n            warmup_steps: int = 10000,\n            optimizer_class: Type[Optimizer] = transformers.AdamW,\n            optimizer_params: Dict[str, object] = {'lr': 2e-5},\n            weight_decay: float = 0.01,\n            evaluation_steps: int = 0,\n            output_path: str = None,\n            save_best_model: bool = True,\n            max_grad_norm: float = 1,\n            use_amp: bool = False,\n            callback: Callable[[float, int, int], None] = None,\n            ):\n        \"\"\"\n        Train the model with the given training objective\n        Each training objective is sampled in turn for one batch.\n        We sample only as many batches from each objective as there are in the smallest one\n        to make sure of equal training with each dataset.\n        :param train_dataloader: DataLoader with training InputExamples\n        :param evaluator: An evaluator (sentence_transformers.evaluation) evaluates the model performance during training on held-out dev data. It is used to determine the best model that is saved to disc.\n        :param epochs: Number of epochs for training\n        :param loss_fct: Which loss function to use for training. If None, will use nn.BCEWithLogitsLoss() if self.config.num_labels == 1 else nn.CrossEntropyLoss()\n        :param activation_fct: Activation function applied on top of logits output of model.\n        :param scheduler: Learning rate scheduler. Available schedulers: constantlr, warmupconstant, warmuplinear, warmupcosine, warmupcosinewithhardrestarts\n        :param warmup_steps: Behavior depends on the scheduler. For WarmupLinear (default), the learning rate is increased from o up to the maximal learning rate. After these many training steps, the learning rate is decreased linearly back to zero.\n        :param optimizer_class: Optimizer\n        :param optimizer_params: Optimizer parameters\n        :param weight_decay: Weight decay for model parameters\n        :param evaluation_steps: If > 0, evaluate the model using evaluator after each number of training steps\n        :param output_path: Storage path for the model and evaluation files\n        :param save_best_model: If true, the best model (according to evaluator) is stored at output_path\n        :param max_grad_norm: Used for gradient normalization.\n        :param use_amp: Use Automatic Mixed Precision (AMP). Only for Pytorch >= 1.6.0\n        :param callback: Callback function that is invoked after each evaluation.\n                It must accept the following three parameters in this order:\n                `score`, `epoch`, `steps`\n        \"\"\"\n        train_dataloader.collate_fn = self.smart_batching_collate\n\n        if use_amp:\n            from torch.cuda.amp import autocast\n            scaler = torch.cuda.amp.GradScaler()\n\n        self.model.to(self._target_device)\n\n        if output_path is not None:\n            os.makedirs(output_path, exist_ok=True)\n\n        self.best_score = -9999999\n        num_train_steps = int(len(train_dataloader) * epochs)\n\n        # Prepare optimizers\n        param_optimizer = list(self.model.named_parameters())\n\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n\n        optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n\n        if isinstance(scheduler, str):\n            scheduler = SentenceTransformer._get_scheduler(optimizer, scheduler=scheduler, warmup_steps=warmup_steps, t_total=num_train_steps)\n\n        if loss_fct is None:\n            loss_fct = nn.BCEWithLogitsLoss() if self.config.num_labels == 1 else nn.CrossEntropyLoss()\n\n\n        skip_scheduler = False\n        for epoch in trange(epochs, desc=\"Epoch\"):\n            training_steps = 0\n            self.model.zero_grad()\n            self.model.train()\n\n            for features, labels in tqdm(train_dataloader, desc=\"Iteration\", smoothing=0.05):\n                if use_amp:\n                    with autocast():\n                        model_predictions = self.model(**features, return_dict=True)\n                        logits = activation_fct(model_predictions.logits)\n                        if self.config.num_labels == 1:\n                            logits = logits.view(-1)\n                        loss_value = loss_fct(logits, labels)\n\n                    scale_before_step = scaler.get_scale()\n                    scaler.scale(loss_value).backward()\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n                    scaler.step(optimizer)\n                    scaler.update()\n\n                    skip_scheduler = scaler.get_scale() != scale_before_step\n                else:\n                    model_predictions = self.model(**features, return_dict=True)\n                    logits = activation_fct(model_predictions.logits)\n                    if self.config.num_labels == 1:\n                        logits = logits.view(-1)\n                    loss_value = loss_fct(logits, labels)\n                    loss_value.backward()\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n                    optimizer.step()\n\n                optimizer.zero_grad()\n\n                if not skip_scheduler:\n                    scheduler.step()\n\n                training_steps += 1\n\n                if evaluator is not None and evaluation_steps > 0 and training_steps % evaluation_steps == 0:\n                    self._eval_during_training(evaluator, output_path, save_best_model, epoch, training_steps, callback)\n\n                    self.model.zero_grad()\n                    self.model.train()\n\n            if evaluator is not None:\n                self._eval_during_training(evaluator, output_path, save_best_model, epoch, -1, callback)\n\n\n\n    def predict(self, sentences: List[List[str]],\n               batch_size: int = 32,\n               show_progress_bar: bool = None,\n               num_workers: int = 0,\n               activation_fct = None,\n               apply_softmax = False,\n               convert_to_numpy: bool = True,\n               convert_to_tensor: bool = False\n               ):\n        \"\"\"\n        Performs predicts with the CrossEncoder on the given sentence pairs.\n        :param sentences: A list of sentence pairs [[Sent1, Sent2], [Sent3, Sent4]]\n        :param batch_size: Batch size for encoding\n        :param show_progress_bar: Output progress bar\n        :param num_workers: Number of workers for tokenization\n        :param activation_fct: Activation function applied on the logits output of the CrossEncoder. If None, nn.Sigmoid() will be used if num_labels=1, else nn.Identity\n        :param convert_to_numpy: Convert the output to a numpy matrix.\n        :param apply_softmax: If there are more than 2 dimensions and apply_softmax=True, applies softmax on the logits output\n        :param convert_to_tensor:  Conver the output to a tensor.\n        :return: Predictions for the passed sentence pairs\n        \"\"\"\n        input_was_string = False\n        if isinstance(sentences[0], str):  # Cast an individual sentence to a list with length 1\n            sentences = [sentences]\n            input_was_string = True\n\n        inp_dataloader = DataLoader(sentences, batch_size=batch_size, collate_fn=self.smart_batching_collate_text_only, num_workers=num_workers, shuffle=False)\n\n        if show_progress_bar is None:\n            show_progress_bar = (logger.getEffectiveLevel() == logging.INFO or logger.getEffectiveLevel() == logging.DEBUG)\n\n        iterator = inp_dataloader\n        if show_progress_bar:\n            iterator = tqdm(inp_dataloader, desc=\"Batches\")\n\n        if activation_fct is None:\n            activation_fct = self.default_activation_function\n\n        pred_scores = []\n        self.model.eval()\n        self.model.to(self._target_device)\n        with torch.no_grad():\n            for features in iterator:\n                model_predictions = self.model(**features, return_dict=True)\n                logits = activation_fct(model_predictions.logits)\n\n                if apply_softmax and len(logits[0]) > 1:\n                    logits = torch.nn.functional.softmax(logits, dim=1)\n                pred_scores.extend(logits)\n\n        if self.config.num_labels == 1:\n            pred_scores = [score[0] for score in pred_scores]\n\n        if convert_to_tensor:\n            pred_scores = torch.stack(pred_scores)\n        elif convert_to_numpy:\n            pred_scores = np.asarray([score.cpu().detach().numpy() for score in pred_scores])\n\n        if input_was_string:\n            pred_scores = pred_scores[0]\n\n        return pred_scores\n\n\n    def _eval_during_training(self, evaluator, output_path, save_best_model, epoch, steps, callback):\n        \"\"\"Runs evaluation during the training\"\"\"\n        if evaluator is not None:\n            score = evaluator(self, output_path=output_path, epoch=epoch, steps=steps)\n            if callback is not None:\n                callback(score, epoch, steps)\n            if score > self.best_score:\n                self.best_score = score\n                if save_best_model:\n                    self.save(output_path)\n\n    def save(self, path):\n        \"\"\"\n        Saves all model and tokenizer to path\n        \"\"\"\n        if path is None:\n            return\n\n        logger.info(\"Save model to {}\".format(path))\n        self.model.save_pretrained(path)\n        self.tokenizer.save_pretrained(path)\n\n    def save_pretrained(self, path):\n        \"\"\"\n        Same function as save\n        \"\"\"\n        return self.save(path)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:56:14.098862Z","iopub.execute_input":"2021-07-28T15:56:14.099218Z","iopub.status.idle":"2021-07-28T15:56:14.147417Z","shell.execute_reply.started":"2021-07-28T15:56:14.099186Z","shell.execute_reply":"2021-07-28T15:56:14.146601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluator Classes","metadata":{}},{"cell_type":"code","source":"import itertools\nfrom tqdm.notebook import trange\nfrom torch.utils.data import DataLoader\nimport math\nimport torch\nfrom sentence_transformers import LoggingHandler, util\nfrom sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\nfrom sentence_transformers import InputExample\nimport logging\nfrom datetime import datetime\nimport sys\nimport os\nimport gzip\nfrom torch.nn import KLDivLoss\nimport csv\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef get_rmse(model, data, target):\n    preds = model.predict(data)\n    preds = [probability_to_ability(p) for p in preds]\n    return mean_squared_error(target, preds, squared=False)\n\ndef construct_input_example_kl(row):\n    p = get_probability(row.target_x, row.target_y)\n    label = [p, 1-p]\n    return InputExample(texts=[row.excerpt_x, row.excerpt_y], label=label)\n\ndef construct_input_example(row):\n    p = get_probability(row.target_x, row.target_y)\n    return InputExample(texts=[row.excerpt_x, row.excerpt_y], label=p)\n\ndef get_train_samples(train, n, kl, random_state=42):\n    if kl:\n        fn = construct_input_example_kl\n    else:\n        fn = construct_input_example\n    return train.sample(n=n,random_state=random_state).reset_index().apply(fn, axis=1)\n\ndef get_test_samples(test_df):\n    return test_df.apply(lambda row: [BASE_EXCERPT, row.excerpt], axis=1).values\n\ndef get_probability(t1, t2):\n    return np.exp(t1) / (np.exp(t1) + np.exp(t2))\n\ndef probability_to_ability(p):\n    return np.log((1-p) / p)\n\ndef gen_probability_matrix(df):\n    m = np.zeros((len(df), len(df)))\n    for i, j in itertools.permutations(range(len(df)), r=2):\n        t1, t2 = df.iloc[i].target, df.iloc[j].target\n        m[i][j] = get_probability(t1, t2)\n\n    return m\n\ndef mle(pmat, max_iter=1000):\n    n = pmat.shape[0]\n    wins = np.sum(pmat, axis=0)\n    params = np.ones(n, dtype=float)\n    for _ in range(max_iter):\n        tiled = np.tile(params, (n, 1))\n        combined = 1.0 / (tiled + tiled.T)\n        np.fill_diagonal(combined, 0)\n        nxt = wins / np.sum(combined, axis=0)\n        nxt = nxt / np.mean(nxt)\n        if np.linalg.norm(nxt - params, ord=np.inf) < 1e-6:\n            return nxt\n        params = nxt\n    print(f'faulty matrix: {pmat}')\n    print(f'end params: {params}')\n    raise RuntimeError('did not converge')\n\n\nclass SimpleBTEvaluator(SentenceEvaluator):\n    def __init__(self, test_df):\n        self.data = test_df.apply(lambda row: [row.excerpt, BASE_EXCERPT], axis=1).values\n        self.target = test_df.target\n        \n    def __call__(self, model, output_path, epoch=-1, steps=-1):\n        preds = model.predict(self.data, apply_softmax=True)\n        preds = [probability_to_ability(p) for p in preds[:, 0]]\n        error = mean_squared_error(self.target, preds, squared=False)\n        print(f'mse for epoch {epoch} step {steps}: {error}')\n        return error\n    \nclass SampleBTEvaluator(SentenceEvaluator):\n    def __init__(self, test_df, samples, kl):\n        cross_df = cartesian(test_df, samples)\n        self.samples = samples\n        self.target = test_df.target\n        self.data = cross_df.apply(lambda row: [row.excerpt_x, row.excerpt_y], axis=1).values\n        self.rev_data = cross_df.apply(lambda row: [row.excerpt_y, row.excerpt_x], axis=1).values\n        self.kl = kl\n        \n    def __call__(self, model, output_path, epoch=-1, steps=-1):\n        preds = model.predict(self.data, apply_softmax=True)\n        if self.kl:\n            preds = preds[:, 0]\n        preds = preds.reshape(preds.shape[0] // len(self.samples), len(self.samples))\n        rev_preds = model.predict(self.rev_data, apply_softmax=True)\n        if self.kl:\n            rev_preds = rev_preds[:, 0]\n        rev_preds = rev_preds.reshape(rev_preds.shape[0] // len(self.samples), len(self.samples))\n        p_m = gen_probability_matrix(self.samples)\n        \n#         return preds, rev_preds\n           \n        ability_preds = []\n                                       \n        for i in trange(len(self.target)):\n            inf_matrix = np.zeros((len(p_m) + 1, len(p_m) + 1))\n            inf_matrix[:-1, :-1] = p_m\n            inf_matrix[-1, :] = np.append(preds[i], 0)\n            inf_matrix[:, -1] = np.append(rev_preds[i], 0)\n            params = mle(inf_matrix)\n            scaled_params = -np.log(params) - 1\n            ability_preds.append(scaled_params[-1])\n                                       \n        error = mean_squared_error(self.target, ability_preds, squared=False)\n        print(f'mse for epoch {epoch} step {steps}: {error}')\n        \n        csv_path = './val_metrics.csv'\n        output_file_exists = os.path.isfile(csv_path)\n        with open(csv_path, mode=\"a\" if output_file_exists else 'w', encoding=\"utf-8\") as f:\n            writer = csv.writer(f)\n            if not output_file_exists:\n                writer.writerow(['epoch', 'steps', 'error'])\n            writer.writerow([epoch, steps, error])\n            \n        return error\n    \n    \nclass DummyBTEvaluator(SentenceEvaluator):\n    def __init__(self, test_df, samples):\n        cross_df = cartesian(test_df, samples)\n        self.samples = samples\n        self.data = cross_df.apply(lambda row: [row.excerpt_x, row.excerpt_y], axis=1).values\n        self.rev_data = cross_df.apply(lambda row: [row.excerpt_y, row.excerpt_x], axis=1).values\n        \n        target = cross_df.apply(lambda row: get_probability(row.target_x, row.target_y), axis=1).values\n        target = torch.tensor(target)\n        self.pred = target\n        rev_target = cross_df.apply(lambda row: get_probability(row.target_y, row.target_x), axis=1).values\n        rev_target = torch.tensor(rev_target)\n        self.rev_pred = rev_target\n        \n        self.target = test_df.target\n        \n    def __call__(self, model, output_path, epoch=-1, steps=-1):\n        preds = self.pred.reshape(self.pred.shape[0] // len(self.samples), len(self.samples))     \n        rev_preds = self.rev_pred.reshape(self.rev_pred.shape[0] // len(self.samples), len(self.samples))\n        p_m = gen_probability_matrix(self.samples)\n           \n        ability_preds = []\n                                       \n        for i in trange(len(preds)):\n            inf_matrix = np.zeros((len(p_m) + 1, len(p_m) + 1))\n            inf_matrix[:-1, :-1] = p_m\n            inf_matrix[-1, :] = np.append(preds[i], 0)\n            inf_matrix[:, -1] = np.append(rev_preds[i], 0)\n            params = mle(inf_matrix)\n            scaled_params = -np.log(params) - 1\n            ability_preds.append(scaled_params[-1])\n                                       \n        error = mean_squared_error(self.target, ability_preds, squared=False)\n        print(f'mse for epoch {epoch} step {steps}: {error}')\n        return error\n    \n    \nclass SampleKLEvaluator(SentenceEvaluator):\n    def __init__(self, test_df, samples):\n        cross_df = cartesian(test_df, samples)\n        self.samples = samples\n        self.data = cross_df.apply(lambda row: [row.excerpt_x, row.excerpt_y], axis=1).values\n        targets = cross_df.apply(lambda row: get_probability(row.target_x, row.target_y), axis=1).values\n        self.rev_data = cross_df.apply(lambda row: [row.excerpt_y, row.excerpt_x], axis=1).values\n        rev_targets = cross_df.apply(lambda row: get_probability(row.target_y, row.target_x), axis=1).values\n        self.target = np.concatenate([targets, rev_targets])\n        self.target = torch.tensor(self.target)\n        self.target = torch.stack([self.target, 1 - self.target], axis=1)\n        print(self.target.shape)\n        \n    def __call__(self, model, output_path, epoch=-1, steps=-1):\n        preds = model.predict(self.data, apply_softmax=True, convert_to_tensor=True)\n        preds = torch.log(preds)\n        \n        rev_preds = model.predict(self.rev_data, apply_softmax=True, convert_to_tensor=True)\n        rev_preds = torch.log(rev_preds)\n        \n        loss = KLDivLoss(reduction='batchmean')(torch.cat([preds, rev_preds]), self.target.to(DEVICE))\n\n        print(f'kl divergence for epoch {epoch} step {steps}: {loss}')\n        return loss\n    \nclass SampleEvaluator(SentenceEvaluator):\n    def __init__(self, test_df, samples):\n        cross_df = cartesian(test_df, samples) # it's important that the test_df is on the left. c(a, b) != c(b, a)\n        self.cross_df = cross_df\n        self.samples = samples\n        self.data = cross_df.apply(lambda row: [row.excerpt_x, row.excerpt_y], axis=1).values\n        self.rev_data = cross_df.apply(lambda row: [row.excerpt_y, row.excerpt_x], axis=1).values\n\n        target = cross_df.apply(lambda row: get_probability(row.target_x, row.target_y), axis=1).values\n        target = torch.tensor(target)\n        self.target = torch.stack([target, 1 - target], axis=1)\n        rev_target = cross_df.apply(lambda row: get_probability(row.target_y, row.target_x), axis=1).values\n        rev_target = torch.tensor(rev_target)\n        self.rev_target = torch.stack([rev_target, 1 - rev_target], axis=1)\n        \n    def __call__(self, model, output_path, epoch=-1, steps=-1):\n        preds = model.predict(self.data, apply_softmax=True, convert_to_tensor=True)        \n        rev_preds = model.predict(self.rev_data, apply_softmax=True, convert_to_tensor=True)\n        \n        return preds, rev_preds, self.target.to(DEVICE), self.rev_target.to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.330949Z","iopub.execute_input":"2021-07-28T15:55:10.331311Z","iopub.status.idle":"2021-07-28T15:55:10.430292Z","shell.execute_reply.started":"2021-07-28T15:55:10.331271Z","shell.execute_reply":"2021-07-28T15:55:10.429398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unit test\n\n# model = CrossEncoder('distilroberta-base', num_labels=2)\n# x = SampleKLEvaluator(base_sample, df_test)\n# x(model, '')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.431818Z","iopub.execute_input":"2021-07-28T15:55:10.432214Z","iopub.status.idle":"2021-07-28T15:55:10.436771Z","shell.execute_reply.started":"2021-07-28T15:55:10.432173Z","shell.execute_reply":"2021-07-28T15:55:10.435659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"import logging\nimport torch\n# from sentence_transformers import CrossEncoder\n\nKL = True\n\nmodel_save_path = './fold'\nif KL:\n    model = CrossEncoder('distilroberta-base', num_labels=2, kl=True)\nelse:\n    model = CrossEncoder('distilroberta-base', num_labels=1)\ntrain_samples = get_train_samples(df_train, len(df_train), kl=KL, random_state=RANDOM_STATE)\ntrain_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size)\n\n# Configure the training\nwarmup_steps = math.ceil(len(train_dataloader) * 1 * 0.1) #10% of train data for warm-up\nprint(\"Warmup-steps: {}\".format(warmup_steps))\n\nmodel.fit(train_dataloader=train_dataloader,\n          loss_fct=torch.nn.KLDivLoss(reduction='batchmean'),\n          activation_fct = torch.nn.LogSoftmax(dim=1),\n#               evaluator = SimpleBTEvaluator(df_test),\n          evaluator = SampleBTEvaluator(df_test, base_sample, KL),\n          evaluation_steps = 100,\n          epochs=3,\n          output_path=model_save_path) \n\n# CE\n# model.fit(train_dataloader=train_dataloader,\n#           epochs=3,\n#           warmup_steps=warmup_steps,\n#           output_path=model_save_path, \n#           evaluator = SampleBTEvaluator(df_test, base_sample, False),\n#           evaluation_steps = 100,\n#           use_amp=False)\nmodel.save(model_save_path)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:56:19.601653Z","iopub.execute_input":"2021-07-28T15:56:19.602039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation\n\nFirst visually check one sample to make sure everything looks okay. Go through each step of the code one by one. Prove that it works / doesn't work one time.","metadata":{}},{"cell_type":"code","source":"# load saved model\n\n# model = CrossEncoder('/kaggle/input/crossencoder-kl1epoch/fold/')\n# s = SampleEvaluator(df_test, base_sample)\n# preds, rev_preds, target, rev_target = s(model, '')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.454075Z","iopub.status.idle":"2021-07-28T15:55:10.455019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds[:, 0]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.456662Z","iopub.status.idle":"2021-07-28T15:55:10.457429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = preds[:, 0]\n# rev_preds = rev_preds[:, 0]\n# preds = preds.reshape(preds.shape[0] // len(base_sample), len(base_sample))  \n# rev_preds = rev_preds.reshape(rev_preds.shape[0] // len(base_sample), len(base_sample))\n# p_m = gen_probability_matrix(base_sample)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.458889Z","iopub.status.idle":"2021-07-28T15:55:10.459585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_mse(sample):\n#     ability_preds = []\n#     p_m = gen_probability_matrix(sample)\n\n#     for i in trange(len(preds)):\n#         inf_matrix = np.zeros((len(p_m) + 1, len(p_m) + 1))\n#         inf_matrix[:-1, :-1] = p_m\n#         inf_matrix[-1, :] = np.append(preds[i].cpu(), 0)\n#         inf_matrix[:, -1] = np.append(rev_preds[i].cpu(), 0)\n#         params = mle(inf_matrix)\n#         scaled_params = -np.log(params) - 1\n#         ability_preds.append(scaled_params[-1])\n\n#     return mean_squared_error(df_test.target, ability_preds, squared=False)\n\n# samples = []\n# for i in trange(100):\n#     sample = df_train.sample(n=10)\n#     mse = get_mse(sample)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.461087Z","iopub.status.idle":"2021-07-28T15:55:10.461709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean_squared_error(df_test.target, ability_preds, squared=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.462937Z","iopub.status.idle":"2021-07-28T15:55:10.463672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looks okay so far.\n\n# preds[:10], target[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.464902Z","iopub.status.idle":"2021-07-28T15:55:10.465596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reverse looks okay as well.\n\n# rev_preds[:10], rev_target[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.466832Z","iopub.status.idle":"2021-07-28T15:55:10.467422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for sure no errors code\n\n# preds = []\n\n# for i in trange(len(df_test)):\n#     p_m_true = gen_probability_matrix(base_sample.append(df_test.iloc[0]))\n#     params = mle(p_m_true)\n#     scaled_params = -np.log(params) - 1\n#     preds.append(scaled_params[-1])","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.468627Z","iopub.status.idle":"2021-07-28T15:55:10.469231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean_squared_error(df_test.target, preds, squared=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.470353Z","iopub.status.idle":"2021-07-28T15:55:10.471099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean_squared_error(scaled_params, base_sample.append(df_test.iloc[0]).target, squared=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.472229Z","iopub.status.idle":"2021-07-28T15:55:10.472889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate probability matrix with samples + add sample (using perfect targets)\n\n# p_m = gen_probability_matrix(base_sample)\n\n# inf_matrix = np.zeros((len(p_m) + 1, len(p_m) + 1))\n# inf_matrix[:-1, :-1] = p_m\n# inf_matrix[-1, :] = np.append(target.cpu().numpy()[:10][:, 0], 0)\n# inf_matrix[:, -1] = np.append(rev_target.cpu().numpy()[:10][:, 0], 0)\n# params = mle(inf_matrix)\n# scaled_params = -np.log(params) - 1\n# print(f'pred: {scaled_params[-1]}, true: {df_test.iloc[0].target}, rmse: {np.sqrt((scaled_params[-1] - df_test.iloc[0].target)**2)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.474042Z","iopub.status.idle":"2021-07-28T15:55:10.474669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate probability matrix with samples + add sample (using actual predictions)\n\n# p_m = gen_probability_matrix(base_sample)\n\n# inf_matrix = np.zeros((len(p_m) + 1, len(p_m) + 1))\n# inf_matrix[:-1, :-1] = p_m\n# inf_matrix[:, -1] = np.append(preds.cpu().numpy()[:10][:, 0], 0)\n# inf_matrix[-1, :] = np.append(rev_preds.cpu().numpy()[:10][:, 0], 0)\n# params = mle(inf_matrix)\n# scaled_params = -np.log(params) - 1\n# print(f'pred: {scaled_params[-1]}, true: {df_test.iloc[0].target}, rmse: {np.sqrt((scaled_params[-1] - df_test.iloc[0].target)**2)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.476123Z","iopub.status.idle":"2021-07-28T15:55:10.47686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# d = DummyBTEvaluator(df_test, base_sample)('', '')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.478084Z","iopub.status.idle":"2021-07-28T15:55:10.478668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BASE_EXCERPT = df[df.standard_error == 0].iloc[0].excerpt\n# e = SimpleBTEvaluator(df_test)(model, '')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.480397Z","iopub.status.idle":"2021-07-28T15:55:10.481069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metrics = []\n\n# for i in trange(50):\n#     sample = raw.sample(n=10)\n#     mse = SampleBTEvaluator(df_test, sample)(model, '')\n#     metrics.append((mse, sample))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.482183Z","iopub.status.idle":"2021-07-28T15:55:10.482934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min([x[0] for x in metrics])","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.484212Z","iopub.status.idle":"2021-07-28T15:55:10.484847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ability_preds = []\n# p_m = gen_probability_matrix(base_sample)\n\n# for i in trange(len(df_test)):\n#     inf_matrix = np.zeros((len(p_m) + 1, len(p_m) + 1))\n#     inf_matrix[:-1, :-1] = p_m\n#     inf_matrix[-1, :] = np.append(preds[i], 0)\n#     inf_matrix[:, -1] = np.append(rev_preds[i], 0)\n#     params = mle(inf_matrix)\n#     scaled_params = -np.log(params) - 1\n#     ability_preds.append(scaled_params[-1])\n\n# error = mean_squared_error(self.target, ability_preds, squared=False)\n# print(f'mse for epoch {epoch} step {steps}: {error}')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.485984Z","iopub.status.idle":"2021-07-28T15:55:10.486605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = model.predict(test_samples)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.487735Z","iopub.status.idle":"2021-07-28T15:55:10.488298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x = torch.nn.functional.softmax(torch.tensor(preds), dim=1)[:, 0].numpy()\n\n# mean_squared_error([probability_to_ability(p) for p in x],  df_test.target, squared=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.489445Z","iopub.status.idle":"2021-07-28T15:55:10.490092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test.target","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.491193Z","iopub.status.idle":"2021-07-28T15:55:10.491818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BASE_EXCERPT = df[df.standard_error == 0].iloc[0].excerpt\n# SimpleBTEvaluator\n\n# test_samples = get_test_samples(df_test)\n# rmse = get_rmse(model, test_samples, df_test.target)\n# model.save(model_save_path)\n# print(f'valid rmse for epoch {i}: {rmse}')","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.492915Z","iopub.status.idle":"2021-07-28T15:55:10.493628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check to see if files were saved\n\n# import os\n# for dirname, _, filenames in os.walk('.'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.49475Z","iopub.status.idle":"2021-07-28T15:55:10.495365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Using Bradley-Terry Optimization","metadata":{}},{"cell_type":"code","source":"# import itertools\n# from tqdm import tqdm_notebook\n# from sklearn.metrics import mean_squared_error\n\n# def gen_probability_matrix(df):\n#     m = np.zeros((len(df), len(df)))\n#     for i, j in itertools.permutations(range(len(df)), r=2):\n#         t1, t2 = df.iloc[i].target, df.iloc[j].target\n#         m[i][j] = get_probability(t1, t2)\n#     return m\n\n\n# def mle(pmat, max_iter=1000):\n#     n = pmat.shape[0]\n#     wins = np.sum(pmat, axis=0)\n#     params = np.ones(n, dtype=float)\n#     for _ in range(max_iter):\n#         tiled = np.tile(params, (n, 1))\n#         combined = 1.0 / (tiled + tiled.T)\n#         np.fill_diagonal(combined, 0)\n#         nxt = wins / np.sum(combined, axis=0)\n#         nxt = nxt / np.mean(nxt)\n#         if np.linalg.norm(nxt - params, ord=np.inf) < 1e-6:\n#             return nxt\n#         params = nxt\n#     raise RuntimeError('did not converge')\n\n# def evaluate(preds, rev_preds):\n#     p_m = gen_probability_matrix(t_samples)\n#     ability_preds = []\n\n#     for i, row in tqdm_notebook(df_test.iterrows()):\n#         inf_matrix = np.zeros((len(p_m) + 1, len(p_m) + 1))\n#         inf_matrix[:-1, :-1] = p_m\n#         inf_matrix[:, -1] = np.append(preds[i], 0)\n#         inf_matrix[-1, :] = np.append(rev_preds[i], 0)\n#         params = mle(inf_matrix)\n#         scaled_params = -np.log(params) - 1\n#         ability_preds.append(scaled_params[-1])\n    \n#     return mean_squared_error(df_test.target, ability_preds)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.496503Z","iopub.status.idle":"2021-07-28T15:55:10.497129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# t_samples = df_train[df_train.excerpt.isin(raw_samples.excerpt_l.unique())].sample(n=1500, random_state=42).reset_index(drop=True)\n# samples_with_base = pd.concat([t_samples, df[df.target == 0]])","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.498298Z","iopub.status.idle":"2021-07-28T15:55:10.498948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mean_squared_error(samples_with_base.target, -np.log(mle(gen_probability_matrix(samples_with_base))) - 1, squared=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.499998Z","iopub.status.idle":"2021-07-28T15:55:10.500617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# t_samples.target","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.501668Z","iopub.status.idle":"2021-07-28T15:55:10.502275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Code with Annotations","metadata":{}},{"cell_type":"code","source":"# def smart_batching_collate(self, batch):\n#     texts = [[] for _ in range(len(batch[0].texts))]\n#     labels = []\n\n#     for example in batch:\n#         for idx, text in enumerate(example.texts):\n#             texts[idx].append(text.strip())\n\n#         labels.append(example.label)\n\n#     tokenized = self.tokenizer(*texts, padding=True, truncation='longest_first', return_tensors=\"pt\", max_length=self.max_length)\n#     labels = torch.tensor(labels, dtype=torch.float if self.config.num_labels == 1 else torch.long).to(self._target_device)\n\n#     for name in tokenized:\n#         tokenized[name] = tokenized[name].to(self._target_device)\n\n#     return tokenized, labels\n\n# train_dataloader.collate_fn = smart_batching_collate \n\n# if use_amp:\n#     from torch.cuda.amp import autocast\n#     scaler = torch.cuda.amp.GradScaler()\n\n# self.model.to(self._target_device)\n\n# if output_path is not None:\n#     os.makedirs(output_path, exist_ok=True)\n\n# self.best_score = -9999999\n# num_train_steps = int(len(train_dataloader) * epochs)\n\n# # Prepare optimizers\n# param_optimizer = list(self.model.named_parameters())\n\n# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n# optimizer_grouped_parameters = [\n#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n# ]\n\n# optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n\n# if isinstance(scheduler, str):\n#     scheduler = SentenceTransformer._get_scheduler(optimizer, scheduler=scheduler, warmup_steps=warmup_steps, t_total=num_train_steps)\n\n# if loss_fct is None:\n#     loss_fct = nn.BCEWithLogitsLoss() if self.config.num_labels == 1 else nn.CrossEntropyLoss()\n\n\n# skip_scheduler = False\n# for epoch in trange(epochs, desc=\"Epoch\"):\n#     training_steps = 0\n#     self.model.zero_grad()\n#     self.model.train()\n\n#     for features, labels in tqdm(train_dataloader, desc=\"Iteration\", smoothing=0.05):\n#         if use_amp:\n#             with autocast():\n#                 model_predictions = self.model(**features, return_dict=True)\n#                 logits = activation_fct(model_predictions.logits)\n#                 if self.config.num_labels == 1:\n#                     logits = logits.view(-1)\n#                 loss_value = loss_fct(logits, labels)\n\n#             scale_before_step = scaler.get_scale()\n#             scaler.scale(loss_value).backward()\n#             scaler.unscale_(optimizer)\n#             torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n#             scaler.step(optimizer)\n#             scaler.update()\n\n#             skip_scheduler = scaler.get_scale() != scale_before_step\n#         else:\n#             model_predictions = self.model(**features, return_dict=True)\n#             logits = activation_fct(model_predictions.logits)\n#             if self.config.num_labels == 1:\n#                 logits = logits.view(-1)\n#             loss_value = loss_fct(logits, labels)\n#             loss_value.backward()\n#             torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n#             optimizer.step()\n\n#         optimizer.zero_grad()\n\n#         if not skip_scheduler:\n#             scheduler.step()\n\n#         training_steps += 1\n\n#         if evaluator is not None and evaluation_steps > 0 and training_steps % evaluation_steps == 0:\n#             self._eval_during_training(evaluator, output_path, save_best_model, epoch, training_steps, callback)\n\n#             self.model.zero_grad()\n#             self.model.train()\n\n#     if evaluator is not None:\n#         self._eval_during_training(evaluator, output_path, save_best_model, epoch, -1, callback)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.503405Z","iopub.status.idle":"2021-07-28T15:55:10.504136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### Load model and eval on test set\n# model = CrossEncoder(model_save_path)","metadata":{"execution":{"iopub.status.busy":"2021-07-28T15:55:10.505243Z","iopub.status.idle":"2021-07-28T15:55:10.505881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}