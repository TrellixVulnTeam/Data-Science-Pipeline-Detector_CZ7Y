{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom nltk.tokenize import word_tokenize\nfrom torchtext import vocab\nimport random\nfrom sklearn.model_selection import KFold\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-11T13:18:34.350334Z","iopub.execute_input":"2021-06-11T13:18:34.35071Z","iopub.status.idle":"2021-06-11T13:18:36.960256Z","shell.execute_reply.started":"2021-06-11T13:18:34.350624Z","shell.execute_reply":"2021-06-11T13:18:36.959401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n#     torch.backends.cudnn.deterministic = True\n#     torch.backends.cudnn.benchmark = False\n    print(\"Seeding done\")\nseed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:36.963524Z","iopub.execute_input":"2021-06-11T13:18:36.963782Z","iopub.status.idle":"2021-06-11T13:18:36.973702Z","shell.execute_reply.started":"2021-06-11T13:18:36.963756Z","shell.execute_reply":"2021-06-11T13:18:36.972922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VECTOR_NAME = 'glove.6B.300d.txt'\nVECTOR_PATH = './'\ntrain_dataset_path = '../input/commonlitreadabilityprize/train.csv'\ntest_dataset_path = '../input/commonlitreadabilityprize/test.csv'","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:36.976636Z","iopub.execute_input":"2021-06-11T13:18:36.977206Z","iopub.status.idle":"2021-06-11T13:18:36.981519Z","shell.execute_reply.started":"2021-06-11T13:18:36.977171Z","shell.execute_reply":"2021-06-11T13:18:36.980616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_csv = pd.read_csv(train_dataset_path)\ndata_csv.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:36.98332Z","iopub.execute_input":"2021-06-11T13:18:36.983924Z","iopub.status.idle":"2021-06-11T13:18:37.075939Z","shell.execute_reply.started":"2021-06-11T13:18:36.98387Z","shell.execute_reply":"2021-06-11T13:18:37.075049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data_csv)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:37.077324Z","iopub.execute_input":"2021-06-11T13:18:37.077676Z","iopub.status.idle":"2021-06-11T13:18:37.083524Z","shell.execute_reply.started":"2021-06-11T13:18:37.07764Z","shell.execute_reply":"2021-06-11T13:18:37.082514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEXT_LENGTH = 200\nEMBEDDING_SIZE = 300\nHIDDEN_SIZE = 200\nBATCH_SIZE=64","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:37.085056Z","iopub.execute_input":"2021-06-11T13:18:37.085452Z","iopub.status.idle":"2021-06-11T13:18:37.093021Z","shell.execute_reply.started":"2021-06-11T13:18:37.085413Z","shell.execute_reply":"2021-06-11T13:18:37.092093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_csv['lengths'] = data_csv['excerpt'].apply(lambda x: len(str(x).split()))\ndata_csv['excerpt'] = data_csv['excerpt'].apply(lambda x:str(x).lower())","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:37.096113Z","iopub.execute_input":"2021-06-11T13:18:37.096794Z","iopub.status.idle":"2021-06-11T13:18:37.140449Z","shell.execute_reply.started":"2021-06-11T13:18:37.096636Z","shell.execute_reply":"2021-06-11T13:18:37.139756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_word_to_index(texts):\n    word_to_index = {\n        '<PAD>':0,\n        '<START>':1,\n        '<END>':2,\n    }\n    ind = 3\n    for text in texts:\n        words = word_tokenize(text)\n        for word in words:\n            if word not in word_to_index.keys():\n                word_to_index[word] = ind\n                ind += 1\n                \n    return word_to_index","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:37.143162Z","iopub.execute_input":"2021-06-11T13:18:37.14345Z","iopub.status.idle":"2021-06-11T13:18:37.148791Z","shell.execute_reply.started":"2021-06-11T13:18:37.143419Z","shell.execute_reply":"2021-06-11T13:18:37.148002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_to_index_dict = get_word_to_index(data_csv['excerpt'])\nVOCABULARY_SIZE = len(word_to_index_dict.keys())\nprint(VOCABULARY_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:37.1506Z","iopub.execute_input":"2021-06-11T13:18:37.150965Z","iopub.status.idle":"2021-06-11T13:18:43.260483Z","shell.execute_reply.started":"2021-06-11T13:18:37.150912Z","shell.execute_reply":"2021-06-11T13:18:43.259587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tensor_from_text(text):\n    word_list = []\n    text = text.lower()\n    words = word_tokenize(text)\n    words = [word for word in words if word in word_to_index_dict.keys()]\n    for word in words:\n        word_list.append(word_to_index_dict[word])\n    if len(word_list) > TEXT_LENGTH:\n        word_list = word_list[:200]\n    else:\n        word_list.extend([0]*(TEXT_LENGTH-len(word_list)))\n    \n    tensor_list = torch.tensor(word_list, device=device, dtype=torch.long)\n    return tensor_list","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:43.261769Z","iopub.execute_input":"2021-06-11T13:18:43.2623Z","iopub.status.idle":"2021-06-11T13:18:43.268934Z","shell.execute_reply.started":"2021-06-11T13:18:43.262261Z","shell.execute_reply":"2021-06-11T13:18:43.268019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitDataset(torch.utils.data.Dataset):\n    def __init__(self,dataset):\n        self.dataset = dataset\n    \n    def __getitem__(self,index): \n        text = self.dataset['excerpt'].iloc[index]\n        x = get_tensor_from_text(text)\n        return x\n    \n    def __len__(self):\n        return len(self.dataset)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:43.271601Z","iopub.execute_input":"2021-06-11T13:18:43.271855Z","iopub.status.idle":"2021-06-11T13:18:43.28287Z","shell.execute_reply.started":"2021-06-11T13:18:43.271822Z","shell.execute_reply":"2021-06-11T13:18:43.282043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r  ../input/glove6b/glove.6B.300d.txt ./","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:43.285172Z","iopub.execute_input":"2021-06-11T13:18:43.285424Z","iopub.status.idle":"2021-06-11T13:18:56.16083Z","shell.execute_reply.started":"2021-06-11T13:18:43.2854Z","shell.execute_reply":"2021-06-11T13:18:56.159814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = vocab.Vectors(VECTOR_NAME,VECTOR_PATH)\n\ndef create_embedding_matrix(embeddings):  \n    embedding_matrix = np.random.rand(VOCABULARY_SIZE,EMBEDDING_SIZE)\n    for string,index in word_to_index_dict.items():\n        if not  all(x == 0 for x in embeddings[string].tolist()):\n            embedding_matrix[index] = embeddings[string] \n    return embedding_matrix","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:18:56.162397Z","iopub.execute_input":"2021-06-11T13:18:56.162757Z","iopub.status.idle":"2021-06-11T13:19:46.085559Z","shell.execute_reply.started":"2021-06-11T13:18:56.162715Z","shell.execute_reply":"2021-06-11T13:19:46.084717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitModel(nn.Module):\n    def __init__(self,embedding_matrix,num_layers,batch_size=BATCH_SIZE,hidden_size = HIDDEN_SIZE):\n        super().__init__()\n        self.batch_size = batch_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n                \n        self.embedding_layer = nn.Embedding(VOCABULARY_SIZE,EMBEDDING_SIZE,padding_idx = 0)\n        self.embedding_layer.weight = nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n        self.embedding_layer.weight.requires_grad = False\n        \n        self.lstm_layer = nn.LSTM(EMBEDDING_SIZE,HIDDEN_SIZE,batch_first = True,num_layers = self.num_layers,bidirectional=True)\n        \n        # self.conv1d = nn.Conv1d(in_channels=200,out_channels=400,kernel_size=3,stride=1,padding=0)\n        \n        self.output_layer_1 = nn.Linear(HIDDEN_SIZE,100)\n        self.output_layer_2 = nn.Linear(100,1)\n    \n    def forward(self,input_text,hidden_state,cell_state):\n        self.embeddings = self.embedding_layer(input_text.long().to(device))\n        \n        # Doing spatial dropout 1-dimensional\n        self.embeddings = self.embeddings.permute(0, 2, 1)   # convert to [batch, channels, time]\n        self.embeddings = F.dropout2d(self.embeddings, 0.2, training=self.training)\n        self.embeddings = self.embeddings.permute(0, 2, 1)\n        \n        lstm_output,(hidden_state,cell_state) = self.lstm_layer(self.embeddings)\n#         conv_output = self.conv1d(hidden_state[-1,:,:])\n#         print(conv_output.size())\n        linear_output_1 = self.output_layer_1(hidden_state[-1,:,:])\n        linear_output_2 = self.output_layer_2(linear_output_1)\n        return linear_output_2,hidden_state,cell_state\n        \n    def init_hidden(self):\n        return torch.zeros(self.batch_size,2*self.num_layers,self.hidden_size,dtype = torch.float32,device = device),torch.zeros(self.batch_size,2*self.num_layers,self.hidden_size,dtype = torch.float32,device = device) ","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:19:46.08691Z","iopub.execute_input":"2021-06-11T13:19:46.087273Z","iopub.status.idle":"2021-06-11T13:19:46.104633Z","shell.execute_reply.started":"2021-06-11T13:19:46.087237Z","shell.execute_reply":"2021-06-11T13:19:46.1034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CommonLitAttentionModel(nn.Module):\n    def __init__(self,embedding_matrix,num_layers,batch_size=BATCH_SIZE,hidden_size = HIDDEN_SIZE):\n        super().__init__()\n        self.batch_size = batch_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n                \n        self.embedding_layer = nn.Embedding(VOCABULARY_SIZE,EMBEDDING_SIZE,padding_idx = 0)\n        self.embedding_layer.weight = nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n        self.embedding_layer.weight.requires_grad = False\n        \n        self.lstm_layer = nn.LSTM(EMBEDDING_SIZE,HIDDEN_SIZE,batch_first = True,num_layers = self.num_layers,bidirectional=True)        \n        \n        self.attention_linear_layer = nn.Linear(HIDDEN_SIZE,2*HIDDEN_SIZE)\n        \n        self.lstm_layer_2 = nn.LSTM(2*HIDDEN_SIZE,HIDDEN_SIZE,batch_first=True,num_layers = self.num_layers,bidirectional=True)\n        \n        self.linear_1 = nn.Linear(HIDDEN_SIZE,100)\n        self.linear_2 = nn.Linear(100,1)\n    \n    def forward(self,input_text,hidden_state,cell_state):\n        self.embeddings = self.embedding_layer(input_text.long().to(device))\n        \n        self.embeddings = self.embeddings.permute(0, 2, 1)   # convert to [batch, channels, time]\n        self.embeddings = F.dropout2d(self.embeddings, 0.2, training=self.training)\n        self.embeddings = self.embeddings.permute(0, 2, 1)\n        \n        lstm_output,(hidden_state,cell_state) = self.lstm_layer(self.embeddings)\n        final_state = hidden_state[-1,:,:]\n        \n        attention_linear_output = self.attention_linear_layer(final_state)\n        attention_linear_output = attention_linear_output.unsqueeze(1)\n        attention_multiplied_context = lstm_output * attention_linear_output\n        softmax_attention = F.softmax(attention_multiplied_context,dim=1)\n        global_context = softmax_attention * lstm_output\n        \n        lstm_output_2, (hidden_state_2,cell_state_2) = self.lstm_layer_2(global_context)\n        final_state_2 = hidden_state_2[-1,:,:]        \n        \n        self.linear_output_1 = self.linear_1(final_state_2)\n        final_output_2 = self.linear_2(self.linear_output_1)\n        \n        return final_output_2,hidden_state_2,cell_state_2\n        \n    def init_hidden(self):\n        return torch.zeros(self.batch_size,2*self.num_layers,self.hidden_size,dtype = torch.float32,device = device),torch.zeros(self.batch_size,2*self.num_layers,self.hidden_size,dtype = torch.float32,device = device)  ","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:19:46.106182Z","iopub.execute_input":"2021-06-11T13:19:46.106536Z","iopub.status.idle":"2021-06-11T13:19:46.121671Z","shell.execute_reply.started":"2021-06-11T13:19:46.106483Z","shell.execute_reply":"2021-06-11T13:19:46.120625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reference: General attention mechanism\nclass CommonLitCNNLSTMAttentionEnsembleModel(nn.Module):\n    def __init__(self,embedding_matrix,num_layers,dropout_prob,batch_size=BATCH_SIZE,hidden_size = HIDDEN_SIZE):\n        super().__init__()\n        self.batch_size = batch_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout_prob = dropout_prob\n                \n        self.embedding_layer = nn.Embedding(VOCABULARY_SIZE,EMBEDDING_SIZE,padding_idx = 0)\n        self.embedding_layer.weight = nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n        self.embedding_layer.weight.requires_grad = False\n        \n        self.lstm_layer_1 = nn.LSTM(EMBEDDING_SIZE,hidden_size,dropout=self.dropout_prob,batch_first = True,num_layers = self.num_layers,bidirectional=True)        \n        \n        self.attention_linear_layer = nn.Linear(hidden_size,2*hidden_size)\n        \n        self.lstm_layer_2 = nn.LSTM(4*hidden_size,hidden_size,dropout=self.dropout_prob,batch_first=True,num_layers = self.num_layers,bidirectional=True)\n        \n        ######\n\n        self.lstm_layer_3 = nn.LSTM(EMBEDDING_SIZE,hidden_size,dropout=self.dropout_prob,batch_first = True,num_layers = self.num_layers,bidirectional=True)        \n        self.conv1 = nn.Conv1d(in_channels = 2*hidden_size, out_channels=hidden_size,kernel_size=3,stride=1)\n        self.conv2 = nn.Conv1d(in_channels = 2*hidden_size, out_channels=hidden_size,kernel_size=5,stride=1)\n        self.conv3 = nn.Conv1d(in_channels = 2*hidden_size, out_channels=hidden_size,kernel_size=7,stride=1)\n\n        ######\n\n        self.low_rnn = nn.GRU(hidden_size,hidden_size,dropout=self.dropout_prob,batch_first = True,num_layers = self.num_layers,bidirectional=True)\n        self.med_rnn = nn.GRU(hidden_size,hidden_size,dropout=self.dropout_prob,batch_first = True,num_layers = self.num_layers,bidirectional=True)\n        self.high_rnn = nn.GRU(hidden_size,hidden_size,dropout=self.dropout_prob,batch_first = True,num_layers = self.num_layers,bidirectional=True)\n\n        self.lstm_features_concat_layer = nn.Linear(3*hidden_size,hidden_size)\n\n        ######\n\n        self.output_linear_1 = nn.Linear(2*hidden_size,hidden_size)\n        self.output_linear_2 = nn.Linear(hidden_size,hidden_size // 2)\n        self.output_linear_3 = nn.Linear(hidden_size// 2,1)\n    \n    def forward(self,input_text):\n        \n        self.embeddings = self.embedding_layer(input_text.long().to(device))\n        self.embeddings = self.embeddings.permute(0, 2, 1)   # convert to [batch, channels, time]\n        self.embeddings = F.dropout2d(self.embeddings, 0.2, training=self.training)\n        self.embeddings = self.embeddings.permute(0, 2, 1)\n        \n        ###\n        \n        lstm_output_1,(hidden_state_1,cell_state) = self.lstm_layer_1(self.embeddings)\n        final_state_1 = hidden_state_1[-1,:,:]\n        \n        ###\n        \n        attention_linear_output = self.attention_linear_layer(final_state_1)\n        attention_linear_output = attention_linear_output.unsqueeze(1)\n        attention_multiplied_context = lstm_output_1 * attention_linear_output\n        softmax_attention = F.softmax(attention_multiplied_context,dim=1)\n        global_context = softmax_attention * lstm_output_1\n        \n        final_context_words = torch.cat([global_context,lstm_output_1],dim=2) # 64,sequence, 4*hidden_size\n        \n        lstm_output_2, (hidden_state_2,cell_state_2) = self.lstm_layer_2(final_context_words)\n        final_state_2 = hidden_state_2[-1,:,:]\n        ###\n        \n        lstm_output_3,(hidden_state_3,cell_state_3) = self.lstm_layer_3(self.embeddings)\n        lstm_output_3 = lstm_output_3.permute(0,2,1)\n        conv_1_output = self.conv1(lstm_output_3)\n        conv_1_output = conv_1_output.permute(0,2,1) \n\n        conv_2_output = self.conv2(lstm_output_3)\n        conv_2_output = conv_2_output.permute(0,2,1)\n        \n        conv_3_output = self.conv3(lstm_output_3)\n        conv_3_output = conv_3_output.permute(0,2,1)\n\n        low_lstm_output,hidden_state_low = self.low_rnn(conv_1_output)\n        med_lstm_output,hidden_state_med = self.med_rnn(conv_2_output)\n        high_lstm_output,hidden_state_high = self.high_rnn(conv_3_output)\n        concat_features = torch.cat([hidden_state_low[-1,:,:],hidden_state_med[-1,:,:],hidden_state_high[-1,:,:]],dim=1)\n        lstm_linear_concat_output = self.lstm_features_concat_layer(concat_features)\n\n        ###\n        short_long_context_features = torch.cat([final_state_2,lstm_linear_concat_output],dim=1)\n        linear_output_1 = self.output_linear_1(short_long_context_features)\n        linear_output_2 = self.output_linear_2(linear_output_1)\n        linear_output_3 = self.output_linear_3(linear_output_2)\n\n        return linear_output_3","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:19:46.123048Z","iopub.execute_input":"2021-06-11T13:19:46.123406Z","iopub.status.idle":"2021-06-11T13:19:46.151389Z","shell.execute_reply.started":"2021-06-11T13:19:46.12337Z","shell.execute_reply":"2021-06-11T13:19:46.150617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = create_embedding_matrix(embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:19:46.15278Z","iopub.execute_input":"2021-06-11T13:19:46.153188Z","iopub.status.idle":"2021-06-11T13:19:46.961515Z","shell.execute_reply.started":"2021-06-11T13:19:46.153116Z","shell.execute_reply":"2021-06-11T13:19:46.960489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_csv = pd.read_csv(test_dataset_path)\ncriterion = nn.MSELoss()\ndataset = CommonLitDataset(test_data_csv)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:19:46.962864Z","iopub.execute_input":"2021-06-11T13:19:46.963241Z","iopub.status.idle":"2021-06-11T13:19:46.9858Z","shell.execute_reply.started":"2021-06-11T13:19:46.963204Z","shell.execute_reply":"2021-06-11T13:19:46.98508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(dataset,embedding_matrix,hidden_size=HIDDEN_SIZE,batch_size=BATCH_SIZE):    \n    testloader = torch.utils.data.DataLoader(\n                      dataset,\n                      batch_size=1)\n        \n    with torch.no_grad():   \n        model = torch.load('../input/commonreadability-models/lstm_attention_cnn_gru_attention_1.pkl')\n        model.eval()\n        \n        preds = []\n        print(len(testloader))\n        for inputs in testloader:\n            model = model.to(device)\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            outputs = outputs.squeeze(1)\n            preds.append(outputs.item())\n    \n    data_csv = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n    sub_df = pd.DataFrame({\n        'id':data_csv['id'].values,\n        'target':preds\n    })\n    sub_df = sub_df[['id','target']]\n    sub_df.to_csv('./submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:19:46.987126Z","iopub.execute_input":"2021-06-11T13:19:46.987503Z","iopub.status.idle":"2021-06-11T13:19:46.994648Z","shell.execute_reply.started":"2021-06-11T13:19:46.987468Z","shell.execute_reply":"2021-06-11T13:19:46.993655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test(dataset,embedding_matrix)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:19:46.996128Z","iopub.execute_input":"2021-06-11T13:19:46.996571Z","iopub.status.idle":"2021-06-11T13:19:55.241352Z","shell.execute_reply.started":"2021-06-11T13:19:46.996531Z","shell.execute_reply":"2021-06-11T13:19:55.240447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = pd.read_csv('./submission.csv')\nx","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:19:55.242651Z","iopub.execute_input":"2021-06-11T13:19:55.243006Z","iopub.status.idle":"2021-06-11T13:19:55.255483Z","shell.execute_reply.started":"2021-06-11T13:19:55.242968Z","shell.execute_reply":"2021-06-11T13:19:55.254486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}