{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport os \nimport matplotlib.pyplot as plt\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch \nimport random\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# try to install transformers package.\ntry:\n  from transformers import (\n      AutoModelForSequenceClassification, \n      AutoTokenizer, \n      EvalPrediction,\n      Trainer, \n      TrainingArguments,\n      set_seed\n  )\nexcept ImportError as e:\n    !pip install transformers\n\n    from transformers import (\n      AutoModelForSequenceClassification, \n      AutoTokenizer, \n      EvalPrediction,\n      Trainer, \n      TrainingArguments,\n      set_seed\n    )\n\ntry:\n  import optuna\nexcept ImportError as e:\n    !pip install optuna\n    import optuna \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfor dirname, _, filenames in os.walk('/kaggle/input/commonlitreadabilityprize/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-07T18:55:58.134148Z","iopub.execute_input":"2021-08-07T18:55:58.134517Z","iopub.status.idle":"2021-08-07T18:56:06.083116Z","shell.execute_reply.started":"2021-08-07T18:55:58.134426Z","shell.execute_reply":"2021-08-07T18:56:06.082232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set up for reproducibility\nThe cell below setups the seed of `numpy`, `random`, `torch`, and  `transformers` packages to ensure reproducibility across executions.","metadata":{}},{"cell_type":"code","source":"# path to distilbert pre-trained model.\nPRETRAINED_MODEL  = 'distilbert-base-uncased'\n\n# path to temporary working dir.\nWORK_OUTPUT_DIR = '/kaggle/working/'\n\n# random seed used by packages.\nRANDOM_SEED = 2021\n\n# setup the package seeds.\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\nset_seed(RANDOM_SEED)\nprint('Notebook was set up.')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T18:56:10.061218Z","iopub.execute_input":"2021-08-07T18:56:10.06155Z","iopub.status.idle":"2021-08-07T18:56:10.075147Z","shell.execute_reply.started":"2021-08-07T18:56:10.061513Z","shell.execute_reply":"2021-08-07T18:56:10.073775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and split train dataset\nThe cell below load train dataset provided. Then the this database is split in train and valid datasets.","metadata":{}},{"cell_type":"code","source":"train_dataframe = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv', doublequote=True)\ntrain_dataframe = train_dataframe.rename(columns={'excerpt':'text', 'target':'label'})\ntrain_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T18:56:14.702684Z","iopub.execute_input":"2021-08-07T18:56:14.703023Z","iopub.status.idle":"2021-08-07T18:56:14.824638Z","shell.execute_reply.started":"2021-08-07T18:56:14.702994Z","shell.execute_reply":"2021-08-07T18:56:14.823868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_texts, train_labels = list(train_dataframe['text']), list(train_dataframe['label'])\ntrain_texts, valid_texts, train_labels, valid_labels = train_test_split(train_texts, train_labels, test_size=.25, random_state=RANDOM_SEED)\nprint(\"Train set has {} rows, and valid set has {} rows\".format(len(train_texts), len(valid_texts)))","metadata":{"execution":{"iopub.status.busy":"2021-08-07T18:56:19.262017Z","iopub.execute_input":"2021-08-07T18:56:19.262374Z","iopub.status.idle":"2021-08-07T18:56:19.272238Z","shell.execute_reply.started":"2021-08-07T18:56:19.262342Z","shell.execute_reply":"2021-08-07T18:56:19.271412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"words_counter = train_dataframe['text'].apply(lambda s: len(s.split()))\nsns.histplot(data=words_counter, kde=True)\nprint(\"Max. count words: {}\".format(max(words_counter)))\nprint(\"Min. count words: {}\".format(min(words_counter)))\nprint(\"Median count words: {}\".format(np.median(words_counter)))","metadata":{"execution":{"iopub.status.busy":"2021-08-07T18:58:02.524621Z","iopub.execute_input":"2021-08-07T18:58:02.524944Z","iopub.status.idle":"2021-08-07T18:58:02.736614Z","shell.execute_reply.started":"2021-08-07T18:58:02.524915Z","shell.execute_reply":"2021-08-07T18:58:02.735823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize in the BERT word embeddings\nThe cell below tokenize the excerpt field from train and valid datasets using Distibert tokenizer.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True)\nvalid_encodings = tokenizer(valid_texts, truncation=True, padding=True)\nprint('Train and valid sets tokenized')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T18:58:06.622153Z","iopub.execute_input":"2021-08-07T18:58:06.622493Z","iopub.status.idle":"2021-08-07T18:58:09.285755Z","shell.execute_reply.started":"2021-08-07T18:58:06.622464Z","shell.execute_reply":"2021-08-07T18:58:09.284881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Format train and test datasets in Pytorch format\nThe cell below tokenize the excerpt field from train and valid datasets using Distibert tokenizer.","metadata":{}},{"cell_type":"code","source":"# Create torch dataset\nclass CommonLitDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item['labels'] = torch.tensor(self.labels[idx])\n        \n        return item\n        \n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n               \ntrain_dataset = CommonLitDataset(train_encodings, train_labels)\nvalid_dataset = CommonLitDataset(valid_encodings, valid_labels)\nprint('Torch dataset created.')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T18:58:24.294215Z","iopub.execute_input":"2021-08-07T18:58:24.294569Z","iopub.status.idle":"2021-08-07T18:58:24.302262Z","shell.execute_reply.started":"2021-08-07T18:58:24.294534Z","shell.execute_reply":"2021-08-07T18:58:24.301259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the regression model\nThe cell below train the regression model for predicting the readability.","metadata":{}},{"cell_type":"code","source":"# Define computing metrics Trainer.\nfrom transformers import EarlyStoppingCallback\n\ndef model_init():\n    return AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=1, return_dict=True)\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions \n    preds = np.squeeze(preds) \n    return {\"rmse\": mean_squared_error(y_true=p.label_ids, y_pred=preds, squared=False)}\n\ndef get_objective_metric(metrics):\n    return metrics['eval_rmse']\n\n# Define Trainer parameters\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nbatch_size = 16\ntraining_args = TrainingArguments(\n    # The output directory where the model predictions \n    # and checkpoints will be written.\n    output_dir='/kaggle/working/output',\n    \n    # Overwrite the content of the output directory.\n    overwrite_output_dir=True,\n    \n    # Whether to run training or not.\n    do_train=True,\n    \n    # Whether to run evoluation on the dev or not.\n    do_eval=True,\n    \n    # Batch size GPU/TPU core/CPU for training\n    per_device_train_batch_size=batch_size,\n    \n    # Batch size GPU/TPU core/CPU for evaluation\n    per_device_eval_batch_size=batch_size * 4, \n    \n    # Evaluation strategy to adopt during training.\n    evaluation_strategy=\"steps\",\n    \n    # How often to show logs.\n    logging_steps=100,\n    \n    # Number of update steps between two \n    # evaluations if evaluation_strategy=\"steps\".\n    # Will default to the same value as l\n    # logging_steps if not set.\n    eval_steps=100,\n    \n    # Set prediction loss to 'True' in order to \n    # return loss for perplexity calculation\n    # prediction_loss_only=True,\n    \n    # The initial learning rate for Adam.\n    # Default to 5e-5\n    learning_rate=2.5e-5,\n    \n    # The weigth decay to apply (if not zero)\n    weight_decay=1.5, \n    \n    # Epsilon for Adam optimizer,\n    # Defaults to 1e-8\n    adam_epsilon=3.890824499297403e-10,\n    \n    # Maximum gradient norm (for gradient \n    # clipping). Defaults to 0.\n    max_grad_norm=0,\n    \n    # Total number of training epochs to perform \n    # (if not an integer, will perform the \n    # decimal part percents of\n    # the last epoch before stopping training).\n    num_train_epochs=5,\n\n    # Number of updates steps before two checkpoint saves. \n    # Defaults to 500\n    # save_steps=-1,\n    \n    # Number of steps used for a linear warmup from 0 to learning_rate.\n    warmup_steps=500, \n    \n    # Use in conjunction with load_best_model_at_end to specify \n    # the metric to use to compare two different models.\n    metric_for_best_model=\"eval_rmse\",\n    \n    # Whether or not to load the best model found during training \n    # at the end of training.\n    load_best_model_at_end=True,\n    \n    # Use in conjunction with load_best_model_at_end and metric_for_best_model \n    # to specify if better models should have a greater metric or not. \n    greater_is_better=False,\n    \n    # TensorBoard log directory.\n    logging_dir='/kaggle/working/logs',\n    \n    # Random seed that will be set at the beginning of training.\n    seed=18,\n    \n    # The list of integrations to report the results and logs to.\n    report_to=None\n)\n\nprint('Train arguments set up.')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T19:18:25.197873Z","iopub.execute_input":"2021-08-07T19:18:25.198202Z","iopub.status.idle":"2021-08-07T19:18:25.216955Z","shell.execute_reply.started":"2021-08-07T19:18:25.198172Z","shell.execute_reply":"2021-08-07T19:18:25.214464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Loading `Trainer`...\")\ntrainer = Trainer(\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    compute_metrics=compute_metrics,\n    model_init=model_init,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n    tokenizer=tokenizer\n)\nprint(\"`Trainer` loaded.\")","metadata":{"execution":{"iopub.status.busy":"2021-08-07T19:18:28.626392Z","iopub.execute_input":"2021-08-07T19:18:28.626782Z","iopub.status.idle":"2021-08-07T19:18:34.539493Z","shell.execute_reply.started":"2021-08-07T19:18:28.626749Z","shell.execute_reply":"2021-08-07T19:18:34.538325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train pre-trained model\nif training_args.do_train:\n    train_results = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T19:18:37.824034Z","iopub.execute_input":"2021-08-07T19:18:37.824347Z","iopub.status.idle":"2021-08-07T19:23:10.292087Z","shell.execute_reply.started":"2021-08-07T19:18:37.824316Z","shell.execute_reply":"2021-08-07T19:23:10.290931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the trained model.\nThe cells below the trained model is evaluated using loss, perplexity and RMSE.","metadata":{}},{"cell_type":"code","source":"import math\n# Keep track of train models and evaluate loss.\nloss_history = {'train_loss':[], 'eval_loss':[]}\n\n# Keep track of train and evaluate perplexity\nperplexity_history = {'train_perplexity':[], 'eval_perplexity':[]}\nfor log_history in trainer.state.log_history:\n    if 'loss' in log_history.keys():\n        loss_history['train_loss'].append(log_history['loss'])\n        perplexity_history['train_perplexity'].append(math.exp(log_history['loss']))\n        \n    elif 'eval_loss' in log_history.keys():\n        loss_history['eval_loss'].append(log_history['eval_loss'])\n        perplexity_history['eval_perplexity'].append(math.exp(log_history['eval_loss']))\nprint('Metrics collected.')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T19:24:03.886822Z","iopub.execute_input":"2021-08-07T19:24:03.887242Z","iopub.status.idle":"2021-08-07T19:24:03.895778Z","shell.execute_reply.started":"2021-08-07T19:24:03.887207Z","shell.execute_reply":"2021-08-07T19:24:03.894247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the loss chart","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nsns.lineplot(data=loss_history)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T19:24:15.591246Z","iopub.execute_input":"2021-08-07T19:24:15.591579Z","iopub.status.idle":"2021-08-07T19:24:15.940293Z","shell.execute_reply.started":"2021-08-07T19:24:15.591547Z","shell.execute_reply":"2021-08-07T19:24:15.939197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the perplexity chart","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nsns.lineplot(data=perplexity_history)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T19:24:20.906922Z","iopub.execute_input":"2021-08-07T19:24:20.907244Z","iopub.status.idle":"2021-08-07T19:24:21.255712Z","shell.execute_reply.started":"2021-08-07T19:24:20.907216Z","shell.execute_reply":"2021-08-07T19:24:21.254738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculate the RMSE and Perplexity","metadata":{}},{"cell_type":"code","source":"if training_args.do_eval:    \n    eval_output = trainer.evaluate()\n    perplexity = math.exp(eval_output['eval_loss'])\n    rmse = eval_output['eval_rmse']\n    print('\\nEvaluate Perplexity: {:3,.3f}'.format(perplexity))\n    print('Evaluate RMSE: {:3,.3f}'.format(rmse))\nelse:\n    print('No evaluation needed. No evaluation data provided, `do_eval=False`!')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T19:24:26.317409Z","iopub.execute_input":"2021-08-07T19:24:26.317814Z","iopub.status.idle":"2021-08-07T19:24:29.979573Z","shell.execute_reply.started":"2021-08-07T19:24:26.317778Z","shell.execute_reply":"2021-08-07T19:24:29.978607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the trained model\nEvaluate the trained model using test dataset.","metadata":{}},{"cell_type":"code","source":"# Load test data\ntest_dataframe = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv', doublequote=True)\ntest_texts = test_dataframe.rename(columns={'excerpt':'text'})['text'].to_list()\ntest_encodings  = tokenizer(test_texts, truncation=True, padding=True)\n\n# Create torch dataset\ntest_dataset  = CommonLitDataset(test_encodings)\n\ny_pred=trainer.predict(test_dataset)\n\nprint('Prediction done')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T19:24:35.429032Z","iopub.execute_input":"2021-08-07T19:24:35.429352Z","iopub.status.idle":"2021-08-07T19:24:35.909417Z","shell.execute_reply.started":"2021-08-07T19:24:35.429322Z","shell.execute_reply":"2021-08-07T19:24:35.908125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit to competition","metadata":{}},{"cell_type":"code","source":"# Save test predictions to file\nnp.set_printoptions(precision=1)\noutput = pd.DataFrame({'id': test_dataframe['id'],\n                       'target': y_pred.predictions[:, 0]})\noutput.to_csv('submission.csv', index=False, float_format='%.1f')\nprint(output)\nprint('Test evaluation submitted.')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T19:24:40.394465Z","iopub.execute_input":"2021-08-07T19:24:40.394975Z","iopub.status.idle":"2021-08-07T19:24:40.926769Z","shell.execute_reply.started":"2021-08-07T19:24:40.394929Z","shell.execute_reply":"2021-08-07T19:24:40.925654Z"},"trusted":true},"execution_count":null,"outputs":[]}]}