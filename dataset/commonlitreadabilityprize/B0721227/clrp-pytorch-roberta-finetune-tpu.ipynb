{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:49:07.310951Z","iopub.execute_input":"2021-06-21T15:49:07.311385Z","iopub.status.idle":"2021-06-21T15:49:07.315859Z","shell.execute_reply.started":"2021-06-21T15:49:07.31135Z","shell.execute_reply":"2021-06-21T15:49:07.314785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 20210331 --apt-packages libomp5 libopenblas-dev\n!rm -rf /kaggle/working/*.whl\n!rm -rf /kaggle/working/*.py+\n!pip install accelerate","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-21T15:49:07.317375Z","iopub.execute_input":"2021-06-21T15:49:07.317713Z","iopub.status.idle":"2021-06-21T15:49:57.189183Z","shell.execute_reply.started":"2021-06-21T15:49:07.317682Z","shell.execute_reply":"2021-06-21T15:49:57.187924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ROBERTA moDEL","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModelForSequenceClassification\n)\nfrom transformers import logging\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n_pretrained_model = 'roberta-base'\nlr = 3e-5\nepsilon = 1e-6\nweight_decay = 0.001\nuse_bertadam = False\n\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    _pretrained_model, \n    config=config\n)\n\nno_decay = [\"bias\", \"LayerNorm.weight\"]\noptimizer_grouped_parameters = [{\n    \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n    \"weight_decay\": weight_decay,\n    \"lr\": lr,\n},\n{\n    \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n    \"weight_decay\": 0.0,\n    \"lr\": lr,\n}]\n\noptimizer = AdamW(\n    optimizer_grouped_parameters,\n    lr=lr,\n    eps=epsilon,\n    correct_bias=not use_bertadam # bias correction step\n)\n\ndel model, optimizer_grouped_parameters, optimizer\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:49:57.191834Z","iopub.execute_input":"2021-06-21T15:49:57.192292Z","iopub.status.idle":"2021-06-21T15:49:59.153497Z","shell.execute_reply.started":"2021-06-21T15:49:57.192244Z","shell.execute_reply":"2021-06-21T15:49:59.152408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import RobertaModel, RobertaConfig\nfrom transformers.models.roberta.modeling_roberta import RobertaClassificationHead\n\n_model_type = 'roberta'\n_pretrained_model = 'roberta-base'\nconfig = RobertaConfig.from_pretrained(_pretrained_model)\nadd_pooler = True\nreinit_pooler = True\n\nclass Net(nn.Module):\n    def __init__(self, config, _pretrained_model, add_pooler):\n        super(Net, self).__init__()\n        self.roberta = RobertaModel.from_pretrained(_pretrained_model, add_pooling_layer=add_pooler)\n        self.classifier = RobertaClassificationHead(config)\n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n        sequence_output = outputs[0]\n        logits = self.classifier(sequence_output)\n        return logits\n        \nmodel = Net(config, _pretrained_model, add_pooler)\n\nif reinit_pooler:\n    print('Reinitializing Pooler Layer ...')\n    encoder_temp = getattr(model, _model_type)\n    encoder_temp.pooler.dense.weight.data.normal_(mean=0.0, std=encoder_temp.config.initializer_range)\n    encoder_temp.pooler.dense.bias.data.zero_()\n    for p in encoder_temp.pooler.parameters():\n        p.requires_grad = True\n    print('Done.!')\n    \ndel model\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:49:59.155471Z","iopub.execute_input":"2021-06-21T15:49:59.155912Z","iopub.status.idle":"2021-06-21T15:50:01.056222Z","shell.execute_reply.started":"2021-06-21T15:49:59.155867Z","shell.execute_reply":"2021-06-21T15:50:01.05538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import logging\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\nreinit_layers = 2\n_model_type = 'roberta'\n_pretrained_model = 'roberta-base'\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'num_labels':1})\nmodel = AutoModelForSequenceClassification.from_pretrained(_pretrained_model)\n\nif reinit_layers > 0:\n    print(f'Reinitializing Last {reinit_layers} Layers ...')\n    encoder_temp = getattr(model, _model_type)\n    for layer in encoder_temp.encoder.layer[-reinit_layers:]:\n        for module in layer.modules():\n            if isinstance(module, nn.Linear):\n                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n                if module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.Embedding):\n                module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n                if module.padding_idx is not None:\n                    module.weight.data[module.padding_idx].zero_()\n            elif isinstance(module, nn.LayerNorm):\n                module.bias.data.zero_()\n                module.weight.data.fill_(1.0)\n    print('Done.!')\n\ndel model\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:50:01.057378Z","iopub.execute_input":"2021-06-21T15:50:01.057689Z","iopub.status.idle":"2021-06-21T15:50:03.236061Z","shell.execute_reply.started":"2021-06-21T15:50:01.057662Z","shell.execute_reply":"2021-06-21T15:50:03.23485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import logging\nfrom transformers.models.xlnet.modeling_xlnet import XLNetRelativeAttention\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\nreinit_layers = 2\n_model_type = 'xlnet'\n_pretrained_model = 'xlnet-base-cased'\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'num_labels':1})\nmodel = AutoModelForSequenceClassification.from_pretrained(_pretrained_model)\n\nif reinit_layers > 0:\n    print(f'Reinitializing Last {reinit_layers} Layers ...')\n    for layer in model.transformer.layer[-reinit_layers :]:\n        for module in layer.modules():\n            if isinstance(module, (nn.Linear, nn.Embedding)):\n                module.weight.data.normal_(mean=0.0, std=model.transformer.config.initializer_range)\n                if isinstance(module, nn.Linear) and module.bias is not None:\n                    module.bias.data.zero_()\n            elif isinstance(module, nn.LayerNorm):\n                module.bias.data.zero_()\n                module.weight.data.fill_(1.0)\n            elif isinstance(module, XLNetRelativeAttention):\n                for param in [\n                    module.q,\n                    module.k,\n                    module.v,\n                    module.o,\n                    module.r,\n                    module.r_r_bias,\n                    module.r_s_bias,\n                    module.r_w_bias,\n                    module.seg_embed,\n                ]:\n                    param.data.normal_(mean=0.0, std=model.transformer.config.initializer_range)\n    print('Done.!')\n    \ndel model\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:50:03.238071Z","iopub.execute_input":"2021-06-21T15:50:03.238504Z","iopub.status.idle":"2021-06-21T15:50:05.111311Z","shell.execute_reply.started":"2021-06-21T15:50:03.238462Z","shell.execute_reply":"2021-06-21T15:50:05.110354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import logging\nfrom transformers.models.xlnet.modeling_xlnet import XLNetRelativeAttention\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\nreinit_layers = 2\n_model_type = 'bart'\n_pretrained_model = 'facebook/bart-base'\nconfig = AutoConfig.from_pretrained(_pretrained_model)\nconfig.update({'num_labels':1})\nmodel = AutoModelForSequenceClassification.from_pretrained(_pretrained_model)\n\nif reinit_layers > 0:\n    print(f'Reinitializing Last {reinit_layers} Layers ...')\n    for layer in model.model.decoder.layers[-reinit_layers :]:\n        for module in layer.modules():\n            model.model._init_weights(module)\n    print('Done.!')\n\ndel model\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:50:05.114123Z","iopub.execute_input":"2021-06-21T15:50:05.114555Z","iopub.status.idle":"2021-06-21T15:50:07.413927Z","shell.execute_reply.started":"2021-06-21T15:50:05.114511Z","shell.execute_reply":"2021-06-21T15:50:07.413014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len=256):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        target = torch.tensor(self.targets[idx],dtype=torch.float) \n        return encode, target\n    \n    def __len__(self):\n        return len(self.excerpt)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:50:07.415348Z","iopub.execute_input":"2021-06-21T15:50:07.415634Z","iopub.status.idle":"2021-06-21T15:50:07.450365Z","shell.execute_reply.started":"2021-06-21T15:50:07.415607Z","shell.execute_reply":"2021-06-21T15:50:07.448499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:50:07.45179Z","iopub.status.idle":"2021-06-21T15:50:07.452465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,path):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained(path,output_hidden_states=True)  \n        self.head = AttentionHead(768,768,1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(768,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:50:07.45407Z","iopub.status.idle":"2021-06-21T15:50:07.454711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(fold,verbose=True):\n    \n    def loss_fn(outputs,targets):\n        outputs = outputs.view(-1)\n        targets = targets.view(-1)\n        return torch.sqrt(nn.MSELoss()(outputs,targets))\n    \n    def train_and_evaluate_loop(train_loader,valid_loader,model, loss_fn,optimizer,epoch,fold,best_loss,valid_step=5,lr_scheduler=None):\n        train_loss = 0\n        for i, (inputs1,targets1) in enumerate(train_loader):\n            model.train()\n            optimizer.zero_grad()\n            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n            outputs1 = model(**inputs1)\n            loss1 = loss_fn(outputs1,targets1)\n            accelerator.backward(loss1)\n            optimizer.step()\n            \n            train_loss += loss1.item()\n            \n            if lr_scheduler:\n                lr_scheduler.step()\n            \n            #evaluating for every valid_step\n            if (i % valid_step == 0) or (i == (len(train_loader)-1)):\n                model.eval()\n                valid_loss = 0\n                with torch.no_grad():\n                    for j, (inputs2,targets2) in enumerate(valid_loader):\n                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n                        outputs2 = model(**inputs2)\n                        loss2 = loss_fn(outputs2,targets2)\n                        valid_loss += loss2.item()\n                     \n                    valid_loss /= len(valid_loader)\n                    if valid_loss <= best_loss:\n                        if verbose:                            \n                            xm.master_print(f\"epoch:{epoch} | Train Loss:{train_loss/(i+1)} | Validation loss:{valid_loss}\")\n                            xm.master_print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n\n                        best_loss = valid_loss\n                        xm.save(model.state_dict(),f'./model{fold}/model{fold}.bin')\n                        tokenizer.save_pretrained(f'./model{fold}')\n                        \n        return best_loss\n        \n    accelerator = Accelerator()\n    xm.master_print(f\"{accelerator.device} is used\")\n    x_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\n\n    MODEL_PATH = f'../input/clrp-roberta-base/clrp_roberta_base'\n    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n    model = Model(MODEL_PATH)\n\n    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n    train_dl = DataLoader(train_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=True,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n    valid_dl = DataLoader(valid_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=False,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    optimizer = optim.AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n\n    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= 10 * len(train_dl))\n\n    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n\n    xm.master_print(f\"Fold: {fold}\")\n    best_loss = 9999\n    for epoch in range(config[\"epochs\"]):\n        xm.master_print(f\"Epoch Started:{epoch}\")\n        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,loss_fn,optimizer,epoch,fold,best_loss,\n                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:50:07.456545Z","iopub.status.idle":"2021-06-21T15:50:07.457215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for f in range(config['nfolds']):\n    run(f)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:50:07.458607Z","iopub.status.idle":"2021-06-21T15:50:07.459258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layer_start = 9\npooler = WeightedLayerPooling(\n    config.num_hidden_layers, \n    layer_start=layer_start, layer_weights=None\n)\nfeatures.update({'all_layer_embeddings':outputs[2]})\nfeatures = pooler(features)\nprint(\"Weighted Layer Pooling Embeddings Shape: \", features['token_embeddings'].shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:50:07.460512Z","iopub.status.idle":"2021-06-21T15:50:07.461169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sequence_output = features['token_embeddings'][:, 0]\noutputs = nn.Linear(config.hidden_size, 1)(sequence_output)\nprint(\"Outputs Shape: \", outputs.shape)\n\ndel model, tokenizer\ngc.collect();","metadata":{"execution":{"iopub.status.busy":"2021-06-21T15:50:07.462554Z","iopub.status.idle":"2021-06-21T15:50:07.463221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}