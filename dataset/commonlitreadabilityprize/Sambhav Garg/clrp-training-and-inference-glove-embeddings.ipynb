{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.optimizers as O\nimport tensorflow.keras.losses as Los\n\nfrom sklearn.model_selection import KFold\n\nfrom tqdm import tqdm\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nimport string\n\nstop_words = stopwords.words('english')\nstemmer = nltk.PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:00.586104Z","iopub.execute_input":"2021-06-27T18:47:00.586483Z","iopub.status.idle":"2021-06-27T18:47:06.121299Z","shell.execute_reply.started":"2021-06-27T18:47:00.586402Z","shell.execute_reply":"2021-06-27T18:47:06.120348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.124812Z","iopub.execute_input":"2021-06-27T18:47:06.125087Z","iopub.status.idle":"2021-06-27T18:47:06.243762Z","shell.execute_reply.started":"2021-06-27T18:47:06.125059Z","shell.execute_reply":"2021-06-27T18:47:06.242936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.245423Z","iopub.execute_input":"2021-06-27T18:47:06.245803Z","iopub.status.idle":"2021-06-27T18:47:06.337928Z","shell.execute_reply.started":"2021-06-27T18:47:06.245766Z","shell.execute_reply":"2021-06-27T18:47:06.337074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.339768Z","iopub.execute_input":"2021-06-27T18:47:06.340124Z","iopub.status.idle":"2021-06-27T18:47:06.364811Z","shell.execute_reply.started":"2021-06-27T18:47:06.340089Z","shell.execute_reply":"2021-06-27T18:47:06.364075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of Unique values in the data :')\ntrain_data.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.366145Z","iopub.execute_input":"2021-06-27T18:47:06.366506Z","iopub.status.idle":"2021-06-27T18:47:06.388825Z","shell.execute_reply.started":"2021-06-27T18:47:06.366472Z","shell.execute_reply":"2021-06-27T18:47:06.387902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max = np.max([len(x) for x in train_data.excerpt.values])\nprint(\"Maximum Length of an Excerpt:\", max)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.390168Z","iopub.execute_input":"2021-06-27T18:47:06.390504Z","iopub.status.idle":"2021-06-27T18:47:06.397352Z","shell.execute_reply.started":"2021-06-27T18:47:06.39047Z","shell.execute_reply":"2021-06-27T18:47:06.396276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.398838Z","iopub.execute_input":"2021-06-27T18:47:06.399284Z","iopub.status.idle":"2021-06-27T18:47:06.424192Z","shell.execute_reply.started":"2021-06-27T18:47:06.399247Z","shell.execute_reply":"2021-06-27T18:47:06.423468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.displot(x=train_data.target.values)\nax.set(xlabel='Readability',title='Density plot of Readability')","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.426769Z","iopub.execute_input":"2021-06-27T18:47:06.427083Z","iopub.status.idle":"2021-06-27T18:47:06.749975Z","shell.execute_reply.started":"2021-06-27T18:47:06.427056Z","shell.execute_reply":"2021-06-27T18:47:06.74916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=train_data.target)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.753027Z","iopub.execute_input":"2021-06-27T18:47:06.753338Z","iopub.status.idle":"2021-06-27T18:47:06.847216Z","shell.execute_reply.started":"2021-06-27T18:47:06.753286Z","shell.execute_reply":"2021-06-27T18:47:06.846336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total number of Excerpts: ',len(train_data))\nprint(\"Number of Positive ease of read excerpts: \",len(train_data.target.values[train_data.target.values>=0]))\nprint(\"Number of Negative ease of read excerpts: \",len(train_data.target.values[train_data.target.values<0]))","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.848579Z","iopub.execute_input":"2021-06-27T18:47:06.849026Z","iopub.status.idle":"2021-06-27T18:47:06.856445Z","shell.execute_reply.started":"2021-06-27T18:47:06.848989Z","shell.execute_reply":"2021-06-27T18:47:06.855591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_excerpts = train_data[train_data.target.values>=0]\nnegative_excerpts = train_data[train_data.target.values<0]","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.857912Z","iopub.execute_input":"2021-06-27T18:47:06.858615Z","iopub.status.idle":"2021-06-27T18:47:06.865611Z","shell.execute_reply.started":"2021-06-27T18:47:06.858578Z","shell.execute_reply":"2021-06-27T18:47:06.864705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_wordcloud(data):\n    text = \"\"\n    for i in range(len(data)):\n        text = text + \" \" + data.excerpt.values[i]\n    stopwords = set(STOPWORDS)\n    wordcld = WordCloud(background_color ='white',stopwords=stopwords, min_font_size=10).generate(text)\n    plt.imshow(wordcld)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 0)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.866956Z","iopub.execute_input":"2021-06-27T18:47:06.867513Z","iopub.status.idle":"2021-06-27T18:47:06.87429Z","shell.execute_reply.started":"2021-06-27T18:47:06.867476Z","shell.execute_reply":"2021-06-27T18:47:06.873255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_wordcloud(positive_excerpts)\nplt.title('Positive readablility word cloud')","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:06.875904Z","iopub.execute_input":"2021-06-27T18:47:06.876287Z","iopub.status.idle":"2021-06-27T18:47:07.640397Z","shell.execute_reply.started":"2021-06-27T18:47:06.876252Z","shell.execute_reply":"2021-06-27T18:47:07.639603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_wordcloud(negative_excerpts)\nplt.title('Negative readablility word cloud')","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:07.641691Z","iopub.execute_input":"2021-06-27T18:47:07.642023Z","iopub.status.idle":"2021-06-27T18:47:11.098738Z","shell.execute_reply.started":"2021-06-27T18:47:07.641989Z","shell.execute_reply":"2021-06-27T18:47:11.097949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_wordcloud(train_data)\nplt.title('Whole data Readablility word cloud')","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:11.100045Z","iopub.execute_input":"2021-06-27T18:47:11.100553Z","iopub.status.idle":"2021-06-27T18:47:14.542804Z","shell.execute_reply.started":"2021-06-27T18:47:11.1005Z","shell.execute_reply":"2021-06-27T18:47:14.541971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_len = [len(x) for x in positive_excerpts.excerpt.values]\nnegative_len = [len(x) for x in negative_excerpts.excerpt.values]\nax = sns.displot(data=positive_len,kde=True,color='green')\nax.set(title='Desnity plot Lengths of Positive Readability Excerpts',xlabel='Length')\nax = sns.displot(data=negative_len,kde=True,color='red')\nax.set(title='Desnity plot Lengths of Negative Readability Excerpts',xlabel='Length')","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:14.544159Z","iopub.execute_input":"2021-06-27T18:47:14.544673Z","iopub.status.idle":"2021-06-27T18:47:15.142974Z","shell.execute_reply.started":"2021-06-27T18:47:14.544628Z","shell.execute_reply":"2021-06-27T18:47:15.142019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = L.experimental.preprocessing.TextVectorization(max_tokens=21000,output_sequence_length=205)\ntrain_data_ds = tf.data.Dataset.from_tensor_slices(train_data.excerpt.values)\nvectorizer.adapt(train_data_ds)","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:15.144433Z","iopub.execute_input":"2021-06-27T18:47:15.144802Z","iopub.status.idle":"2021-06-27T18:47:19.401601Z","shell.execute_reply.started":"2021-06-27T18:47:15.144764Z","shell.execute_reply":"2021-06-27T18:47:19.400719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_vec = vectorizer(train_data.excerpt.values).numpy()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:19.402951Z","iopub.execute_input":"2021-06-27T18:47:19.403278Z","iopub.status.idle":"2021-06-27T18:47:19.553224Z","shell.execute_reply.started":"2021-06-27T18:47:19.403243Z","shell.execute_reply":"2021-06-27T18:47:19.552365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_to_glove_file = '../input/glove42b300dtxt/glove.42B.300d.txt'\nembeddings_index = {}\n\nwith open(path_to_glove_file) as f:\n    for line in tqdm(f):\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:47:19.554622Z","iopub.execute_input":"2021-06-27T18:47:19.554971Z","iopub.status.idle":"2021-06-27T18:50:09.301323Z","shell.execute_reply.started":"2021-06-27T18:47:19.554934Z","shell.execute_reply":"2021-06-27T18:50:09.298614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nMAX_LEN = 205\nEPOCHS = 15\nVOCAB_SIZE = len(vectorizer.get_vocabulary())\nEMBEDDING_DIM = 301","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:50:09.302624Z","iopub.execute_input":"2021-06-27T18:50:09.302979Z","iopub.status.idle":"2021-06-27T18:50:09.354823Z","shell.execute_reply.started":"2021-06-27T18:50:09.302941Z","shell.execute_reply":"2021-06-27T18:50:09.35405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_tokens = VOCAB_SIZE + 2\nword_index = dict(zip(vectorizer.get_vocabulary(), range(VOCAB_SIZE)))\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_vector = np.concatenate((embedding_vector,[0]))\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        vector = np.zeros((EMBEDDING_DIM-1))\n        vector = np.concatenate((vector,[5e-1]))\n        embedding_matrix[i] = vector\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:50:09.355985Z","iopub.execute_input":"2021-06-27T18:50:09.356325Z","iopub.status.idle":"2021-06-27T18:50:09.580784Z","shell.execute_reply.started":"2021-06-27T18:50:09.35629Z","shell.execute_reply":"2021-06-27T18:50:09.579891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    inp = L.Input(shape=(MAX_LEN,))\n    emb = L.Embedding(input_dim=num_tokens,output_dim = EMBEDDING_DIM,embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)(inp)\n    X = L.Conv1D(16,3)(emb)\n    X = L.BatchNormalization()(X)\n    X = L.Activation('relu')(X)\n    X = L.MaxPooling1D()(X)\n    X = L.Conv1D(32,3)(emb)\n    X = L.BatchNormalization()(X)\n    X = L.Activation('relu')(X)\n    X = L.MaxPooling1D()(X)\n    X = L.Conv1D(64,3)(emb)\n    X = L.BatchNormalization()(X)\n    X = L.Activation('relu')(X)\n    X = L.MaxPooling1D()(X)\n    X = L.Dropout(0.3)(X)\n    X = L.Bidirectional(L.LSTM(32,recurrent_initializer='glorot_uniform'))(X)\n    X = L.Dense(64,activation='relu')(X)\n    X = L.Dense(32,activation='relu')(X)\n    out = L.Dense(1,kernel_initializer='glorot_uniform')(X)\n    \n    rms = tf.keras.metrics.RootMeanSquaredError()\n    model = M.Model(inputs=inp,outputs=out)\n    model.compile(loss='mse',optimizer='adam',metrics=[rms])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:50:09.582195Z","iopub.execute_input":"2021-06-27T18:50:09.582708Z","iopub.status.idle":"2021-06-27T18:50:09.592694Z","shell.execute_reply.started":"2021-06-27T18:50:09.58267Z","shell.execute_reply":"2021-06-27T18:50:09.591871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:50:09.597128Z","iopub.execute_input":"2021-06-27T18:50:09.597438Z","iopub.status.idle":"2021-06-27T18:50:10.152335Z","shell.execute_reply.started":"2021-06-27T18:50:09.597412Z","shell.execute_reply":"2021-06-27T18:50:10.151567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5,random_state=24,shuffle=True)\n\nfor index,(t_idx,v_idx) in enumerate(kf.split(train_data_vec)):\n    print(f\"\\n ######## STEP {index+1} ######## \\n\")\n    train_x = train_data_vec[t_idx]\n    val_x = train_data_vec[v_idx]\n    train_y = train_data.target.values[t_idx]\n    val_y = train_data.target.values[v_idx]\n    \n    history = model.fit(train_x,\n                        train_y,\n                        validation_data=(val_x,val_y),\n                        epochs=EPOCHS,\n                        batch_size=BATCH_SIZE)\n    \n    pred = model.predict(train_data_vec)\n    plt.scatter(pred,train_data.target.values)\n    plt.xlabel('Predicted')\n    plt.ylabel('Real')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-27T18:50:10.153813Z","iopub.execute_input":"2021-06-27T18:50:10.154153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_vec = vectorizer(test_data.excerpt.values).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(test_data_vec,verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampl = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampl.target = pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampl.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}