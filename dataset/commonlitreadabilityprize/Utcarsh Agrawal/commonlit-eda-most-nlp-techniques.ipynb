{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 align=\"center\">ðŸ“šCommonLit Readability PrizeðŸ“š </h1>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"# Introduction\nIn this notebook I will try to cover most of the NLP techniques which you can apply which will help you in solving any NLP problem. But before that, I will first show you some explorations techniques which will be useful to analyze the data and then we will proceed further. <br>\n\n<h3>So, Let's Get Started!!!</h3><br>\n\nðŸŽ¯ <b>Objective:</b> The objective of this competition is to rate the complexity level of literary passages for grades 3-12 use.\n\nðŸ“ˆ <b>Dataset:</b> The dataset contains excerpts from several time periods and a wide range of reading ease scores.\n\n<b>Columns of the train/test data-</b> \n\n* ```id``` - unique ID for excerpt\n* ```url_legal``` - URL of source (blank in the test set)\n* ```license``` - license of source material (blank in the test set)\n* ```excerpt``` - text to predict reading ease of\n* ```target``` - reading ease\n* ```standard_error``` - measure of spread of scores among multiple raters for each excerpt (not included for test data)","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n    <h2>ðŸ™‚ Please do an upvote if you find it useful ! ðŸ™‚ </h2>\n</div>","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n<ul style=\"list-style-type:square\">\n    <li><a href=\"#1\">Importing Libraries</a></li>\n    <li><a href=\"#2\">Reading the data</a></li>\n    <li><a href=\"#3\">Exploratory Data Analysis</a></li>\n    <li><a href=\"#4\">Data Preprocessing</a></li>\n    <li><a href=\"#5\">ML Models (Baseline)</a></li>\n    <ul>\n        <li><a href=\"#5.1\">Linear Regression</a></li>\n        <li><a href=\"#5.2\">Ridge Regression</a></li>\n        <li><a href=\"#5.3\">Support Vector Regression</a></li>\n        <li><a href=\"#5.4\">Random Forest Regressor</a></li>\n        <li><a href=\"#5.5\">Gradient Boosting Regressor</a></li>\n        <li><a href=\"#5.6\">AdaBoost Regressor</a></li>\n        <li><a href=\"#5.7\">XGBoost Regressor</a></li>\n    </ul>\n    <li><a href=\"#6\">DL Models (Baseline)</a></li>\n    <ul>\n        <li><a href=\"#6.1\">Simple RNN</a></li>\n        <li><a href=\"#6.2\">LSTM</a></li>\n        <li><a href=\"#6.3\">Bidirectional RNN</a></li>\n        <li><a href=\"#6.4\">BERT</a></li>\n    </ul>\n    <li><a href=\"#7\">Ending Notes</a></li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px\nplt.style.use('seaborn-darkgrid')\nfrom textblob import TextBlob\nfrom PIL import Image\nimport requests\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, Dropout, SimpleRNN, Bidirectional\nfrom keras.optimizers import Adam\n\nimport warnings\nwarnings.simplefilter('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-07T05:40:13.832723Z","iopub.execute_input":"2021-06-07T05:40:13.83311Z","iopub.status.idle":"2021-06-07T05:40:24.036843Z","shell.execute_reply.started":"2021-06-07T05:40:13.833029Z","shell.execute_reply":"2021-06-07T05:40:24.03595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n# Reading the data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:40:24.03842Z","iopub.execute_input":"2021-06-07T05:40:24.038781Z","iopub.status.idle":"2021-06-07T05:40:24.118709Z","shell.execute_reply.started":"2021-06-07T05:40:24.038739Z","shell.execute_reply":"2021-06-07T05:40:24.11787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:40:24.120774Z","iopub.execute_input":"2021-06-07T05:40:24.121175Z","iopub.status.idle":"2021-06-07T05:40:24.151523Z","shell.execute_reply.started":"2021-06-07T05:40:24.121134Z","shell.execute_reply":"2021-06-07T05:40:24.15065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:40:24.153294Z","iopub.execute_input":"2021-06-07T05:40:24.153718Z","iopub.status.idle":"2021-06-07T05:40:24.172879Z","shell.execute_reply.started":"2021-06-07T05:40:24.153654Z","shell.execute_reply":"2021-06-07T05:40:24.171893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='3'></a>\n# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### **First let us look the distribution of target.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nsns.distplot(df['target'])\nplt.title('Target Distribution')\nplt.show()\ndf['target'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:40:24.174386Z","iopub.execute_input":"2021-06-07T05:40:24.174793Z","iopub.status.idle":"2021-06-07T05:40:24.672465Z","shell.execute_reply.started":"2021-06-07T05:40:24.174751Z","shell.execute_reply":"2021-06-07T05:40:24.671595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This shows that our data is normally distributed with mean=-0.959319 and standard deviation=1.033579.\nThe target ranges from -3.676268 to 1.711390 where target=-3.67 is the most difficult text and target=1.71 is the easiest.<br> **Let us also look the distribution of standard error.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nsns.distplot(df['standard_error'])\nplt.title('Standard Error Distribution')\nplt.show()\ndf['standard_error'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:40:24.673971Z","iopub.execute_input":"2021-06-07T05:40:24.674366Z","iopub.status.idle":"2021-06-07T05:40:24.958285Z","shell.execute_reply.started":"2021-06-07T05:40:24.674326Z","shell.execute_reply":"2021-06-07T05:40:24.957214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The standard_error basically tells us the measure of spread of scores among the raters for each excerpt, that means each excerpt has been read by many different people and accordingly they have given their score and standard_error measures the difference. It means that lesser the standard_error, more precise the target value.  From the plot we can observe that it is very sqewed to the left. But we also have one outlier and where standard_error=0. This excerpt is considered as the reference except and all other excerpt are compared with this excerpt. ","metadata":{}},{"cell_type":"code","source":"df[df['standard_error']== 0]","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:40:24.959946Z","iopub.execute_input":"2021-06-07T05:40:24.960389Z","iopub.status.idle":"2021-06-07T05:40:24.976228Z","shell.execute_reply.started":"2021-06-07T05:40:24.960347Z","shell.execute_reply":"2021-06-07T05:40:24.974958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Let us also look at the relationship between target and standard_error.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nsns.scatterplot(x='target', y='standard_error', data=df)\nplt.title('Standard Error vs Target')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:40:24.980601Z","iopub.execute_input":"2021-06-07T05:40:24.980983Z","iopub.status.idle":"2021-06-07T05:40:25.169594Z","shell.execute_reply.started":"2021-06-07T05:40:24.980948Z","shell.execute_reply":"2021-06-07T05:40:25.1685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although we can observe there is no linear relationship between target and standard_error. Still we can say that when the target value is very high or very low (i.e. excerpt is either very easy or very difficult) then the standard_error is high that means most of the raters disagreed.","metadata":{}},{"cell_type":"markdown","source":"### **Now let us get some insighsts from the \"excerpt\".**","metadata":{}},{"cell_type":"markdown","source":"First of all we will observe the most common words in the excerpt.\nFor that we will first clean the data and will also remove the stopwords. We will store this in a new column and will then count the number of words using Counter. In the end we will plot 25 most common words. ","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    text = text.lower().split()\n    return [word for word in text if word not in stopwords.words('english')]\n\ndf['temp'] = df['excerpt'].apply(lambda x : clean_text(x))\n\ntop = Counter([word for words in df['temp'] for word in words])\ndf_temp = pd.DataFrame(top.most_common(25))\ndf_temp.columns = ['Common_words','count']\n\nfig = px.bar(df_temp, x='count', y='Common_words', title='Most Common Words in excerpt', orientation='h', width=700,height=700, color='Common_words')\nfig.show()\n\nfig = px.treemap(df_temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:40:25.171862Z","iopub.execute_input":"2021-06-07T05:40:25.172233Z","iopub.status.idle":"2021-06-07T05:41:18.01596Z","shell.execute_reply.started":"2021-06-07T05:40:25.172193Z","shell.execute_reply":"2021-06-07T05:41:18.014792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WordCloud","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\ntext = df['excerpt'].values\nurl = 'https://static.vecteezy.com/system/resources/previews/000/263/280/non_2x/vector-open-book.jpg'\nim = np.array(Image.open(requests.get(url, stream=True).raw))\ncloud = WordCloud(stopwords = STOPWORDS,\n                  background_color='white',\n                  mask = im,\n                  max_words = 200,\n                  ).generate(\" \".join(text))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:41:18.017682Z","iopub.execute_input":"2021-06-07T05:41:18.01807Z","iopub.status.idle":"2021-06-07T05:41:20.930862Z","shell.execute_reply.started":"2021-06-07T05:41:18.018028Z","shell.execute_reply":"2021-06-07T05:41:20.930018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we can also plot the distribution of top part-of-speech tags of excerpt corpus. Part-Of-Speech Tagging (POS) is a process of assigning parts of speech to each word, such as noun, verb, adjective, etc.\nFor this we will use TextBlog to dive into POS of our \"excerpt\" data.","metadata":{}},{"cell_type":"code","source":"text = ' '.join(df['excerpt'])\nblob = TextBlob(text)\ntop = Counter([pos[1] for pos in blob.tags])\ndf_temp = pd.DataFrame(top.most_common(15))\ndf_temp.columns = ['Part_of_Speech','count']\nfig = px.bar(df_temp, x='Part_of_Speech', y='count', title='Top 15 Part-Of-Speech tagging', width=700,height=700, color='Part_of_Speech')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:41:20.932045Z","iopub.execute_input":"2021-06-07T05:41:20.932383Z","iopub.status.idle":"2021-06-07T05:42:00.28935Z","shell.execute_reply.started":"2021-06-07T05:41:20.932345Z","shell.execute_reply":"2021-06-07T05:42:00.28841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **After that, now we will explore the data on the basis of complexity of the text.**","metadata":{}},{"cell_type":"markdown","source":"### **Number of words in each passage**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\ntext_len = df[df['target'] <= 0]['excerpt'].str.split().map(lambda x: len(x))\nsns.distplot(text_len, ax=ax[0], color='red')\nax[0].set_title('High Complexity')\n\ntext_len = df[df['target'] > 0]['excerpt'].str.split().map(lambda x: len(x))\nsns.distplot(text_len, ax=ax[1], color='blue')\nax[1].set_title('Low Complexity')\n\nfig.suptitle('Number of Words in text')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:00.290632Z","iopub.execute_input":"2021-06-07T05:42:00.291134Z","iopub.status.idle":"2021-06-07T05:42:00.702521Z","shell.execute_reply.started":"2021-06-07T05:42:00.291095Z","shell.execute_reply":"2021-06-07T05:42:00.701519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Average word length in each passage**","metadata":{}},{"cell_type":"code","source":"def avg_word_len(text):\n    avg_len = text.str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n    return avg_len\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\navg_len = avg_word_len(df[df['target'] <= 0]['excerpt'])\nsns.distplot(avg_len, ax=ax[0], color='red')\nax[0].set_title('High Complexity')\n\navg_len = avg_word_len(df[df['target'] > 0]['excerpt'])\nsns.distplot(avg_len, ax=ax[1], color='blue')\nax[1].set_title('Low Complexity')\n\nfig.suptitle('Average word length in a text')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:00.703981Z","iopub.execute_input":"2021-06-07T05:42:00.704324Z","iopub.status.idle":"2021-06-07T05:42:01.325645Z","shell.execute_reply.started":"2021-06-07T05:42:00.704286Z","shell.execute_reply":"2021-06-07T05:42:01.324632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Number of Sentences in text**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\nno_sents = df[df['target'] <= 0]['excerpt'].apply(lambda x : len(x.split('\\n')))\nsns.distplot(no_sents, ax=ax[0], color='red')\nax[0].set_title('High Complexity')\n\nno_sents = df[df['target'] > 0]['excerpt'].apply(lambda x : len(x.split('\\n')))\nsns.distplot(no_sents, ax=ax[1], color='blue')\nax[1].set_title('Low Complexity')\n\nfig.suptitle('Number of Sentences in text')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:01.327011Z","iopub.execute_input":"2021-06-07T05:42:01.327351Z","iopub.status.idle":"2021-06-07T05:42:01.78695Z","shell.execute_reply.started":"2021-06-07T05:42:01.327312Z","shell.execute_reply":"2021-06-07T05:42:01.785928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Now let us also compare these with the 'target'.**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 1, figsize=(10,15))\n\ndf['text_len'] = df['excerpt'].str.split().map(lambda x: len(x))\nsns.scatterplot(x='text_len', y='target', data=df, ax=ax[0])\nax[0].set_title(\"Word Count vs Target\", fontweight =\"bold\")\n\navg_len = avg_word_len(df['excerpt'])\ndf['avg_word_len'] = avg_len\nsns.scatterplot(x='avg_word_len', y='target', data=df, color='red', ax=ax[1])\nax[1].set_title(\"Average Word Length vs Target\", fontweight =\"bold\")\n\ndf['no_sents'] = df['excerpt'].apply(lambda x : len(x.split('\\n')))\nsns.scatterplot(x='no_sents', y='target', data=df, color='orange', ax=ax[2])\nax[2].set_title(\"Sentence Count vs Target\", fontweight =\"bold\")\n\nplt.subplots_adjust(hspace=0.35)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:01.78845Z","iopub.execute_input":"2021-06-07T05:42:01.789003Z","iopub.status.idle":"2021-06-07T05:42:02.552322Z","shell.execute_reply.started":"2021-06-07T05:42:01.788957Z","shell.execute_reply":"2021-06-07T05:42:02.551498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"corr = df.corr()\nfig = plt.figure(figsize=(10,10))\nsns.heatmap(corr, cmap=\"YlGnBu\", center=0, square=True, linewidths=.5, annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:02.553722Z","iopub.execute_input":"2021-06-07T05:42:02.554086Z","iopub.status.idle":"2021-06-07T05:42:02.870342Z","shell.execute_reply.started":"2021-06-07T05:42:02.554048Z","shell.execute_reply":"2021-06-07T05:42:02.869454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='4'></a>\n# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"wnl = WordNetLemmatizer()\ndef clean_text(text):\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    text = text.lower().split()\n    text = [wnl.lemmatize(word) for word in text if word not in stopwords.words('english')]\n    text = \" \".join(text)\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:02.871621Z","iopub.execute_input":"2021-06-07T05:42:02.871983Z","iopub.status.idle":"2021-06-07T05:42:02.878361Z","shell.execute_reply.started":"2021-06-07T05:42:02.871946Z","shell.execute_reply":"2021-06-07T05:42:02.877386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['clean_text'] = df['excerpt'].apply(lambda x : clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:02.879831Z","iopub.execute_input":"2021-06-07T05:42:02.880218Z","iopub.status.idle":"2021-06-07T05:42:58.376624Z","shell.execute_reply.started":"2021-06-07T05:42:02.880175Z","shell.execute_reply":"2021-06-07T05:42:58.375764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df['clean_text'].values\ny = df['target'].values","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:58.377848Z","iopub.execute_input":"2021-06-07T05:42:58.378196Z","iopub.status.idle":"2021-06-07T05:42:58.383649Z","shell.execute_reply.started":"2021-06-07T05:42:58.37816Z","shell.execute_reply":"2021-06-07T05:42:58.381423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:58.385357Z","iopub.execute_input":"2021-06-07T05:42:58.385985Z","iopub.status.idle":"2021-06-07T05:42:58.395903Z","shell.execute_reply.started":"2021-06-07T05:42:58.385944Z","shell.execute_reply":"2021-06-07T05:42:58.394916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After all the explorations and preprocessing now it's time to create some models. Firstly, we will build some different baseline machine learning models. I will be just creating some basic models so that you get to know how can use machine learning for nlp tasks. By using more features and some hyperparameter tuning, you can achieve better results also.","metadata":{}},{"cell_type":"markdown","source":"<a id='5'></a>\n# ML Models (Baseline)","metadata":{}},{"cell_type":"markdown","source":"Before applying different kinds of ml algorithms, we first have to convert our string data into some numerical form(or vectorial form). So we can convert the text data into vector form through many ways. Here I will be using TF-IDF vectorizer.","metadata":{}},{"cell_type":"code","source":"tfidf = TfidfVectorizer(binary=True)\nvect = tfidf.fit(X_train)\nX_train = vect.transform(X_train)\nX_val = vect.transform(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:58.399338Z","iopub.execute_input":"2021-06-07T05:42:58.399644Z","iopub.status.idle":"2021-06-07T05:42:58.915577Z","shell.execute_reply.started":"2021-06-07T05:42:58.399601Z","shell.execute_reply":"2021-06-07T05:42:58.914731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mse_plot = {} # For plotting purpose","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:58.91686Z","iopub.execute_input":"2021-06-07T05:42:58.917199Z","iopub.status.idle":"2021-06-07T05:42:58.921495Z","shell.execute_reply.started":"2021-06-07T05:42:58.917163Z","shell.execute_reply":"2021-06-07T05:42:58.920529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.1'></a>\n## Linear Regression","metadata":{}},{"cell_type":"code","source":"model_lr = LinearRegression().fit(X_train, y_train)\ny_pred = model_lr.predict(X_val)\nmse = mean_squared_error(y_val, y_pred)\nmse_plot['LinearRegression'] = mse\nprint(f\"Model Name: Linear Regression ====>>> MSE:{mse}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:58.927223Z","iopub.execute_input":"2021-06-07T05:42:58.927625Z","iopub.status.idle":"2021-06-07T05:42:59.020498Z","shell.execute_reply.started":"2021-06-07T05:42:58.927593Z","shell.execute_reply":"2021-06-07T05:42:59.019556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.2'></a>\n## Ridge Regression","metadata":{}},{"cell_type":"code","source":"model_rr = Ridge().fit(X_train, y_train)\ny_pred = model_rr.predict(X_val)\nmse = mean_squared_error(y_val, y_pred)\nmse_plot['RidgeRegression'] = mse\nprint(f\"Model Name: Ridge Regression ====>>> MSE:{mse}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:59.022716Z","iopub.execute_input":"2021-06-07T05:42:59.023243Z","iopub.status.idle":"2021-06-07T05:42:59.054952Z","shell.execute_reply.started":"2021-06-07T05:42:59.023204Z","shell.execute_reply":"2021-06-07T05:42:59.053806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.3'></a>\n## Support Vector Regression","metadata":{}},{"cell_type":"code","source":"model_svr = SVR().fit(X_train, y_train)\ny_pred = model_svr.predict(X_val)\nmse = mean_squared_error(y_val, y_pred)\nmse_plot['SVR'] = mse\nprint(f\"Model Name: Support Vector Regression ====>>> MSE:{mse}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:42:59.056302Z","iopub.execute_input":"2021-06-07T05:42:59.056878Z","iopub.status.idle":"2021-06-07T05:43:03.465883Z","shell.execute_reply.started":"2021-06-07T05:42:59.056835Z","shell.execute_reply":"2021-06-07T05:43:03.464817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.4'></a>\n## Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"model_rf = RandomForestRegressor().fit(X_train, y_train)\ny_pred = model_rf.predict(X_val)\nmse = mean_squared_error(y_val, y_pred)\nmse_plot['RandomForest'] = mse\nprint(f\"Model Name: Random Forest Regressor ====>>> MSE:{mse}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:43:03.467321Z","iopub.execute_input":"2021-06-07T05:43:03.467732Z","iopub.status.idle":"2021-06-07T05:44:52.553997Z","shell.execute_reply.started":"2021-06-07T05:43:03.467689Z","shell.execute_reply":"2021-06-07T05:44:52.552053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.5'></a>\n## Gradient Boosting Regressor","metadata":{}},{"cell_type":"code","source":"model_gbr = GradientBoostingRegressor().fit(X_train, y_train)\ny_pred = model_gbr.predict(X_val)\nmse = mean_squared_error(y_val, y_pred)\nmse_plot['GradientBoosting'] = mse\nprint(f\"Model Name: Gradient Boosting Regressor ====>>> MSE:{mse}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:44:52.555605Z","iopub.execute_input":"2021-06-07T05:44:52.556034Z","iopub.status.idle":"2021-06-07T05:44:56.833871Z","shell.execute_reply.started":"2021-06-07T05:44:52.555991Z","shell.execute_reply":"2021-06-07T05:44:56.832872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.6'></a>\n## AdaBoost Regressor","metadata":{}},{"cell_type":"code","source":"model_abr = AdaBoostRegressor().fit(X_train, y_train)\ny_pred = model_abr.predict(X_val)\nmse = mean_squared_error(y_val, y_pred)\nmse_plot['AdaBoost'] = mse\nprint(f\"Model Name: AdaBoost Regressor ====>>> MSE:{mse}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:44:56.835318Z","iopub.execute_input":"2021-06-07T05:44:56.835972Z","iopub.status.idle":"2021-06-07T05:44:58.939763Z","shell.execute_reply.started":"2021-06-07T05:44:56.835924Z","shell.execute_reply":"2021-06-07T05:44:58.938761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.7'></a>\n## XGBoost Regressor","metadata":{}},{"cell_type":"code","source":"model_xgb = XGBRegressor().fit(X_train, y_train)\ny_pred = model_xgb.predict(X_val)\nmse = mean_squared_error(y_val, y_pred)\nmse_plot['XGBoost'] = mse\nprint(f\"Model Name: XGBoost Regressor ====>>> MSE:{mse}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:44:58.941131Z","iopub.execute_input":"2021-06-07T05:44:58.941682Z","iopub.status.idle":"2021-06-07T05:45:06.402119Z","shell.execute_reply.started":"2021-06-07T05:44:58.941626Z","shell.execute_reply":"2021-06-07T05:45:06.401168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.barplot(list(mse_plot.keys()), list(mse_plot.values()))\nplt.xlabel(\"ML Models\")\nplt.ylabel(\"Mean_Squared_Error\")\nplt.title(\"Comparison Graph\", fontsize=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:45:06.405399Z","iopub.execute_input":"2021-06-07T05:45:06.405707Z","iopub.status.idle":"2021-06-07T05:45:06.595071Z","shell.execute_reply.started":"2021-06-07T05:45:06.405676Z","shell.execute_reply":"2021-06-07T05:45:06.594145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='6'></a>\n# DL Models (Baseline)\nNext, let us also talk about deep learning models. <br>\nWe will be using the same data which we got after applying data preprcoessing steps. But before creating the models we first have to process the data differently.<br>\nJust as we converted our text data into vector form previously, here also we will convert our text data but using different technique.\nFirstly, we will convert our data into one hot representation and for that we will be using keras Tokenizer. Then we will do padding i.e., we will make all the sentence length to be equal.","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=None)\ntokenizer.fit_on_texts(X)\nsequences = tokenizer.texts_to_sequences(X)\nword_index = tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:45:06.59644Z","iopub.execute_input":"2021-06-07T05:45:06.596865Z","iopub.status.idle":"2021-06-07T05:45:06.94835Z","shell.execute_reply.started":"2021-06-07T05:45:06.596822Z","shell.execute_reply":"2021-06-07T05:45:06.947481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 256\npadded = pad_sequences(sequences, maxlen=max_len, padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:45:06.949635Z","iopub.execute_input":"2021-06-07T05:45:06.950153Z","iopub.status.idle":"2021-06-07T05:45:07.033718Z","shell.execute_reply.started":"2021-06-07T05:45:06.950107Z","shell.execute_reply":"2021-06-07T05:45:07.032816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(padded, y, test_size=0.2, random_state=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:45:07.034881Z","iopub.execute_input":"2021-06-07T05:45:07.03522Z","iopub.status.idle":"2021-06-07T05:45:07.044369Z","shell.execute_reply.started":"2021-06-07T05:45:07.035186Z","shell.execute_reply":"2021-06-07T05:45:07.043457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='6.1'></a>\n## Simple RNN","metadata":{}},{"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(Embedding(len(word_index)+1, 250, input_length=max_len))\nmodel1.add(SimpleRNN(100, return_sequences=True))\nmodel1.add(SimpleRNN(100))\nmodel1.add(Dense(100, activation='linear'))\nmodel1.add(Dense(1, activation='linear'))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:45:07.045581Z","iopub.execute_input":"2021-06-07T05:45:07.04612Z","iopub.status.idle":"2021-06-07T05:45:09.65805Z","shell.execute_reply.started":"2021-06-07T05:45:07.046078Z","shell.execute_reply":"2021-06-07T05:45:09.657127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:45:09.659249Z","iopub.execute_input":"2021-06-07T05:45:09.659587Z","iopub.status.idle":"2021-06-07T05:45:09.669346Z","shell.execute_reply.started":"2021-06-07T05:45:09.659543Z","shell.execute_reply":"2021-06-07T05:45:09.668394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.compile(optimizer='Adam', loss='mean_squared_error', metrics=['mse'])","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:45:09.670853Z","iopub.execute_input":"2021-06-07T05:45:09.671531Z","iopub.status.idle":"2021-06-07T05:45:09.690109Z","shell.execute_reply.started":"2021-06-07T05:45:09.671471Z","shell.execute_reply":"2021-06-07T05:45:09.689164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:45:09.691743Z","iopub.execute_input":"2021-06-07T05:45:09.692176Z","iopub.status.idle":"2021-06-07T05:47:44.698841Z","shell.execute_reply.started":"2021-06-07T05:45:09.692068Z","shell.execute_reply":"2021-06-07T05:47:44.698021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='6.2'></a>\n## LSTM","metadata":{}},{"cell_type":"code","source":"model2 = Sequential()\nmodel2.add(Embedding(len(word_index)+1, 250, input_length=max_len))\nmodel2.add(LSTM(100, return_sequences = True))\nmodel2.add(LSTM(100))\nmodel2.add(Dense(100, activation='linear'))\nmodel2.add(Dense(1, activation='linear'))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:47:44.700209Z","iopub.execute_input":"2021-06-07T05:47:44.700557Z","iopub.status.idle":"2021-06-07T05:47:45.184224Z","shell.execute_reply.started":"2021-06-07T05:47:44.700519Z","shell.execute_reply":"2021-06-07T05:47:45.183283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:47:45.185533Z","iopub.execute_input":"2021-06-07T05:47:45.185926Z","iopub.status.idle":"2021-06-07T05:47:45.197648Z","shell.execute_reply.started":"2021-06-07T05:47:45.185888Z","shell.execute_reply":"2021-06-07T05:47:45.194611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.compile(optimizer='Adam', loss='mean_squared_error', metrics=['mse'])\n\nmodel2.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:47:45.199162Z","iopub.execute_input":"2021-06-07T05:47:45.19976Z","iopub.status.idle":"2021-06-07T05:48:18.796988Z","shell.execute_reply.started":"2021-06-07T05:47:45.199717Z","shell.execute_reply":"2021-06-07T05:48:18.795967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='6.3'></a>\n## Bidirectional RNN","metadata":{}},{"cell_type":"code","source":"model3 = Sequential()\nmodel3.add(Embedding(len(word_index)+1, 250, input_length = max_len))\nmodel3.add(Bidirectional(LSTM(100, return_sequences = True)))\nmodel3.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.3)))\nmodel3.add(Dense(100, activation='linear'))\nmodel3.add(Dense(1, activation='linear'))","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:48:18.798514Z","iopub.execute_input":"2021-06-07T05:48:18.798874Z","iopub.status.idle":"2021-06-07T05:48:19.504879Z","shell.execute_reply.started":"2021-06-07T05:48:18.798837Z","shell.execute_reply":"2021-06-07T05:48:19.504005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:48:19.506172Z","iopub.execute_input":"2021-06-07T05:48:19.506529Z","iopub.status.idle":"2021-06-07T05:48:19.517544Z","shell.execute_reply.started":"2021-06-07T05:48:19.506485Z","shell.execute_reply":"2021-06-07T05:48:19.516607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3.compile(optimizer='Adam', loss='mean_squared_error', metrics=['mse'])\n\nmodel3.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=64, epochs=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:48:19.519012Z","iopub.execute_input":"2021-06-07T05:48:19.5194Z","iopub.status.idle":"2021-06-07T05:54:51.074085Z","shell.execute_reply.started":"2021-06-07T05:48:19.519358Z","shell.execute_reply":"2021-06-07T05:54:51.073248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='6.4'></a>\n# BERT\nBidirectional Encoder Representations from Transformers (BERT) is a Transformer-based machine learning technique developed by Google. Before we see implementation of BERT, if you are a beginner and had never used BERT, I would recommend you to go through the resources which I have listed below so that you know the basics and then implement BERT with a better understanding.\n\n### Sequence To Sequence Models\n* https://www.youtube.com/watch?v=jCrgzJlxTKg&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm&index=26&ab_channel=KrishNaik\n\n### Attention Models\n* https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a\n* https://www.youtube.com/watch?v=fdhojC37_Co&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm&index=28&ab_channel=KrishNaik\n\n### Transformers\n* http://jalammar.github.io/illustrated-transformer/\n* https://www.youtube.com/watch?v=SMZQrJ_L1vo&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm&index=29&ab_channel=KrishNaik\n\n### BERT\n* http://jalammar.github.io/illustrated-bert/\n* https://www.youtube.com/watch?v=xI0HHN5XKDo&ab_channel=CodeEmporium\n\nAfter going through all the resources, you will have a sound understanding of all the topics. So now its time to implement BERT. We will fine-tune our model for our task using TF/Keras.\n\n**Reference :- [TF/Keras BERT Baseline (Training/Inference)](https://www.kaggle.com/jeongyoonlee/tf-keras-bert-baseline-training-inference/notebook)**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras import Model, Input, backend as K\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom transformers import TFBertModel, BertConfig, BertTokenizerFast\nfrom tensorflow.keras.callbacks import LearningRateScheduler","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:54:51.098164Z","iopub.execute_input":"2021-06-07T05:54:51.098427Z","iopub.status.idle":"2021-06-07T05:54:51.479252Z","shell.execute_reply.started":"2021-06-07T05:54:51.098401Z","shell.execute_reply":"2021-06-07T05:54:51.478433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenization using transformers","metadata":{}},{"cell_type":"code","source":"pretrained_dir = '../input/tfbert-base-uncased'","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:54:51.480541Z","iopub.execute_input":"2021-06-07T05:54:51.481049Z","iopub.status.idle":"2021-06-07T05:54:51.485183Z","shell.execute_reply.started":"2021-06-07T05:54:51.48101Z","shell.execute_reply":"2021-06-07T05:54:51.484118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained(pretrained_dir)\n\nmodel_config = BertConfig.from_pretrained(pretrained_dir)\nmodel_config.output_hidden_states = True\n\nbert_model = TFBertModel.from_pretrained(pretrained_dir, config=model_config)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:54:51.486497Z","iopub.execute_input":"2021-06-07T05:54:51.487214Z","iopub.status.idle":"2021-06-07T05:54:58.846302Z","shell.execute_reply.started":"2021-06-07T05:54:51.487149Z","shell.execute_reply":"2021-06-07T05:54:58.845242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_encode(texts, tokenizer, max_len=max_len):\n    input_ids = []\n    token_type_ids = []\n    attention_mask = []\n    \n    for text in texts:\n        token = tokenizer(text, max_length=max_len, truncation=True, padding='max_length',\n                         add_special_tokens=True)\n        input_ids.append(token['input_ids'])\n        token_type_ids.append(token['token_type_ids'])\n        attention_mask.append(token['attention_mask'])\n    \n    return np.array(input_ids), np.array(token_type_ids), np.array(attention_mask)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:54:58.847732Z","iopub.execute_input":"2021-06-07T05:54:58.848096Z","iopub.status.idle":"2021-06-07T05:54:58.864398Z","shell.execute_reply.started":"2021-06-07T05:54:58.848063Z","shell.execute_reply":"2021-06-07T05:54:58.86354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = bert_encode(X, tokenizer, max_len=max_len)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:54:58.866084Z","iopub.execute_input":"2021-06-07T05:54:58.866506Z","iopub.status.idle":"2021-06-07T05:55:00.645125Z","shell.execute_reply.started":"2021-06-07T05:54:58.866466Z","shell.execute_reply":"2021-06-07T05:55:00.644262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training ","metadata":{}},{"cell_type":"code","source":"def build_model(bert_model, max_len):\n    input_ids = Input(shape=(max_len, ), dtype=tf.int32, name='input_ids')\n    attention_mask = Input(shape=(max_len, ), dtype=tf.int32, name='attention_masks')\n    token_type_ids = Input(shape=(max_len, ), dtype=tf.int32, name='token_type_ids')\n    \n    sequence_output = bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n    output = sequence_output[:, 0, :]\n    output = Dropout(0.2)(output)\n    output = Dense(1, activation='linear')(output)\n    \n    model = Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=output)\n    model.compile(Adam(lr=1e-5), loss='mean_squared_error', metrics=[RootMeanSquaredError()])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:55:00.646445Z","iopub.execute_input":"2021-06-07T05:55:00.646814Z","iopub.status.idle":"2021-06-07T05:55:00.655087Z","shell.execute_reply.started":"2021-06-07T05:55:00.646778Z","shell.execute_reply":"2021-06-07T05:55:00.654051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model4 = build_model(bert_model, max_len=max_len)\nmodel4.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:55:00.65659Z","iopub.execute_input":"2021-06-07T05:55:00.657023Z","iopub.status.idle":"2021-06-07T05:55:13.514813Z","shell.execute_reply.started":"2021-06-07T05:55:00.656978Z","shell.execute_reply":"2021-06-07T05:55:13.513856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def scheduler(epoch, lr, warmup=5, decay_start=10):\n    if epoch <= warmup:\n        return lr / (warmup - epoch + 1)\n    elif warmup < epoch <= decay_start:\n        return lr\n    else:\n        return lr * tf.math.exp(-.1)\n\nls = LearningRateScheduler(scheduler, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:55:13.517743Z","iopub.execute_input":"2021-06-07T05:55:13.518015Z","iopub.status.idle":"2021-06-07T05:55:13.522544Z","shell.execute_reply.started":"2021-06-07T05:55:13.517987Z","shell.execute_reply":"2021-06-07T05:55:13.521697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model4.fit( X, y, validation_split=0.2, epochs=5, batch_size=8, callbacks=[ls])","metadata":{"execution":{"iopub.status.busy":"2021-06-07T05:55:13.523549Z","iopub.execute_input":"2021-06-07T05:55:13.523893Z","iopub.status.idle":"2021-06-07T06:02:35.451448Z","shell.execute_reply.started":"2021-06-07T05:55:13.523856Z","shell.execute_reply":"2021-06-07T06:02:35.450597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics1 = pd.DataFrame(model1.history.history)\nmetrics2 = pd.DataFrame(model2.history.history)\nmetrics3 = pd.DataFrame(model3.history.history)\nmetrics4 = pd.DataFrame(model4.history.history)","metadata":{"execution":{"iopub.status.busy":"2021-06-07T06:02:35.454406Z","iopub.execute_input":"2021-06-07T06:02:35.454705Z","iopub.status.idle":"2021-06-07T06:02:35.465116Z","shell.execute_reply.started":"2021-06-07T06:02:35.454674Z","shell.execute_reply":"2021-06-07T06:02:35.464209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(9, 6))\nmetrics1['val_loss'].plot(label='SimpleRNN', marker='o')\nmetrics2['val_loss'].plot(label='LSTM', marker='o')\nmetrics3['val_loss'].plot(label='Bidirectional RNN', marker='o')\nmetrics4['val_loss'].plot(label='BERT', marker='o')\nplt.xlabel(\"Epochs\", fontsize=12)\nplt.ylabel(\"Model Val_Loss\", fontsize=12)\nplt.title(\"Model Val_Loss vs Epochs\", fontsize=16)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-07T06:08:59.805654Z","iopub.execute_input":"2021-06-07T06:08:59.806013Z","iopub.status.idle":"2021-06-07T06:09:00.055651Z","shell.execute_reply.started":"2021-06-07T06:08:59.805982Z","shell.execute_reply":"2021-06-07T06:09:00.054684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='7'></a>\n# Ending Notes\nI have tried to cover most of the techniques but still there are some other methods too which I have not covered in this notebook. If you want to understand Word2Vec and Glove which I have not covered here : - [Click Here](https://github.com/Printutcarsh/Complete-NLP-notebook-Part-2-Word2Vec-and-Glove) <br>\nSo, I hope this notebook will help you in this competition and in other NLP tasks as well. <br>\n<div class=\"alert alert-block alert-info\">\n    <h2 align=\"center\">Please do an upvote if you find it useful !</h2>\n</div>","metadata":{}}]}