{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CommonLit Readability Basic EDA and RoBerta-base\n","metadata":{}},{"cell_type":"markdown","source":"## Import required modules","metadata":{}},{"cell_type":"code","source":"import re\nimport time\nimport nltk\nimport random\nimport warnings\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import optimizers, losses, metrics, Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom transformers import TFAutoModelForSequenceClassification, TFAutoModel, AutoTokenizer\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \nseed = 0\nseed_everything(seed)\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:32:25.460401Z","iopub.execute_input":"2021-05-23T04:32:25.460729Z","iopub.status.idle":"2021-05-23T04:32:33.452841Z","shell.execute_reply.started":"2021-05-23T04:32:25.4607Z","shell.execute_reply":"2021-05-23T04:32:33.452031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## load dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:32:40.710675Z","iopub.execute_input":"2021-05-23T04:32:40.711059Z","iopub.status.idle":"2021-05-23T04:32:40.823543Z","shell.execute_reply.started":"2021-05-23T04:32:40.711027Z","shell.execute_reply":"2021-05-23T04:32:40.822735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:32:42.299607Z","iopub.execute_input":"2021-05-23T04:32:42.29994Z","iopub.status.idle":"2021-05-23T04:32:42.32466Z","shell.execute_reply.started":"2021-05-23T04:32:42.299912Z","shell.execute_reply":"2021-05-23T04:32:42.323666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:32:44.169685Z","iopub.execute_input":"2021-05-23T04:32:44.170002Z","iopub.status.idle":"2021-05-23T04:32:44.180949Z","shell.execute_reply.started":"2021-05-23T04:32:44.169971Z","shell.execute_reply":"2021-05-23T04:32:44.179994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:32:45.709654Z","iopub.execute_input":"2021-05-23T04:32:45.710014Z","iopub.status.idle":"2021-05-23T04:32:45.715912Z","shell.execute_reply.started":"2021-05-23T04:32:45.709981Z","shell.execute_reply":"2021-05-23T04:32:45.714499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:32:47.075678Z","iopub.execute_input":"2021-05-23T04:32:47.078235Z","iopub.status.idle":"2021-05-23T04:32:47.092038Z","shell.execute_reply.started":"2021-05-23T04:32:47.078194Z","shell.execute_reply":"2021-05-23T04:32:47.091076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We don't have any missing values in the columns of our interest, i.e., excerpt, target and standard_error!","metadata":{}},{"cell_type":"markdown","source":"## Target : \nOur target variable starts at -3.67, the highest possible difficulty and stops at 1.71, which is the lowest difficulty to read.","metadata":{}},{"cell_type":"code","source":"print('Min Target Value = ', train['target'].min())\nprint('\\nText : ', train[train['target'] == train['target'].min()]['excerpt'][1705])\n\nprint('\\n\\nMax Target Value = ', train['target'].max())\nprint('\\nText : ', train[train['target'] == train['target'].max()]['excerpt'][2829])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:32:54.730298Z","iopub.execute_input":"2021-05-23T04:32:54.730628Z","iopub.status.idle":"2021-05-23T04:32:54.755526Z","shell.execute_reply.started":"2021-05-23T04:32:54.730598Z","shell.execute_reply":"2021-05-23T04:32:54.754757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(train['target'])\nplt.title('Target Distribution', size=15)\nplt.xlabel('Value')\nplt.ylabel('Frequency')","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:32:58.869611Z","iopub.execute_input":"2021-05-23T04:32:58.869941Z","iopub.status.idle":"2021-05-23T04:32:59.108146Z","shell.execute_reply.started":"2021-05-23T04:32:58.869909Z","shell.execute_reply":"2021-05-23T04:32:59.107201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's see standard error","metadata":{}},{"cell_type":"code","source":"print('Min Standard Error : ', train['standard_error'].min())\nprint('Target Value : ', train[train['standard_error'] == train['standard_error'].min()]['target'][106])\n\nprint('\\nText : ',train[train['standard_error'] == train['standard_error'].min()]['excerpt'][106])\n\nprint('\\n\\nMax Standard Error : ', train['standard_error'].max())\nprint('Target Value : ', train[train['standard_error'] == train['standard_error'].max()]['target'][2235])\n\nprint('\\nText : ',train[train['standard_error'] == train['standard_error'].max()]['excerpt'][2235])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:33:03.199795Z","iopub.execute_input":"2021-05-23T04:33:03.200154Z","iopub.status.idle":"2021-05-23T04:33:03.215382Z","shell.execute_reply.started":"2021-05-23T04:33:03.200122Z","shell.execute_reply":"2021-05-23T04:33:03.21417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(x=train['standard_error'], color='red')\nplt.title('Standard Error Distribution', size=15)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:33:09.100373Z","iopub.execute_input":"2021-05-23T04:33:09.100792Z","iopub.status.idle":"2021-05-23T04:33:09.399155Z","shell.execute_reply.started":"2021-05-23T04:33:09.100753Z","shell.execute_reply":"2021-05-23T04:33:09.398294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can standard error has outliers","metadata":{}},{"cell_type":"markdown","source":"## Target vs Standard error","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nsns.scatterplot(x=train['target'], y=train['standard_error'], color='black', size=train['standard_error'])\nplt.title('Target vs Standard Error', size=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:33:13.869757Z","iopub.execute_input":"2021-05-23T04:33:13.870132Z","iopub.status.idle":"2021-05-23T04:33:14.251428Z","shell.execute_reply.started":"2021-05-23T04:33:13.870078Z","shell.execute_reply":"2021-05-23T04:33:14.250509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see only one outlier present.","metadata":{}},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"code","source":"def clean_data(data):\n    cleaned_excerpt = []\n    for text in data['excerpt']:\n        text = re.sub('[^a-zA-Z]', ' ', text)\n        text = text.lower()\n        text = nltk.word_tokenize(text)\n        \n        text = [word for word in text if word not in stopwords.words('english')]\n        \n        lemma = nltk.WordNetLemmatizer()\n        text = [lemma.lemmatize(word) for word in text]\n        text = ' '.join(text)\n        \n        cleaned_excerpt.append(text)\n    return cleaned_excerpt","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:33:26.080043Z","iopub.execute_input":"2021-05-23T04:33:26.080403Z","iopub.status.idle":"2021-05-23T04:33:26.086113Z","shell.execute_reply.started":"2021-05-23T04:33:26.080371Z","shell.execute_reply":"2021-05-23T04:33:26.085164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['cleaned_excerpt'] = clean_data(train)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:33:31.694608Z","iopub.execute_input":"2021-05-23T04:33:31.694938Z","iopub.status.idle":"2021-05-23T04:34:28.457118Z","shell.execute_reply.started":"2021-05-23T04:33:31.694907Z","shell.execute_reply":"2021-05-23T04:34:28.456255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:34:31.169452Z","iopub.execute_input":"2021-05-23T04:34:31.169773Z","iopub.status.idle":"2021-05-23T04:34:31.182558Z","shell.execute_reply.started":"2021-05-23T04:34:31.169743Z","shell.execute_reply":"2021-05-23T04:34:31.181559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's plot top unigrams, bigrams and trigrams","metadata":{}},{"cell_type":"code","source":"def top_n_ngrams(corpus, n_gram=(1, 1), n=None):\n    vec = CountVectorizer(ngram_range = n_gram).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n   \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    \n    return words_freq[:n]\n\nunigrams = top_n_ngrams(train['cleaned_excerpt'], n_gram = (1, 1), n=20)\nbigrams = top_n_ngrams(train['cleaned_excerpt'], n_gram = (2, 2), n=20)\ntrigrams = top_n_ngrams(train['cleaned_excerpt'], n_gram = (3, 3), n=20)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:34:35.061024Z","iopub.execute_input":"2021-05-23T04:34:35.061366Z","iopub.status.idle":"2021-05-23T04:34:39.019255Z","shell.execute_reply.started":"2021-05-23T04:34:35.061334Z","shell.execute_reply":"2021-05-23T04:34:39.018358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataframe(data):\n    word = []\n    freq = []\n    for d in data:\n        word.append(d[0])\n        freq.append(d[1])\n    return pd.DataFrame({'word': word, 'freq': freq})\n\nuni_df = create_dataframe(unigrams)\nbi_df = create_dataframe(bigrams)\ntri_df = create_dataframe(trigrams)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:34:44.279607Z","iopub.execute_input":"2021-05-23T04:34:44.279921Z","iopub.status.idle":"2021-05-23T04:34:44.286671Z","shell.execute_reply.started":"2021-05-23T04:34:44.279892Z","shell.execute_reply":"2021-05-23T04:34:44.28587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 20))\nsns.barplot(x='freq', y='word', color='#00e6b8', data=uni_df, ax= ax1)\nsns.barplot(x='freq', y='word', color='#ff5050', data=bi_df, ax= ax2)\nsns.barplot(x='freq', y='word', color='#e600e6', data=tri_df, ax= ax3)\n\nax1.set_title('Top 20 Uni-grams', size=12)\nax2.set_title('Top 20 Bi-grams', size=12)\nax3.set_title('Top 20 Tri-grams', size=12)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:34:49.439832Z","iopub.execute_input":"2021-05-23T04:34:49.440175Z","iopub.status.idle":"2021-05-23T04:34:50.503336Z","shell.execute_reply.started":"2021-05-23T04:34:49.440143Z","shell.execute_reply":"2021-05-23T04:34:50.502537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## WordCloud","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nwc = WordCloud(stopwords=STOPWORDS,background_color=\"white\", contour_width=2, contour_color='blue',\n               width=1500, height=750,max_words=150, max_font_size=256,random_state=42)\n\nwc.generate(' '.join(train['cleaned_excerpt']))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:34:56.270032Z","iopub.execute_input":"2021-05-23T04:34:56.270376Z","iopub.status.idle":"2021-05-23T04:35:00.7169Z","shell.execute_reply.started":"2021-05-23T04:34:56.270344Z","shell.execute_reply":"2021-05-23T04:35:00.715937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hardware configuration for TPU","metadata":{}},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f\"Running on TPU {tpu.master()}\")\nexcept ValueError:\n    tpu = None\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS : {REPLICAS}')","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:35:07.259827Z","iopub.execute_input":"2021-05-23T04:35:07.260163Z","iopub.status.idle":"2021-05-23T04:35:07.272132Z","shell.execute_reply.started":"2021-05-23T04:35:07.26013Z","shell.execute_reply":"2021-05-23T04:35:07.270991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Parameters","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 8 * REPLICAS\nLEARNING_RATE = 1e-5 * REPLICAS\nEPOCHS = 35\nES_PATIENCE = 7\nPATIENCE = 2\nN_FOLDS = 5\nSEQ_LEN = 256\nBASE_MODEL = '/kaggle/input/huggingface-roberta/roberta-base/'","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:35:13.495477Z","iopub.execute_input":"2021-05-23T04:35:13.495808Z","iopub.status.idle":"2021-05-23T04:35:13.500229Z","shell.execute_reply.started":"2021-05-23T04:35:13.495766Z","shell.execute_reply":"2021-05-23T04:35:13.499326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Auxiliary functions","metadata":{}},{"cell_type":"code","source":"def custom_standardization(text):\n    text = text.lower()\n    text = text.strip()\n    return text\n\ndef sample_target(features, target):\n    mean, stddev = target\n    sampled_target = tf.random.normal([], mean=tf.cast(mean, dtype=tf.float32), stddev=tf.cast(stddev, dtype=tf.float32), dtype=tf.float32)\n    return (features, sampled_target)\n\ndef get_dataset(df, tokenizer, labeled=True, ordered=False, repeated=False, is_sampled=False, batch_size=32, seq_len=128):\n    \"\"\"\n        Return a Tensorflow dataset ready for training or inference\n    \"\"\"\n    text = [custom_standardization(text) for text in df['excerpt']]\n    \n    tokenized_inputs = tokenizer(text, max_length=seq_len, truncation=True, padding='max_length', return_tensors='tf')\n    \n    if labeled:\n        dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs['input_ids'],\n                                                      'attention_mask' : tokenized_inputs['attention_mask']},\n                                                     (df['target'], df['standard_error'])))\n        if is_sampled:\n            dataset = dataset.map(sample_target, num_parallel_calls = tf.data.AUTOTUNE)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices({'input_ids': tokenized_inputs['input_ids'],\n                                                         'attention_mask': tokenized_inputs['attention_mask']})\n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        \n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:35:29.060367Z","iopub.execute_input":"2021-05-23T04:35:29.060684Z","iopub.status.idle":"2021-05-23T04:35:29.070395Z","shell.execute_reply.started":"2021-05-23T04:35:29.060654Z","shell.execute_reply":"2021-05-23T04:35:29.069457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"def model_fn(encoder, seq_len=256):\n    input_ids = L.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n    input_attention_mask = L.Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    outputs = encoder({'input_ids': input_ids,\n                      'attention_mask': input_attention_mask})\n    \n    model = Model(inputs=[input_ids, input_attention_mask], outputs=outputs)\n    \n    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(optimizer=optimizer,\n                 loss = losses.MeanSquaredError(),\n                 metrics=[metrics.RootMeanSquaredError()])\n    return model\nwith strategy.scope():\n    encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n    model = model_fn(encoder, SEQ_LEN)\n    \nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:35:34.469718Z","iopub.execute_input":"2021-05-23T04:35:34.470038Z","iopub.status.idle":"2021-05-23T04:35:54.210388Z","shell.execute_reply.started":"2021-05-23T04:35:34.470008Z","shell.execute_reply":"2021-05-23T04:35:54.207162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\nskf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\noof_pred = []\noof_labels = []\nhistory_list = []\ntest_pred = []\n\nfor fold, (idxT, idxV) in enumerate(skf.split(train)):\n    if tpu:\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n    print(f'\\nFOLD: {fold+1}')\n    print(f'Train: {len(idxT)} Valid: {len(idxV)}')\n    \n    K.clear_session()\n    with strategy.scope():\n        encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n        model = model_fn(encoder, SEQ_LEN)\n        \n    model_path = f'model_{fold}.h5'\n    es = EarlyStopping(monitor='val_root_mean_squared_error',\n                       mode='min', patience=ES_PATIENCE,\n                       restore_best_weights=True, verbose=1)\n    checkpoint = ModelCheckpoint(model_path,\n                                 monitor='val_root_mean_squared_error',\n                                 mode='min', save_best_only=True,\n                                 save_weights_only=True)\n    \n    history = model.fit(x=get_dataset(train.loc[idxT],\n                                     tokenizer, repeated=True, is_sampled=True,\n                                     batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                       validation_data=get_dataset(train.loc[idxV], tokenizer,\n                                                  ordered=True, batch_size=BATCH_SIZE,\n                                                  seq_len=SEQ_LEN),\n                       steps_per_epoch=50,\n                       callbacks=[es, checkpoint],\n                       epochs=EPOCHS,\n                       verbose=2).history\n    history_list.append(history)\n    model.load_weights(model_path)\n    \n    print(f\"#### Fold {fold+1} OOF RMSE = {np.min(history['val_root_mean_squared_error']):.4f}\")\n    \n    valid_ds = get_dataset(train.loc[idxV], tokenizer, ordered=True,\n                          batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    oof_labels.append([target[0].numpy() for sample, target in iter(valid_ds.unbatch())])\n    x_oof = valid_ds.map(lambda sample, target: sample)\n    \n    oof_pred.append(model.predict(x_oof)['logits'])\n    \n    test_ds = get_dataset(test, tokenizer, labeled=False, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    x_test = test_ds.map(lambda sample: sample)\n    test_pred.append(model.predict(x_test)['logits'])","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:36:08.655074Z","iopub.execute_input":"2021-05-23T04:36:08.655436Z","iopub.status.idle":"2021-05-23T05:26:41.755522Z","shell.execute_reply.started":"2021-05-23T04:36:08.655407Z","shell.execute_reply":"2021-05-23T05:26:41.754614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model loss and metrics graph","metadata":{}},{"cell_type":"code","source":"def plot_metrics(history):\n    metric_list = list(history.keys())\n    size = len(metric_list) // 2\n    fig, axes = plt.subplots(size, 1, sharex='col', figsize=(20, size*5))\n    axes = axes.flatten()\n        \n    for index in range(len(metric_list)//2):\n        metric_name = metric_list[index]\n        val_metric_name = metric_list[index+size]\n        axes[index].plot(history[metric_name], label='Train %s ' % metric_name)\n        axes[index].plot(history[val_metric_name], label='Validation %s' % metric_name)\n        axes[index].legend(loc='best', fontsize=16)\n        axes[index].set_title(metric_name)\n            \n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()\n    \nfor fold, history in enumerate(history_list):\n    print(f'Fold : {fold+1}')\n    plot_metrics(history)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T05:36:26.102667Z","iopub.execute_input":"2021-05-23T05:36:26.103062Z","iopub.status.idle":"2021-05-23T05:36:27.51203Z","shell.execute_reply.started":"2021-05-23T05:36:26.103027Z","shell.execute_reply":"2021-05-23T05:36:27.511192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model evaluation\n> We are evaluating the model on the OOF predictions, it stands for Out Of Fold, since we are training using K-Fold our model will see all the data, and the correct way to evaluate each fold is by looking at the predictions that are not from that fold.","metadata":{}},{"cell_type":"markdown","source":"## OOF Metrics","metadata":{}},{"cell_type":"code","source":"y_true = np.concatenate(oof_labels)\ny_preds = np.concatenate(oof_pred)\n\nfor fold, history in enumerate(history_list):\n    print(f\"Fold {fold+1} RMSE : {np.min(history['val_root_mean_squared_error']):.4f}\")\n    \nprint(f\"OOF RMSE: {mse(y_true, y_preds, squared=False):.4f}\")","metadata":{"execution":{"iopub.status.busy":"2021-05-23T05:36:40.37204Z","iopub.execute_input":"2021-05-23T05:36:40.372395Z","iopub.status.idle":"2021-05-23T05:36:40.382983Z","shell.execute_reply.started":"2021-05-23T05:36:40.372357Z","shell.execute_reply":"2021-05-23T05:36:40.379107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Error analysis, label x prediction distribution\nHere we can compare the distribution from the labels and the predicted values, in a perfect scenario they should align.","metadata":{}},{"cell_type":"code","source":"preds_df = pd.DataFrame({'Label': y_true, 'Prediction': y_preds[:, 0]})\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(preds_df['Label'], ax=ax, label='Label')\nsns.distplot(preds_df['Prediction'], ax=ax, label='Prediction')\nax.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T05:36:47.902747Z","iopub.execute_input":"2021-05-23T05:36:47.903079Z","iopub.status.idle":"2021-05-23T05:36:48.209766Z","shell.execute_reply.started":"2021-05-23T05:36:47.903047Z","shell.execute_reply":"2021-05-23T05:36:48.208808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.jointplot(data=preds_df, x='Label', y='Prediction', kind='reg', height=10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T05:36:57.60151Z","iopub.execute_input":"2021-05-23T05:36:57.601844Z","iopub.status.idle":"2021-05-23T05:36:58.98094Z","shell.execute_reply.started":"2021-05-23T05:36:57.601804Z","shell.execute_reply":"2021-05-23T05:36:58.980074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Submission","metadata":{}},{"cell_type":"code","source":"submission = test[['id']]\nsubmission['target'] = np.mean(test_pred, axis=0)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-05-23T05:37:07.790845Z","iopub.execute_input":"2021-05-23T05:37:07.791175Z","iopub.status.idle":"2021-05-23T05:37:07.802382Z","shell.execute_reply.started":"2021-05-23T05:37:07.791143Z","shell.execute_reply":"2021-05-23T05:37:07.80136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T05:37:16.401959Z","iopub.execute_input":"2021-05-23T05:37:16.402319Z","iopub.status.idle":"2021-05-23T05:37:16.678722Z","shell.execute_reply.started":"2021-05-23T05:37:16.402285Z","shell.execute_reply":"2021-05-23T05:37:16.677926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reference - https://www.kaggle.com/dimitreoliveira/commonlit-readability-eda-roberta-tf-baseline","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}