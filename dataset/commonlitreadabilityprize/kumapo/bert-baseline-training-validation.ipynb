{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# BERT baseline[training + validation]\n\nit's a kernel that trains `BertForSequenceClassification` from huggingface's transformers.\n\ni'm appreciated if you enjoy and upvote it.\nif helps you somewhat, i will post an inference kernel.","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport torch\nimport numpy as np\nimport pandas as pd\n    \ndef seed_everything(seed: int = 42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport transformers\n\nfrom typing import Tuple, Dict, Union\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(\n        self, \n        df: pd.DataFrame,\n        tokenizer: transformers.tokenization_utils.PreTrainedTokenizer,\n        max_sequence_length: int\n    ):\n        super().__init__()\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_sequence_length = max_sequence_length\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, i: int) -> Union[Tuple, Dict]:\n        text = self.df.iloc[i]['excerpt']\n        target = self.df.iloc[i].get('target', None)\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_sequence_length,\n            truncation=True,\n            padding='max_length'\n        )\n        input_ids_and_mask = dict(\n            input_ids=torch.tensor(inputs['input_ids'], dtype=torch.long),\n            attention_mask=torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            token_type_ids=torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n        )\n        if target is None:\n            return input_ids_and_mask\n\n        return input_ids_and_mask, torch.tensor(target, dtype=torch.float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.metrics\nfrom typing import Dict\n\ndef validate(\n    model: transformers.PreTrainedModel,\n    tokenizer: transformers.PreTrainedTokenizer,\n    valid_df: pd.DataFrame,\n    max_sequence_length: int,\n    batch_size: int\n) -> Dict:\n\n    dataset = Dataset(\n        valid_df, tokenizer,\n        max_sequence_length=max_sequence_length\n    )\n    loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False\n    )\n\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    else:\n        device = torch.device('cpu')\n\n    model.to(device)    \n\n    losses = []\n    logits = []\n    labels = []\n    for step, (_inputs, _labels) in enumerate(loader):\n        model.eval()\n\n        _inputs = {\n            k: _inputs[k].to(device)\n            for k in _inputs.keys() if k in {'input_ids','attention_mask','token_type_ids'}\n        }\n\n        with torch.no_grad():\n            _loss, _logits, *_ = model(\n                **_inputs, \n                labels=_labels.to(device),\n                return_dict=False\n            )\n            losses.append(_loss.item())\n            logits.append(_logits.detach().cpu().numpy())\n            labels.append(_labels.reshape((-1,1)))\n\n    del _inputs, _loss, _logits\n    rmse = sklearn.metrics.mean_squared_error(\n        np.vstack(labels), np.vstack(logits),\n        squared=False\n    )\n\n    return dict(\n        mean_loss=(sum(losses)/len(losses)),\n        rmse=rmse\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.model_selection\n\ndef make_folds(num_folds: int) -> pd.DataFrame:\n    df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n    \n    folds = pd.Series([-1] * len(df))\n    kf = sklearn.model_selection.KFold(num_folds)\n    for fold, (_, valid_index) in enumerate(kf.split(df)):\n        folds[valid_index] = fold\n\n    df.loc[:, 'fold'] = folds\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_everything()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    BertForSequenceClassification,\n    BertTokenizer,\n    AdamW\n)\n\ndf = make_folds(5)\n\ntrain_df = df[df['fold'].isin(list(range(4))) == True]\nvalid_df = df[df['fold'].isin(list(range(4))) == False]\n\nmodel_name_or_path = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n\ndataset = Dataset(\n    train_df, tokenizer,\n    max_sequence_length=512\n)\nloader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=12,\n    shuffle=True\n)\n# regression\nmodel = BertForSequenceClassification.from_pretrained(\n    model_name_or_path,\n    num_labels=1\n)\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n\nmodel.to(device)\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nfor epoch in range(1, 5+1):\n    losses = []\n    for step, (inputs, labels) in enumerate(loader):\n        model.train()\n\n        inputs = {\n            k: inputs[k].to(device)\n            for k in inputs.keys() if k in {'input_ids','attention_mask','token_type_ids'}\n        }\n        optimizer.zero_grad()\n\n        loss, *_ = model(\n            **inputs, \n            labels=labels.to(device),\n            return_dict=False\n        )\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n\n    del inputs, loss\n    print('training loss@%d=%f' % (epoch, sum(losses)/len(losses)))\n    result = validate(\n        model, tokenizer, valid_df,\n        max_sequence_length=512,\n        batch_size=12\n    )\n    print('validation loss@%d=%f, rmse=%f' % (epoch, result['mean_loss'], result['rmse']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\noutput_dir = Path('output/')\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)\n\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint('saved to model and tokenizer to %s' % str(output_dir))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}