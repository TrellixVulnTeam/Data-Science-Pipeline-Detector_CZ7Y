{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import codecs\nimport copy\nimport csv\nimport gc\nfrom itertools import chain\nimport os\nimport pickle\nimport random\nimport time\nfrom typing import Dict, List, Tuple, Union\nimport warnings","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nimport nltk\nfrom nltk.corpus import wordnet\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom transformers import AutoConfig, AutoTokenizer, TFAutoModel","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskCalculator(tf.keras.layers.Layer):\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(MaskCalculator, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        super(MaskCalculator, self).build(input_shape)\n\n    def call(self, inputs, **kwargs):\n        return tf.keras.backend.permute_dimensions(\n            x=tf.keras.backend.repeat(\n                x=tf.keras.backend.cast(\n                    x=tf.keras.backend.greater(\n                        x=inputs,\n                        y=0\n                    ),\n                    dtype='float32'\n                ),\n                n=self.output_dim\n            ),\n            pattern=(0, 2, 1)\n        )\n\n    def compute_output_shape(self, input_shape):\n        assert len(input_shape) == 1\n        shape = list(input_shape)\n        shape.append(self.output_dim)\n        return tuple(shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DatasetGen(tf.keras.utils.Sequence):\n    def __init__(self, data: Dict[str, Tuple[List[int], float, float, np.ndarray]],\n                 data_IDs: List[str], apply_augmentation: bool,\n                 feature_scaler: Pipeline,\n                 token_indices: np.ndarray, pad_token_id: int,\n                 batch_size: int, batches_per_epoch: Union[int, None] = None):\n        self.data = copy.deepcopy(data)\n        self.token_indices = token_indices\n        self.pad_token_id = pad_token_id\n        self.batch_size = batch_size\n        self.batches_per_epoch = batches_per_epoch\n        self.feature_scaler = feature_scaler\n        self.apply_augmentation = apply_augmentation\n        self.pairs = set()\n        for key1 in data_IDs:\n            for key2 in data_IDs:\n                if key1 == key2:\n                    continue\n                if (key1, key2) not in self.pairs:\n                    self.pairs.add((key1, key2))\n        self.pairs = list(self.pairs)\n        random.shuffle(self.pairs)\n        self.n_samples = min(len(self.pairs), len(data_IDs) * 4)\n    \n    def __len__(self):\n        if self.batches_per_epoch is None:\n            return int(np.ceil(self.n_samples / float(self.batch_size)))\n        return self.batches_per_epoch\n\n    def __getitem__(self, idx):\n        x_left = np.zeros(\n            shape=(self.batch_size, self.token_indices.shape[1]),\n            dtype=np.int32\n        )\n        left_features = []\n        x_right = np.zeros(\n            shape=(self.batch_size, self.token_indices.shape[1]),\n            dtype=np.int32\n        )\n        right_features = []\n        batch_y = np.zeros(\n            (self.batch_size, 1),\n            dtype=np.float32\n        )\n        if self.batches_per_epoch is None:\n            batch_start = idx * self.batch_size\n            batch_end = min(len(self.pairs), batch_start + self.batch_size)\n            for sample_idx in range(batch_end - batch_start):\n                left_key, right_key = self.pairs[sample_idx + batch_start]\n                left_idx = self.data[left_key][0][0]\n                left_features.append(self.data[left_key][3][0:1])\n                left_target = self.data[left_key][1]\n                right_idx = self.data[right_key][0][0]\n                right_target = self.data[right_key][1]\n                right_features.append(self.data[right_key][3][0:1])\n                x_left[sample_idx] = self.token_indices[left_idx]\n                x_right[sample_idx] = self.token_indices[right_idx]\n                batch_y[sample_idx, 0] = left_target - right_target\n            n_pad = self.batch_size - (batch_end - batch_start)\n            if n_pad > 0:\n                for sample_idx in range(batch_end - batch_start, self.batch_size):\n                    x_left[sample_idx] = x_left[sample_idx - 1]\n                    x_right[sample_idx] = x_right[sample_idx - 1]\n                    left_features.append(left_features[-1])\n                    right_features.append(right_features[-1])\n                    batch_y[sample_idx, 0] = batch_y[sample_idx - 1, 0]\n        else:\n            for sample_idx in range(self.batch_size):\n                left_key, right_key = random.choice(self.pairs)\n                if self.apply_augmentation:\n                    p = np.ones((len(self.data[left_key][0]),),\n                                dtype=np.float64)\n                    p[0] = max(2.0, p.shape[0] - 1.0)\n                    p /= p.sum()\n                    left_idx_ = np.random.choice(\n                        list(range(len(self.data[left_key][0]))),\n                        p=p\n                    )\n                    left_target = np.random.normal(\n                        loc=self.data[left_key][1],\n                        scale=self.data[left_key][2]\n                    )\n                else:\n                    left_idx_ = 0\n                    left_target = self.data[left_key][1]\n                left_idx = self.data[left_key][0][left_idx_]\n                left_features.append(self.data[left_key][3][left_idx_:(left_idx_ + 1)])\n                if self.apply_augmentation:\n                    p = np.ones((len(self.data[right_key][0]),),\n                                dtype=np.float64)\n                    p[0] = max(2.0, p.shape[0] - 1.0)\n                    p /= p.sum()\n                    right_idx_ = np.random.choice(\n                        list(range(len(self.data[right_key][0]))),\n                        p=p\n                    )\n                    right_target = np.random.normal(\n                        loc=self.data[right_key][1],\n                        scale=self.data[right_key][2]\n                    )\n                else:\n                    right_idx_ = 0\n                    right_target = self.data[right_key][1]\n                right_idx = self.data[right_key][0][right_idx_]\n                right_features.append(self.data[right_key][3][right_idx_:(right_idx_ + 1)])\n                x_left[sample_idx] = self.token_indices[left_idx]\n                x_right[sample_idx] = self.token_indices[right_idx]\n                batch_y[sample_idx, 0] = left_target - right_target\n        batch_x = [\n            x_left,\n            generate_attention_mask(x_left, self.pad_token_id),\n            self.feature_scaler.transform(np.vstack(left_features)),\n            x_right,\n            generate_attention_mask(x_right, self.pad_token_id), \n            self.feature_scaler.transform(np.vstack(right_features))\n        ]\n        del x_left, x_right\n        return batch_x, batch_y, None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_attention_mask(token_indices: np.ndarray, padding_id: int) -> np.ndarray:\n    attention = np.zeros(token_indices.shape, dtype=np.int32)\n    for sample_idx in range(token_indices.shape[0]):\n        for token_idx in range(token_indices.shape[1]):\n            if token_indices[sample_idx, token_idx] == padding_id:\n                break\n            attention[sample_idx, token_idx] = 1\n    return attention","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_text_features(texts: List[List[str]], tok: AutoTokenizer) -> np.ndarray:\n    f = np.zeros((len(texts), 9), dtype=np.float32)\n    for idx, sentences in enumerate(texts):\n        f[idx, 0] = len(sentences)\n        words = []\n        pure_words = []\n        for cur_sent in sentences:\n            words_in_sentence = nltk.word_tokenize(cur_sent)\n            words += words_in_sentence\n            pure_words += list(filter(lambda it: it.isalpha(), words_in_sentence))\n        f[idx, 1] = len(words) / f[idx, 0]\n        f[idx, 2] = len(pure_words) / f[idx, 0]\n        f[idx, 3] = len(' '.join(sentences))\n        f[idx, 4] = len(pure_words)\n        f[idx, 5] = np.mean([len(w) for w in pure_words])\n        for w in pure_words:\n            syllables = tok.tokenize(w.lower())\n            f[idx, 6] += len(syllables)\n            f[idx, 7] += sum(map(lambda it: len(it), syllables))\n        f[idx, 7] /= f[idx, 6]\n        f[idx, 8] = f[idx, 6] / f[idx, 4]\n    return f","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data_for_training(\n    fname: str,\n    tok: AutoTokenizer\n) -> List[Dict[str, Tuple[List[str], float, float, np.ndarray]]]:\n    loaded_header = []\n    id_col_idx = -1\n    text_col_idx = -1\n    target_col_idx = -1\n    std_col_idx = -1\n    line_idx = 1\n    data = dict()\n    set_of_texts = set()\n    with codecs.open(fname, mode='r', encoding='utf-8') as fp:\n        data_reader = csv.reader(fp, quotechar='\"', delimiter=',')\n        for row in data_reader:\n            if len(row) > 0:\n                err_msg = f'File {fname}: line {line_idx} is wrong!'\n                if len(loaded_header) == 0:\n                    loaded_header = copy.copy(row)\n                    try:\n                        text_col_idx = loaded_header.index('excerpt')\n                    except:\n                        text_col_idx = -1\n                    if text_col_idx <= 0:\n                        raise ValueError(err_msg + ' Field \"excerpt\" is not found!')\n                    try:\n                        id_col_idx = loaded_header.index('id')\n                    except:\n                        id_col_idx = -1\n                    if id_col_idx < 0:\n                        raise ValueError(err_msg + ' Field \"id\" is not found!')\n                    try:\n                        target_col_idx = loaded_header.index('target')\n                    except:\n                        target_col_idx = -1\n                    if target_col_idx < 0:\n                        raise ValueError(err_msg + ' Field \"target\" is not found!')\n                    try:\n                        std_col_idx = loaded_header.index('standard_error')\n                    except:\n                        std_col_idx = -1\n                    if std_col_idx < 0:\n                        err_msg2 = f'{err_msg} Field \"standard_error\" is not found!'\n                        raise ValueError(err_msg2)\n                else:\n                    sample_id = row[id_col_idx]\n                    if sample_id != sample_id.strip():\n                        raise ValueError(err_msg + f' {sample_id} is wrong sample ID!')\n                    if sample_id in data:\n                        err_msg2 = f'{err_msg} {sample_id} is not unique sample ID!'\n                        raise ValueError(err_msg2)\n                    text = row[text_col_idx].replace('\\r', '\\n')\n                    if len(text) == 0:\n                        raise ValueError(err_msg + f' Text {sample_id} is empty!')\n                    sentences = []\n                    for paragraph in map(lambda it: it.strip(), text.split('\\n')):\n                        if len(paragraph) > 0:\n                            sentences += nltk.sent_tokenize(paragraph)\n                    if len(sentences) == 0:\n                        raise ValueError(err_msg + f' Text {sample_id} is empty!')\n                    text = ' '.join([cur_sent for cur_sent in sentences])\n                    if text.lower() in set_of_texts:\n                        raise ValueError(err_msg + f' Text {sample_id} is not unique!')\n                    set_of_texts.add(text.lower())\n                    added_texts = [sentences]\n                    try:\n                        target_val = float(row[target_col_idx])\n                        ok = True\n                    except:\n                        target_val = 0.0\n                        ok = False\n                    if not ok:\n                        err_msg2 = err_msg\n                        err_msg2 += f' {row[target_col_idx]} is wrong target for ' \\\n                                    f'text {sample_id}.'\n                        raise ValueError(err_msg2)\n                    try:\n                        std_val = float(row[std_col_idx])\n                        ok = (std_val > 0.0)\n                    except:\n                        std_val = 0.0\n                        ok = False\n                    if not ok:\n                        err_msg2 = err_msg\n                        err_msg2 += f' {row[std_col_idx]} is wrong standard error' \\\n                                    f' for text {sample_id}.'\n                        warnings.warn(err_msg2)\n                    else:\n                        for _ in range(3):\n                            new_augmented_text = []\n                            for cur_sent in sentences:\n                                new_sent = cur_sent.strip()\n                                if len(new_sent) > 0:\n                                    new_augmented_text.append(new_sent)\n                            assert len(new_augmented_text) > 0\n                            random.shuffle(new_augmented_text)\n                            new_augmented_text_ = ' '.join(new_augmented_text)\n                            if (len(new_augmented_text_) > 0) and \\\n                                    (new_augmented_text_.lower() not in set_of_texts):\n                                set_of_texts.add(new_augmented_text_.lower())\n                                added_texts.append(new_augmented_text)\n                            del new_augmented_text, new_augmented_text_\n                        data[sample_id] = (\n                            list(map(lambda it: ' '.join(it), added_texts)),\n                            target_val, std_val,\n                            calc_text_features(added_texts, tok)\n                        )\n            line_idx += 1\n    return data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data_for_testing(fname: str, tok: AutoTokenizer, batch_size: int):\n    loaded_header = []\n    id_col_idx = -1\n    text_col_idx = -1\n    target_col_idx = -1\n    std_col_idx = -1\n    line_idx = 1\n    data = dict()\n    with codecs.open(fname, mode='r', encoding='utf-8') as fp:\n        data_reader = csv.reader(fp, quotechar='\"', delimiter=',')\n        for row in data_reader:\n            if len(row) > 0:\n                err_msg = f'File {fname}: line {line_idx} is wrong!'\n                if len(loaded_header) == 0:\n                    loaded_header = copy.copy(row)\n                    try:\n                        text_col_idx = loaded_header.index('excerpt')\n                    except:\n                        text_col_idx = -1\n                    if text_col_idx <= 0:\n                        raise ValueError(err_msg + ' Field \"excerpt\" is not found!')\n                    try:\n                        id_col_idx = loaded_header.index('id')\n                    except:\n                        id_col_idx = -1\n                    if id_col_idx < 0:\n                        raise ValueError(err_msg + ' Field \"id\" is not found!')\n                else:\n                    sample_id = row[id_col_idx]\n                    if sample_id != sample_id.strip():\n                        raise ValueError(err_msg + f' {sample_id} is wrong sample ID!')\n                    if sample_id in data:\n                        err_msg2 = f'{err_msg} {sample_id} is not unique sample ID!'\n                        raise ValueError(err_msg2)\n                    text = row[text_col_idx].replace('\\n', ' ').replace('\\r', ' ')\n                    text = ' '.join(text.split()).strip()\n                    if len(text) == 0:\n                        raise ValueError(err_msg + f' Text {sample_id} is empty!')\n                    features = calc_text_features([nltk.sent_tokenize(text)], tok) \n                    data[sample_id] = (text, features)\n                    if len(data) >= batch_size:\n                        yield data\n                        del data\n                        data = dict()\n            line_idx += 1\n    if len(data) > 0:\n        yield data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_feature_scaler(data: Dict[str, Tuple[List[int], float, float,\n                                               np.ndarray]]) -> Pipeline:\n    features_for_training = []\n    for sample_id in data:\n        features_for_training.append(data[sample_id][3])\n    features_for_training = np.vstack(features_for_training)\n    scaler = Pipeline(steps=[\n        ('scaler', StandardScaler()),\n        ('transformer', PowerTransformer())\n    ])\n    return scaler.fit(features_for_training)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_data(\n    data: Union[List[Dict[str, Tuple[str, np.ndarray]]],\n                List[Dict[str, Tuple[List[str], float, float, np.ndarray]]]],\n    tokenizer: AutoTokenizer, max_seq_len: int\n) -> Tuple[Union[Dict[str, Tuple[int, np.ndarray]],\n                 Dict[str, Tuple[List[int], float, float, np.ndarray]]],\n           np.ndarray]:\n    tokenized_data = dict()\n    all_tokens_matrix = []\n    for sample_idx, cur_ID in enumerate(sorted(list(data.keys()))):\n        if len(data[cur_ID]) == 2:\n            tokens = tokenizer.tokenize(data[cur_ID][0])\n            tokenized_data[cur_ID] = (len(all_tokens_matrix), data[cur_ID][1])\n            token_ids = tokenizer.convert_tokens_to_ids(\n                [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n            )\n            ndiff = max_seq_len - len(token_ids)\n            if ndiff > 0:\n                token_ids += [tokenizer.pad_token_id for _ in range(ndiff)]\n            elif ndiff < 0:\n                token_ids = token_ids[:max_seq_len]\n            all_tokens_matrix.append(token_ids)\n        else:\n            text_idx_list = []\n            for cur_text in data[cur_ID][0]:\n                tokens = tokenizer.tokenize(cur_text)\n                token_ids = tokenizer.convert_tokens_to_ids(\n                    [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n                )\n                ndiff = max_seq_len - len(token_ids)\n                if ndiff > 0:\n                    token_ids += [tokenizer.pad_token_id for _ in range(ndiff)]\n                elif ndiff < 0:\n                    token_ids = token_ids[:max_seq_len]\n                text_idx_list.append(len(all_tokens_matrix))\n                all_tokens_matrix.append(token_ids)\n            tokenized_data[cur_ID] = (text_idx_list, data[cur_ID][1], data[cur_ID][2],\n                                      data[cur_ID][3])\n    return tokenized_data, np.array(all_tokens_matrix, dtype=np.int32)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_info_about_data(\n    data: Union[List[Dict[str, Tuple[str, np.ndarray]]],\n                List[Dict[str, Tuple[List[str], float, float, np.ndarray]]]],\n    identifiers: List[str]\n):\n    for_training = (len(data[identifiers[0]]) == 4)\n    if for_training:\n        print(f'Number of samples for training is {len(data)}.')\n    else:\n        print(f'Number of samples for submission is {len(data)}.')\n    print('')\n    print(f'{len(identifiers)} random samples:')\n    for cur_id in identifiers:\n        print('')\n        print(f'  Sample {cur_id}')\n        if for_training:\n            print('  Text:')\n            print(f'    {data[cur_id][0][0]}')\n            print(f'  Number of augmented texts is {len(data[cur_id][0]) - 1}.')\n            if (len(data[cur_id][0]) - 1) > 0:\n                if (len(data[cur_id][0]) - 1) > 1:\n                    print('  2 augmented texts:')\n                    for augmented in data[cur_id][0][1:3]:\n                        print(f'    {augmented}')\n                else:\n                    print('  Augmented text:')\n                    for augmented in data[cur_id][0][1:2]:\n                        print(f'    {augmented}')\n            print('  Target:')\n            print(f'    {data[cur_id][1]} +- {data[cur_id][2]}')\n            print('  Features:')\n            for it in data[cur_id][3].tolist(): print(f'    {it}') \n        else:\n            print(' Text:')\n            print(f'    {data[cur_id][0]}')\n            print(' Features:')\n            print(f'    {data[cur_id][1].tolist()[0]}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_info_about_tokenized_data(\n    data: Union[Dict[str, Tuple[int, np.ndarray]],\n                Dict[str, Tuple[List[int], float, float, np.ndarray]]],\n    matrix: np.ndarray,\n    identifiers: List[str]\n):\n    for_training = (len(data[identifiers[0]]) == 4)\n    if for_training:\n        print(f'Number of tokenized samples for training is {len(data)}.')\n    else:\n        print(f'Number of tokenized samples for submission is {len(data)}.')\n    print('')\n    print(f'{len(identifiers)} random samples:')\n    for cur_id in identifiers:\n        print('')\n        print(f'Sample {cur_id}')\n        print('')\n        sample_idx = data[cur_id][0][0]\n        print(matrix[sample_idx].tolist())\n        print('')\n        print(data[cur_id][-1][0].tolist())\n        print('')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_feature_extractor(bert_name: str, max_seq_len: int, feature_vector_size: int,\n                            batch_size: int) -> Tuple[tf.keras.Model, int]:\n    transformer_model = TFAutoModel.from_pretrained(\n        pretrained_model_name_or_path=bert_name,\n        name='BaseTransformer'\n    )\n    united_embedding_size = 256\n    transformer_config = AutoConfig.from_pretrained(bert_name)\n    united_emb_layer = tf.keras.layers.Dense(\n        units=united_embedding_size, input_dim=transformer_config.hidden_size,\n        activation='elu',\n        kernel_initializer=tf.keras.initializers.HeNormal(seed=42),\n        bias_initializer='zeros',\n        name='UnitedEmbeddingLayer'\n    )\n    print('Transformer Configuration')\n    print('=========================')\n    print(transformer_config)\n    tokens_input = tf.keras.layers.Input(shape=(max_seq_len,), batch_size=batch_size,\n                                         dtype=tf.int32, name='word_ids_base')\n    attention_input = tf.keras.layers.Input(shape=(max_seq_len,), batch_size=batch_size,\n                                            dtype=tf.int32, name='attention_mask_base')\n    features_input = tf.keras.layers.Input(shape=(feature_vector_size,), dtype=tf.float32,\n                                           batch_size=batch_size, name='features_base')\n    sequence_output = transformer_model([tokens_input, attention_input])[0]\n    output_mask = MaskCalculator(\n        output_dim=transformer_config.hidden_size, trainable=False,\n        name='OutMaskCalculator'\n    )(attention_input)\n    masked_output = tf.keras.layers.Multiply(\n        name='OutMaskMultiplicator'\n    )([output_mask, sequence_output])\n    masked_output = tf.keras.layers.Masking(name='OutMasking')(masked_output)\n    final_output = tf.keras.layers.GlobalAvgPool1D(name='AvePool')(masked_output)\n    final_output = tf.keras.layers.LayerNormalization(\n        name='LayerNorm1'\n    )(final_output)\n    final_output = tf.keras.layers.Concatenate(\n        name='Concat'\n    )([final_output, features_input])\n    final_output = tf.keras.layers.Dropout(\n        rate=0.3, seed=42, name='EmbeddingDropout' \n    )(final_output)\n    final_output = united_emb_layer(final_output)\n    final_output = tf.keras.layers.LayerNormalization(\n        name='LayerNorm2'\n    )(final_output) \n    fe_model = tf.keras.Model(\n        inputs=[tokens_input, attention_input, features_input],\n        outputs=final_output,\n        name='FeatureExtractionModel'\n    )\n    fe_model.build(input_shape=[(batch_size, max_seq_len),\n                                (batch_size, max_seq_len),\n                                (batch_size, feature_vector_size)])\n    return fe_model, united_embedding_size","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_twin_regressor(feature_vector_size: int, batch_size: int) -> tf.keras.Model:\n    left_input = tf.keras.layers.Input(shape=(feature_vector_size,), dtype=tf.float32,\n                                       batch_size=batch_size, name='features_left')\n    right_input = tf.keras.layers.Input(shape=(feature_vector_size,), dtype=tf.float32,\n                                        batch_size=batch_size, name='features_right')\n    concatenated_features = tf.keras.layers.Concatenate(\n        name='ConcatFeatures'\n    )([left_input, right_input])\n    dropout_layer = tf.keras.layers.Dropout(\n        rate=0.3,\n        name='RegressionDropout'\n    )(concatenated_features) \n    regression_layer = tf.keras.layers.Dense(\n        units=1, input_dim=feature_vector_size * 2, activation=None,\n        kernel_initializer=tf.keras.initializers.GlorotNormal(seed=42),\n        bias_initializer='zeros',\n        name='RegressionLayer'\n    )(dropout_layer)\n    twin_regression_model = tf.keras.Model(\n        inputs=[left_input, right_input],\n        outputs=regression_layer,\n        name='TwinRegressionModel'\n    )\n    twin_regression_model.build(input_shape=[(batch_size, feature_vector_size),\n                                             (batch_size, feature_vector_size)])\n    return twin_regression_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_neural_network(bert_name: str, max_seq_len: int, feature_vector_size: int,\n                         batch_size: int) -> Tuple[tf.keras.Model, tf.keras.Model,\n                                                   tf.keras.Model]:\n    fe_layer, ft_vec_size = build_feature_extractor(bert_name, max_seq_len,\n                                                    feature_vector_size, batch_size)\n    left_tokens = tf.keras.layers.Input(shape=(max_seq_len,), batch_size=batch_size,\n                                        dtype=tf.int32, name='word_ids')\n    left_attention = tf.keras.layers.Input(shape=(max_seq_len,), batch_size=batch_size,\n                                           dtype=tf.int32, name='attention_mask')\n    left_features = tf.keras.layers.Input(shape=(feature_vector_size,), dtype=tf.float32,\n                                          batch_size=batch_size, name='features')\n    right_tokens = tf.keras.layers.Input(shape=(max_seq_len,), batch_size=batch_size,\n                                         dtype=tf.int32, name='right_word_ids')\n    right_attention = tf.keras.layers.Input(shape=(max_seq_len,), batch_size=batch_size,\n                                            dtype=tf.int32, name='right_attention_mask')\n    right_features = tf.keras.layers.Input(shape=(feature_vector_size,), dtype=tf.float32,\n                                           batch_size=batch_size, name='right_features')\n    left_output = fe_layer([left_tokens, left_attention, left_features])\n    right_output = fe_layer([right_tokens, right_attention, right_features])\n    regression_model = build_twin_regressor(ft_vec_size, batch_size)\n    regression_layer = regression_model([left_output, right_output])\n    siamese_model = tf.keras.Model(\n        inputs=[left_tokens, left_attention, left_features,\n                right_tokens, right_attention, right_features],\n        outputs=regression_layer,\n        name='SiameseModel'\n    )\n    radam = tfa.optimizers.RectifiedAdam(learning_rate=1e-6)\n    ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n    siamese_model.compile(optimizer=ranger, loss=tf.keras.losses.MeanSquaredError())\n    return siamese_model, fe_layer, regression_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_minibatch(X: List[np.ndarray], y: np.ndarray):\n    assert len(X) == 6\n    print('')\n    print('X1')\n    for it in X[0].tolist(): print(it)\n    print('')\n    print('X2')\n    for it in X[1].tolist(): print(it)\n    print('')\n    print('X3')\n    for it in X[2].tolist(): print(it)\n    print('')\n    print('X4')\n    for it in X[3].tolist(): print(it)\n    print('')\n    print('X5')\n    for it in X[4].tolist(): print(it)\n    print('X6')\n    for it in X[5].tolist(): print(it) \n    print('')\n    print('y')\n    for it in y.tolist(): print(it)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_tsne(fe: tf.keras.Model, batch_size: int,\n              data: Dict[str, Tuple[List[int], float, float, np.ndarray]],\n              feature_scaler: Pipeline,\n              token_matrix: np.ndarray,\n              identifiers: List[str], pad_id: int, title: str, figure_id: int):\n    indices = list(map(lambda it: data[it][0][0], identifiers))\n    colors = np.array(\n        list(map(lambda it: data[it][1], identifiers)),\n        dtype=np.float64\n    )\n    area = np.array(\n        list(map(lambda it: data[it][2], identifiers)),\n        dtype=np.float64\n    )\n    area /= np.max(area)\n    area *= 10.0\n    area = np.power(area, 2)\n    texts = token_matrix[indices]\n    src_features = np.vstack(\n        list(map(\n            lambda it: data[it][3][0:1], \n            identifiers\n        ))\n    )\n    assert src_features.shape[0] == texts.shape[0]\n    ndiff = texts.shape[0] % batch_size\n    if ndiff > 0:\n        last_text_idx = texts.shape[0] - 1\n        texts = np.vstack(\n            [texts] + \n            [texts[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n        src_features = np.vstack(\n            [src_features] +\n            [src_features[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n    attentions = generate_attention_mask(texts, pad_id)\n    assert texts.shape[0] % batch_size == 0, f'{texts.shape[0] % batch_size}'\n    features = fe.predict(\n        [texts, attentions, feature_scaler.transform(src_features)],\n        batch_size=batch_size\n    )\n    features = features[:len(indices)]\n    projected_features = TSNE(n_components=2, n_jobs=-1).fit_transform(features)\n    fig = plt.figure(figure_id, figsize=(11, 11))\n    plt.scatter(x=projected_features[:, 0], y=projected_features[:, 1],\n                marker='o', cmap=plt.cm.get_cmap(\"jet\"), s=area,\n                c=colors, norm=Normalize(vmin=np.min(colors), vmax=np.max(colors)))\n    plt.title('t-SNE projections of texts ' + title)\n    plt.colorbar()\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_training_process(history: tf.keras.callbacks.History, metric_name: str,\n                          figure_id: int):\n    val_metric_name = 'val_' + metric_name\n    possible_metrics = list(history.history.keys())\n    if metric_name not in history.history:\n        err_msg = f'The metric \"{metric_name}\" is not found!'\n        err_msg += f' Available metrics are: {possible_metrics}.'\n        raise ValueError(err_msg)\n    fig = plt.figure(figure_id, figsize=(7, 7))\n    metric_values = history.history[metric_name]\n    plt.plot(list(range(len(metric_values))), metric_values,\n             label='Training {0}'.format(metric_name))\n    if val_metric_name in history.history:\n        val_metric_values = history.history['val_' + metric_name]\n        assert len(metric_values) == len(val_metric_values)\n        plt.plot(list(range(len(val_metric_values))), val_metric_values,\n                 label='Validation {0}'.format(metric_name))\n    plt.xlabel('Epochs')\n    plt.ylabel(metric_name)\n    plt.title('Training process')\n    plt.legend(loc='best')\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_new_trainset(fe: tf.keras.Model, feature_scaler: Pipeline,\n                          batch_size: int,\n                          data: Dict[str, Tuple[List[int], float, float]],\n                          token_matrix: np.ndarray, pad_id: int,\n                          identifiers: List[str]) -> Tuple[np.ndarray, np.ndarray]:\n    indices = list(map(lambda it: data[it][0][0], identifiers))\n    texts = token_matrix[indices]\n    src_features = np.vstack(list(map(lambda it: data[it][3][0:1], identifiers)))\n    targets = np.array(list(map(lambda it: data[it][1], identifiers)),\n                       dtype=np.float64)\n    assert texts.shape[0] == src_features.shape[0]\n    ndiff = texts.shape[0] % batch_size\n    if ndiff > 0:\n        last_text_idx = texts.shape[0] - 1\n        texts = np.vstack(\n            [texts] + \n            [texts[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n        src_features = np.vstack(\n            [src_features] +\n            [src_features[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n    attentions = generate_attention_mask(texts, pad_id)\n    assert texts.shape[0] % batch_size == 0, f'{texts.shape[0] % batch_size}'\n    target_features = fe.predict(\n        [texts, attentions, feature_scaler.transform(src_features)],\n        batch_size=batch_size\n    )\n    assert target_features.shape[1] > 1\n    target_features = target_features[:len(identifiers)]\n    return target_features, targets","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_dist_matrix(y: np.ndarray) -> np.ndarray:\n    assert len(y.shape) == 1\n    assert y.shape[0] > 1\n    d = np.zeros((y.shape[0], y.shape[0]), dtype=np.float32)\n    for idx1 in range(y.shape[0]):\n        for idx2 in range(y.shape[0]):\n            diff = y[idx1] - y[idx2]\n            d[idx1, idx2] = np.sqrt(diff * diff)\n    return d","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def select_train_samples(y: np.ndarray, dist_matrix: np.ndarray,\n                         n: int) -> List[int]:\n    assert len(y.shape) == 1\n    assert len(dist_matrix.shape) == 2\n    assert dist_matrix.shape[0] == y.shape[0]\n    assert dist_matrix.shape[1] == dist_matrix.shape[0]\n    assert n < y.shape[0]\n    indices_of_samples = list(range(y.shape[0]))\n    selected = {np.random.choice(indices_of_samples)}\n    for _ in range(n - 1):\n        indices_of_samples = sorted(list(set(indices_of_samples) - selected))\n        p = [dist_matrix[idx, list(selected)].mean() for idx in indices_of_samples]\n        p = np.array(p, dtype=np.float64)\n        p /= p.sum()\n        selected.add(np.random.choice(indices_of_samples, p=p))\n    return sorted(list(selected))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_predictions(fe: tf.keras.Model, regressor: tf.keras.Model,\n                   feature_scaler: Pipeline, batch_size: int,\n                   data_for_anchors: Tuple[np.ndarray, np.ndarray],\n                   dist_matrix: np.ndarray,\n                   data: Union[Dict[str, int],\n                               Dict[str, Tuple[List[int], float, float]]],\n                   token_matrix: np.ndarray, pad_id: int,\n                   identifiers: List[str]=None) -> Dict[str, Tuple[float, float]]:\n    if identifiers is None:\n        identifiers_ = sorted(list(data.keys()))\n    else:\n        identifiers_ = sorted(identifiers)\n    indices = list(map(\n        lambda it: data[it][0] if len(data[it]) == 2 else data[it][0][0],\n        identifiers_\n    ))\n    texts = token_matrix[indices]\n    src_features = np.vstack(\n        list(map(\n            lambda it: data[it][1] if len(data[it]) == 2 else data[it][3][0:1],\n            identifiers_\n        ))\n    )\n    assert texts.shape[0] == src_features.shape[0]\n    ndiff = texts.shape[0] % batch_size\n    if ndiff > 0:\n        last_text_idx = texts.shape[0] - 1\n        texts = np.vstack(\n            [texts] + \n            [texts[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n        src_features = np.vstack(\n            [src_features] +\n            [src_features[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n    attentions = generate_attention_mask(texts, pad_id)\n    assert texts.shape[0] % batch_size == 0, f'{texts.shape[0] % batch_size}'\n    target_features = fe.predict(\n        [texts, attentions, feature_scaler.transform(src_features)],\n        batch_size=batch_size\n    )\n    assert target_features.shape[1] > 1\n    assert target_features.shape[1] == data_for_anchors[0].shape[1]\n    assert target_features.shape[0] >= len(indices)\n    target_features = target_features[0:len(indices)]\n    selected_inputs = []\n    predicted_features = []\n    selected_targets = []\n    n_selected = batch_size\n    while n_selected < 8:\n        n_selected += batch_size\n    for sample_idx, cur_id in enumerate(identifiers_):\n        selected_indices_for_training = select_train_samples(\n            y=data_for_anchors[1],\n            dist_matrix=dist_matrix,\n            n=n_selected\n        )\n        selected_inputs.append(data_for_anchors[0][selected_indices_for_training])\n        selected_targets.append(data_for_anchors[1][selected_indices_for_training])\n        predicted_features.append(np.full(\n            fill_value=target_features[sample_idx],\n            shape=(n_selected, target_features.shape[1])\n        ))\n    selected_inputs = np.vstack(selected_inputs)\n    predicted_features = np.vstack(predicted_features)\n    selected_targets = np.concatenate(selected_targets)\n    prediction_diff = regressor.predict(\n        [selected_inputs, predicted_features],\n        batch_size=batch_size\n    ).reshape(selected_targets.shape)\n    predictions = dict()\n    for sample_idx, cur_id in enumerate(identifiers_):\n        start_pos = sample_idx * n_selected\n        end_pos = start_pos + n_selected\n        instant_predictions = selected_targets[start_pos:end_pos] - \\\n                              prediction_diff[start_pos:end_pos]\n        predictions[cur_id] = (np.mean(instant_predictions),\n                               np.std(instant_predictions))\n    return predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_TEXT_LEN = 128\nPRETRAINED_BERT = '/kaggle/input/tf-distilroberta-base'\nMINIBATCH_SIZE = 32","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/commonlitreadabilityprize'\nMODEL_DIR = '/kaggle/working'\nprint(f'{DATA_DIR} {os.path.isdir(DATA_DIR)}')\nprint(f'{MODEL_DIR} {os.path.isdir(MODEL_DIR)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset_name = os.path.join(DATA_DIR, 'train.csv')\nprint(f'{trainset_name} {os.path.isfile(trainset_name)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testset_name = os.path.join(DATA_DIR, 'test.csv')\nprint(f'{testset_name} {os.path.isfile(testset_name)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_name = os.path.join(MODEL_DIR, 'submission.csv')\nprint(f'{submission_name} {os.path.isfile(submission_name)}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fe_model_name = os.path.join(MODEL_DIR, 'fe_nn.h5')\nregression_model_name = os.path.join(MODEL_DIR, 'regression_nn.h5')\nscaler_name = os.path.join(MODEL_DIR, 'feature_scaler.pkl')\nfigure_identifier = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BERT)\nprint(f'Vocabulary size is {pretrained_tokenizer.vocab_size}.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_for_training = load_data_for_training(trainset_name,\n                                           pretrained_tokenizer)\nassert len(data_for_training) > 100","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_IDs = sorted(list(data_for_training.keys()))\nselected_IDs_for_training = random.sample(\n    population=all_IDs,\n    k=3\n)\nprint_info_about_data(data_for_training, selected_IDs_for_training)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_for_training, tokens_for_training = tokenize_data(\n    data=data_for_training,\n    tokenizer=pretrained_tokenizer,\n    max_seq_len=MAX_TEXT_LEN\n)\nprint_info_about_tokenized_data(\n    data=labels_for_training,\n    matrix=tokens_for_training,\n    identifiers=selected_IDs_for_training\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_feature_scaler = train_feature_scaler(labels_for_training) \nwith open(scaler_name, 'wb') as scaler_fp:\n    pickle.dump(text_feature_scaler, scaler_fp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.shuffle(all_IDs)\nn_train_size = int(round(len(all_IDs) * 0.9))\nn_val_size = int(round(len(all_IDs) * 0.05))\nIDs_for_training = all_IDs[:n_train_size]\nIDs_for_validation = all_IDs[n_train_size:(n_train_size + n_val_size)]\nIDs_for_final_testing = all_IDs[(n_train_size + n_val_size):]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen_for_validation = DatasetGen(\n    data=labels_for_training,\n    data_IDs=IDs_for_validation,\n    token_indices=tokens_for_training,\n    pad_token_id=pretrained_tokenizer.pad_token_id,\n    batch_size=MINIBATCH_SIZE,\n    apply_augmentation=False,\n    feature_scaler=text_feature_scaler\n)\nn_batches_per_validset = len(datagen_for_validation)\nprint(f'Mini-batches per validation set is {n_batches_per_validset}.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_, y_, _ = datagen_for_validation[0]\nshow_minibatch(X_, y_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_batches_per_epoch = n_batches_per_validset * 10\ndatagen_for_training = DatasetGen(\n    data=labels_for_training,\n    data_IDs=IDs_for_training,\n    token_indices=tokens_for_training,\n    pad_token_id=pretrained_tokenizer.pad_token_id,\n    batch_size=MINIBATCH_SIZE,\n    batches_per_epoch=n_batches_per_epoch,\n    apply_augmentation=True,\n    feature_scaler=text_feature_scaler\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_, y_, _ = datagen_for_training[0] \nshow_minibatch(X_, y_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_for_training, fe_model, model_for_regression = build_neural_network(\n    bert_name=PRETRAINED_BERT,\n    max_seq_len=MAX_TEXT_LEN,\n    feature_vector_size=text_feature_scaler.named_steps['scaler'].scale_.shape[0],\n    batch_size=MINIBATCH_SIZE\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_for_training.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_for_regression.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fe_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_tsne(fe=fe_model, batch_size=MINIBATCH_SIZE,\n          feature_scaler=text_feature_scaler,\n          data=labels_for_training, token_matrix=tokens_for_training,\n          identifiers=IDs_for_validation + IDs_for_final_testing,\n          pad_id=pretrained_tokenizer.pad_token_id,\n          title='before training', figure_id=figure_identifier)\nfigure_identifier += 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anchor_data = generate_new_trainset(\n    fe=fe_model, feature_scaler=text_feature_scaler,\n    batch_size=MINIBATCH_SIZE,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_training\n)\nanchor_distances = calculate_dist_matrix(anchor_data[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\npredictions_for_validation = do_predictions(\n    fe=fe_model, regressor=model_for_regression,\n    feature_scaler=text_feature_scaler, batch_size=MINIBATCH_SIZE,\n    data_for_anchors=anchor_data, dist_matrix=anchor_distances,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_validation\n)\npredict_duration = (time.time() - start_time) / float(len(IDs_for_validation))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error = 0.0\nfor cur_id in IDs_for_validation:\n    difference = predictions_for_validation[cur_id][0] - labels_for_training[cur_id][1]\n    error += (difference * difference)\nerror /= float(len(IDs_for_validation))\nerror = np.sqrt(error)\nprint(f'RMSE on validation set before training = {error}')\nprint(f'Prediction duration per sample = {predict_duration} seconds.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del predictions_for_validation, error","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\npredictions_for_testing = do_predictions(\n    fe=fe_model, regressor=model_for_regression,\n    feature_scaler=text_feature_scaler, batch_size=MINIBATCH_SIZE,\n    data_for_anchors=anchor_data, dist_matrix=anchor_distances,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_final_testing\n)\npredict_duration = (time.time() - start_time) / float(len(IDs_for_final_testing))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error = 0.0\nfor cur_id in IDs_for_final_testing:\n    difference = predictions_for_testing[cur_id][0] - labels_for_training[cur_id][1]\n    error += (difference * difference)\nerror /= float(len(IDs_for_final_testing))\nerror = np.sqrt(error)\nprint(f'RMSE on test set before training = {error}')\nprint(f'Prediction duration per sample = {predict_duration} seconds.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del predictions_for_testing, error","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del anchor_data, anchor_distances","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        patience=10,\n        verbose=True,\n        restore_best_weights=True\n    ),\n    tfa.callbacks.TimeStopping(\n        seconds=int(round(3600 * 1.9)),\n        verbose=True\n    )\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_for_training.fit(datagen_for_training,\n                                 validation_data=datagen_for_validation,\n                                 epochs=1000, callbacks=callbacks)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_for_regression.save_weights(regression_model_name)\nfe_model.save_weights(fe_model_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training_process(history, \"loss\", figure_identifier)\nfigure_identifier += 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_tsne(fe=fe_model, batch_size=MINIBATCH_SIZE,\n          feature_scaler=text_feature_scaler,\n          data=labels_for_training, token_matrix=tokens_for_training,\n          identifiers=IDs_for_validation + IDs_for_final_testing,\n          pad_id=pretrained_tokenizer.pad_token_id,\n          title='after training', figure_id=figure_identifier)\nfigure_identifier += 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anchor_data = generate_new_trainset(\n    fe=fe_model, feature_scaler=text_feature_scaler,\n    batch_size=MINIBATCH_SIZE,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_training\n)\nanchor_distances = calculate_dist_matrix(anchor_data[1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\npredictions_for_validation = do_predictions(\n    fe=fe_model, regressor=model_for_regression,\n    feature_scaler=text_feature_scaler, batch_size=MINIBATCH_SIZE,\n    data_for_anchors=anchor_data, dist_matrix=anchor_distances,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_validation\n)\npredict_duration = (time.time() - start_time) / float(len(IDs_for_validation))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error = 0.0\nfor cur_id in IDs_for_validation:\n    difference = predictions_for_validation[cur_id][0] - labels_for_training[cur_id][1]\n    error += (difference * difference)\nerror /= float(len(IDs_for_validation))\nerror = np.sqrt(error)\nprint(f'RMSE on validation set after training = {error}')\nprint(f'Prediction duration per sample = {predict_duration} seconds.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del predictions_for_validation, error","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\npredictions_for_testing = do_predictions(\n    fe=fe_model, regressor=model_for_regression,\n    feature_scaler=text_feature_scaler, batch_size=MINIBATCH_SIZE,\n    data_for_anchors=anchor_data, dist_matrix=anchor_distances,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_final_testing\n)\npredict_duration = (time.time() - start_time) / float(len(IDs_for_final_testing))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error = 0.0\nfor cur_id in IDs_for_final_testing:\n    difference = predictions_for_testing[cur_id][0] - labels_for_training[cur_id][1]\n    error += (difference * difference)\nerror /= float(len(IDs_for_final_testing))\nerror = np.sqrt(error)\nprint(f'RMSE on test set after training = {error}')\nprint(f'Prediction duration per sample = {predict_duration} seconds.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = sorted(\n    [(cur_id, predictions_for_testing[cur_id][0], predictions_for_testing[cur_id][1])\n     for cur_id in predictions_for_testing],\n    key=lambda it: (it[2], it[1], it[0])\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Top-5 most certain predictions:')\nprint('')\nfor cur_id, pred_mean, pred_std in pred[0:5]:\n    print('True:      {0:.6f} +- {1:.6f}'.format(data_for_training[cur_id][1],\n                                                 data_for_training[cur_id][2]))\n    print('Predicted: {0:.6f} +- {1:.6f}'.format(pred_mean, pred_std))\n    print(data_for_training[cur_id][0][0])\n    print('')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Top-5 most uncertain predictions:')\nprint('')\nfor cur_id, pred_mean, pred_std in pred[-5:]:\n    print('True:      {0:.6f} +- {1:.6f}'.format(data_for_training[cur_id][1],\n                                                 data_for_training[cur_id][2]))\n    print('Predicted: {0:.6f} +- {1:.6f}'.format(pred_mean, pred_std))\n    print(data_for_training[cur_id][0][0])\n    print('')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del predictions_for_testing, error, pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del datagen_for_training, datagen_for_validation\ndel labels_for_training, tokens_for_training\ndel data_for_training\ndel IDs_for_training, IDs_for_validation, IDs_for_final_testing\ndel model_for_training\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with codecs.open(submission_name, mode='w', encoding='utf-8') as fp:\n    data_writer = csv.writer(fp, quotechar='\"', delimiter=',')\n    data_writer.writerow(['id', 'target'])\n    for data_part in load_data_for_testing(testset_name, pretrained_tokenizer,\n                                           MINIBATCH_SIZE * 8):\n        labels_for_submission, tokens_for_submission = tokenize_data(\n            data=data_part,\n            tokenizer=pretrained_tokenizer,\n            max_seq_len=MAX_TEXT_LEN\n        )\n        del data_part\n        predictions_for_submission = do_predictions(\n            fe=fe_model, regressor=model_for_regression,\n            feature_scaler=text_feature_scaler, batch_size=MINIBATCH_SIZE,\n            data_for_anchors=anchor_data, dist_matrix=anchor_distances,\n            data=labels_for_submission, token_matrix=tokens_for_submission,\n            pad_id=pretrained_tokenizer.pad_token_id\n        )\n        for cur_id in predictions_for_submission:\n            predicted = predictions_for_submission[cur_id][0]\n            data_writer.writerow([cur_id, f'{predicted}'])\n        del predictions_for_submission\n        del labels_for_submission, tokens_for_submission\n        gc.collect()","metadata":{},"execution_count":null,"outputs":[]}]}