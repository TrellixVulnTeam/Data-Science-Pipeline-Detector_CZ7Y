{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ndf.title = df.excerpt","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min(df.target.tolist())\n# df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preliminary Analysis\n\n## Number of characters present in the publication titles.","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig = sns.displot(x=df.excerpt.str.len(), data=df, color='black', kde=False, height=6, aspect=3, kind='hist')\n\nprint(df.excerpt.str.len().min())\nprint(df.excerpt.str.len().max())\nprint(df.excerpt.str.len().mean())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see the length of excerpts range from 669 to 1341 characters. On average, the publication title length is 972.","metadata":{}},{"cell_type":"markdown","source":"## Number of words in publication titles","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntemp = df.title.str.split().map(lambda x: len(x))\n\nfig = sns.displot(x=temp, color='blue', kde=False, height=6, aspect=3, kind='hist')\n\nprint(temp.min())\nprint(temp.max())\nprint(temp.mean())","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see the number of words in the excerpt range from 135 to 205. On average, we have 172 words in an excerpt.","metadata":{}},{"cell_type":"markdown","source":"# Most occuring words","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nstop = set(stopwords.words('english'))\n\ncorpus = []\ntitle = df.title.str.split()\ntitle = title.values.tolist()\ncorpus = [word for i in title for word in i]\n\nfrom collections import defaultdict\n\ndic = defaultdict(int)\n\nfor word in corpus:\n    if word in stop:\n        dic[word] += 1","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nfrom nltk.stem import PorterStemmer\n\nsns.set(rc={'figure.figsize':(15,15)})\n\nps = PorterStemmer()\ncounter = Counter(corpus)\nmost = counter.most_common()\n\nx, y = [], []\nlookup = []\nfor word,count in most[:120]:\n    if (word.lower() not in stop) and (ps.stem(word.lower()) not in lookup) and word.isalpha():\n        x.append(word)\n        y.append(count)\n        lookup.append(ps.stem(word.lower()))\n        \nsns.barplot(x=y,y=x)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# N-Gram Exploration\n\n## Most common bigrams","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef get_top_ngram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    fwords_freq = []\n    for i in words_freq:\n        temp = 0\n        for j in i[0].split():\n            if j in stop:\n                temp += 1\n        if temp != len(i[0].split()):\n            fwords_freq.append(i)\n    words_freq = fwords_freq\n    words_freq =sorted(words_freq, key=lambda x: x[1], reverse=True)\n    return words_freq[:100]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_n_bigrams = get_top_ngram(df.title, 2)[:20]\n\nx, y = map(list, zip(*top_n_bigrams)) \n\nsns.barplot(x=y, y=x, palette='hls')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Most common trigrams","metadata":{}},{"cell_type":"code","source":"top_n_trigrams = get_top_ngram(df.title, 3)[:20]\n\nx, y = map(list, zip(*top_n_trigrams)) \n\nsns.barplot(x=y, y=x, palette='coolwarm')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Topic Modelling","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk import word_tokenize\n\ndef preprocess_news(df):\n    corpus = []\n    stem = PorterStemmer()\n    lem = WordNetLemmatizer()\n    for news in df.title:\n        words = [w for w in word_tokenize(news) if (w.lower() not in stop and w.isalpha())]\n        words = [lem.lemmatize(w) for w in words if len(w) > 2]\n        corpus.append(words)\n    return corpus\n\ncorpus = preprocess_news(df)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\n\ndic = gensim.corpora.Dictionary(corpus)\nbow_corpus = [dic.doc2bow(doc) for doc in corpus]","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lda_model = gensim.models.LdaMulticore(bow_corpus, \n                                   num_topics = 5, \n                                   id2word = dic,                                    \n                                   passes = 10,\n                                   workers = 2)\nlda_model.show_topics()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyLDAvis\nimport pyLDAvis.gensim_models\n\nLDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dic)\npyLDAvis.display(LDAvis_prepared)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wordcloud Analysis","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data):\n    wordcloud = WordCloud(\n        background_color=None,\n        stopwords=stopwords,\n        max_words=1000,\n        max_font_size=30,\n        scale=4,\n        random_state=42,\n        mode='RGBA',\n        colormap='plasma')\n   \n    wordcloud=wordcloud.generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(corpus)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis\n\n## We'll first see the polarity of the publication titles.","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\n\nsns.set(rc={'figure.figsize':(6, 6)})\n\ndef polarity(text):\n    return TextBlob(text).sentiment.polarity\n\ndf.polarity_score = df.title.apply(lambda x : polarity(x))\ndf.polarity_score.hist(color='skyblue')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentiment(x):\n    if x < 0:\n        return 'neg'\n    elif x == 0:\n        return 'neu'\n    else:\n        return 'pos'\n\nsns.set(rc={'figure.figsize':(6, 6)})\ndf.sentiment = df.polarity_score.map(lambda x: sentiment(x))\n\nsns.barplot(x=df.sentiment.value_counts().index, y=df.sentiment.value_counts(), palette='coolwarm')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NER Analysis","metadata":{}},{"cell_type":"code","source":"! python -m spacy download en_core_web_sm","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ner(text):\n    doc = nlp(text)\n    return [X.label_ for X in doc.ents]\n\nent = df.title.apply(lambda x : ner(x))\nent = [x for sub in ent for x in sub]\ncounter = Counter(ent)\ncount = counter.most_common()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x, y = map(list, zip(*count))\nsns.set(rc={'figure.figsize':(15, 15)})\nsns.barplot(x=y, y=x, palette='husl')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install pandarallel","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pandarallel import pandarallel\n\npandarallel.initialize()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ner(text, ent=\"PERSON\"):\n    doc = nlp(text)\n    return [X.text for X in doc.ents if X.label_ == ent]\n\norg = df.title.parallel_apply(lambda x: ner(x))\norg = [i for x in org for i in x]\ncounter = Counter(org)\n\nx, y = map(list,zip(*counter.most_common(20)))\nsns.barplot(y, x, palette='coolwarm')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ner(text, ent=\"CARDINAL\"):\n    doc = nlp(text)\n    return [X.text for X in doc.ents if X.label_ == ent]\n\norg = df.title.parallel_apply(lambda x: ner(x))\norg = [i for x in org for i in x]\ncounter = Counter(org)\n\nx, y = map(list,zip(*counter.most_common(20)))\nsns.barplot(y, x, palette='coolwarm')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ner(text, ent=\"DATE\"):\n    doc = nlp(text)\n    return [X.text for X in doc.ents if X.label_ == ent]\n\norg = df.title.parallel_apply(lambda x: ner(x))\norg = [i for x in org for i in x]\ncounter = Counter(org)\n\nx, y = map(list,zip(*counter.most_common(20)))\nsns.barplot(y, x, palette='coolwarm')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ner(text, ent=\"GPE\"):\n    doc = nlp(text)\n    return [X.text for X in doc.ents if X.label_ == ent]\n\norg = df.title.parallel_apply(lambda x: ner(x))\norg = [i for x in org for i in x]\ncounter = Counter(org)\n\nx, y = map(list,zip(*counter.most_common(20)))\nsns.barplot(y, x, palette='coolwarm')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ner(text, ent=\"ORG\"):\n    doc = nlp(text)\n    return [X.text for X in doc.ents if X.label_ == ent]\n\norg = df.title.parallel_apply(lambda x: ner(x))\norg = [i for x in org for i in x]\ncounter = Counter(org)\n\nx, y = map(list,zip(*counter.most_common(20)))\nsns.barplot(y, x, palette='coolwarm')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# POS Tagging\n## We'll now do Part-of-Speech Tagging.\n\n**Here's the list of tags**:\n\nNoun (NN)- Joseph, London, table, cat, teacher, pen, city\n\nVerb (VB)- read, speak, run, eat, play, live, walk, have, like, are, is\n\nAdjective(JJ)- beautiful, happy, sad, young, fun, three\n\nAdverb(RB)- slowly, quietly, very, always, never, too, well, tomorrow\n\nPreposition (IN)- at, on, in, from, with, near, between, about, under\n\nDeterminer (DT) - one, many\n\nConjunction (CC)- and, or, but, because, so, yet, unless, since, if\n\nPronoun(PRP)- I, you, we, they, he, she, it, me, us, them, him, her, this\n\nInterjection (INT)- Ouch! Wow! Great! Help! Oh! Hey! Hi!","metadata":{}},{"cell_type":"code","source":"def pos(text):\n    pos = nltk.pos_tag(word_tokenize(text))\n    pos = list(map(list,zip(*pos)))[1]\n    return pos\n\ntags = df.title.parallel_apply(lambda x : pos(x))\ntags = [x for l in tags for x in l]\ncounter = Counter(tags)\n\nx, y = list(map(list,zip(*counter.most_common(6))))\nsns.barplot(x=y, y=x, palette='coolwarm')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_nouns(text):\n    noun = []\n    pos = nltk.pos_tag(word_tokenize(text))\n    for word, tag in pos:\n        if tag == 'NN' and word.isalpha():\n            noun.append(word)\n    return noun\n\nwords = df.title.parallel_apply(lambda x : get_nouns(x))\nwords = [x for l in words for x in l]\ncounter = Counter(words)\n\nx, y = list(map(list,zip(*counter.most_common(10))))\nsns.barplot(x=y, y=x, palette='magma')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_nouns(text):\n    noun = []\n    pos = nltk.pos_tag(word_tokenize(text))\n    for word, tag in pos:\n        if tag == 'JJ' and word.isalpha():\n            noun.append(word)\n    return noun\n\nwords = df.title.parallel_apply(lambda x : get_nouns(x))\nwords = [x for l in words for x in l]\ncounter = Counter(words)\n\nx, y = list(map(list,zip(*counter.most_common(10))))\nsns.barplot(x=y, y=x, palette='magma')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}