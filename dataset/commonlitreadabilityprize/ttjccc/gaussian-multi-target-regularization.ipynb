{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Improve performance and robustness with Gaussian Multi-Target Regularization","metadata":{}},{"cell_type":"markdown","source":"Hi all! In this notebook I will share a strategy that can easily be added to your model, which improves both model performance and robustness! I will also share a minimal implementation of this strategy.","metadata":{}},{"cell_type":"markdown","source":"## Motivation\n\n* there is a `standard_error` term in the training data, we should try to utilize it!\n\n## Introduction\n\n* First, \"gaussian multi-target regularization\" is just a random name I came up with.\n* The idea is to have a model that predicts multiple outputs, where the targets are sampled from a Gaussian distribution, based on `target` and `standard_error`\n\n## Benefits\n\nBefore diving into code, let's first think what are the potential benefits of this strategy.\n\n1. data augmentation\n    * sampling the target from a normal distribution should have a regularization effect\n2. ensemble\n    * multi-target means we will have multiple slightly different heads for ensemble, which should improve performance\n \n## Code\n\n* To make it short, I will only include code snippet copied from my local enviroment.\n\n\n## Result\n\n* Based on my local experiments, the proposed method is indeed better!\n    * The validation RMSE is lower compared to the baseline.\n    * The validation RMSE has lower variations between runs, compared to the baseline.\n* Since we are only increasing the number of output in the last layer, the increase in memory/compute is fairly negligible. (I tried at most 1024 targets.)\n* As shown below, the code change is also minimal, so try it out!\n* Please give this notebook a upvote if you find it useful!","metadata":{}},{"cell_type":"markdown","source":"## Baseline code","metadata":{}},{"cell_type":"code","source":"from torch import nn\nclass TrainingModule(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.crit = nn.MSELoss()\n\n    def forward(self, target=None, standard_error=None, **kwargs):\n        out = self.model(**kwargs)\n        logits = out.logits\n        loss = self.crit(logits.view(-1), target)\n        return loss\n\n    def predict(self, target=None, standard_error=None, **kwargs):\n        out = self.model(**kwargs)\n        logits = out.logits\n        return logits\n# model_name_or_path = 'roberta-base'\n# model = AutoModelForSequenceClassification.from_pretrained(\n#         model_name_or_path, num_labels=1\n# )\n# model = TrainingModule(model)\n# # dataset, dataloader\n# loss = model(**batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Code for proposed strategy","metadata":{}},{"cell_type":"code","source":"num_heads = 32\nclass TrainingModule(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n        self.crit = nn.MSELoss()\n\n    def forward(self, target=None, standard_error=None, **kwargs):\n        mean = einops.repeat(target, \"b -> b n\", n=num_heads)\n        std = einops.repeat(standard_error, \"b -> b n\", n=num_heads)\n        targets = torch.normal(mean=mean, std=std)\n        out = self.model(**kwargs)\n        logits = out.logits\n        loss = self.crit(logits, targets)\n        return loss\n\n    def predict(self, target=None, standard_error=None, **kwargs):\n        out = self.model(**kwargs)\n        logits = out.logits\n        return logits.mean(-1)\n# model_name_or_path = 'roberta-base'\n# model = AutoModelForSequenceClassification.from_pretrained(\n#         model_name_or_path, num_labels=num_heads\n# )\n# model = TrainingModule(model)\n# # dataset, dataloader\n# loss = model(**batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}