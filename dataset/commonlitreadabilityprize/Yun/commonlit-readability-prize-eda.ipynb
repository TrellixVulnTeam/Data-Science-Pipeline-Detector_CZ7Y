{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"こちらの方のほぼ写経 // this notebook is copied and modified from the link below.\n\nhttps://www.kaggle.com/ruchi798/commonlit-readability-prize-eda-baseline/comments#Pre-processing-excerpt-%E2%9C%82%EF%B8%8F","metadata":{}},{"cell_type":"markdown","source":"# Set up","metadata":{}},{"cell_type":"code","source":"# The installation of pycaret 2.3.2, the latest version came up with an error.\n# -------\n# ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n# pdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.4.1 which is incompatible.\n# -------\n# Therefore, I decided to install version 2.2.1 which happened to work.\n\n#!python -m pip download pycaret==2.2.1 -d ./pycaret-2.2.1\n!python -m pip install --find-links=../input/mylibraries pycaret==2.2.1\n!python -m pip install --find-links=../input/mylibraries/textstat-0.7.1 textstat==0.7.1\n\n# In case internet access is allowed, pip install is the easiest way.\n#!pip install pycaret\n#!pip install textstat","metadata":{"execution":{"iopub.status.busy":"2021-07-17T14:08:13.934548Z","iopub.execute_input":"2021-07-17T14:08:13.934972Z","iopub.status.idle":"2021-07-17T14:08:26.022514Z","shell.execute_reply.started":"2021-07-17T14:08:13.934924Z","shell.execute_reply":"2021-07-17T14:08:26.021573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport time\n\nimport pandas_profiling as pdp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom nltk import pos_tag\nfrom wordcloud import WordCloud,STOPWORDS\n\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport textstat\n\nfrom pycaret.regression import *","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-17T14:10:40.941021Z","iopub.execute_input":"2021-07-17T14:10:40.941735Z","iopub.status.idle":"2021-07-17T14:10:40.946414Z","shell.execute_reply.started":"2021-07-17T14:10:40.94169Z","shell.execute_reply":"2021-07-17T14:10:40.945791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-17T14:10:44.411074Z","iopub.execute_input":"2021-07-17T14:10:44.411401Z","iopub.status.idle":"2021-07-17T14:10:44.553833Z","shell.execute_reply.started":"2021-07-17T14:10:44.411365Z","shell.execute_reply":"2021-07-17T14:10:44.552998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Profile data","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.290881Z","iopub.status.idle":"2021-07-04T10:12:07.291342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.excerpt[0]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.292365Z","iopub.status.idle":"2021-07-04T10:12:07.292832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"excerpt (抜粋) から、人によってつけられた文章の読みやすさスコアみたいなの（target）をを当てる\n\nスコアは複数人でつけるため、人による採点ばらつき（standard error）がある","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,3,figsize=(18,6))\n\nsns.histplot(train_df['target'], ax=ax[0], kde=True, alpha=0.2)\nsns.histplot(train_df['standard_error'], ax=ax[1], kde=True, alpha=0.2)\n\nsns.histplot(\n    train_df, x=\"target\", y=\"standard_error\",\n    bins=30, discrete=(False, False), log_scale=(False, False),\n    cbar=True, cbar_kws=dict(shrink=.75), \n    ax=ax[2]\n)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.294015Z","iopub.status.idle":"2021-07-04T10:12:07.294457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.295407Z","iopub.status.idle":"2021-07-04T10:12:07.295885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile = pdp.ProfileReport(train_df)\nprofile","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.296829Z","iopub.status.idle":"2021-07-04T10:12:07.297271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"url, license は、訓練データの 70% が欠損している","metadata":{"execution":{"iopub.status.busy":"2021-06-28T07:33:57.728972Z","iopub.execute_input":"2021-06-28T07:33:57.729475Z","iopub.status.idle":"2021-06-28T07:33:57.736122Z","shell.execute_reply.started":"2021-06-28T07:33:57.729389Z","shell.execute_reply":"2021-06-28T07:33:57.734397Z"}}},{"cell_type":"code","source":"pdp.ProfileReport(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.298273Z","iopub.status.idle":"2021-07-04T10:12:07.2987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx,row in train_df.sort_values(by=['target'], ascending=False).head(3).iterrows():\n    print('index:' + str(idx) + ', target ' + str(row.target))\n    print(row.excerpt + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.299878Z","iopub.status.idle":"2021-07-04T10:12:07.300315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx,row in train_df.sort_values(by=['target'], ascending=False).tail(3).iterrows():\n    print('index:' + str(idx) + ', target ' + str(row.target))\n    print(row.excerpt + '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.301228Z","iopub.status.idle":"2021-07-04T10:12:07.301655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess Data\n以下の流れで処理\n1. 文章内のアルファベット以外を空白に変換\n2. stopwords の削除\n3. 単語のレンマ化","metadata":{"execution":{"iopub.status.busy":"2021-06-13T09:52:21.614457Z","iopub.execute_input":"2021-06-13T09:52:21.614796Z","iopub.status.idle":"2021-06-13T09:52:21.619322Z","shell.execute_reply.started":"2021-06-13T09:52:21.614768Z","shell.execute_reply":"2021-06-13T09:52:21.618051Z"}}},{"cell_type":"code","source":"excerpt1 = train_df['excerpt'].min()\nprint(\"Before preprocessing: \\n\")\nprint(excerpt1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-04T10:12:07.302632Z","iopub.status.idle":"2021-07-04T10:12:07.303117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e = re.sub(\"[^a-zA-Z]\", \" \", excerpt1) # アルファベット以外は空白に変換\ne = e.lower() # 小文字に変換\ne = nltk.word_tokenize(e) # tokenizer を使って単語に分割\ne[:8]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-04T10:12:07.304176Z","iopub.status.idle":"2021-07-04T10:12:07.304603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e = [word for word in e if not word in set(stopwords.words(\"english\"))] # stopwords に登録されている単語は除外する\nstopwords.words(\"english\")[:10] # ちなみに stopwords に登録されている単語はこんなの","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-04T10:12:07.305654Z","iopub.status.idle":"2021-07-04T10:12:07.306126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemma = nltk.WordNetLemmatizer()\ne = [lemma.lemmatize(word) for word in e] # lemmatizer を使ってlemmatizeする\nnltk.WordNetLemmatizer().lemmatize(\"dogs\") # lemmatize の例 dogs -> dog","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-04T10:12:07.30719Z","iopub.status.idle":"2021-07-04T10:12:07.307645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"e=\" \".join(e)\nprint(\"After preprocessing: \\n\")\nprint(e)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.30864Z","iopub.status.idle":"2021-07-04T10:12:07.309108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(data):\n    excerpt_processed=[]\n    lemma = nltk.WordNetLemmatizer()\n    for e in data['excerpt']:\n        e = re.sub(\"[^a-zA-Z]\", \" \", e) # アルファベット以外は空白に変換\n        e = e.lower() # 小文字に変換\n        e = nltk.word_tokenize(e) # tokenizer を使って単語に分割\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))] # stopwords に登録されている単語は除外する\n        e = [lemma.lemmatize(word) for word in e] # lemmatizer を使って lemmatize する\n        e=\" \".join(e)\n        excerpt_processed.append(e)\n    return excerpt_processed ","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.310097Z","iopub.status.idle":"2021-07-04T10:12:07.310528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['preprocessed_excerpt'] = preprocess(train_df)\ntest_df['preprocessed_excerpt'] = preprocess(test_df)\n\n# 時間がかかるので、保存しておく\n#train_df.to_csv(\"train_excerpt_preprocessed.csv\")\n#test_df.to_csv(\"test_excerpt_preprocessed.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.311568Z","iopub.status.idle":"2021-07-04T10:12:07.312042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 8))\nsns.countplot(y=\"license\",data=train_df,linewidth=3)\nplt.title(\"License Distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.312891Z","iopub.status.idle":"2021-07-04T10:12:07.31332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"頻出単語(unigram), 頻出bigram, trigram を抽出する","metadata":{}},{"cell_type":"code","source":"print('train data の数:', len(train_df['preprocessed_excerpt']))\nvec = CV(ngram_range=(1, 1)).fit(train_df['preprocessed_excerpt'])\nprint('vocabulary の種類:', len(vec.vocabulary_))\nprint('vocabulary の例:', list(vec.vocabulary_.keys())[:3])\n\nbow = vec.transform(train_df['preprocessed_excerpt'])\nprint('bag of words の shape', bow.shape)\n\nsum_words = bow.sum(axis=0)\nprint('sum of words の shape', sum_words.shape)\nprint('sum of words', sum_words)\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\nprint('頻出単語の例', words_freq[:5])\n\n# 登場回数で並べ替え\nwords_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\nprint('頻出単語上位', words_freq[:5])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.314255Z","iopub.status.idle":"2021-07-04T10:12:07.314694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    vec = CV().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CV(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CV(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.315866Z","iopub.status.idle":"2021-07-04T10:12:07.316329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"common_words = get_top_n_words(train_df['preprocessed_excerpt'], 20)\ncommon_words_df1 = pd.DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 6))\nax = sns.barplot(x='freq', y='word', data=common_words_df1,\n                 facecolor=(0, 0, 0, 0),linewidth=3,\n                 edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",20))\n\nplt.title(\"Top 20 unigrams\",font='Serif')\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");\n\ncommon_words = get_top_n_bigram(train_df['preprocessed_excerpt'], 20)\ncommon_words_df2 = pd.DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 6))\nax = sns.barplot(x='freq', y='word', data=common_words_df2,\n                 facecolor=(0, 0, 0, 0),linewidth=3,\n                 edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",20))\n\nplt.title(\"Top 20 bigrams\",font='Serif')\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");\n\ncommon_words = get_top_n_trigram(train_df['preprocessed_excerpt'], 20)\ncommon_words_df2 = pd.DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 6))\nax = sns.barplot(x='freq', y='word', data=common_words_df2,\n                 facecolor=(0, 0, 0, 0),linewidth=3,\n                 edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",20))\n\nplt.title(\"Top 20 trigrams\",font='Serif')\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.317386Z","iopub.status.idle":"2021-07-04T10:12:07.317857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"trigram の２位に登場する \"th\" は stop words に加えたほうがよい？","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(16,16))\nwc = WordCloud(stopwords=STOPWORDS,background_color=\"white\", \n               contour_width=2, contour_color='blue', width=1600, height=800,\n               max_words=150, max_font_size=256,random_state=42)\nwc.generate(' '.join(train_df['preprocessed_excerpt']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.318823Z","iopub.status.idle":"2021-07-04T10:12:07.319264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"stop words に 入れたほうがよいword:u \n動詞は過去形のものも残っている","metadata":{}},{"cell_type":"code","source":"def avg_word_len(df):\n    \"\"\" 登場する単語の文字数の平均を算出する \"\"\"\n    df = df.str.split().apply(\n        lambda x : [len(i) for i in x] # 各単語の文字数\n    ).map(lambda x: np.mean(x)) # 文字数の平均\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.32044Z","iopub.status.idle":"2021-07-04T10:12:07.320901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['excerpt'][0]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.321769Z","iopub.status.idle":"2021-07-04T10:12:07.322228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 文字数 // number of characters in the text\ntext_len = train_df['excerpt'].str.len()\ntext_len_pre = train_df['preprocessed_excerpt'].str.len()\ntext_len_dif = train_df['excerpt'].str.len() - train_df['preprocessed_excerpt'].str.len()\n\n# 単語の文字数の平均 // average of number of characters in words present in the text\navg_text = avg_word_len(train_df['excerpt'])\navg_text_pre = avg_word_len(train_df['preprocessed_excerpt'])\n\n# 単語数 // number of words present in the text\nlexicon_count = [] \nlexicon_count_pre = []\nlexicon_count_dif = []\n# 文章数 // number of sentences present in the text\nsentence_count = []\n# 文章あたりの単語数 // mean/max/min number of words in each sentence\nmean_lexicon_count_per_st = []\nmax_lexicon_count_per_st = []\nmin_lexicon_count_per_st = []\n\nfor i in range(len(train_df)):\n    lc = textstat.lexicon_count(train_df['excerpt'][i])\n    lcp = textstat.lexicon_count(train_df['preprocessed_excerpt'][i])\n    lcd = lc - lcp\n    sc = textstat.sentence_count(train_df['excerpt'][i])\n    lexicon_count.append(lc)\n    lexicon_count_pre.append(lcp)\n    lexicon_count_dif.append(lcd)\n    sentence_count.append(sc)\n    mean_lexicon_count_per_st.append(np.mean([textstat.lexicon_count(x) for x in train_df['excerpt'][i].split(\".\")]))\n    max_lexicon_count_per_st.append(np.max([textstat.lexicon_count(x) for x in train_df['excerpt'][i].split(\".\")]))\n    min_lexicon_count_per_st.append(np.min([textstat.lexicon_count(x) for x in train_df['excerpt'][i].split(\".\") if textstat.lexicon_count(x) > 1]))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.323425Z","iopub.status.idle":"2021-07-04T10:12:07.323889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_props = train_df.copy()\ntext_props['text_len'] = text_len\ntext_props['text_len_pre'] = text_len_pre\ntext_props['text_len_dif'] = text_len_dif\ntext_props['lexicon_count'] = lexicon_count\ntext_props['lexicon_count_pre'] = lexicon_count_pre\ntext_props['lexicon_count_dif'] = lexicon_count_dif # preprocess と original の差\ntext_props['avg_text'] = avg_text\ntext_props['avg_text_pre'] = avg_text_pre\ntext_props['sentence_count'] = sentence_count\ntext_props['mean_lexicon_count_per_st'] = mean_lexicon_count_per_st\ntext_props['max_lexicon_count_per_st'] = max_lexicon_count_per_st\ntext_props['min_lexicon_count_per_st'] = min_lexicon_count_per_st\ntext_props.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.324849Z","iopub.status.idle":"2021-07-04T10:12:07.325297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_distribution(col1,col2,title1,title2):\n    fig, ax = plt.subplots(1,2,figsize=(12,6))\n    sns.kdeplot(data=text_props, x=col1,label=\"Excerpt\",ax=ax[0])\n    sns.kdeplot(data=text_props, x=col2,label=\"Excerpt preprocessed\",ax=ax[0])\n    ax[0].set_title(title1,font=\"Serif\")\n    ax[0].legend()\n\n    sns.scatterplot(data=text_props,x=col1,y='target',label=\"Excerpt\",ax=ax[1],markers='.')\n    sns.scatterplot(data=text_props,x=col2,y='target',label=\"Excerpt preprocessed\", ax=ax[1],markers='.', alpha=0.3)\n    ax[1].set_title(title2,font=\"Serif\")\n    ax[1].legend()\n\n    plt.show()\n\nplot_distribution(\"text_len\",\"text_len_pre\",\"Character count distribution\",\"Character count vs Target\")\nplot_distribution(\"lexicon_count\",\"lexicon_count_pre\",\"Word count distribution\",\"Word count vs Target\")\nplot_distribution(\"avg_text\",\"avg_text_pre\", \"Average word length distribution\",\"Average word length vs Target\")\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=sentence_count,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Sentence count distribution\",font=\"Serif\")\nax[0].set_xlabel(\"sentence_count\")\nsns.scatterplot(data=text_props,x='sentence_count',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Sentence count vs Target\",font=\"Serif\")\nplt.show()\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=text_len_dif,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Character count reduction distribution\",font=\"Serif\")\nax[0].set_xlabel(\"Character count reduction by preprocess\")\nsns.scatterplot(data=text_props,x='text_len_dif',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Character count reduction by preprocess vs Target\",font=\"Serif\")\nplt.show()\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=lexicon_count_dif,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Word count reduction distribution\",font=\"Serif\")\nax[0].set_xlabel(\"Word count reduction by preprocess\")\nsns.scatterplot(data=text_props,x='lexicon_count_dif',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Word count reduction by preprocess vs Target\",font=\"Serif\")\nplt.show()\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=mean_lexicon_count_per_st,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Mean of word count per centence\",font=\"Serif\")\nax[0].set_xlabel(\"Word count\")\nsns.scatterplot(data=text_props,x='mean_lexicon_count_per_st',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Mean of word count per centence vs Target\",font=\"Serif\")\nplt.show()\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=max_lexicon_count_per_st,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Max of word count per centence\",font=\"Serif\")\nax[0].set_xlabel(\"Word count\")\nsns.scatterplot(data=text_props,x='max_lexicon_count_per_st',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Max of word count per centence vs Target\",font=\"Serif\")\nplt.show()\n\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=min_lexicon_count_per_st,label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Min of word count per centence\",font=\"Serif\")\nax[0].set_xlabel(\"Word count\")\nsns.scatterplot(data=text_props,x='min_lexicon_count_per_st',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Min of word count per centence vs Target\",font=\"Serif\")\nplt.show()\n\nnum_cols = ['text_len','text_len_pre','text_len_dif', \n            'lexicon_count','lexicon_count_pre','lexicon_count_dif', \n            'avg_text','avg_text_pre','sentence_count',\n            'mean_lexicon_count_per_st', 'max_lexicon_count_per_st', 'min_lexicon_count_per_st',\n            'target']\ncorr = text_props[num_cols].corr()\n\nfig = plt.figure(figsize=(8,8),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='BuPu', robust=True, center=0, annot=True,\n            square=True, linewidths=.5)\nplt.title('Correlation of text properties', fontsize=15,font=\"Serif\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.326482Z","iopub.status.idle":"2021-07-04T10:12:07.326966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- avg_text (単語の文字数の平均)やtext_len（文字数）は target と負の相関→文字数が少ないほど読みやすい\n- sentence_count（文章の数）は、target と弱い正の相関→文章数が少ないほど読みやすい\n- いずれも直観的な感覚と一致する","metadata":{}},{"cell_type":"markdown","source":"### 文章中に登場する単語の品詞をカウントする","metadata":{}},{"cell_type":"code","source":"text_props['pos_tags'] = text_props['preprocessed_excerpt'].str.split().map(pos_tag)\n\nprint(text_props['preprocessed_excerpt'][0][:50])\nprint(text_props['pos_tags'][0][:5])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.327917Z","iopub.status.idle":"2021-07-04T10:12:07.328354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_tags(pos_tags):\n    tag_count = {}\n    for word,tag in pos_tags:\n        if tag in tag_count:\n            tag_count[tag] += 1\n        else:\n            tag_count[tag] = 1\n    return tag_count\n\ntext_props['tag_counts'] = text_props['pos_tags'].map(count_tags)\nprint(text_props['tag_counts'].head())","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.329229Z","iopub.status.idle":"2021-07-04T10:12:07.329654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'JJ' とか 'NNS' とかが品詞の種類、後に続く value が登場回数を表す\n\n定義はoriginal kernel 参照\n\nhttps://www.kaggle.com/ruchi798/commonlit-readability-prize-eda-baseline/comments?scriptVersionId=62607090&cellId=47","metadata":{}},{"cell_type":"code","source":"set_pos = set([tag for tags in text_props['tag_counts'] for tag in tags])\ntag_cols = list(set_pos)\n\nfor tag in tag_cols:\n    text_props[tag] = text_props['tag_counts'].map(lambda x: x.get(tag, 0))\ntext_props[tag_cols].head()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.3307Z","iopub.status.idle":"2021-07-04T10:12:07.331171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos = text_props[tag_cols].sum().sort_values(ascending = False)\nplt.figure(figsize=(16,8))\nax = sns.barplot(x=pos.index, y=pos.values)\nplt.xticks(rotation = 50)\nax.set_yscale('log')\nplt.title('POS tags frequency',fontsize=15,font=\"Serif\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.332422Z","iopub.status.idle":"2021-07-04T10:12:07.332877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- [original の kernel](https://www.kaggle.com/ruchi798/commonlit-readability-prize-eda-baseline/comments?scriptVersionId=62607090&cellId=54)によると、textstat には読みやすさに関するスコアの算出方法が実装されているそうな\n","metadata":{}},{"cell_type":"code","source":"flesch_re, flesch_kg, fog_scale, automated_r,coleman, linsear, text_standard  = ([] for i in range(7))\nfor i in range(len(text_props)):\n    flr = textstat.flesch_reading_ease(train_df['excerpt'][i])\n    flkg = textstat.flesch_kincaid_grade(train_df['excerpt'][i])\n    fs = textstat.gunning_fog(train_df['excerpt'][i])\n    ar = textstat.automated_readability_index(train_df['excerpt'][i])\n    cole = textstat.coleman_liau_index(train_df['excerpt'][i])\n    lins = textstat.linsear_write_formula(train_df['excerpt'][i])\n    ts = textstat.text_standard(train_df['excerpt'][i])\n    \n    flesch_re.append(flr)\n    flesch_kg.append(flkg)\n    fog_scale.append(fs)\n    automated_r.append(ar)\n    coleman.append(cole)\n    linsear.append(lins)\n    text_standard.append(ts)\n    \ntext_props['flesch_re'] = flesch_re\ntext_props['flesch_kg'] = flesch_kg\ntext_props['fog_scale'] = fog_scale\ntext_props['automated_r'] = automated_r\ntext_props['coleman'] = coleman\ntext_props['linsear'] = linsear\ntext_props['text_standard'] = text_standard","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.333833Z","iopub.status.idle":"2021-07-04T10:12:07.334267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"readability_cols = ['flesch_re','flesch_kg','fog_scale','automated_r','coleman','linsear','text_standard','target']\n\ncorr = text_props[readability_cols].corr()\nfig = plt.figure(figsize=(8,8),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='PuBuGn', robust=True, center=0,\n            square=True, linewidths=.5,annot=True)\nplt.title('Correlation of readability tests', fontsize=15,font=\"Serif\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.335244Z","iopub.status.idle":"2021-07-04T10:12:07.335677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Flesch Readability Ease の値と target には正の相関","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(12,6))\nsns.kdeplot(data=text_props, x=flesch_re,ax=ax[0])\nax[0].set_title(\"Flesch Reading Ease Distribution\",font=\"Serif\")\nax[0].set_xlabel(\"Flesch Reading Ease Test Score\")\nsns.scatterplot(data=text_props,x='flesch_re',y='target',ax=ax[1],markers='.')\nax[1].set_title(\"Flesch Reading Ease Test Score vs Target\",font=\"Serif\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.336727Z","iopub.status.idle":"2021-07-04T10:12:07.33718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ここまでの処理を関数化して、学習データ、テストデータを処理しておく","metadata":{}},{"cell_type":"code","source":"def count_character_lexicon_sentence(df):\n\n    # 文章中に登場する文字の数 // number of characters in the text\n    text_len = df['excerpt'].str.len()\n    text_len_pre = df['preprocessed_excerpt'].str.len()\n    text_len_dif = df['excerpt'].str.len() - df['preprocessed_excerpt'].str.len()\n\n    # 文章中に登場する単語の文字数の平均 // average of number of characters in words present in the text\n    avg_text = avg_word_len(df['excerpt'])\n    avg_text_pre = avg_word_len(df['preprocessed_excerpt'])\n\n    # 単語数 // number of words present in the text\n    lexicon_count = [] \n    lexicon_count_pre = []\n    lexicon_count_dif = []\n    # 文章数 // number of sentences present in the text\n    sentence_count = []\n    # 文章あたりの単語数 // mean/max/min number of words in each sentence\n    mean_lexicon_count_per_st = []\n    max_lexicon_count_per_st = []\n    min_lexicon_count_per_st = []\n    \n    for i in range(len(df)):\n        lc = textstat.lexicon_count(df['excerpt'][i])\n        lcp = textstat.lexicon_count(df['preprocessed_excerpt'][i])\n        lcd = lc - lcp\n        sc = textstat.sentence_count(df['excerpt'][i])\n        lexicon_count.append(lc)\n        lexicon_count_pre.append(lcp)\n        lexicon_count_dif.append(lcd)\n        sentence_count.append(sc)\n        mean_lexicon_count_per_st.append(np.mean([textstat.lexicon_count(x) for x in df['excerpt'][i].split(\".\")]))\n        max_lexicon_count_per_st.append(np.max([textstat.lexicon_count(x) for x in df['excerpt'][i].split(\".\")]))\n        min_lexicon_count_per_st.append(np.min([textstat.lexicon_count(x) for x in df['excerpt'][i].split(\".\") if textstat.lexicon_count(x) > 1]))\n\n    df['text_len'] = text_len\n    df['text_len_pre'] = text_len_pre\n    df['text_len_dif'] = text_len_dif\n    df['lexicon_count'] = lexicon_count\n    df['lexicon_count_pre'] = lexicon_count_pre\n    df['lexicon_count_dif'] = lexicon_count_dif # preprocess と original の差\n    df['avg_text'] = avg_text\n    df['avg_text_pre'] = avg_text_pre\n    df['sentence_count'] = sentence_count\n    df['mean_lexicon_count_per_st'] = mean_lexicon_count_per_st\n    df['max_lexicon_count_per_st'] = max_lexicon_count_per_st\n    df['min_lexicon_count_per_st'] = min_lexicon_count_per_st\n    return df\n\n\ndef count_pos(df1, df2):\n    df1['pos_tags'] = df1['preprocessed_excerpt'].str.split().map(pos_tag)\n    df1['tag_counts'] = df1['pos_tags'].map(count_tags)\n\n    df2['pos_tags'] = df2['preprocessed_excerpt'].str.split().map(pos_tag)\n    df2['tag_counts'] = df2['pos_tags'].map(count_tags)\n    \n    # train, test 両方に登場するposのみを扱う\n    set_pos1 = set([tag for tags in df1['tag_counts'] for tag in tags])\n    set_pos2 = set([tag for tags in df2['tag_counts'] for tag in tags])\n    tag_cols = list(set_pos1.intersection(set_pos2))\n\n    for tag in tag_cols:\n        df1[tag] = df1['tag_counts'].map(lambda x: x.get(tag, 0))\n        df2[tag] = df2['tag_counts'].map(lambda x: x.get(tag, 0))\n    return df1,df2\n\ndef extract_readability(df):\n    flesch_re, flesch_kg, fog_scale, automated_r,coleman, linsear, text_standard  = ([] for i in range(7))\n    for i in range(len(df)):\n        flr = textstat.flesch_reading_ease(df['excerpt'][i])\n        flkg = textstat.flesch_kincaid_grade(df['excerpt'][i])\n        fs = textstat.gunning_fog(df['excerpt'][i])\n        ar = textstat.automated_readability_index(df['excerpt'][i])\n        cole = textstat.coleman_liau_index(df['excerpt'][i])\n        lins = textstat.linsear_write_formula(df['excerpt'][i])\n        ts = textstat.text_standard(df['excerpt'][i])\n    \n        flesch_re.append(flr)\n        flesch_kg.append(flkg)\n        fog_scale.append(fs)\n        automated_r.append(ar)\n        coleman.append(cole)\n        linsear.append(lins)\n        text_standard.append(ts)\n    \n    df['flesch_re'] = flesch_re\n    df['flesch_kg'] = flesch_kg\n    df['fog_scale'] = fog_scale\n    df['automated_r'] = automated_r\n    df['coleman'] = coleman\n    df['linsear'] = linsear\n    df['text_standard'] = text_standard\n\n    return df\n\ntrain_df = count_character_lexicon_sentence(train_df)\ntest_df = count_character_lexicon_sentence(test_df)\n\ntrain_df, test_df = count_pos(train_df, test_df)\n    \ntrain_df = extract_readability(train_df)\ntest_df = extract_readability(test_df)\n\ntrain_df.to_csv(\"train_excerpt_preprocessed.csv\")\ntest_df.to_csv(\"test_excerpt_preprocessed.csv\")\n\n#train_df = pd.read_csv(\"train_excerpt_preprocessed.csv\", index_col=0)\n#test_df = pd.read_csv(\"test_excerpt_preprocessed.csv\", index_col=0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-04T10:12:07.338479Z","iopub.status.idle":"2021-07-04T10:12:07.338939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.339983Z","iopub.status.idle":"2021-07-04T10:12:07.340421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.341402Z","iopub.status.idle":"2021-07-04T10:12:07.341846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KeyError: '[\\'WP\\', \\'TO\\', \"\\'\\'\", \\'WP$\\', \\'NNPS\\', \\'POS\\', \\'WDT\\', \\'EX\\', \\'$\\', \\'PRP$\\', \\'SYM\\', \\'PRP\\', \\'PDT\\', \\'NNP\\', \\'RBS\\'] not in index'set","metadata":{}},{"cell_type":"code","source":"reg = setup(data = train_df, \n             target = 'target',\n             #numeric_imputation = 'mean',\n             categorical_features = ['text_standard'], \n             ignore_features = ['id', 'url_legal', 'license', 'standard_error', 'excerpt', 'preprocessed_excerpt',\n                               'pos_tags', 'tag_counts'],\n             normalize = True,\n             silent = True,\n             session_id = 123)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.342886Z","iopub.status.idle":"2021-07-04T10:12:07.343323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\ncompare_models()\nprint('elapsed time : ', time.time() - start)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.34439Z","iopub.status.idle":"2021-07-04T10:12:07.344855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nstart = time.time()\nridge = create_model('ridge')\nprint('elapsed time : ', time.time() - start)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.345894Z","iopub.status.idle":"2021-07-04T10:12:07.346331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\ntuned = tune_model(ridge, optimize='RMSE')\nprint('elapsed time : ', time.time() - start)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.347288Z","iopub.status.idle":"2021-07-04T10:12:07.347764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction\npredictions = predict_model(tuned, data = test_df)\n\n# submission file\nsubmission_df = pd.DataFrame({'id': test_df.id, 'target': 0})\nsubmission_df.target = predictions['Label']\n\nsubmission_df.to_csv('./submission.csv', index=False)\n\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2021-07-04T10:12:07.348686Z","iopub.status.idle":"2021-07-04T10:12:07.349164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"最後まで読んでいただきどうもありがとうございます。少しでもお役に立てそうと感じていただけたら、Upvoteよろしくお願いいたします。とても励みになります!!\nThank for your iterest. Please upvote if you think this notebook would be helpful for you. It is really encouraging me.","metadata":{}}]}