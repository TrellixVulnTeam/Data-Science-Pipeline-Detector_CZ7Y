{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Beginner's gradual improvement from 0.909 to 0.779 (English / 日本語)**\n\nThis is the first time for me to participate the Competition after joining Kaggle 2 months ago. I intend to accumulate my experiences by joinnig anything. Below is the record of gradual score improvement which a beginner like me made after trial and error.\n\nKaggleを始めて2か月、始めてCompetitionに参加してみる。Titanicも決して上位ではないが、とにかく色々参加して経験値を積んでいこうと思う。初心者が四苦八苦して、パブリックスコアを0.909→0.900→0.885→0.860→0.798→0.779へと、徐々に改善していった記録です。","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-10T02:37:43.054324Z","iopub.execute_input":"2021-07-10T02:37:43.054654Z","iopub.status.idle":"2021-07-10T02:37:43.06254Z","shell.execute_reply.started":"2021-07-10T02:37:43.054623Z","shell.execute_reply":"2021-07-10T02:37:43.0619Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nimport nltk\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport nltk\nfrom nltk.tokenize import word_tokenize","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:37:45.889572Z","iopub.execute_input":"2021-07-10T02:37:45.890048Z","iopub.status.idle":"2021-07-10T02:37:48.552907Z","shell.execute_reply.started":"2021-07-10T02:37:45.890005Z","shell.execute_reply":"2021-07-10T02:37:48.55196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nall_df = pd.concat([train, test], sort = False).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:37:48.554113Z","iopub.execute_input":"2021-07-10T02:37:48.554391Z","iopub.status.idle":"2021-07-10T02:37:48.682462Z","shell.execute_reply.started":"2021-07-10T02:37:48.554365Z","shell.execute_reply":"2021-07-10T02:37:48.681624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['excerpt_len'] = train['excerpt'].apply(lambda x : len(x))\ntrain['excerpt_wordCnt'] = train['excerpt'].apply(lambda x : len(x.split(' ')))\ntrain['excerpt_sentCnt'] = train['excerpt'].apply(lambda x : len(x.split('.')))\ntrain[\"mean_WordPerSent\"] = train[\"excerpt_wordCnt\"]/train[\"excerpt_sentCnt\"]\ntrain[\"mean_LenPerWord\"] = train[\"excerpt_len\"]/train[\"excerpt_wordCnt\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:37:50.903366Z","iopub.execute_input":"2021-07-10T02:37:50.903726Z","iopub.status.idle":"2021-07-10T02:37:50.951015Z","shell.execute_reply.started":"2021-07-10T02:37:50.903695Z","shell.execute_reply":"2021-07-10T02:37:50.950153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When think about \"what is readability?\", what I came up with at first was \"less sentences, short sentences, less words\". Based on this easy idea, tried to use number of words/sentences which were created by easy method, as well as average lengths...\n\n\n「読みやすさとは何か？」を考えたとき、まず思いつくのは「文章が少ない、各文が短い、単語が少ない」ということかな？ということで、文字数・単語数・文章数」を特徴量として出してみる。単語数・文章数は簡易的にスペース・ピリオドで区切った数とする。それらを割り算して文章当たりの平均単語数・単語当たりの文字数も出してみる。","metadata":{}},{"cell_type":"code","source":"X_train = train.drop([\"url_legal\",\"license\",\"target\",\"excerpt\",\"standard_error\"],axis=1)\nY_train = train[\"target\"]\n\nX_train[\"id\"] = X_train[\"id\"].astype(\"category\")\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:37:53.634788Z","iopub.execute_input":"2021-07-10T02:37:53.635114Z","iopub.status.idle":"2021-07-10T02:37:53.66349Z","shell.execute_reply.started":"2021-07-10T02:37:53.635085Z","shell.execute_reply":"2021-07-10T02:37:53.662535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From a quick look, it seems 'not so bad', and try to submit using these features as first try. I'll try to use LightGBM which I'm familiar with, after cross-validation.\n\n\nまずまずのようにも見えるので、とりあえずこれらの特徴量を使って最初のSubmitをしてみる。使い慣れたLightGBMで、クロスバリデーションをした後、予測してみる。","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits = 3)\nmodels = []\nrmses =[]\nlgbm_params ={\"objective\":\"regression\", \"random_seed\":1234}\n\nfor train_index, val_index in kf.split(X_train):\n    XX_train = X_train.iloc[train_index]\n    XX_valid = X_train.iloc[val_index]\n    YY_train = Y_train.iloc[train_index]\n    YY_valid = Y_train.iloc[val_index]\n    \n    lgbm_train = lgbm.Dataset(XX_train, YY_train)\n    lgbm_eval = lgbm.Dataset(XX_valid, YY_valid)\n    \n    model_lgbm = lgbm.train(lgbm_params,\n                           lgbm_train,\n                           valid_sets = lgbm_eval,\n                           num_boost_round = 100,\n                           early_stopping_rounds = 20,\n                           verbose_eval = 10,\n                           )\n    y_pred = model_lgbm.predict(XX_valid, num_iteration = model_lgbm.best_iteration)\n    tmp_rmse = np.sqrt(mean_squared_error(YY_valid, y_pred))\n    print (tmp_rmse)\n    \n    models.append(model_lgbm)\n    rmses.append(tmp_rmse)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:37:57.242616Z","iopub.execute_input":"2021-07-10T02:37:57.243093Z","iopub.status.idle":"2021-07-10T02:37:57.557857Z","shell.execute_reply.started":"2021-07-10T02:37:57.243064Z","shell.execute_reply":"2021-07-10T02:37:57.557089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(Prediction portion omitted) First submission. The score was 0.909...\"Bad!\" \n（予測部分は省略）まずは最初のSubmit。スコアの結果は\"0.909\"・・・「低っ！」","metadata":{}},{"cell_type":"markdown","source":"Try to consider other Features. What came up is the idea \"Readability = written by easy words\", but not sure how to capture 'easy words'... I could say \"the easier and easier, the more and more frequently appears\". Based on this idea, try to use Countvectorizer. \n\n\n他の特徴量を考えてみる。読みやすさ＝「簡単な単語で書かれている」ということかな？とも思うのだが、どうやって「簡単な」を把握すれば良いか分からない・・・「簡単な単語ほど何度も出てくる」と言えなくもないので、単純にCountvectorizerで出現頻度を加えてみる。既存の特徴量と合わせて学習させるため、合算してPandasのデータフレームに追加する。","metadata":{}},{"cell_type":"code","source":"countvec = CountVectorizer(stop_words = \"english\")\ncv = countvec.fit_transform(all_df[\"excerpt\"])\ndfcv = cv.toarray()\ndfcv_sum = pd.DataFrame(np.sum(dfcv,axis=1))\ndfcv_sum.columns = [\"excerpt_cv\"]\n\nall_df = pd.concat([train, test], sort = False).reset_index(drop=True)\nall_df['excerpt_len'] = all_df['excerpt'].apply(lambda x : len(x))\nall_df['excerpt_wordCnt'] = all_df['excerpt'].apply(lambda x : len(x.split(' ')))\nall_df['excerpt_sentCnt'] = all_df['excerpt'].apply(lambda x : len(x.split('.')))\nall_df[\"mean_WordPerSent\"] = all_df[\"excerpt_wordCnt\"]/all_df[\"excerpt_sentCnt\"]\nall_df[\"mean_LenPerWord\"] = all_df[\"excerpt_len\"]/all_df[\"excerpt_wordCnt\"]\n\nall_df = pd.concat([all_df, dfcv_sum], axis=1)\nX_train = all_df[~all_df[\"target\"].isnull()]\nX_train = X_train.drop([\"url_legal\",\"license\",\"target\",\"excerpt\",\"standard_error\"],axis=1)\nY_train = train[\"target\"]\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:38:03.950737Z","iopub.execute_input":"2021-07-10T02:38:03.951065Z","iopub.status.idle":"2021-07-10T02:38:04.767532Z","shell.execute_reply.started":"2021-07-10T02:38:03.951035Z","shell.execute_reply":"2021-07-10T02:38:04.766569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Try to submit after adding the sum of Countetorizer as a feature. The score becomes 0.99. Slightly improved, but alomost the same...Based on the idea that DIFFICULT sentence includes difficult words which rarely appear, try to add Tfidf.\n\n\nCountvectorizerの合算値を特徴量に加えて再度Submitしてみる。スコアの結果は「0.900」。微妙に改善したが殆ど変わらない。読みにくい文章にはあまり使われないムズカシイ単語が出てくる、ということも言えるかなという考えで、Tfidfも特徴量に加えてみる。","metadata":{}},{"cell_type":"code","source":"tfvec = TfidfVectorizer(stop_words=\"english\")\ntfv = tfvec.fit_transform(all_df[\"excerpt\"])\ndftfv = tfv.toarray()\n\ndftfv_sum = pd.DataFrame(np.sum(dftfv,axis=1))\ndftfv_sum.columns = [\"excerpt_tfv\"]\n\nall_df = pd.concat([all_df, dfcv_sum, dftfv_sum], axis=1)\nX_train = all_df[~all_df[\"target\"].isnull()]\nX_train = X_train.drop([\"url_legal\",\"license\",\"target\",\"excerpt\",\"standard_error\"],axis=1)\nY_train = train[\"target\"]\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:39:07.608936Z","iopub.execute_input":"2021-07-10T02:39:07.609487Z","iopub.status.idle":"2021-07-10T02:39:08.687961Z","shell.execute_reply.started":"2021-07-10T02:39:07.609441Z","shell.execute_reply":"2021-07-10T02:39:08.686925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Predict again using the newly-added Tfidif sum. The score shows 0.885. Slightly improved again, but still alomost the same...By chance, it may have been bad that I used the easy method to separate using a space or a period in order to count the number of words or sentences. try to separate using the tokenizer.\n\n\n新たに加えたTfidf合算値も使って再度予測。結果は0.885。ちょっと良くなったかなぁ。でもやはり余り変わらない。単語や分の数を数えるのにスペースで区切ったりピリオドで区切ったりと簡易的な方法を用いたのが良くなかったのかもしれない。tokenizerで区切ってみる。","metadata":{}},{"cell_type":"code","source":"all_df['tokenized_excerpt'] = [word_tokenize(p.lower()) for p in all_df[\"excerpt\"]]\nall_df['word_count_tk'] = all_df['tokenized_excerpt'].apply(lambda x: len(x))\nall_df[\"avg_LenPerWord\"] = all_df[\"excerpt_len\"]/all_df[\"excerpt_wordCnt\"]\nall_df['sent_excerpt'] = [nltk.tokenize.sent_tokenize(x) for x in all_df[\"excerpt\"]]\nall_df['excerpt_sentCnt'] = all_df['sent_excerpt'].apply(lambda x: len(x))\nall_df[\"avg_WordPerSent\"] = all_df[\"excerpt_wordCnt\"]/all_df[\"excerpt_sentCnt\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-10T02:43:58.939173Z","iopub.execute_input":"2021-07-10T02:43:58.939603Z","iopub.status.idle":"2021-07-10T02:44:03.616372Z","shell.execute_reply.started":"2021-07-10T02:43:58.939567Z","shell.execute_reply":"2021-07-10T02:44:03.615394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Try to predict again and submit after re-counting words or sentences using Tokenizer. The score shows 0.860. Again, a little bit improved. I'd like to expect more improvement. Reconsider whether my first approach to count words or sentences.. Was it really correct? As for the countvectorizer or tfidfvectorizer as well, would it be better to use them as an array, rather than sum up? \n\n\nTokenizerを使って単語数・文数・平均値等を数えなおし、再度予測をやり直しSubmitしてみる。結果は0.860。また微妙に少し改善。徐々には改善しているものの、もう少し大幅に改善したい。最初に思いついた単語数とか文章数とかの特徴量って、本当に有効だったのだろうか？CountvectorizerとかTfidfvectorizerとかも配列を合算して値として用いているがそのまま配列として使ったほうが良かったりしないか？","metadata":{}},{"cell_type":"code","source":"import scipy\n\ncountvec = CountVectorizer()\ncv = countvec.fit_transform(all_df[\"excerpt\"])\n\ntfvec = TfidfVectorizer(stop_words=\"english\")\ntfv = tfvec.fit_transform(all_df[\"excerpt\"])\n\nX = scipy.sparse.hstack((cv, tfv)).tocsr()\n\nX_train = X[:len(train)]\nX_test = X[len(train):]\nY_train = train[\"target\"]\n\nX_train = X_train.toarray()\nX_test = X_test.toarray()\n\nX_train = pd.DataFrame(X_train)\nX_test = pd.DataFrame(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-10T03:02:43.856438Z","iopub.execute_input":"2021-07-10T03:02:43.856766Z","iopub.status.idle":"2021-07-10T03:02:45.880162Z","shell.execute_reply.started":"2021-07-10T03:02:43.856736Z","shell.execute_reply":"2021-07-10T03:02:45.879051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Try to predict and submit again, using Countectorizer and Tfidfvectorizer as an array and not using number of words / sentences. The score shows 0.798. Improvement is the biggest. As expected, it'd be better not to stick to the number of words / sentences. Aiming a bit more improvement, try to optimize the parameters using Optuna.\n\n\n単語数や文章数は使わずに、CountvectorizerとTfidfvectorizerをそのまま配列として用い、再度予測・submitしてみる。結果は0.798。これまでより改善幅が大きい。やっぱり変に単語数や文章数を用いないほうが良さそう。もう少し改善しないかなぁということでOptunaでパラメーターを最適化してみる。","metadata":{}},{"cell_type":"code","source":"models = []\nrmses =[]\nlgbm_params ={\n    \"objective\":\"regression\",\n    \"random_seed\":1234,\n    \"learning_rate\":0.05,\n    \"n_estimators\":1000,\n    'num_leaves': 64,\n    'max_bin': 82,\n    'bagging_fraction': 0.7315391015500504,\n    'bagging_freq': 3,\n    'feature_fraction': 0.41032549973286436,\n    'min_data_in_leaf': 13,\n    'min_sum_hessian_in_leaf': 4\n}\n\nfor train_index, val_index in kf.split(X_train):\n    XX_train = X_train.iloc[train_index]\n    XX_valid = X_train.iloc[val_index]\n    YY_train = Y_train.iloc[train_index]\n    YY_valid = Y_train.iloc[val_index]\n    \n    lgbm_train = lgbm.Dataset(XX_train, YY_train)\n    lgbm_eval = lgbm.Dataset(XX_valid, YY_valid)\n    \n    model_lgbm = lgbm.train(lgbm_params,\n                           lgbm_train,\n                           valid_sets = lgbm_eval,\n                           num_boost_round = 100,\n                           early_stopping_rounds = 20,\n                           verbose_eval = 10,\n                           )\n    y_pred = model_lgbm.predict(XX_valid, num_iteration = model_lgbm.best_iteration)\n    tmp_rmse = np.sqrt(mean_squared_error(YY_valid, y_pred))\n    print (tmp_rmse)\n    \n    models.append(model_lgbm)\n    rmses.append(tmp_rmse)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Execute LightGBM again after setting the best parameters found by Optuna.\n\n\nOptunaで得られたベストなパラメーターをセットし、LightGBMを再度実行。","metadata":{}},{"cell_type":"code","source":"preds = []\n\nfor model in models:\n    pred = model.predict(X_test)\n    preds.append(pred)\n    \npreds_array = np.array(preds)\npreds_mean = np.mean(preds_array, axis =0)\n\nsubmission = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")\nsubmission[\"target\"] = preds_mean\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The result is 0.779. This is what a begginer like me can do after trial and error.\n\n結果は0.779。初心者の試行錯誤としてはこんなものでしょうか。","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}