{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\nimport logging\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom tensorflow.keras import backend as K\nfrom transformers import RobertaTokenizer, TFRobertaModel\nfrom kaggle_datasets import KaggleDatasets\ntf.get_logger().setLevel(logging.ERROR)\nfrom kaggle_datasets import KaggleDatasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-30T13:26:28.791616Z","iopub.execute_input":"2021-05-30T13:26:28.791975Z","iopub.status.idle":"2021-05-30T13:26:28.798526Z","shell.execute_reply.started":"2021-05-30T13:26:28.791946Z","shell.execute_reply":"2021-05-30T13:26:28.797419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configurations\nEPOCHS = 70\n# Batch size\nBATCH_SIZE = 6 # 6 ok # 10 12 # 24\n# Seed\nSEED = 123\n# Learning rate\n# LR = 0.000040 # raw ,large 20epoch时候val loss只能到0.9不收敛\nLR = 0.000010 # instead\n\n# Verbosity\nVERBOSE = 2\n# Number of folds for training\nFOLDS = 5\n\n# Max length\nMAX_LEN = 250\n\n# Get the trained model we want to use\n# MODEL = 'roberta-base'\nMODEL = 'roberta-large'\n\n# Let's load our model tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(MODEL)\n\n# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:26:29.047454Z","iopub.execute_input":"2021-05-30T13:26:29.048011Z","iopub.status.idle":"2021-05-30T13:26:31.249144Z","shell.execute_reply.started":"2021-05-30T13:26:29.047967Z","shell.execute_reply":"2021-05-30T13:26:31.248263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------- add new --------------------------\nfrom sklearn import datasets\nfrom sklearn import model_selection\n\n\ndef create_folds(data, num_splits, SEED=42):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle = True, random_state = SEED)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\n# df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n# df = create_folds(df, FOLDS, SEED)\n# df.head()\n# ----------------------------------------","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:26:31.250715Z","iopub.execute_input":"2021-05-30T13:26:31.251041Z","iopub.status.idle":"2021-05-30T13:26:31.258532Z","shell.execute_reply.started":"2021-05-30T13:26:31.251007Z","shell.execute_reply":"2021-05-30T13:26:31.257745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\n# This function tokenize the text according to a transformers model tokenizer\ndef regular_encode(texts, tokenizer, maxlen = MAX_LEN):\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        padding = 'max_length',\n        truncation = True,\n        max_length = maxlen,\n    )\n    \n    return np.array(enc_di['input_ids'])\n\n# This function encode our training sentences\ndef encode_texts(x_train, x_val, MAX_LEN):\n    x_train = regular_encode(x_train.tolist(), tokenizer, maxlen = MAX_LEN)\n    x_val = regular_encode(x_val.tolist(), tokenizer, maxlen = MAX_LEN)\n    return x_train, x_val\n\n# Function to transform arrays to tensors\ndef transform_to_tensors(x_train, x_val, y_train, y_val):\n    \n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_train, y_train))\n        .repeat()\n        .shuffle(2048)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n    \n    valid_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_val, y_val))\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n    \n    return train_dataset, valid_dataset\n\n# Function to build our model\ndef build_roberta_base_model(max_len = MAX_LEN):\n    transformer = TFRobertaModel.from_pretrained(MODEL)\n    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n    sequence_output = transformer(input_word_ids)[0]\n    # We only need the cls_token, resulting in a 2d array\n    cls_token = sequence_output[:, 0, :]\n    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [output])\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = LR),\n                  loss = [tf.keras.losses.MeanSquaredError()],\n                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n    return model\n\n# Function to train and evaluate our model\ndef train_and_evaluate():\n    \n    # Read our training data\n    df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n     \n    # Seed everything\n    seed_everything(SEED)\n#     ### raw\n#     # Initiate kfold object with shuffle and a specific seed\n#     kfold = KFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n    \n    # -------------- instead ------------------------------------\n#     df['url_legal']=df['url_legal'].astype(str)\n#     df['url_legal']=df['url_legal'].apply(lambda x:x.split('/')[-1])\n#     df['excerpt']=df['excerpt']+' '+df['url_legal']+' '+df['license'].astype(str)\n    df = create_folds(df, FOLDS, SEED)\n    # --------------------------------------------------\n    \n    # Create out of folds array to store predictions\n    oof_predictions = np.zeros(len(df))\n#     # raw\n#     for fold, (trn_ind, val_ind) in enumerate(kfold.split(df)):\n    # instead\n    for fold in range(FOLDS):\n        \n        print('\\n')\n        print('-'*50)\n        print(f'Training fold {fold + 1}')\n        K.clear_session()\n        \n        # ---------------------raw-----------------------------\n        # Get text features and target\n#         x_train, x_val = df['excerpt'].iloc[trn_ind], df['excerpt'].iloc[val_ind]\n#         y_train, y_val = df['target'].iloc[trn_ind].values, df['target'].iloc[val_ind].values\n        # instead\n        df_train = df[df['kfold'] != fold].reset_index(drop=True)\n        df_valid = df[df['kfold'] == fold].reset_index(drop=True)\n        x_train, x_val = df_train['excerpt'], df_valid['excerpt']\n        y_train, y_val = df_train['target'].values, df_valid['target'].values        \n        del df_train, df_valid\n        # --------------------------------------------------\n        \n        \n        # Encode our text with Roberta tokenizer\n        x_train, x_val = encode_texts(x_train, x_val, MAX_LEN)\n        # Function to transform our numpy array to a tf Dataset\n        train_dataset, valid_dataset = transform_to_tensors(x_train, x_val, y_train, y_val)\n        # Build model\n        model = build_roberta_base_model(max_len = MAX_LEN)\n        # Model checkpoint\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Roberta_Base_{SEED}_{fold + 1}.h5', \n                                                        monitor = 'val_root_mean_squared_error', \n                                                        verbose = VERBOSE, \n                                                        save_best_only = True,\n                                                        save_weights_only = True, \n                                                        mode = 'min')\n        steps = x_train.shape[0] // (BATCH_SIZE * 16)\n        # Training phase\n        history = model.fit(train_dataset,\n                            batch_size = BATCH_SIZE,\n                            epochs = EPOCHS,\n                            verbose = VERBOSE,\n                            callbacks = [checkpoint],\n                            validation_data = valid_dataset,\n                            steps_per_epoch = steps)\n        \n        \n        # Load best epoch weights\n        model.load_weights(f'Roberta_Base_{SEED}_{fold + 1}.h5')\n        # Predict validation set to save them in the out of folds array\n        val_pred = model.predict(valid_dataset)\n#         # raw\n#         oof_predictions[val_ind] = val_pred.reshape(-1)\n        # instead\n        oof_predictions[df['kfold']==fold]=val_pred.reshape(-1)\n        del model,x_train, x_val,y_train, y_val,train_dataset, valid_dataset\n        import gc\n        gc.collect()\n    print('\\n')\n    print('-'*50)\n    # Calculate out of folds root mean squared error\n    oof_rmse = np.sqrt(mean_squared_error(df['target'], oof_predictions))\n    print(f'Our out of folds RMSE is {oof_rmse}')\n    \n\ntrain_and_evaluate()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T13:26:31.260306Z","iopub.execute_input":"2021-05-30T13:26:31.260851Z"},"trusted":true},"execution_count":null,"outputs":[]}]}