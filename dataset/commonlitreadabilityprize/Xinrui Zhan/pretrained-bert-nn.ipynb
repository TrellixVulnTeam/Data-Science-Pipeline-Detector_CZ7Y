{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd \nimport nltk\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm_notebook\nimport numpy as np\n\nbase = '/kaggle/input/commonlitreadabilityprize/'\ntrain = pd.read_csv(base + 'train.csv')\ntrain.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.excerpt.apply(lambda x: len(x)).hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class commonlit(Dataset):\n    def __init__(\n        self,\n        inputs,\n        tokenizer,\n        max_len,\n        labels,\n    ):\n\n        self.inputs = inputs\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        text = self.inputs[idx]\n        inps = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        inps_dict = {k: inps[k].squeeze(0) for k in inps}  \n        labels = self.labels[idx]\n        return inps_dict, torch.tensor(labels, dtype=torch.float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BERTClass(nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\")\n        self.fc1 = nn.Linear(768, 100)\n        self.relu =  nn.ReLU()\n        self.fc2 = nn.Linear(100, 1)\n\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        _, output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=False,\n        )\n        output = self.fc1(output)\n        #output = self.relu(output)\n        output = self.fc2(output)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Trainer():\n    \n    def __init__(self,model,train_set,test_set,opts):\n        self.model = model\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.model.to(self.device)\n        \n        self.epochs = opts['epochs']\n        print(model)\n        self.optimizer = torch.optim.Adam(model.parameters(), opts['lr']) \n        self.criterion = torch.nn.MSELoss()                     \n        self.train_loader = torch.utils.data.DataLoader(dataset=train_set,\n                                                        batch_size=opts['batch_size'],\n                                                        shuffle=True)\n        self.test_loader = torch.utils.data.DataLoader(dataset=test_set,\n                                                       batch_size=opts['batch_size'],\n                                                       shuffle=False)\n    def train(self):\n        self.model.train() #put model in training mode\n        for epoch in range(self.epochs):\n            self.tr_loss = []\n            for i, (data,labels) in tqdm_notebook(enumerate(self.train_loader),\n                                                   total = len(self.train_loader)):\n                input_ids  = data['input_ids'].to(self.device)\n                token = data['token_type_ids'].to(self.device) \n                mask = data['attention_mask'].to(self.device)\n                labels = labels.to(self.device)\n                self.optimizer.zero_grad()  \n                outputs = self.model(\n                                    input_ids=input_ids,\n                                    attention_mask=mask,\n                                    token_type_ids=token)   \n                loss = self.criterion(outputs, labels) \n                loss.backward()                        \n                self.optimizer.step()                  \n                self.tr_loss.append(loss.item())       \n            \n            self.test(epoch) # run through the validation set\n    \n    def test(self,epoch):\n            \n            self.model.eval()    # puts model in eval mode - not necessary for this demo but good to know\n            self.test_loss = []\n            self.test_accuracy = []\n            \n            for i, (data, labels) in enumerate(self.test_loader):\n                \n                input_ids  = data['input_ids'].to(self.device)\n                token = data['token_type_ids'].to(self.device) \n                mask = data['attention_mask'].to(self.device)\n                labels = labels.to(self.device)\n                # pass data through network\n                # turn off gradient calculation to speed up calcs and reduce memory\n                with torch.no_grad():\n                    outputs = self.model(\n                                        input_ids=input_ids,\n                                        attention_mask=mask,\n                                        token_type_ids=token)   \n                \n                # make our predictions and update our loss info\n                loss = self.criterion(outputs, labels)\n                self.test_loss.append(loss.item())\n            \n            print('epoch: {}, train loss: {}, test loss: {}'.format( \n                  epoch+1, np.mean(self.tr_loss), np.mean(self.test_loss)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = train.loc[:, \"excerpt\"].values.tolist()\nlabels = train.loc[:, \"target\"].values.tolist()\ntokenizer = transformers.BertTokenizer.from_pretrained(\n    \"bert-base-uncased\", do_lower_case=True\n)\ndataset = commonlit(inputs=inputs, tokenizer=tokenizer, max_len=185, labels=labels)\ndisplay(len(dataset))\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [2000, 834])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERTClass()\nopts = {\n    'lr': 2e-4,\n    'epochs': 30,\n    'batch_size': 64\n}\n\nCommonTrainer = Trainer(model = model,\n                      train_set = train_dataset,\n                      test_set = test_dataset,opts = opts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CommonTrainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}