{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport torch\nimport copy\nimport math\nimport gc\nfrom tqdm import tqdm\nimport torch.utils.data as D\nimport random\nimport os\nfrom transformers import AutoModelWithLMHead, AutoTokenizer,RobertaConfig, RobertaModel,AutoModelForSequenceClassification,AutoModelForMaskedLM\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch import nn\nfrom torch import optim\nimport time\nimport torch.nn.functional as F\nfrom transformers import (\n    AutoModel,\n    CONFIG_MAPPING,\n    MODEL_MAPPING,\n    AdamW,\n    AutoConfig,\n    AutoModelForMaskedLM,\n    AutoTokenizer,\n    DataCollatorForLanguageModeling,\n    SchedulerType,\n    get_scheduler,\n    set_seed,\n)\nfrom sklearn.linear_model import Ridge\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-19T16:03:34.109574Z","iopub.execute_input":"2021-07-19T16:03:34.110029Z","iopub.status.idle":"2021-07-19T16:03:42.910795Z","shell.execute_reply.started":"2021-07-19T16:03:34.109942Z","shell.execute_reply":"2021-07-19T16:03:42.909884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# parameters for this notebook","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed=12345\n    test_avg_n=1\n    val_avg_n=5\n    max_len=256\n    batch_size=24\n    dropout_p=0.1\n    folds=5\n    cv_shuffle=False\n    pad_token_id=1\n    device=torch.device('cuda:0')\n    dtype=torch.float32","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:42.912149Z","iopub.execute_input":"2021-07-19T16:03:42.912636Z","iopub.status.idle":"2021-07-19T16:03:42.917969Z","shell.execute_reply.started":"2021-07-19T16:03:42.912593Z","shell.execute_reply":"2021-07-19T16:03:42.916776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ntest_df=pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\nres_df=pd.read_csv('/kaggle/input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:42.919625Z","iopub.execute_input":"2021-07-19T16:03:42.920169Z","iopub.status.idle":"2021-07-19T16:03:43.080066Z","shell.execute_reply.started":"2021-07-19T16:03:42.920122Z","shell.execute_reply":"2021-07-19T16:03:43.079101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaDataset(D.Dataset):\n    def __init__(self, token, target):\n        self.token = token\n        self.target = target\n        \n    def __len__(self):\n        return self.token.shape[0]\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.token[idx].input_ids), \\\n                torch.tensor(self.token[idx].attention_mask), self.target[idx]\n    \ndef collate(batch):\n    ids, attns, targets = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True,padding_value=CFG.pad_token_id).to(CFG.device)\n    attns = pad_sequence(attns, batch_first=True,padding_value=CFG.pad_token_id).to(CFG.device)\n    targets = torch.tensor(targets).float().to(CFG.device)\n    return ids, attns, targets\n\ndef CV_split(m,k=5,shuffle=False,seed=7):\n    index=np.arange(m)\n    if shuffle:\n        np.random.seed(seed)\n        np.random.shuffle(index)\n    test_size=math.ceil(m/k)\n    split_indices=[]\n    for i in range(k):\n        bool_index=np.zeros(m)\n        bool_index[test_size*i:test_size*(i+1)]=1\n        bool_index=bool_index.astype('bool')\n        val_index=index[bool_index]\n        train_index=index[~bool_index]\n        split_indices.append((train_index,val_index))\n    return split_indices\n\ndef rmse(y1,y2):\n    score=np.sqrt(((y1-y2)**2).mean())\n    return score\n\ndef score_test(model,test_ldr,mode='train',avg_n=1):\n    if mode=='eval':\n        model.eval()\n    elif mode=='train':\n        model.train()\n    avg_pred=pd.DataFrame()\n    for i in range(avg_n):\n        preds=[]\n        for texts, attns, idx in test_ldr:\n            with torch.no_grad():\n                pred = model(texts,attns)\n                preds.append(pred)\n        preds=torch.cat(preds,axis=0)\n        preds=preds.to('cpu').numpy().reshape(-1)\n        avg_pred[f'pred{i+1}']=preds\n    #print(avg_pred.corr())\n    return avg_pred.values.mean(axis=1)\n\ndef tokenize(tokenizer,texts):\n    tokens=[]\n    for text in texts:\n        token=tokenizer(text,max_length=CFG.max_len,truncation=True, padding='max_length',add_special_tokens=True)\n        tokens.append(token)\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.081721Z","iopub.execute_input":"2021-07-19T16:03:43.082065Z","iopub.status.idle":"2021-07-19T16:03:43.100102Z","shell.execute_reply.started":"2021-07-19T16:03:43.082034Z","shell.execute_reply":"2021-07-19T16:03:43.098859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"* remeber that different models has different mask token id\n* use different seed in dataloader for diversity","metadata":{}},{"cell_type":"markdown","source":"* F1\n* CV 0.477 LB 0.469\n* roberta base train with ITPT weights\n","metadata":{}},{"cell_type":"code","source":"class MyModel1(nn.Module):\n    def __init__(self, model):\n        super(MyModel1, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF1_CFG={}\nF1_CFG['max_len']=256\nF1_CFG['seed']=12345\nF1_CFG['batch_size']=24\nF1_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.101594Z","iopub.execute_input":"2021-07-19T16:03:43.102049Z","iopub.status.idle":"2021-07-19T16:03:43.116289Z","shell.execute_reply.started":"2021-07-19T16:03:43.102018Z","shell.execute_reply":"2021-07-19T16:03:43.11521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F2\n* CV 0.4839 LB 0.470\n* roberta large\n* mv state dict to cpu\n\n","metadata":{}},{"cell_type":"code","source":"class MyModel2(nn.Module):\n    def __init__(self, model):\n        super(MyModel2, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF2_CFG={}\nF2_CFG['max_len']=256\nF2_CFG['seed']=7\nF2_CFG['batch_size']=8\nF2_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.117673Z","iopub.execute_input":"2021-07-19T16:03:43.118004Z","iopub.status.idle":"2021-07-19T16:03:43.133855Z","shell.execute_reply.started":"2021-07-19T16:03:43.117965Z","shell.execute_reply":"2021-07-19T16:03:43.132714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F3\n* CV 0.487 LB 0.486\n* SimCSE roberta base","metadata":{}},{"cell_type":"code","source":"class MyModel3(nn.Module):\n    def __init__(self, model):\n        super(MyModel3, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF3_CFG={}\nF3_CFG['max_len']=256\nF3_CFG['seed']=33\nF3_CFG['batch_size']=24\nF3_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.135276Z","iopub.execute_input":"2021-07-19T16:03:43.135587Z","iopub.status.idle":"2021-07-19T16:03:43.150264Z","shell.execute_reply.started":"2021-07-19T16:03:43.135557Z","shell.execute_reply":"2021-07-19T16:03:43.148888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F4","metadata":{}},{"cell_type":"markdown","source":"* F5\n* CV 0.4773 LB 0.474\n* roberta base train with ITPT weights\n* ITPT model from: Maunish's pre-trained model","metadata":{}},{"cell_type":"code","source":"class MyModel5(nn.Module):\n    def __init__(self, model):\n        super(MyModel5, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF5_CFG={}\nF5_CFG['max_len']=256\nF5_CFG['seed']=12345\nF5_CFG['batch_size']=24\nF5_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.152984Z","iopub.execute_input":"2021-07-19T16:03:43.153282Z","iopub.status.idle":"2021-07-19T16:03:43.163377Z","shell.execute_reply.started":"2021-07-19T16:03:43.153254Z","shell.execute_reply":"2021-07-19T16:03:43.162637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F6\n* CV 0.4855 LB 0.471\n* almost no diversity same seed and model and pretrain","metadata":{}},{"cell_type":"code","source":"class MyModel6(nn.Module):\n    def __init__(self, model):\n        super(MyModel6, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF6_CFG={}\nF6_CFG['max_len']=256\nF6_CFG['seed']=12345\nF6_CFG['batch_size']=24\nF6_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.165573Z","iopub.execute_input":"2021-07-19T16:03:43.166051Z","iopub.status.idle":"2021-07-19T16:03:43.182023Z","shell.execute_reply.started":"2021-07-19T16:03:43.166005Z","shell.execute_reply":"2021-07-19T16:03:43.180904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F7\n* CV 0.4781 LB 0.473","metadata":{}},{"cell_type":"code","source":"class MyModel7(nn.Module):\n    def __init__(self,model):\n        super(MyModel7,self).__init__()\n        self.roberta = model             \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        return self.regressor(context_vector)\n    \nF7_CFG={}\nF7_CFG['max_len']=256\nF7_CFG['seed']=666\nF7_CFG['batch_size']=24\nF7_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.183306Z","iopub.execute_input":"2021-07-19T16:03:43.183617Z","iopub.status.idle":"2021-07-19T16:03:43.195703Z","shell.execute_reply.started":"2021-07-19T16:03:43.183583Z","shell.execute_reply":"2021-07-19T16:03:43.194683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F8\n* CV 0.4815  LB 0.472","metadata":{}},{"cell_type":"code","source":"class MyModel8(nn.Module):\n    def __init__(self, model):\n        super(MyModel8, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.output_layer(text_emb)\n        return x2\nF8_CFG={}\nF8_CFG['max_len']=256\nF8_CFG['seed']=54321\nF8_CFG['batch_size']=24\nF8_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.19738Z","iopub.execute_input":"2021-07-19T16:03:43.198086Z","iopub.status.idle":"2021-07-19T16:03:43.214371Z","shell.execute_reply.started":"2021-07-19T16:03:43.19804Z","shell.execute_reply":"2021-07-19T16:03:43.213369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F9\n* CV 0.4799 LB 0.475\n* roberta base\n* one linear output layer\n* 2 layer re-init","metadata":{}},{"cell_type":"code","source":"class MyModel9(nn.Module):\n    def __init__(self, model):\n        super(MyModel9, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.output_layer(text_emb)\n        return x2\nF9_CFG={}\nF9_CFG['max_len']=256\nF9_CFG['seed']=54321\nF9_CFG['batch_size']=24\nF9_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.216192Z","iopub.execute_input":"2021-07-19T16:03:43.216903Z","iopub.status.idle":"2021-07-19T16:03:43.228336Z","shell.execute_reply.started":"2021-07-19T16:03:43.216854Z","shell.execute_reply":"2021-07-19T16:03:43.227367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F10\n* CV 0.479 LB 0.479\n\n* roberta base\n* two linear output layer\n* 2 layer re-init\n* seed 777 to compare with F1","metadata":{}},{"cell_type":"code","source":"class MyModel10(nn.Module):\n    def __init__(self, model):\n        super(MyModel10, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF10_CFG={}\nF10_CFG['max_len']=256\nF10_CFG['seed']=12345\nF10_CFG['batch_size']=24\nF10_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.229654Z","iopub.execute_input":"2021-07-19T16:03:43.229962Z","iopub.status.idle":"2021-07-19T16:03:43.247616Z","shell.execute_reply.started":"2021-07-19T16:03:43.229933Z","shell.execute_reply":"2021-07-19T16:03:43.246291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F11\n* CV 0.4811 LB 0.466\n* roberta large\n* two linear output layer\n* 5 layer re init","metadata":{}},{"cell_type":"code","source":"class MyModel11(nn.Module):\n    def __init__(self, model):\n        super(MyModel11, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF11_CFG={}\nF11_CFG['max_len']=256\nF11_CFG['seed']=7\nF11_CFG['batch_size']=8\nF11_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.248948Z","iopub.execute_input":"2021-07-19T16:03:43.249261Z","iopub.status.idle":"2021-07-19T16:03:43.262567Z","shell.execute_reply.started":"2021-07-19T16:03:43.249233Z","shell.execute_reply":"2021-07-19T16:03:43.261089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F12\n* CV 0.4773 LB 0.473\n* roberta base\n* seed 777 to compare with F1\n* 10% warm up","metadata":{}},{"cell_type":"code","source":"class MyModel12(nn.Module):\n    def __init__(self, model):\n        super(MyModel12, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF12_CFG={}\nF12_CFG['max_len']=256\nF12_CFG['seed']=12345\nF12_CFG['batch_size']=24\nF12_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.264558Z","iopub.execute_input":"2021-07-19T16:03:43.264919Z","iopub.status.idle":"2021-07-19T16:03:43.275119Z","shell.execute_reply.started":"2021-07-19T16:03:43.264885Z","shell.execute_reply":"2021-07-19T16:03:43.274046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F13\n* CV 0.4816 LB 0.484\n* roberta base \n* two linear output layer\n* 3 layer re-init\n* seed 777 to compare with F1\n* lr diff don't apply to re-init layers\n* seems roberta base (not large) with re init do poorly in the LB test fold...","metadata":{}},{"cell_type":"code","source":"class MyModel13(nn.Module):\n    def __init__(self, model):\n        super(MyModel13, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF13_CFG={}\nF13_CFG['max_len']=256\nF13_CFG['seed']=12345\nF13_CFG['batch_size']=24\nF13_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.276569Z","iopub.execute_input":"2021-07-19T16:03:43.2769Z","iopub.status.idle":"2021-07-19T16:03:43.293178Z","shell.execute_reply.started":"2021-07-19T16:03:43.27687Z","shell.execute_reply":"2021-07-19T16:03:43.292031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F14\n* CV 0.4778 LB 0.468\n* roberta base \n* seed 777 to compare with F1\n* 10% warm up\n* batch size 32 and lr 8e-5","metadata":{}},{"cell_type":"code","source":"class MyModel14(nn.Module):\n    def __init__(self, model):\n        super(MyModel14, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF14_CFG={}\nF14_CFG['max_len']=256\nF14_CFG['seed']=12345\nF14_CFG['batch_size']=24\nF14_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.295262Z","iopub.execute_input":"2021-07-19T16:03:43.295607Z","iopub.status.idle":"2021-07-19T16:03:43.304922Z","shell.execute_reply.started":"2021-07-19T16:03:43.295574Z","shell.execute_reply":"2021-07-19T16:03:43.303798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F15\n* CV 0.4839 LB\n* roberta large\n* two linear output layer\n* 5 layer re init\n* lr with warm up","metadata":{}},{"cell_type":"code","source":"class MyModel15(nn.Module):\n    def __init__(self, model):\n        super(MyModel15, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF15_CFG={}\nF15_CFG['max_len']=256\nF15_CFG['seed']=7\nF15_CFG['batch_size']=8\nF15_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.306516Z","iopub.execute_input":"2021-07-19T16:03:43.306919Z","iopub.status.idle":"2021-07-19T16:03:43.316829Z","shell.execute_reply.started":"2021-07-19T16:03:43.306881Z","shell.execute_reply":"2021-07-19T16:03:43.315827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F16\n* CV 0.4824 LB 0.468\n* roberta large\n* two linear output layer\n* 5 layer re init\n* 6 epochs (=large lr in 5epochs)","metadata":{}},{"cell_type":"code","source":"class MyModel16(nn.Module):\n    def __init__(self, model):\n        super(MyModel16, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF16_CFG={}\nF16_CFG['max_len']=256\nF16_CFG['seed']=7\nF16_CFG['batch_size']=8\nF16_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.318253Z","iopub.execute_input":"2021-07-19T16:03:43.318695Z","iopub.status.idle":"2021-07-19T16:03:43.333519Z","shell.execute_reply.started":"2021-07-19T16:03:43.318649Z","shell.execute_reply":"2021-07-19T16:03:43.332569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F17\n* CV 0.47581798 LB 0.468\n* roberta base train with ITPT weights\n* bs 32 \n* fix oom\n* fix possible token id change\n* add reinit code","metadata":{}},{"cell_type":"code","source":"class MyModel17(nn.Module):\n    def __init__(self, model):\n        super(MyModel17, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF17_CFG={}\nF17_CFG['max_len']=256\nF17_CFG['seed']=12345\nF17_CFG['batch_size']=24\nF17_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.334895Z","iopub.execute_input":"2021-07-19T16:03:43.335369Z","iopub.status.idle":"2021-07-19T16:03:43.346108Z","shell.execute_reply.started":"2021-07-19T16:03:43.335335Z","shell.execute_reply":"2021-07-19T16:03:43.344945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F18\n* CV 0.4874678 LB 0.488\n* Simcse base \n* seed 754\n* with attention head","metadata":{}},{"cell_type":"code","source":"class MyModel18(nn.Module):\n    def __init__(self,model):\n        super(MyModel18,self).__init__()\n        self.model = model             \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.model(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        return self.regressor(context_vector)\n    \nF18_CFG={}\nF18_CFG['max_len']=256\nF18_CFG['seed']=755\nF18_CFG['batch_size']=24\nF18_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.347695Z","iopub.execute_input":"2021-07-19T16:03:43.348473Z","iopub.status.idle":"2021-07-19T16:03:43.360193Z","shell.execute_reply.started":"2021-07-19T16:03:43.348428Z","shell.execute_reply":"2021-07-19T16:03:43.359206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F19\n* CV 0.4845 LB 0.483\n* Simcse base \n* seed 754\n* 2 layer re init","metadata":{}},{"cell_type":"code","source":"class MyModel19(nn.Module):\n    def __init__(self, model):\n        super(MyModel19, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\nF19_CFG={}\nF19_CFG['max_len']=256\nF19_CFG['seed']=755\nF19_CFG['batch_size']=24\nF19_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.361235Z","iopub.execute_input":"2021-07-19T16:03:43.361503Z","iopub.status.idle":"2021-07-19T16:03:43.37672Z","shell.execute_reply.started":"2021-07-19T16:03:43.361477Z","shell.execute_reply":"2021-07-19T16:03:43.37568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F20\n* CV 0.4796 LB 0.464\n* Simcse large\n* seed 754\n* two linear output layer\n* 5 layer re init","metadata":{}},{"cell_type":"code","source":"class MyModel20(nn.Module):\n    def __init__(self, model):\n        super(MyModel20, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\nF20_CFG={}\nF20_CFG['max_len']=256\nF20_CFG['seed']=755\nF20_CFG['batch_size']=8\nF20_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.381474Z","iopub.execute_input":"2021-07-19T16:03:43.381927Z","iopub.status.idle":"2021-07-19T16:03:43.390514Z","shell.execute_reply.started":"2021-07-19T16:03:43.381894Z","shell.execute_reply":"2021-07-19T16:03:43.389423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F22\n* CV LB\n* roberta large\n* gradient accumulation\n* batch size 16 lr 4e-5","metadata":{}},{"cell_type":"code","source":"class MyModel22(nn.Module):\n    def __init__(self, model):\n        super(MyModel22, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF22_CFG={}\nF22_CFG['max_len']=256\nF22_CFG['seed']=7\nF22_CFG['batch_size']=8\nF22_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.392838Z","iopub.execute_input":"2021-07-19T16:03:43.39313Z","iopub.status.idle":"2021-07-19T16:03:43.405909Z","shell.execute_reply.started":"2021-07-19T16:03:43.393102Z","shell.execute_reply":"2021-07-19T16:03:43.405104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F23\n* CV 0.4792 LB 0.465\n* roberta large\n* gradient accumulation\n* batch size 16 lr 2.5e-5","metadata":{}},{"cell_type":"code","source":"class MyModel23(nn.Module):\n    def __init__(self, model):\n        super(MyModel23, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF23_CFG={}\nF23_CFG['max_len']=256\nF23_CFG['seed']=7\nF23_CFG['batch_size']=8\nF23_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.407137Z","iopub.execute_input":"2021-07-19T16:03:43.407626Z","iopub.status.idle":"2021-07-19T16:03:43.417177Z","shell.execute_reply.started":"2021-07-19T16:03:43.407597Z","shell.execute_reply":"2021-07-19T16:03:43.41586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F24\n* CV 0.4815 LB 0.488\n* deberta base \n* seed 5311\n* bs 24","metadata":{}},{"cell_type":"code","source":"class MyModel24(nn.Module):\n    def __init__(self, model):\n        super(MyModel24, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF24_CFG={}\nF24_CFG['max_len']=256\nF24_CFG['seed']=721\nF24_CFG['batch_size']=24\nF24_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.419703Z","iopub.execute_input":"2021-07-19T16:03:43.420046Z","iopub.status.idle":"2021-07-19T16:03:43.429758Z","shell.execute_reply.started":"2021-07-19T16:03:43.420015Z","shell.execute_reply":"2021-07-19T16:03:43.428737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F25\n* CV 0.4831 LB 0.488\n* deberta base \n* seed 5311\n* bs 24\n* re init 2 layers","metadata":{}},{"cell_type":"code","source":"class MyModel25(nn.Module):\n    def __init__(self, model):\n        super(MyModel25, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF25_CFG={}\nF25_CFG['max_len']=256\nF25_CFG['seed']=721\nF25_CFG['batch_size']=24\nF25_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.431025Z","iopub.execute_input":"2021-07-19T16:03:43.431329Z","iopub.status.idle":"2021-07-19T16:03:43.444625Z","shell.execute_reply.started":"2021-07-19T16:03:43.431299Z","shell.execute_reply":"2021-07-19T16:03:43.443644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F26\n* CV 0.4757 LB 0.465\n* deberta large\n* seed 5311\n* re init 5 layers\n* bs 4\n* grad accum 4 lr 2.5","metadata":{}},{"cell_type":"code","source":"class MyModel26(nn.Module):\n    def __init__(self, model):\n        super(MyModel26, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF26_CFG={}\nF26_CFG['max_len']=256\nF26_CFG['seed']=721\nF26_CFG['batch_size']=8\nF26_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.445911Z","iopub.execute_input":"2021-07-19T16:03:43.446209Z","iopub.status.idle":"2021-07-19T16:03:43.45634Z","shell.execute_reply.started":"2021-07-19T16:03:43.44618Z","shell.execute_reply":"2021-07-19T16:03:43.455375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F27\n* CV 0.486 LB 0.464\n* simcse large\n* seed 754\n* re init 5 layers\n* grad accum 2 lr 2.5","metadata":{}},{"cell_type":"code","source":"class MyModel27(nn.Module):\n    def __init__(self, model):\n        super(MyModel27, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF27_CFG={}\nF27_CFG['max_len']=256\nF27_CFG['seed']=755\nF27_CFG['batch_size']=8\nF27_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.457663Z","iopub.execute_input":"2021-07-19T16:03:43.457982Z","iopub.status.idle":"2021-07-19T16:03:43.468388Z","shell.execute_reply.started":"2021-07-19T16:03:43.457954Z","shell.execute_reply":"2021-07-19T16:03:43.467694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F28\n* CV LB","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F29\n* CV 0.50150967 LB\n* bert base cased\n* seed 2334\n* bs 32 lr 8","metadata":{}},{"cell_type":"code","source":"class MyModel29(nn.Module):\n    def __init__(self, model):\n        super(MyModel29, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF29_CFG={}\nF29_CFG['max_len']=256\nF29_CFG['seed']=2\nF29_CFG['batch_size']=24\nF29_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.469481Z","iopub.execute_input":"2021-07-19T16:03:43.469925Z","iopub.status.idle":"2021-07-19T16:03:43.485178Z","shell.execute_reply.started":"2021-07-19T16:03:43.469895Z","shell.execute_reply":"2021-07-19T16:03:43.484146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F30\n* CV 0.47575  LB 0.462\n* deberta large\n* seed 5311\n* re init 5 layers\n* bs 4\n* grad accum 2 lr 2","metadata":{}},{"cell_type":"code","source":"class MyModel30(nn.Module):\n    def __init__(self, model):\n        super(MyModel30, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF30_CFG={}\nF30_CFG['max_len']=256\nF30_CFG['seed']=721\nF30_CFG['batch_size']=8\nF30_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.48676Z","iopub.execute_input":"2021-07-19T16:03:43.487328Z","iopub.status.idle":"2021-07-19T16:03:43.496485Z","shell.execute_reply.started":"2021-07-19T16:03:43.487281Z","shell.execute_reply":"2021-07-19T16:03:43.495524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F31\n* CV LB\n* bert large cased\n* seed 2334\n* bs 32 lr 8","metadata":{}},{"cell_type":"code","source":"class MyModel31(nn.Module):\n    def __init__(self, model):\n        super(MyModel31, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\nF31_CFG={}\nF31_CFG['max_len']=256\nF31_CFG['seed']=7212\nF31_CFG['batch_size']=8\nF31_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.497685Z","iopub.execute_input":"2021-07-19T16:03:43.498031Z","iopub.status.idle":"2021-07-19T16:03:43.508611Z","shell.execute_reply.started":"2021-07-19T16:03:43.498002Z","shell.execute_reply":"2021-07-19T16:03:43.507671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F32\n* CV 0.4749 LB 0.469\n* deberta large\n* seed 12222\n* no re init\n* bs 4\n* grad accum 2 lr 2\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel32(nn.Module):\n    def __init__(self, model):\n        super(MyModel32, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF32_CFG={}\nF32_CFG['max_len']=256\nF32_CFG['seed']=20134\nF32_CFG['batch_size']=8\nF32_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.510063Z","iopub.execute_input":"2021-07-19T16:03:43.510423Z","iopub.status.idle":"2021-07-19T16:03:43.52283Z","shell.execute_reply.started":"2021-07-19T16:03:43.510389Z","shell.execute_reply":"2021-07-19T16:03:43.521891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F33\n* CV 0.4835 LB 0.464\n* roberta large with ITPT\n* seed 555666\n* re init 5 layers\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel33(nn.Module):\n    def __init__(self, model):\n        super(MyModel33, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF33_CFG={}\nF33_CFG['max_len']=256\nF33_CFG['seed']=20134\nF33_CFG['batch_size']=8\nF33_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.524197Z","iopub.execute_input":"2021-07-19T16:03:43.524474Z","iopub.status.idle":"2021-07-19T16:03:43.533923Z","shell.execute_reply.started":"2021-07-19T16:03:43.524448Z","shell.execute_reply":"2021-07-19T16:03:43.532989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F34\n* CV LB\n* bart base\n* seed 9999\n* one linear layer\n","metadata":{}},{"cell_type":"code","source":"class MyModel34(nn.Module):\n    def __init__(self, model):\n        super(MyModel34, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['last_hidden_state']\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n\nF34_CFG={}\nF34_CFG['max_len']=256\nF34_CFG['seed']=201\nF34_CFG['batch_size']=24\nF34_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.535418Z","iopub.execute_input":"2021-07-19T16:03:43.53615Z","iopub.status.idle":"2021-07-19T16:03:43.546758Z","shell.execute_reply.started":"2021-07-19T16:03:43.536117Z","shell.execute_reply":"2021-07-19T16:03:43.545784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F35\n* CV 0.4986 LB\n* electra base\n* seed 75412\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel35(nn.Module):\n    def __init__(self, model):\n        super(MyModel35, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF35_CFG={}\nF35_CFG['max_len']=256\nF35_CFG['seed']=75412\nF35_CFG['batch_size']=24\nF35_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.548261Z","iopub.execute_input":"2021-07-19T16:03:43.548641Z","iopub.status.idle":"2021-07-19T16:03:43.560927Z","shell.execute_reply.started":"2021-07-19T16:03:43.548612Z","shell.execute_reply":"2021-07-19T16:03:43.559958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F36\n* CV 0.4931 LB 0.480\n* electra large\n* seed 75412\n* 1 linear layer\n* re init 5 layers","metadata":{}},{"cell_type":"code","source":"class MyModel36(nn.Module):\n    def __init__(self, model):\n        super(MyModel36, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF36_CFG={}\nF36_CFG['max_len']=256\nF36_CFG['seed']=75412\nF36_CFG['batch_size']=8\nF36_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.562328Z","iopub.execute_input":"2021-07-19T16:03:43.562942Z","iopub.status.idle":"2021-07-19T16:03:43.572009Z","shell.execute_reply.started":"2021-07-19T16:03:43.562906Z","shell.execute_reply":"2021-07-19T16:03:43.570968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F37\n* CV LB\n* xlm-roberta base\n* seed 900\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel37(nn.Module):\n    def __init__(self, model):\n        super(MyModel37, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF37_CFG={}\nF37_CFG['max_len']=256\nF37_CFG['seed']=900\nF37_CFG['batch_size']=24\nF37_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.573235Z","iopub.execute_input":"2021-07-19T16:03:43.573513Z","iopub.status.idle":"2021-07-19T16:03:43.586047Z","shell.execute_reply.started":"2021-07-19T16:03:43.573486Z","shell.execute_reply":"2021-07-19T16:03:43.584897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F38\n* CV 0.4782 LB 0.477\n* roberta base\n* seed 900\n* 1 linear layer\n* len 300\n* bs 24","metadata":{}},{"cell_type":"code","source":"class MyModel38(nn.Module):\n    def __init__(self, model):\n        super(MyModel38, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF38_CFG={}\nF38_CFG['max_len']=300\nF38_CFG['seed']=900\nF38_CFG['batch_size']=24\nF38_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.587319Z","iopub.execute_input":"2021-07-19T16:03:43.58762Z","iopub.status.idle":"2021-07-19T16:03:43.601391Z","shell.execute_reply.started":"2021-07-19T16:03:43.587591Z","shell.execute_reply":"2021-07-19T16:03:43.600196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F39\n* CV LB\n* xlnet-base\n* seed 900\n* 1 linear layer\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F40\n* CV 0.4836 LB 0.463\n* Simcse large unsup\n* seed 754\n* re init 5 layers\n* bs 8 lr 2e-5\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel40(nn.Module):\n    def __init__(self, model):\n        super(MyModel40, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF40_CFG={}\nF40_CFG['max_len']=256\nF40_CFG['seed']=901\nF40_CFG['batch_size']=8\nF40_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.60261Z","iopub.execute_input":"2021-07-19T16:03:43.602923Z","iopub.status.idle":"2021-07-19T16:03:43.613224Z","shell.execute_reply.started":"2021-07-19T16:03:43.602888Z","shell.execute_reply":"2021-07-19T16:03:43.612025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F41\n* CV 0.4796 LB 0.475\n* deberta-large-mnli\n* seed 5311\n* re init 5 layers\n* bs 4\n* grad accum 2 lr 2\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel41(nn.Module):\n    def __init__(self, model):\n        super(MyModel41, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF41_CFG={}\nF41_CFG['max_len']=256\nF41_CFG['seed']=111\nF41_CFG['batch_size']=8\nF41_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.614723Z","iopub.execute_input":"2021-07-19T16:03:43.615213Z","iopub.status.idle":"2021-07-19T16:03:43.62574Z","shell.execute_reply.started":"2021-07-19T16:03:43.615169Z","shell.execute_reply":"2021-07-19T16:03:43.624791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F42\n* CV 0.4883 LB \n* roberta large roberta-large-mnli\n* seed 5551\n* re init 5 layers\n* bs 8 lr 2e-5\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel42(nn.Module):\n    def __init__(self, model):\n        super(MyModel42, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF42_CFG={}\nF42_CFG['max_len']=256\nF42_CFG['seed']=1112\nF42_CFG['batch_size']=8\nF42_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.627042Z","iopub.execute_input":"2021-07-19T16:03:43.627467Z","iopub.status.idle":"2021-07-19T16:03:43.640854Z","shell.execute_reply.started":"2021-07-19T16:03:43.627438Z","shell.execute_reply":"2021-07-19T16:03:43.639908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F43\n* CV 0.4970 LB \n* roberta-large-openai-detector\n* seed 5551\n* re init 5 layers\n* bs 8 lr 2e-5\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel43(nn.Module):\n    def __init__(self, model):\n        super(MyModel43, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF43_CFG={}\nF43_CFG['max_len']=256\nF43_CFG['seed']=1112\nF43_CFG['batch_size']=8\nF43_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.642247Z","iopub.execute_input":"2021-07-19T16:03:43.642545Z","iopub.status.idle":"2021-07-19T16:03:43.651403Z","shell.execute_reply.started":"2021-07-19T16:03:43.642511Z","shell.execute_reply":"2021-07-19T16:03:43.650707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F45\n* roberta large -5 layer output","metadata":{}},{"cell_type":"code","source":"class MyModel45(nn.Module):\n    def __init__(self, model):\n        super(MyModel45, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-5]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF45_CFG={}\nF45_CFG['max_len']=256\nF45_CFG['seed']=7\nF45_CFG['batch_size']=8\nF45_CFG['dropout_p']=0.1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F50\n* funnel large\n* bs 16\n* lr 3e-5","metadata":{}},{"cell_type":"code","source":"class MyModel50(nn.Module):\n    def __init__(self, model):\n        super(MyModel50, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF50_CFG={}\nF50_CFG['max_len']=256\nF50_CFG['seed']=711223\nF50_CFG['batch_size']=8\nF50_CFG['dropout_p']=0.1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F51\n* electra large\n* bs 16\n* lr 3.5e-5\n* no reinit","metadata":{}},{"cell_type":"code","source":"class MyModel51(nn.Module):\n    def __init__(self, model):\n        super(MyModel51, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF51_CFG={}\nF51_CFG['max_len']=256\nF51_CFG['seed']=91\nF51_CFG['batch_size']=8\nF51_CFG['dropout_p']=0.1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def inference_wrapper(model_name,model_class,model_CFG,test_df,train_df):\n    # moad model cfg\n    for key,val in model_CFG.items():\n        setattr(CFG,key,val)\n    # fix random seed    \n    random.seed(CFG.seed)\n    os.environ['PYTHONHASHSEED'] = str(CFG.seed)\n    np.random.seed(CFG.seed)\n    torch.manual_seed(CFG.seed)\n    torch.cuda.manual_seed(CFG.seed)\n    torch.cuda.manual_seed_all(CFG.seed)\n    torch.backends.cudnn.deterministic = True\n    # load model\n    notebook=model_name+'-train'\n    path='../input/'+notebook+'/model_init/'\n    config = AutoConfig.from_pretrained(path, output_hidden_states=True,attention_probs_dropout_prob=CFG.dropout_p,hidden_dropout_prob=CFG.dropout_p)\n    tokenizer = AutoTokenizer.from_pretrained(path,model_max_length=CFG.max_len)\n    CFG.pad_token_id=tokenizer.pad_token_id\n    if int(model_name[1:])<=23:\n        model =AutoModelForMaskedLM.from_pretrained(path,config=config)\n    else:\n        model =AutoModel.from_pretrained(path,config=config)\n    model=model_class(model)\n    # load state dicts\n    state_dicts=[]\n    for i in range(CFG.folds):\n        checkpoint=torch.load('../input/'+notebook+f'/fold_{i+1}_model',map_location=CFG.device)\n        state_dicts.append(checkpoint['model_state_dict'])\n    # create data loader\n    \n    # test df\n    if test_df is not None:\n        test_df['token'] = tokenize(tokenizer,test_df.excerpt)\n        test_dataset = RobertaDataset(test_df.token, test_df.index)\n        # shuffle = False !!\n        test_dataloader = D.DataLoader(test_dataset, batch_size=CFG.batch_size,\n                                     shuffle=False, collate_fn = collate,num_workers=0)\n        test_df['test/'+model_name]=0\n        for i in range(CFG.folds):\n            model.load_state_dict(state_dicts[i])\n            model.to(CFG.device)\n            preds=score_test(model,test_dataloader,mode='train',avg_n=CFG.test_avg_n)\n            test_df['test/'+model_name]+=preds/CFG.folds\n    # train\n    if train_df is not None:\n        train_df['val/'+model_name]=0\n        train_df['token'] = tokenize(tokenizer,train_df.excerpt)\n        train_dataset = RobertaDataset(train_df.token, train_df.index)\n        split_indices=CV_split(len(train_df),k=CFG.folds,shuffle=CFG.cv_shuffle,seed=7)\n        for i in range(CFG.folds):\n            train_index,val_index=split_indices[i]\n            val_dataset = D.Subset(train_dataset, val_index)\n            val_dataloader = D.DataLoader(val_dataset, batch_size=CFG.batch_size,\n                                          shuffle=False, collate_fn = collate,num_workers=0)\n            model.load_state_dict(state_dicts[i])\n            model.to(CFG.device)\n            preds=score_test(model,val_dataloader,mode='train',avg_n=CFG.val_avg_n)\n            train_df.loc[val_index,'val/'+model_name]=preds\n    return ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.652695Z","iopub.execute_input":"2021-07-19T16:03:43.653248Z","iopub.status.idle":"2021-07-19T16:03:43.672385Z","shell.execute_reply.started":"2021-07-19T16:03:43.653208Z","shell.execute_reply":"2021-07-19T16:03:43.671637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('../input/f-inference-1/ensemble_train.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inference_tasks=[\n                ('f50',MyModel50,F50_CFG),\n                ('f51',MyModel51,F51_CFG),\n                ]\nfor model_name,model_class,model_CFG in tqdm(inference_tasks):\n    inference_wrapper(model_name,model_class,model_CFG,test_df=None,train_df=train_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.673351Z","iopub.execute_input":"2021-07-19T16:03:43.673755Z","iopub.status.idle":"2021-07-19T16:03:43.689461Z","shell.execute_reply.started":"2021-07-19T16:03:43.673726Z","shell.execute_reply":"2021-07-19T16:03:43.688366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation & Ensemble","metadata":{}},{"cell_type":"code","source":"val_cols=[]\ntest_cols=[]\nfor model_name,_,_ in inference_tasks:\n    val_cols.append('val/'+model_name)\n    test_cols.append('test/'+model_name)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.705604Z","iopub.status.idle":"2021-07-19T16:03:43.706083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['val_oof_pred']=0\n#test_df['final_test_pred']=0\nmodel_weight=0\nbias=0\nfor i,(train_index,val_index) in enumerate(CV_split(len(train_df),k=CFG.folds,shuffle=False,seed=7)):\n    model=Ridge(alpha=0.01,fit_intercept=True,normalize=False)\n    model.fit(train_df.loc[train_index,val_cols],train_df.loc[train_index,'target'])\n    val_preds=model.predict(train_df.loc[val_index,val_cols])\n    #test_preds=model.predict(test_df[test_cols])\n    train_df.loc[val_index,'val_oof_pred']=val_preds\n    #test_df['final_test_pred']+=test_preds/CFG.folds\n    model_weight+=model.coef_/CFG.folds\n    bias+=model.intercept_/CFG.folds\nprint('ensemble cv score is:',rmse(train_df['target'],train_df['val_oof_pred']))\nprint('################################################################')\nfor i in range(len(val_cols)):\n    print(val_cols[i],' weight is:',np.round(model_weight[i],3))\nprint('ensemble bias is:',bias)\nprint('################################################################')\nprint(train_df[val_cols].corr())\ntrain_df.to_csv('ensemble_train.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T16:03:43.707553Z","iopub.status.idle":"2021-07-19T16:03:43.708305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}