{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport torch\nimport copy\nimport math\nimport gc\nfrom tqdm import tqdm\nimport torch.utils.data as D\nimport random\nimport os\nfrom transformers import AutoModelWithLMHead, AutoTokenizer,RobertaConfig, RobertaModel,AutoModelForSequenceClassification,AutoModelForMaskedLM\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch import nn\nfrom torch import optim\nimport time\nimport torch.nn.functional as F\nfrom transformers import (\n    AutoModel,\n    CONFIG_MAPPING,\n    MODEL_MAPPING,\n    AdamW,\n    AutoConfig,\n    AutoModelForMaskedLM,\n    AutoTokenizer,\n    DataCollatorForLanguageModeling,\n    SchedulerType,\n    get_scheduler,\n    set_seed,\n)\nfrom sklearn.linear_model import Ridge,Lasso\nimport lightgbm as lgb\nimport time\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-31T02:17:55.382564Z","iopub.execute_input":"2021-07-31T02:17:55.383008Z","iopub.status.idle":"2021-07-31T02:18:05.536018Z","shell.execute_reply.started":"2021-07-31T02:17:55.382923Z","shell.execute_reply":"2021-07-31T02:18:05.534958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# parameters for this notebook","metadata":{}},{"cell_type":"code","source":"class CFG:\n    seed=12345\n    test_avg_n=5\n    val_avg_n=5\n    max_len=256\n    batch_size=24\n    dropout_p=0.1\n    folds=5\n    cv_shuffle=False\n    pad_token_id=1\n    device=torch.device('cuda:0')\n    dtype=torch.float32","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.537698Z","iopub.execute_input":"2021-07-31T02:18:05.538062Z","iopub.status.idle":"2021-07-31T02:18:05.544776Z","shell.execute_reply.started":"2021-07-31T02:18:05.53802Z","shell.execute_reply":"2021-07-31T02:18:05.543932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df=pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ntest_df=pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\nres_df=pd.read_csv('/kaggle/input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.546716Z","iopub.execute_input":"2021-07-31T02:18:05.547294Z","iopub.status.idle":"2021-07-31T02:18:05.640666Z","shell.execute_reply.started":"2021-07-31T02:18:05.547256Z","shell.execute_reply":"2021-07-31T02:18:05.639698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RobertaDataset(D.Dataset):\n    def __init__(self, token, target):\n        self.token = token\n        self.target = target\n        \n    def __len__(self):\n        return self.token.shape[0]\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.token[idx].input_ids), \\\n                torch.tensor(self.token[idx].attention_mask), self.target[idx]\n    \ndef collate(batch):\n    ids, attns, targets = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True,padding_value=CFG.pad_token_id).to(CFG.device)\n    attns = pad_sequence(attns, batch_first=True,padding_value=CFG.pad_token_id).to(CFG.device)\n    targets = torch.tensor(targets).float().to(CFG.device)\n    return ids, attns, targets\n\ndef CV_split(m,k=5,shuffle=False,seed=7):\n    index=np.arange(m)\n    if shuffle:\n        np.random.seed(seed)\n        np.random.shuffle(index)\n    test_size=math.ceil(m/k)\n    split_indices=[]\n    for i in range(k):\n        bool_index=np.zeros(m)\n        bool_index[test_size*i:test_size*(i+1)]=1\n        bool_index=bool_index.astype('bool')\n        val_index=index[bool_index]\n        train_index=index[~bool_index]\n        split_indices.append((train_index,val_index))\n    return split_indices\n\ndef rmse(y1,y2):\n    score=np.sqrt(((y1-y2)**2).mean())\n    return score\n\ndef score_test(model,test_ldr,mode='train',avg_n=1):\n    if mode=='eval':\n        model.eval()\n    elif mode=='train':\n        model.train()\n    avg_pred=pd.DataFrame()\n    for i in range(avg_n):\n        preds=[]\n        for texts, attns, idx in test_ldr:\n            with torch.no_grad():\n                pred = model(texts,attns)\n                preds.append(pred)\n        preds=torch.cat(preds,axis=0)\n        preds=preds.to('cpu').numpy().reshape(-1)\n        avg_pred[f'pred{i+1}']=preds\n    #print(avg_pred.corr())\n    return avg_pred.values.mean(axis=1)\n\ndef tokenize(tokenizer,texts):\n    tokens=[]\n    for text in texts:\n        token=tokenizer(text,max_length=CFG.max_len,truncation=True, padding='max_length',add_special_tokens=True)\n        tokens.append(token)\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.642373Z","iopub.execute_input":"2021-07-31T02:18:05.642813Z","iopub.status.idle":"2021-07-31T02:18:05.660606Z","shell.execute_reply.started":"2021-07-31T02:18:05.642771Z","shell.execute_reply":"2021-07-31T02:18:05.659276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"markdown","source":"* remeber that different models has different mask token id\n* use different seed in dataloader for diversity","metadata":{}},{"cell_type":"markdown","source":"* F1\n* CV 0.477 LB 0.469\n* roberta base train with ITPT weights\n","metadata":{}},{"cell_type":"code","source":"class MyModel1(nn.Module):\n    def __init__(self, model):\n        super(MyModel1, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF1_CFG={}\nF1_CFG['max_len']=256\nF1_CFG['seed']=12345\nF1_CFG['batch_size']=24\nF1_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.661983Z","iopub.execute_input":"2021-07-31T02:18:05.662372Z","iopub.status.idle":"2021-07-31T02:18:05.673349Z","shell.execute_reply.started":"2021-07-31T02:18:05.662334Z","shell.execute_reply":"2021-07-31T02:18:05.672562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F2\n* CV 0.4839 LB 0.470\n* roberta large\n* mv state dict to cpu\n\n","metadata":{}},{"cell_type":"code","source":"class MyModel2(nn.Module):\n    def __init__(self, model):\n        super(MyModel2, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF2_CFG={}\nF2_CFG['max_len']=256\nF2_CFG['seed']=7\nF2_CFG['batch_size']=8\nF2_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.674879Z","iopub.execute_input":"2021-07-31T02:18:05.675321Z","iopub.status.idle":"2021-07-31T02:18:05.686962Z","shell.execute_reply.started":"2021-07-31T02:18:05.67528Z","shell.execute_reply":"2021-07-31T02:18:05.686092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F3\n* CV 0.487 LB 0.486\n* SimCSE roberta base","metadata":{}},{"cell_type":"code","source":"class MyModel3(nn.Module):\n    def __init__(self, model):\n        super(MyModel3, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF3_CFG={}\nF3_CFG['max_len']=256\nF3_CFG['seed']=33\nF3_CFG['batch_size']=24\nF3_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.6885Z","iopub.execute_input":"2021-07-31T02:18:05.688866Z","iopub.status.idle":"2021-07-31T02:18:05.698574Z","shell.execute_reply.started":"2021-07-31T02:18:05.688831Z","shell.execute_reply":"2021-07-31T02:18:05.697825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F4","metadata":{}},{"cell_type":"markdown","source":"* F5\n* CV 0.4773 LB 0.474\n* roberta base train with ITPT weights\n* ITPT model from: Maunish's pre-trained model","metadata":{}},{"cell_type":"code","source":"class MyModel5(nn.Module):\n    def __init__(self, model):\n        super(MyModel5, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF5_CFG={}\nF5_CFG['max_len']=256\nF5_CFG['seed']=12345\nF5_CFG['batch_size']=24\nF5_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.703157Z","iopub.execute_input":"2021-07-31T02:18:05.70343Z","iopub.status.idle":"2021-07-31T02:18:05.713096Z","shell.execute_reply.started":"2021-07-31T02:18:05.703405Z","shell.execute_reply":"2021-07-31T02:18:05.712211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F6\n* CV 0.4855 LB 0.471\n* almost no diversity same seed and model and pretrain","metadata":{}},{"cell_type":"code","source":"class MyModel6(nn.Module):\n    def __init__(self, model):\n        super(MyModel6, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF6_CFG={}\nF6_CFG['max_len']=256\nF6_CFG['seed']=12345\nF6_CFG['batch_size']=24\nF6_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.717034Z","iopub.execute_input":"2021-07-31T02:18:05.717295Z","iopub.status.idle":"2021-07-31T02:18:05.72959Z","shell.execute_reply.started":"2021-07-31T02:18:05.71727Z","shell.execute_reply":"2021-07-31T02:18:05.728596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F7\n* CV 0.4781 LB 0.473","metadata":{}},{"cell_type":"code","source":"class MyModel7(nn.Module):\n    def __init__(self,model):\n        super(MyModel7,self).__init__()\n        self.roberta = model             \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        return self.regressor(context_vector)\n    \nF7_CFG={}\nF7_CFG['max_len']=256\nF7_CFG['seed']=666\nF7_CFG['batch_size']=24\nF7_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.730945Z","iopub.execute_input":"2021-07-31T02:18:05.731431Z","iopub.status.idle":"2021-07-31T02:18:05.743159Z","shell.execute_reply.started":"2021-07-31T02:18:05.731357Z","shell.execute_reply":"2021-07-31T02:18:05.742115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F8\n* CV 0.4815  LB 0.472","metadata":{}},{"cell_type":"code","source":"class MyModel8(nn.Module):\n    def __init__(self, model):\n        super(MyModel8, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.output_layer(text_emb)\n        return x2\nF8_CFG={}\nF8_CFG['max_len']=256\nF8_CFG['seed']=54321\nF8_CFG['batch_size']=24\nF8_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.744718Z","iopub.execute_input":"2021-07-31T02:18:05.745135Z","iopub.status.idle":"2021-07-31T02:18:05.756102Z","shell.execute_reply.started":"2021-07-31T02:18:05.745091Z","shell.execute_reply":"2021-07-31T02:18:05.755202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F9\n* CV 0.4799 LB 0.475\n* roberta base\n* one linear output layer\n* 2 layer re-init","metadata":{}},{"cell_type":"code","source":"class MyModel9(nn.Module):\n    def __init__(self, model):\n        super(MyModel9, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.output_layer(text_emb)\n        return x2\nF9_CFG={}\nF9_CFG['max_len']=256\nF9_CFG['seed']=54321\nF9_CFG['batch_size']=24\nF9_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.757389Z","iopub.execute_input":"2021-07-31T02:18:05.758156Z","iopub.status.idle":"2021-07-31T02:18:05.767615Z","shell.execute_reply.started":"2021-07-31T02:18:05.758114Z","shell.execute_reply":"2021-07-31T02:18:05.766869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F10\n* CV 0.479 LB 0.479\n\n* roberta base\n* two linear output layer\n* 2 layer re-init\n* seed 777 to compare with F1","metadata":{}},{"cell_type":"code","source":"class MyModel10(nn.Module):\n    def __init__(self, model):\n        super(MyModel10, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF10_CFG={}\nF10_CFG['max_len']=256\nF10_CFG['seed']=12345\nF10_CFG['batch_size']=24\nF10_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.769684Z","iopub.execute_input":"2021-07-31T02:18:05.769938Z","iopub.status.idle":"2021-07-31T02:18:05.778962Z","shell.execute_reply.started":"2021-07-31T02:18:05.769915Z","shell.execute_reply":"2021-07-31T02:18:05.777901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F11\n* CV 0.4811 LB 0.466\n* roberta large\n* two linear output layer\n* 5 layer re init","metadata":{}},{"cell_type":"code","source":"class MyModel11(nn.Module):\n    def __init__(self, model):\n        super(MyModel11, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF11_CFG={}\nF11_CFG['max_len']=256\nF11_CFG['seed']=7\nF11_CFG['batch_size']=8\nF11_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.780332Z","iopub.execute_input":"2021-07-31T02:18:05.780864Z","iopub.status.idle":"2021-07-31T02:18:05.789567Z","shell.execute_reply.started":"2021-07-31T02:18:05.78083Z","shell.execute_reply":"2021-07-31T02:18:05.788633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F12\n* CV 0.4773 LB 0.473\n* roberta base\n* seed 777 to compare with F1\n* 10% warm up","metadata":{}},{"cell_type":"code","source":"class MyModel12(nn.Module):\n    def __init__(self, model):\n        super(MyModel12, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF12_CFG={}\nF12_CFG['max_len']=256\nF12_CFG['seed']=12345\nF12_CFG['batch_size']=24\nF12_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.790917Z","iopub.execute_input":"2021-07-31T02:18:05.791492Z","iopub.status.idle":"2021-07-31T02:18:05.80063Z","shell.execute_reply.started":"2021-07-31T02:18:05.79145Z","shell.execute_reply":"2021-07-31T02:18:05.799758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F13\n* CV 0.4816 LB 0.484\n* roberta base \n* two linear output layer\n* 3 layer re-init\n* seed 777 to compare with F1\n* lr diff don't apply to re-init layers\n* seems roberta base (not large) with re init do poorly in the LB test fold...","metadata":{}},{"cell_type":"code","source":"class MyModel13(nn.Module):\n    def __init__(self, model):\n        super(MyModel13, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF13_CFG={}\nF13_CFG['max_len']=256\nF13_CFG['seed']=12345\nF13_CFG['batch_size']=24\nF13_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.801915Z","iopub.execute_input":"2021-07-31T02:18:05.802294Z","iopub.status.idle":"2021-07-31T02:18:05.812171Z","shell.execute_reply.started":"2021-07-31T02:18:05.802257Z","shell.execute_reply":"2021-07-31T02:18:05.811338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F14\n* CV 0.4778 LB 0.468\n* roberta base \n* seed 777 to compare with F1\n* 10% warm up\n* batch size 32 and lr 8e-5","metadata":{}},{"cell_type":"code","source":"class MyModel14(nn.Module):\n    def __init__(self, model):\n        super(MyModel14, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF14_CFG={}\nF14_CFG['max_len']=256\nF14_CFG['seed']=12345\nF14_CFG['batch_size']=24\nF14_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.813731Z","iopub.execute_input":"2021-07-31T02:18:05.814181Z","iopub.status.idle":"2021-07-31T02:18:05.823611Z","shell.execute_reply.started":"2021-07-31T02:18:05.81414Z","shell.execute_reply":"2021-07-31T02:18:05.822787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F15\n* CV 0.4839 LB\n* roberta large\n* two linear output layer\n* 5 layer re init\n* lr with warm up","metadata":{}},{"cell_type":"code","source":"class MyModel15(nn.Module):\n    def __init__(self, model):\n        super(MyModel15, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF15_CFG={}\nF15_CFG['max_len']=256\nF15_CFG['seed']=7\nF15_CFG['batch_size']=8\nF15_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.825053Z","iopub.execute_input":"2021-07-31T02:18:05.825813Z","iopub.status.idle":"2021-07-31T02:18:05.835133Z","shell.execute_reply.started":"2021-07-31T02:18:05.825772Z","shell.execute_reply":"2021-07-31T02:18:05.834181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F16\n* CV 0.4824 LB 0.468\n* roberta large\n* two linear output layer\n* 5 layer re init\n* 6 epochs (=large lr in 5epochs)","metadata":{}},{"cell_type":"code","source":"class MyModel16(nn.Module):\n    def __init__(self, model):\n        super(MyModel16, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF16_CFG={}\nF16_CFG['max_len']=256\nF16_CFG['seed']=7\nF16_CFG['batch_size']=8\nF16_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.837195Z","iopub.execute_input":"2021-07-31T02:18:05.837557Z","iopub.status.idle":"2021-07-31T02:18:05.849137Z","shell.execute_reply.started":"2021-07-31T02:18:05.83753Z","shell.execute_reply":"2021-07-31T02:18:05.848206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F17\n* CV 0.47581798 LB 0.468\n* roberta base train with ITPT weights\n* bs 32 \n* fix oom\n* fix possible token id change\n* add reinit code","metadata":{}},{"cell_type":"code","source":"class MyModel17(nn.Module):\n    def __init__(self, model):\n        super(MyModel17, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\n    \nF17_CFG={}\nF17_CFG['max_len']=256\nF17_CFG['seed']=12345\nF17_CFG['batch_size']=24\nF17_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.851475Z","iopub.execute_input":"2021-07-31T02:18:05.852078Z","iopub.status.idle":"2021-07-31T02:18:05.861255Z","shell.execute_reply.started":"2021-07-31T02:18:05.852037Z","shell.execute_reply":"2021-07-31T02:18:05.860197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F18\n* CV 0.4874678 LB 0.488\n* Simcse base \n* seed 754\n* with attention head","metadata":{}},{"cell_type":"code","source":"class MyModel18(nn.Module):\n    def __init__(self,model):\n        super(MyModel18,self).__init__()\n        self.model = model             \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.model(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        return self.regressor(context_vector)\n    \nF18_CFG={}\nF18_CFG['max_len']=256\nF18_CFG['seed']=755\nF18_CFG['batch_size']=24\nF18_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.863915Z","iopub.execute_input":"2021-07-31T02:18:05.864219Z","iopub.status.idle":"2021-07-31T02:18:05.874091Z","shell.execute_reply.started":"2021-07-31T02:18:05.86417Z","shell.execute_reply":"2021-07-31T02:18:05.872968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F19\n* CV 0.4845 LB 0.483\n* Simcse base \n* seed 754\n* 2 layer re init","metadata":{}},{"cell_type":"code","source":"class MyModel19(nn.Module):\n    def __init__(self, model):\n        super(MyModel19, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\nF19_CFG={}\nF19_CFG['max_len']=256\nF19_CFG['seed']=755\nF19_CFG['batch_size']=24\nF19_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.875733Z","iopub.execute_input":"2021-07-31T02:18:05.87622Z","iopub.status.idle":"2021-07-31T02:18:05.88809Z","shell.execute_reply.started":"2021-07-31T02:18:05.876182Z","shell.execute_reply":"2021-07-31T02:18:05.886735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F20\n* CV 0.4796 LB 0.464\n* Simcse large\n* seed 754\n* two linear output layer\n* 5 layer re init","metadata":{}},{"cell_type":"code","source":"class MyModel20(nn.Module):\n    def __init__(self, model):\n        super(MyModel20, self).__init__()\n        self.model = model\n        self.output_layer = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.model.lm_head.dense(text_emb)\n        x3=self.output_layer(x2)\n        return x3\nF20_CFG={}\nF20_CFG['max_len']=256\nF20_CFG['seed']=755\nF20_CFG['batch_size']=8\nF20_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.894703Z","iopub.execute_input":"2021-07-31T02:18:05.895111Z","iopub.status.idle":"2021-07-31T02:18:05.902066Z","shell.execute_reply.started":"2021-07-31T02:18:05.895083Z","shell.execute_reply":"2021-07-31T02:18:05.901174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F22\n* CV LB\n* roberta large\n* gradient accumulation\n* batch size 16 lr 4e-5","metadata":{}},{"cell_type":"code","source":"class MyModel22(nn.Module):\n    def __init__(self, model):\n        super(MyModel22, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF22_CFG={}\nF22_CFG['max_len']=256\nF22_CFG['seed']=7\nF22_CFG['batch_size']=8\nF22_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.904481Z","iopub.execute_input":"2021-07-31T02:18:05.905059Z","iopub.status.idle":"2021-07-31T02:18:05.914463Z","shell.execute_reply.started":"2021-07-31T02:18:05.905023Z","shell.execute_reply":"2021-07-31T02:18:05.913661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F23\n* CV 0.4792 LB 0.465\n* roberta large\n* gradient accumulation\n* batch size 16 lr 2.5e-5","metadata":{}},{"cell_type":"code","source":"class MyModel23(nn.Module):\n    def __init__(self, model):\n        super(MyModel23, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF23_CFG={}\nF23_CFG['max_len']=256\nF23_CFG['seed']=7\nF23_CFG['batch_size']=8\nF23_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.915693Z","iopub.execute_input":"2021-07-31T02:18:05.916058Z","iopub.status.idle":"2021-07-31T02:18:05.926046Z","shell.execute_reply.started":"2021-07-31T02:18:05.916021Z","shell.execute_reply":"2021-07-31T02:18:05.924944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F24\n* CV 0.4815 LB 0.488\n* deberta base \n* seed 5311\n* bs 24","metadata":{}},{"cell_type":"code","source":"class MyModel24(nn.Module):\n    def __init__(self, model):\n        super(MyModel24, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF24_CFG={}\nF24_CFG['max_len']=256\nF24_CFG['seed']=721\nF24_CFG['batch_size']=24\nF24_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.927476Z","iopub.execute_input":"2021-07-31T02:18:05.927891Z","iopub.status.idle":"2021-07-31T02:18:05.937753Z","shell.execute_reply.started":"2021-07-31T02:18:05.927854Z","shell.execute_reply":"2021-07-31T02:18:05.936696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F25\n* CV 0.4831 LB 0.488\n* deberta base \n* seed 5311\n* bs 24\n* re init 2 layers","metadata":{}},{"cell_type":"code","source":"class MyModel25(nn.Module):\n    def __init__(self, model):\n        super(MyModel25, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF25_CFG={}\nF25_CFG['max_len']=256\nF25_CFG['seed']=721\nF25_CFG['batch_size']=24\nF25_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.939324Z","iopub.execute_input":"2021-07-31T02:18:05.939834Z","iopub.status.idle":"2021-07-31T02:18:05.949421Z","shell.execute_reply.started":"2021-07-31T02:18:05.939744Z","shell.execute_reply":"2021-07-31T02:18:05.948576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F26\n* CV 0.4757 LB 0.465\n* deberta large\n* seed 5311\n* re init 5 layers\n* bs 4\n* grad accum 4 lr 2.5","metadata":{}},{"cell_type":"code","source":"class MyModel26(nn.Module):\n    def __init__(self, model):\n        super(MyModel26, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF26_CFG={}\nF26_CFG['max_len']=256\nF26_CFG['seed']=721\nF26_CFG['batch_size']=8\nF26_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.952556Z","iopub.execute_input":"2021-07-31T02:18:05.952841Z","iopub.status.idle":"2021-07-31T02:18:05.961261Z","shell.execute_reply.started":"2021-07-31T02:18:05.952816Z","shell.execute_reply":"2021-07-31T02:18:05.960127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F27\n* CV 0.486 LB 0.464\n* simcse large\n* seed 754\n* re init 5 layers\n* grad accum 2 lr 2.5","metadata":{}},{"cell_type":"code","source":"class MyModel27(nn.Module):\n    def __init__(self, model):\n        super(MyModel27, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF27_CFG={}\nF27_CFG['max_len']=256\nF27_CFG['seed']=755\nF27_CFG['batch_size']=8\nF27_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.963067Z","iopub.execute_input":"2021-07-31T02:18:05.963561Z","iopub.status.idle":"2021-07-31T02:18:05.972635Z","shell.execute_reply.started":"2021-07-31T02:18:05.963524Z","shell.execute_reply":"2021-07-31T02:18:05.971663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F28\n* CV LB","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F29\n* CV 0.50150967 LB\n* bert base cased\n* seed 2334\n* bs 32 lr 8","metadata":{}},{"cell_type":"code","source":"class MyModel29(nn.Module):\n    def __init__(self, model):\n        super(MyModel29, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,768)\n        self.linear2 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\nF29_CFG={}\nF29_CFG['max_len']=256\nF29_CFG['seed']=2\nF29_CFG['batch_size']=24\nF29_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.973924Z","iopub.execute_input":"2021-07-31T02:18:05.974473Z","iopub.status.idle":"2021-07-31T02:18:05.985059Z","shell.execute_reply.started":"2021-07-31T02:18:05.974436Z","shell.execute_reply":"2021-07-31T02:18:05.984265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F30\n* CV 0.47575  LB 0.462\n* deberta large\n* seed 5311\n* re init 5 layers\n* bs 4\n* grad accum 2 lr 2","metadata":{}},{"cell_type":"code","source":"class MyModel30(nn.Module):\n    def __init__(self, model):\n        super(MyModel30, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1024)\n        self.linear2 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        x3=self.linear2(x2)\n        return x3\n    \nF30_CFG={}\nF30_CFG['max_len']=256\nF30_CFG['seed']=721\nF30_CFG['batch_size']=8\nF30_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.986413Z","iopub.execute_input":"2021-07-31T02:18:05.986806Z","iopub.status.idle":"2021-07-31T02:18:05.995092Z","shell.execute_reply.started":"2021-07-31T02:18:05.986751Z","shell.execute_reply":"2021-07-31T02:18:05.994161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F31\n* CV LB\n* bert large cased\n* seed 2334\n* bs 32 lr 8","metadata":{}},{"cell_type":"code","source":"class MyModel31(nn.Module):\n    def __init__(self, model):\n        super(MyModel31, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\nF31_CFG={}\nF31_CFG['max_len']=256\nF31_CFG['seed']=7212\nF31_CFG['batch_size']=8\nF31_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:05.996725Z","iopub.execute_input":"2021-07-31T02:18:05.997349Z","iopub.status.idle":"2021-07-31T02:18:06.006841Z","shell.execute_reply.started":"2021-07-31T02:18:05.997307Z","shell.execute_reply":"2021-07-31T02:18:06.005937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F32\n* CV 0.4749 LB 0.469\n* deberta large\n* seed 12222\n* no re init\n* bs 4\n* grad accum 2 lr 2\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel32(nn.Module):\n    def __init__(self, model):\n        super(MyModel32, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF32_CFG={}\nF32_CFG['max_len']=256\nF32_CFG['seed']=20134\nF32_CFG['batch_size']=8\nF32_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.008276Z","iopub.execute_input":"2021-07-31T02:18:06.008741Z","iopub.status.idle":"2021-07-31T02:18:06.019721Z","shell.execute_reply.started":"2021-07-31T02:18:06.008702Z","shell.execute_reply":"2021-07-31T02:18:06.018685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F33\n* CV 0.4835 LB 0.464\n* roberta large with ITPT\n* seed 555666\n* re init 5 layers\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel33(nn.Module):\n    def __init__(self, model):\n        super(MyModel33, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF33_CFG={}\nF33_CFG['max_len']=256\nF33_CFG['seed']=20134\nF33_CFG['batch_size']=8\nF33_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.021176Z","iopub.execute_input":"2021-07-31T02:18:06.021705Z","iopub.status.idle":"2021-07-31T02:18:06.030789Z","shell.execute_reply.started":"2021-07-31T02:18:06.021666Z","shell.execute_reply":"2021-07-31T02:18:06.029562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F34\n* CV LB\n* bart base\n* seed 9999\n* one linear layer\n","metadata":{}},{"cell_type":"code","source":"class MyModel34(nn.Module):\n    def __init__(self, model):\n        super(MyModel34, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(768,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['last_hidden_state']\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n\nF34_CFG={}\nF34_CFG['max_len']=256\nF34_CFG['seed']=201\nF34_CFG['batch_size']=24\nF34_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.032485Z","iopub.execute_input":"2021-07-31T02:18:06.032957Z","iopub.status.idle":"2021-07-31T02:18:06.042342Z","shell.execute_reply.started":"2021-07-31T02:18:06.032915Z","shell.execute_reply":"2021-07-31T02:18:06.041472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F35\n* CV 0.4986 LB\n* electra base\n* seed 75412\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel35(nn.Module):\n    def __init__(self, model):\n        super(MyModel35, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF35_CFG={}\nF35_CFG['max_len']=256\nF35_CFG['seed']=75412\nF35_CFG['batch_size']=24\nF35_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.043716Z","iopub.execute_input":"2021-07-31T02:18:06.044128Z","iopub.status.idle":"2021-07-31T02:18:06.054849Z","shell.execute_reply.started":"2021-07-31T02:18:06.044087Z","shell.execute_reply":"2021-07-31T02:18:06.053976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F36\n* CV 0.4931 LB 0.480\n* electra large\n* seed 75412\n* 1 linear layer\n* re init 5 layers","metadata":{}},{"cell_type":"code","source":"class MyModel36(nn.Module):\n    def __init__(self, model):\n        super(MyModel36, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF36_CFG={}\nF36_CFG['max_len']=256\nF36_CFG['seed']=75412\nF36_CFG['batch_size']=8\nF36_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.056275Z","iopub.execute_input":"2021-07-31T02:18:06.056646Z","iopub.status.idle":"2021-07-31T02:18:06.065591Z","shell.execute_reply.started":"2021-07-31T02:18:06.056607Z","shell.execute_reply":"2021-07-31T02:18:06.064625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F37\n* CV LB\n* xlm-roberta base\n* seed 900\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel37(nn.Module):\n    def __init__(self, model):\n        super(MyModel37, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF37_CFG={}\nF37_CFG['max_len']=256\nF37_CFG['seed']=900\nF37_CFG['batch_size']=24\nF37_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.066992Z","iopub.execute_input":"2021-07-31T02:18:06.06741Z","iopub.status.idle":"2021-07-31T02:18:06.074936Z","shell.execute_reply.started":"2021-07-31T02:18:06.067351Z","shell.execute_reply":"2021-07-31T02:18:06.074088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F38\n* CV 0.4782 LB 0.477\n* roberta base\n* seed 900\n* 1 linear layer\n* len 300\n* bs 24","metadata":{}},{"cell_type":"code","source":"class MyModel38(nn.Module):\n    def __init__(self, model):\n        super(MyModel38, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(768,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF38_CFG={}\nF38_CFG['max_len']=300\nF38_CFG['seed']=900\nF38_CFG['batch_size']=24\nF38_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.076403Z","iopub.execute_input":"2021-07-31T02:18:06.076966Z","iopub.status.idle":"2021-07-31T02:18:06.086689Z","shell.execute_reply.started":"2021-07-31T02:18:06.07693Z","shell.execute_reply":"2021-07-31T02:18:06.085661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F39\n* CV LB\n* xlnet-base\n* seed 900\n* 1 linear layer\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F40\n* CV 0.4836 LB 0.463\n* Simcse large unsup\n* seed 754\n* re init 5 layers\n* bs 8 lr 2e-5\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel40(nn.Module):\n    def __init__(self, model):\n        super(MyModel40, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF40_CFG={}\nF40_CFG['max_len']=256\nF40_CFG['seed']=901\nF40_CFG['batch_size']=8\nF40_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.08808Z","iopub.execute_input":"2021-07-31T02:18:06.088563Z","iopub.status.idle":"2021-07-31T02:18:06.096823Z","shell.execute_reply.started":"2021-07-31T02:18:06.088527Z","shell.execute_reply":"2021-07-31T02:18:06.095911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F41\n* CV 0.4796 LB 0.475\n* deberta-large-mnli\n* seed 5311\n* re init 5 layers\n* bs 4\n* grad accum 2 lr 2\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel41(nn.Module):\n    def __init__(self, model):\n        super(MyModel41, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF41_CFG={}\nF41_CFG['max_len']=256\nF41_CFG['seed']=111\nF41_CFG['batch_size']=8\nF41_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.098356Z","iopub.execute_input":"2021-07-31T02:18:06.098912Z","iopub.status.idle":"2021-07-31T02:18:06.107417Z","shell.execute_reply.started":"2021-07-31T02:18:06.098876Z","shell.execute_reply":"2021-07-31T02:18:06.106521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F42\n* CV 0.4883 LB \n* roberta large roberta-large-mnli\n* seed 5551\n* re init 5 layers\n* bs 8 lr 2e-5\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel42(nn.Module):\n    def __init__(self, model):\n        super(MyModel42, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF42_CFG={}\nF42_CFG['max_len']=256\nF42_CFG['seed']=1112\nF42_CFG['batch_size']=8\nF42_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.1087Z","iopub.execute_input":"2021-07-31T02:18:06.109077Z","iopub.status.idle":"2021-07-31T02:18:06.11918Z","shell.execute_reply.started":"2021-07-31T02:18:06.109007Z","shell.execute_reply":"2021-07-31T02:18:06.118407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F43\n* CV 0.4970 LB \n* roberta-large-openai-detector\n* seed 5551\n* re init 5 layers\n* bs 8 lr 2e-5\n* 1 linear layer","metadata":{}},{"cell_type":"code","source":"class MyModel43(nn.Module):\n    def __init__(self, model):\n        super(MyModel43, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF43_CFG={}\nF43_CFG['max_len']=256\nF43_CFG['seed']=1112\nF43_CFG['batch_size']=8\nF43_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.120668Z","iopub.execute_input":"2021-07-31T02:18:06.121042Z","iopub.status.idle":"2021-07-31T02:18:06.129219Z","shell.execute_reply.started":"2021-07-31T02:18:06.121003Z","shell.execute_reply":"2021-07-31T02:18:06.128292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F50\n* CV 0.480 LB 0.465\n* funnel large\n* bs 16\n* lr 3e-5","metadata":{}},{"cell_type":"code","source":"class MyModel50(nn.Module):\n    def __init__(self, model):\n        super(MyModel50, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF50_CFG={}\nF50_CFG['max_len']=256\nF50_CFG['seed']=711223\nF50_CFG['batch_size']=8\nF50_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.130754Z","iopub.execute_input":"2021-07-31T02:18:06.131443Z","iopub.status.idle":"2021-07-31T02:18:06.140184Z","shell.execute_reply.started":"2021-07-31T02:18:06.13132Z","shell.execute_reply":"2021-07-31T02:18:06.139251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* F51\n* CV 0.479 LB 0.469\n* electra large\n* bs 16\n* lr 3.5e-5\n* no reinit","metadata":{}},{"cell_type":"code","source":"class MyModel51(nn.Module):\n    def __init__(self, model):\n        super(MyModel51, self).__init__()\n        self.model = model\n        self.linear1 = nn.Linear(1024,1)\n        \n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2\n    \nF51_CFG={}\nF51_CFG['max_len']=256\nF51_CFG['seed']=91\nF51_CFG['batch_size']=8\nF51_CFG['dropout_p']=0.1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.141605Z","iopub.execute_input":"2021-07-31T02:18:06.142198Z","iopub.status.idle":"2021-07-31T02:18:06.151292Z","shell.execute_reply.started":"2021-07-31T02:18:06.142156Z","shell.execute_reply":"2021-07-31T02:18:06.150469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def inference_wrapper(model_name,model_class,model_CFG,test_df,train_df):\n    # moad model cfg\n    for key,val in model_CFG.items():\n        setattr(CFG,key,val)\n    # fix random seed    \n    random.seed(CFG.seed)\n    os.environ['PYTHONHASHSEED'] = str(CFG.seed)\n    np.random.seed(CFG.seed)\n    torch.manual_seed(CFG.seed)\n    torch.cuda.manual_seed(CFG.seed)\n    torch.cuda.manual_seed_all(CFG.seed)\n    torch.backends.cudnn.deterministic = True\n    # load model\n    notebook=model_name+'-train'\n    path='../input/'+notebook+'/model_init/'\n    config = AutoConfig.from_pretrained(path, output_hidden_states=True,attention_probs_dropout_prob=CFG.dropout_p,hidden_dropout_prob=CFG.dropout_p)\n    tokenizer = AutoTokenizer.from_pretrained(path,model_max_length=CFG.max_len)\n    CFG.pad_token_id=tokenizer.pad_token_id\n    if int(model_name[1:])<=23:\n        model =AutoModelForMaskedLM.from_pretrained(path,config=config)\n    else:\n        model =AutoModel.from_pretrained(path,config=config)\n    model=model_class(model)\n    # load state dicts\n    state_dicts=[]\n    for i in range(CFG.folds):\n        checkpoint=torch.load('../input/'+notebook+f'/fold_{i+1}_model',map_location=CFG.device)\n        state_dicts.append(checkpoint['model_state_dict'])\n    # create data loader\n    \n    # test df\n    if test_df is not None:\n        test_df['token'] = tokenize(tokenizer,test_df.excerpt)\n        test_dataset = RobertaDataset(test_df.token, test_df.index)\n        # shuffle = False !!\n        test_dataloader = D.DataLoader(test_dataset, batch_size=CFG.batch_size,\n                                     shuffle=False, collate_fn = collate,num_workers=0)\n        test_df['test/'+model_name]=0\n        for i in range(CFG.folds):\n            model.load_state_dict(state_dicts[i])\n            model.to(CFG.device)\n            preds=score_test(model,test_dataloader,mode='train',avg_n=CFG.test_avg_n)\n            test_df['test/'+model_name]+=preds/CFG.folds\n    # train\n    if train_df is not None:\n        print('bug?')\n        train_df['val/'+model_name]=0\n        train_df['token'] = tokenize(tokenizer,train_df.excerpt)\n        train_dataset = RobertaDataset(train_df.token, train_df.index)\n        split_indices=CV_split(len(train_df),k=CFG.folds,shuffle=CFG.cv_shuffle,seed=7)\n        for i in range(CFG.folds):\n            train_index,val_index=split_indices[i]\n            val_dataset = D.Subset(train_dataset, val_index)\n            val_dataloader = D.DataLoader(val_dataset, batch_size=CFG.batch_size,\n                                          shuffle=False, collate_fn = collate,num_workers=0)\n            model.load_state_dict(state_dicts[i])\n            model.to(CFG.device)\n            preds=score_test(model,val_dataloader,mode='train',avg_n=CFG.val_avg_n)\n            train_df.loc[val_index,'val/'+model_name]=preds\n    del model\n    del state_dicts\n    del checkpoint\n    del test_dataloader\n    torch.cuda.empty_cache()\n    gc.collect()\n    return ","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.152629Z","iopub.execute_input":"2021-07-31T02:18:06.153155Z","iopub.status.idle":"2021-07-31T02:18:06.171902Z","shell.execute_reply.started":"2021-07-31T02:18:06.153114Z","shell.execute_reply":"2021-07-31T02:18:06.170939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inference_tasks=[\n                 ('f2',MyModel2,F2_CFG),\n                 #('f14',MyModel14,F14_CFG),\n                 ('f17',MyModel17,F17_CFG),\n                 #('f20',MyModel20,F20_CFG),\n                 ('f23',MyModel23,F23_CFG),\n                 #('f30',MyModel30,F30_CFG),\n                 ('f32',MyModel32,F32_CFG),\n                 #('f33',MyModel33,F33_CFG),\n                 #('f36',MyModel36,F36_CFG),\n                 #('f40',MyModel40,F40_CFG),\n                 ('f41',MyModel41,F41_CFG),\n                ('f50',MyModel50,F50_CFG),\n                ('f51',MyModel51,F51_CFG),\n                ]\n\nfor i,(model_name,model_class,model_CFG) in tqdm(enumerate(inference_tasks)):\n    model_CFG['seed']=i\n    model_CFG['batch_size']*=6\n    inference_wrapper(model_name,model_class,model_CFG,test_df=test_df,train_df=None)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-31T02:18:06.174338Z","iopub.execute_input":"2021-07-31T02:18:06.174929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation & Ensemble","metadata":{}},{"cell_type":"code","source":"berts_val=[]\nberts_test=[]\nfor model_name,_,_ in inference_tasks:\n    berts_val.append('val/'+model_name)\n    berts_test.append('test/'+model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* model predictions on trainning set","metadata":{}},{"cell_type":"code","source":"train_df=pd.read_csv('../input/f-inference-1-1/ensemble_train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* some feature engineering","metadata":{}},{"cell_type":"code","source":"def feature_generate(df):\n    new_features=[]\n    # count of char\n    # rmk: as it is excerpt, so lengh may not matter\n    df['char_count']=df['excerpt'].apply(len)\n    new_features.append('char_count')\n    # count of words\n    df['word_count']=df['excerpt'].apply(lambda s:len(s.split(' ')))\n    new_features.append('word_count')\n    # count of sentence\n    # rmk: what if end with '!' '?' ...\n    df['sentence_count']=df['excerpt'].apply(lambda s:len(s.split('.')))\n    new_features.append('sentence_count')\n    # average char per word\n    df['c2w']=df['char_count']/df['word_count']\n    new_features.append('c2w')\n    # avg word per sentence\n    df['w2s']=df['word_count']/df['sentence_count']\n    new_features.append('w2s')\n    # count the unique words\n    df['unique_word_count'] = df['excerpt'].apply(lambda s: len(set( s.split(' ') )))\n    new_features.append('unique_word_count')\n    # text diversity\n    df['word_diversity'] = df['unique_word_count'] / df['word_count']\n    new_features.append('word_diversity')\n    # word lengths\n    words = df['excerpt'].apply(lambda s: s.split(' '))\n    word_lengths = words.apply(lambda s: [len(f) for f in s ])\n    df['longest_w_len'] = word_lengths.apply(max)\n    new_features.append('longest_w_len')\n    df['avg_w_len'] = word_lengths.apply(np.mean)\n    new_features.append('avg_w_len')\n#     # word freq\n#     word_freqs = words.apply(lambda s: [word_freq(f) for f in s ])\n#     df['avg_freq']=word_freqs.apply(np.mean)\n#     df['min_freq']=word_freqs.apply(min)\n#     new_features.append('avg_freq')\n#     new_features.append('min_freq')\n    return new_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extra_features=feature_generate(train_df)\n_=feature_generate(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.Ridge Regression","metadata":{}},{"cell_type":"code","source":"val_cols=berts_val\ntest_cols=berts_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['ridge_pred']=0\ntest_df['ridge_pred']=0\nmodel_weight=0\nbias=0\nfor i,(train_index,val_index) in enumerate(CV_split(len(train_df),k=CFG.folds,shuffle=False,seed=7)):\n    model=Ridge(alpha=0.025,fit_intercept=True,normalize=True)\n    model.fit(train_df.loc[train_index,val_cols],train_df.loc[train_index,'target'])\n    val_preds=model.predict(train_df.loc[val_index,val_cols])\n    test_preds=model.predict(test_df[test_cols])\n    train_df.loc[val_index,'ridge_pred']=val_preds\n    test_df['ridge_pred']+=test_preds/CFG.folds\n    model_weight+=model.coef_/CFG.folds\n    bias+=model.intercept_/CFG.folds\nprint('ensemble cv score is:',rmse(train_df['target'],train_df['ridge_pred']))\nprint('################################################################')\nfor i in range(len(val_cols)):\n    print(val_cols[i],' weight is:',np.round(model_weight[i],3))\nprint('ensemble bias is:',bias)\nprint('################################################################')\nprint(train_df[val_cols].corr())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.Lightgbm","metadata":{}},{"cell_type":"code","source":"params=\\\n  {'n_estimators':3000,\n   'boosting_type': 'gbdt',\n   'objective':'regression',\n   'metric':'rmse',\n   'subsample': 0.7, \n   'subsample_freq': 1,\n   #'num_leaves':124,\n   'min_data_in_leaf':40,\n   'feature_fraction_bynode':np.sqrt(0.6),\n   'feature_fraction': np.sqrt(0.6),            \n   'learning_rate': 0.005,\n   'max_bin':255,\n   #'cat_l2':10,\n   #'max_depth':5,\n   'boost_from_average':True,\n   'nthread' : 8,\n    'lambda_l1': 2,  \n    'lambda_l2': 20,\n  #'min_gain_to_split':0.0001\n   'early_stopping_rounds':200,\n   'verbose':-1\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_cols=berts_val+extra_features\ntest_cols=berts_test+extra_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_indices=CV_split(len(train_df),k=5,shuffle=False,seed=7)\ntrain_df['lgb_pred']=0\ntest_df['lgb_pred']=0\nfor fold,(train_index,val_index) in enumerate(split_indices):\n    X_train=train_df.loc[train_index,val_cols].values\n    X_val=train_df.loc[val_index,val_cols].values\n    Y_train=train_df.loc[train_index,'target'].values\n    Y_val=train_df.loc[val_index,'target'].values\n    \n    lgb_train= lgb.Dataset(X_train,Y_train,feature_name=val_cols)\n    lgb_val= lgb.Dataset(X_val,Y_val,feature_name=val_cols)\n    model=lgb.train(params,\n                   lgb_train,\n                   valid_sets=(lgb_val),\n                   valid_names=('fold '+str(fold+1)+' val set'),\n                   verbose_eval=100)\n\n    val_preds=model.predict(X_val)\n    test_preds=model.predict(test_df[test_cols])\n    train_df.loc[val_index,'lgb_pred']=val_preds\n    test_df['lgb_pred']+=test_preds/CFG.folds\nprint('################################################################')\nprint('ensemble cv score is:',rmse(train_df['target'],train_df['lgb_pred']))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation","metadata":{}},{"cell_type":"code","source":"final_features=['ridge_pred','lgb_pred']\nX=train_df[final_features].values\nY=train_df['target'].values.reshape(-1,1)\nw=np.linalg.inv(X.T@X)@X.T@Y\nfor i,feature in enumerate(final_features):\n    print(feature,' weight: ',w[i])\ntrain_df['final_pred']=(train_df[final_features].values@w).reshape(-1)\ntest_df['final_pred']=(test_df[final_features].values@w).reshape(-1)\nprint('final cv: ',rmse(train_df['target'],train_df['final_pred']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res_df['target']=test_df['final_pred']\nres_df.to_csv('submission.csv',index=False)\nres_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}