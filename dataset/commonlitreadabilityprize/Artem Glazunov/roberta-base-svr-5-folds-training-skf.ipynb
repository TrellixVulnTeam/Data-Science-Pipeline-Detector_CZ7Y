{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-20T18:21:35.931514Z","iopub.execute_input":"2021-07-20T18:21:35.931907Z","iopub.status.idle":"2021-07-20T18:21:35.955805Z","shell.execute_reply.started":"2021-07-20T18:21:35.931805Z","shell.execute_reply":"2021-07-20T18:21:35.95505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.svm import SVR\nimport tqdm\nfrom matplotlib import pyplot as plt\nimport copy\nimport gc\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:21:35.958523Z","iopub.execute_input":"2021-07-20T18:21:35.958792Z","iopub.status.idle":"2021-07-20T18:21:42.56477Z","shell.execute_reply.started":"2021-07-20T18:21:35.958768Z","shell.execute_reply":"2021-07-20T18:21:42.563901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%bash\ncp ../input/roberta-base-save/rob.zip .\ncp ../input/roberta-base-save/rob_tok.zip .\nunzip rob.zip\nunzip rob_tok.zip \nrm -r rob.zip rob_tok.zip","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:21:42.566432Z","iopub.execute_input":"2021-07-20T18:21:42.566721Z","iopub.status.idle":"2021-07-20T18:21:52.394888Z","shell.execute_reply.started":"2021-07-20T18:21:42.566695Z","shell.execute_reply":"2021-07-20T18:21:52.393984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import StratifiedKFold\nimport tqdm\nimport gc\nfrom sklearn.svm import SVR\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom hyperopt.pyll import scope as ho_scope","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:21:52.396916Z","iopub.execute_input":"2021-07-20T18:21:52.397325Z","iopub.status.idle":"2021-07-20T18:21:52.80069Z","shell.execute_reply.started":"2021-07-20T18:21:52.397283Z","shell.execute_reply":"2021-07-20T18:21:52.799825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:21:52.802687Z","iopub.execute_input":"2021-07-20T18:21:52.802932Z","iopub.status.idle":"2021-07-20T18:21:52.856462Z","shell.execute_reply.started":"2021-07-20T18:21:52.802907Z","shell.execute_reply":"2021-07-20T18:21:52.855627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:21:52.860272Z","iopub.execute_input":"2021-07-20T18:21:52.860516Z","iopub.status.idle":"2021-07-20T18:21:52.980441Z","shell.execute_reply.started":"2021-07-20T18:21:52.860492Z","shell.execute_reply":"2021-07-20T18:21:52.979391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ContinuousStratifiedKFold(StratifiedKFold):\n    def split(self, X, y, groups=None):\n        num_bins = int(np.floor(1 + np.log2(len(y))))\n        bins = pd.cut(y, bins=num_bins, labels=False)\n        return super().split(X, bins, groups)\n    \nclass BERTRegressor(torch.nn.Module): \n    def __init__(self, pretrained_src = 'rob'): \n        super().__init__()\n        self.bert = RobertaModel.from_pretrained(pretrained_src)\n        self.linear = torch.nn.Linear(768, 1)\n        self.dropout = torch.nn.Dropout(0.15)\n        \n    def forward(self, input_ids, attention_mask): #x - tokenized batch\n        hidden = self.bert(input_ids, \n                           attention_mask=attention_mask)[0][:, 0, :]#CLS token output                                                          \n        output = self.linear(self.dropout(hidden))\n        return output\n\nclass RMSELoss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = torch.nn.MSELoss()\n        \n    def forward(self,yhat,y):\n        loss = torch.sqrt(self.mse(yhat,y))\n        return loss\n\ndef rmse_metric(y_true, y_pred):\n    return np.sqrt(mse(y_true, y_pred))\n\n\ndef evaluate(dataloader_val, model):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in tqdm.notebook.tqdm(dataloader_val):\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1]\n        }\n        target = batch[2]\n\n        with torch.no_grad():        \n            output = model(**inputs)\n            \n        loss = criterion(output, target.view(-1,1))\n        loss_val_total += loss\n\n        output = output.detach().cpu().numpy()\n        target = target.cpu().numpy()\n        predictions.append(output)\n        true_vals.append(target)\n    \n    loss_val_avg = loss_val_total / len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals\n\n\ndef get_bert_embeddings(embedder, dataloader, device = device):\n    embedder.eval()\n    embeddings_all = []\n    for batch in tqdm.notebook.tqdm(dataloader):        \n        batch = tuple(b.to(device) for b in batch)        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1]\n                 }\n        with torch.no_grad():        \n            output = embedder(**inputs)[0][:, 0, :]#CLS token output \n\n        embeddings_batch = output.detach().cpu().numpy()\n        embeddings_all.append(embeddings_batch)\n\n    return np.vstack(embeddings_all)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:21:52.981906Z","iopub.execute_input":"2021-07-20T18:21:52.982271Z","iopub.status.idle":"2021-07-20T18:21:52.998184Z","shell.execute_reply.started":"2021-07-20T18:21:52.982237Z","shell.execute_reply":"2021-07-20T18:21:52.996864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(\n    'rob_tok'\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:21:53.001265Z","iopub.execute_input":"2021-07-20T18:21:53.001715Z","iopub.status.idle":"2021-07-20T18:21:53.114783Z","shell.execute_reply.started":"2021-07-20T18:21:53.001671Z","shell.execute_reply":"2021-07-20T18:21:53.113783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\n\nwarm_prop = 0.1\nepochs = 8\nclip = 1","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:21:53.116255Z","iopub.execute_input":"2021-07-20T18:21:53.1166Z","iopub.status.idle":"2021-07-20T18:21:53.896494Z","shell.execute_reply.started":"2021-07-20T18:21:53.116564Z","shell.execute_reply":"2021-07-20T18:21:53.895372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = ContinuousStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nprint('START')\nfor k, (train_ids, val_ids) in enumerate(tqdm.notebook.tqdm(kf.split(X=data, y=data['target'].values))):\n    print('**************')\n    print('------------')\n    print('**************')\n    print(f'ITERATION {k} starts')\n\n    gc.collect()\n\n    print('------------')\n    print('data slicing ...')\n\n    data_train = data.iloc[train_ids]['excerpt'].values\n    data_val = data.iloc[val_ids]['excerpt'].values\n    print(f'train/val data shapes: {data_train.shape}, {data_val.shape}')\n    target_train = data.iloc[train_ids]['target'].values\n    target_val = data.iloc[val_ids]['target'].values\n\n\n    print('------------')\n    print('data preparation ...')\n    encoded_data_train = tokenizer.batch_encode_plus(\n        data_train,\n        add_special_tokens=True,\n        return_attention_mask=True,\n        pad_to_max_length=True,\n        max_length=512,\n        return_tensors='pt',\n    )\n\n    encoded_data_val = tokenizer.batch_encode_plus(\n        data_val,\n        add_special_tokens=True,\n        return_attention_mask=True,\n        pad_to_max_length=True,\n        max_length=512,\n        return_tensors='pt'\n    )\n\n\n    input_ids_train = encoded_data_train['input_ids']\n    attention_masks_train = encoded_data_train['attention_mask']\n    values_train = torch.tensor(target_train, dtype=torch.float)\n\n    input_ids_val = encoded_data_val['input_ids']\n    attention_masks_val = encoded_data_val['attention_mask']\n    values_val = torch.tensor(target_val, dtype=torch.float)\n\n    dataset_train = TensorDataset(input_ids_train,\n                                 attention_masks_train,\n                                 values_train)\n    dataset_val = TensorDataset(input_ids_val,\n                                attention_masks_val,\n                                values_val)\n\n    dataloader_train = DataLoader(\n        dataset_train,\n        sampler=RandomSampler(dataset_train),\n        batch_size=BATCH_SIZE\n    )\n\n    dataloader_val = DataLoader(\n        dataset_val,\n        sampler=RandomSampler(dataset_val),\n        batch_size=2*BATCH_SIZE\n    )\n\n    print('------------')\n    print('Roberta finetuning ...')\n\n    model = BERTRegressor().to(device)\n    criterion = RMSELoss()\n    optimizer = AdamW(\n        model.parameters(),\n        lr= 3e-5,#the original paper:2e-5 -> 5e-5\n        eps=1e-8\n    )\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(len(dataloader_train)*epochs * warm_prop),\n        num_training_steps=len(dataloader_train)*epochs\n    )\n\n\n    best_val_loss = float('inf')\n    for epoch in tqdm.notebook.tqdm(range(epochs)):\n        print(f'Epoch {epoch}')\n        model.train()\n\n        epoch_loss = 0\n        for batch in tqdm.notebook.tqdm(dataloader_train):\n\n            batch = tuple(b.to(device) for b in batch)\n            inputs = {'input_ids':      batch[0],\n                    'attention_mask': batch[1]\n              }\n            target = batch[2]\n\n            optimizer.zero_grad()        \n\n            output = model(**inputs)     \n            loss = criterion(output, target.view(-1,1))      \n            loss.backward()\n            epoch_loss += loss.item()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)      \n            optimizer.step()\n            scheduler.step()     \n\n        val_loss, predictions, true_vals = evaluate(dataloader_val, model)\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss        \n            torch.save(model.state_dict(), f'roberta_base_{k}.pt')\n            best_model = copy.deepcopy(model).cpu()\n        train_loss = epoch_loss / len(dataloader_train)\n        rmse_val = rmse_metric(true_vals, predictions)\n        print('-------')\n        print(f'Training loss: {train_loss}')\n        print(f'Validation loss: {val_loss}')\n        print(f\"RMSE on validation: {rmse_val}\")\n    \n    print('loaded best model with lm head performance...')\n    dataloader_val_frozen = DataLoader(\n        dataset_val,\n        batch_size=2*BATCH_SIZE\n    )  \n    \n    model = BERTRegressor()\n    PATH = f'roberta_base_{k}.pt'\n    model.load_state_dict(torch.load(PATH), strict=False)\n    model.to(device)\n    val_loss, predictions, true_vals = evaluate(dataloader_val_frozen, model)\n    rmse_val = rmse_metric(true_vals, predictions)\n    print(f\"RMSE on validation: {rmse_val}\")\n    \n    del model\n    gc.collect()\n\n    print('------------')\n    print('Embeddings extraction ...')\n\n    embedder = best_model.bert.to(device)    \n    \n    dataloader_train_frozen = DataLoader(\n        dataset_train,\n        batch_size=2*BATCH_SIZE,\n    )\n\n    embeddings_train = get_bert_embeddings(embedder, dataloader_train_frozen)\n    embeddings_val = get_bert_embeddings(embedder, dataloader_val_frozen)\n    \n    del embedder\n    gc.collect()\n    \n    print('------------')\n    print('SVR head HP tuning ...')\n    def hyperopt_train_test(params):\n        estimator = SVR(**params)  \n        estimator.fit(embeddings_train, target_train)\n        preds = estimator.predict(embeddings_val)\n        metric = rmse_metric(target_val, preds)\n        return metric\n\n    space_svr = {'C':  hp.loguniform('C', np.log(0.0001), np.log(1000)) - 0.0001,\n                 'gamma':  hp.loguniform('gamma', np.log(0.0001), np.log(1000)) - 0.0001\n                }\n    def f(params):\n        rmse_metric_val = hyperopt_train_test(params)\n        return {'loss': rmse_metric_val, 'status': STATUS_OK}\n\n    trials = Trials()\n    best = fmin(f, space_svr, algo = tpe.suggest, max_evals = 50, trials=trials)\n    print('best', best)\n\n\n    svr_head = SVR(**best)    \n    svr_head.fit(embeddings_train, target_train)\n    filename = f'svr_head_{k}.pkl'\n    pickle.dump(svr_head, open(filename, 'wb'))\n    svr_head = pickle.load(open(filename, 'rb'))\n    preds_svr = svr_head.predict(embeddings_val)\n    rmse_val = rmse_metric(target_val, preds_svr)\n    print(f\"RMSE on validation: {rmse_val}\")    \n\n    print('------------')\n    print(f'Iteration {k} completed.')\n    \nprint('**************')\nprint('------------')\nprint('**************')    \nprint('FINISH')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:26:20.164852Z","iopub.execute_input":"2021-07-20T18:26:20.165217Z","iopub.status.idle":"2021-07-20T18:46:51.600342Z","shell.execute_reply.started":"2021-07-20T18:26:20.165186Z","shell.execute_reply":"2021-07-20T18:46:51.598755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r rob rob_tok","metadata":{"execution":{"iopub.status.busy":"2021-07-20T18:24:25.601074Z","iopub.status.idle":"2021-07-20T18:24:25.6018Z"},"trusted":true},"execution_count":null,"outputs":[]}]}