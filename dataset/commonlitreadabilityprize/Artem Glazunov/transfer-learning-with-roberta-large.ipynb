{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transfer learning with transformers using Roberta large\nby Artyom Glazunov","metadata":{}},{"cell_type":"markdown","source":"In this notebook you can find one example on how to use transfer learning with the transfermers library. There is also some information on how to load such models in kaggle notebooks, because the inference in some competition does not support Internet connection (the solution is to load some notebooks output with saved models in the inference notebook as an input). The notebook with saved Roberta model you can find here https://www.kaggle.com/artemglazunov1990/roberta-save, this model is used here to be finetuned on our regression task. The inference example you can find here https://www.kaggle.com/artemglazunov1990/inference-with-roberta","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download pretrained Roberta large model with tokenizer","metadata":{}},{"cell_type":"code","source":"%%bash\ncp ../input/roberta-save/rob.zip .\ncp ../input/roberta-save/rob_tok.zip .\nunzip rob.zip\nunzip rob_tok.zip \nrm -r rob.zip rob_tok.zip ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import usefull packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import train_test_split\nimport tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get train and validation sets","metadata":{}},{"cell_type":"code","source":"data_train, data_val, y_err_train, y_err_val = train_test_split(data['excerpt'].values, data[['target', 'standard_error']].values,\n                                                        test_size=0.15,\n                                                        random_state=42)\ndata_train.shape, data_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create Roberta tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(\n    'rob_tok'\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encode data","metadata":{}},{"cell_type":"code","source":"%%time\nencoded_data_train = tokenizer.batch_encode_plus(\n    data_train,\n    add_special_tokens=True,\n    return_attention_mask=True,\n    pad_to_max_length=True,\n    max_length=512,\n    return_tensors='pt',\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    data_val,\n    add_special_tokens=True,\n    return_attention_mask=True,\n    pad_to_max_length=True,\n    max_length=512,\n    return_tensors='pt'\n)\n\ninput_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nvalues_train = torch.tensor(y_err_train[:, 0],dtype=torch.float)\nerrors_train = torch.tensor(y_err_train[:, 1],dtype=torch.float)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nvalues_val = torch.tensor(y_err_val[:, 0], dtype=torch.float)\nerrors_val = torch.tensor(y_err_val[:, 1],dtype=torch.float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a result, we have pytorch tensors of padded ids lists (ids of tokens in our texts from our pretrained Roberta vocab), attention masks (to show the model where is our padding, we do not want it to shange the model's behavior), target and errors (here, it isn't used, but you can try to use it as an uncertainty level in your criterion later).","metadata":{}},{"cell_type":"markdown","source":"Let's create tensor datasets and, after that, dataloaders (iterators that will be providing us with batches)","metadata":{}},{"cell_type":"code","source":"dataset_train = TensorDataset(input_ids_train,\n                             attention_masks_train,\n                             values_train,\n                             errors_train)\ndataset_val = TensorDataset(input_ids_val,\n                            attention_masks_val,\n                            values_val,\n                            errors_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 4\n\ndataloader_train = DataLoader(\n    dataset_train,\n    sampler=RandomSampler(dataset_train),\n    batch_size=batch_size\n)\n\ndataloader_val = DataLoader(\n    dataset_val,\n    sampler=RandomSampler(dataset_val),\n    batch_size=2*batch_size\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create device (we will load our model and batches on it, but be carefull and watch the memory)","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's initialize our model class, criterion and rmse function","metadata":{}},{"cell_type":"code","source":"class BERTRegressor(torch.nn.Module): \n    def __init__(self, pretrained_src = 'rob'): \n        super().__init__()\n        self.bert = RobertaModel.from_pretrained(pretrained_src)\n        self.linear = torch.nn.Linear(1024, 1)\n        self.dropout = torch.nn.Dropout(0.15)\n        \n    def forward(self, input_ids, attention_mask): #x - tokenized batch\n        hidden = self.bert(input_ids, \n                           attention_mask=attention_mask)[0][:, 0, :]#CLS token output                                                          \n        output = self.linear(self.dropout(hidden))\n        return output\n\n\nclass RMSELoss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = torch.nn.MSELoss()\n        \n    def forward(self,yhat,y):\n        loss = torch.sqrt(self.mse(yhat,y))\n        return loss\n\ndef rmse_metric(y_true, y_pred):\n    return np.sqrt(mse(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our params for training","metadata":{}},{"cell_type":"code","source":"warm_prop = 0.1 # we want our learning rate to grow for a while\nepochs = 8\nclip = 1 #we do not want too big gradients\n\nmodel = BERTRegressor().to(device)\ncriterion = RMSELoss()\noptimizer = AdamW(\n    model.parameters(),\n    lr= 3e-5,#the original paper:2e-5 -> 5e-5\n    eps=1e-8\n)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(len(dataloader_train)*epochs * warm_prop),\n    num_training_steps=len(dataloader_train)*epochs\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's initialize our function for evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in tqdm.notebook.tqdm(dataloader_val):\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1]\n        }\n        target = batch[2]\n\n        with torch.no_grad():        \n            output = model(**inputs)\n            \n        loss = criterion(output, target.view(-1,1))\n        loss_val_total += loss\n\n        output = output.detach().cpu().numpy()\n        target = target.cpu().numpy()\n        predictions.append(output)\n        true_vals.append(target)\n    \n    loss_val_avg = loss_val_total / len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start our transfer learning process","metadata":{}},{"cell_type":"code","source":"best_val_loss = float('inf')\nfor epoch in tqdm.notebook.tqdm(range(epochs)):\n    model.train()\n\n    epoch_loss = 0\n    for batch in tqdm.notebook.tqdm(dataloader_train):\n\n        batch = tuple(b.to(device) for b in batch)\n        inputs = {'input_ids':      batch[0],\n                'attention_mask': batch[1]\n          }\n        target = batch[2]\n\n        optimizer.zero_grad()        \n\n        output = model(**inputs)     \n        loss = criterion(output, target.view(-1,1))      \n        loss.backward()\n        epoch_loss += loss.item()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)      \n        optimizer.step()\n        scheduler.step()     \n\n    val_loss, predictions, true_vals = evaluate(dataloader_val)\n    if val_loss < best_val_loss:        \n        #here can be you code, if you want to save your best model\n        pass\n    train_loss = epoch_loss / len(dataloader_train)\n    rmse_val = rmse_metric(true_vals, predictions)\n    print('-------')\n    print(f'Training loss: {train_loss}')\n    print(f'Validation loss: {val_loss}')\n    print(f\"RMSE on validation: {rmse_val}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r rob rob_tok","metadata":{},"execution_count":null,"outputs":[]}]}