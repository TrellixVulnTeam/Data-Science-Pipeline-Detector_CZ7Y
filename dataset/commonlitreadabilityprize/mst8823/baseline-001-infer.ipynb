{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"class Config:\n    name = \"baseline-001\"\n    only_inference = True\n\n    model_name = \"../input/roberta-base\"\n    learning_rate = 1e-5\n    max_length = 256\n    epochs = 8\n    batch_size = 16\n\n    n_fold = 5\n    trn_fold = [0, 1, 2, 3, 4]\n    seed = 2022\n    target_col = \"target\"\n    debug = False\n\n    # Colab Env\n    upload_from_colab = False\n    api_path = \"/content/drive/MyDrive/kaggle.json\"\n    drive_path = \"/content/drive/MyDrive/CommonLit-Readability-Prize\"\n    \n    # Kaggle Env\n    kaggle_dataset_path = \"../input/baseline-001\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport warnings\nimport shutil\nimport logging\nimport joblib\nimport random\nimport datetime\nimport sys\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Logger:\n    \"\"\"参考) https://github.com/ghmagazine/kagglebook/blob/master/ch04-model-interface/code/util.py\"\"\"\n    def __init__(self, path):\n        self.general_logger = logging.getLogger(path)\n        stream_handler = logging.StreamHandler()\n        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n        if len(self.general_logger.handlers) == 0:\n            self.general_logger.addHandler(stream_handler)\n            self.general_logger.addHandler(file_general_handler)\n            self.general_logger.setLevel(logging.INFO)\n\n    def info(self, message):\n        # display time\n        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n\n    @staticmethod\n    def now_string():\n        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n    \n\ndef seed_everything(seed=42):\n    \"\"\"参考) https://qiita.com/kaggle_grandmaster-arai-san/items/d59b2fb7142ec7e270a5\"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COLAB = \"google.colab\" in sys.modules\nif COLAB:\n    print(\"This environment is Google Colab\")\n    \n    # mount\n    from google.colab import drive\n    if not os.path.isdir(\"/content/drive\"):\n        drive.mount('/content/drive') \n\t\n    # import library\n    ! pip install --quiet transformers\n    ! pip install --quiet iterative-stratification\n    ! pip install --quiet tensorflow-addons\n\n    # use kaggle api (need kaggle token)\n    f = open(Config.api_path, 'r')\n    json_data = json.load(f) \n    os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n    os.environ[\"KAGGLE_KEY\"] = json_data[\"key\"]\n    \n    # set dirs\n    DRIVE = Config.drive_path\n    EXP = (Config.name if Config.name is not None \n           else get(\"http://172.28.0.2:9000/api/sessions\").json()[0][\"name\"][:-6])\n    INPUT = os.path.join(DRIVE, \"Input\")\n    OUTPUT = os.path.join(DRIVE, \"Output\")\n    SUBMISSION = os.path.join(DRIVE, \"Submission\")\n    OUTPUT_EXP = os.path.join(OUTPUT, EXP) \n    EXP_MODEL = os.path.join(OUTPUT_EXP, \"model\")\n    EXP_FIG = os.path.join(OUTPUT_EXP, \"fig\")\n    EXP_PREDS = os.path.join(OUTPUT_EXP, \"preds\")\n\n    # make dirs\n    for d in [INPUT, SUBMISSION, EXP_MODEL, EXP_FIG, EXP_PREDS]:\n        os.makedirs(d, exist_ok=True)\n\n    if not os.path.isfile(os.path.join(INPUT, \"train.csv.zip\")):\n        # load dataset\n        ! kaggle competitions download -c commonlitreadabilityprize -p $INPUT \n    \n    # utils\n    logger = Logger(OUTPUT_EXP)\n\nelse:\n    print(\"This environment is Kaggle Kernel\")\n    \n    # set dirs\n    INPUT = \"../input/commonlitreadabilityprize\"\n    EXP, OUTPUT, SUBMISSION = \"./\", \"./\", \"./\"\n    EXP_MODEL = os.path.join(EXP, \"model\")\n    EXP_FIG = os.path.join(EXP, \"fig\")\n    EXP_PREDS = os.path.join(EXP, \"preds\")\n    \n    # copy dirs\n    if Config.kaggle_dataset_path is not None:\n        KD_MODEL = os.path.join(Config.kaggle_dataset_path, \"model\")\n        KD_EXP_PREDS = os.path.join(Config.kaggle_dataset_path, \"preds\")\n        shutil.copytree(KD_MODEL, EXP_MODEL)\n        shutil.copytree(KD_EXP_PREDS, EXP_PREDS)\n\n    # make dirs\n    for d in [EXP_MODEL, EXP_FIG, EXP_PREDS]:\n        os.makedirs(d, exist_ok=True)\n        \n    # utils\n    logger = Logger(EXP)\n\n# utils\nwarnings.filterwarnings(\"ignore\")\nsns.set(style='whitegrid')\nseed_everything(seed=Config.seed)\n\n# 2nd import\nfrom transformers import AutoTokenizer, TFAutoModel\nimport tensorflow_addons as tfa","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data\ntrain = pd.read_csv(os.path.join(INPUT, \"train.csv.zip\" if COLAB else \"train.csv\"))\ntest = pd.read_csv(os.path.join(INPUT, \"test.csv\"))\nsample_submission = pd.read_csv(os.path.join(INPUT, \"sample_submission.csv\"))\n\nif Config.debug:\n    train = train.sample(100).reset_index(drop=True)\n\n# cv split\ntrain[\"fold\"] = -1\nfor i_fold, lst in enumerate(\n    KFold(\n        n_splits=Config.n_fold, \n        shuffle=True,\n        random_state=Config.seed).split(\n            X=train, \n            y=train[Config.target_col]\n            )):\n    \n    if i_fold in Config.trn_fold:\n        train.loc[lst[1].tolist(), \"fold\"] = i_fold\n\ndisplay(train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model(model_name, max_length):\n    transformer = TFAutoModel.from_pretrained(model_name)\n    input_ids = tf.keras.layers.Input(shape=(max_length, ), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = tf.keras.layers.Input(shape=(max_length, ), dtype=tf.int32, name=\"attention_mask\")\n\n    x = transformer(input_ids, attention_mask=attention_mask)\n    x = x[0][:, 0, :]\n\n    output = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n    model = tf.keras.Model(inputs=[input_ids, attention_mask],\n                           outputs=[output])\n    return model\n\ndef get_tokenizer(model_name):\n    return AutoTokenizer.from_pretrained(model_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_score(y_true, y_pred):\n    \"\"\"evaluation\"\"\"\n    return mean_squared_error(y_true, y_pred) ** 0.5\n\n\ndef get_model_inputs(text, tokenizer, max_length):\n    \"\"\"bert model input\"\"\"\n    input_text = tokenizer.batch_encode_plus(\n        text,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length,\n    )\n    return dict(input_text)\n\n\ndef get_tf_dataset(X, y=None):\n    \"\"\"transform to tf dataset\"\"\"\n    dataset = (tf.data.Dataset.from_tensor_slices((X, y)).\n               batch(Config.batch_size).\n               prefetch(tf.data.experimental.AUTOTUNE))\n    return dataset\n\n\ndef training(train_df, valid_df, filepath):\n    \"\"\"training for one fold\"\"\"\n    # model input\n    tokenizer = get_tokenizer(model_name=Config.model_name)\n    train_x = get_model_inputs(\n        text=train_df[\"excerpt\"].tolist(), \n        tokenizer=tokenizer, \n        max_length=Config.max_length)\n    \n    valid_x = get_model_inputs(\n        text=valid_df[\"excerpt\"].tolist(), \n        tokenizer=tokenizer, \n        max_length=Config.max_length)\n    \n    train_y = train_df[Config.target_col].values\n    valid_y = valid_df[Config.target_col].values\n    train_dataset = get_tf_dataset(X=train_x, y=train_y)\n    valid_dataset = get_tf_dataset(X=valid_x, y=valid_y)\n    print(\"ds\",train_dataset)\n\n    # model setting\n    model = get_model(model_name=Config.model_name, max_length=Config.max_length)\n    optimizer = tfa.optimizers.AdamW(learning_rate=Config.learning_rate, weight_decay=1e-5)\n    loss = tf.keras.losses.MeanSquaredError()\n    metrics = tf.keras.metrics.RootMeanSquaredError()\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath, \n        monitor=\"val_loss\", \n        verbose=1, \n        save_best_only=True, \n        save_weights_only=True,\n        mode=\"min\")\n    \n    # model training \n    model.fit(\n        train_dataset,\n        epochs=Config.epochs, \n        verbose=1, \n        callbacks=[checkpoint],\n        batch_size=Config.batch_size,\n        validation_data=valid_dataset)\n    \n\ndef inference(test_df, filepath):\n    # model inputs\n    tokenizer = get_tokenizer(model_name=Config.model_name)\n    test_x = get_model_inputs(\n        text=test_df[\"excerpt\"].tolist(), \n        tokenizer=tokenizer, \n        max_length=Config.max_length)\n    test_dataset = get_tf_dataset(test_x)\n    \n    # model setting\n    model = get_model(model_name=Config.model_name, max_length=Config.max_length)\n    model.load_weights(filepath)\n\n    # model prediction\n    preds = model.predict(test_dataset)\n\n    return preds.reshape(-1)\n\n\ndef train_cv(train):\n    oof = np.zeros(len(train))\n    for i_fold in range(Config.n_fold):\n        if i_fold in Config.trn_fold:\n            K.clear_session()\n            filepath = os.path.join(\n                        EXP_MODEL,\n                        f\"{Config.name}-seed{Config.seed}-fold{i_fold}.h5\")\n            \n            valid_mask = np.array(train[\"fold\"] == i_fold, dtype=bool)\n            tr_df, va_df = (train[~valid_mask].reset_index(drop=True),\n                            train[valid_mask].reset_index(drop=True))\n                        \n            if not os.path.isfile(filepath):  # if trained model, no training\n                training(tr_df, va_df, filepath)\n            \n            preds = inference(va_df, filepath)\n            oof[valid_mask] = preds\n\n            # fold score\n            score = get_score(va_df[Config.target_col].values, preds)\n            logger.info(f\"{Config.name}-seed{Config.seed}-fold{i_fold} >>>>> Score={score:.4f}\")\n\n    return oof\n\n\ndef predict_cv(test):\n    K.clear_session()\n    fold_preds = []\n    for i_fold in range(Config.n_fold):\n        if i_fold in Config.trn_fold:\n            filepath = os.path.join(\n                EXP_MODEL,\n                f\"{Config.name}-seed{Config.seed}-fold{i_fold}.h5\")\n            \n            preds = inference(test, filepath)\n            fold_preds.append(preds)\n    return fold_preds\n\n\ndef plot_result(target, oof):\n    \"\"\"regression result\"\"\"\n    fig, ax = plt.subplots(figsize=(8, 8))\n    ax = sns.distplot(target, label=\"y\", color='cyan', ax=ax)\n    ax = sns.distplot(oof, label=\"oof\", color=\"magenta\", ax=ax)\n    ax.legend()\n    return fig","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not Config.only_inference:\n    # training\n    print(\"# ---------- # Start Training # ---------- #\")\n    oof = train_cv(train)\n    oof_df = pd.DataFrame(oof, columns=[Config.target_col])\n    oof_df.to_csv(os.path.join(EXP_PREDS, \"oof_df.csv\"), index=False)\n\n    fold_mask = train[\"fold\"].isin(Config.trn_fold)\n    target = train[Config.target_col]\n\n    # get oof score\n    score = get_score(target[fold_mask].values, oof[fold_mask])\n    logger.info(f\"OOF-Score >>>>> {score:.4f}\")\n\n    # save result\n    fig = plot_result(target[fold_mask].values, oof[fold_mask])\n    fig.savefig(os.path.join(EXP_FIG, \"regression_result.png\"), dpi=300)\n\n# prediction\nprint(\"# ---------- # Start Inference # ---------- #\")\nfold_preds = predict_cv(test)\nfold_preds_df = pd.DataFrame()\nfor i in range(len(fold_preds)):  # single fold ok\n    fold_preds_df[f\"FOLD{i}\"] = fold_preds[i]\nfold_preds_df.to_csv(os.path.join(EXP_PREDS, f\"fold_preds_df.csv\"), index=False)\n\n# make submission\nprint(\"# ---------- # Make Submission # ---------- #\")\nsample_submission[\"target\"] = fold_preds_df.mean(axis=1)\nfilename = Config.name + \".csv\" if COLAB else \"submission.csv\"\nsample_submission.to_csv(os.path.join(SUBMISSION, filename), index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# upload output folder to kaggle dataset\nif Config.upload_from_colab:\n    from kaggle.api.kaggle_api_extended import KaggleApi\n\n    def dataset_create_new(dataset_name, upload_dir):\n        dataset_metadata = {}\n        dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n        dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n        dataset_metadata['title'] = dataset_name\n        with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n            json.dump(dataset_metadata, f, indent=4)\n        api = KaggleApi()\n        api.authenticate()\n        api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')\n\n    dataset_create_new(dataset_name=EXP, upload_dir=OUTPUT_EXP)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}