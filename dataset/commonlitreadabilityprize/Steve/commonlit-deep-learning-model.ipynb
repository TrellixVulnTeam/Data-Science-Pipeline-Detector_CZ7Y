{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The purpose of this notebook is to illustrate how a pre-trained large language model can be fine-tuned for a specific task. In short, we are using the pre-trained DistilBERT model, but with two additional linear layers added on that are not trained (the parameters for those two layers are initialized to random values). \n\nThe original DistilBERT model was trained on ~16GB of data. The fine-tuning done here uses the the relatively small dataset of ~2500 observations, where each observation is roughly a paragraph of text. \n\nA good reference on fine-tuning a large language model:\n* https://mccormickml.com/2019/07/22/BERT-fine-tuning/","metadata":{}},{"cell_type":"code","source":"import gc\nimport json\nimport torch\nimport itertools\nimport time\nimport datetime\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport numpy.ma as ma\n\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler, random_split\n\n#from transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import AdamW \nfrom transformers import get_linear_schedule_with_warmup\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-16T19:54:18.142954Z","iopub.execute_input":"2021-11-16T19:54:18.143464Z","iopub.status.idle":"2021-11-16T19:54:20.687656Z","shell.execute_reply.started":"2021-11-16T19:54:18.143379Z","shell.execute_reply":"2021-11-16T19:54:20.686773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_file = '../input/commonlitreadabilityprize/train.csv'\n\nMAX_LENGTH = 256\nBATCH_SIZE = 32","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:20.693219Z","iopub.execute_input":"2021-11-16T19:54:20.693739Z","iopub.status.idle":"2021-11-16T19:54:20.698227Z","shell.execute_reply.started":"2021-11-16T19:54:20.6937Z","shell.execute_reply":"2021-11-16T19:54:20.697389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(train_file)\nprint(f'train data shape: {train_data.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:20.69989Z","iopub.execute_input":"2021-11-16T19:54:20.700238Z","iopub.status.idle":"2021-11-16T19:54:20.747953Z","shell.execute_reply.started":"2021-11-16T19:54:20.700203Z","shell.execute_reply":"2021-11-16T19:54:20.747024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:20.749121Z","iopub.execute_input":"2021-11-16T19:54:20.749446Z","iopub.status.idle":"2021-11-16T19:54:20.774344Z","shell.execute_reply.started":"2021-11-16T19:54:20.749411Z","shell.execute_reply":"2021-11-16T19:54:20.773236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download the DistilBERT tokenizer and model using the huggingface transformers module. Note that the num_labels parameter set to 1 indicates that we have a regression output (rather than classification). ","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n                                                           num_labels=1,\n                                                           output_attentions=False,\n                                                           output_hidden_states=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:20.77576Z","iopub.execute_input":"2021-11-16T19:54:20.776252Z","iopub.status.idle":"2021-11-16T19:54:22.805275Z","shell.execute_reply.started":"2021-11-16T19:54:20.776211Z","shell.execute_reply":"2021-11-16T19:54:22.80424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next two cells are simply to check out the data a little more. ","metadata":{}},{"cell_type":"code","source":"X = train_data.excerpt.values # X and y are both numpy arrays\ny = train_data.target.values\nprint(X.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:22.80673Z","iopub.execute_input":"2021-11-16T19:54:22.807114Z","iopub.status.idle":"2021-11-16T19:54:22.813118Z","shell.execute_reply.started":"2021-11-16T19:54:22.807075Z","shell.execute_reply":"2021-11-16T19:54:22.812255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('original: \\n', X[0])\n\nprint('\\n\\ntokenized: \\n', tokenizer.tokenize(X[0]))\nprint('len(tokenized(X[0])): \\n', len(tokenizer.tokenize(X[0])))\n\nprint('\\n\\ntoken IDs: \\n', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(X[0])))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:22.814592Z","iopub.execute_input":"2021-11-16T19:54:22.815254Z","iopub.status.idle":"2021-11-16T19:54:22.845954Z","shell.execute_reply.started":"2021-11-16T19:54:22.815215Z","shell.execute_reply":"2021-11-16T19:54:22.845116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We set the max length already above, but this would be a check to see what the max length is over all observations (earlier we created a histogram of the lengths and determined 256 is a reasonable max length). ","metadata":{}},{"cell_type":"code","source":"observed_max_len = 0\n\n# For every sentence...\nfor exc in X:\n\n    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n    input_ids = tokenizer.encode(exc, add_special_tokens=True)\n\n    # Update the maximum sentence length.\n    observed_max_len = max(observed_max_len, len(input_ids))\n\nprint('Max sentence length: ', observed_max_len) # max len in training data is 314, but 256 will fully cover most observations","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:22.848862Z","iopub.execute_input":"2021-11-16T19:54:22.849298Z","iopub.status.idle":"2021-11-16T19:54:37.723512Z","shell.execute_reply.started":"2021-11-16T19:54:22.849269Z","shell.execute_reply":"2021-11-16T19:54:37.7225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the DistilBERT tokenizer to tokenize the raw data. ","metadata":{}},{"cell_type":"code","source":"# Tokenize all of excerpts and map their tokens to their word IDs\ninput_ids = []\nattention_masks = []\n\n# For every sentence...\nfor exc in X:\n    # `encode_plus` will:\n    #   (1) Tokenize the sentence.\n    #   (2) Prepend the `[CLS]` token to the start.\n    #   (3) Append the `[SEP]` token to the end.\n    #   (4) Map tokens to their IDs.\n    #   (5) Pad or truncate the sentence to `max_length`\n    #   (6) Create attention masks for [PAD] tokens.\n    encoded_dict = tokenizer.encode_plus(\n                        exc,                       # Sentence to encode\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        truncation = True,\n                        padding = 'max_length',\n                        max_length = MAX_LENGTH,          # Pad & truncate all sentences        \n                        #pad_to_max_length = True,\n                        return_attention_mask = True,   # Construct attn. masks\n                        return_tensors = 'pt',     # Return pytorch tensors\n                   )\n    \n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    \n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(y).float()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:37.725341Z","iopub.execute_input":"2021-11-16T19:54:37.725739Z","iopub.status.idle":"2021-11-16T19:54:53.183096Z","shell.execute_reply.started":"2021-11-16T19:54:37.7257Z","shell.execute_reply":"2021-11-16T19:54:53.18221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print sentence 0, now as a list of IDs.\nprint('original X[0]: ', X[0])\nprint('\\n\\ntoken IDs for X[0]:', input_ids[0])","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:53.184334Z","iopub.execute_input":"2021-11-16T19:54:53.184687Z","iopub.status.idle":"2021-11-16T19:54:53.19274Z","shell.execute_reply.started":"2021-11-16T19:54:53.184652Z","shell.execute_reply":"2021-11-16T19:54:53.191715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the data into training and validation sets. ","metadata":{}},{"cell_type":"code","source":"# Combine the training inputs into a TensorDataset.\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n# Create a 90-10 train-validation split and calc sizes of each.\ntrain_size = int(0.85 * len(dataset))\nval_size = len(dataset) - train_size\n\n# Divide the dataset by randomly selecting samples.\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:53.194075Z","iopub.execute_input":"2021-11-16T19:54:53.194652Z","iopub.status.idle":"2021-11-16T19:54:53.205745Z","shell.execute_reply.started":"2021-11-16T19:54:53.194602Z","shell.execute_reply":"2021-11-16T19:54:53.204926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The DataLoader needs to know our batch size for training, so we specify it \n# here. Smaller batch sizes are generally recommended for fine-tuning BERT \n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \ntrain_dataloader = DataLoader(\n            train_dataset,  # The training samples.\n            sampler = RandomSampler(train_dataset), # Select batches randomly\n            batch_size = BATCH_SIZE # Trains with this batch size.\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nvalidation_dataloader = DataLoader(\n            val_dataset, # The validation samples.\n            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n            batch_size = BATCH_SIZE # Evaluate with this batch size.\n        )","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:53.207756Z","iopub.execute_input":"2021-11-16T19:54:53.208369Z","iopub.status.idle":"2021-11-16T19:54:53.216357Z","shell.execute_reply.started":"2021-11-16T19:54:53.208331Z","shell.execute_reply":"2021-11-16T19:54:53.215533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the following cell we will look at all of the layers/cells in the model. ","metadata":{}},{"cell_type":"code","source":"model.cuda()\n\n# Get all of the model's parameters as a list of tuples.\nparams = list(model.named_parameters())\n\nprint('The DistilBERT model number of layers: {}.\\n'.format(len(params)))\n\nfor i, p in enumerate(params):\n    print(\"layer {:>3}: {:<55} {:>12}\".format(i, p[0], str(tuple(p[1].size()))))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:53.218408Z","iopub.execute_input":"2021-11-16T19:54:53.219055Z","iopub.status.idle":"2021-11-16T19:54:55.713245Z","shell.execute_reply.started":"2021-11-16T19:54:53.219018Z","shell.execute_reply":"2021-11-16T19:54:55.706767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create our own optimizer that sets a different (much lower) learning rate for the layers \n# that are already pre-trained, and then a larger learning rate for the two final linear\n# layers that have not been trained at all (but are instead initialized to random values).\ndef create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    bert_parameters = named_parameters[:100]\n    regressor_parameters = named_parameters[100:]\n        \n    bert_group = [params for (name, params) in bert_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n\n    #for layer_num, (name, params) in enumerate(bert_parameters):\n    for name, params in bert_parameters:        \n        lr = 1e-5\n        parameters.append({\"params\": params,\n                           \"lr\": lr})\n\n    #for layer_num, (name, params) in enumerate(regressor_parameters):\n    for name, params in regressor_parameters:\n        lr = 1e-3 \n        parameters.append({\"params\": params,\n                           \"lr\": lr})\n\n    return AdamW(parameters)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.MSELoss()\n\n#optimizer = AdamW(model.parameters(),\n#                  lr = 1e-5, # args.learning_rate - default is 5e-5\n#                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n#                )\noptimizer = create_optimizer(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:55.723308Z","iopub.execute_input":"2021-11-16T19:54:55.723685Z","iopub.status.idle":"2021-11-16T19:54:55.760794Z","shell.execute_reply.started":"2021-11-16T19:54:55.723643Z","shell.execute_reply":"2021-11-16T19:54:55.759981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 4\n\n# Number of training epochs does not need to be a lot for fine-tuning, \n# recommendations for BERT models are between 2-4\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\ntotal_steps = len(train_dataloader) * EPOCHS\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:55.76374Z","iopub.execute_input":"2021-11-16T19:54:55.764028Z","iopub.status.idle":"2021-11-16T19:54:55.7712Z","shell.execute_reply.started":"2021-11-16T19:54:55.763997Z","shell.execute_reply":"2021-11-16T19:54:55.770283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_time(elapsed):\n    ''' Convert time in seconds and returns a string hh:mm:ss '''\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:55.772772Z","iopub.execute_input":"2021-11-16T19:54:55.773247Z","iopub.status.idle":"2021-11-16T19:54:55.78113Z","shell.execute_reply.started":"2021-11-16T19:54:55.773198Z","shell.execute_reply":"2021-11-16T19:54:55.780187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the seed value all over the place to make this reproducible.\nseed_val = 1\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\ntorch.set_default_dtype(torch.float64)\n\n# We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, EPOCHS):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n    batch_squared_errors = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    y_train = {'actual':[], 'predicted':[]}\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 40 batches.\n        if step % 25 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks, not needed for DistilBERT\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is sometimes desired \n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).        \n        outputs = model(b_input_ids, \n                        labels=b_labels)\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_train_loss += outputs[0].item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss = criterion(outputs[1].flatten(), b_labels.float())#.sqrt()\n        \n        # backpropagation\n        loss.backward()\n        \n        # for plotting results later on\n        y_train['actual'] += b_labels.float().cpu().numpy().flatten().tolist()\n        y_train['predicted'] += outputs[1].detach().cpu().numpy().flatten().tolist()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n        \n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epoch took: {:}\".format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put model in evaluation mode (don't calculate gradients, no dropout, etc.)\n    model.eval()\n\n    # Tracking variables \n    batch_squared_errors = 0\n    total_eval_loss = 0\n\n    # Evaluate data for one epoch\n    y_val = {'actual':[], 'predicted':[]}\n    for step, batch in enumerate(validation_dataloader):\n                \n        # Progress update every 40 batches.\n        if step % 5 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(validation_dataloader), elapsed))\n        \n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using \n        # the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks, not needed for DistilBERT\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n\n            # Forward pass, calculate predictions\n            outputs = model(b_input_ids, \n                            labels=b_labels)\n\n        # Accumulate the validation loss.\n        loss = outputs[0]\n        total_eval_loss += loss.item()\n        \n        # Move labels/targets and predictions to CPU\n        preds = outputs[1].detach().cpu().numpy()\n        targets = b_labels.to('cpu').numpy()\n        \n        # for plotting results later on\n        y_val['actual'] += targets.flatten().tolist()\n        y_val['predicted'] += preds.flatten().tolist()\n\n        # Calculate MSE\n        batch_squared_errors += np.square(targets - preds.flatten()).sum()\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:54:55.782382Z","iopub.execute_input":"2021-11-16T19:54:55.782818Z","iopub.status.idle":"2021-11-16T19:55:59.163084Z","shell.execute_reply.started":"2021-11-16T19:54:55.782764Z","shell.execute_reply":"2021-11-16T19:55:59.161295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_mse = mean_squared_error(y_train['predicted'], y_train['actual'])\nvalid_mse = mean_squared_error(y_val['predicted'], y_val['actual'])\nprint(f\"DistilBERT model training MSE = {train_mse:.6f}\")\nprint(f\"DistilBERT model validation MSE = {valid_mse:.6f}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:55:59.164685Z","iopub.execute_input":"2021-11-16T19:55:59.165067Z","iopub.status.idle":"2021-11-16T19:55:59.174147Z","shell.execute_reply.started":"2021-11-16T19:55:59.165023Z","shell.execute_reply":"2021-11-16T19:55:59.172985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = batch[0]\nt.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-16T20:10:53.681285Z","iopub.execute_input":"2021-11-16T20:10:53.681611Z","iopub.status.idle":"2021-11-16T20:10:53.687467Z","shell.execute_reply.started":"2021-11-16T20:10:53.681581Z","shell.execute_reply":"2021-11-16T20:10:53.686547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntraining_losses = [epoch_stats['Training Loss'] for epoch_stats in training_stats]\nvalidation_losses = [epoch_stats['Valid. Loss'] for epoch_stats in training_stats]\nplt.plot(range(1,len(training_losses)+1), training_losses, c='r')\nplt.plot(range(1,len(validation_losses)+1), validation_losses, c='b')\nplt.xticks(range(1, len(training_losses)+1))\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:55:59.1757Z","iopub.execute_input":"2021-11-16T19:55:59.176286Z","iopub.status.idle":"2021-11-16T19:55:59.296533Z","shell.execute_reply.started":"2021-11-16T19:55:59.176245Z","shell.execute_reply":"2021-11-16T19:55:59.295237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matplotlib.rc('figure', figsize=(15,4))\n_, (ax1, ax2) = plt.subplots(1, 2)\nax1.plot([0,1], [0,1], transform=ax1.transAxes, c='k', alpha=0.2)\nax1.scatter(y_train['actual'], y_train['predicted'], c='b', alpha=0.2)\nax1.set_title(\"Training data\")\nax1.set_xlabel(\"actual\")\nax1.set_ylabel(\"predicted\")\nax2.plot([0,1], [0,1], transform=ax2.transAxes, c='k', alpha=0.2)\nax2.scatter(y_val['actual'], y_val['predicted'], c='g', alpha=0.4)\nax2.set_title(\"Validation data\")\nax2.set_xlabel(\"actual\")\nax2.set_ylabel(\"predicted\")","metadata":{"execution":{"iopub.status.busy":"2021-11-16T19:55:59.297974Z","iopub.execute_input":"2021-11-16T19:55:59.29832Z","iopub.status.idle":"2021-11-16T19:55:59.634177Z","shell.execute_reply.started":"2021-11-16T19:55:59.298282Z","shell.execute_reply":"2021-11-16T19:55:59.633229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}