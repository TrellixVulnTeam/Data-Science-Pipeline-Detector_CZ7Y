{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sklearn\nfrom scipy import stats, special\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n\nimport xgboost as xgb\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import RobertaTokenizer, RobertaModel, RobertaConfig","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:08:38.180766Z","iopub.execute_input":"2021-07-16T03:08:38.181269Z","iopub.status.idle":"2021-07-16T03:08:44.798683Z","shell.execute_reply.started":"2021-07-16T03:08:38.181158Z","shell.execute_reply":"2021-07-16T03:08:44.797745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CONFIG:\n\n    len248 = 248\n    len256 = 256\n    len307 = 307\n    \n    batch = 512\n    \n    whole_model = [\n        r\"../input/clrpensemblemodels/whole-model/model_1.pth\",\n        r\"../input/clrpensemblemodels/whole-model/model_2.pth\",\n        r\"../input/clrpensemblemodels/whole-model/model_3.pth\",\n        r\"../input/clrpensemblemodels/whole-model/model_4.pth\",\n        r\"../input/clrpensemblemodels/whole-model/model_5.pth\"\n    ]\n    onesided_target = [\n        r\"../input/clrpensemblemodels/onesided-target/best_model_0.pt\",\n        r\"../input/clrpensemblemodels/onesided-target/best_model_1.pt\",\n        r\"../input/clrpensemblemodels/onesided-target/best_model_2.pt\",\n        r\"../input/clrpensemblemodels/onesided-target/best_model_3.pt\",\n        r\"../input/clrpensemblemodels/onesided-target/best_model_4.pt\"\n    ]\n    logit_model = [\n        r\"../input/clrpensemblemodels/logit-model/roberta-base-attention-logit-model-88.52.bin\",\n        r\"../input/clrpensemblemodels/logit-model/roberta-base-attention-logit-model-88.71.bin\"\n    ]\n    logit_cnn_model = r\"../input/clrpensemblemodels/logit-model/roberta-base-cnn-concat-conv3579-logit-model-88.18.bin\"\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:08:44.802567Z","iopub.execute_input":"2021-07-16T03:08:44.802849Z","iopub.status.idle":"2021-07-16T03:08:44.856627Z","shell.execute_reply.started":"2021-07-16T03:08:44.802822Z","shell.execute_reply":"2021-07-16T03:08:44.855561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prep_text(str_):\n    str_ = re.sub(\"\\n\", \"\", str_)\n    str_ = re.sub(\"\\'s\",r\"'s\", str_)\n    return str_","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:08:44.8587Z","iopub.execute_input":"2021-07-16T03:08:44.858993Z","iopub.status.idle":"2021-07-16T03:08:44.866178Z","shell.execute_reply.started":"2021-07-16T03:08:44.858966Z","shell.execute_reply":"2021-07-16T03:08:44.863356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"median_shift = -0.91332221\n\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ntest_data[\"excerpt\"] = test_data[\"excerpt\"].apply(lambda x: prep_text(x))\n\n\n# test_data = test_data[~(test_data.target==0.0) & ~(test_data.standard_error==0.0)].reset_index(drop=True)\n# test_data['mod_target'] = test_data['target']-median_shift\n# test_data['is_difficult'] = test_data['mod_target'] < 0\n# test_data['is_difficult'] = test_data['is_difficult'].astype(int)\n\n# test_data['mod_abs'] = test_data['mod_target'].abs()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:08:44.8686Z","iopub.execute_input":"2021-07-16T03:08:44.869029Z","iopub.status.idle":"2021-07-16T03:08:44.891389Z","shell.execute_reply.started":"2021-07-16T03:08:44.868993Z","shell.execute_reply":"2021-07-16T03:08:44.890571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRPDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.text = df['excerpt'].values \n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer(text, \n                                return_tensors='pt', \n                                max_length=self.max_len, \n                                padding='max_length', \n                                truncation=True)\n        return {\n            'ids': inputs['input_ids'].squeeze(),\n            'mask': inputs['attention_mask'].squeeze()        \n        }\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:08:44.892873Z","iopub.execute_input":"2021-07-16T03:08:44.893259Z","iopub.status.idle":"2021-07-16T03:08:44.89992Z","shell.execute_reply.started":"2021-07-16T03:08:44.89319Z","shell.execute_reply":"2021-07-16T03:08:44.898881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OneSidedNet(nn.Module):\n    def __init__(self, model_path, model_config):\n        super(OneSidedNet, self).__init__()\n        \n        self.base_model = RobertaModel.from_pretrained(model_path, config=model_config)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n    def forward(self, inputs):\n        ids, mask = inputs\n        \n        base_out = self.base_model(ids, attention_mask=mask)[0]\n            \n        weights = self.attention(base_out)\n                \n        \n        context_vector = torch.sum(weights * base_out, dim=1)        \n        \n\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:08:44.901637Z","iopub.execute_input":"2021-07-16T03:08:44.902056Z","iopub.status.idle":"2021-07-16T03:08:44.911247Z","shell.execute_reply.started":"2021-07-16T03:08:44.901964Z","shell.execute_reply":"2021-07-16T03:08:44.9099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WholeNet(nn.Module):\n    def __init__(self, model_path, model_config):\n        super(WholeNet, self).__init__()\n        \n        self.roberta = RobertaModel.from_pretrained(model_path, config=model_config)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n    def forward(self, inputs):\n        ids, mask = inputs\n        \n        base_out = self.roberta(ids, attention_mask=mask)[0]\n            \n        weights = self.attention(base_out)\n                \n        \n        context_vector = torch.sum(weights * base_out, dim=1)        \n        \n\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:08:44.91289Z","iopub.execute_input":"2021-07-16T03:08:44.913362Z","iopub.status.idle":"2021-07-16T03:08:44.924647Z","shell.execute_reply.started":"2021-07-16T03:08:44.91332Z","shell.execute_reply":"2021-07-16T03:08:44.923335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LogitNet(nn.Module):\n    def __init__(self, model_path, model_config):\n        super(LogitNet, self).__init__()\n        \n        self.base_model = RobertaModel.from_pretrained(model_path, config=model_config)\n        \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n    def forward(self, inputs):\n        ids, mask = inputs\n        \n        base_out = self.base_model(ids, attention_mask=mask)[0]\n            \n        weights = self.attention(base_out)\n                \n        \n        context_vector = torch.sum(weights * base_out, dim=1)        \n        \n\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:08:44.928304Z","iopub.execute_input":"2021-07-16T03:08:44.928806Z","iopub.status.idle":"2021-07-16T03:08:44.9388Z","shell.execute_reply.started":"2021-07-16T03:08:44.928765Z","shell.execute_reply":"2021-07-16T03:08:44.937592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNNLogitNet(nn.Module):\n    def __init__(self, model_path, model_config):\n        super(CNNLogitNet, self).__init__()\n        \n        self.base_model = RobertaModel.from_pretrained(model_path, config=model_config)\n        \n        self.conv1 = nn.Conv1d(in_channels=768, out_channels=1, kernel_size=3, padding=2, stride=1)\n        self.conv2 = nn.Conv1d(in_channels=768, out_channels=1, kernel_size=5, padding=4, stride=1)\n        self.conv3 = nn.Conv1d(in_channels=768, out_channels=1, kernel_size=7, padding=6, stride=1)\n        self.conv4 = nn.Conv1d(in_channels=768, out_channels=1, kernel_size=9, padding=8, stride=1)      \n        \n        self.bn = nn.BatchNorm1d(1)\n        \n        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=1)\n        self.pool2 = nn.MaxPool1d(kernel_size=5, stride=1)\n        self.pool3 = nn.MaxPool1d(kernel_size=7, stride=1)\n        self.pool4 = nn.MaxPool1d(kernel_size=9, stride=1)\n        \n        self.hidden_layer = nn.Linear(4*307, 256)\n        self.final_out = nn.Linear(256, 1)\n        \n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, inputs):\n        ids, mask = inputs\n        \n        base_out = self.base_model(ids, attention_mask=mask)[0]\n        base_out = base_out.permute(0, 2, 1)\n        \n        x1 = self.bn(self.conv1(base_out))\n        x1 = F.relu(x1)\n        x1 = self.pool1(x1)\n        x1 = x1.view(-1, 307)\n        \n        x2 = self.bn(self.conv2(base_out))\n        x2 = F.relu(x2)\n        x2 = self.pool2(x2)\n        x2 = x2.view(-1, 307)\n        \n        x3 = self.bn(self.conv3(base_out))\n        x3 = F.relu(x3)\n        x3 = self.pool3(x3)\n        x3 = x3.view(-1, 307)\n        \n        x4 = self.bn(self.conv4(base_out))\n        x4 = F.relu(x4)\n        x4 = self.pool4(x4)\n        x4 = x4.view(-1, 307)\n        \n        \n        out = torch.cat([x1, x2, x3, x4], dim=1)\n        \n        out = self.dropout(out)\n        out = self.hidden_layer(out)\n        \n        out = self.dropout(out)\n        out = self.final_out(out)\n\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:08:44.941008Z","iopub.execute_input":"2021-07-16T03:08:44.941753Z","iopub.status.idle":"2021-07-16T03:08:44.96034Z","shell.execute_reply.started":"2021-07-16T03:08:44.941709Z","shell.execute_reply":"2021-07-16T03:08:44.959255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = r\"../input/clrpensemblemodels/roberta-base\"\nmodel_config = RobertaConfig.from_pretrained(model_path)\nmodel_config.output_hidden_states = True\ntokenizer = RobertaTokenizer.from_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:08:44.9619Z","iopub.execute_input":"2021-07-16T03:08:44.962599Z","iopub.status.idle":"2021-07-16T03:08:45.110028Z","shell.execute_reply.started":"2021-07-16T03:08:44.962557Z","shell.execute_reply":"2021-07-16T03:08:45.109156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nmodel = CNNLogitNet(model_path, model_config)\nmodel.to(CONFIG.device)\nmodel.load_state_dict(torch.load(CONFIG.logit_cnn_model, map_location=CONFIG.device))\n\ntest_dataset = CLRPDataset(test_data, tokenizer, CONFIG.len307)\ntest_dataloader = DataLoader(test_dataset, batch_size=CONFIG.batch, shuffle=False)\n\npreds = []\nfor batch in test_dataloader:\n    with torch.no_grad():\n        device=CONFIG.device\n        ids = batch[\"ids\"].to(device)\n        mask = batch[\"mask\"].to(device)\n        output= model((ids, mask)) \n\n        preds.append(output)\n        \npreds = torch.cat(preds).cpu().detach().numpy()\npreds = torch.sigmoid(torch.tensor(preds)).squeeze().cpu().detach().numpy()\ntest_data['cnn_logits_0'] = preds","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:08:45.112499Z","iopub.execute_input":"2021-07-16T03:08:45.112882Z","iopub.status.idle":"2021-07-16T03:09:05.267065Z","shell.execute_reply.started":"2021-07-16T03:08:45.112845Z","shell.execute_reply":"2021-07-16T03:09:05.26621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = r\"../input/clrpensemblemodels/pretrained_roberta_base\"\nmodel_config = RobertaConfig.from_pretrained(model_path)\nmodel_config.output_hidden_states = True\nmodel_config.hidden_dropout_prob = 0.0\nmodel_config.layer_norm_eps = 1e-7\ntokenizer = RobertaTokenizer.from_pretrained(model_path)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:09:05.270339Z","iopub.execute_input":"2021-07-16T03:09:05.270685Z","iopub.status.idle":"2021-07-16T03:09:05.389198Z","shell.execute_reply.started":"2021-07-16T03:09:05.270658Z","shell.execute_reply":"2021-07-16T03:09:05.388364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nidx=1\ntest_dataset = CLRPDataset(test_data, tokenizer, CONFIG.len256)\ntest_dataloader = DataLoader(test_dataset, batch_size=CONFIG.batch, shuffle=False)\n    \nfor trained_model in CONFIG.logit_model:   \n    model = LogitNet(model_path, model_config)\n    model.to(CONFIG.device)\n    model.load_state_dict(torch.load(trained_model, map_location=CONFIG.device))\n    \n    preds = []\n    for batch in test_dataloader:\n        with torch.no_grad():\n            device=CONFIG.device\n            ids = batch[\"ids\"].to(device)\n            mask = batch[\"mask\"].to(device)\n            output= model((ids, mask)) \n\n            preds.append(output)\n            \n    preds = torch.cat(preds).cpu().detach().numpy()\n    preds = torch.sigmoid(torch.tensor(preds)).squeeze().cpu().detach().numpy()\n    \n    test_data['cnn_logits_'+str(idx)] = preds\n    idx+=1","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:09:05.390492Z","iopub.execute_input":"2021-07-16T03:09:05.390823Z","iopub.status.idle":"2021-07-16T03:09:18.777038Z","shell.execute_reply.started":"2021-07-16T03:09:05.390789Z","shell.execute_reply":"2021-07-16T03:09:18.776121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nidx=0\ntest_dataset = CLRPDataset(test_data, tokenizer, CONFIG.len256)\ntest_dataloader = DataLoader(test_dataset, batch_size=CONFIG.batch, shuffle=False)\n\nmodel = OneSidedNet(model_path, model_config)\nmodel.to(CONFIG.device)    \n\nfor trained_model in CONFIG.onesided_target:\n    model.load_state_dict(torch.load(trained_model, map_location=CONFIG.device))\n    \n    preds = []\n    for batch in test_dataloader:\n        with torch.no_grad():\n            device=CONFIG.device\n            ids = batch[\"ids\"].to(device)\n            mask = batch[\"mask\"].to(device)\n            output= model((ids, mask))\n\n            preds.append(output)\n            \n    preds = torch.cat(preds).cpu().detach().numpy()\n    \n    test_data['onesided_model_'+str(idx)] = preds\n    idx+=1","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:09:18.781363Z","iopub.execute_input":"2021-07-16T03:09:18.783608Z","iopub.status.idle":"2021-07-16T03:09:39.263811Z","shell.execute_reply.started":"2021-07-16T03:09:18.783554Z","shell.execute_reply":"2021-07-16T03:09:39.26293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()\n\nidx=0\ntest_dataset = CLRPDataset(test_data, tokenizer, CONFIG.len248)\ntest_dataloader = DataLoader(test_dataset, batch_size=CONFIG.batch, shuffle=False)\nmodel = WholeNet(model_path, model_config)\nmodel.to(CONFIG.device)    \n\nfor trained_model in CONFIG.whole_model:   \n    \n    model.load_state_dict(torch.load(trained_model, map_location=CONFIG.device))\n    \n    preds = []\n    for batch in test_dataloader:\n        with torch.no_grad():\n            device=CONFIG.device\n            ids = batch[\"ids\"].to(device)\n            mask = batch[\"mask\"].to(device)\n            output= model((ids, mask)) \n\n            preds.append(output)\n            \n    preds = torch.cat(preds).cpu().detach().numpy()\n    \n    test_data['whole_model_'+str(idx)] = preds\n    idx+=1\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:09:39.265212Z","iopub.execute_input":"2021-07-16T03:09:39.265555Z","iopub.status.idle":"2021-07-16T03:09:59.093679Z","shell.execute_reply.started":"2021-07-16T03:09:39.265519Z","shell.execute_reply":"2021-07-16T03:09:59.0927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_preds = []\ncolumns = ['cnn_logits_0', 'cnn_logits_1', 'cnn_logits_2', 'onesided_model_0', 'onesided_model_1', 'onesided_model_2', 'onesided_model_3', 'onesided_model_4', 'whole_model_0', 'whole_model_1', 'whole_model_2', 'whole_model_3', 'whole_model_4']\n\nfor k in range(5):\n    xgb_model = xgb.Booster()\n    xgb_model.load_model('../input/clrpensemblemodels/xgb_regression/xgb_fold{}.json'.format(str(k)))\n    ensemble_values = xgb.DMatrix(test_data[columns].values)\n    xgb_preds.append(xgb_model.predict(ensemble_values))\n\nsubmission_df = pd.DataFrame()\nsubmission_df['id'] = test_data['id']\nsubmission_df['target'] = np.mean(xgb_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:09:59.095242Z","iopub.execute_input":"2021-07-16T03:09:59.095654Z","iopub.status.idle":"2021-07-16T03:09:59.448511Z","shell.execute_reply.started":"2021-07-16T03:09:59.095602Z","shell.execute_reply":"2021-07-16T03:09:59.447609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:09:59.449813Z","iopub.execute_input":"2021-07-16T03:09:59.450153Z","iopub.status.idle":"2021-07-16T03:09:59.459961Z","shell.execute_reply.started":"2021-07-16T03:09:59.450107Z","shell.execute_reply":"2021-07-16T03:09:59.459004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission_df = pd.DataFrame()\n# submission_df['id'] = test_data['id']\n# submission_df['logit'] = np.where(test_data.cnn_logits_0.round() + test_data.cnn_logits_0.round() + test_data.cnn_logits_0.round()>1, -1, 1)\n# submission_df['onesided'] = test_data[['onesided_model_0', 'onesided_model_1', 'onesided_model_2', 'onesided_model_3', 'onesided_model_4']].mean(axis=1)\n# submission_df['target'] = (submission_df['logit']*submission_df['onesided']) + median_shift\n\n# submission_df = submission_df[['id', 'target']]\n\n# submission_df['onesided_inf'] = submission_df['logit']*submission_df['onesided'] + median_shift\n# submission_df['whole'] = test_data[['whole_model_0', 'whole_model_1', 'whole_model_2', 'whole_model_3', 'whole_model_4']].mean(axis=1)\n# submission_df['target'] = test_data['target']","metadata":{"execution":{"iopub.status.busy":"2021-07-16T03:09:59.461444Z","iopub.execute_input":"2021-07-16T03:09:59.462061Z","iopub.status.idle":"2021-07-16T03:09:59.466545Z","shell.execute_reply.started":"2021-07-16T03:09:59.462011Z","shell.execute_reply":"2021-07-16T03:09:59.465491Z"},"trusted":true},"execution_count":null,"outputs":[]}]}