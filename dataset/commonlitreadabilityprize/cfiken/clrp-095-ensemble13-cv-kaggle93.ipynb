{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport warnings\nwarnings.simplefilter('ignore', RuntimeWarning)\nimport sys\nimport logging\n\nIS_DEBUG = False\nIS_KAGGLE = True\n\nif IS_KAGGLE:\n    package_paths = [\n        '/kaggle/input/git-mykaggle/mykaggle/'\n    ]\n    for pth in package_paths:\n        sys.path.append(pth)\n\nfrom typing import Any, Dict, Tuple, List, Optional\nimport gc\nimport copy\nfrom pathlib import Path\nfrom enum import Enum\nimport random\nimport math\nimport yaml\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast\nfrom transformers import (\n    AutoTokenizer, PreTrainedTokenizerFast, PreTrainedModel, AutoConfig, AutoModel,\n    BertModel, RobertaModel, ElectraModel, DebertaModel, AlbertModel\n)\nfrom argparse import ArgumentParser, Namespace\n\n\ndef fix_seed(seed: int = 1019) -> None:\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = True  # type: ignore\n\n\nSETTINGS = yaml.safe_load('''\nname: '095_ensemble13_cv_kaggle93'\ncompetition: commonlitreadabilityprize\nmode: ensemble\nseed: 1019\ndevice: cuda\nis_full_training: true\ntraining:\n    trainer: class\n    use_amp: true\n    num_gpus: 1\n    train_file: train_mod.csv\n    test_file: test.csv\n    additional_data:\n    num_folds: 5\n    folds: stratified_5fold3.pkl\n    learning_rate: 0.00002\n    learning_rate_output: 0.0001\n    num_epochs: 6\n    batch_size: 4\n    test_batch_size: 4\n    num_accumulations: 4\n    num_workers: 4\n    scheduler: LinearDecayWithWarmUp\n    scheduler_epoch: 10\n    batch_scheduler: true\n    max_length: 256\n    warmup_epochs: 0.6\n    logger_verbose_step: 10\n    optimizer: AdamW\n    weight_decay: 0.0\n    optimizer_debias: true\n    use_layerwise_optim_params: true\n    use_large_output_lr: true\n    optim_layerwise_type: 2\n    loss: histgram\n    loss_reduction: 'none'\n    loss_rank_margin: 0.5\n    loss_huber_delta: 0.5\n    loss_use_rdrop: true\n    loss_rdrop_alpha: 2.0\n    use_rank_and_rmse_loss: true\n    loss_rmse_weight: 1.0\n    loss_external_weight: 1.0\n    val_check_interval: 10 # 0 or None means per epoch\n    ckpt_callback_verbose: true\n    use_standard_error: false\n    use_numerical_features: false\n    num_numerical_features: 0\n    mlm_probability: 0.15\nmodel:\n    model_name: microsoft/deberta-large\n    model_type: custom_head_class\n    pretrained: true\n    num_classes: 1\n    encoder_attn_dropout_rate: 0.1\n    encoder_ffn_dropout_rate: 0.1\n    layer_norm_eps: 0.0000001\n    dropout_rate: 0.3\n    output_activation: False\n    custom_head_types: ['cls', 'avg', 'max', 'attn'] # ['cls', 'attn', 'avg', 'max', 'conv']\n    custom_head_ensemble: avg\n    output_head_features: true\n    num_use_layers_as_output: 4\n    num_reinit_layers: 5\n    num_reinit_aux_layers: 0\n    second_output_dim: 256\n    head_hidden_dim: 1024\n    head_intermediate_dim: 512\n    num_output_heads: 2\n    use_middle_layers: [-1]\n    mlm_use_pooler: true\n    ckpt_from: ckpt/929_de_pt_922m05/model_{fold}.pt\n    head_stack:\n    aux_num_hidden_layers: 4\n    aux_hidden_size: 256\nheadstack:\n    model_name:\nensemble:\n    num_permutations: 12\n    fold_only:\n    ensemble_type: Nelder-Mead # Nelder-Mead\n    lb_weights: [\n        0.464,\n        0.462,\n        0.461,\n        0.457,\n        0.456,\n        # 0.454,\n        0.465,\n        # 0.454,\n        0.456,\n        0.455,\n        0.455,\n        0.458,\n        # 0.456,\n        # 0.454,\n        0.461,\n        0.463,\n        0.464,\n        # 0.459\n    ]\n    lb_constant_weight: 1.5\n    models:\n        - 680_de_ft535_head\n        - 713_de_ft535_head\n        - 801_de_ft795_attn\n        - 839_de_ft819_heads\n        - 840_de_ft819_headsw\n        # - 875_de_ft843_heads\n        - 881_ro_ft853_heads\n        # - 884_de_ft843_heads\n        - 885_de_ft843_headsw\n        - 906_de_ft843_heads # 10\n        - 919_de_ft891_heads\n        - 920_de_aux_ft890\n        # - 931_de_chaux_ft890\n        # - 938_gauss_ft843_ch\n        - 941_alxxl_ft924\n        - 960_alxxl_ft927_heads\n        - 969_el_ft945_heads\n        # - 081_de_ft961_classmodel\n''')\n\n\ndef get_logger(name: str, level: int = logging.INFO) -> logging.Logger:\n    '''\n    ライブラリ側で使用する logger を取得します。\n    :param name: logger の名前空間\n    '''\n    logger = logging.getLogger(name)\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setLevel(level)\n    logger.addHandler(handler)\n    logger.setLevel(level)\n    return logger\n\n\nLOGGER = get_logger(__name__)\n\n# misc\nif not IS_KAGGLE:\n    import dotenv\n    from mykaggle.util.ml_logger import MLLogger, assert_env\n    torch.multiprocessing.set_sharing_strategy('file_system')\n    dotenv.load_dotenv()\n    assert_env()\n    SETTINGS['model']['pretrained'] = False\nelse:\n    MLLogger = Any  # type: ignore\n\n# preparing constant path\n\nif IS_KAGGLE:\n    DATADIR = Path('/kaggle/input/') / SETTINGS[\"competition\"]\n    CKPTDIR = Path('/kaggle/input/ckpt-mykaggle/') / SETTINGS['name']\n    OUTPUTDIR = Path('/kaggle/working')\nelse:\n    DATADIR = Path('./data/')\n    CKPTDIR = Path('./ckpt/') / SETTINGS['name']\n    OUTPUTDIR = CKPTDIR\n\n    if not CKPTDIR.exists():\n        CKPTDIR.mkdir()\n\nROOT_CKPTDIR = CKPTDIR.parent\nTRAINDIR = DATADIR / 'train'\nTESTDIR = DATADIR / 'test'\n\n# load data\n\nif IS_KAGGLE:\n    df_train = pd.read_csv(DATADIR / 'train.csv')\n    df_test = pd.read_csv(DATADIR / 'test.csv')\n    df_sub = pd.read_csv(DATADIR / 'sample_submission.csv')\nelse:\n    if SETTINGS['training']['train_file'].endswith('ftr'):\n        df_train = pd.read_feather(DATADIR / SETTINGS['training']['train_file'])\n    else:\n        df_train = pd.read_csv(DATADIR / SETTINGS['training']['train_file'])\n        if not IS_KAGGLE and SETTINGS['training']['loss'] == 'histgram' and SETTINGS['mode'] == 'training':\n            hist_target = pickle.load(open(DATADIR / f'hist_target_c{SETTINGS[\"model\"][\"num_classes\"]}.pkl', 'rb'))\n            df_train[[f'hist_target_{i}' for i in range(SETTINGS['model']['num_classes'])]] = hist_target\n    df_test = pd.read_csv(DATADIR / SETTINGS['training']['test_file'])\n    df_sub = pd.read_csv(DATADIR / SETTINGS['training']['test_file'])\n    if SETTINGS['training']['additional_data']:\n        df_additional = pd.read_csv(DATADIR / SETTINGS['training']['additional_data'])\n\n\nif IS_DEBUG:\n    df_train = df_train.iloc[:100]\n\n# necessary parameters\nSETTINGS['ckptdir'] = str(CKPTDIR)\nSETTINGS['training']['ckptdir'] = str(CKPTDIR)\nSETTINGS['training']['num_classes'] = SETTINGS['model']['num_classes']\nSETTINGS['training']['num_batches'] = math.ceil((\n    math.ceil(len(df_train) * (SETTINGS['training']['num_folds'] - 1) / SETTINGS['training']['num_folds'])\n) / (SETTINGS['training']['batch_size'] * SETTINGS['training']['num_accumulations']))\nSETTINGS['training']['num_total_steps'] = SETTINGS['training']['num_batches'] * SETTINGS['training']['scheduler_epoch']\nif SETTINGS['mode'] == 'stacking':\n    SETTINGS['stacking']['training']['num_total_steps'] = SETTINGS['training']['num_total_steps']\n    SETTINGS['stacking']['training']['num_batches'] = SETTINGS['training']['num_batches']\n\nLOGGER.info(f'Loaded data, train shape:{df_train.shape}, test shape:{df_test.shape}')\n\n\ndef parse() -> Namespace:\n    parser = ArgumentParser(description='Process some integers.')\n    parser.add_argument(\n        '--gpus', type=str, help='index of gpus. if multiple, use comma to list.'\n    )\n    args = parser.parse_args()\n    return args\n\n\nclass Mode(Enum):\n    TRAIN = 'TRAIN'\n    VALID = 'VALID'\n    TEST = 'TEST'\n\n\nclass MyDataset(Dataset):\n    '''Get raw dataframe and json dir, prepare model inputs and feed by tf.data.Dataset\n    '''\n    def __init__(\n        self,\n        settings: Dict,\n        df: pd.DataFrame,\n        datadir: Optional[Path],\n        tokenizer: PreTrainedTokenizerFast,\n        mode: Mode,\n        fold: int,\n    ) -> None:\n        super().__init__()\n        self.settings = settings\n        self.df = df.reset_index(drop=True).copy()\n        self.tokenizer = tokenizer\n        self.mode = mode\n\n        def clean_text(text: str) -> str:\n            return text.replace('\\n', ' ').strip()\n\n        self.df['excerpt'] = self.df['excerpt'].apply(clean_text)\n\n        inputs = self.tokenizer(\n            self.df['excerpt'].values.tolist(),\n            padding='max_length',\n            truncation=True,\n            max_length=self.settings['max_length'],\n        )\n        self.inputs = {k: np.array(v).astype(np.int32) for k, v in inputs.items()}\n        if 'target' in self.df.columns:\n            self.labels = self.df['target'].values\n        else:\n            self.labels = None\n        if 'weight' not in self.df.columns:\n            self.df['weight'] = 1.0\n            if 'data_type' in self.df.columns:\n                self.df.loc[~df['data_type'].isna(), 'weight'] = self.settings.get('loss_external_weight', 1.0)\n        self.keys = list(self.inputs.keys())\n        self.epoch = 0\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, index: int):\n        data = self.df.iloc[index]\n        weight = data['weight'] if 'weight' in data else 1.0\n        inp = {}\n        for key in self.keys:\n            inp[key] = self.inputs[key][index]\n        if self.settings.get('use_standard_error', False) and self.mode == Mode.TRAIN:\n            std = data['standard_error']\n            inp['std'] = std\n\n        if self.labels is not None:\n            target = self.labels[index]\n            if 'dist_target' in data and self.mode == Mode.TRAIN and not np.isnan(data['dist_target']):\n                target = data['dist_target'] or target\n            return inp, (torch.tensor(target, dtype=torch.double), weight)\n        return inp\n\n\ndef get_dataloader(\n    settings: Dict[str, Any],\n    dataset: Dataset,\n    mode: Mode,\n    fold: int,\n    *args, **kwargs\n) -> DataLoader:\n    batch_size = settings['batch_size'] if mode == Mode.TRAIN else settings['test_batch_size']\n    dataloader: DataLoader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        pin_memory=False,\n        drop_last=False,\n        shuffle=mode == Mode.TRAIN,\n        num_workers=settings['num_workers'],\n    )\n    return dataloader\n\n\ndef get_transformers_model(\n    settings: Dict[str, Any],\n    model_name: str,\n    pretrained: bool = True,\n    ckptdir: Optional[Path] = None,\n) -> PreTrainedModel:\n    model_path = model_name if pretrained else str(ckptdir)\n    config = AutoConfig.from_pretrained(model_path)\n    config.attention_probs_dropout_prob = settings.get('encoder_attn_dropout_rate', 0.1)\n    config.hidden_dropout_prob = settings.get('encoder_ffn_dropout_rate', 0.1)\n    config.layer_norm_eps = settings.get('layer_norm_eps', 1e-5)\n    if pretrained:\n        model = AutoModel.from_pretrained(model_name, config=config)\n        return model\n\n    if 'albert' in model_name:\n        model = AlbertModel(config=config)\n    elif 'roberta' in model_name:\n        model = RobertaModel(config=config)\n    elif 'deberta' in model_name:\n        model = DebertaModel(config=config)\n    elif 'bert' in model_name:\n        model = BertModel(config=config)\n    elif 'electra' in model_name:\n        model = ElectraModel(config=config)\n    else:\n        model = BertModel(config=config)\n    return model\n\n\nclass ModelCustomHeadEnsemble(nn.Module):\n    def __init__(\n        self,\n        settings: Dict[str, Any],\n        model: PreTrainedModel\n    ) -> None:\n        super().__init__()\n        self.settings = settings\n        self.model = model\n        self.num_reinit_layers = settings['model'].get('num_reinit_layers', 0)\n        self.head_types = settings['model']['custom_head_types']\n        self.num_use_layers = self.settings['model']['num_use_layers_as_output']\n        output_layers = {}\n\n        if 'attn' in self.head_types:\n            self.hidden_dim = self.settings['model']['head_hidden_dim']\n            self.intermediate_dim = self.settings['model'].get('head_intermediate_dim', self.hidden_dim)\n            self.attn_head = AttentionHead(self.hidden_dim, self.intermediate_dim)\n        if 'conv' in self.head_types:\n            hidden_dim = self.settings['model'].get('conv_head_hidden_dim', 256)\n            kernel_size = self.settings['model'].get('conv_head_kernel_size', 2)\n            self.conv1 = nn.Conv1d(self.model.config.hidden_size, hidden_dim, kernel_size=kernel_size, padding=1)\n            self.conv2 = nn.Conv1d(hidden_dim, 1, kernel_size=kernel_size, padding=1)\n        if 'layers_sum' in self.head_types:\n            self.layer_weight = nn.Parameter(torch.tensor([1] * self.num_use_layers, dtype=torch.float))\n\n        for head in self.head_types:\n            if 'concat' in head:\n                output_layers[head] = nn.Linear(self.model.config.hidden_size * self.num_use_layers, 1)\n            elif head == 'conv':\n                continue\n            else:\n                output_layers[head] = nn.Linear(self.model.config.hidden_size, 1)\n        self.output_layers = nn.ModuleDict(output_layers)\n\n        self.dropout = nn.Dropout(settings['model']['dropout_rate'])\n        self.ensemble_type = settings['model']['custom_head_ensemble']\n        if self.ensemble_type == 'weight':\n            self.ensemble_weight = nn.Linear(len(self.head_types), 1, bias=False)\n        self.output_head_features = settings['model'].get('output_head_features', False)\n\n        self.initialize()\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs, output_hidden_states=True)\n        head_features = []\n        features = []\n        if 'cls' in self.head_types:\n            cls_state = outputs.last_hidden_state[:, 0, :]\n            feature = self.output_layers['cls'](self.dropout(cls_state))\n            head_features.append(cls_state)\n            features.append(feature)\n        if 'avg' in self.head_types:\n            avg_pool = torch.mean(outputs.last_hidden_state, 1)\n            feature = self.output_layers['avg'](self.dropout(avg_pool))\n            head_features.append(avg_pool)\n            features.append(feature)\n        if 'max' in self.head_types:\n            max_pool = torch.max(outputs.last_hidden_state, 1)[0]\n            feature = self.output_layers['max'](self.dropout(max_pool))\n            head_features.append(max_pool)\n            features.append(feature)\n        if 'attn' in self.head_types:\n            attn_state = self.attn_head(outputs.last_hidden_state)\n            feature = self.output_layers['attn'](self.dropout(attn_state))\n            head_features.append(attn_state)\n            features.append(feature)\n        if 'conv' in self.head_types:\n            conv_state = self.conv1(outputs.last_hidden_state.permute(0, 2, 1))\n            conv_state = F.relu(self.conv2(conv_state))\n            feature, _ = torch.max(conv_state, -1)\n            head_features.append(conv_state)\n            features.append(feature)\n        if 'layers_concat' in self.head_types:\n            hidden_states = outputs.hidden_states[-self.num_use_layers:]\n            cat_feature = torch.cat([state[:, 0, :] for state in hidden_states], -1)\n            feature = self.output_layers['layers_concat'](self.dropout(cat_feature))\n            head_features.append(cat_feature)\n            features.append(feature)\n        if 'layers_avg' in self.head_types:\n            hidden_states = torch.stack(outputs.hidden_states[-self.num_use_layers:], -1)[:, 0, :, :]\n            avg_feature = torch.mean(hidden_states, -1)\n            feature = self.output_layers['layers_avg'](self.dropout(avg_feature))\n            head_features.append(avg_feature)\n            features.append(feature)\n        if 'layers_sum' in self.head_types:\n            hidden_states = torch.stack(outputs.hidden_states[-self.num_use_layers:], -1)[:, 0, :, :]\n            weight = self.layer_weight[None, None, :] / self.layer_weight.sum()\n            weighted_sum_feature = torch.sum(hidden_states * weight, -1)\n            feature = self.output_layers['layers_sum'](self.dropout(weighted_sum_feature))\n            head_features.append(weighted_sum_feature)\n            features.append(feature)\n        if 'layers_attn' in self.head_types:\n            hidden_states = torch.stack(outputs.hidden_states[-self.num_use_layers:], -1)[:, 0, :, :]\n            attn_state = self.layer_attn(hidden_states)\n            feature = self.output_layers['layers_attn'](self.dropout(attn_state))\n            head_features.append(attn_state)\n            features.append(feature)\n\n        outputs = torch.cat(features, -1)\n        if len(self.head_types) > 1:\n            if self.ensemble_type == 'avg':\n                outputs = torch.mean(outputs, -1)\n            elif self.ensemble_type == 'weight':\n                if self.settings['training']['trainer'] == 'multi':\n                    outputs = outputs.detach()\n                weight = self.ensemble_weight.weight / torch.sum(self.ensemble_weight.weight)\n                outputs = torch.sum(weight * outputs, -1)\n        outputs = outputs.reshape(inputs['input_ids'].shape[0])\n        if self.output_head_features:\n            features = [f.reshape(inputs['input_ids'].shape[0]) for f in features]\n            return outputs, features, head_features\n        return outputs\n\n    def initialize(self):\n        if self.ensemble_type == 'weight':\n            torch.nn.init.constant_(self.ensemble_weight.weight, 1.0)\n        self.output_layers.apply(self._init_weight)\n        for i in range(self.num_reinit_layers):\n            self.model.encoder.layer[-(1 + i)].apply(self._init_weight)\n\n    def _init_weight(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n\nclass ModelCustomHeadClassification(nn.Module):\n    def __init__(\n        self,\n        settings: Dict[str, Any],\n        model: PreTrainedModel\n    ) -> None:\n        super().__init__()\n        self.settings = settings\n        self.model = model\n        self.num_classes = settings['model']['num_classes']\n        self.num_reinit_layers = settings['model'].get('num_reinit_layers', 0)\n        self.head_types = settings['model']['custom_head_types']\n        self.num_use_layers = self.settings['model']['num_use_layers_as_output']\n        output_layers = {}\n\n        if 'attn' in self.head_types:\n            self.hidden_dim = self.settings['model']['head_hidden_dim']\n            self.intermediate_dim = self.settings['model'].get('head_intermediate_dim', self.hidden_dim)\n            self.attn_head = AttentionHead(self.hidden_dim, self.intermediate_dim)\n        if 'layers_sum' in self.head_types:\n            self.layer_weight = nn.Parameter(torch.tensor([1] * self.num_use_layers, dtype=torch.float))\n\n        for head in self.head_types:\n            if 'concat' in head:\n                output_layers[head] = nn.Linear(self.model.config.hidden_size * self.num_use_layers, self.num_classes)\n            else:\n                output_layers[head] = nn.Linear(self.model.config.hidden_size, self.num_classes)\n        self.output_layers = nn.ModuleDict(output_layers)\n\n        self.dropout = nn.Dropout(settings['model']['dropout_rate'])\n        self.ensemble_type = settings['model']['custom_head_ensemble']\n        if self.ensemble_type == 'weight':\n            self.ensemble_weight = nn.Linear(len(self.head_types), 1, bias=False)\n        self.output_head_features = settings['model'].get('output_head_features', False)\n\n        self.initialize()\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs, output_hidden_states=True)\n        head_features = []\n        features = []\n        if 'cls' in self.head_types:\n            cls_state = outputs.last_hidden_state[:, 0, :]\n            feature = self.output_layers['cls'](self.dropout(cls_state))\n            head_features.append(cls_state)\n            features.append(feature)\n        if 'avg' in self.head_types:\n            avg_pool = torch.mean(outputs.last_hidden_state, 1)\n            feature = self.output_layers['avg'](self.dropout(avg_pool))\n            head_features.append(avg_pool)\n            features.append(feature)\n        if 'max' in self.head_types:\n            max_pool = torch.max(outputs.last_hidden_state, 1)[0]\n            feature = self.output_layers['max'](self.dropout(max_pool))\n            head_features.append(max_pool)\n            features.append(feature)\n        if 'attn' in self.head_types:\n            attn_state = self.attn_head(outputs.last_hidden_state)\n            feature = self.output_layers['attn'](self.dropout(attn_state))\n            head_features.append(attn_state)\n            features.append(feature)\n        if 'layers_concat' in self.head_types:\n            hidden_states = outputs.hidden_states[-self.num_use_layers:]\n            cat_feature = torch.cat([state[:, 0, :] for state in hidden_states], -1)\n            feature = self.output_layers['layers_concat'](self.dropout(cat_feature))\n            head_features.append(cat_feature)\n            features.append(feature)\n        if 'layers_sum' in self.head_types:\n            hidden_states = torch.stack(outputs.hidden_states[-self.num_use_layers:], -1)[:, 0, :, :]\n            weight = self.layer_weight[None, None, :] / self.layer_weight.sum()\n            weighted_sum_feature = torch.sum(hidden_states * weight, -1)\n            feature = self.output_layers['layers_sum'](self.dropout(weighted_sum_feature))\n            head_features.append(weighted_sum_feature)\n            features.append(feature)\n\n        features = [F.softmax(f, -1) for f in features]\n        outputs = torch.stack(features, -1)\n        if len(self.head_types) > 1:\n            if self.ensemble_type == 'avg':\n                outputs = torch.mean(outputs, -1)\n            elif self.ensemble_type == 'weight':\n                if self.settings['training']['trainer'] == 'multi':\n                    outputs = outputs.detach()\n                weight = self.ensemble_weight.weight / torch.sum(self.ensemble_weight.weight)\n                outputs = torch.sum(weight * outputs, -1)\n        outputs = outputs.reshape((inputs['input_ids'].shape[0], -1))\n        if self.output_head_features:\n            return outputs, features, head_features\n        return outputs\n\n    def initialize(self):\n        if self.ensemble_type == 'weight':\n            torch.nn.init.constant_(self.ensemble_weight.weight, 1.0)\n        self.output_layers.apply(self._init_weight)\n        for i in range(self.num_reinit_layers):\n            self.model.encoder.layer[-(1 + i)].apply(self._init_weight)\n\n    def _init_weight(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n\nclass ModelCustomHeadWithClassOutput(nn.Module):\n    def __init__(\n        self,\n        settings: Dict[str, Any],\n        model: PreTrainedModel,\n        class_model: ModelCustomHeadClassification\n    ) -> None:\n        super().__init__()\n        self.settings = settings\n        self.model = model\n        self.class_model = class_model\n        self.num_add_classes = class_model.num_classes\n        self.num_classes = settings['model']['num_classes']\n        self.num_reinit_layers = settings['model'].get('num_reinit_layers', 0)\n        self.head_types = settings['model']['custom_head_types']\n        self.num_use_layers = self.settings['model']['num_use_layers_as_output']\n        output_layers = {}\n\n        if 'attn' in self.head_types:\n            self.hidden_dim = self.settings['model']['head_hidden_dim']\n            self.intermediate_dim = self.settings['model'].get('head_intermediate_dim', self.hidden_dim)\n            self.attn_head = AttentionHead(self.hidden_dim, self.intermediate_dim)\n        if 'layers_sum' in self.head_types:\n            self.layer_weight = nn.Parameter(torch.tensor([1] * self.num_use_layers, dtype=torch.float))\n\n        for head in self.head_types:\n            if 'concat' in head:\n                output_layers[head] = nn.Linear(self.model.config.hidden_size * self.num_use_layers, self.num_classes)\n            else:\n                output_layers[head] = nn.Linear(self.model.config.hidden_size + self.num_add_classes, self.num_classes)\n        self.output_layers = nn.ModuleDict(output_layers)\n\n        self.dropout = nn.Dropout(settings['model']['dropout_rate'])\n        self.ensemble_type = settings['model']['custom_head_ensemble']\n        if self.ensemble_type == 'weight':\n            self.ensemble_weight = nn.Linear(len(self.head_types), 1, bias=False)\n        self.output_head_features = settings['model'].get('output_head_features', False)\n\n        self.initialize()\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs, output_hidden_states=True)\n        with torch.no_grad():  # class model のパラメータは触らない\n            class_outputs = self.class_model(inputs)[0]  # [batch_size, num_classes]\n            if self.settings['model'].get('use_log_for_class_output', False):\n                class_outputs = torch.log(class_outputs)\n        head_features = []\n        features = []\n        if 'cls' in self.head_types:\n            cls_state = outputs.last_hidden_state[:, 0, :]\n            cls_state = torch.cat([cls_state, class_outputs], -1)\n            feature = self.output_layers['cls'](self.dropout(cls_state))\n            head_features.append(cls_state)\n            features.append(feature)\n        if 'avg' in self.head_types:\n            avg_pool = torch.mean(outputs.last_hidden_state, 1)\n            avg_pool = torch.cat([avg_pool, class_outputs], -1)\n            feature = self.output_layers['avg'](self.dropout(avg_pool))\n            head_features.append(avg_pool)\n            features.append(feature)\n        if 'max' in self.head_types:\n            max_pool = torch.max(outputs.last_hidden_state, 1)[0]\n            max_pool = torch.cat([max_pool, class_outputs], -1)\n            feature = self.output_layers['max'](self.dropout(max_pool))\n            head_features.append(max_pool)\n            features.append(feature)\n        if 'attn' in self.head_types:\n            attn_state = self.attn_head(outputs.last_hidden_state)\n            attn_state = torch.cat([attn_state, class_outputs], -1)\n            feature = self.output_layers['attn'](self.dropout(attn_state))\n            head_features.append(attn_state)\n            features.append(feature)\n        if 'layers_sum' in self.head_types:\n            hidden_states = torch.stack(outputs.hidden_states[-self.num_use_layers:], -1)[:, 0, :, :]\n            weight = self.layer_weight[None, None, :] / self.layer_weight.sum()\n            weighted_sum_feature = torch.sum(hidden_states * weight, -1)\n            weighted_sum_feature = torch.cat([weighted_sum_feature, class_outputs], -1)\n            feature = self.output_layers['layers_sum'](self.dropout(weighted_sum_feature))\n            head_features.append(weighted_sum_feature)\n            features.append(feature)\n\n        outputs = torch.stack(features, -1)\n        if len(self.head_types) > 1:\n            if self.ensemble_type == 'avg':\n                outputs = torch.mean(outputs, -1)\n            elif self.ensemble_type == 'weight':\n                if self.settings['training']['trainer'] == 'multi':\n                    outputs = outputs.detach()\n                weight = self.ensemble_weight.weight / torch.sum(self.ensemble_weight.weight)\n                outputs = torch.sum(weight * outputs, -1)\n        outputs = outputs.reshape((inputs['input_ids'].shape[0]))\n        if self.output_head_features:\n            return outputs, features, head_features\n        return outputs\n\n    def initialize(self):\n        if self.ensemble_type == 'weight':\n            torch.nn.init.constant_(self.ensemble_weight.weight, 1.0)\n        self.output_layers.apply(self._init_weight)\n        for i in range(self.num_reinit_layers):\n            self.model.encoder.layer[-(1 + i)].apply(self._init_weight)\n\n    def _init_weight(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n\nclass ModelCustomHeadAuxNetwork(nn.Module):\n    def __init__(\n        self,\n        settings: Dict[str, Any],\n        config,\n        model: PreTrainedModel\n    ) -> None:\n        super().__init__()\n        self.settings = settings\n        self.model = model\n        self.num_reinit_layers = settings['model'].get('num_reinit_layers', 0)\n        self.num_reinit_aux_layers = settings['model'].get('num_reinit_aux_layers', 0)\n        self.head_types = settings['model']['custom_head_types']\n        self.num_use_layers = self.settings['model']['num_use_layers_as_output']\n        output_layers = {}\n\n        if 'attn' in self.head_types:\n            self.hidden_dim = self.settings['model']['head_hidden_dim']\n            self.intermediate_dim = self.settings['model'].get('head_intermediate_dim', self.hidden_dim)\n            self.attn_head = AttentionHead(self.hidden_dim, self.intermediate_dim)\n        if 'layers_sum' in self.head_types:\n            self.layer_weight = nn.Parameter(torch.tensor([1] * self.num_use_layers, dtype=torch.float))\n\n        aux_config = copy.deepcopy(config)\n        aux_config.num_hidden_layers = settings['model']['aux_num_hidden_layers']\n        aux_config.hidden_size = settings['model']['aux_hidden_size']\n        aux_config.num_attention_heads = aux_config.hidden_size // 8\n        aux_config.intermediate_size = aux_config.hidden_size * 4\n        self.aux_model = DebertaModel(aux_config)\n\n        for head in self.head_types:\n            if 'concat' in head:\n                output_layers[head] = nn.Linear(\n                    self.model.config.hidden_size * self.num_use_layers + aux_config.hidden_size, 1\n                )\n            elif head == 'conv':\n                continue\n            else:\n                output_layers[head] = nn.Linear(self.model.config.hidden_size + aux_config.hidden_size, 1)\n        self.output_layers = nn.ModuleDict(output_layers)\n\n        self.dropout = nn.Dropout(settings['model']['dropout_rate'])\n        self.ensemble_type = settings['model']['custom_head_ensemble']\n        if self.ensemble_type == 'weight':\n            self.ensemble_weight = nn.Linear(len(self.head_types), 1, bias=False)\n        self.output_head_features = settings['model'].get('output_head_features', False)\n\n        self.initialize()\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs, output_hidden_states=True)\n        aux_outputs = self.aux_model(**inputs).last_hidden_state[:, 0, :]\n        head_features = []\n        features = []\n        if 'cls' in self.head_types:\n            cls_state = outputs.last_hidden_state[:, 0, :]\n            feature = self.output_layers['cls'](self.dropout(torch.cat([cls_state, aux_outputs], axis=-1)))\n            head_features.append(cls_state)\n            features.append(feature)\n        if 'avg' in self.head_types:\n            avg_pool = torch.mean(outputs.last_hidden_state, 1)\n            feature = self.output_layers['avg'](self.dropout(torch.cat([avg_pool, aux_outputs], axis=-1)))\n            head_features.append(avg_pool)\n            features.append(feature)\n        if 'max' in self.head_types:\n            max_pool = torch.max(outputs.last_hidden_state, 1)[0]\n            feature = self.output_layers['max'](self.dropout(torch.cat([max_pool, aux_outputs], axis=-1)))\n            head_features.append(max_pool)\n            features.append(feature)\n        if 'attn' in self.head_types:\n            attn_state = self.attn_head(outputs.last_hidden_state)\n            feature = self.output_layers['attn'](self.dropout(torch.cat([attn_state, aux_outputs], axis=-1)))\n            head_features.append(attn_state)\n            features.append(feature)\n        if 'layers_concat' in self.head_types:\n            hidden_states = outputs.hidden_states[-self.num_use_layers:]\n            cat_feature = torch.cat([state[:, 0, :] for state in hidden_states], -1)\n            feature = self.output_layers['layers_concat'](self.dropout(torch.cat([cat_feature, aux_outputs], axis=-1)))\n            head_features.append(cat_feature)\n            features.append(feature)\n        if 'layers_sum' in self.head_types:\n            hidden_states = torch.stack(outputs.hidden_states[-self.num_use_layers:], -1)[:, 0, :, :]\n            weight = self.layer_weight[None, None, :] / self.layer_weight.sum()\n            weighted_sum_feature = torch.sum(hidden_states * weight, -1)\n            feature = self.output_layers['layers_sum'](\n                self.dropout(torch.cat([weighted_sum_feature, aux_outputs], axis=-1))\n            )\n            head_features.append(weighted_sum_feature)\n            features.append(feature)\n\n        outputs = torch.cat(features, -1)\n        if len(self.head_types) > 1:\n            if self.ensemble_type == 'avg':\n                outputs = torch.mean(outputs, -1)\n            elif self.ensemble_type == 'weight':\n                if self.settings['training']['trainer'] == 'multi':\n                    outputs = outputs.detach()\n                weight = self.ensemble_weight.weight / torch.sum(self.ensemble_weight.weight)\n                outputs = torch.sum(weight * outputs, -1)\n        outputs = outputs.reshape(inputs['input_ids'].shape[0])\n        if self.output_head_features:\n            features = [f.reshape(inputs['input_ids'].shape[0]) for f in features]\n            return outputs, features, head_features\n        return outputs\n\n    def initialize(self):\n        if self.ensemble_type == 'weight':\n            torch.nn.init.constant_(self.ensemble_weight.weight, 1.0)\n        self.output_layers.apply(self._init_weight)\n        for i in range(self.num_reinit_layers):\n            self.model.encoder.layer[-(1 + i)].apply(self._init_weight)\n        for i in range(self.num_reinit_aux_layers):\n            self.aux_model.encoder.layer[-(1 + i)].apply(self._init_weight)\n\n    def _init_weight(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n\nclass ModelAuxNetwork(nn.Module):\n\n    def __init__(\n        self,\n        settings: Dict[str, Any],\n        config,\n        model: PreTrainedModel,\n    ) -> None:\n        super().__init__()\n        self.settings = settings\n        self.config = config\n        self.model = model\n        self.num_reinit_layers = settings['model'].get('num_reinit_layers', 0)\n        aux_config = copy.deepcopy(config)\n        aux_config.num_hidden_layers = settings['model']['aux_num_hidden_layers']\n        aux_config.hidden_size = settings['model']['aux_hidden_size']\n        aux_config.num_attention_heads = aux_config.hidden_size // 8\n        aux_config.intermediate_size = aux_config.hidden_size * 4\n\n        self.aux_model = DebertaModel(aux_config)\n        self.output_layer = nn.Linear(self.model.config.hidden_size + aux_config.hidden_size, 1)\n        self.dropout = nn.Dropout(settings['model']['dropout_rate'])\n\n    def forward(self, inputs):\n        outputs = self.model(**inputs)\n        cls_state = outputs.last_hidden_state[:, 0, :]\n\n        aux_outputs = self.aux_model(**inputs)\n        aux_outputs = aux_outputs.last_hidden_state[:, 0, :]\n        outputs = self.output_layer(self.dropout(torch.cat([cls_state, aux_outputs], -1)))\n        outputs = outputs.reshape(inputs['input_ids'].shape[0])\n        return outputs\n\n    def initialize(self):\n        self.output_layer.apply(self._init_weight)\n        for i in range(self.num_reinit_layers):\n            self.model.encoder.layer[-(1 + i)].apply(self._init_weight)\n\n    def _init_weight(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n\nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\n\ndef get_model(\n    settings: Dict[str, Any],\n    fold: Optional[int] = None,\n    is_kaggle: bool = False,\n    pretrained: bool = True,\n    hg_model: Optional[PreTrainedModel] = None,\n    *args, **kwargs\n) -> nn.Module:\n    mst = settings['model']\n    fold = fold or 0\n    if hg_model is None:\n        hg_model = get_transformers_model(\n            settings['model'], mst['model_name'], pretrained, ckptdir=settings['ckptdir']\n        )\n    if fold == 0:\n        LOGGER.info(hg_model.config)\n    model_type = settings['model'].get('model_type', 'cls_base')\n    model: nn.Module\n    if model_type == 'custom_head':\n        model = ModelCustomHeadEnsemble(settings, hg_model)\n    elif model_type == 'custom_head_class':\n        model = ModelCustomHeadClassification(settings, hg_model)\n    elif model_type == 'custom_head_aux':\n        model = ModelCustomHeadAuxNetwork(settings, hg_model.config, hg_model)\n    elif model_type == 'custom_head_with_class':\n        class_model = kwargs['class_model']\n        model = ModelCustomHeadWithClassOutput(settings, hg_model, class_model)\n    elif model_type == 'aux_network':\n        model = ModelAuxNetwork(settings, hg_model.config, hg_model)\n    else:\n        model = ModelCustomHeadEnsemble(settings, hg_model)\n\n    return model\n\n\ndef predict(\n    model: nn.Module,\n    df: pd.DataFrame,\n    dataloader: DataLoader,\n    batch_size: int,\n    num_classes: int,\n    use_amp: bool = True\n) -> np.ndarray:\n    if num_classes > 1:\n        preds = np.zeros((len(df), num_classes), dtype=np.float32)\n    else:\n        preds = np.zeros((len(df)), dtype=np.float32)\n    device = torch.device('cuda')\n    model.to(device)\n    model.eval()\n    for i, batch in enumerate(dataloader):\n        if isinstance(batch, (list, tuple)):\n            inputs = batch[0]\n        else:\n            inputs = batch\n        if 'label' in inputs.keys():\n            inputs.pop('label')\n        if 'comparison_label' in inputs.keys():\n            inputs.pop('comparison_label')\n        for key in inputs.keys():\n            inputs[key] = inputs[key].to(device).long()\n        with autocast(enabled=use_amp):\n            with torch.no_grad():\n                outputs = model(inputs)\n                if isinstance(outputs, (list, tuple)):\n                    outputs = outputs[0]\n        preds[i * batch_size:(i + 1) * batch_size] = outputs.detach().cpu().numpy()\n    return preds\n\n\ndef test(\n    settings: Dict[str, Any],\n    model: nn.Module,\n    dataloader: DataLoader,\n    df_test: pd.DataFrame,\n) -> np.ndarray:\n    batch_size = settings['training']['test_batch_size']\n    use_amp = settings['training']['use_amp']\n    num_classes = settings['training']['num_classes']\n    preds = predict(model, df_test, dataloader, batch_size, num_classes, use_amp)\n    return preds\n\n\ndef ensemble_inference(\n    models: List[str],\n    df: pd.DataFrame,\n    num_classes: int,\n    ckptdir: Path,\n    datadir: Path,\n    is_kaggle: bool,\n    ensemble_type: str = 'avg',\n    weights: Optional[np.ndarray] = None\n) -> np.ndarray:\n    device = torch.device('cuda')\n\n    whole_preds = np.zeros((len(models), len(df)))\n    for i, model_name in enumerate(models):\n        gc.collect()\n        torch.cuda.empty_cache()\n        LOGGER.info(f'inference by {model_name} start.')\n        model_ckptdir = ckptdir / model_name\n        model_settings = yaml.safe_load(open(model_ckptdir / 'settings.yml', 'r'))\n        model_settings['ckptdir'] = model_ckptdir\n        model_settings['training']['ckptdir'] = model_ckptdir\n        model_settings['model']['ckpt_from'] = None\n        tokenizer = AutoTokenizer.from_pretrained(model_ckptdir)\n        for fold in range(model_settings['training']['num_folds']):\n            gc.collect()\n            torch.cuda.empty_cache()\n            ds = MyDataset(model_settings['training'], df, datadir, tokenizer, Mode.TEST, fold)\n            dataloader = get_dataloader(model_settings['training'], ds, Mode.TEST, fold)\n            if model_settings['name'] == '081_de_ft961_classmodel':\n                class_model_settings = yaml.safe_load(open(model_ckptdir / '049_settings.yml', 'r'))\n                class_model_settings['ckptdir'] = str(model_ckptdir)\n                class_model = get_model(class_model_settings, fold=fold, is_kaggle=IS_KAGGLE, pretrained=False).cuda()\n                model = get_model(\n                    model_settings, fold=fold, is_kaggle=IS_KAGGLE, pretrained=False, class_model=class_model\n                )\n            else:\n                class_model = None\n                model = get_model(model_settings, fold=fold, is_kaggle=is_kaggle, pretrained=False)\n            model.load_state_dict(torch.load(model_ckptdir / f'model_{fold}.pt'))\n            model.to(device)\n            preds = test(model_settings, model, dataloader, df)\n            whole_preds[i] += preds / model_settings['training']['num_folds']\n            del model, class_model\n    if ensemble_type == 'avg':\n        whole_preds = np.mean(whole_preds, axis=0)\n    else:\n        if weights is not None:\n            whole_preds = np.sum(whole_preds * weights[:, np.newaxis], axis=0)\n        else:\n            whole_preds = np.mean(whole_preds, axis=0)\n\n    return whole_preds\n\n\ndef do_head_stacking(\n    settings: Dict[str, Any],\n    df_train: pd.DataFrame,\n    df_test: pd.DataFrame\n) -> Tuple[np.ndarray, np.ndarray]:\n    pass\n\n\ndef do_ensemble(\n    settings: Dict[str, Any],\n    df_train: pd.DataFrame,\n    df_test: pd.DataFrame,\n    fold: Optional[int] = None\n) -> Tuple[np.ndarray, np.ndarray]:\n    models = settings['ensemble']['models']\n    best_ensemble_weights = np.array([\n        0.02372718, 0.08941032, 0.04026971, 0.04065091, 0.11687406,\n        0.069342, 0.12299674, 0.01315429, 0.05438842, 0.04086522, 0.16173649, 0.11391101, 0.12956535\n    ])\n\n    preds = ensemble_inference(\n        models, df_test, settings['model']['num_classes'],\n        CKPTDIR.parent, TESTDIR, IS_KAGGLE,\n        settings['ensemble']['ensemble_type'], best_ensemble_weights\n    )\n    return preds\n\n\ndef do_stacking(\n    settings: Dict[str, Any],\n    df_train: pd.DataFrame,\n    df_test: pd.DataFrame\n) -> Tuple[np.ndarray, np.ndarray]:\n    pass\n\n\ndef do_training(settings: Dict[str, Any], df: pd.DataFrame):\n    pass\n\n\ndef do_inference(settings: Dict[str, Any], df: pd.DataFrame):\n    tst = settings['training']\n    tokenizer = AutoTokenizer.from_pretrained(CKPTDIR)\n    whole_preds = np.zeros((len(df)))\n    if not settings['is_full_training']:\n        LOGGER.info('inference is skipped since is_full_training is False')\n        return whole_preds\n    for fold in range(settings['training']['num_folds']):\n        ds = MyDataset(tst, df, TESTDIR, tokenizer, Mode.TEST, fold)\n        dataloader = get_dataloader(tst, ds, Mode.TEST, fold)\n        model = get_model(settings, fold=fold, is_kaggle=IS_KAGGLE, pretrained=False)\n        model.load_state_dict(torch.load(CKPTDIR / f'model_{fold}.pt'))\n        preds = test(settings, model, dataloader, df)\n        if 'class' in tst['trainer']:\n            centers = pickle.load(open(CKPTDIR / 'centers.pkl', 'rb'))\n            preds = np.sum(preds * centers[None, :], -1)\n        whole_preds += preds / settings['training']['num_folds']\n    return whole_preds\n\n\ndef submit(settings: Dict[str, Any], df_sub: pd.DataFrame, preds: np.ndarray, suffix: str = '') -> None:\n    df_sub['target'] = preds\n    df_sub.to_csv(OUTPUTDIR / f'submission{suffix}.csv', index=False)\n    print(df_sub)\n\n\ndef main(settings: Dict[str, Any]):\n    mode = settings.get('mode', 'training')\n    if mode == 'training' and IS_KAGGLE:\n        mode = 'inference'\n    LOGGER.info(f'start {mode}')\n    if mode == 'ensemble':\n        fold = settings['ensemble'].get('fold_only')\n        if fold is not None:\n            preds = do_ensemble(settings, df_train, df_test, fold=fold)\n            submit(settings, df_sub, preds, str(fold))\n        else:\n            preds = do_ensemble(settings, df_train, df_test)\n            submit(settings, df_sub, preds)\n    elif mode == 'stacking':\n        preds = do_stacking(settings, df_train, df_test)\n        submit(settings, df_sub, preds)\n    elif mode == 'inference':\n        preds = do_inference(settings, df_test)\n        submit(settings, df_sub, preds)\n    elif mode == 'headstack':\n        preds = do_head_stacking(settings, df_train, df_test)\n        submit(settings, df_sub, preds)\n    elif mode == 'other_training':\n        do_training(settings, df_train)\n    else:\n        do_training(settings, df_train)\n        preds = do_inference(settings, df_test)\n        submit(settings, df_sub, preds)\n\n\nif __name__ == '__main__':\n    if IS_KAGGLE:\n        LOGGER.info('starting in kaggle environment')\n        fix_seed(SETTINGS['seed'])\n        main(SETTINGS)\n    else:\n        args = parse()\n        LOGGER.info(f'starting with args: {args}')\n        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpus\n        os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n        fix_seed(SETTINGS['seed'])\n        main(SETTINGS)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-02T08:55:19.249083Z","iopub.execute_input":"2021-08-02T08:55:19.249421Z","iopub.status.idle":"2021-08-02T08:55:39.610177Z","shell.execute_reply.started":"2021-08-02T08:55:19.24939Z","shell.execute_reply":"2021-08-02T08:55:39.607447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}