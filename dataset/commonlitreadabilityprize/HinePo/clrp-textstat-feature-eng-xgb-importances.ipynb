{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this notebook\n\n- Feature engineering: 40 features created, most of them using Textstat and Spacy\n- XGB train\n- Feature importance analysis\n- Submission","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport sys\nfrom sklearn import preprocessing\nimport collections\nfrom tqdm import tqdm\n\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import unicode_literals, print_function\nfrom spacy.lang.en import English\nimport spacy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# needs internet or package as input data\n# pip install textstat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# not using internet\n\n# Access to textstat files\nsys.path.append(\"../input/textstat\")\nsys.path.append(\"../input/textstat/textstat-master\")\n\n# Access to pyphen files\nsys.path.append(\"../input/pyphen\")\nsys.path.append(\"../input/pyphen/Pyphen-master\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyphen\nimport textstat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_original = train_df[['excerpt','target']]\ntrain_original","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test data","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\ntest_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"[Textstat package](https://pypi.org/project/textstat/)","metadata":{}},{"cell_type":"code","source":"train_original.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef feat_eng(df):\n    df = df.copy() # .head(3) # head for testing/debugging\n    \n    df['syllable_count'] = [textstat.syllable_count(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['lexicon_count'] = [textstat.lexicon_count(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['sentence_count'] = [textstat.sentence_count(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['flesch_reading_ease'] = [textstat.flesch_reading_ease(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['flesch_kincaid_grade'] = [textstat.flesch_kincaid_grade(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['gunning_fog'] = [textstat.gunning_fog(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['smog_index'] = [textstat.smog_index(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['automated_readability_index'] = [textstat.automated_readability_index(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['coleman_liau_index'] = [textstat.coleman_liau_index(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    \n    df['linsear_write_formula'] = [textstat.linsear_write_formula(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['linsear_write_formula'] = round(df['linsear_write_formula'], 3)\n    \n    df['dale_chall_readability_score'] = [textstat.dale_chall_readability_score(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    df['crawford'] = [textstat.crawford(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    \n    df['text_standard'] = [textstat.text_standard(df.excerpt.iloc[i]) for i in range(0, len(df))]\n    le = preprocessing.LabelEncoder()\n    df['text_standard'] = le.fit_transform(df['text_standard']) # categorical feature\n    \n    df['word_count'] = [len(df.excerpt.iloc[i].split(' ')) for i in range(0, len(df))]\n    \n    for row in tqdm(range(0, len(df))):\n        nlp = English()\n        complicated_signals = nlp(':;-&')\n        \n        full_text = []\n        full_text = df.excerpt.iloc[row] # or full_text = df.loc[row, 'excerpt']\n        doc = nlp(full_text)\n        words_ns = [token.text for token in doc if not token.is_stop and not token.is_punct] # not stopwords & not punct\n        df.loc[row, 'word_count_ns'] = len(words_ns)\n        \n        punct_count = [token.text for token in doc if token.is_punct] \n        df.loc[row, 'punctuation_count'] = len(punct_count)\n        \n        complicated = [token.text for token in doc if token in complicated_signals]\n        df.loc[row, 'complicated_signals'] = len(complicated)\n        \n        df.loc[row, 'vocab_len'] = len(doc.vocab)\n        \n        \n        # POS: Parts of Speech\n        all_tags_in_a_row = []\n        nlp = spacy.load('en_core_web_sm') # load model\n        doc = nlp(df.loc[row, 'excerpt'])\n        all_tags_in_a_row.append([token.pos_ for token in doc]) # list with tags (POS)\n        row_dict = collections.Counter(all_tags_in_a_row[0]) # Counter object\n#         print(row_dict)\n\n        # create columns accessing Counter object\n        df.loc[row, 'n_ADJ'] = row_dict['ADJ']\n        df.loc[row, 'n_ADP'] = row_dict['ADP']\n        df.loc[row, 'n_ADV'] = row_dict['ADV']\n        df.loc[row, 'n_AUX'] = row_dict['AUX']\n        df.loc[row, 'n_CCONJ'] = row_dict['CCONJ']\n        df.loc[row, 'n_DET'] = row_dict['DET']\n        df.loc[row, 'n_INTJ'] = row_dict['INTJ']\n        df.loc[row, 'n_NOUN'] = row_dict['NOUN']\n        df.loc[row, 'n_NUM'] = row_dict['NUM']\n        df.loc[row, 'n_PART'] = row_dict['PART']\n        df.loc[row, 'n_PRON'] = row_dict['PRON']\n        df.loc[row, 'n_PROPN'] = row_dict['PROPN']\n#         df.loc[row, 'n_PUNCT'] = row_dict['PUNCT'] # same as 'punctuation_count' column\n        df.loc[row, 'n_SCONJ'] = row_dict['SCONJ']\n        df.loc[row, 'n_VERB'] = row_dict['VERB']\n        \n        \n        # sentences\n        nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\n        sentences = [sent.string.strip() for sent in doc.sents]\n        df.loc[row, 'n_sentences'] = len(sentences)\n        \n        df.loc[row, 'avg_words_per_sentence'] = df.loc[row, 'word_count']/len(sentences)\n        df['avg_words_per_sentence'] = round(df['avg_words_per_sentence'], 2)\n        df.loc[row, 'avg_words_ns_per_sentence'] = df.loc[row, 'word_count_ns']/len(sentences)\n        df['avg_words_ns_per_sentence'] = round(df['avg_words_ns_per_sentence'], 2)\n        \n        \n        for sentence in sentences:\n            flesch_list = []\n            flesch_list.append(textstat.flesch_reading_ease(sentence))\n            df.loc[row, 'max_flesch_per_sentence'] = max(flesch_list)\n            df.loc[row, 'min_flesch_per_sentence'] = min(flesch_list)\n            \n            flesch_kincaid_list = []\n            flesch_kincaid_list.append(textstat.flesch_kincaid_grade(sentence))\n            df.loc[row, 'max_flesch_kincaid_per_sentence'] = max(flesch_kincaid_list)\n            df.loc[row, 'min_flesch_kincaid_per_sentence'] = min(flesch_kincaid_list)            \n            \n        \n\n    \n    df['percentage_stopwords'] = round(100*(df['word_count'] - df['word_count_ns'])/df['word_count'], 2)\n    \n\n\n    ### add more features?\n    # percentage of verbs, nouns...?\n    # max ADJ per sentence...\n    # avg, max, min words\n    # max/min flesch (ok)\n    # max/min other textstat features    \n    \n    return df\n    \n\n#### Uncomment these 3 lines below to generate the features\n# train = feat_eng(train_original)\n# train.to_csv('./train_features.csv', index=False) # export train features to csv file\n# train.shape\n\n#### ~39 min to generate train features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reading train features created in previous run (saves ~40 min)\ntrain = pd.read_csv('../input/train-features-40/train_features_40.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train\n# train[['min_flesch_per_sentence', 'min_flesch_kincaid_per_sentence']]\n# train[['punctuation_count', 'n_PUNCT']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = [i for i in train.columns if i not in ['excerpt', 'target']]\nlen(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby(['text_standard'])['target'].count() # imbalance?? still need to check these categories...","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGB Regressor","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train[features], train['target'], test_size=0.1, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## check data types\n# train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', random_state = 42)\n\nxgb_reg.fit(X_train, y_train)\n\npreds = xgb_reg.predict(X_test)\n\nrmse = np.sqrt(mse(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nxgb_reg.fit(train[features], train['target'])\n\ntest = feat_eng(test_df)\ntest_pred = xgb_reg.predict(test[features])\ntest_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape, test_pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import plot_importance\nfrom matplotlib import pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = (22, 16)\nplot_importance(xgb_reg)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_reg.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"/kaggle/working/submission.csv\", index=False)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}