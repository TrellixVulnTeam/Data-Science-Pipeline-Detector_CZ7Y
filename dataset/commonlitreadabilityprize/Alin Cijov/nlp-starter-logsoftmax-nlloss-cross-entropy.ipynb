{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np\nimport string\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\nstopwords = set(STOPWORDS)\ndata =\"\"\"When forty winters shall besiege thy brow,\nAnd dig deep trenches in thy beauty's field,\nThy youth's proud livery so gazed on now,\nWill be a totter'd weed of small worth held:\nThen being asked, where all thy beauty lies,\nWhere all the treasure of thy lusty days;\nTo say, within thine own deep sunken eyes,\nWere an all-eating shame, and thriftless praise.\nHow much more praise deserv'd thy beauty's use,\nIf thou couldst answer 'This fair child of mine\nShall sum my count, and make my old excuse,'\nProving his beauty by succession thine!\nThis were to be new made when thou art old,\nAnd see thy blood warm when thou feel'st it cold.\"\"\"\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(data)\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(24, 24))\naxes[0].imshow(wordcloud)\naxes[0].axis('off')\naxes[1].imshow(wordcloud)\naxes[1].axis('off')\naxes[2].imshow(wordcloud)\naxes[2].axis('off')\nfig.tight_layout()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"definition\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Definition\n        <a class=\"anchor-link\" href=\"#definition\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"Word embeddings are dense vectors of real numbers, one per word in your vocabulary. In NLP, it is almost always the case that your features are words! But how should you represent a word in a computer? You could store its ascii character representation, but that only tells you what the word is, it doesn’t say much about what it means (you might be able to derive its part of speech from its affixes, or properties from its capitalization, but not much). Even more, in what sense could you combine these representations? We often want dense outputs from our neural networks, where the inputs are |V| dimensional, where V is our vocabulary, but often the outputs are only a few dimensional (if we are only predicting a handful of labels, for instance)","metadata":{}},{"cell_type":"markdown","source":"<h1 id=\"dataset\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"sentences = \"\"\"When forty winters shall besiege thy brow,\nAnd dig deep trenches in thy beauty's field,\nThy youth's proud livery so gazed on now,\nWill be a totter'd weed of small worth held:\nThen being asked, where all thy beauty lies,\nWhere all the treasure of thy lusty days;\nTo say, within thine own deep sunken eyes,\nWere an all-eating shame, and thriftless praise.\nHow much more praise deserv'd thy beauty's use,\nIf thou couldst answer 'This fair child of mine\nShall sum my count, and make my old excuse,'\nProving his beauty by succession thine!\nThis were to be new made when thou art old,\nAnd see thy blood warm when thou feel'st it cold.\"\"\"","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare data","metadata":{}},{"cell_type":"code","source":"# remove special characters\nsentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n\n# remove 1 letter words\nsentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n\n# lower all characters\nsentences = sentences.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare the dictionaries","metadata":{}},{"cell_type":"code","source":"# list of words\nwords = sentences.split()\n\n# get vocabulary (set(words)) - unique words\nvocab = set(words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word to id\nword_to_ix = {w: i for i, w in enumerate(vocab)}\n\n# id to word\nix_to_word = {i: w for i, w in enumerate(vocab)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_to_ix['the']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ix_to_word[83]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"prepare\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Embeddings and Trigrams\n        <a class=\"anchor-link\" href=\"#prepare\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"### Prepare the embeddings matrix","metadata":{}},{"cell_type":"code","source":"vocab_size = len(set(vocab))\nembedding_dimension = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#embedding matrix = vocabulary_size x embedding_dimension\nembeddings =  np.random.random_sample((vocab_size, embedding_dimension))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Input/Targets","metadata":{}},{"cell_type":"code","source":"trigrams = [([words[i], words[i+1]], words[i+2]) for i in range(len(words) - 2)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inputs = ['word1', 'word2']\n# targets = 'target'\n# ( ['word1', 'word2'], 'target')\ntrigrams[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"implementation\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Implementation\n        <a class=\"anchor-link\" href=\"#implementation\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"### Activation functions","metadata":{}},{"cell_type":"code","source":"def relu(x):\n    return x * (x > 0)\n\ndef drelu(x):\n    return 1. * (x > 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return np.log(e_x / e_x.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss function","metadata":{}},{"cell_type":"code","source":"def NLLLoss(logs, targets):\n    out = logs[range(len(targets)), targets]\n    return -out.sum()/len(out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear models","metadata":{}},{"cell_type":"code","source":"def linear1(x, theta):\n    w1, w2 = theta\n    return np.dot(x, w1.T)\n\ndef linear2(o, theta):\n    w1, w2 = theta\n    return np.dot(o, w2.T)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Forward function","metadata":{}},{"cell_type":"code","source":"def forward(x, theta):\n    m = embeddings[x].reshape(1, -1)\n    n = linear1(m, theta)\n    o = relu(n)\n    p = linear2(o, theta)\n    q = log_softmax(p)\n    \n    params = m, n, o, p, q\n    return(params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Log-Softmax Cross-Entropy\n\nCross Entropy = NLLoss(log_softmax(p))\n\nWe could also say that Cross Entropy is NLLoss + log_softmax","metadata":{}},{"cell_type":"code","source":"def log_softmax_crossentropy_with_logits(logits,target):\n\n    out = np.zeros_like(logits)\n    out[np.arange(len(logits)),target] = 1\n    \n    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n    \n    return (- out + softmax) / logits.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Backward function","metadata":{}},{"cell_type":"code","source":"def backward(x, y, theta, params):\n    m, n, o, p, q = params\n    w1, w2 = theta\n    \n    dlog = log_softmax_crossentropy_with_logits(p, y)\n    drelu = relu(n)\n    \n    # dw2 = dlog * o\n    do = np.dot(dlog,w2)\n    dw2 = np.dot(dlog.T, o)\n    \n    # dw1 = do * drelu * m\n    dn = do * drelu\n    dw1 = np.dot(dn.T, m)\n    \n    return dw1, dw2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optimizer function","metadata":{}},{"cell_type":"code","source":"def optimize(theta, grads, lr=0.03):\n    w1, w2 = theta\n    dw1, dw2 = grads\n    \n    w1 -= dw1 * lr\n    w2 -= dw2 * lr\n    \n    return theta","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"training\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"theta = np.random.uniform(-1,1, (128, 20)), np.random.uniform(-1,1, (85, 128))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context, target  = trigrams[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert context('word1', 'word2') into [#1, #2] word to ix\ncontext_ix = [word_to_ix[c] for c in context]\n\n# convert to numpy array and convert from (2,) to (1,2)\ncontext_ix = np.array(context_ix)\ncontext_ix = context_ix.reshape(1, 2)\n\n# forward propagation (predict)\nparams = forward(context_ix, theta)\n\n# get the looses and append to epoch losses\nloss = NLLLoss(params[-1], [word_to_ix[target]])\n\n# get the gradients from back propagation\ngrads = backward(context_ix, [word_to_ix[target]], theta, params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params[-2].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[word_to_ix[target]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_softmax_crossentropy_with_logits(params[-2], [word_to_ix[target]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses = {}\n\nfor epoch in range(1000):\n\n    epoch_losses = []\n    \n    for context, target in trigrams:\n        \n        # convert context('word1', 'word2') into [#1, #2] word to ix\n        context_ix = [word_to_ix[c] for c in context]\n\n        # convert to numpy array and convert from (2,) to (1,2)\n        context_ix = np.array(context_ix)\n        context_ix = context_ix.reshape(1, 2)\n\n        # forward propagation (predict)\n        params = forward(context_ix, theta)\n\n        # get the looses and append to epoch losses\n        loss = NLLLoss(params[-1], [word_to_ix[target]])\n        epoch_losses.append(loss)\n\n        # get the gradients from back propagation\n        grads = backward(context_ix, [word_to_ix[target]], theta, params)\n\n        # optimize the weights Stochastic gradient descent (SGD)\n        theta = optimize(theta, grads)\n        \n    losses[epoch] = epoch_losses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"analyze\" style=\"color:#1f9f88; background:white; border:0.5px dotted;\"> \n    <center>Analyze\n        <a class=\"anchor-link\" href=\"#analyze\" target=\"_self\">¶</a>\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"### Plot losses","metadata":{}},{"cell_type":"code","source":"ix = np.arange(0,35)\n\nfig = plt.figure()\nfig.suptitle('Epoch/Losses', fontsize=20)\nplt.plot(ix,[losses[i][0] for i in ix])\nplt.xlabel('Epochs', fontsize=12)\nplt.ylabel('Losses', fontsize=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict function","metadata":{}},{"cell_type":"code","source":"def predict(words):\n    context_ix = [word_to_ix[c] for c in words]\n    params = forward(context_ix, theta)\n    word = ix_to_word[np.argmax(params[-1])]\n    \n    return word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test prediction","metadata":{}},{"cell_type":"code","source":"# 'And dig deep trenches in thy beauty's field'\n# expected answer 'deep'\npredict([\"and\", \"dig\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy():\n    wrong = 0\n\n    for context, target in trigrams:\n        if(predict(context) != target):\n            wrong += 1\n            \n    return (1 - (wrong / len(trigrams)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy of 96.49%, please don't forget to up-vote if you enjoy it :)\n\nIf you want to learn more about this integration, read more on this pytorch tutorial:<br>\nhttps://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html","metadata":{}}]}