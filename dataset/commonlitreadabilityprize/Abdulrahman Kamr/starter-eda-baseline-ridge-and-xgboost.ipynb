{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\n\n\nfrom wordcloud import WordCloud\n\n\nfrom collections import Counter\nimport os\nimport numpy as np\nimport re\nimport string\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.util import ngrams    \n\nimport html\nimport unicodedata\n\nstop_words = stopwords.words('english')\n%config InlineBackend.figure_format = 'retina'\n","metadata":{"id":"_PWCUpvfaJUT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wordcloud(text,ngram=1):\n    wordcloud = WordCloud(width=1400, \n                            height=800,\n                            random_state=2021,\n                            background_color='black',\n                            )\n    if ngram ==1:\n        wordc = wordcloud.generate(' '.join(text))\n    else:\n        wordc = wordcloud.generate_from_frequencies(text)\n    plt.figure(figsize=(12,6), facecolor='k')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n      \n\ndef get_n_grans_count(text, n_grams, min_freq):\n    output = {}\n    tokens = nltk.word_tokenize(text)\n\n    #Create the n_gram\n    if n_grams == 2:\n        gs = nltk.bigrams(tokens)\n        \n    elif n_grams == 3:\n        gs = nltk.trigrams(tokens)\n\n    else:\n        return 'Only 2_grams and 3_grams are supported'\n    \n    # compute frequency distribution for all the bigrams in the text by threshold with min_freq\n    fdist = nltk.FreqDist(gs)\n    for k,v in fdist.items():\n        if v > min_freq:\n            index = ' '.join(k)\n            output[index] = v\n    \n    return output\n    \ndef remove_special_chars(text):\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x1))\n\n\ndef remove_non_ascii(text):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n\n\ndef to_lowercase(text):\n    return text.lower()\n\n\n\ndef remove_punctuation(text):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\n\ndef replace_numbers(text):\n    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n    return re.sub(r'\\d+', '', text)\n\n\ndef remove_whitespaces(text):\n    return text.strip()\n\n\ndef remove_stopwords(words, stop_words):\n    \"\"\"\n    :param words:\n    :type words:\n    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n    or\n    from spacy.lang.en.stop_words import STOP_WORDS\n    :type stop_words:\n    :return:\n    :rtype:\n    \"\"\"\n    return [word for word in words if word not in stop_words]\n\n\ndef stem_words(words):\n    \"\"\"Stem words in text\"\"\"\n    stemmer = PorterStemmer()\n    return [stemmer.stem(word) for word in words]\n\ndef lemmatize_words(words):\n    \"\"\"Lemmatize words in text, and by defult lemmatize nouns\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return [lemmatizer.lemmatize(word) for word in words]\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in text\"\"\"\n\n    lemmatizer = WordNetLemmatizer()\n    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n\ndef text2words(text):\n    return word_tokenize(text)\n\ndef normalize_text( text):\n    text = remove_special_chars(text)\n    text = remove_non_ascii(text)\n    text = remove_punctuation(text)\n    text = to_lowercase(text)\n    text = replace_numbers(text)\n    words = text2words(text)\n    words = remove_stopwords(words, stop_words)\n    #words = stem_words(words)# Either stem or lemmatize\n    words = lemmatize_words(words)\n    words = lemmatize_verbs(words)\n\n    return ''.join(words)","metadata":{"id":"_QPKCb46k0lY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Goal:building algorithms to rate the complexity of reading passages for grade 3-12 classroom use**","metadata":{"id":"xWK7pcZTb5fZ"}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsubmission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n\ntrain_data.head()","metadata":{"id":"YDCwxxg6aLuC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{"id":"2CJyr2BGIsO4"}},{"cell_type":"code","source":"train_data.describe()","metadata":{"id":"QUCubcG4aXEC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# showing if any feature has at least one null value\ntrain_data.isnull().any()","metadata":{"id":"EgArnaKEHg5y","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count null values\ntrain_data.isnull().sum()","metadata":{"id":"ydcBzfgAaXGR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target and Standard Error Distributions","metadata":{"id":"3WcJLtv7M18j"}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2,figsize=(12,7))\nsns.histplot(train_data['target'], kde= True, ax=ax[0])\nsns.histplot(train_data['standard_error'], kde= True, ax=ax[1])\nax[0].set_title(\"Target Distribution\")\nax[1].set_title(\"Standard Error Distribution\")\nplt.show();","metadata":{"id":"RAAP-s3lKZfL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## license","metadata":{"id":"20I8FAxNM8pS"}},{"cell_type":"code","source":"train_data['license'].value_counts()","metadata":{"id":"3qCKRwAGJewM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.countplot(data= train_data, y= 'license')\nplt.title('License Distribution')\nplt.show();","metadata":{"id":"UxhUm5pWJ0TZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Url legal","metadata":{"id":"3FeNgMqjNJRI"}},{"cell_type":"code","source":"# showing the shaper of url's\nurls = train_data['url_legal'].dropna()\nurls = [url for url in urls]\nurls[:5]","metadata":{"id":"erQGSvF4NKXt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract all url's\nurl_list = train_data['url_legal'].dropna().apply(lambda x : re.findall('https?://([A-Za-z_0-9.-]+).*',x)[0])\nurl_list = [url for url in url_list]\nurl_list[:10]","metadata":{"id":"ZV3ayge6TO52","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count url's and sort them descending order \nurls_counts = Counter(url_list)\nurls_counts_sorted = sorted(urls_counts.items(), key=lambda pair: pair[1], reverse=True)\nurls_counts_df = pd.DataFrame(urls_counts_sorted, columns=['sites', 'counts'])\nurls_counts_df","metadata":{"id":"uCyKEJagVwdc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.barplot(data= urls_counts_df, x= 'counts', y= 'sites')\nplt.title('Unique Sites count')\nplt.show();","metadata":{"id":"IEO2Cd7kaHCC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## excerpt","metadata":{"id":"pwTuiq62kabY"}},{"cell_type":"markdown","source":"**The original text**","metadata":{"id":"TAmtVXJ5nWpd"}},{"cell_type":"code","source":"train_data['excerpt'][0]","metadata":{"id":"0stILCT9mXtY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The cleaned text**","metadata":{"id":"vwWFAHoanZuw"}},{"cell_type":"code","source":"normalize_text(train_data['excerpt'][0])","metadata":{"id":"U9scvvFRa2pf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Adding cleat text in the data frame**","metadata":{"id":"12fJSZTJnrhv"}},{"cell_type":"code","source":"train_data['clean_text'] = [normalize_text(sent) for sent in train_data['excerpt']]\ntrain_data.head()","metadata":{"id":"Zb3sv1i8kdjr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Also we should make text preprocessing on text data\ntest_data['excerpt'] = [normalize_text(sent) for sent in test_data['excerpt']]","metadata":{"id":"tKJNhLbg0CF0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Frequent words**","metadata":{"id":"dc_NlraFoH8z"}},{"cell_type":"code","source":"# make all clear sentence as a huge text, then tokenize it\nwords_list = text2words(''.join(sents for sents in train_data['clean_text']))\nwords_list[:10]\n","metadata":{"id":"vs2ZxXEJkdlw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of words we have\nlen(words_list)","metadata":{"id":"uWpVFiGakdoG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# frequent of the most 30 words\nwords_list_freq = Counter(words_list)\nwords_list_freq_sorted = sorted(words_list_freq.items(), key=lambda pair: pair[1], reverse=True)\n\nwords_list_freq_sorted_df = pd.DataFrame(words_list_freq_sorted, columns=['words', 'counts'])[:30]\nwords_list_freq_sorted_df.head()","metadata":{"id":"cEVXM4oCtDEo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.barplot(data= words_list_freq_sorted_df, y= 'words', x= 'counts')\nplt.title('Top 30 frequent words')\nplt.show();","metadata":{"id":"ouFy0y-wgZdt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Word Cloud for all words**","metadata":{"id":"_CLY6YJYvftd"}},{"cell_type":"code","source":"wordcloud(train_data['excerpt'])","metadata":{"id":"s-2fYhsMuytt","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bigrams**","metadata":{"id":"WdfbixvXxnV-"}},{"cell_type":"code","source":"text= ' '.join(setns for setns in train_data['clean_text'])","metadata":{"id":"qU8OOl-Ivixy","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"two_grams = get_n_grans_count(text, n_grams=2, min_freq=10)\ntwo_grams_df = pd.DataFrame(two_grams.items(), columns= ['two_grams', 'counts']).sort_values(by='counts',ascending=False)\ntwo_grams_df.head()","metadata":{"id":"VY1_P7rbvi2c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.barplot(data= two_grams_df[:30], y= 'two_grams', x= 'counts')\nplt.title('Top 30 frequent bigram')\nplt.show();","metadata":{"id":"6tUPyawAysvw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"two_grams_wordcloud = {w.replace(' ','_'): c for w,c in two_grams.items()}\nwordcloud(two_grams_wordcloud,ngram=2)","metadata":{"id":"mz3_mOz7ysyz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modeling","metadata":{}},{"cell_type":"markdown","source":"Before going further it is important that we split the data into training and validation sets. We can do it using train_test_split from the model_selection module of scikit-learn.","metadata":{}},{"cell_type":"code","source":"X = train_data['clean_text']\ny = train_data['target']\nX_train, X_valid, y_train, y_valid =  train_test_split(X, y, \n                                                  random_state=42, \n                                                  test_size=0.3, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (X_train.shape)\nprint (X_valid.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Our first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression.**","metadata":{}},{"cell_type":"code","source":"# Make an Sklearn pipeline for this Ridge Regression\nridge = Ridge(fit_intercept=True, normalize=False)\n\nridge_pipline = make_pipeline(\n    TfidfVectorizer(binary= True, min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)\n    , ridge)\n\n# training\nridge_pipline.fit(X_train, y_train)\n\n# Evaluation\ny_pred = ridge_pipline.predict(X_valid)\nmse_loss = mean_squared_error(y_pred, y_valid)\n\nprint(f\"MSE Loss using Ridge and TfIdfVectorizer: {mse_loss}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make an Sklearn pipeline for this xgboost Regression\nxgboost = xgb.XGBRegressor(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\n\nxgboost_pipline = make_pipeline(\n    TfidfVectorizer(binary= True, min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)\n    , xgboost)\n\n# training\nxgboost_pipline.fit(X_train, y_train)\n\n# Evaluation\ny_pred = xgboost_pipline.predict(X_valid)\nmse_loss = mean_squared_error(y_pred, y_valid)\n\nprint(f\"MSE Loss using xgboost and TfIdfVectorizer: {mse_loss}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Seems like no luck with XGBoost!**","metadata":{}},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"test_text = test_data['excerpt']\ntest_pred = ridge_pipline.predict(test_text)\n\nsubmission = pd.DataFrame()\nsubmission['id'] = test_data['id']\nsubmission['target'] = test_pred\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}