{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center> <h1>Exploratory Data Analysis for CommonLit Reading Prize</h1> </center>\n\n| ![](https://cdn.pixabay.com/photo/2019/03/31/07/25/kid-4092599_960_720.jpg) |\n|:--:|\n| Photo by [Vlad Vasnetsov](https://pixabay.com/users/vladvictoria-9785604/) on [Pixabay](https://pixabay.com/)|\n\n## *What is Exploratory Data Analysis (EDA)*?\nEDA, according to [Wickham & Grolemund](https://r4ds.had.co.nz/exploratory-data-analysis.html), is the process of getting to know our data primarily through simple visualizations before fitting a model.\n\n## _Why is it done_?\nFirst, if we attempt to fit models without inspecting our data, our code will throw an error (unless we're using a toy dataset but what fun is that?). \n\nSecond, in the unlikely event the code did run, we would fail to identify: \n* unbalanced data sets\n* missing values\n* [collinearity](https://medium.com/future-vision/collinearity-what-it-means-why-its-bad-and-how-does-it-affect-other-models-94e1db984168)\n\nand more which would cause our model to produce inferior results. \n\n## _How is it done in Python?_\nWhile EDA [is more of an attitude than a definitive list of steps](https://r4ds.had.co.nz/exploratory-data-analysis.html), if you're looking for a list of steps to serve as a guide until you develop your own intuition, a great place to look is AurÃ©lien Geron's fantastic checklists for all stages in a Machine Learning project found in [Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow](https://github.com/ageron/handson-ml/blob/master/ml-project-checklist.md).\n\n\n### Step 1: Frame the Problem\nAccording to the [overview](https://www.kaggle.com/c/commonlitreadabilityprize/overview/description), the task is to answer the following, \n\n> _Can machine learning identify the appropriate reading level of a passage of text, and help inspire learning?_\n\nIf you want to be pedantic, this question has two parts: \n1. Assess the complexity of a passage of text\n2. Measure the impact machine learning has on motivating learners\n\nHowever, the rest of the overview makes clear this competition is concerned with the former rather than the latter.\n\n### Step 2: Get the Data\nLucky for us, the organizers have provided us with the data we need to start with so we can move on to the next step. \n\n### Step 3: Explore the Data to Gain Insights\nLet's start by loading some useful libraries as well as the data. ","metadata":{}},{"cell_type":"code","source":"pip install textstat","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-20T12:14:58.794002Z","iopub.execute_input":"2021-08-20T12:14:58.794762Z","iopub.status.idle":"2021-08-20T12:15:07.960389Z","shell.execute_reply.started":"2021-08-20T12:14:58.794628Z","shell.execute_reply":"2021-08-20T12:15:07.959622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#For data manipulation\nimport numpy as np\nimport pandas as pd\nimport textstat\n\n#For visualization\nimport matplotlib.pyplot as plt\nimport missingno as msno \nimport seaborn as sns \n\nRANDOM_STATE = 42\n\ndf = pd.read_csv('../input/commonlitreadabilityprize/train.csv')","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-08-20T12:15:07.962439Z","iopub.execute_input":"2021-08-20T12:15:07.963147Z","iopub.status.idle":"2021-08-20T12:15:08.999111Z","shell.execute_reply.started":"2021-08-20T12:15:07.963096Z","shell.execute_reply":"2021-08-20T12:15:08.998056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, a definitive set of steps does not exist for this stage but I like to start by asking and answering the [Five W's and 1 H](https://www.workfront.com/blog/project-management-101-the-5-ws-and-1-h-that-should-be-asked-of-every-project)\n\n* Who\n* What\n* When\n* Where \n* Why \n* How\n\nSo, in no particular order, let's start with: \n#### _Where did the data come from?_\nFrom the [overview](https://www.kaggle.com/c/commonlitreadabilityprize/overview/description) we can infer that data was produced by [CommonLit](https://www.commonlit.org/en), a non-profit EdTech company focused on promoting literacy, in conjunction with the [Applied Linguistics and ESL Department at Georgia State University](https://alsl.gsu.edu/). \n\n#### _How much data do we have?_","metadata":{"execution":{"iopub.status.busy":"2021-06-13T12:36:05.215557Z","iopub.execute_input":"2021-06-13T12:36:05.216147Z","iopub.status.idle":"2021-06-13T12:36:05.224218Z","shell.execute_reply.started":"2021-06-13T12:36:05.216097Z","shell.execute_reply":"2021-06-13T12:36:05.223078Z"}}},{"cell_type":"code","source":"df.shape","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-08-20T12:15:09.00052Z","iopub.execute_input":"2021-08-20T12:15:09.00078Z","iopub.status.idle":"2021-08-20T12:15:09.008936Z","shell.execute_reply.started":"2021-08-20T12:15:09.000755Z","shell.execute_reply":"2021-08-20T12:15:09.0079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK, so we have six features and 2,834 observations in the training set. \n\n>_What about the test set?_\n\nThe public test set has seven observations while the [hidden test set has ~2,000](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/236335#1292356).\n\n#### _What type of data do we have?_","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.010573Z","iopub.execute_input":"2021-08-20T12:15:09.010843Z","iopub.status.idle":"2021-08-20T12:15:09.041468Z","shell.execute_reply.started":"2021-08-20T12:15:09.010818Z","shell.execute_reply":"2021-08-20T12:15:09.040457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As expected, given the scope of the challenge, we have a mix of strings and floats. \n\nLet's see some examples: ","metadata":{}},{"cell_type":"code","source":"df.sample(10, random_state=RANDOM_STATE)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.042634Z","iopub.execute_input":"2021-08-20T12:15:09.042939Z","iopub.status.idle":"2021-08-20T12:15:09.064262Z","shell.execute_reply.started":"2021-08-20T12:15:09.042909Z","shell.execute_reply":"2021-08-20T12:15:09.063248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Excellent! Next question that comes to mind is: \n\n#### _What are the key features?_ \n\nWe're fortunate that this dataset has clear variable names so we can surmise: \n\n* ```id```: unique identifier for each text sample\n* ```excerpt```: text passage to be used for modeling\n* ```target```: difficulty rating of the text\n* ```standard_error```: variance in scoring amongst the raters\n\n> _What about_ ```url_legal``` _and_ ```license```_?_ \n\nSince those variables [won't be available at test time](https://www.kaggle.com/c/commonlitreadabilityprize/data), they shouldn't be used for training and will subsequently be dropped. \n\n#### _Who are the raters?_\nAccording to Dr. Crossley, [teachers from the target grade ranges rated the samples, with a majority of teachers drawn from sixth through 10th grade](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240423).\n\n#### _How were the teachers selected?_\nUnfortunately, that information hasn't been provided yet but [watch this space](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240423#1347995). \n\n#### _How was the target computed?_\nTeachers were given a pair of passages and asked, [of these two, which is easier for a child to understand?](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240423#1322910) Target scores were then computed based on the responses of the teachers. \n\n> _OK, but how were the scores computed?_\n\nThis one's complicated so, in the spirit of T.S. Eliot who wrote,[\"Immature poets imitate; mature poets steal\"](https://www.uvu.edu/arts/applause/posts/stealing.html#:~:text=%E2%80%9CGood%20artists%20borrow%2C%20great%20artists%20steal.%E2%80%9D&text=Eliot's%20dictum%3A%20%E2%80%9CImmature%20poets%20imitate,or%20at%20least%20something%20different.), I'm going to direct you to [Shahebaz's fabulous notebook](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240886) which provides an in depth explanation and/or read this [paper](https://www.jstatsoft.org/article/view/v012i01/v12i01.pdf) which was recommended by the competition host in this [discussion](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/236671#1296867).\n\n#### _How many missing values are there?_","metadata":{}},{"cell_type":"code","source":"df = df.drop(columns=['url_legal', 'license'])\nmsno.bar(df);","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.065836Z","iopub.execute_input":"2021-08-20T12:15:09.066226Z","iopub.status.idle":"2021-08-20T12:15:09.64873Z","shell.execute_reply.started":"2021-08-20T12:15:09.066186Z","shell.execute_reply":"2021-08-20T12:15:09.647628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"EXCELLENT!!! We are not missing any values.\n\n#### _What is the distribution of our target?_","metadata":{}},{"cell_type":"code","source":"df.target.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.6503Z","iopub.execute_input":"2021-08-20T12:15:09.650705Z","iopub.status.idle":"2021-08-20T12:15:09.665396Z","shell.execute_reply.started":"2021-08-20T12:15:09.650663Z","shell.execute_reply":"2021-08-20T12:15:09.664576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize that to make it easier to comprehend.","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.668812Z","iopub.execute_input":"2021-08-20T12:15:09.66935Z","iopub.status.idle":"2021-08-20T12:15:09.683106Z","shell.execute_reply.started":"2021-08-20T12:15:09.669306Z","shell.execute_reply":"2021-08-20T12:15:09.681984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20, 10)})\nsns.set_theme()\ng = sns.ecdfplot(data=df, x=\"target\")\ng.set_xticks(range(-4, 3));","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.685263Z","iopub.execute_input":"2021-08-20T12:15:09.685672Z","iopub.status.idle":"2021-08-20T12:15:09.924043Z","shell.execute_reply.started":"2021-08-20T12:15:09.685632Z","shell.execute_reply":"2021-08-20T12:15:09.923021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK, our target appears to be normally distributed. \n\n> _Wait! Why didn't you use a histogram?_\n\nHistograms introduce additional bias into our analysis and are, therefore, best avoided when possible. Click [here](https://towardsdatascience.com/6-reasons-why-you-should-stop-using-histograms-and-which-plot-you-should-use-instead-31f937a0a81c) to learn more. \n\n#### _What does our target mean?_\n\nThat's a really good question. \n\nAs previously mentioned, the raters were simply asked to [identify which of the two text passages was easier for students to understand](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240423#1322910) and then the scores were tabulated using the method outlined [here](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240886). \n\n> _But what do those scores actually mean?_ \n\nð¤ Again, that's a really good question. \n\nLuckily for us, [some kind souls have already answered this question](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/236402#1295630) so we know that the lower the score, the more difficult the text. Inversely, the higher the score, the easier the text. \n\n> _So that means the higher the target, the lower the grade?_\n\n~~You would think so but [we're still waiting for confirmation](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240423#1360081).~~  \n\nYes it does! So let's rescale our target using the method outlined [here](https://stackoverflow.com/a/929107/4691538) to make it a little easier to read.\n\nAll we have to do is the following: \n\n`OriginalRange = (OriginalMax - OriginalMin)`  \n`ScaledRange = (ScaledMax - ScaledMin)`  \n`ScaledValue = (((OriginalScore - OriginalMin) * ScaledRange) / OriginalRange) + ScaledMin`  \n\nSo what's going on here?\n\nFirst we define the original highest and lowest scores in `df.target`.\n\n**Key Point:** the higher the value in `df.target` the easier the text which means:","metadata":{}},{"cell_type":"code","source":"OriginalMax = df.target.min()\nOriginalMin = df.target.max()\n\nOriginalRange = (OriginalMax - OriginalMin)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.925589Z","iopub.execute_input":"2021-08-20T12:15:09.926127Z","iopub.status.idle":"2021-08-20T12:15:09.93234Z","shell.execute_reply.started":"2021-08-20T12:15:09.92608Z","shell.execute_reply":"2021-08-20T12:15:09.931112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now for the easy part: defining our scaled values.","metadata":{}},{"cell_type":"code","source":"ScaledMax = 12\nScaledMin = 3\nScaledRange = (ScaledMax - ScaledMin)","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.935013Z","iopub.execute_input":"2021-08-20T12:15:09.935459Z","iopub.status.idle":"2021-08-20T12:15:09.943143Z","shell.execute_reply.started":"2021-08-20T12:15:09.935419Z","shell.execute_reply":"2021-08-20T12:15:09.942355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now for the magic: we can pass the highest score from `df.target` (i.e., the easiest passage) and we _should_ get `3` back.","metadata":{}},{"cell_type":"code","source":"OriginalScore = df.target.max()\n\nScaledValue = (((OriginalScore - OriginalMin) * ScaledRange) / OriginalRange) + ScaledMin\n\nprint(f'The highest target score scaled is a {ScaledValue}.')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.944436Z","iopub.execute_input":"2021-08-20T12:15:09.945149Z","iopub.status.idle":"2021-08-20T12:15:09.955692Z","shell.execute_reply.started":"2021-08-20T12:15:09.945107Z","shell.execute_reply":"2021-08-20T12:15:09.954601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Excellent! Now let's try the lowest score in `df.target` (i.e. the hardest passage) and we _should_ get `12` back.","metadata":{"execution":{"iopub.status.busy":"2021-07-06T17:09:47.996592Z","iopub.execute_input":"2021-07-06T17:09:47.997119Z","iopub.status.idle":"2021-07-06T17:09:48.002682Z","shell.execute_reply.started":"2021-07-06T17:09:47.997085Z","shell.execute_reply":"2021-07-06T17:09:48.001956Z"}}},{"cell_type":"code","source":"OriginalScore = df.target.min()\n\nScaledValue = (((OriginalScore - OriginalMin) * ScaledRange) / OriginalRange) + ScaledMin\n\nprint(f'The lowest target score scaled is a {ScaledValue}.')","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.956747Z","iopub.execute_input":"2021-08-20T12:15:09.957097Z","iopub.status.idle":"2021-08-20T12:15:09.967029Z","shell.execute_reply.started":"2021-08-20T12:15:09.957069Z","shell.execute_reply":"2021-08-20T12:15:09.965934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Excellent! Now for the **REALLY** fun part: scaling the `target` for all observations.\n\nHow do we do that?\n1. create a function to convert the target to a grade\n2. use `apply()` to apply the function to every observation","metadata":{}},{"cell_type":"code","source":"OriginalMax = df.target.min()\nOriginalMin = df.target.max()\nOriginalRange = (OriginalMax - OriginalMin)\n\n\nScaledMax = 12\nScaledMin = 3\nScaledRange = (ScaledMax - ScaledMin)\n\n\ndef rescale_target(OriginalScore):\n    \"\"\"Converts original target to range 3 - 12\"\"\"\n    return (((OriginalScore - OriginalMin) * ScaledRange) / OriginalRange) + ScaledMin","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.96839Z","iopub.execute_input":"2021-08-20T12:15:09.968764Z","iopub.status.idle":"2021-08-20T12:15:09.978955Z","shell.execute_reply.started":"2021-08-20T12:15:09.968722Z","shell.execute_reply":"2021-08-20T12:15:09.977898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It should work but let's test it first: ","metadata":{}},{"cell_type":"code","source":"print(f\"The most difficult text's scaled score is {rescale_target(OriginalMax)} while the easiest's is {rescale_target(OriginalMin)}.\")","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.980281Z","iopub.execute_input":"2021-08-20T12:15:09.980636Z","iopub.status.idle":"2021-08-20T12:15:09.990349Z","shell.execute_reply.started":"2021-08-20T12:15:09.980607Z","shell.execute_reply":"2021-08-20T12:15:09.98918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perfect, now all we have to do is apply it to `target`  to create a `scaled_target` feature.","metadata":{}},{"cell_type":"code","source":"df.loc[:, 'scaled_target'] = df.target.apply(lambda target: rescale_target(target))\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:09.991834Z","iopub.execute_input":"2021-08-20T12:15:09.992275Z","iopub.status.idle":"2021-08-20T12:15:10.016042Z","shell.execute_reply.started":"2021-08-20T12:15:09.992232Z","shell.execute_reply":"2021-08-20T12:15:10.014964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Excellent!\n\nWhat does the distribution of our scaled target look like? ","metadata":{}},{"cell_type":"code","source":"sns.set(rc={'figure.figsize':(20, 10)})\nsns.set_theme()\ng = sns.ecdfplot(data=df, x=\"scaled_target\")\ng.set_xticks(range(3, 13));","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:10.01744Z","iopub.execute_input":"2021-08-20T12:15:10.017831Z","iopub.status.idle":"2021-08-20T12:15:10.274843Z","shell.execute_reply.started":"2021-08-20T12:15:10.017792Z","shell.execute_reply":"2021-08-20T12:15:10.273877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fascinating - approximately 20% of our observations are texts rated at the elementary (aka, up to 6th grade) and high school (i.e. 9th - 12th grade) levels meaning 60% of texts were rated at the middle school level. \n\nI'd be fascinated to know if the experts who selected the passages for training purposes expected this type of distribution; it's tempting to think that books for your children and high school students are obvious whereas books for middle school students have more gradations but that may not be the case. \n\nLet's come back to this later.\n\n#### _How many duplicates do we have?_","metadata":{}},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:10.276833Z","iopub.execute_input":"2021-08-20T12:15:10.277276Z","iopub.status.idle":"2021-08-20T12:15:10.294577Z","shell.execute_reply.started":"2021-08-20T12:15:10.277235Z","shell.execute_reply":"2021-08-20T12:15:10.293625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, so we do not have any duplicate rows, but do we have any duplicates in any of the columns?","metadata":{}},{"cell_type":"code","source":"df.apply(lambda x : x.duplicated()).sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:10.295879Z","iopub.execute_input":"2021-08-20T12:15:10.296275Z","iopub.status.idle":"2021-08-20T12:15:10.310178Z","shell.execute_reply.started":"2021-08-20T12:15:10.296234Z","shell.execute_reply":"2021-08-20T12:15:10.309112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK, so that means none of the columns contain duplicates.  \n\nLet's look at the `standard_error`.\n\n#### _What's the standard error?_\n\nThe hosts of the competition stated,\n\n> [\"[...] individual raters saw only a fraction of the excerpts, while every excerpt was seen by numerous raters.\"](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240423) \n\nWhat does that mean? Basically, if the standard error for a passage is greater than 0, then there was disagreement on the difficulty of that passage amongst the raters. \n\nAnd do we have any passages with a standard error of zero? ","metadata":{}},{"cell_type":"code","source":"sns.ecdfplot(data=df, x='standard_error');","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:10.311576Z","iopub.execute_input":"2021-08-20T12:15:10.311915Z","iopub.status.idle":"2021-08-20T12:15:10.558637Z","shell.execute_reply.started":"2021-08-20T12:15:10.311875Z","shell.execute_reply":"2021-08-20T12:15:10.557699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the chart above, we certainly do.\n\nBut how many do we have? ","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth',1000)\n\ndf[df.standard_error==0]","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:10.559996Z","iopub.execute_input":"2021-08-20T12:15:10.560287Z","iopub.status.idle":"2021-08-20T12:15:10.574018Z","shell.execute_reply.started":"2021-08-20T12:15:10.560257Z","shell.execute_reply":"2021-08-20T12:15:10.572979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Curious: the excerpt with a  ```standard_error``` of zero also has a ```target``` of zero. \n\n> _Surely this is an outlier, right?_\n\nNope. According to the [host](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/236403#1293029) this observation is valid. \n\n> _So what to make of an observation with a_ ```standard_error``` _of 0.0?_\n\nEssentially, the raters were in complete agreement when rating it compared to its competition. \n\n> _How can that happen?_\n\n1. The excerpt is **REALLY** easy/hard making the choice obvious for the raters\n2. The excerpt was paired with excerpts which were **REALLY** easy/hard, again, making the choice obvious for the raters\n3. Luck ð\n\nGiven that the ```target``` for this excerpt is 0.0, I was thinking it was either option 1 or 2 rather than dumb luck but now I'm not so sure. \n\nLooking at the distribution of the ```target``` below, we see that ~80% of all observations are **more difficult** than this passage; remember, [the higher the score, the easier the text](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/236402#1295630). ","metadata":{}},{"cell_type":"code","source":"observation = len(df.loc[df['target'] <0])/df.shape[0]\nchart = sns.ecdfplot(data=df, x='target')\nchart.annotate('Excerpt of Interest', \n               xy=(0, observation),\n               xytext=(-1.5, 0.8), \n               fontsize=20, \n               arrowprops=dict(arrowstyle=\"->\", color='b'));","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-20T12:15:10.575478Z","iopub.execute_input":"2021-08-20T12:15:10.575862Z","iopub.status.idle":"2021-08-20T12:15:10.978802Z","shell.execute_reply.started":"2021-08-20T12:15:10.57582Z","shell.execute_reply":"2021-08-20T12:15:10.97792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interesting... I would have thought for sure that this passage would have been either significantly more or less difficult than the other passages but I guess not. \n\nAgain, the competition host says this observation is correct (i.e., not a typo) so it just goes to show that things happen when there are human raters involved. \n\nLet's move on. \n\n#### _What is the relationship between_ ```target``` _and_ ```standard_error```_?_","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nsns.scatterplot(data=df, x='target', y='standard_error')\n# sns.kdeplot(x=x, y=y, levels=5, color=\"w\", linewidths=1)\nsns.kdeplot(data=df, x='target', y='standard_error', color='w', linewidths=2)\nax.set_title(f'Standard Error vs Target', size=20)\nax.set_xlabel('Target', size=15)\nax.set_ylabel('Standard Error', size=15)\nax.annotate('Excerpt of Interest', \n               xy=(0, 0),\n               xytext=(-1, 0.1), \n               fontsize=12, \n               arrowprops=dict(arrowstyle=\"->\", color='b'));\nplt.show;\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-20T12:15:10.983934Z","iopub.execute_input":"2021-08-20T12:15:10.984252Z","iopub.status.idle":"2021-08-20T12:15:14.102763Z","shell.execute_reply.started":"2021-08-20T12:15:10.984219Z","shell.execute_reply":"2021-08-20T12:15:14.102036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So what is going on here? \n\nEssentially, passages in the Goldilocks' Zone (i.e., neither too easy nor too difficult) have less error than those on the edges. \n\n> _\"Why are the easiest/most difficult passages harder to rate?\"_\n\nI honestly do not know. Based on my experience as a [language teacher, teacher trainer, and examiner](https://www.linkedin.com/in/evansimpson1/), I would think the passages on the ends would be the easiest to rate. \n\nSince they're not, maybe something else is going on. \n\nTo that end, let's look more closely at the target and at some excerpts. \n\n#### _\"What's the most difficult passage\"_\n","metadata":{}},{"cell_type":"code","source":"def get_values(quart, var):\n    \"\"\"Wrapper for the pandas quantile function\"\"\"\n    return df.loc[df.target==df.target.quantile(q=float(quart), interpolation='nearest'), var].values[0]\n\ndef get_scores_and_examples(percentile):\n    \"\"\"Wrapper for the wrapper :D\"\"\"\n    print(f'ID: {get_values(percentile,\"id\")}')\n    print(\" \")\n    print(f'Target score: {get_values(percentile,\"target\")}')\n    print(\" \")\n    print(f'Scaled target: {get_values(percentile, \"scaled_target\")}')\n    print(\" \")\n    print(f'Standard Error: {get_values(percentile,\"standard_error\")}')\n    print(\" \")\n    print('Excerpt:')\n    print(get_values(percentile, 'excerpt'))\n    \nget_scores_and_examples(0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-20T12:15:14.104114Z","iopub.execute_input":"2021-08-20T12:15:14.104531Z","iopub.status.idle":"2021-08-20T12:15:14.120744Z","shell.execute_reply.started":"2021-08-20T12:15:14.104489Z","shell.execute_reply":"2021-08-20T12:15:14.119279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _75th percentile?_","metadata":{}},{"cell_type":"code","source":"get_scores_and_examples(.25)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-20T12:15:14.122451Z","iopub.execute_input":"2021-08-20T12:15:14.122923Z","iopub.status.idle":"2021-08-20T12:15:14.139413Z","shell.execute_reply.started":"2021-08-20T12:15:14.122822Z","shell.execute_reply":"2021-08-20T12:15:14.138319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _Median?_","metadata":{"execution":{"iopub.status.busy":"2021-06-17T13:28:13.163217Z","iopub.execute_input":"2021-06-17T13:28:13.163591Z","iopub.status.idle":"2021-06-17T13:28:13.16808Z","shell.execute_reply.started":"2021-06-17T13:28:13.163568Z","shell.execute_reply":"2021-06-17T13:28:13.16719Z"}}},{"cell_type":"code","source":"get_scores_and_examples(.5)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-20T12:15:14.141116Z","iopub.execute_input":"2021-08-20T12:15:14.141509Z","iopub.status.idle":"2021-08-20T12:15:14.157306Z","shell.execute_reply.started":"2021-08-20T12:15:14.141468Z","shell.execute_reply":"2021-08-20T12:15:14.156351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### _25th Percentile_","metadata":{}},{"cell_type":"code","source":"get_scores_and_examples(.75)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-20T12:15:14.158313Z","iopub.execute_input":"2021-08-20T12:15:14.158711Z","iopub.status.idle":"2021-08-20T12:15:14.171977Z","shell.execute_reply.started":"2021-08-20T12:15:14.158674Z","shell.execute_reply":"2021-08-20T12:15:14.1711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Really? 75% of passages are **more difficult** than the one above? I would think the [schemata](https://evolllution.com/programming/teaching-and-learning/schemata-and-instructional-strategies/) (i.e., Nixon, FBI, CIA, IRS) necessary to understand the passage would rate it higher but, as always, it depends on what it was paired with. ","metadata":{}},{"cell_type":"markdown","source":"#### _What's the easiest passage?_","metadata":{}},{"cell_type":"code","source":"get_scores_and_examples(1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-20T12:15:14.172906Z","iopub.execute_input":"2021-08-20T12:15:14.173292Z","iopub.status.idle":"2021-08-20T12:15:14.195547Z","shell.execute_reply.started":"2021-08-20T12:15:14.173264Z","shell.execute_reply":"2021-08-20T12:15:14.19456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK, so a passage discussing dinosaur fossils found by paleontologists on different continents was tabulated as the ***easiest*** passage in the set?\n\nLet's look at all five of these passages together.","metadata":{}},{"cell_type":"code","source":"examples = ('4626100d8', '519ca97e9', '25ca8f498', 'bac396931', '9f8e4b6a8')\n\n(df[df.id.isin(examples)].sort_values(by='target')\n                         .style.set_properties(subset=['excerpt'],**{'text-align': 'left'}))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-20T12:15:14.196978Z","iopub.execute_input":"2021-08-20T12:15:14.197546Z","iopub.status.idle":"2021-08-20T12:15:14.244823Z","shell.execute_reply.started":"2021-08-20T12:15:14.197503Z","shell.execute_reply":"2021-08-20T12:15:14.24383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fascinating - of the five passages, three are non-fiction while two are fiction. Additionally, of the non-fiction, the most difficult is on engineering while the topics for the second easiest and easiest are history (politics?) and paleontology. \n\nIt would be a lot of fun to run a topic model [like this](https://www.kaggle.com/maartengr/topic-modeling-arxiv-abstract-with-bertopic/notebook) on this dataset to identify what, if any, impact the topic of the extract has on the difficulty of the text but I'll save that for a different notebook. \n\nInstead, I want to know more about the excerpts like:\n\n* How long are the excerpts?\n* What are some summary stats for the excerpts like:\n    * average sentence length  \n    *~~percentage of each excerpt which is named entities~~ [sigh, another time]\n* How do traditional readability algorithms correlate with our target?\n\n~~To that end, watch this space.~~  \nWithout further ado....","metadata":{}},{"cell_type":"markdown","source":"# How do traditional readability algorithms correlate with our target?\n","metadata":{}},{"cell_type":"markdown","source":"Huge hat tip to [Shoku-pan](https://www.kaggle.com/yhirakawa) and [Ruchi Bhatia](https://www.kaggle.com/ruchi798) for sharing their notebooks ([here](https://www.kaggle.com/yhirakawa/textstat-how-to-evaluate-readability) and [here](https://www.kaggle.com/ruchi798/commonlit-readability-prize-eda-baseline)) which outline how to use the [textstat](https://pypi.org/project/textstat/) package to compute some standard readability statistics. ","metadata":{}},{"cell_type":"code","source":"def textstat_stats(text):\n    \"\"\"Return readability metrics for a passage\"\"\"\n    n_syllable = textstat.syllable_count(text)\n    n_words = textstat.lexicon_count(text, removepunct=True)\n    n_sentences = textstat.sentence_count(text)\n    avg_words = n_words/n_sentences\n    avg_syllables = n_syllable/n_sentences\n    flesch_diff = textstat.flesch_reading_ease(text)\n    fleschgrade_diff = textstat.flesch_kincaid_grade(text)\n    gfog = textstat.gunning_fog(text)\n    ari = textstat.automated_readability_index(text)\n    cli = textstat.coleman_liau_index(text)\n    lwf = textstat.linsear_write_formula(text)\n    dcrs = textstat.dale_chall_readability_score(text)\n    \n    return n_syllable, n_words, n_sentences, avg_words, avg_syllables, flesch_diff, fleschgrade_diff, gfog, ari, cli, lwf, dcrs","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:14.246248Z","iopub.execute_input":"2021-08-20T12:15:14.246841Z","iopub.status.idle":"2021-08-20T12:15:14.254766Z","shell.execute_reply.started":"2021-08-20T12:15:14.246799Z","shell.execute_reply":"2021-08-20T12:15:14.254088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_stats(df):\n    df_stats = df.apply(lambda x: textstat_stats(x.excerpt), axis='columns', result_type='expand')\n    columns = ['n_syllable', 'n_words', 'n_sentences', 'avg_words', 'avg_syllables','flesch_diff', \n               'fleschgrade_diff', 'gfog', 'ari', 'cli', 'lwf', 'dcrs']\n    df_stats.columns = columns\n    return pd.merge(df, df_stats, left_index=True, right_index=True)\n\ndf_full = get_stats(df)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-20T12:15:14.256262Z","iopub.execute_input":"2021-08-20T12:15:14.256894Z","iopub.status.idle":"2021-08-20T12:15:21.101775Z","shell.execute_reply.started":"2021-08-20T12:15:14.256834Z","shell.execute_reply":"2021-08-20T12:15:21.100884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you're an eagle-eyed observer, you'll notice I omitted the [SMOG](https://en.wikipedia.org/wiki/SMOG) index from the function above. Why? Because it works best on passages of 30 sentences or longer and [not at all](http://www.aspiruslibrary.org/literacy/SMOG%20Readability%20Formula.pdf) for texts less than ten. \n\nAnd what proportion of our excerpts are shorter than ten sentences in length? ","metadata":{}},{"cell_type":"code","source":"sns.ecdfplot(data=df_full, x=\"n_sentences\");","metadata":{"execution":{"iopub.status.busy":"2021-08-20T12:15:21.102811Z","iopub.execute_input":"2021-08-20T12:15:21.103137Z","iopub.status.idle":"2021-08-20T12:15:21.331985Z","shell.execute_reply.started":"2021-08-20T12:15:21.103109Z","shell.execute_reply":"2021-08-20T12:15:21.331009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now on to the question of does our target correlate with traditional measures of readability.","metadata":{"execution":{"iopub.status.busy":"2021-07-09T14:34:47.373179Z","iopub.execute_input":"2021-07-09T14:34:47.373523Z","iopub.status.idle":"2021-07-09T14:34:47.381576Z","shell.execute_reply.started":"2021-07-09T14:34:47.373494Z","shell.execute_reply":"2021-07-09T14:34:47.380578Z"}}},{"cell_type":"code","source":"columns = ['target', 'scaled_target','n_syllable', 'n_words', \n           'n_sentences','avg_words', 'avg_syllables', 'flesch_diff',\n           'fleschgrade_diff', 'gfog', 'ari', 'cli', 'lwf', 'dcrs']\n    \ncorr = df_full.loc[:, columns].corr()\n\n#Generate a mask to over the upper-right side of the matrix\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\n#Plot the heatmap with correlations\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(28, 12))\n    ax = sns.heatmap(corr, mask=mask, annot=True, square=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-20T12:15:21.333181Z","iopub.execute_input":"2021-08-20T12:15:21.333454Z","iopub.status.idle":"2021-08-20T12:15:22.294928Z","shell.execute_reply.started":"2021-08-20T12:15:21.333428Z","shell.execute_reply":"2021-08-20T12:15:22.29393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, I'm glad to see```target``` and ```scaled_target``` are perfectly correlated meaning I did the linear transformation correctly ð\n\nNext, it's illuminating to see [Flesch-Kincaid Grade Level](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level) (```fleschgrade_diff```) correlate so strongly with [Gunning FOG](https://en.wikipedia.org/wiki/Gunning_fog_index) (```gfog```), [Automatic Readability Index](https://en.wikipedia.org/wiki/Automated_readability_index) (```ari```), and, to a lesser extent, [Linsear Write Formula](https://en.wikipedia.org/wiki/Linsear_Write) (```lwf```). \n\nHowever, what's most interesting, and what I'm sure the hosts of the competition are thrilled to see, is that the highest correlation for the `target` is with [Dale-Chall Readability Score](https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula) at -0.55 while the correlations between the scaled target and the average number of syllables and words per sentence (i.e., `avg_syllables`, `avg_words`) is .38 and .27 respectively. \n\nWhy do I bring those numbers up? \n\nIf the strength of relationships between the target/scaled target and the established readability scores were strong (i.e. approx. $\\pm$.8), it would indicate the ratings created by the [expert reviewers](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240423#1360081) were superflous -  why bother creating a new rating system if it provides the same results as previous ones?\n\nBesides, the competition hosts stated the [\"[w]inning models will be sure to incorporate text cohesion and semantics.\"](https://www.kaggle.com/c/commonlitreadabilityprize/overview)\nNeedless to say, if the target correlated too highly with features which ignore text cohesion and semantics, well, let's just say it would be a bad look ð\n\n# Conclusion\n\nI could keep inspecting this dataset for weeks. In fact, I haven't even touched on the [discussion](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240423#1316843) covering the validity of the ratings themselves. \n\nFor example, [Daedalus](https://www.kaggle.com/daedalusai) has highlighted multiple excerpts which have nearly identical target scores, but don't seem to pass the eye test like this: \n","metadata":{}},{"cell_type":"code","source":"samples = [\"a666c1db9\", 'b55026bd9']\ndf_full.loc[df_full.id.isin(samples), ['id', 'excerpt', 'target', 'scaled_target']]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-20T12:16:42.226827Z","iopub.execute_input":"2021-08-20T12:16:42.2274Z","iopub.status.idle":"2021-08-20T12:16:42.240756Z","shell.execute_reply.started":"2021-08-20T12:16:42.227357Z","shell.execute_reply":"2021-08-20T12:16:42.240098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What to do with this information? \n\nWell, I'm really not sure if issues like the one above can be solved algorithmically which further reinforces my belief in the topic Andrew Ng recently presented on: [From Model-centric to Data-centric AI](https://www.youtube.com/watch?v=06-AZXmwHjo). \n\nIn other words, it doesn't matter how fancy the model is if the data it's training on is shaky (aka, garbage in, garbage out). \n\nHappy coding everyone! ","metadata":{}}]}