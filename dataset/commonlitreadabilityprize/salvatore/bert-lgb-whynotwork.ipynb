{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-06T02:45:49.554749Z","iopub.execute_input":"2021-06-06T02:45:49.55518Z","iopub.status.idle":"2021-06-06T02:45:49.567622Z","shell.execute_reply.started":"2021-06-06T02:45:49.55507Z","shell.execute_reply":"2021-06-06T02:45:49.566326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntest = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:49.570616Z","iopub.execute_input":"2021-06-06T02:45:49.570986Z","iopub.status.idle":"2021-06-06T02:45:49.736752Z","shell.execute_reply.started":"2021-06-06T02:45:49.570956Z","shell.execute_reply":"2021-06-06T02:45:49.735304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# basic params","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:49.738612Z","iopub.execute_input":"2021-06-06T02:45:49.739074Z","iopub.status.idle":"2021-06-06T02:45:49.744426Z","shell.execute_reply.started":"2021-06-06T02:45:49.739025Z","shell.execute_reply":"2021-06-06T02:45:49.743149Z"}}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize\nfrom statistics import mean","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:49.747029Z","iopub.execute_input":"2021-06-06T02:45:49.747518Z","iopub.status.idle":"2021-06-06T02:45:51.611791Z","shell.execute_reply.started":"2021-06-06T02:45:49.747469Z","shell.execute_reply":"2021-06-06T02:45:51.610519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def min_max_mean_sentence_length(text):\n    '''\n    句子长度：句子包含单词个数\n    返回：最大，最小，平均值\n    '''\n    tokened_sent = sent_tokenize(text)#把一段话分成多个句子，以空格、换行分割\n    main_dict = {}\n    for item in tokened_sent:\n        item1 = list(item.split(\" \"))#空格分割,把每个句子分成不同单词\n        item2 = [' '.join(item1)]#空格合并\n        Length = []\n        Length.append(len(item1))\n        mydict = dict(zip(item2, Length))\n        main_dict.update(mydict)\n\n    return max(main_dict.values()), min(main_dict.values()), round(mean(main_dict.values()),3)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:51.61386Z","iopub.execute_input":"2021-06-06T02:45:51.614273Z","iopub.status.idle":"2021-06-06T02:45:51.622393Z","shell.execute_reply.started":"2021-06-06T02:45:51.614237Z","shell.execute_reply":"2021-06-06T02:45:51.621068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def basic_features(_):\n    '''\n    增加变量：excerpt_len段落中单词加空格长度，单词个数，句子单词个数的最大最小平均 \n    '''\n    df= _.copy()\n    df['excerpt_len'] = df['excerpt'].apply(lambda x : len(x))\n    df['excerpt_word_count'] = df['excerpt'].apply(lambda x : len(x.split(' ')))\n    df[['max_len_sent','min_len_sent','avg_len_sent']] = df.apply(lambda x: min_max_mean_sentence_length(x['excerpt']),axis=1, result_type='expand')\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:51.62368Z","iopub.execute_input":"2021-06-06T02:45:51.624034Z","iopub.status.idle":"2021-06-06T02:45:51.637315Z","shell.execute_reply.started":"2021-06-06T02:45:51.624Z","shell.execute_reply":"2021-06-06T02:45:51.636059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = basic_features(train)\ntest = basic_features(test)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:51.639063Z","iopub.execute_input":"2021-06-06T02:45:51.639421Z","iopub.status.idle":"2021-06-06T02:45:53.721939Z","shell.execute_reply.started":"2021-06-06T02:45:51.639387Z","shell.execute_reply":"2021-06-06T02:45:53.720752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(columns='target'), train['target'].values, random_state=42,test_size=0.20)\nprint(len(X_train), len(y_train))\nprint(len(X_test), len(y_test))\nfeatures = ['excerpt_len', 'excerpt_word_count', 'min_len_sent', 'max_len_sent', 'avg_len_sent']\nimport lightgbm as lgb\ngbm = lgb.LGBMRegressor(random_state=42)\ngbm.fit(X_train[features],y_train,eval_metric='mse')\npred_y = gbm.predict(X_test[features])\nprint(f' Test RMSE using basic features {round(np.sqrt(mean_squared_error(y_test,pred_y)),4)}')","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:53.723397Z","iopub.execute_input":"2021-06-06T02:45:53.723801Z","iopub.status.idle":"2021-06-06T02:45:55.084347Z","shell.execute_reply.started":"2021-06-06T02:45:53.72376Z","shell.execute_reply":"2021-06-06T02:45:55.083331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 可信度","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/duboisian/first-draft-model?scriptVersionId=63553418&cellId=2\ndef GrunningFog(excerpt):\n    \"\"\"\n    预测年级 grade level 基于 Grunning Fog index method\n    \"\"\"\n    document = excerpt\n    document = document.replace('\\n',' ').split('.')\n    document = [x for x in document if len(x)>1]\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    words = []\n    ComplexCount = []\n    for sentence in document:\n        tokens = nltk.word_tokenize(sentence)#\n        words.append(len(tokens))\n        tokens = [lemmatizer.lemmatize(x) for x in tokens]#词根还原\n        Complex = [1 if syllable_count(token) >=3 else 0 for token in tokens]#单词大于等于三个音节，算为复杂\n        ComplexCount.append(np.sum(Complex))#每一句的复杂单词个数\n    ASL = np.mean(words) #Average words per sentence 平均单词长度\n    PropComplex = np.sum(ComplexCount)/np.sum(words) #proprtion of complex words (>= 3 sylables) 段落复杂单词个数/段落单词个数\n    GrunFog = 0.4*(ASL + (100*PropComplex))\n    return(GrunFog)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:55.089965Z","iopub.execute_input":"2021-06-06T02:45:55.092429Z","iopub.status.idle":"2021-06-06T02:45:55.104003Z","shell.execute_reply.started":"2021-06-06T02:45:55.092359Z","shell.execute_reply":"2021-06-06T02:45:55.1026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def SMOG(excerpt):\n    '''\n    文段分行转空格，以.分割成多个句子，\n    nltk.word_tokenize处理句子\n    '''\n    document = excerpt\n    document = document.replace('\\n',' ').split('.')\n    document = [x for x in document if len(x)>1]\n    words = []\n    ComplexCount = []\n    for sentence in document:\n        tokens = nltk.word_tokenize(sentence)\n        words.append(len(tokens))\n        Complex = [1 if syllable_count(token) >=3 else 0 for token in tokens]\n        ComplexCount.append(np.sum(Complex))\n    SMOGScore = (1.0430 * np.sqrt(np.sum(ComplexCount) * (30/len(words)))) + 3.1291\n    return(SMOGScore)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:55.107558Z","iopub.execute_input":"2021-06-06T02:45:55.107947Z","iopub.status.idle":"2021-06-06T02:45:55.119951Z","shell.execute_reply.started":"2021-06-06T02:45:55.107912Z","shell.execute_reply":"2021-06-06T02:45:55.118311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://stackoverflow.com/a/46759549\ndef syllable_count(word):\n    '''\n    统计单词元音数量\n    '''\n    word = word.lower()\n    count = 0\n    vowels = \"aeiouy\"\n    if word[0] in vowels:\n        count += 1\n    for index in range(1, len(word)):\n        if word[index] in vowels and word[index - 1] not in vowels:\n            count += 1\n    if word.endswith(\"e\"):\n        count -= 1\n    if count == 0:\n        count += 1\n    return count","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:55.121694Z","iopub.execute_input":"2021-06-06T02:45:55.122031Z","iopub.status.idle":"2021-06-06T02:45:55.14223Z","shell.execute_reply.started":"2021-06-06T02:45:55.122002Z","shell.execute_reply":"2021-06-06T02:45:55.140787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def asw_asl(_):\n    df = _.copy()\n    df['ASL'] = df['excerpt'].apply(lambda row: np.sum([len(x.split(' ')) for x in row.replace('\\n','').split('.')])/len([len(x.split(' ')) for x in row.replace('\\n','').split('.')]))\n    df['ASW'] = df['excerpt'].apply(lambda row: np.sum([syllable_count(x) if len(x)>0 else 0 for x in row.replace('\\n','').replace('.','').split(' ')])/len([x for x in row.replace('\\n','').replace('.','').split(' ')]))\n    \n    df['RE'] = df.apply(lambda row: 206.835 - (1.015 * row['ASL']) - (84.6 * row['ASW']),axis = 1)\n    df['FKRA'] = df.apply(lambda row: (0.39 * row['ASL']) + (11.8 * row['ASW']) -15.59 ,axis = 1)\n    df['GrunFog'] = df['excerpt'].apply(lambda row: GrunningFog(row))\n    df['SMOG'] = df['excerpt'].apply(lambda row: SMOG(row))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:55.143854Z","iopub.execute_input":"2021-06-06T02:45:55.1445Z","iopub.status.idle":"2021-06-06T02:45:55.157189Z","shell.execute_reply.started":"2021-06-06T02:45:55.144446Z","shell.execute_reply":"2021-06-06T02:45:55.155954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = asw_asl(train)\ntest = asw_asl(test)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:45:55.158531Z","iopub.execute_input":"2021-06-06T02:45:55.159Z","iopub.status.idle":"2021-06-06T02:46:17.830013Z","shell.execute_reply.started":"2021-06-06T02:45:55.15897Z","shell.execute_reply":"2021-06-06T02:46:17.828747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train.drop(columns='target'), train['target'].values, random_state=42,test_size=0.20)\nprint(len(X_train), len(y_train))\nprint(len(X_test), len(y_test))\nfeatures = ['excerpt_len', 'excerpt_word_count', 'min_len_sent', 'max_len_sent', 'avg_len_sent','ASL',\n 'ASW','RE','FKRA','GrunFog','SMOG']\n\n\ngbm = lgb.LGBMRegressor(random_state=42)\ngbm.fit(X_train[features],y_train,eval_metric='mse')\npred_y = gbm.predict(X_test[features])\nprint(f' Test RMSE using basic features {round(np.sqrt(mean_squared_error(y_test,pred_y)),4)}')","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:46:17.831536Z","iopub.execute_input":"2021-06-06T02:46:17.831986Z","iopub.status.idle":"2021-06-06T02:46:18.006284Z","shell.execute_reply.started":"2021-06-06T02:46:17.831943Z","shell.execute_reply":"2021-06-06T02:46:18.005292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# +clbert","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:24:35.653414Z","iopub.execute_input":"2021-06-05T13:24:35.653867Z","iopub.status.idle":"2021-06-05T13:24:35.65761Z","shell.execute_reply.started":"2021-06-05T13:24:35.65383Z","shell.execute_reply":"2021-06-05T13:24:35.656752Z"}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom transformers import AutoModel\nfrom transformers import AutoTokenizer\nfrom transformers import AutoConfig\n\nfrom torch.cuda.amp import GradScaler\nfrom torch.cuda.amp import autocast\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Device: ', device.type)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:46:18.010649Z","iopub.execute_input":"2021-06-06T02:46:18.012796Z","iopub.status.idle":"2021-06-06T02:46:19.858343Z","shell.execute_reply.started":"2021-06-06T02:46:18.012741Z","shell.execute_reply":"2021-06-06T02:46:19.856273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Distilbert\nDISTILBERT = '../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased'\n# Roberta\nROBERTA = '../input/huggingface-roberta-variants/roberta-base/roberta-base'\nBERT = '../input/huggingface-bert-variants/bert-base-uncased/bert-base-uncased'\n\nARCH_PATH = DISTILBERT\nif_train=False\ncfg={}\ncfg['train'] ={\n    'n_folds': 5,\n    'n_epochs': 100\n}\ncfg['dl_val'] = {\n    'batch_size': 8 if device.type=='cpu' else 64, \n    'shuffle': False, \n    'num_workers': os.cpu_count(), \n    'pin_memory': True\n}\ncfg['model'] = {'name': ARCH_PATH}\ncfg['tokenizer'] ={'name': ARCH_PATH,'max_length': 210}\nclass cldataset(Dataset):\n    '''\n    call时，传入index，将该句分词，\n    返回第一部分{ids,mask,token_type_ids}，供bert使用\n    第二部分target，作为label\n    '''\n    def __init__(self,df,tokenizer,max_len):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,index):\n        text = self.df.loc[index, 'excerpt']\n        inputs = self.tokenizer.encode_plus(\n            text,                                 \n            add_special_tokens=True,\n            padding='max_length',\n            max_length=self.max_len,\n            truncation=True\n        )\n        ids=inputs['input_ids']\n        mask=inputs['attention_mask']\n        if cfg['tokenizer']['name']=='bert-base-uncased':\n            token_type_ids = inputs['token_type_ids'] \n        else:\n            token_type_ids = 1.\n        target=self.df.loc[index,['target']]\n        return {\n            'ids': torch.LongTensor(ids),#单词在词典中编码\n            'mask': torch.LongTensor(mask),#self-attention操作指定\n             'token_type_ids': torch.tensor(token_type_ids)#区分两个句子的编码\n            },{\n            'target': torch.Tensor(target)\n    }\nclass clbert(nn.Module):\n    def __init__(self,name,dropout=True):\n        super(clbert, self).__init__()\n        self.bert = AutoModel.from_pretrained(name)#导入预训练模型\n        self.name = name\n        \n        if name == BERT:\n            self.in_features = self.bert.pooler.dense.out_features\n        elif name == DISTILBERT:\n            self.in_features = self.bert.transformer.layer[5].output_layer_norm.normalized_shape[0]\n        elif name == ROBERTA:\n            self.in_features = self.bert.pooler.dense.out_features\n        else:\n            self.in_features = 768\n        \n        self.fc = nn.Linear(self.in_features, 1)\n        self.dense = nn.Linear(self.in_features, self.in_features)\n        self.activation = nn.Tanh()\n        self.dropout = nn.Dropout(p=0.2)\n        \n        torch.nn.init.kaiming_normal_(self.dense.weight)\n        torch.nn.init.kaiming_normal_(self.fc.weight)\n        \n    def forward(self, ids, mask, token_type_ids):\n        if self.name == BERT:\n            last_hidden_state, output = self.bert(ids,\n                                                  attention_mask=mask,\n                                                  token_type_ids=token_type_ids,\n                                                  return_dict=False)\n        elif self.name == DISTILBERT:\n            last_hidden_state = self.bert(ids, \n                                           attention_mask=mask, \n                                           return_dict=False)\n            first_token_tensor = last_hidden_state[0][:, 0]\n            output = self.dense(first_token_tensor)\n            output = self.activation(output)\n            \n        elif self.name == ROBERTA:\n            last_hidden_state, output = self.bert(ids,\n                                                  attention_mask=mask,\n#                                                   token_type_ids=token_type_ids,\n                                                  return_dict=False)\n        output = self.dropout(output)\n        output = self.fc(output)\n        return output\ndef val_fn_cv(model, dl):\n    '''\n    用model预测传入的dl数据\n    '''\n    scaler = GradScaler()\n    preds = []\n    \n    model.eval()\n    model.to(device)    \n    progress_bar = tqdm(dl, desc='cv')\n    \n    with torch.no_grad():\n        for i, data in enumerate(progress_bar):\n            inputs = {key: value.to(device) for key, value in data[0].items()}\n            targets = data[1]['target'].to(device)            \n            with autocast():\n                outputs = model(**inputs)\n            preds.append(outputs.detach().cpu().numpy())\n    \n    preds = np.concatenate(preds)    \n    return preds\n\ndef main_infer(): \n    '''\n    预测test数据\n    '''\n    df = pd.read_csv(TEST)\n    df['target'] = 0.\n    \n    tokenizer = AutoTokenizer.from_pretrained(cfg['tokenizer']['name'])\n    \n    for fold in range(cfg['train']['n_folds']):\n        print('Fold:', fold)\n        test_ds =cldataset(df, tokenizer=tokenizer,max_len=cfg['tokenizer']['max_length'])\n    \n        test_dl = DataLoader(test_ds, **cfg['dl_val'])\n        #加载训练好的模型\n        model = clbert(name=cfg['model']['name'])\n        PATH = os.path.join( MODEL_NAME +f'_fold{fold}.tar')\n        state_dict = torch.load(PATH, map_location=device)['model']\n        model.load_state_dict(state_dict)\n        #定义测试集的输入\n        inputs = {'model': model,\n                  'dl': test_dl}\n        #引用了另一个函数\n        preds = val_fn_cv(**inputs)\n        df['target'] = df['target'] + np.concatenate(preds)\n    \n    df['target'] = df['target'] / cfg['train']['n_folds']\n    return df\nMODEL_NAME = 'clbert'\nif if_train==False:\n    MODEL_NAME='../input/clberttrainingoutputs/clbert'\nTEST = '../input/commonlitreadabilityprize/test.csv'\ndf = main_infer()","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:46:19.859624Z","iopub.execute_input":"2021-06-06T02:46:19.859899Z","iopub.status.idle":"2021-06-06T02:46:54.274641Z","shell.execute_reply.started":"2021-06-06T02:46:19.859871Z","shell.execute_reply":"2021-06-06T02:46:54.272489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['id','target']]","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:46:54.277335Z","iopub.execute_input":"2021-06-06T02:46:54.277791Z","iopub.status.idle":"2021-06-06T02:46:54.300992Z","shell.execute_reply.started":"2021-06-06T02:46:54.277734Z","shell.execute_reply":"2021-06-06T02:46:54.299756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1=pd.read_csv(r'../input/clberttrainingoutputs/oof_df.csv')\n# f2=pd.read_csv(r'../input/clberttrainingoutputs/submission.csv')\ntrain_b=train.merge(f1[['id','oof']].rename(columns={'oof':'clbert'}))\n# test_b=test.merge(f2[['id','target']].rename(columns={'target':'clbert'}))\ntest_b=test.merge(df[['id','target']].rename(columns={'target':'clbert'}))","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:46:54.305653Z","iopub.execute_input":"2021-06-06T02:46:54.306036Z","iopub.status.idle":"2021-06-06T02:46:54.430356Z","shell.execute_reply.started":"2021-06-06T02:46:54.306004Z","shell.execute_reply":"2021-06-06T02:46:54.429184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:46:54.432031Z","iopub.execute_input":"2021-06-06T02:46:54.432354Z","iopub.status.idle":"2021-06-06T02:46:54.470087Z","shell.execute_reply.started":"2021-06-06T02:46:54.432324Z","shell.execute_reply":"2021-06-06T02:46:54.4689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(train_b.drop(columns='target'), train_b['target'].values, random_state=42,test_size=0.2)\nprint(len(X_train), len(y_train))\nprint(len(X_test), len(y_test))\n\nfeatures = ['excerpt_len', 'excerpt_word_count', 'min_len_sent', 'max_len_sent', 'avg_len_sent',#基础变量\n            'ASL','ASW','RE','FKRA','GrunFog','SMOG',#预测阅读文章对应年级变量\n            'clbert'#文本预测，bert变量\n           ]\n\ngbm = lgb.LGBMRegressor(random_state=42)\ngbm.fit(X_train[features],y_train,eval_metric='mse')\npred_y = gbm.predict(X_test[features])\nprint(f' Test RMSE using basic features {round(np.sqrt(mean_squared_error(y_test,pred_y)),4)}')\n# 纳入clbert变量，从0.823 降到了0.509","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:46:54.471931Z","iopub.execute_input":"2021-06-06T02:46:54.47231Z","iopub.status.idle":"2021-06-06T02:46:54.642856Z","shell.execute_reply.started":"2021-06-06T02:46:54.472275Z","shell.execute_reply":"2021-06-06T02:46:54.641782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_y=gbm.predict(test_b[features])\nsample['target'] = sub_y\nsample[['id','target']].to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-06T02:46:54.644525Z","iopub.execute_input":"2021-06-06T02:46:54.644838Z","iopub.status.idle":"2021-06-06T02:46:54.659449Z","shell.execute_reply.started":"2021-06-06T02:46:54.6448Z","shell.execute_reply":"2021-06-06T02:46:54.658213Z"},"trusted":true},"execution_count":null,"outputs":[]}]}