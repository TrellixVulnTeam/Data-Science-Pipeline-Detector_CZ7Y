{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import LayerNormalization\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras.layers import Conv1D, Flatten, Dense\nfrom tensorflow.keras.layers import Input, Dropout, Activation\n\nfrom transformers import RobertaTokenizer, TFRobertaModel, RobertaConfig\nfrom transformers import XLMRobertaTokenizer, TFXLMRobertaModel, XLMRobertaConfig","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:15.988896Z","iopub.execute_input":"2021-06-17T11:11:15.98929Z","iopub.status.idle":"2021-06-17T11:11:22.00363Z","shell.execute_reply.started":"2021-06-17T11:11:15.989185Z","shell.execute_reply":"2021-06-17T11:11:22.002754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! mkdir \"./Roberta-Base\"\n! mkdir \"./XLM-Roberta-Base\"\n! mkdir \"./DistilRoberta-Base\"","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:22.005141Z","iopub.execute_input":"2021-06-17T11:11:22.005477Z","iopub.status.idle":"2021-06-17T11:11:23.906291Z","shell.execute_reply.started":"2021-06-17T11:11:22.005442Z","shell.execute_reply":"2021-06-17T11:11:23.904991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load source datasets","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntrain_df[\"excerpt_wordlen\"] = train_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\ntrain_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\ntrain_df.set_index(\"id\", inplace=True)\nprint(f\"train_df: {train_df.shape}\\n\")\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:23.908508Z","iopub.execute_input":"2021-06-17T11:11:23.908811Z","iopub.status.idle":"2021-06-17T11:11:24.026519Z","shell.execute_reply.started":"2021-06-17T11:11:23.908779Z","shell.execute_reply":"2021-06-17T11:11:24.025533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntest_df[\"excerpt_wordlen\"] = test_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\ntest_df.drop(['url_legal','license'], inplace=True, axis=1)\ntest_df.set_index(\"id\", inplace=True)\nprint(f\"test_df: {test_df.shape}\\n\")\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:24.028385Z","iopub.execute_input":"2021-06-17T11:11:24.028813Z","iopub.status.idle":"2021-06-17T11:11:24.048979Z","shell.execute_reply.started":"2021-06-17T11:11:24.028777Z","shell.execute_reply":"2021-06-17T11:11:24.048062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract target label","metadata":{}},{"cell_type":"code","source":"Ytrain = train_df['target'].values\nYtrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\ntrain_df.drop(['target'], inplace=True, axis=1)\nprint(f\"Ytrain: {Ytrain.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:24.050435Z","iopub.execute_input":"2021-06-17T11:11:24.050771Z","iopub.status.idle":"2021-06-17T11:11:24.061412Z","shell.execute_reply.started":"2021-06-17T11:11:24.050736Z","shell.execute_reply":"2021-06-17T11:11:24.060176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Hyperparameters","metadata":{}},{"cell_type":"code","source":"FOLD = 5\nNUM_SEED = 1\nVERBOSE = 1\nMINI_BATCH_SIZE = 16\nNUM_EPOCH = 20\nMAX_LEN = max(train_df['excerpt_wordlen'].max(), \n              test_df['excerpt_wordlen'].max()) + 11\n\nROBERTA_BASE = \"../input/huggingface-roberta-variants/roberta-base/roberta-base\"\nXLM_ROBERTA_BASE = \"../input/huggingface-roberta-variants/tf-xlm-roberta-base/tf-xlm-roberta-base\"\nDISTILROBERTA_BASE = \"../input/huggingface-roberta-variants/distilroberta-base/distilroberta-base\"","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:24.062919Z","iopub.execute_input":"2021-06-17T11:11:24.0634Z","iopub.status.idle":"2021-06-17T11:11:24.069274Z","shell.execute_reply.started":"2021-06-17T11:11:24.063362Z","shell.execute_reply":"2021-06-17T11:11:24.068151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"def sent_encode(texts, tokenizer):\n    input_ids = []\n    attention_mask = []\n    token_type_ids = []\n\n    for text in tqdm(texts):\n        tokens = tokenizer.encode_plus(text, max_length=MAX_LEN, truncation=True, \n                                       padding='max_length', add_special_tokens=True, \n                                       return_attention_mask=True, return_token_type_ids=True, \n                                       return_tensors='tf')\n        \n        input_ids.append(tokens['input_ids'])\n        attention_mask.append(tokens['attention_mask'])\n        token_type_ids.append(tokens['token_type_ids'])\n\n    return np.array(input_ids), np.array(attention_mask), np.array(token_type_ids)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:24.070659Z","iopub.execute_input":"2021-06-17T11:11:24.071094Z","iopub.status.idle":"2021-06-17T11:11:24.080941Z","shell.execute_reply.started":"2021-06-17T11:11:24.071057Z","shell.execute_reply":"2021-06-17T11:11:24.080042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, dtype=tf.float32)\n    y_pred = tf.cast(y_pred, dtype=tf.float32)\n    return tf.math.sqrt(tf.math.reduce_mean((y_true - y_pred)**2))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:24.084164Z","iopub.execute_input":"2021-06-17T11:11:24.084644Z","iopub.status.idle":"2021-06-17T11:11:24.090802Z","shell.execute_reply.started":"2021-06-17T11:11:24.084611Z","shell.execute_reply":"2021-06-17T11:11:24.08979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def commonlit_model(transformer_model, use_tokens_type_ids=True):\n    \n    input_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n    token_type_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"token_type_ids\")\n\n    if use_tokens_type_ids:\n        embed = transformer_model(input_id, token_type_ids=token_type_id, attention_mask=attention_mask)[0]\n    \n    else:\n        embed = transformer_model(input_id, attention_mask=attention_mask)[0]\n    \n    #x = embed[:, 0, :]\n    embed = LayerNormalization()(embed)\n    \n    x = WeightNormalization(\n            Conv1D(filters=384, kernel_size=5, \n                   strides=2, padding='same', \n                   kernel_regularizer=l2(0.0001),\n                   kernel_initializer='he_uniform'))(embed)\n    x = LayerNormalization()(x)\n    x = Activation('relu')(x)\n    x = SpatialDropout1D(rate=0.25)(x)\n    \n    x = WeightNormalization(\n            Conv1D(filters=192, kernel_size=5, \n                   strides=2, padding='same', \n                   kernel_regularizer=l2(0.0001),\n                   kernel_initializer='he_uniform'))(x)\n    x = LayerNormalization()(x)\n    x = Activation('relu')(x)\n    x = SpatialDropout1D(rate=0.25)(x)\n    \n    x = Flatten()(x)\n    x = Dropout(rate=0.5)(x)\n    \n    x = Dense(units=1, kernel_initializer='lecun_normal')(x)\n\n    model = Model(inputs=[input_id, attention_mask, token_type_id], outputs=x, \n                  name='CommonLit_Readability_Model')\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Roberta-Base Model","metadata":{}},{"cell_type":"markdown","source":"### Generate word tokens and attention masks","metadata":{}},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(ROBERTA_BASE)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T10:51:37.961142Z","iopub.execute_input":"2021-06-17T10:51:37.961495Z","iopub.status.idle":"2021-06-17T10:51:38.282918Z","shell.execute_reply.started":"2021-06-17T10:51:37.961463Z","shell.execute_reply":"2021-06-17T10:51:38.282093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n\nXtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\nXtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\nXtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T10:51:39.031386Z","iopub.execute_input":"2021-06-17T10:51:39.031722Z","iopub.status.idle":"2021-06-17T10:51:46.149676Z","shell.execute_reply.started":"2021-06-17T10:51:39.031693Z","shell.execute_reply":"2021-06-17T10:51:46.148886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n\nXtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\nXtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\nXtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T10:51:46.151018Z","iopub.execute_input":"2021-06-17T10:51:46.151367Z","iopub.status.idle":"2021-06-17T10:51:46.175235Z","shell.execute_reply.started":"2021-06-17T10:51:46.151316Z","shell.execute_reply":"2021-06-17T10:51:46.174319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initialize the Bert-Base model","metadata":{}},{"cell_type":"code","source":"config = RobertaConfig.from_pretrained(ROBERTA_BASE)\nconfig.output_hidden_states = False\n\ntransformer_model = TFRobertaModel.from_pretrained(ROBERTA_BASE, config=config)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T10:52:41.712037Z","iopub.execute_input":"2021-06-17T10:52:41.712381Z","iopub.status.idle":"2021-06-17T10:52:48.284479Z","shell.execute_reply.started":"2021-06-17T10:52:41.712332Z","shell.execute_reply":"2021-06-17T10:52:48.283693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = commonlit_model(transformer_model)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T10:52:50.191709Z","iopub.execute_input":"2021-06-17T10:52:50.192029Z","iopub.status.idle":"2021-06-17T10:52:56.219932Z","shell.execute_reply.started":"2021-06-17T10:52:50.191998Z","shell.execute_reply":"2021-06-17T10:52:56.219164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fit the model with K-Fold validation","metadata":{}},{"cell_type":"code","source":"np.random.seed(23)\nseeds = np.random.randint(0, 100, size=NUM_SEED)\n\ncounter = 0\noof_score = 0\ny_pred_final1 = 0\n\n\nfor sidx, seed in enumerate(seeds):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n        counter += 1\n\n        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n        train_y, val_y = Ytrain[train], Ytrain[val]\n        \n        tf.random.set_seed(seed)\n\n        model = commonlit_model(transformer_model)\n        \n        model.compile(loss=rmse_loss,\n                      metrics=[RootMeanSquaredError(name='rmse')],\n                      optimizer=Adam(lr=8e-5))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=5, verbose=VERBOSE)\n        \n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n                                      min_lr=1e-7, patience=2, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'./Roberta-Base/CLRP_Roberta_Base_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model.fit(\n            [train_x_id, train_x_mask, train_x_token], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=NUM_EPOCH, \n            verbose=VERBOSE, \n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n        )\n        \n        model.load_weights(f'./Roberta-Base/CLRP_Roberta_Base_{counter}C.h5')\n        \n        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n        y_pred_final1 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score += score\n        seed_score += score\n        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n\n\ny_pred_final1 = y_pred_final1 / float(counter)\noof_score /= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T10:52:56.222862Z","iopub.execute_input":"2021-06-17T10:52:56.223127Z","iopub.status.idle":"2021-06-17T11:00:53.296568Z","shell.execute_reply.started":"2021-06-17T10:52:56.223095Z","shell.execute_reply":"2021-06-17T11:00:53.29373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XLM-Roberta-Base Model","metadata":{}},{"cell_type":"markdown","source":"### Generate word tokens and attention masks","metadata":{}},{"cell_type":"code","source":"tokenizer = XLMRobertaTokenizer.from_pretrained(XLM_ROBERTA_BASE)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:46.75885Z","iopub.execute_input":"2021-06-17T11:11:46.759159Z","iopub.status.idle":"2021-06-17T11:11:47.407098Z","shell.execute_reply.started":"2021-06-17T11:11:46.75913Z","shell.execute_reply":"2021-06-17T11:11:47.406257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n\nXtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\nXtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\nXtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:47.408642Z","iopub.execute_input":"2021-06-17T11:11:47.408985Z","iopub.status.idle":"2021-06-17T11:11:52.180996Z","shell.execute_reply.started":"2021-06-17T11:11:47.408949Z","shell.execute_reply":"2021-06-17T11:11:52.179957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n\nXtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\nXtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\nXtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:52.182719Z","iopub.execute_input":"2021-06-17T11:11:52.183073Z","iopub.status.idle":"2021-06-17T11:11:52.202046Z","shell.execute_reply.started":"2021-06-17T11:11:52.183033Z","shell.execute_reply":"2021-06-17T11:11:52.201111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initialize the Albert-V2 model","metadata":{}},{"cell_type":"code","source":"config = XLMRobertaConfig.from_pretrained(XLM_ROBERTA_BASE)\nconfig.output_hidden_states = False\n\ntransformer_model = TFXLMRobertaModel.from_pretrained(XLM_ROBERTA_BASE, config=config)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:11:56.214904Z","iopub.execute_input":"2021-06-17T11:11:56.215223Z","iopub.status.idle":"2021-06-17T11:12:14.021843Z","shell.execute_reply.started":"2021-06-17T11:11:56.215193Z","shell.execute_reply":"2021-06-17T11:12:14.021129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = commonlit_model(transformer_model)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:12:14.023232Z","iopub.execute_input":"2021-06-17T11:12:14.0235Z","iopub.status.idle":"2021-06-17T11:12:20.098755Z","shell.execute_reply.started":"2021-06-17T11:12:14.023474Z","shell.execute_reply":"2021-06-17T11:12:20.097728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fit the model with K-Fold validation","metadata":{}},{"cell_type":"code","source":"np.random.seed(29)\nseeds = np.random.randint(0, 100, size=NUM_SEED)\n\ncounter = 0\noof_score = 0\ny_pred_final2 = 0\n\n\nfor sidx, seed in enumerate(seeds):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n        counter += 1\n\n        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n        train_y, val_y = Ytrain[train], Ytrain[val]\n        \n        tf.random.set_seed(seed)\n\n        model = commonlit_model(transformer_model)\n        \n        model.compile(loss=rmse_loss,\n                      metrics=[RootMeanSquaredError(name='rmse')],\n                      optimizer=Adam(lr=8e-5))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=5, verbose=VERBOSE)\n        \n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n                                      min_lr=1e-7, patience=2, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'./XLM-Roberta-Base/CLRP_XLMRoberta_Base_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model.fit(\n            [train_x_id, train_x_mask, train_x_token], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=NUM_EPOCH, \n            verbose=VERBOSE, \n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n        )\n        \n        model.load_weights(f'./XLM-Roberta-Base/CLRP_XLMRoberta_Base_{counter}C.h5')\n        \n        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n        y_pred_final2 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score += score\n        seed_score += score\n        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n\n\ny_pred_final2 = y_pred_final2 / float(counter)\noof_score /= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:12:20.10079Z","iopub.execute_input":"2021-06-17T11:12:20.101161Z","iopub.status.idle":"2021-06-17T11:26:09.330348Z","shell.execute_reply.started":"2021-06-17T11:12:20.101122Z","shell.execute_reply":"2021-06-17T11:26:09.327418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DistilRoberta-Base Model","metadata":{}},{"cell_type":"markdown","source":"### Generate word tokens and attention masks","metadata":{}},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(DISTILROBERTA_BASE)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:26:15.014437Z","iopub.execute_input":"2021-06-17T11:26:15.01476Z","iopub.status.idle":"2021-06-17T11:26:15.174938Z","shell.execute_reply.started":"2021-06-17T11:26:15.014729Z","shell.execute_reply":"2021-06-17T11:26:15.174075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain_id, Xtrain_mask, Xtrain_token = sent_encode(train_df['excerpt'].values, tokenizer)\n\nXtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\nXtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\nXtrain_token = Xtrain_token.reshape((Xtrain_token.shape[0], Xtrain_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape} \\nToken-type-ids: {Xtrain_token.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:26:16.414283Z","iopub.execute_input":"2021-06-17T11:26:16.414636Z","iopub.status.idle":"2021-06-17T11:26:21.840033Z","shell.execute_reply.started":"2021-06-17T11:26:16.414597Z","shell.execute_reply":"2021-06-17T11:26:21.839013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtest_id, Xtest_mask, Xtest_token = sent_encode(test_df['excerpt'].values, tokenizer)\n\nXtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\nXtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\nXtest_token = Xtest_token.reshape((Xtest_token.shape[0], Xtest_token.shape[2]))\n    \nprint(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape} \\nToken-type-ids: {Xtest_token.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:26:21.841618Z","iopub.execute_input":"2021-06-17T11:26:21.841965Z","iopub.status.idle":"2021-06-17T11:26:21.866403Z","shell.execute_reply.started":"2021-06-17T11:26:21.841926Z","shell.execute_reply":"2021-06-17T11:26:21.865464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initialize the DistilBert-Base model","metadata":{}},{"cell_type":"code","source":"config = RobertaConfig.from_pretrained(DISTILROBERTA_BASE)\nconfig.output_hidden_states = False\n\ntransformer_model = TFRobertaModel.from_pretrained(DISTILROBERTA_BASE, config=config)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:26:22.233676Z","iopub.execute_input":"2021-06-17T11:26:22.234002Z","iopub.status.idle":"2021-06-17T11:26:26.631735Z","shell.execute_reply.started":"2021-06-17T11:26:22.233971Z","shell.execute_reply":"2021-06-17T11:26:26.630824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = commonlit_model(transformer_model, use_tokens_type_ids=False)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:26:26.633356Z","iopub.execute_input":"2021-06-17T11:26:26.633925Z","iopub.status.idle":"2021-06-17T11:26:27.670791Z","shell.execute_reply.started":"2021-06-17T11:26:26.633885Z","shell.execute_reply":"2021-06-17T11:26:27.669945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fit the model with K-Fold validation","metadata":{}},{"cell_type":"code","source":"np.random.seed(31)\nseeds = np.random.randint(0, 100, size=NUM_SEED)\n\ncounter = 0\noof_score = 0\ny_pred_final3 = 0\n\n\nfor sidx, seed in enumerate(seeds):\n    seed_score = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain_id, Ytrain_strat)):\n        counter += 1\n\n        train_x_id, train_x_mask, train_x_token = Xtrain_id[train], Xtrain_mask[train], Xtrain_token[train]\n        val_x_id, val_x_mask, val_x_token = Xtrain_id[val], Xtrain_mask[val], Xtrain_token[val]\n        train_y, val_y = Ytrain[train], Ytrain[val]\n        \n        tf.random.set_seed(seed)\n\n        model = commonlit_model(transformer_model, use_tokens_type_ids=False)\n        \n        model.compile(loss=rmse_loss,\n                      metrics=[RootMeanSquaredError(name='rmse')],\n                      optimizer=Adam(lr=8e-5))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=5, verbose=VERBOSE)\n        \n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.5, \n                                      min_lr=1e-7, patience=2, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'./DistilRoberta-Base/CLRP_DistilRoberta_Base_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model.fit(\n            [train_x_id, train_x_mask, train_x_token], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=NUM_EPOCH, \n            verbose=VERBOSE, \n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x_token], val_y)\n        )\n        \n        model.load_weights(f'./DistilRoberta-Base/CLRP_DistilRoberta_Base_{counter}C.h5')\n        \n        y_pred = model.predict([val_x_id, val_x_mask, val_x_token])\n        y_pred_final3 += model.predict([Xtest_id, Xtest_mask, Xtest_token])\n        \n        score = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score += score\n        seed_score += score\n        print(\"Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score))\n    \n    print(\"\\nSeed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score / FOLD)))\n\n\ny_pred_final3 = y_pred_final3 / float(counter)\noof_score /= float(counter)\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T11:26:31.304957Z","iopub.execute_input":"2021-06-17T11:26:31.305297Z","iopub.status.idle":"2021-06-17T11:30:18.517885Z","shell.execute_reply.started":"2021-06-17T11:26:31.305261Z","shell.execute_reply":"2021-06-17T11:30:18.515273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create submission file","metadata":{}},{"cell_type":"code","source":"y_pred_final = (y_pred_final1 + y_pred_final2 + y_pred_final3) / 3.0\n\nsubmit_df = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmit_df['target'] = y_pred_final\nsubmit_df.to_csv(\"./submission.csv\", index=False)\nsubmit_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}