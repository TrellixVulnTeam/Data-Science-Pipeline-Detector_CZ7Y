{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Experiment Log:**\n\n|Version |Models Used |CV Score |LB Score| Changes Made\n| --- | --- | --- | --- | --- |\n|v1 |Lasso, ElasticNet | 0.8361 | 0.843 | Baseline\n|v2 |Lasso, ElasticNet | 0.8361 | 0.843 | TF-IDF\n|v3 |Lasso, ElasticNet | 0.8361 | NA | TF-IDF (-) <br> Glove (+)\n|v4 |Lasso, ElasticNet <br> LightGBM, XGBoost | 0.7485 | 0.654 | Glove\n|v5 |Conv1D, Bidirectional LSTM | 0.6344 | 0.603 | Glove\n|v6 |Conv1D, Bidirectional LSTM | 0.6436 | NA | Glove + FastText <br> Model architecture modified\n|v7 |Conv1D, Bidirectional LSTM | 0.6443 | 0.583 | Model architecture modified\n|v8 |Conv1D, Bidirectional LSTM | 0.6652 | 0.629 | Bert Embeddings\n|v9 |Conv1D, Bidirectional LSTM | 0.6416 | 0.596 | Bert Embeddings (-) <br> Glove + FastText (+)\n|v10 |Transformer block with attention | 0.6466 | NA | Glove + FastText\n|v11 |Transformer block with attention | 0.6497 | 0.596 | Changed TF initialization process\n|v12 |Conv1D, Bidirectional LSTM | 0.6482 | NA | Reverting to v7 model architecture <br> (with TF initialization changes)\n|v13 |Conv1D, Bidirectional LSTM | 0.6496 | 0.575 | Bug fix in predictions\n|v14 |Conv1D, Bidirectional LSTM | 0.3216 | NA | Pretrained DistilBert Embeddings <br> (with same model backbone)\n|v15 |Conv1D, Bidirectional LSTM | 0.3283 | 0.561 | Pretrained DistilBert Embeddings <br> (training for single seed)\n|v16 |Conv1D, Bidirectional LSTM | 0.6712 | 0.593 | Glove + FastText (Common Crawl)\n|v17 |Bidirectional LSTM & GRU <br> (with attention) | 0.6312 | 0.592 | Model architecture modified\n|v18 |Bidirectional LSTM & GRU <br> (with attention) | 0.6313 | 0.583 | Glove + FastText (Wiki-News)\n|v19 |Bidirectional LSTM & GRU <br> (with attention) | 0.6413 | 0.597 | Fix for OOV tokens\n|v20 |Conv1D, Bidirectional LSTM | 0.6563 | 0.615 | Switched to v13 model architecture\n|v21 |Conv1D, Bidirectional LSTM | 0.6372 | NA | Glove + FastText + Word2Vec <br> (Weighted embeddings) <br> Model architecture modified\n|v22 |Conv1D, Bidirectional LSTM | 0.6367 | 0.589 | Glove + FastText <br> (Weighted embeddings) <br> Model architecture modified\n|v23 |Conv1D, Bidirectional LSTM | 0.6568 | 0.593 | Glove + FastText <br> (Weighted embeddings) <br> v13 Model architecture\n|v24 |Conv1D, Bidirectional LSTM | TBD | TBD | Pretrained DistilRoberta Embeddings","metadata":{"papermill":{"duration":0.026702,"end_time":"2021-06-28T09:24:07.87216","exception":false,"start_time":"2021-06-28T09:24:07.845458","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Import libraries","metadata":{"papermill":{"duration":0.025449,"end_time":"2021-06-28T09:24:07.922942","exception":false,"start_time":"2021-06-28T09:24:07.897493","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import gc\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport re\nimport nltk\nimport spacy\nimport string\nfrom textblob import TextBlob\nfrom collections import Counter\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adamax\nfrom tensorflow.keras.initializers import LecunNormal\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.layers import LayerNormalization\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Attention, Dropout\nfrom tensorflow.keras.layers import Dense, Input, Conv1D\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras.layers import GlobalAveragePooling1D\nfrom tensorflow.keras.layers import LSTM, Flatten, Bidirectional\nfrom tensorflow.keras.layers import Activation, SpatialDropout1D\nfrom tensorflow.keras.layers import GlobalMaxPool1D, Concatenate\n\nfrom transformers import RobertaTokenizer, TFRobertaModel, RobertaConfig","metadata":{"papermill":{"duration":8.120423,"end_time":"2021-06-28T09:24:16.069005","exception":false,"start_time":"2021-06-28T09:24:07.948582","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:10.592132Z","iopub.execute_input":"2021-07-04T16:56:10.592665Z","iopub.status.idle":"2021-07-04T16:56:18.7768Z","shell.execute_reply.started":"2021-07-04T16:56:10.592555Z","shell.execute_reply":"2021-07-04T16:56:18.775946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load source datasets","metadata":{"papermill":{"duration":0.025436,"end_time":"2021-06-28T09:24:16.12063","exception":false,"start_time":"2021-06-28T09:24:16.095194","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntrain_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\ntrain_df.set_index(\"id\", inplace=True)\nprint(f\"train_df: {train_df.shape}\\n\")\ntrain_df.head()","metadata":{"papermill":{"duration":0.113226,"end_time":"2021-06-28T09:24:16.259396","exception":false,"start_time":"2021-06-28T09:24:16.14617","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:20.300481Z","iopub.execute_input":"2021-07-04T16:56:20.300803Z","iopub.status.idle":"2021-07-04T16:56:20.385506Z","shell.execute_reply.started":"2021-07-04T16:56:20.300773Z","shell.execute_reply":"2021-07-04T16:56:20.384451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntest_df.drop(['url_legal','license'], inplace=True, axis=1)\ntest_df.set_index(\"id\", inplace=True)\nprint(f\"test_df: {test_df.shape}\\n\")\ntest_df.head()","metadata":{"papermill":{"duration":0.046741,"end_time":"2021-06-28T09:24:16.333507","exception":false,"start_time":"2021-06-28T09:24:16.286766","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:20.460906Z","iopub.execute_input":"2021-07-04T16:56:20.461159Z","iopub.status.idle":"2021-07-04T16:56:20.478197Z","shell.execute_reply.started":"2021-07-04T16:56:20.461133Z","shell.execute_reply":"2021-07-04T16:56:20.477334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract target label","metadata":{"papermill":{"duration":0.026525,"end_time":"2021-06-28T09:24:16.386723","exception":false,"start_time":"2021-06-28T09:24:16.360198","status":"completed"},"tags":[]}},{"cell_type":"code","source":"_, ax = plt.subplots(1, 2, figsize=(15, 5))\nsns.boxplot(x='target', data=train_df, ax=ax[0])\nsns.histplot(x='target', data=train_df, ax=ax[1])\nax[0].title.set_text('Box Plot - target')\nax[1].title.set_text('Hist Plot - target')","metadata":{"papermill":{"duration":0.315353,"end_time":"2021-06-28T09:24:16.728738","exception":false,"start_time":"2021-06-28T09:24:16.413385","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:20.871277Z","iopub.execute_input":"2021-07-04T16:56:20.871577Z","iopub.status.idle":"2021-07-04T16:56:21.155626Z","shell.execute_reply.started":"2021-07-04T16:56:20.87155Z","shell.execute_reply":"2021-07-04T16:56:21.154268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Ytrain = train_df['target'].values\nYtrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\ntrain_df.drop(['target'], inplace=True, axis=1)\nprint(\"Ytrain: {}\".format(Ytrain.shape))","metadata":{"papermill":{"duration":0.038729,"end_time":"2021-06-28T09:24:16.795585","exception":false,"start_time":"2021-06-28T09:24:16.756856","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:21.157009Z","iopub.execute_input":"2021-07-04T16:56:21.157358Z","iopub.status.idle":"2021-07-04T16:56:21.165515Z","shell.execute_reply.started":"2021-07-04T16:56:21.157312Z","shell.execute_reply":"2021-07-04T16:56:21.164519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{"papermill":{"duration":0.027818,"end_time":"2021-06-28T09:24:16.85069","exception":false,"start_time":"2021-06-28T09:24:16.822872","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def contraction_count(sent):\n    count = 0\n    count += re.subn(r\"won\\'t\", '', sent)[1]\n    count += re.subn(r\"can\\'t\", '', sent)[1]\n    count += re.subn(r\"n\\'t\", '', sent)[1]\n    count += re.subn(r\"\\'re\", '', sent)[1]\n    count += re.subn(r\"\\'s\", '', sent)[1]\n    count += re.subn(r\"\\'d\", '', sent)[1]\n    count += re.subn(r\"\\'ll\", '', sent)[1]\n    count += re.subn(r\"\\'t\", '', sent)[1]\n    count += re.subn(r\"\\'ve\", '', sent)[1]\n    count += re.subn(r\"\\'m\", '', sent)[1]\n    return count","metadata":{"papermill":{"duration":0.036916,"end_time":"2021-06-28T09:24:16.916018","exception":false,"start_time":"2021-06-28T09:24:16.879102","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:21.670956Z","iopub.execute_input":"2021-07-04T16:56:21.67139Z","iopub.status.idle":"2021-07-04T16:56:21.683879Z","shell.execute_reply.started":"2021-07-04T16:56:21.671351Z","shell.execute_reply":"2021-07-04T16:56:21.683061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pos_count(sent):\n    nn_count = 0   #Noun\n    pr_count = 0   #Pronoun\n    vb_count = 0   #Verb\n    jj_count = 0   #Adjective\n    uh_count = 0   #Interjection\n    cd_count = 0   #Numerics\n    \n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n\n    for token in sent:\n        if token[1] in ['NN','NNP','NNS']:\n            nn_count += 1\n\n        if token[1] in ['PRP','PRP$']:\n            pr_count += 1\n\n        if token[1] in ['VB','VBD','VBG','VBN','VBP','VBZ']:\n            vb_count += 1\n\n        if token[1] in ['JJ','JJR','JJS']:\n            jj_count += 1\n\n        if token[1] in ['UH']:\n            uh_count += 1\n\n        if token[1] in ['CD']:\n            cd_count += 1\n    \n    return pd.Series([nn_count, pr_count, vb_count, jj_count, uh_count, cd_count])","metadata":{"papermill":{"duration":0.042289,"end_time":"2021-06-28T09:24:16.986175","exception":false,"start_time":"2021-06-28T09:24:16.943886","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:21.918382Z","iopub.execute_input":"2021-07-04T16:56:21.918697Z","iopub.status.idle":"2021-07-04T16:56:21.925594Z","shell.execute_reply.started":"2021-07-04T16:56:21.918667Z","shell.execute_reply":"2021-07-04T16:56:21.924701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dialog_parser(text):\n    \n    tokenized = nltk.word_tokenize(text)\n    \n    # let's set up some lists to hold our pieces of narrative and dialog\n    parsed_dialog = []\n    parsed_narrative = []\n    \n    # and this list will be a bucket for the text we're currently exploring\n    current = []\n\n    # now let's set up values that will help us loop through the text\n    length = len(tokenized)\n    found_q = False\n    counter = 0\n    quote_open, quote_close = '``', \"''\"\n\n    # now we'll start our loop saying that as long as our sentence is...\n    while counter < length:\n        word = tokenized[counter]\n\n        # until we find a quotation mark, we're working with narrative\n        if quote_open not in word and quote_close not in word:\n            current.append(word)\n\n        # here's what we do when we find a closed quote\n        else:\n            # we append the narrative we've collected & clear our our\n            # current variable\n            parsed_narrative.append(current)\n            current = []\n            \n            # now current is ready to hold dialog and we're working on\n            # a piece of dialog\n            current.append(word)\n            found_q = True\n\n            # while we're in the quote, we're going to increment the counter\n            # and append to current in this while loop\n            while found_q and counter < length-1:\n                counter += 1\n                if quote_close not in tokenized[counter]:\n                    current.append(tokenized[counter])\n                else:\n                    # if we find a closing quote, we add our dialog to the\n                    # appropriate list, clear current and flip our found_q\n                    # variable to False\n                    current.append(tokenized[counter])\n                    parsed_dialog.append(current)\n                    current = []\n                    found_q = False\n\n        # increment the counter to move us through the text\n        counter += 1\n    \n    if len(parsed_narrative) == 0:\n        parsed_narrative.append(current)\n    \n    mean_dialog_word_len = 0\n    \n    if len(parsed_dialog) > 0:\n        for text in parsed_dialog:\n            join_text = \" \".join(text)\n            join_text = join_text.replace('\"','')\n            join_text = join_text.replace(\"''\",\"\")\n            mean_dialog_word_len += len(join_text.split())\n        \n        mean_dialog_word_len /= float(len(parsed_dialog))\n    \n    mean_narrative_word_len = 0\n    \n    if len(parsed_narrative) > 0:\n        for text in parsed_narrative:\n            join_text = \" \".join(text)\n            join_text = join_text.replace('\"','')\n            join_text = join_text.replace(\"''\",\"\")\n            mean_narrative_word_len += len(join_text.split())\n        \n        mean_narrative_word_len /= float(len(parsed_narrative))\n\n    return len(parsed_dialog), len(parsed_narrative), mean_dialog_word_len, mean_narrative_word_len","metadata":{"papermill":{"duration":0.041088,"end_time":"2021-06-28T09:24:17.055325","exception":false,"start_time":"2021-06-28T09:24:17.014237","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:22.074172Z","iopub.execute_input":"2021-07-04T16:56:22.074459Z","iopub.status.idle":"2021-07-04T16:56:22.086201Z","shell.execute_reply.started":"2021-07-04T16:56:22.074428Z","shell.execute_reply":"2021-07-04T16:56:22.084852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decontraction(phrase):\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","metadata":{"papermill":{"duration":0.035865,"end_time":"2021-06-28T09:24:17.118974","exception":false,"start_time":"2021-06-28T09:24:17.083109","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:22.360174Z","iopub.execute_input":"2021-07-04T16:56:22.36047Z","iopub.status.idle":"2021-07-04T16:56:22.366858Z","shell.execute_reply.started":"2021-07-04T16:56:22.360442Z","shell.execute_reply":"2021-07-04T16:56:22.366009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punctuations(text):\n    for punctuation in list(string.punctuation):\n        text = text.replace(punctuation, '')\n    return text","metadata":{"papermill":{"duration":0.033462,"end_time":"2021-06-28T09:24:17.180349","exception":false,"start_time":"2021-06-28T09:24:17.146887","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:22.610232Z","iopub.execute_input":"2021-07-04T16:56:22.610543Z","iopub.status.idle":"2021-07-04T16:56:22.614731Z","shell.execute_reply.started":"2021-07-04T16:56:22.610515Z","shell.execute_reply":"2021-07-04T16:56:22.613822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatize_words(text):\n    lemmatizer = WordNetLemmatizer()\n    wordnet_map = {\n        \"N\": wordnet.NOUN, \n        \"V\": wordnet.VERB, \n        \"J\": wordnet.ADJ, \n        \"R\": wordnet.ADV\n    }\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])","metadata":{"papermill":{"duration":0.034581,"end_time":"2021-06-28T09:24:17.242552","exception":false,"start_time":"2021-06-28T09:24:17.207971","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:22.759924Z","iopub.execute_input":"2021-07-04T16:56:22.760201Z","iopub.status.idle":"2021-07-04T16:56:22.765942Z","shell.execute_reply.started":"2021-07-04T16:56:22.76017Z","shell.execute_reply":"2021-07-04T16:56:22.76512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df = train_df.append(test_df, sort=False, ignore_index=False)\n\ndel train_df\ndel test_df\ngc.collect()\n\ncombined_df.head()","metadata":{"papermill":{"duration":0.221127,"end_time":"2021-06-28T09:24:17.491194","exception":false,"start_time":"2021-06-28T09:24:17.270067","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:23.140637Z","iopub.execute_input":"2021-07-04T16:56:23.140944Z","iopub.status.idle":"2021-07-04T16:56:23.326164Z","shell.execute_reply.started":"2021-07-04T16:56:23.140915Z","shell.execute_reply":"2021-07-04T16:56:23.325332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt = Counter()\nfor text in combined_df[\"excerpt\"].values:\n    for word in text.split():\n        cnt[word] += 1\n\n\nfreq_words = []\nfor (w, wc) in tqdm(cnt.most_common()):\n    if wc > 3500:\n        freq_words.append(w)\n\nlen(freq_words)","metadata":{"papermill":{"duration":0.285872,"end_time":"2021-06-28T09:24:17.805113","exception":false,"start_time":"2021-06-28T09:24:17.519241","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:23.760336Z","iopub.execute_input":"2021-07-04T16:56:23.76068Z","iopub.status.idle":"2021-07-04T16:56:24.017384Z","shell.execute_reply.started":"2021-07-04T16:56:23.760647Z","shell.execute_reply":"2021-07-04T16:56:24.016632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df[\"excerpt_num_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\ncombined_df[\"excerpt_num_unique_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\ncombined_df[\"excerpt_num_chars\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x)))\ncombined_df[\"excerpt_num_stopwords\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\ncombined_df[\"excerpt_num_punctuations\"] =combined_df['excerpt'].apply(lambda x: len([c for c in str(x) if c in list(string.punctuation)]))\ncombined_df[\"excerpt_num_words_upper\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ncombined_df[\"excerpt_num_words_title\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ncombined_df[\"excerpt_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ncombined_df[\"excerpt_num_paragraphs\"] = combined_df[\"excerpt\"].apply(lambda x: len(x.split('\\n')))\ncombined_df[\"excerpt_num_sentences\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x).split('.')))\ncombined_df[\"excerpt_num_contractions\"] = combined_df[\"excerpt\"].apply(contraction_count)\ncombined_df[\"excerpt_freq_words\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w in freq_words]))\ncombined_df[\"excerpt_num_dialog\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[0])\ncombined_df[\"excerpt_num_narrative\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[1])\ncombined_df[\"excerpt_dialog_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[2])\ncombined_df[\"excerpt_narrative_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[3])\ncombined_df['excerpt_polarity'] = combined_df['excerpt'].apply(lambda x: TextBlob(x).sentiment[0])\ncombined_df['excerpt_subjectivity'] = combined_df['excerpt'].apply(lambda x: TextBlob(x).sentiment[1])\ncombined_df[['nn_count','pr_count','vb_count','jj_count','uh_count','cd_count']] = combined_df['excerpt'].apply(pos_count)\ncombined_df.head()","metadata":{"papermill":{"duration":117.748166,"end_time":"2021-06-28T09:26:15.582373","exception":false,"start_time":"2021-06-28T09:24:17.834207","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:56:24.018928Z","iopub.execute_input":"2021-07-04T16:56:24.019274Z","iopub.status.idle":"2021-07-04T16:58:21.476965Z","shell.execute_reply.started":"2021-07-04T16:56:24.019238Z","shell.execute_reply":"2021-07-04T16:58:21.475126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = combined_df[:Ytrain.shape[0]].copy()\ndf['target'] = Ytrain\nplt.figure(figsize=(20, 15))\nsns.heatmap(df.corr(), annot=True, fmt='.2f', cmap=\"RdYlGn\")","metadata":{"papermill":{"duration":2.610153,"end_time":"2021-06-28T09:26:18.22368","exception":false,"start_time":"2021-06-28T09:26:15.613527","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:58:21.478954Z","iopub.execute_input":"2021-07-04T16:58:21.479294Z","iopub.status.idle":"2021-07-04T16:58:24.35818Z","shell.execute_reply.started":"2021-07-04T16:58:21.479257Z","shell.execute_reply":"2021-07-04T16:58:24.357366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to lower case\ncombined_df['excerpt'] = combined_df['excerpt'].apply(lambda x: str(x).lower().replace('\\\\', '').replace('_', ' '))\n\n# Remove double spaces\ncombined_df['excerpt'] = combined_df['excerpt'].apply(lambda x: re.sub('\\s+',  ' ', x))\n\n# Replace contractions (\"don't\" with \"do not\" and \"we've\" with \"we have\")\ncombined_df['excerpt'] = combined_df['excerpt'].apply(lambda x: decontraction(x))\n\n# Remove punctuations\ncombined_df['excerpt'] = combined_df['excerpt'].apply(remove_punctuations)\n\n# Lemmatize words\ncombined_df['excerpt'] = combined_df['excerpt'].apply(lambda text: lemmatize_words(text))\n\ncombined_df.head()","metadata":{"papermill":{"duration":25.713485,"end_time":"2021-06-28T09:26:43.976195","exception":false,"start_time":"2021-06-28T09:26:18.26271","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:58:24.359473Z","iopub.execute_input":"2021-07-04T16:58:24.359823Z","iopub.status.idle":"2021-07-04T16:58:49.866536Z","shell.execute_reply.started":"2021-07-04T16:58:24.359773Z","shell.execute_reply":"2021-07-04T16:58:49.865656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract excerpts\ntrain_list = combined_df[:Ytrain.shape[0]]['excerpt'].tolist()\ntest_list = combined_df[Ytrain.shape[0]:]['excerpt'].tolist()\nMAX_LEN = combined_df['excerpt_num_words'].max() + 11\nprint(f\"Train excerpts: {len(train_list)} \\nTest excerpts: {len(test_list)} \\nMAX_LEN: {MAX_LEN}\")","metadata":{"papermill":{"duration":0.049415,"end_time":"2021-06-28T09:26:44.066277","exception":false,"start_time":"2021-06-28T09:26:44.016862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:58:49.867904Z","iopub.execute_input":"2021-07-04T16:58:49.86845Z","iopub.status.idle":"2021-07-04T16:58:49.877336Z","shell.execute_reply.started":"2021-07-04T16:58:49.868392Z","shell.execute_reply":"2021-07-04T16:58:49.876443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features = ['excerpt_num_words','excerpt_num_unique_words','excerpt_num_chars',\n                     'excerpt_num_punctuations','excerpt_num_words_title','excerpt_mean_word_len',\n                     'excerpt_num_paragraphs','excerpt_num_contractions','excerpt_freq_words',\n                     'excerpt_num_dialog','excerpt_narrative_mean_word_len','nn_count','pr_count',\n                     'vb_count','jj_count','uh_count','cd_count']\n\ncombined_df = combined_df[selected_features].copy()","metadata":{"papermill":{"duration":0.049028,"end_time":"2021-06-28T09:26:44.155685","exception":false,"start_time":"2021-06-28T09:26:44.106657","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:58:49.878735Z","iopub.execute_input":"2021-07-04T16:58:49.87935Z","iopub.status.idle":"2021-07-04T16:58:49.887843Z","shell.execute_reply.started":"2021-07-04T16:58:49.879308Z","shell.execute_reply":"2021-07-04T16:58:49.886998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain = combined_df[:Ytrain.shape[0]].copy()\nXtest = combined_df[Ytrain.shape[0]:].copy()\nprint(f\"Xtrain: {Xtrain.shape} \\nXtest: {Xtest.shape}\")\n\ndel combined_df\ngc.collect()","metadata":{"papermill":{"duration":0.285951,"end_time":"2021-06-28T09:26:44.481418","exception":false,"start_time":"2021-06-28T09:26:44.195467","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T16:58:49.889897Z","iopub.execute_input":"2021-07-04T16:58:49.890287Z","iopub.status.idle":"2021-07-04T16:58:50.139248Z","shell.execute_reply.started":"2021-07-04T16:58:49.890251Z","shell.execute_reply":"2021-07-04T16:58:50.138299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate word token and attention mask","metadata":{"papermill":{"duration":0.040569,"end_time":"2021-06-28T09:26:44.562751","exception":false,"start_time":"2021-06-28T09:26:44.522182","status":"completed"},"tags":[]}},{"cell_type":"code","source":"DISTILROBERTA_BASE = \"../input/huggingface-roberta-variants/distilroberta-base/distilroberta-base\"","metadata":{"papermill":{"duration":0.04698,"end_time":"2021-06-28T09:26:44.650214","exception":false,"start_time":"2021-06-28T09:26:44.603234","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:08.640849Z","iopub.execute_input":"2021-07-04T17:01:08.641191Z","iopub.status.idle":"2021-07-04T17:01:08.644912Z","shell.execute_reply.started":"2021-07-04T17:01:08.641156Z","shell.execute_reply":"2021-07-04T17:01:08.643834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sent_encode(texts, tokenizer):\n    input_ids = []\n    attention_mask = []\n\n    for text in tqdm(texts):\n        tokens = tokenizer.encode_plus(text, max_length=MAX_LEN, truncation=True, \n                                       padding='max_length', add_special_tokens=True, \n                                       return_attention_mask=True, return_token_type_ids=False, \n                                       return_tensors='tf')\n        \n        input_ids.append(tokens['input_ids'])\n        attention_mask.append(tokens['attention_mask'])\n\n    return np.array(input_ids), np.array(attention_mask)","metadata":{"papermill":{"duration":0.049292,"end_time":"2021-06-28T09:26:44.740027","exception":false,"start_time":"2021-06-28T09:26:44.690735","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:09.164052Z","iopub.execute_input":"2021-07-04T17:01:09.164353Z","iopub.status.idle":"2021-07-04T17:01:09.170041Z","shell.execute_reply.started":"2021-07-04T17:01:09.164325Z","shell.execute_reply":"2021-07-04T17:01:09.169068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = RobertaTokenizer.from_pretrained(DISTILROBERTA_BASE)","metadata":{"papermill":{"duration":0.163975,"end_time":"2021-06-28T09:26:44.95815","exception":false,"start_time":"2021-06-28T09:26:44.794175","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:10.140637Z","iopub.execute_input":"2021-07-04T17:01:10.140962Z","iopub.status.idle":"2021-07-04T17:01:10.25821Z","shell.execute_reply.started":"2021-07-04T17:01:10.140931Z","shell.execute_reply":"2021-07-04T17:01:10.257349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtrain_id, Xtrain_mask = sent_encode(train_list, tokenizer)\n\nXtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\nXtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\n    \nprint(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape}\")","metadata":{"papermill":{"duration":15.751269,"end_time":"2021-06-28T09:27:00.784979","exception":false,"start_time":"2021-06-28T09:26:45.03371","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:11.040532Z","iopub.execute_input":"2021-07-04T17:01:11.040873Z","iopub.status.idle":"2021-07-04T17:01:16.812721Z","shell.execute_reply.started":"2021-07-04T17:01:11.040841Z","shell.execute_reply":"2021-07-04T17:01:16.810391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xtest_id, Xtest_mask = sent_encode(test_list, tokenizer)\n\nXtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\nXtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\n    \nprint(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape}\")","metadata":{"papermill":{"duration":0.120328,"end_time":"2021-06-28T09:27:00.980379","exception":false,"start_time":"2021-06-28T09:27:00.860051","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:16.81435Z","iopub.execute_input":"2021-07-04T17:01:16.814716Z","iopub.status.idle":"2021-07-04T17:01:16.83567Z","shell.execute_reply.started":"2021-07-04T17:01:16.814678Z","shell.execute_reply":"2021-07-04T17:01:16.834829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base Model\n\n* **DistilRoberta Embeddings**\n* **Conv1D + Bidirectional LSTM**","metadata":{"papermill":{"duration":0.075942,"end_time":"2021-06-28T09:27:01.131133","exception":false,"start_time":"2021-06-28T09:27:01.055191","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def rmse_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, dtype=tf.float32)\n    y_pred = tf.cast(y_pred, dtype=tf.float32)\n    return tf.math.sqrt(tf.math.reduce_mean((y_true - y_pred)**2))","metadata":{"papermill":{"duration":0.081972,"end_time":"2021-06-28T09:27:01.288365","exception":false,"start_time":"2021-06-28T09:27:01.206393","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:17.880459Z","iopub.execute_input":"2021-07-04T17:01:17.880782Z","iopub.status.idle":"2021-07-04T17:01:17.885099Z","shell.execute_reply.started":"2021-07-04T17:01:17.880753Z","shell.execute_reply":"2021-07-04T17:01:17.8843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distilroberta_model(transformer_model):\n    \n    input_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n\n    embed = transformer_model(input_id, attention_mask=attention_mask)[0]    \n    embed = LayerNormalization(name='Embedding')(embed)\n    \n    x = WeightNormalization(\n            Conv1D(filters=384, kernel_size=5, \n                   strides=2, padding='same', \n                   kernel_regularizer=l2(0.0001),\n                   kernel_initializer='he_uniform'))(embed)\n    x = LayerNormalization()(x)\n    x = Activation('relu')(x)\n    x = SpatialDropout1D(rate=0.25)(x)\n    \n    x = WeightNormalization(\n            Conv1D(filters=192, kernel_size=5, \n                   strides=2, padding='same', \n                   kernel_regularizer=l2(0.0001),\n                   kernel_initializer='he_uniform'))(x)\n    x = LayerNormalization()(x)\n    x = Activation('relu')(x)\n    x = SpatialDropout1D(rate=0.25)(x)\n    \n    x = Flatten()(x)\n    x = Dropout(rate=0.5)(x)\n    \n    x = Dense(units=1, kernel_initializer='lecun_normal')(x)\n\n    model = Model(inputs=[input_id, attention_mask], outputs=x, \n                  name='Pretrained_DistilRoberta_Model')\n    return model","metadata":{"papermill":{"duration":0.086393,"end_time":"2021-06-28T09:27:01.449226","exception":false,"start_time":"2021-06-28T09:27:01.362833","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:19.297513Z","iopub.execute_input":"2021-07-04T17:01:19.297832Z","iopub.status.idle":"2021-07-04T17:01:19.307975Z","shell.execute_reply.started":"2021-07-04T17:01:19.297804Z","shell.execute_reply":"2021-07-04T17:01:19.306866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(distilroberta_embeddings, stat_input, seed=0):\n    \n    x1 = Bidirectional(LSTM(units=96, activation='tanh',\n                            return_sequences=True, dropout=0.25,\n                            kernel_regularizer=l2(0.0001),\n                            kernel_initializer=LecunNormal(seed=seed)))(distilroberta_embeddings)\n    x1 = LayerNormalization(epsilon=1e-5)(x1)\n    \n    x2 = Conv1D(filters=96, kernel_size=5, \n                strides=2, padding='same', \n                kernel_regularizer=l2(0.0003),\n                kernel_initializer=LecunNormal(seed=seed))(x1)\n    x2 = Activation('relu')(x2)\n    x2 = LayerNormalization(epsilon=1e-5)(x2)\n    x2 = SpatialDropout1D(rate=0.25)(x2)\n    \n    x2 = Conv1D(filters=192, kernel_size=3, \n                strides=2, padding='same', \n                kernel_regularizer=l2(0.0003),\n                kernel_initializer=LecunNormal(seed=seed))(x2)\n    x2 = Activation('relu')(x2)\n    x2 = LayerNormalization(epsilon=1e-5)(x2)\n    x2 = SpatialDropout1D(rate=0.25)(x2)\n    \n    attn = Attention()([x2, x1])\n    \n    avg_pool2 = GlobalAveragePooling1D()(x2)\n    avg_pool3 = GlobalAveragePooling1D()(attn)\n    \n    max_pool2 = GlobalMaxPool1D()(x2)\n    max_pool3 = GlobalMaxPool1D()(attn)\n    \n    x3 = Dense(units=32, kernel_initializer=LecunNormal(seed=seed), \n               kernel_regularizer=l2(0.0003))(stat_input)\n    x3 = Activation('relu')(x3)\n    x3 = BatchNormalization()(x3)\n    \n    x4 = Concatenate()([avg_pool2, avg_pool3, max_pool2, max_pool3])\n    x4 = BatchNormalization()(x4)\n    x5 = Dropout(rate=0.35)(x4)\n    \n    x5 = Dense(units=96, kernel_initializer=LecunNormal(seed=seed), \n               kernel_regularizer=l2(0.0003))(x5)\n    x5 = Activation('relu')(x5)\n    x5 = BatchNormalization()(x5)\n    \n    x = Concatenate()([x5, x3, x4])\n    x = BatchNormalization()(x)\n    x = Dropout(rate=0.35)(x)\n    \n    x = Dense(units=24, kernel_initializer=LecunNormal(seed=seed), \n              kernel_regularizer=l2(0.0001))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(rate=0.15)(x)\n    \n    x = Dense(units=1, kernel_initializer=LecunNormal(seed=seed))(x)\n    \n    return x","metadata":{"papermill":{"duration":0.091225,"end_time":"2021-06-28T09:27:01.616237","exception":false,"start_time":"2021-06-28T09:27:01.525012","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:21.582264Z","iopub.execute_input":"2021-07-04T17:01:21.582632Z","iopub.status.idle":"2021-07-04T17:01:21.595469Z","shell.execute_reply.started":"2021-07-04T17:01:21.582599Z","shell.execute_reply":"2021-07-04T17:01:21.594512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = RobertaConfig.from_pretrained(DISTILROBERTA_BASE)\nconfig.output_hidden_states = False\ntransformer_model = TFRobertaModel.from_pretrained(DISTILROBERTA_BASE, config=config)","metadata":{"papermill":{"duration":11.423843,"end_time":"2021-06-28T09:27:13.11528","exception":false,"start_time":"2021-06-28T09:27:01.691437","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:25.120876Z","iopub.execute_input":"2021-07-04T17:01:25.121217Z","iopub.status.idle":"2021-07-04T17:01:36.484993Z","shell.execute_reply.started":"2021-07-04T17:01:25.121186Z","shell.execute_reply":"2021-07-04T17:01:36.484215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_model = distilroberta_model(transformer_model)\npretrained_model.trainable = False","metadata":{"papermill":{"duration":5.362036,"end_time":"2021-06-28T09:27:18.553981","exception":false,"start_time":"2021-06-28T09:27:13.191945","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:36.486544Z","iopub.execute_input":"2021-07-04T17:01:36.486892Z","iopub.status.idle":"2021-07-04T17:01:41.973479Z","shell.execute_reply.started":"2021-07-04T17:01:36.486856Z","shell.execute_reply":"2021-07-04T17:01:41.972628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stat_input = Input(shape=(Xtrain.shape[1],), dtype=tf.int32, name=\"numerical_input\")\nmodel = Model(inputs=[pretrained_model.input, stat_input], \n              outputs=build_model(pretrained_model.get_layer('Embedding').output, stat_input),\n              name='CommonLit_Readability_Model')\nmodel.summary()","metadata":{"papermill":{"duration":0.70695,"end_time":"2021-06-28T09:27:19.339298","exception":false,"start_time":"2021-06-28T09:27:18.632348","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:41.974935Z","iopub.execute_input":"2021-07-04T17:01:41.975241Z","iopub.status.idle":"2021-07-04T17:01:42.569146Z","shell.execute_reply.started":"2021-07-04T17:01:41.975207Z","shell.execute_reply":"2021-07-04T17:01:42.568336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(\n    model, to_file='./CommonLit_Readability_Model.png', \n    show_shapes=True, show_layer_names=True\n)","metadata":{"execution":{"iopub.execute_input":"2021-06-28T09:27:19.49967Z","iopub.status.busy":"2021-06-28T09:27:19.498859Z","iopub.status.idle":"2021-06-28T09:27:20.325299Z","shell.execute_reply":"2021-06-28T09:27:20.325736Z"},"papermill":{"duration":0.907885,"end_time":"2021-06-28T09:27:20.325883","exception":false,"start_time":"2021-06-28T09:27:19.417998","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLD = 5\nVERBOSE = 1\nMINI_BATCH_SIZE = 16\nSEEDS = [2020]\n\ncounter = 0\noof_score1 = 0\noof_score2 = 0\ny_pred_final = 0\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score1 = 0\n    seed_score2 = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain, Ytrain_strat)):\n        counter += 1\n\n        train_x_id, train_x_mask = Xtrain_id[train], Xtrain_mask[train]\n        val_x_id, val_x_mask = Xtrain_id[val], Xtrain_mask[val]\n        train_x2, val_x2 = Xtrain.iloc[train], Xtrain.iloc[val]\n        train_y, val_y = Ytrain[train], Ytrain[val]\n        \n        \n        #==================================================================\n        #                 Load pretrained DistilBert Model\n        #==================================================================\n        \n        pretrained_model = distilroberta_model(transformer_model)\n        pretrained_model.load_weights(f'../input/commonlit-roberta-variants-p1/DistilRoberta-Base/CLRP_DistilRoberta_Base_{(idx+1)}C.h5')\n        pretrained_model.trainable = False\n        \n        \n        #==================================================================\n        #                              Model-1\n        #==================================================================\n        \n        tf.random.set_seed(seed+idx+13)\n\n        stat_input = Input(shape=(Xtrain.shape[1],), dtype=tf.int32, name=\"numerical_input\")\n        model1 = Model(inputs=[pretrained_model.input, stat_input], \n                       outputs=build_model(pretrained_model.get_layer('Embedding').output, stat_input, seed+idx+13),\n                       name='CommonLit_Readability_Model')\n        \n        model1.compile(loss=rmse_loss,\n                       metrics=[RootMeanSquaredError(name='rmse')],\n                       optimizer=Adamax(lr=8e-3))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=9, verbose=VERBOSE)\n\n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.25, \n                                      min_lr=1e-6, patience=4, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'./CommonLit_Readability_Model1_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model1.fit(\n            [train_x_id, train_x_mask, train_x2], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=50, \n            verbose=VERBOSE, \n            workers=5,\n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x2], val_y)\n        )\n        \n        model1.load_weights(f'./CommonLit_Readability_Model1_{counter}C.h5')\n\n        y_pred = model1.predict([val_x_id, val_x_mask, val_x2])\n        score1 = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score1 += score1\n        seed_score1 += score1\n        print(\"Model-1 | Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score1))\n        \n        \n        #==================================================================\n        #                              Model-2\n        #==================================================================\n        \n        tf.random.set_seed(seed+idx+23)\n        \n        stat_input = Input(shape=(Xtrain.shape[1],), dtype=tf.int32, name=\"numerical_input\")\n        model2 = Model(inputs=[pretrained_model.input, stat_input], \n                       outputs=build_model(pretrained_model.get_layer('Embedding').output, stat_input, seed+idx+23),\n                       name='CommonLit_Readability_Model')\n        \n        model2.compile(loss=rmse_loss,\n                       metrics=[RootMeanSquaredError(name='rmse')],\n                       optimizer=Adamax(lr=8e-3))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=9, verbose=VERBOSE)\n\n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.25, \n                                      min_lr=1e-6, patience=4, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'./CommonLit_Readability_Model2_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model2.fit(\n            [train_x_id, train_x_mask, train_x2], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=50, \n            verbose=VERBOSE, \n            workers=5,\n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x2], val_y)\n        )\n        \n        model2.load_weights(f'./CommonLit_Readability_Model2_{counter}C.h5')\n\n        y_pred = model2.predict([val_x_id, val_x_mask, val_x2])\n        score2 = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score2 += score2\n        seed_score2 += score2\n        print(\"Model-2 | Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score2))\n        \n        if score1 > score2:\n            y_pred_final += model2.predict([Xtest_id, Xtest_mask, Xtest])\n            print(\"Predictions made using Model-2\\n\")\n        \n        elif score1 < score2:\n            y_pred_final += model1.predict([Xtest_id, Xtest_mask, Xtest])\n            print(\"Predictions made using Model-1\\n\")\n        \n        else:\n            y_pred_final += model1.predict([Xtest_id, Xtest_mask, Xtest])\n            print(\"Predictions made using Model-1\\n\")\n    \n    print(\"\\nModel-1 | Seed: {} | Aggregate OOF Score: {}\".format(seed, (seed_score1 / FOLD)))\n    print(\"Model-2 | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score2 / FOLD)))\n\n\ny_pred_final = y_pred_final / float(counter)\noof_score1 /= float(counter)\noof_score2 /= float(counter)\noof_score = (oof_score1 + oof_score2) / 2.0\nprint(\"Model-1 OOF Score: {}\".format(oof_score1))\nprint(\"Model-2 OOF Score: {}\".format(oof_score2))\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","metadata":{"papermill":{"duration":4468.244736,"end_time":"2021-06-28T10:41:48.661917","exception":false,"start_time":"2021-06-28T09:27:20.417181","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-07-04T17:01:46.602642Z","iopub.execute_input":"2021-07-04T17:01:46.602992Z","iopub.status.idle":"2021-07-04T17:03:48.349455Z","shell.execute_reply.started":"2021-07-04T17:01:46.602961Z","shell.execute_reply":"2021-07-04T17:03:48.347195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create submission file","metadata":{"papermill":{"duration":0.095795,"end_time":"2021-06-28T10:41:48.846245","exception":false,"start_time":"2021-06-28T10:41:48.75045","status":"completed"},"tags":[]}},{"cell_type":"code","source":"submit_df = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmit_df['target'] = y_pred_final\nsubmit_df.to_csv(\"./submission.csv\", index=False)\nsubmit_df.head()","metadata":{"execution":{"iopub.execute_input":"2021-06-28T10:41:49.07905Z","iopub.status.busy":"2021-06-28T10:41:49.078547Z","iopub.status.idle":"2021-06-28T10:41:49.256627Z","shell.execute_reply":"2021-06-28T10:41:49.256149Z"},"papermill":{"duration":0.275725,"end_time":"2021-06-28T10:41:49.256761","exception":false,"start_time":"2021-06-28T10:41:48.981036","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.08736,"end_time":"2021-06-28T10:41:49.431946","exception":false,"start_time":"2021-06-28T10:41:49.344586","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}