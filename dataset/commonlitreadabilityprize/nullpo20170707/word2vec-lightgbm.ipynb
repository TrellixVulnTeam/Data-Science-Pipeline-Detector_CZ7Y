{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**I wrote this while looking at https://www.kaggle.com/ruchi798/commonlit-readability-prize-eda-baseline., https://www.kaggle.com/syurenuko/clrp-word2vec-lightgbm-baseline**\n\n**datasets: https://www.kaggle.com/alvaromunoz/textstat**\n\n**datasets: https://www.kaggle.com/nltkdata/stopwords**\n\n**How to https://stackoverflow.com/questions/3522372/how-to-config-nltk-data-directory-from-code**","metadata":{}},{"cell_type":"code","source":"!pip install ../input/textstat/Pyphen-0.10.0-py3-none-any.whl\n!pip install ../input/textstat/textstat-0.7.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:26:49.72365Z","iopub.execute_input":"2021-07-05T10:26:49.724001Z","iopub.status.idle":"2021-07-05T10:26:56.935141Z","shell.execute_reply.started":"2021-07-05T10:26:49.723968Z","shell.execute_reply":"2021-07-05T10:26:56.933976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport re\nimport nltk\nimport textstat\nimport time\nimport spacy\nimport gensim\n\n\nnltk.data.path.append(\"/kaggle/input/stopwords/stopwords\")\n\nfrom pandas import DataFrame\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom collections import Counter\nfrom wordcloud import WordCloud,STOPWORDS\nfrom spacy import displacy\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport optuna\n#nltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:26:56.937045Z","iopub.execute_input":"2021-07-05T10:26:56.937379Z","iopub.status.idle":"2021-07-05T10:26:56.944114Z","shell.execute_reply.started":"2021-07-05T10:26:56.937343Z","shell.execute_reply":"2021-07-05T10:26:56.942916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\nsample_submission = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:26:56.946374Z","iopub.execute_input":"2021-07-05T10:26:56.946748Z","iopub.status.idle":"2021-07-05T10:26:57.003026Z","shell.execute_reply.started":"2021-07-05T10:26:56.946709Z","shell.execute_reply":"2021-07-05T10:26:57.002089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"excerpt1 = train_df['excerpt'].min()\n\ne = re.sub(\"[^a-zA-Z]\", \" \", excerpt1)\ne = e.lower()\n        \ne = nltk.word_tokenize(e)\n        \ne = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \nlemma = nltk.WordNetLemmatizer()\ne = [lemma.lemmatize(word) for word in e]\ne=\" \".join(e)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:26:57.004915Z","iopub.execute_input":"2021-07-05T10:26:57.005308Z","iopub.status.idle":"2021-07-05T10:26:57.037568Z","shell.execute_reply.started":"2021-07-05T10:26:57.005267Z","shell.execute_reply":"2021-07-05T10:26:57.036636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#====== Preprocessing function ======\ndef preprocess(data):\n    excerpt_processed=[]\n    for e in data['excerpt']:\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed ","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:26:57.039617Z","iopub.execute_input":"2021-07-05T10:26:57.039937Z","iopub.status.idle":"2021-07-05T10:26:57.046206Z","shell.execute_reply.started":"2021-07-05T10:26:57.039909Z","shell.execute_reply":"2021-07-05T10:26:57.044786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[\"excerpt_preprocessed\"] = preprocess(train_df)\ntest_df[\"excerpt_preprocessed\"] = preprocess(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T10:26:57.047854Z","iopub.execute_input":"2021-07-05T10:26:57.048354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def avg_feature_vector(sentence, model, num_features):\n    words = sentence.replace('\\n',\" \").replace(',',' ').replace('.',\" \").split()\n    feature_vec = np.zeros((num_features,),dtype=\"float32\")#特徴ベクトルの初期化\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, model[word])\n        except KeyError as error:\n            feature_vec \n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Part-of-Speech tagging**","metadata":{"execution":{"iopub.status.busy":"2021-06-05T12:53:35.651889Z","iopub.execute_input":"2021-06-05T12:53:35.652325Z","iopub.status.idle":"2021-06-05T12:53:35.658586Z","shell.execute_reply.started":"2021-06-05T12:53:35.65228Z","shell.execute_reply":"2021-06-05T12:53:35.657116Z"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin', binary=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word2vec_train = np.zeros((len(train_df.index),300),dtype=\"float32\")\nword2vec_test = np.zeros((len(test_df.index),300),dtype=\"float32\")\n\nfor i in range(len(train_df.index)):\n    word2vec_train[i] = avg_feature_vector(train_df[\"excerpt_preprocessed\"][i],word2vec_model, 300)\n    \nfor i in range(len(test_df.index)):\n    word2vec_test[i] = avg_feature_vector(test_df[\"excerpt_preprocessed\"][i],word2vec_model, 300)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optuna + LightGBM\n1. optimize params\n1. cross validation","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"target = train_df['target'].to_numpy()\n\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'seed': 42,\n    'n_jobs': -1\n}\n\n\ndef objective(trial):\n    params = {\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'objective': 'regression',\n        'seed': 42,\n        'n_jobs': -1,\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n    \n    train_x, test_x, train_y, test_y = train_test_split(word2vec_train, target, test_size=0.25, random_state=42)\n    train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.3, random_state=42)\n    \n    train_set = lgb.Dataset(train_x, train_y)\n    valid_set = lgb.Dataset(valid_x, valid_y, reference=train_set)\n    \n    model = lgb.train(params,\n                      train_set,\n                      num_boost_round=10000,\n                      early_stopping_rounds=100,\n                      valid_sets=[train_set, valid_set])\n    \n    for_param_pred = model.predict(test_x)\n    rmse = np.sqrt(mse(test_y, for_param_pred))\n    return rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\n\npred = np.zeros(test_df.shape[0])\n\nfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncv=list(fold.split(word2vec_train, target))\nrmses = []\nfor tr_idx, val_idx in cv: \n    x_tr, x_va = word2vec_train[tr_idx], word2vec_train[val_idx]\n    y_tr, y_va = target[tr_idx], target[val_idx]\n        \n    train_set = lgb.Dataset(x_tr, y_tr)\n    val_set = lgb.Dataset(x_va, y_va, reference=train_set)\n        \n    # Training\n    for_inference_param = {\n                            'boosting_type': 'gbdt',\n                            'metric': 'rmse',\n                            'objective': 'regression',\n                            'seed': 42,\n                            'n_jobs': -1}\n    \n    for_inference_param.update(study.best_params)\n    model = lgb.train(for_inference_param,\n                      train_set,\n                      num_boost_round=10000,\n                      early_stopping_rounds=100,\n                      valid_sets=[train_set, val_set])\n        \n    y_pred = model.predict(x_va)\n    rmse =  np.sqrt(mse(y_va, y_pred))\n    rmses.append(rmse)\n        \n    #Inference\n    test_pred = model.predict(word2vec_test)\n    pred += test_pred / 5  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# submission file","metadata":{"execution":{"iopub.status.busy":"2021-06-06T13:19:06.212547Z","iopub.execute_input":"2021-06-06T13:19:06.213005Z","iopub.status.idle":"2021-06-06T13:19:06.217801Z","shell.execute_reply.started":"2021-06-06T13:19:06.212917Z","shell.execute_reply":"2021-06-06T13:19:06.2166Z"}}},{"cell_type":"code","source":"sample_submission.target = pred\nsample_submission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}