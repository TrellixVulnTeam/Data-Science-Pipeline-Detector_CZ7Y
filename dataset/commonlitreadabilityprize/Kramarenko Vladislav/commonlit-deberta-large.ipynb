{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notes","metadata":{}},{"cell_type":"markdown","source":"This kernel if only for inference, the training one is on its road.  \nThe experimental model is **roBERTa**. But, as we're using the **huggingface**'s **AutoModel** interface, you can easily choose whatever you want.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, optim\nfrom  torch.utils.data import Dataset, DataLoader, Sampler\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\nfrom torch.cuda.amp import autocast, GradScaler\nfrom matplotlib import pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, AdaBoostRegressor\nimport os, random, gc\nimport re, time, json, pickle\nfrom catboost import CatBoostRegressor,CatBoostClassifier\nimport spacy\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge, SGDRegressor,BayesianRidge\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVR,SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:15.19869Z","iopub.execute_input":"2021-08-03T08:04:15.199174Z","iopub.status.idle":"2021-08-03T08:04:17.907414Z","shell.execute_reply.started":"2021-08-03T08:04:15.199076Z","shell.execute_reply":"2021-08-03T08:04:17.906517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nfrom  torch.utils.data import Dataset, DataLoader\nimport math\nimport pickle\nimport multiprocessing\nimport more_itertools\n\nfrom tqdm.notebook import tqdm\nfrom transformers import AdamW,get_cosine_schedule_with_warmup\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification, AutoModelForSequenceClassification,AutoModel","metadata":{"id":"2dt7oG43VAqc","execution":{"iopub.status.busy":"2021-08-03T08:04:17.909468Z","iopub.execute_input":"2021-08-03T08:04:17.909824Z","iopub.status.idle":"2021-08-03T08:04:21.801064Z","shell.execute_reply.started":"2021-08-03T08:04:17.909777Z","shell.execute_reply":"2021-08-03T08:04:21.800125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_folds(data, num_splits, seed):\n    data[\"fold\"] = -1\n    kf = KFold(n_splits=num_splits, random_state=seed, shuffle=True)\n    for fold, (_, val_set) in enumerate(kf.split(np.arange(len(data)))):\n        data.loc[val_set, \"fold\"] = fold\n    return data   \ndef create_folds2(data, num_splits, seed):\n    # we create a new column called kfold and fill it with -1\n    data[\"fold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=seed)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'fold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:21.803037Z","iopub.execute_input":"2021-08-03T08:04:21.803462Z","iopub.status.idle":"2021-08-03T08:04:21.812688Z","shell.execute_reply.started":"2021-08-03T08:04:21.80342Z","shell.execute_reply":"2021-08-03T08:04:21.811712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CV = 5\nMODEL_ROOT = Path(\".\")\nMODE = 'my'\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nTRAIN_NUM_WORKERS = 2\nVAL_NUM_WORKERS = 2\nprint(\"Device:\", DEVICE)\nNUM_EPOCHS = 4\nBATCH_SIZE = 6\nMAX_LEN = 256\nEVAL_SCHEDULE = [(0.52, 256),(0.50, 16), (0.48, 8), (0.46, 4), (0.44, 2), (-1., 1)]\nROBERTA_PATH = \"../input/huggingface-deberta-variants/deberta-large/deberta-large\"#480\nTOKENIZER_PATH = \"../input/huggingface-deberta-variants/deberta-large/deberta-large\"\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nTOKENIZER = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:21.814206Z","iopub.execute_input":"2021-08-03T08:04:21.814559Z","iopub.status.idle":"2021-08-03T08:04:22.157553Z","shell.execute_reply.started":"2021-08-03T08:04:21.81452Z","shell.execute_reply":"2021-08-03T08:04:22.156566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:22.158873Z","iopub.execute_input":"2021-08-03T08:04:22.159282Z","iopub.status.idle":"2021-08-03T08:04:22.164994Z","shell.execute_reply.started":"2021-08-03T08:04:22.15923Z","shell.execute_reply":"2021-08-03T08:04:22.163884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n\n# Remove incomplete entries if any.\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\n\ntest_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n\nwiki = pd.read_csv(\"../input/commonlit/wiki2.csv\").iloc[:10000]\nwiki['standard_error'] = 1\n\n\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:22.166615Z","iopub.execute_input":"2021-08-03T08:04:22.166984Z","iopub.status.idle":"2021-08-03T08:04:22.429119Z","shell.execute_reply.started":"2021-08-03T08:04:22.166948Z","shell.execute_reply":"2021-08-03T08:04:22.428111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitDataset(Dataset):\n    def __init__(self, df,tokenizer = TOKENIZER, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n            self.std = torch.tensor(df.standard_error.values, dtype=torch.float32)\n     \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            #target = torch.normal(self.target[index], self.std[index]/5)#.astype(np.float32)\n            return (input_ids, attention_mask, target)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:22.430803Z","iopub.execute_input":"2021-08-03T08:04:22.431361Z","iopub.status.idle":"2021-08-03T08:04:22.441629Z","shell.execute_reply.started":"2021-08-03T08:04:22.43125Z","shell.execute_reply":"2021-08-03T08:04:22.440517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n        \n        #freeze_embedding = False\n        #self.roberta.base_model.embeddings.requires_grad_(not freeze_embedding)\n        \n        #768\n        self.attention = nn.Sequential(            \n            nn.Linear(1024, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )     \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(1024, 1)                        \n        )\n        \n    def freeze(self):\n        for param in self.roberta.parameters():\n            param.requires_grad = False\n      \n    def unfreeze(self):\n        for param in self.roberta.parameters():\n            param.requires_grad = True\n            \n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        #last_layer_hidden_states2 = roberta_output.hidden_states[-2]#last_hidden_state\n        #last_layer_hidden_states = torch.cat((last_layer_hidden_states, last_layer_hidden_states2), 2)\n\n        weights = self.attention(last_layer_hidden_states)\n        y = torch.sum(weights * last_layer_hidden_states, dim=1)        \n\n\n        #y = last_layer_hidden_states[:,0]\n        #y = torch.mean(last_layer_hidden_states, 1)\n        \n        # Now we reduce the context vector to the prediction score.\n        return (y,self.regressor(y))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:22.444992Z","iopub.execute_input":"2021-08-03T08:04:22.44543Z","iopub.status.idle":"2021-08-03T08:04:22.45874Z","shell.execute_reply.started":"2021-08-03T08:04:22.445384Z","shell.execute_reply":"2021-08-03T08:04:22.457858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_mse(model, data_loader):\n    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n    model.eval()            \n    mse_sum = 0\n\n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)                        \n            target = target.to(DEVICE)           \n            \n            pred = model(input_ids, attention_mask)[1]                      \n\n            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n                \n\n    return mse_sum / len(data_loader.dataset)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:22.4603Z","iopub.execute_input":"2021-08-03T08:04:22.460803Z","iopub.status.idle":"2021-08-03T08:04:22.471178Z","shell.execute_reply.started":"2021-08-03T08:04:22.460754Z","shell.execute_reply":"2021-08-03T08:04:22.470397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    preds1 = []\n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n            preds1.append(pred[0].cpu().numpy())\n            result[index : index + pred[1].shape[0]] = pred[1].flatten().to(\"cpu\")\n            index += pred[1].shape[0]\n    preds1 = np.concatenate(preds1)\n    return preds1, result","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:22.4737Z","iopub.execute_input":"2021-08-03T08:04:22.473962Z","iopub.status.idle":"2021-08-03T08:04:22.484501Z","shell.execute_reply.started":"2021-08-03T08:04:22.473936Z","shell.execute_reply":"2021-08-03T08:04:22.483614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, model_path, train_loader, val_loader,\n          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n    best_val_rmse = None\n    best_epoch = 0\n    step = 0\n    last_eval_step = 0\n    eval_period = EVAL_SCHEDULE[0][1]    \n    scaler = GradScaler()\n    start = time.time()\n    #model.unfreeze()\n \n    for epoch in range(num_epochs):                           \n        val_rmse = None         \n\n        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)            \n            target = target.to(DEVICE)                        \n\n            optimizer.zero_grad()\n            \n            model.train()\n            #with autocast():\n            pred = model(input_ids, attention_mask)[1]\n            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n                        \n            mse.backward()\n            optimizer.step()\n            #scaler.scale(mse).backward()\n            #scaler.step(optimizer)\n            #scaler.update()            \n            \n            if scheduler:\n                scheduler.step()\n            \n            if step >= last_eval_step + eval_period:\n                # Evaluate the model on val_loader.\n                elapsed_seconds = time.time() - start\n                num_steps = step - last_eval_step\n                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n                last_eval_step = step\n                \n                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n\n                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n                      f\"val_rmse: {val_rmse:0.4}\",f\"lr: {scheduler.get_lr()[0]}\")\n                if epoch>0:\n                    for rmse, period in EVAL_SCHEDULE:\n                        if val_rmse >= rmse:\n                            eval_period = period\n                            break                               \n                \n                if not best_val_rmse or val_rmse < best_val_rmse:                    \n                    best_val_rmse = val_rmse\n                    best_epoch = epoch\n                    torch.save(model.state_dict(), model_path)\n                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n                    if epoch >= 20:\n                        print('Model Frozen -> Train Classifier Only')\n                        model.freeze()           \n                else:       \n                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n                          f\"(from epoch {best_epoch})\")                                    \n                    \n                start = time.time()\n                                            \n            step += 1\n                        \n    \n    return best_val_rmse","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:22.487819Z","iopub.execute_input":"2021-08-03T08:04:22.488133Z","iopub.status.idle":"2021-08-03T08:04:22.501359Z","shell.execute_reply.started":"2021-08-03T08:04:22.488086Z","shell.execute_reply":"2021-08-03T08:04:22.500394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_optimizer(model):\n    LR = 1e-4\n    named_parameters = list(model.named_parameters())    \n    #print(model)\n    print(len(named_parameters))\n    \n    roberta_parameters = named_parameters[:-5]    \n    attention_parameters = named_parameters[-5:-1]\n    regressor_parameters = named_parameters[-1:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = LR/20\n        \"\"\"\n        if layer_num >= 50:        \n            lr = LR/10\n\n        if layer_num >= 100:\n            lr = LR/5\n        \n        if layer_num >= 150:\n            lr = LR/2\n        \"\"\"\n        if layer_num >= 100:        \n            lr = LR/10\n\n        if layer_num >= 200:\n            lr = LR/5\n        \n        if layer_num >= 300:\n            lr = LR/2\n        \n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return AdamW(parameters,lr=5e-4)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:22.50439Z","iopub.execute_input":"2021-08-03T08:04:22.504684Z","iopub.status.idle":"2021-08-03T08:04:22.515116Z","shell.execute_reply.started":"2021-08-03T08:04:22.504654Z","shell.execute_reply":"2021-08-03T08:04:22.51414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(test_df)>-7:\n    gc.collect()\n    set_random_seed(0)\n    list_val_rmse = []\n  \n    print(\"model_pretrain\")\n    model_path = \"model_pretrain.pt\"\n    \n   \n    train_dataset = LitDataset(wiki,tokenizer = TOKENIZER,)    \n    val_dataset = LitDataset(train_df,tokenizer = TOKENIZER,)    \n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              drop_last=True, shuffle=True, num_workers=8)  \n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*6,\n                            drop_last=False, shuffle=False, num_workers=8)   \n\n    model = LitModel().to(DEVICE)\n    \n    optimizer = create_optimizer(model)                        \n    \n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_training_steps= len(train_loader),\n        num_warmup_steps=len(train_loader)//3)    \n    \n    #scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=3)\n    train(model, model_path, train_loader,\n                                val_loader, optimizer, scheduler=scheduler,\n                                num_epochs=1)\n\n    del model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:04:22.516429Z","iopub.execute_input":"2021-08-03T08:04:22.517177Z","iopub.status.idle":"2021-08-03T08:15:34.200101Z","shell.execute_reply.started":"2021-08-03T08:04:22.51707Z","shell.execute_reply":"2021-08-03T08:15:34.197866Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EVAL_SCHEDULE = [(0.51, 32),(0.48, 16), (0.45, 8), (0.43, 4), (0.4, 2), (-1., 1)]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:16:07.888688Z","iopub.execute_input":"2021-08-03T08:16:07.889023Z","iopub.status.idle":"2021-08-03T08:16:07.89543Z","shell.execute_reply.started":"2021-08-03T08:16:07.88899Z","shell.execute_reply":"2021-08-03T08:16:07.894476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(test_df)>-7:\n    gc.collect()\n    folds = [0]\n    list_val_rmse = []\n    for seed in [0,1,2,3,4]:\n        set_random_seed(seed)\n        train_df = create_folds2(train_df, num_splits=CV,seed = seed)\n        fold_bar = tqdm(train_df.reset_index(drop=True).reset_index().groupby(\"fold\").index.apply(list).items(), total=train_df.fold.max()+1)\n        \n\n        for fold, val_set in fold_bar:\n            if folds and not fold in folds:\n                continue            \n            print(f\"\\n_seed{seed} Fold {fold + 1}/{len(folds)}\")\n            model_path = f\"model_{fold + 1}_seed{seed}.pth\"\n            \n\n            train_set = np.setdiff1d(train_df.index, val_set)\n            \n            train_dataset = LitDataset(train_df.iloc[train_set],tokenizer = TOKENIZER,)    \n            val_dataset = LitDataset(train_df.iloc[val_set],tokenizer = TOKENIZER,)    \n\n            train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                                      drop_last=True, shuffle=True, num_workers=2)  \n            val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE*6,\n                                    drop_last=False, shuffle=False, num_workers=2)   \n\n            model = LitModel().to(DEVICE)\n            \n            model.load_state_dict(torch.load(\"./model_pretrain.pt\"))    \n            model.to(DEVICE)     \n            \"\"\"\n            reinit_layers = 2\n            if reinit_layers > 0:\n                print(f'Reinitializing Last {reinit_layers} Layers ...')\n                encoder_temp = getattr(model, 'roberta')\n                for layer in encoder_temp.encoder.layer[-reinit_layers:]:\n                    for module in layer.modules():\n                        if isinstance(module, nn.Linear):\n                            module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n                            if module.bias is not None:\n                                module.bias.data.zero_()\n                        elif isinstance(module, nn.Embedding):\n                            module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n                            if module.padding_idx is not None:\n                                module.weight.data[module.padding_idx].zero_()\n                        elif isinstance(module, nn.LayerNorm):\n                            module.bias.data.zero_()\n                            module.weight.data.fill_(1.0)\n                print('Done.!')\n            \"\"\"   \n            optimizer = create_optimizer(model)                        \n            \n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer,\n                num_training_steps=NUM_EPOCHS * len(train_loader),\n                num_warmup_steps=len(train_loader)//2)    \n            \n            #scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=3)\n            list_val_rmse.append(train(model, model_path, train_loader,\n                                       val_loader, optimizer, scheduler=scheduler))\n\n            del model\n            gc.collect()\n\n            print(\"\\nPerformance estimates:\")\n            print(list_val_rmse)\n            print(\"Mean:\", np.array(list_val_rmse).mean())","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:16:10.006163Z","iopub.execute_input":"2021-08-03T08:16:10.006601Z","iopub.status.idle":"2021-08-03T08:16:25.592536Z","shell.execute_reply.started":"2021-08-03T08:16:10.006567Z","shell.execute_reply":"2021-08-03T08:16:25.590145Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds9 = 0\nif (MODE == 'all' or MODE == \"my\") and len(test_df)>-7:#\n    #checkpoint_paths = list(Path(f\"../input/vlomme-deberta-large\").glob(\"*.pth\"))\n    checkpoint_paths = list(Path(f\"./\").glob(\"*.pth\"))\n    print(checkpoint_paths)\n    preds9 = []\n\n    test_dataset = LitDataset(test_df,tokenizer = TOKENIZER, inference_only=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE*3,\n                             drop_last=False, shuffle=False, num_workers=2)\n\n    for i,model_path in enumerate(checkpoint_paths):            \n        print(f\"\\nUsing {model_path}\")\n        \n        model = LitModel()\n        model.load_state_dict(torch.load(model_path))    \n        model.to(DEVICE)\n\n        _,preds = predict(model, test_loader)\n        preds = preds.T\n        preds9.append(preds)\n\n        \n        del model\n        gc.collect()\n    preds9 = np.array(preds9)\n    preds9 = np.mean(preds9, axis = 0)    \nprint(preds9)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:15:34.204008Z","iopub.status.idle":"2021-08-03T08:15:34.204545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id':test_df.id,'target':preds9})\nsubmission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T08:15:34.205461Z","iopub.status.idle":"2021-08-03T08:15:34.206028Z"},"trusted":true},"execution_count":null,"outputs":[]}]}