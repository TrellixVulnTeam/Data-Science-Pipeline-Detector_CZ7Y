{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Use mlm-scoring(BERT)\nI think it way is very good but I can't do it. So, could you do it !!!!!\nThe code works locally, but apparently I did it forgetting that Internet use is forbidden, so I couldn't do it.\nIt use mlm-scoring.\nhttps://github.com/awslabs/mlm-scoring","metadata":{}},{"cell_type":"code","source":"pip install ../input/mlmscoring","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-01T02:36:24.138325Z","iopub.execute_input":"2021-08-01T02:36:24.13865Z","iopub.status.idle":"2021-08-01T02:36:48.28491Z","shell.execute_reply.started":"2021-08-01T02:36:24.138612Z","shell.execute_reply":"2021-08-01T02:36:48.284073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import mxnet as mx\nimport pandas as pd\nimport lightgbm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom mlm.scorers import MLMScorer, MLMScorerPT, LMScorer\nfrom mlm.models import get_pretrained\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom statistics import mean, median,variance,stdev\nctxs = [mx.gpu()]\n\nmodel, vocab, tokenizer = get_pretrained(ctxs, 'bert-base-en-cased')\nscorer = MLMScorer(model, vocab, tokenizer, ctxs)\n\ntrain_data = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\nX = []\nX1 = []\nfor i in range(len(train_data[\"excerpt\"])):\n    a = scorer.score_sentences(train_data[\"excerpt\"][i])\n    X1.append(a)\ny = train_data[\"target\"].values\nx = {}\nx[\"m\"] = []\nx[\"median\"] = []\nx[\"variance\"] = []\nx[\"stdev\"] = []\nfor i in range(len(X1)):\n    x[\"m\"].append(mean(X1[i]))\n    x[\"median\"].append(median(X1[i]))\n    x[\"variance\"].append(variance(X1[i]))\n    x[\"stdev\"].append(stdev(X1[i]))\nX = pd.DataFrame(data=x)\n\n# X_train, X_test, y_train, y_test = train_test_split(X.values, y, test_size=0, random_state=1)\nX_train = X.values\ny_train = y\n\nmms = MinMaxScaler()\nX_train_norm = mms.fit_transform(X_train)\n# X_test_norm = mms.fit_transform(X_test)\n\nstdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(X_train_norm)\n# X_test_std = stdsc.transform(X_test_norm)\n# lr = LogisticRegression(penalty='l1', C=1.0, solver='liblinear', multi_class='ovr')\nlr = lightgbm.LGBMRegressor()\nlr.fit(X_train_std, y_train)\n\n\ntest_data = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\nsample_data = pd.read_csv('/kaggle/input/commonlitreadabilityprize/sample_submission.csv')\n\ntest = []\nfor i in range(len(test_data[\"excerpt\"])):\n    a = scorer.score_sentences(test_data[\"excerpt\"][i])\n    test.append(a)\nxT = {}\nxT[\"m\"] = []\nxT[\"median\"] = []\nxT[\"variance\"] = []\nxT[\"stdev\"] = []\nfor i in range(len(test)):\n    xT[\"m\"].append(mean(test[i]))\n    xT[\"median\"].append(median(test[i]))\n    xT[\"variance\"].append(variance(test[i]))\n    xT[\"stdev\"].append(stdev(test[i]))\nTest = pd.DataFrame(data=xT)\nTest_test_norm = mms.fit_transform(Test)\nTest_test_std = stdsc.transform(Test_test_norm)\nsample_data[\"target\"] = lr.predict(Test_test_std)\n\nsample_data.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T03:06:30.405469Z","iopub.execute_input":"2021-08-01T03:06:30.405792Z","iopub.status.idle":"2021-08-01T03:16:24.111728Z","shell.execute_reply.started":"2021-08-01T03:06:30.405762Z","shell.execute_reply":"2021-08-01T03:16:24.110831Z"},"trusted":true},"execution_count":null,"outputs":[]}]}