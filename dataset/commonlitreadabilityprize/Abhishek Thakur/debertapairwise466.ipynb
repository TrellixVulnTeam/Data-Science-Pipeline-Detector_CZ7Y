{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/tez-lib/\")\nimport torch\nfrom sklearn import metrics\nimport transformers\nimport torch.nn as nn\nimport numpy as np\nimport tez\nimport pandas as pd\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-31T11:43:54.172588Z","iopub.execute_input":"2021-07-31T11:43:54.173044Z","iopub.status.idle":"2021-07-31T11:43:56.959748Z","shell.execute_reply.started":"2021-07-31T11:43:54.172956Z","shell.execute_reply":"2021-07-31T11:43:56.958858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttentionHead(nn.Module):\n    def __init__(self, in_size: int = 768, hidden_size: int = 512) -> None:\n        super().__init__()\n        self.W = nn.Linear(in_size, hidden_size)\n        self.V = nn.Linear(hidden_size, 1)\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n        output = self.dropout(context_vector)\n        return output\n\n\nclass CommonlitModel(tez.Model):\n    def __init__(self, model_name, num_train_steps, steps_per_epoch, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.steps_per_epoch = steps_per_epoch\n        self.model_name = model_name\n        self.num_train_steps = num_train_steps\n        self.step_scheduler_after = \"batch\"\n        hidden_dropout_prob: float = 0.0\n        layer_norm_eps: float = 1e-7\n        config = transformers.AutoConfig.from_pretrained(model_name)\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n            }\n        )\n        self.transformer = transformers.AutoModel.from_pretrained(model_name, config=config)\n        self.attention = AttentionHead(in_size=config.hidden_size, hidden_size=config.hidden_size)\n        self.regressor = nn.Linear(config.hidden_size * 2, 2)\n\n    def forward(self, ids1, mask1, ids2, mask2, targets=None):\n        output1 = self.transformer(ids1, mask1)\n        output2 = self.transformer(ids2, mask2)\n        output1 = self.attention(output1.last_hidden_state)\n        output2 = self.attention(output2.last_hidden_state)\n        output = torch.cat((output1, output2), dim=1)\n        output = self.regressor(output)\n        # output = self.regressor(output.pooler_output)\n        return output, 0, {}\n\n\nclass CommonlitDataset:\n    def __init__(self, excerpts, target_dict, tokenizer, max_len, num_samples=None):\n        self.excerpts = excerpts\n        self.target_dict = target_dict\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return len(self.excerpts)\n\n    def __getitem__(self, item):\n        text1 = str(self.excerpts[item][1])\n        text2 = str(self.excerpts[item][0])\n        target = [self.target_dict[text2], self.target_dict[text1]]\n        inputs1 = self.tokenizer(text1, max_length=self.max_len, padding=\"max_length\", truncation=True)\n        inputs2 = self.tokenizer(text2, max_length=self.max_len, padding=\"max_length\", truncation=True)\n        ids1 = inputs1[\"input_ids\"]\n        mask1 = inputs1[\"attention_mask\"]\n        ids2 = inputs2[\"input_ids\"]\n        mask2 = inputs2[\"attention_mask\"]\n        return {\n            \"ids1\": torch.tensor(ids1, dtype=torch.long),\n            \"mask1\": torch.tensor(mask1, dtype=torch.long),\n            \"ids2\": torch.tensor(ids2, dtype=torch.long),\n            \"mask2\": torch.tensor(mask2, dtype=torch.long),\n            \"targets\": torch.tensor(target, dtype=torch.float),\n        }","metadata":{"execution":{"iopub.status.busy":"2021-07-31T11:43:59.738301Z","iopub.execute_input":"2021-07-31T11:43:59.738672Z","iopub.status.idle":"2021-07-31T11:43:59.758603Z","shell.execute_reply.started":"2021-07-31T11:43:59.738629Z","shell.execute_reply":"2021-07-31T11:43:59.757664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 256\nmodel_name = \"microsoft/deberta-large\"\n\ndf = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\nbase_string = df.loc[df.target == 0, \"excerpt\"].values[0]\n\ndf = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ndf[\"target\"] = 0\ntest_pairs = [(base_string, k) for k in df.excerpt.values.tolist()]\ntarget_dict = dict(zip(df.excerpt.values.tolist(), df.target.values.tolist()))\ntarget_dict[base_string] = 0\n\nmpath = \"../input/huggingface-deberta-variants/deberta-large/deberta-large\"\ntokenizer = transformers.AutoTokenizer.from_pretrained(mpath)\n\nfinal_preds = []\nfor _fold in range(5):\n    test_dataset = CommonlitDataset(\n        excerpts=test_pairs,\n        target_dict=target_dict,\n        tokenizer=tokenizer,\n        max_len=max_len,\n    )\n\n    model = CommonlitModel(\n        model_name=mpath,\n        num_train_steps=1,\n        learning_rate=1,\n        steps_per_epoch=1,\n    )\n\n    model.load(\n        f\"../input/deberta-pairwise-466/{model_name.replace('/','')}__fold_{_fold}.bin\",\n        weights_only=True,\n    )\n\n    temp_final_preds = []\n    for p in tqdm(model.predict(test_dataset, batch_size=64, n_jobs=-1)):\n        temp_preds = p[:, 1].tolist()\n        temp_final_preds.extend(temp_preds)\n\n    final_preds.append(temp_final_preds)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T11:46:34.874598Z","iopub.execute_input":"2021-07-31T11:46:34.874986Z","iopub.status.idle":"2021-07-31T11:48:59.28973Z","shell.execute_reply.started":"2021-07-31T11:46:34.874943Z","shell.execute_reply":"2021-07-31T11:48:59.287601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_preds = np.column_stack(final_preds)\n_preds = np.mean(_preds, axis=1)\nsubmission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\nsubmission.target = _preds\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T11:49:09.928987Z","iopub.execute_input":"2021-07-31T11:49:09.929362Z","iopub.status.idle":"2021-07-31T11:49:10.008323Z","shell.execute_reply.started":"2021-07-31T11:49:09.929325Z","shell.execute_reply":"2021-07-31T11:49:10.007507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T11:49:11.121819Z","iopub.execute_input":"2021-07-31T11:49:11.122182Z","iopub.status.idle":"2021-07-31T11:49:11.149394Z","shell.execute_reply.started":"2021-07-31T11:49:11.122152Z","shell.execute_reply":"2021-07-31T11:49:11.14836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}