{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom stop_words import get_stop_words\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nimport nltk \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-21T15:23:18.169551Z","iopub.execute_input":"2021-05-21T15:23:18.169901Z","iopub.status.idle":"2021-05-21T15:23:18.181375Z","shell.execute_reply.started":"2021-05-21T15:23:18.16987Z","shell.execute_reply":"2021-05-21T15:23:18.180053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all functions \ndef syllable_count(word):\n    word = word.lower()\n    count = 0\n    vowels = \"aeiouy\"\n    if word[0] in vowels:\n        count += 1\n    for index in range(1, len(word)):\n        if word[index] in vowels and word[index - 1] not in vowels:\n            count += 1\n    if word.endswith(\"e\"):\n        count -= 1\n    if count == 0:\n        count += 1\n    return count\n\ndef NounCount(string):\n    is_noun = lambda pos: pos == 'NOUN'\n    tokenized = nltk.word_tokenize(string)\n    nouns = [word for (word, pos) in nltk.pos_tag(tokenized,tagset='universal') if is_noun(pos)]\n    return (len(nouns))\n\ndef VerbCount(string):\n    is_verb = lambda pos: pos == 'VERB'\n    tokenized = nltk.word_tokenize(string)\n    verbs = [word for (word, pos) in nltk.pos_tag(tokenized,tagset='universal') if is_verb(pos)]\n    return (len(verbs))\n\ndef AdjCount(string):\n    is_adj = lambda pos: pos == 'ADJ'\n    tokenized = nltk.word_tokenize(string)\n    adjs = [word for (word, pos) in nltk.pos_tag(tokenized,tagset='universal') if is_adj(pos)]\n    return (len(adjs))\n\ndef PronounCount(string):\n    is_pron = lambda pos: pos == 'PRON'\n    tokenized = nltk.word_tokenize(string)\n    pron = [word for (word, pos) in nltk.pos_tag(tokenized,tagset='universal') if is_pron(pos)]\n    return (len(pron))\n\ndef IndependentClause(string):\n    tockentest = nltk.word_tokenize(string)\n    testlist = [[word,pos] for (word, pos) in nltk.pos_tag(tockentest,tagset='universal')]\n    Clausecounter = 0\n    for i in range(len(testlist)-1):\n        if testlist[i][1] == 'NOUN' and testlist[i+1][1] == 'VERB':\n            Clausecounter +=1\n    return(Clausecounter)\n\ndef leaves(tree):\n    NPCount = 0\n    VPCount = 0\n    \"\"\"Finds NP  and VP in leaf nodes of a chunk tree.\"\"\"\n    for subtree in tree.subtrees(filter = lambda t: t.label() == 'NP'):\n        NPCount += 1\n    for subtree in tree.subtrees(filter = lambda t: t.label() == 'VP'):\n        VPCount += 1\n    return(NPCount,VPCount)\n\ndef Phrases(excerpt):\n    '''\n    Function will take a passage, find the mean Noun Phrases (NP) per sentense, mean Verb Phrases (VP) per sentence, \n    mean phrases per sentence and proportion of sentences without VP\n    Idea from this paper: https://www.cs.utexas.edu/~ml/papers/kate.coling10.pdf\n    '''\n    document = excerpt.replace('\\n','').split('.')\n    document = [x for x in document if len(x) > 1]\n    Results = []\n    for doc in document:\n        tokens = [nltk.word_tokenize(sent) for sent in [doc]]\n        postag = [nltk.pos_tag(sent) for sent in tokens][0]\n        # Rule for NP chunk and VB Chunk\n        grammar = r\"\"\"\n            NBAR:\n                {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n            NP:\n                {<NBAR>}\n                {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n            VP:\n                {<RB.?>*<VB.?>*<JJ>*<VB.?>+<VB>?} # Verbs and Verb Phrases        \n        \"\"\"\n        #Chunking\n        cp = nltk.RegexpParser(grammar)\n\n        # the result is a tree\n        tree = cp.parse(postag)\n        #print(leaves(tree))\n        Results.append(leaves(tree))\n    sents = len(Results)\n    NP = 0\n    VP = 0\n    Miss = 0\n    Total = 0\n    for i in Results:\n        NP += i[0]\n        VP += i[1]\n        if i[1] == 0:\n            Miss +=1\n        Total += NP + VP\n    return pd.Series([NP/sents,VP/sents,Miss/sents,Total/sents])\n\ndef GrunningFog(excerpt):\n    \"\"\"\n    function takes a passage and determines the grade level based on the Grunning Fog index method\n    \"\"\"\n    document = excerpt\n    document = document.replace('\\n',' ').split('.')\n    document = [x for x in document if len(x)>1]\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    words = []\n    ComplexCount = []\n    for sentence in document:\n        tokens = nltk.word_tokenize(sentence)\n        words.append(len(tokens))\n        tokens = [lemmatizer.lemmatize(x) for x in tokens]\n        Complex = [1 if syllable_count(token) >=3 else 0 for token in tokens]\n        ComplexCount.append(np.sum(Complex))\n    ASL = np.mean(words) #Average words per sentence\n    PropComplex = np.sum(ComplexCount)/np.sum(words) #proprtion of complex words (>= 3 sylables)\n    GrunFog = 0.4*(ASL + (100*PropComplex))\n    return(GrunFog)\n\ndef SMOG(excerpt):\n    document = excerpt\n    document = document.replace('\\n',' ').split('.')\n    document = [x for x in document if len(x)>1]\n    words = []\n    ComplexCount = []\n    for sentence in document:\n        tokens = nltk.word_tokenize(sentence)\n        words.append(len(tokens))\n        Complex = [1 if syllable_count(token) >=3 else 0 for token in tokens]\n        ComplexCount.append(np.sum(Complex))\n    SMOGScore = (1.0430 * np.sqrt(np.sum(ComplexCount) * (30/len(words)))) + 3.1291\n    return(SMOGScore)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:38:29.647579Z","iopub.execute_input":"2021-05-21T16:38:29.647976Z","iopub.status.idle":"2021-05-21T16:38:29.67941Z","shell.execute_reply.started":"2021-05-21T16:38:29.647945Z","shell.execute_reply":"2021-05-21T16:38:29.678554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainSet = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\ntestSet = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\nsample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:23:23.80827Z","iopub.execute_input":"2021-05-21T15:23:23.808799Z","iopub.status.idle":"2021-05-21T15:23:23.866718Z","shell.execute_reply.started":"2021-05-21T15:23:23.808757Z","shell.execute_reply":"2021-05-21T15:23:23.865873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(trainSet.info())\nprint(testSet.info())\nprint(sample.info())","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:23:25.311999Z","iopub.execute_input":"2021-05-21T15:23:25.313893Z","iopub.status.idle":"2021-05-21T15:23:25.346495Z","shell.execute_reply.started":"2021-05-21T15:23:25.313845Z","shell.execute_reply":"2021-05-21T15:23:25.345369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,ax = plt.subplots(nrows = 2)\ntrainSet.target.hist(ax = ax[0])\ntrainSet.standard_error.hist(ax = ax[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:23:27.227185Z","iopub.execute_input":"2021-05-21T15:23:27.227536Z","iopub.status.idle":"2021-05-21T15:23:27.492614Z","shell.execute_reply.started":"2021-05-21T15:23:27.227504Z","shell.execute_reply":"2021-05-21T15:23:27.491404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainSet.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T19:00:04.520489Z","iopub.execute_input":"2021-05-20T19:00:04.520847Z","iopub.status.idle":"2021-05-20T19:00:04.54067Z","shell.execute_reply.started":"2021-05-20T19:00:04.520815Z","shell.execute_reply":"2021-05-20T19:00:04.539997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature Engineering\nstop_words = get_stop_words('english')\nstop_words = [x.upper() for x in stop_words]\ntrainSet['Paragraphs'] = trainSet['excerpt'].apply(lambda row: len(row.split('\\n')))\ntrainSet['Words'] = trainSet['excerpt'].apply(lambda row: len(row.replace('\\n',' ').split(' ')))\ntrainSet['AvgWordsPerPar'] = trainSet['excerpt'].apply(lambda row: np.mean([len(x.split(' ')) for x in row.split('\\n')]))\ntrainSet['AvgSentPerPar'] = trainSet['excerpt'].apply(lambda row: np.mean([len(x.split('.')) for x in row.split('\\n')]))\ntrainSet['ASL'] = trainSet['excerpt'].apply(lambda row: np.sum([len(x.split(' ')) for x in row.replace('\\n','').split('.')])/len([len(x.split(' ')) for x in row.replace('\\n','').split('.')]))\ntrainSet['ASW'] = trainSet['excerpt'].apply(lambda row: np.sum([syllable_count(x) if len(x)>0 else 0 for x in row.replace('\\n','').replace('.','').split(' ')])/len([x for x in row.replace('\\n','').replace('.','').split(' ')]))\ntrainSet['FleschEase'] = trainSet.apply(lambda row: 206.835 - (1.015 * row['ASL']) - (84.6 * row['ASW']),axis = 1)\ntrainSet['NonStopWords'] = trainSet['excerpt'].apply(lambda row: len([x for x in row.upper().replace('\\n',' ').split(' ') if x not in stop_words]))\ntrainSet['StopWords'] = trainSet['excerpt'].apply(lambda row: len([x for x in row.upper().replace('\\n',' ').split(' ') if x in stop_words]))\ntrainSet['ratio'] = trainSet['NonStopWords'] / trainSet['StopWords']\ntrainSet['Nouns'] = trainSet['excerpt'].apply(lambda row: NounCount(row))\ntrainSet['Verbs'] = trainSet['excerpt'].apply(lambda row: VerbCount(row))\ntrainSet['Adjectives'] = trainSet['excerpt'].apply(lambda row: AdjCount(row))\ntrainSet['Pronouns'] = trainSet['excerpt'].apply(lambda row: PronounCount(row))\ntrainSet['ClausePerSentence'] = trainSet['excerpt'].apply(lambda row: np.mean([IndependentClause(sentence) for sentence in row.replace('\\n','').split('.')]))\ntrainSet[['AvgNP','AvgVP','AvgMissingVP','AvgAllPhrase']] = trainSet['excerpt'].apply(lambda row: Phrases(row))\ntrainSet['GrunFog'] = trainSet['excerpt'].apply(lambda row: GrunningFog(row))\ntrainSet['SMOG'] = trainSet['excerpt'].apply(lambda row: SMOG(row))\nprint(trainSet.head(10))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T16:39:32.072744Z","iopub.execute_input":"2021-05-21T16:39:32.073388Z","iopub.status.idle":"2021-05-21T16:43:48.332037Z","shell.execute_reply.started":"2021-05-21T16:39:32.073085Z","shell.execute_reply":"2021-05-21T16:43:48.331197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(trainSet)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:26:57.99899Z","iopub.execute_input":"2021-05-21T15:26:57.999417Z","iopub.status.idle":"2021-05-21T15:27:40.35402Z","shell.execute_reply.started":"2021-05-21T15:26:57.999382Z","shell.execute_reply":"2021-05-21T15:27:40.351668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#try to model something\nx_train,x_test,y_train,y_test = train_test_split(trainSet.drop(columns = ['id','url_legal','license','excerpt','target','standard_error']),trainSet['target'],train_size = 0.75)\nlgb_train = lgb.Dataset(x_train,y_train, free_raw_data=False)\n\nlgbm = lgb.LGBMRegressor(boosting_type='gbdt',objective='regression',importance_type= 'gain',n_estimators = 500)\nparameters = {'max_depth':[4],'learning_rate':[0.01],'num_leaves':[2**4]}\ndef rmse(actual,pred):\n    mse = metrics.mean_squared_error(actual,pred)\n    RMSE = np.sqrt(mse)\n    return(RMSE)\n    \nrmse = metrics.make_scorer(rmse,greater_is_better=False)\ngbm = GridSearchCV(lgbm,parameters,cv = 5,scoring = rmse)\ngbm.fit(x_train,y_train,eval_set = (x_train,y_train),early_stopping_rounds = 5,verbose = False)\nprint(f'Best model score is {gbm.best_score_}')\nprint(f'Best model parameters are\\n{gbm.best_params_}')\nprint(f'Score on the test hold out is {gbm.score(x_test,y_test)}')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-21T17:17:06.386955Z","iopub.execute_input":"2021-05-21T17:17:06.387365Z","iopub.status.idle":"2021-05-21T17:17:42.802781Z","shell.execute_reply.started":"2021-05-21T17:17:06.38733Z","shell.execute_reply":"2021-05-21T17:17:42.801735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model evaluation \nfig,ax = plt.subplots()\nlgb.plot_importance(gbm.best_estimator_.booster_,ax = ax,\n                    importance_type='gain',\n                    max_num_features=15)\nax.set_yticklabels(ax.get_yticklabels(),fontsize = 7)\nplt.title('Variable Importance')\nplt.show()\n\nimport shap\nexplainer = shap.TreeExplainer(model = gbm.best_estimator_,\n                               data = None,\n                               model_output = 'raw',\n                               feature_perturbation='tree_path_dependent')\nshap_values = explainer(x_train)\nexplainer.shap_values(x_train)\n# visualize the first prediction's explanation\nshap.plots.waterfall(shap_values[0],show = False)\nf1 = plt.gcf()\nfor ax in f1.get_axes():\n      ax.tick_params(axis='both', labelsize=7)\nplt.show()\n\nshap.plots.beeswarm(shap_values,show = False)\nf2 = plt.gcf()\nfor ax in f2.get_axes():\n      ax.tick_params(axis='both', labelsize=7)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-21T17:18:58.132949Z","iopub.execute_input":"2021-05-21T17:18:58.133477Z","iopub.status.idle":"2021-05-21T17:19:02.326787Z","shell.execute_reply.started":"2021-05-21T17:18:58.133444Z","shell.execute_reply":"2021-05-21T17:19:02.326007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predict on test set\ntestSet['Paragraphs'] = testSet['excerpt'].apply(lambda row: len(row.split('\\n')))\ntestSet['Words'] = testSet['excerpt'].apply(lambda row: len(row.replace('\\n',' ').split(' ')))\ntestSet['AvgWordsPerPar'] = testSet['excerpt'].apply(lambda row: np.mean([len(x.split(' ')) for x in row.split('\\n')]))\ntestSet['AvgSentPerPar'] = testSet['excerpt'].apply(lambda row: np.mean([len(x.split('.')) for x in row.split('\\n')]))\ntestSet['ASL'] = testSet['excerpt'].apply(lambda row: np.sum([len(x.split(' ')) for x in row.replace('\\n','').split('.')])/len([len(x.split(' ')) for x in row.replace('\\n','').split('.')]))\ntestSet['ASW'] = testSet['excerpt'].apply(lambda row: np.sum([syllable_count(x) if len(x)>0 else 0 for x in row.replace('\\n','').replace('.','').split(' ')])/len([x for x in row.replace('\\n','').replace('.','').split(' ')]))\ntestSet['FleschEase'] = testSet.apply(lambda row: 206.835 - (1.015 * row['ASL']) - (84.6 * row['ASW']),axis = 1)\ntestSet['NonStopWords'] = testSet['excerpt'].apply(lambda row: len([x for x in row.upper().replace('\\n',' ').split(' ') if x not in stop_words]))\ntestSet['StopWords'] = testSet['excerpt'].apply(lambda row: len([x for x in row.upper().replace('\\n',' ').split(' ') if x in stop_words]))\ntestSet['ratio'] = testSet['NonStopWords'] / testSet['StopWords']\ntestSet['Nouns'] = testSet['excerpt'].apply(lambda row: NounCount(row))\ntestSet['Verbs'] = testSet['excerpt'].apply(lambda row: VerbCount(row))\ntestSet['Adjectives'] = testSet['excerpt'].apply(lambda row: AdjCount(row))\ntestSet['Pronouns'] = testSet['excerpt'].apply(lambda row: PronounCount(row))\ntestSet['ClausePerSentence'] = testSet['excerpt'].apply(lambda row: np.mean([IndependentClause(sentence) for sentence in row.replace('\\n','').split('.')]))\ntestSet[['AvgNP','AvgVP','AvgMissingVP','AvgAllPhrase']] = testSet['excerpt'].apply(lambda row: Phrases(row))\ntestSet['GrunFog'] = testSet['excerpt'].apply(lambda row: GrunningFog(row))\ntestSet['SMOG'] = testSet['excerpt'].apply(lambda row: SMOG(row))","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:29:30.73662Z","iopub.execute_input":"2021-05-21T15:29:30.736991Z","iopub.status.idle":"2021-05-21T15:29:31.00723Z","shell.execute_reply.started":"2021-05-21T15:29:30.736958Z","shell.execute_reply":"2021-05-21T15:29:31.006232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testSet.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:29:33.816609Z","iopub.execute_input":"2021-05-21T15:29:33.816962Z","iopub.status.idle":"2021-05-21T15:29:33.841438Z","shell.execute_reply.started":"2021-05-21T15:29:33.816931Z","shell.execute_reply":"2021-05-21T15:29:33.840113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = gbm.predict(testSet.drop(columns = ['id','url_legal','license','excerpt']))\nprint(pd.concat([testSet['id'],pd.Series(y_pred)],axis = 1, keys = ['id','target']))\npd.concat([testSet['id'],pd.Series(y_pred)],axis = 1, keys = ['id','target']).to_csv('submission.csv',index = False)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T15:29:37.063799Z","iopub.execute_input":"2021-05-21T15:29:37.064194Z","iopub.status.idle":"2021-05-21T15:29:37.083768Z","shell.execute_reply.started":"2021-05-21T15:29:37.064158Z","shell.execute_reply":"2021-05-21T15:29:37.082941Z"},"trusted":true},"execution_count":null,"outputs":[]}]}