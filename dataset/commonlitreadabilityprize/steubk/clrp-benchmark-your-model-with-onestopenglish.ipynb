{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## OneStopEnglish corpus ","metadata":{}},{"cell_type":"markdown","source":"\nOne of the challenges of this competition is to fine tuning very large models (350/400M parameters) with a small sample (~2.800 rows).\n\nOne way to avoid overfit train/validation set is to check how your model generalizes with a completely different dataset.\n\nOneStopEnglish corpus of texts (https://aclanthology.org/W18-0535/) is a dataset written at three reading levels.  \n\nThe corpus consists of 189 texts, each represented in three versions: elementary, intermediate and advanced text:\n\n\n<img src=\"https://d3i71xaburhd42.cloudfront.net/f6d485c14786abbab731b0cf5e1f4de6b69dc57b/5-Table1-1.png\" alt=\"Example sentences for three reading levels\" style=\"width:600px;\"/>\n\n\nThis dataset is not designed specifically for children's readability, but accuracy on such dataset may show your model's skills on assessing readability.\n","metadata":{}},{"cell_type":"markdown","source":"The accuracy on this dataset can be defined in this way\n- if *pred* for elementary text is grater than pred for intermediate text than 1 else 0 (elementary text is predicted to be more readable then intermediate text)\n- if *pred* for itermediate text is grater than pred for adcanced text than 1 else 0 (itermediate text is predicted to be more readable then advanced text)\n","metadata":{}},{"cell_type":"markdown","source":"In this notebook we show how some of our models prepared for the CommonLit competition generalize well on this dataset (**83.5%-88.6%** accuracy) as well as how a linear combination of these models with classical readability features outperform accuracy on the dataset as reported in litterature (**98.9%**)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T14:33:22.474715Z","iopub.execute_input":"2021-08-03T14:33:22.475182Z","iopub.status.idle":"2021-08-03T14:33:30.8972Z","shell.execute_reply.started":"2021-08-03T14:33:22.475092Z","shell.execute_reply":"2021-08-03T14:33:30.895899Z"}}},{"cell_type":"markdown","source":"|Model|OneEnglishCorpus (Accuracy)|CommonLit CV (RMSE)|CommonLit Public LB (RMSE)|CommonLit Private LB (RMSE)|\n|:----|:-------------------------:|:-----------------:|:------------------------:|:-------------------------:|\n|clrp-deberta-large-ppln4-atthead|0.85979|0.48395|0.472|0.469|\n|clrp-deberta-large-2-se|0.88624|0.48962|0.472|0.471|\n|clrp-deberta-large-4-se|0.87831|0.49059|0.468|0.470|\n|clrp-roberta-large-2f-se|0.83598|0.48927|0.472|0.470|\n|clrp-roberta-large-2h-atthead-se|0.84656|0.49703|NA|NA|\n|**ensemble**|**0.98942**|**0.45964**|**0.458**|**0.454**|\n","metadata":{}},{"cell_type":"markdown","source":"### Download OneStopEnglish corpus dataset ","metadata":{}},{"cell_type":"markdown","source":"We download the OneStop EnglishCorpus from Github and we assemble it into a dataset.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/nishkalavallabhi/OneStopEnglishCorpus.git","metadata":{"execution":{"iopub.status.busy":"2021-08-03T14:33:30.899254Z","iopub.execute_input":"2021-08-03T14:33:30.899651Z","iopub.status.idle":"2021-08-03T14:33:36.332459Z","shell.execute_reply.started":"2021-08-03T14:33:30.899606Z","shell.execute_reply":"2021-08-03T14:33:36.331488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\n\nname = []\nele = []\ninter = []\nadv= []\n\npath_adv = './OneStopEnglishCorpus/Texts-SeparatedByReadingLevel/Adv-Txt'\npath_ele = './OneStopEnglishCorpus/Texts-SeparatedByReadingLevel/Ele-Txt'\npath_int = './OneStopEnglishCorpus/Texts-SeparatedByReadingLevel/Int-Txt'\n\nfor dirname, _, filenames in os.walk(path_adv):\n    for filename in filenames:\n        path =  dirname +\"/\" +filename\n        if \".txt\" in path:\n            name.append (filename.replace(\"-adv.txt\",\"\"))\n            with open(path) as f:\n                content = \" \".join(f.readlines()) \n                adv.append(content)\n            with open(path_ele + \"/\" + filename.replace(\"-adv\",\"-ele\")) as f:\n                content = \" \".join(f.readlines()) \n                ele.append(content)\n            with open(path_int + \"/\" + filename.replace(\"-adv\",\"-int\")) as f:\n                content = \" \".join(f.readlines()[1:]) \n                inter.append(content)\n                \ndf_1 = pd.DataFrame ({\n    \"name\": name,\n    \"excerpt\": ele\n})\ndf_1[\"level\"] = \"ele\"\n\ndf_2 = pd.DataFrame ({\n    \"name\": name,\n    \"excerpt\": inter\n})\ndf_2[\"level\"] = \"inter\"\n\ndf_3 = pd.DataFrame ({\n    \"name\": name,\n    \"excerpt\": adv\n})\ndf_3[\"level\"] = \"adv\"\n\ndf = df_1.append(df_2).append(df_3)\n\ndf.to_csv(\"OneStopEnglishCorpus.csv\", index=False)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-03T14:33:36.336153Z","iopub.execute_input":"2021-08-03T14:33:36.336426Z","iopub.status.idle":"2021-08-03T14:33:36.733084Z","shell.execute_reply.started":"2021-08-03T14:33:36.336396Z","shell.execute_reply":"2021-08-03T14:33:36.732298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict readability on OneStopEnglish corpus","metadata":{}},{"cell_type":"markdown","source":"### Single Models","metadata":{}},{"cell_type":"markdown","source":"Here we entrust 4 of our competition models to predict readability on the OneStopEnglish corpus. Notice that we first write on disk a python script for each model prediciton pipeline, then we execute them by calling them separately by a shell command. In such a way we avoid problems with extensive and repetive usage of memory and GPU in the same script.","metadata":{}},{"cell_type":"code","source":"%%writefile ensemble.py\n\nimport numpy as np \nimport pandas as pd \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n\nimport torch\nimport transformers\n\nimport random\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nclass CLRPDataset():\n    def __init__(self,df,max_len, tokenizer):\n        self.excerpt = df['excerpt'].values\n        self.max_len = max_len\n        self.tokenizer = tokenizer \n\n\n        if \"target\" in df.columns:\n            self.target = df['target'].values\n        else:\n            self.target = None\n    \n    def __getitem__(self,index):\n        encode = self.tokenizer(self.excerpt[index],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                return_token_type_ids = True,\n                                truncation=True)  \n\n        #token_ids = encode['input_ids'].squeeze(0)\n        #attn_masks = encode['attention_mask'].squeeze(0)\n        #token_type_ids = encode['token_type_ids'].squeeze(0)\n\n        token_ids = encode['input_ids'][0]\n        attn_masks = encode['attention_mask'][0]\n        token_type_ids = encode['token_type_ids'][0]\n        \n        \n        if self.target is None:\n            return token_ids, attn_masks, token_type_ids\n\n\n        target = self.target[index]\n        target = torch.tensor(target).float()    \n\n        return token_ids, attn_masks, token_type_ids, target  \n\n\n    def __len__(self):\n        return len(self.excerpt)\n    \nclass BertRegreesion(torch.nn.Module):\n\n    def __init__(self, dropout, bert_model, model_path,  freeze_bert=False):\n        super(BertRegreesion, self).__init__()\n        \n        self.bert_layer = transformers.AutoModel.from_pretrained(model_path)\n        \n        #  Fix the hidden-state size of the encoder outputs (If you want to add other pre-trained models here, search for the encoder output size)\n        if bert_model == \"roberta-base\":  \n            hidden_size = 768\n        elif bert_model == \"roberta-large\":  \n            hidden_size = 1024\n        elif bert_model == \"microsoft/deberta-large\":  \n            hidden_size = 1024\n            \n        # Freeze bert layers and only train the regression layer weights\n        if freeze_bert:\n            for p in self.bert_layer.parameters():\n                p.requires_grad = False\n\n        # ReGression layer\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.head = torch.nn.Linear(hidden_size, 1)\n        self.bert_model = bert_model\n    \n    def forward(self, input_ids, attn_masks, token_type_ids):\n\n        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n        if  self.bert_model == \"microsoft/deberta-large\":\n            output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n            output = output[0]\n            output = output[:,0,:].squeeze(1)\n        else:  \n            cont_reps, output = self.bert_layer(input_ids, attn_masks, token_type_ids,  return_dict=False)\n\n        output = self.head(self.dropout(output))\n\n        return output\n    \nBATCH_SIZE = 4\nFOLDS = 5\nNUM_WORKERS = 4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass Config:\n    epochs = 20 \n    eval_per_epoch = 8 #4\n    dropout = 0.4\n    max_len = 256\n    bert_model = \"roberta-base\"\n    warm_up_steps = 300\n\ndef get_preds (bert_model, instances_path, max_len, df_test):\n    print(instances_path)\n    model_path = \"/kaggle/input/transformers/\" + bert_model + \"-hf\"\n    print(f\"model_path:{model_path}\")\n\n    vocab_path = model_path\n    instances_path = \"/kaggle/input/\" + instances_path\n    instance_name = bert_model.replace(\"/\",\"_\")\n\n    device = DEVICE\n    tokenizer = transformers.AutoTokenizer.from_pretrained(vocab_path)\n\n    test_data = CLRPDataset(df_test,max_len, tokenizer=tokenizer)\n    test_loader = torch.utils.data.DataLoader(test_data,\n                                          batch_size=BATCH_SIZE,\n                                          shuffle=False,\n                                          num_workers=NUM_WORKERS)\n\n    p = np.zeros((len(df_test),))\n    for fold in range(FOLDS): \n        preds = []\n\n        model = BertRegreesion (dropout = Config.dropout, bert_model=bert_model, model_path= model_path, freeze_bert=False)\n        \n        filename = f\"{instances_path}/{instance_name}_{fold}.pt\"\n        model.load_state_dict(torch.load(filename, map_location=torch.device(device)))\n        model.to(device)\n        model.eval()\n    \n        with torch.no_grad():\n            for token_ids, attn_masks, token_type_ids in tqdm(test_loader):\n                token_ids = token_ids.to(device)\n                attn_masks = attn_masks.to(device)\n                token_type_ids = token_type_ids.to(device)\n\n                output = model.forward(token_ids, attn_masks, token_type_ids)\n                output = output.detach().cpu()[:,0]\n\n                preds.append(output)\n        preds = np.concatenate(preds)\n        p += preds\n        del model\n    \n    return p/FOLDS\n\ndef main():\n    from numba import cuda \n    device = cuda.get_current_device()\n    device.reset()\n    \n    df_test = pd.read_csv(\"OneStopEnglishCorpus.csv\")\n    args = sys.argv[1:]\n    \n    preds = get_preds (args[0],args[1], max_len = Config.max_len, df_test = df_test)\n    df_test [args[1]] = preds\n    \n    df_test[[args[1]]].to_pickle(args[1]+\".pkl\")\n\nif __name__ == \"__main__\":\n    main()            \n            ","metadata":{"execution":{"iopub.status.busy":"2021-08-03T14:33:36.734781Z","iopub.execute_input":"2021-08-03T14:33:36.735114Z","iopub.status.idle":"2021-08-03T14:33:36.742012Z","shell.execute_reply.started":"2021-08-03T14:33:36.735079Z","shell.execute_reply":"2021-08-03T14:33:36.741103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile ensemble_atthead.py\n\nimport numpy as np \nimport pandas as pd \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\n\n\nimport torch\nimport transformers\n\nimport random\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nclass CLRPDataset():\n    def __init__(self,df,max_len, tokenizer):\n        self.excerpt = df['excerpt'].values\n        self.max_len = max_len\n        self.tokenizer = tokenizer \n\n\n        if \"target\" in df.columns:\n            self.target = df['target'].values\n        else:\n            self.target = None\n    \n    def __getitem__(self,index):\n        encode = self.tokenizer(self.excerpt[index],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                return_token_type_ids = True,\n                                truncation=True)  \n\n        #token_ids = encode['input_ids'].squeeze(0)\n        #attn_masks = encode['attention_mask'].squeeze(0)\n        #token_type_ids = encode['token_type_ids'].squeeze(0)\n\n        token_ids = encode['input_ids'][0]\n        attn_masks = encode['attention_mask'][0]\n        token_type_ids = encode['token_type_ids'][0]\n        \n        \n        if self.target is None:\n            return token_ids, attn_masks, token_type_ids\n\n\n        target = self.target[index]\n        target = torch.tensor(target).float()    \n\n        return token_ids, attn_masks, token_type_ids, target  \n\n\n    def __len__(self):\n        return len(self.excerpt)\n    \nclass AttentionHead(torch.nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n        self.W = torch.nn.Linear(in_features, hidden_dim)\n        self.V = torch.nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\n\nclass BertRegreesion(torch.nn.Module):\n\n    def __init__(self, dropout, bert_model, model_path,  freeze_bert=False):\n        super(BertRegreesion, self).__init__()\n        \n        self.bert_layer = transformers.AutoModel.from_pretrained(model_path, output_hidden_states=True)\n\n        #  Fix the hidden-state size of the encoder outputs (If you want to add other pre-trained models here, search for the encoder output size)\n        if bert_model == \"roberta-base\":  \n            hidden_size = 768\n        elif bert_model == \"roberta-large\":  \n            hidden_size = 1024\n        elif bert_model == \"microsoft/deberta-large\":  \n            hidden_size = 1024\n\n        # Freeze bert layers and only train the regression layer weights\n        if freeze_bert:\n            for p in self.bert_layer.parameters():\n                p.requires_grad = False\n\n        self.head = AttentionHead(hidden_size,hidden_size,1)\n                \n        # ReGression layer\n        self.dropout = torch.nn.Dropout(p=dropout)\n        self.linear = torch.nn.Linear(hidden_size, 1)\n        self.bert_model = bert_model\n    \n    def forward(self, input_ids, attn_masks, token_type_ids):\n\n        # Feeding the inputs to the BERT-based model to obtain contextualized representations\n        if  self.bert_model == \"microsoft/deberta-large\":\n            output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n            output = output[0]\n\n        else:  \n            output = self.bert_layer(input_ids, attn_masks, token_type_ids,  return_dict=False)\n            output = output[0]\n        \n        output = self.head(output)\n        output = self.dropout(output)\n        output = self.linear(output)\n\n        return output\n    \nBATCH_SIZE = 4\nFOLDS = 5\nNUM_WORKERS = 4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nclass Config:\n    epochs = 20 \n    eval_per_epoch = 8 #4\n    dropout = 0.4\n    max_len = 256\n    bert_model = \"roberta-base\"\n    warm_up_steps = 300\n\ndef get_preds (bert_model, instances_path, max_len, df_test):\n    print(instances_path)\n    model_path = \"/kaggle/input/transformers/\" + bert_model + \"-hf\"\n    print(f\"model_path:{model_path}\")\n\n    vocab_path = model_path\n    instances_path = \"/kaggle/input/\" + instances_path\n    instance_name = bert_model.replace(\"/\",\"_\")\n\n    \n    device = DEVICE\n    tokenizer = transformers.AutoTokenizer.from_pretrained(vocab_path)\n\n    test_data = CLRPDataset(df_test,max_len, tokenizer=tokenizer)\n    test_loader = torch.utils.data.DataLoader(test_data,\n                                          batch_size=BATCH_SIZE,\n                                          shuffle=False,\n                                          num_workers=NUM_WORKERS)\n\n    p = np.zeros((len(df_test),))\n    for fold in range(FOLDS): \n        preds = []\n\n        model = BertRegreesion (dropout = Config.dropout, bert_model=bert_model, model_path= model_path, freeze_bert=False)\n        \n        filename = f\"{instances_path}/{instance_name}_{fold}.pt\"\n        model.load_state_dict(torch.load(filename, map_location=torch.device(device)))\n        model.to(device)\n        model.eval()\n    \n        with torch.no_grad():\n            for token_ids, attn_masks, token_type_ids in tqdm(test_loader):\n                token_ids = token_ids.to(device)\n                attn_masks = attn_masks.to(device)\n                token_type_ids = token_type_ids.to(device)\n\n                output = model.forward(token_ids, attn_masks, token_type_ids)\n                output = output.detach().cpu()[:,0]\n\n                preds.append(output)\n        preds = np.concatenate(preds)\n        p += preds\n        del model\n    \n    return p/FOLDS\n\ndef main():\n    from numba import cuda \n    device = cuda.get_current_device()\n    device.reset()\n    \n    df_test = pd.read_csv(\"OneStopEnglishCorpus.csv\")\n    args = sys.argv[1:]\n    \n    preds = get_preds (args[0],args[1], max_len = Config.max_len, df_test = df_test)\n    df_test [args[1]] = preds\n    \n    df_test[[args[1]]].to_pickle(args[1]+\".pkl\")\n\nif __name__ == \"__main__\":\n    main()            \n            ","metadata":{"execution":{"iopub.status.busy":"2021-08-03T14:33:36.743569Z","iopub.execute_input":"2021-08-03T14:33:36.744154Z","iopub.status.idle":"2021-08-03T14:33:36.753549Z","shell.execute_reply.started":"2021-08-03T14:33:36.744117Z","shell.execute_reply":"2021-08-03T14:33:36.752439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python ensemble_atthead.py \"roberta-large\" \"clrp-roberta-large-2h-atthead-se\"\n\n!python ensemble_atthead.py \"microsoft/deberta-large\" \"clrp-deberta-large-ppln4-atthead\"\n!python ensemble.py \"roberta-large\" \"clrp-roberta-large-2f-se\"\n\n!python ensemble.py \"microsoft/deberta-large\" \"clrp-deberta-large-2-se\"\n!python ensemble.py \"microsoft/deberta-large\" \"clrp-deberta-large-4-se\"","metadata":{"execution":{"iopub.status.busy":"2021-08-03T14:33:36.75547Z","iopub.execute_input":"2021-08-03T14:33:36.756052Z","iopub.status.idle":"2021-08-03T14:56:07.609277Z","shell.execute_reply.started":"2021-08-03T14:33:36.755944Z","shell.execute_reply":"2021-08-03T14:56:07.60696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ensemble","metadata":{}},{"cell_type":"markdown","source":"In this part of the code, we retireve the out of fold predictions (OOF) from the CommonLit dataset, we produce a series of readability measures (Syllable count, Gunning Fog, Readability index, Coleman Liau Index, Text Standard, Dale Chall Readability Score) and we combine all together into a linear combination with L2 regularization (a Ridge regression). Though such is based on the CommonLit dataset, we apply it to the OneStopEnglish corpus in order to obtain an ensemble.","metadata":{}},{"cell_type":"code","source":"!pip install textstat","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport sklearn.linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nimport textstat","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FOLDS = 5\n\ndef read_oof (name, return_all = True):\n    oof = pd.read_csv(f\"/kaggle/input/{name}/oof.csv\")\n    \n    if \"pred_x\" in oof.columns:\n        oof = oof.rename (columns={\"pred_x\":name})\n    else:\n        oof = oof.rename (columns={\"pred\":name})\n        \n    if return_all:\n        return oof\n    else:\n        return oof[[\"id\",name]]\n\n\nname_1 = \"clrp-roberta-large-2f-se\"\nname_2 = \"clrp-deberta-large-2-se\"\nname_3 = \"clrp-deberta-large-4-se\"\nname_4 = \"clrp-deberta-large-ppln4-atthead\"\nname_5 = \"clrp-roberta-large-2h-atthead-se\"\n\noof = read_oof (name_1, return_all =True)\ntmp = read_oof (name_2, return_all =False)\noof = oof.merge(tmp, on =\"id\")\ntmp = read_oof (name_3, return_all =False)\noof = oof.merge(tmp, on =\"id\")\ntmp = read_oof (name_4, return_all =False)\noof = oof.merge(tmp, on =\"id\")\ntmp = read_oof (name_5, return_all =False)\noof = oof.merge(tmp, on =\"id\")\n\nfor name in [name_1, name_2, name_3, name_4, name_5]:\n    score = mean_squared_error (oof[\"target\"], oof[name], squared=False)\n    print(f\"rmse {name}: {score:.5f}\"  )\n\noof_textstats = pd.read_csv(\"/kaggle/input/clrp-textstats/textstats.csv\")\noof = oof.merge(oof_textstats, on=\"id\")\n\noof[\"t1\"] = oof[\"syllable_count\"]**2\noof[\"t2\"] = oof[\"coleman_liau_index\"]**2\n\ntextstats_feats = [\n    'syllable_count', \n    'gunning_fog', 'automated_readability_index', 'coleman_liau_index', 'text_standard', \n    \"dale_chall_readability_score\",\n    \"t1\", \n    \"t2\", \n] ","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:17:31.22171Z","iopub.execute_input":"2021-08-03T15:17:31.222117Z","iopub.status.idle":"2021-08-03T15:17:31.577908Z","shell.execute_reply.started":"2021-08-03T15:17:31.222084Z","shell.execute_reply":"2021-08-03T15:17:31.576446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(\"OneStopEnglishCorpus.csv\")\n\npred = pd.read_pickle(name_1 + \".pkl\")\ndf_test[name_1] = pred.values\n\npred = pd.read_pickle(name_2+ \".pkl\")\ndf_test[name_2] = pred.values\n\npred = pd.read_pickle(name_3+ \".pkl\")\ndf_test[name_3] = pred.values\n\npred = pd.read_pickle(name_4+ \".pkl\")\ndf_test[name_4] = pred.values\n\npred = pd.read_pickle(name_5+ \".pkl\")\ndf_test[name_5] = pred.values\n\ndf_test[\"syllable_count\"] = df_test [\"excerpt\"].map(lambda x: textstat.syllable_count(x))\ndf_test[\"gunning_fog\"] = df_test [\"excerpt\"].map(lambda x: textstat.gunning_fog(x))\ndf_test[\"automated_readability_index\"] = df_test [\"excerpt\"].map(lambda x: textstat.automated_readability_index(x))\ndf_test[\"coleman_liau_index\"] = df_test [\"excerpt\"].map(lambda x: textstat.coleman_liau_index(x))\ndf_test[\"text_standard\"] = df_test [\"excerpt\"].map(lambda x: textstat.text_standard(x, float_output=True))\ndf_test[\"dale_chall_readability_score\"] = df_test [\"excerpt\"].map(lambda x: textstat.dale_chall_readability_score(x))\n\n\ndf_test[\"t1\"] = df_test[\"syllable_count\"]**2\ndf_test[\"t2\"] = df_test[\"coleman_liau_index\"]**2\n\ndf_test[\"target\"] = 0","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:17:34.819516Z","iopub.execute_input":"2021-08-03T15:17:34.819918Z","iopub.status.idle":"2021-08-03T15:17:42.958866Z","shell.execute_reply.started":"2021-08-03T15:17:34.819879Z","shell.execute_reply":"2021-08-03T15:17:42.95789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [name_1, name_2, name_3, name_4, name_5] + textstats_feats","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:17:46.815229Z","iopub.execute_input":"2021-08-03T15:17:46.815569Z","iopub.status.idle":"2021-08-03T15:17:46.819853Z","shell.execute_reply.started":"2021-08-03T15:17:46.815538Z","shell.execute_reply":"2021-08-03T15:17:46.818727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler().fit(oof[cols])\n\noof[cols] = scaler.transform(oof[cols])\ndf_test[cols] = scaler.transform(df_test[cols])\n\noof_pred = []\noof_target = []\n\nX_test = df_test[cols].values\n\nfor fold in range(FOLDS):\n    df_val = oof.query(\"kfold == @fold\")\n    X_val = df_val[cols].values\n    y_val = df_val[\"target\"].values\n    \n    df_train = oof.query(\"kfold != @fold\")\n    X_train = df_train[cols].values\n    y_train = df_train[\"target\"].values\n    \n    model = sklearn.linear_model.Ridge(alpha=5.0)\n    model.fit (X_train, y_train)\n    p_val = model.predict (X_val)\n    \n    df_test[\"target\"] += model.predict(X_test)\n    \n    oof_pred.append(p_val)\n    oof_target.append (y_val)\n    score = mean_squared_error (y_val, p_val, squared=False)\n\noof_pred = np.concatenate (oof_pred)\noof_target = np.concatenate (oof_target)\nens_score = mean_squared_error (oof_target, oof_pred, squared=False)\nprint(f\"rmse ens: {ens_score:.5f}\"  )","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:17:49.985921Z","iopub.execute_input":"2021-08-03T15:17:49.986275Z","iopub.status.idle":"2021-08-03T15:17:50.128205Z","shell.execute_reply.started":"2021-08-03T15:17:49.986245Z","shell.execute_reply":"2021-08-03T15:17:50.127201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[\"target\"] /= FOLDS  ","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:18:00.301821Z","iopub.execute_input":"2021-08-03T15:18:00.302252Z","iopub.status.idle":"2021-08-03T15:18:00.307023Z","shell.execute_reply.started":"2021-08-03T15:18:00.302218Z","shell.execute_reply":"2021-08-03T15:18:00.306013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate OneStopEnglish accuracy on single models and ensemble","metadata":{}},{"cell_type":"markdown","source":"At this point we just need to evaluate performances of each model alone and of their ensemble on the OneStopEnglish corpus (a blend by a ridge linear regression on out of fold predictions on the commonlit dataset). Please notice how the ensemble outruns all the single models.","metadata":{}},{"cell_type":"code","source":"def eval_onecorpus_accuracy(df_test, model_name):\n    df_ele =  df_test.query(\"level == 'ele'\")[[\"name\",model_name]].copy().rename(columns={model_name:\"ele\"})\n    df_inter =  df_test.query(\"level == 'inter'\")[[\"name\",model_name]].copy().rename(columns={model_name:\"inter\"})\n    df_adv =  df_test.query(\"level == 'adv'\")[[\"name\",model_name]].copy().rename(columns={model_name:\"adv\"})\n\n    df_test = df_ele.merge(df_inter, on=\"name\").merge(df_adv, on=\"name\")\n    ## accuracy\n    df_test[\"flag_1\"] =  (df_test[\"ele\"] > df_test[\"inter\"]).astype(int) \n    df_test[\"flag_2\"] =  (df_test[\"inter\"] > df_test[\"adv\"]).astype(int)\n    accuracy = (df_test[\"flag_1\"].sum() + df_test[\"flag_2\"].sum())/(2*df_test.shape[0])\n    return accuracy    ","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:20:43.258334Z","iopub.execute_input":"2021-08-03T15:20:43.25869Z","iopub.status.idle":"2021-08-03T15:20:43.269559Z","shell.execute_reply.started":"2021-08-03T15:20:43.258658Z","shell.execute_reply":"2021-08-03T15:20:43.268743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model_name in [name_1, name_2, name_3, name_4, name_5]:\n    accuracy = eval_onecorpus_accuracy(df_test, model_name)\n    print (f\"model:{model_name}, accuracy:{accuracy:.5f}\")\n\naccuracy = eval_onecorpus_accuracy(df_test, \"target\")\nprint (f\"model:ensemble, accuracy:{accuracy:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-03T15:21:14.386841Z","iopub.execute_input":"2021-08-03T15:21:14.387238Z","iopub.status.idle":"2021-08-03T15:21:14.515584Z","shell.execute_reply.started":"2021-08-03T15:21:14.387206Z","shell.execute_reply":"2021-08-03T15:21:14.514755Z"},"trusted":true},"execution_count":null,"outputs":[]}]}