{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch import nn\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-02T10:27:24.12205Z","iopub.execute_input":"2021-07-02T10:27:24.122424Z","iopub.status.idle":"2021-07-02T10:27:24.146099Z","shell.execute_reply.started":"2021-07-02T10:27:24.12239Z","shell.execute_reply":"2021-07-02T10:27:24.145345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n#         config = AutoConfig.from_pretrained(ROBERTA_PATH)\n#         config.update({\"output_hidden_states\":True, \n#                        \"hidden_dropout_prob\": 0.0,\n#                        \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained('../input/clrpauxitptrobertabase/clrp_roberta_base', output_hidden_states=True)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:24.147516Z","iopub.execute_input":"2021-07-02T10:27:24.147834Z","iopub.status.idle":"2021-07-02T10:27:24.156199Z","shell.execute_reply.started":"2021-07-02T10:27:24.147802Z","shell.execute_reply":"2021-07-02T10:27:24.155163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = 248,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:24.15821Z","iopub.execute_input":"2021-07-02T10:27:24.15861Z","iopub.status.idle":"2021-07-02T10:27:24.168802Z","shell.execute_reply.started":"2021-07-02T10:27:24.158538Z","shell.execute_reply":"2021-07-02T10:27:24.168055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.cuda()\n            attention_mask = attention_mask.cuda()\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:24.376087Z","iopub.execute_input":"2021-07-02T10:27:24.376355Z","iopub.status.idle":"2021-07-02T10:27:24.382458Z","shell.execute_reply.started":"2021-07-02T10:27:24.376319Z","shell.execute_reply":"2021-07-02T10:27:24.381286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ntest_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\ntokenizer = torch.load('../input/tokenizers/roberta-tokenizer.pt')\nall_predictions = np.zeros((5, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=16,\n                         drop_last=False, shuffle=False, num_workers=2)\n\n\n# for fold in range(5):\n# print('fold:', fold+1)\nmodel = LitModel()\nmodel.load_state_dict(torch.load(f'../input/pre-trained-roberta-solution-in-pytorch/model_{2}.pth'))\nmodel.cuda()\n\npredictions = predict(model, test_loader)\ndel model\ngc.collect()\n\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:24.384182Z","iopub.execute_input":"2021-07-02T10:27:24.384723Z","iopub.status.idle":"2021-07-02T10:27:33.682975Z","shell.execute_reply.started":"2021-07-02T10:27:24.384581Z","shell.execute_reply":"2021-07-02T10:27:33.68213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")\n# predictions = all_predictions.mean(axis=0)\nsubmission_df.target = predictions\nprint(submission_df)\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T10:27:33.684822Z","iopub.execute_input":"2021-07-02T10:27:33.685153Z","iopub.status.idle":"2021-07-02T10:27:33.703861Z","shell.execute_reply.started":"2021-07-02T10:27:33.685113Z","shell.execute_reply":"2021-07-02T10:27:33.702844Z"},"trusted":true},"execution_count":null,"outputs":[]}]}