{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nfrom pathlib import Path\nin_folder_path = Path('../input/clrp-finetune-best-score')\nscripts_dir = Path(in_folder_path / 'scripts')\nimport torch\nimport pandas as pd\nfrom torch import nn\nfrom torch import Tensor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-22T09:38:58.817877Z","iopub.execute_input":"2021-07-22T09:38:58.81823Z","iopub.status.idle":"2021-07-22T09:38:58.823581Z","shell.execute_reply.started":"2021-07-22T09:38:58.818184Z","shell.execute_reply":"2021-07-22T09:38:58.822487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, masked):\n        super().__init__()\n        assert d_model % num_heads == 0, \"num_heads must evenly chunk d_model\"\n        self.num_heads = num_heads\n        self.wq = nn.Linear(d_model, d_model, bias=False)  # QQ what if bias=True?\n        self.wk = nn.Linear(d_model, d_model, bias=False)\n        self.wv = nn.Linear(d_model, d_model, bias=False)\n        self.masked = masked\n        self.softmax = nn.Softmax(dim=2)\n\n    def forward(self, q, k, v):\n#         print(q.shape, self.num_heads)\n        qs = self.wq(q).chunk(self.num_heads, dim=2)\n        ks = self.wk(k).chunk(self.num_heads, dim=2)\n        vs = self.wv(v).chunk(self.num_heads, dim=2)\n        outs = []\n        # TODO Use einsum instead of for loop\n        for qi, ki, vi in zip(qs, ks, vs):\n            attns = qi.bmm(ki.transpose(1, 2)) / (ki.shape[2] ** 0.5)\n            if self.masked:\n                attns = attns.tril()  # Zero out upper triangle so it can't look ahead\n            attns = self.softmax(attns)\n            outs.append(attns.bmm(vi))\n        return torch.cat(outs, dim=2)\n\n\nclass AddNorm(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.ln = nn.LayerNorm(d_model)\n\n    def forward(self, x1, x2):\n        return self.ln(x1+x2)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.l1 = nn.Linear(d_model, d_model)\n        self.relu = nn.ReLU()\n        self.l2 = nn.Linear(d_model, d_model)\n    def forward(self, x):\n        return self.l2(self.relu(self.l1(x)))\n\n\ndef pos_encode(x):\n    pos, dim = torch.meshgrid(torch.arange(x.shape[1]), torch.arange(x.shape[2]))\n    dim = 2 * (dim // 2)\n    enc_base = pos/(10_000**(dim / x.shape[2]))\n    addition = torch.zeros_like(x)\n    for d in range(x.shape[2]):\n        enc_func = torch.sin if d % 2 == 0 else torch.cos\n        addition[:,:,d] = enc_func(enc_base[:,d])\n    if x.is_cuda:\n        addition = addition.cuda()\n    return x + addition\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads, masked=False)\n        self.an1 = AddNorm(d_model)\n        self.ff = FeedForward(d_model)\n        self.an2 = AddNorm(d_model)\n\n    def forward(self, x):\n        x = self.an1(x, self.mha(q=x, k=x, v=x))\n        return self.an2(x, self.ff(x))\n\n\nclass AttentionAggregation(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.query = nn.Linear(d_model, 1, bias=False)\n\n    def forward(self, x):  # (b, s, m)\n        attns = self.query(x).softmax(dim=1)  # (b, s, 1)\n        enc = torch.bmm(attns.transpose(1, 2), x)  # (b, 1, m)\n        return enc.squeeze(1)\n\n\nclass LinTanh(nn.Module):\n    def __init__(self, d_model):\n        super().__init__()\n        self.lin = nn.Linear(d_model, d_model)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        return self.tanh(self.lin(x))\n\n\nclass LinFeatConcat(nn.Module):\n    def __init__(self, d_model, n_feats, n_out):\n        super().__init__()\n        self.lin = nn.Linear(d_model + n_feats, n_out, bias=False)  # TODO what if True?\n\n    def forward(self, x, feats):\n        return self.lin(torch.cat([x, feats], dim=1))\n\n\nclass ReadNetBlock(nn.Module):\n    def __init__(self, d_model, n_heads, n_blocks, n_feats, n_out):\n        super().__init__()\n        self.blocks = nn.Sequential(*[EncoderBlock(d_model=d_model, num_heads=n_heads) for _ in range(n_blocks)])\n        self.lin_tanh = LinTanh(d_model=d_model)\n        self.attn_agg = AttentionAggregation(d_model=d_model)\n        self.lin_feat_concat = LinFeatConcat(d_model=d_model, n_feats=n_feats, n_out=n_out)\n\n    def forward(self, x, feats):  # (b, s, m), (b, f)\n        x = pos_encode(x)\n        x = self.blocks(x)\n        x = self.lin_tanh(x)\n        x = self.attn_agg(x)\n        return self.lin_feat_concat(x, feats)\n\n\nclass GloveEmbedding(nn.Module):\n    def __init__(self, num):\n        super().__init__()\n        # Make embedding\n        self.embed = nn.Embedding(400_000 + 1, num)\n        emb_w = pd.read_csv(\n            root_dir / '../input/glove6b200d/glove.6B.200d.txt', header=None, sep=\" \", quoting=csv.QUOTE_NONE\n        ).values[:, 1:].astype('float64')\n        emb_w = Tensor(emb_w)\n        emb_w = torch.cat([emb_w, torch.zeros(1, num)], dim=0)\n        self.embed.weight = nn.Parameter(emb_w)\n\n    def forward(self, x):\n        return self.embed(x.to(torch.long))\n\n\nclass ReadNet(nn.Module):\n    def __init__(self, embed, d_model, n_heads, n_blocks, n_feats_sent, n_feats_doc):\n        super().__init__()\n        self.embed = embed\n        self.sent_block = ReadNetBlock(\n            d_model=d_model, n_heads=n_heads, n_blocks=n_blocks, n_feats=n_feats_sent, n_out=d_model\n        )\n        self.doc_block = ReadNetBlock(\n            d_model=d_model, n_heads=n_heads, n_blocks=n_blocks, n_feats=n_feats_doc, n_out=d_model + n_feats_doc\n        )\n        self.head = nn.Sequential(\n            nn.Linear(d_model + n_feats_doc, 1),\n        )\n\n    def forward(self, x, feats_sent=None, feats_doc=None):  # (b, d, s) tokens, (b, d, n_f_s), (b, n_f_d)\n        if feats_sent is None: feats_sent = Tensor([])\n        if feats_doc is None: feats_doc = Tensor([])\n        if x.is_cuda:\n            feats_sent = feats_sent.cuda()\n            feats_doc = feats_doc.cuda()\n        x = self.embed(x)\n        b, d, s, m = x.shape\n        x = x.reshape(b * d, s, m)\n        sents_enc = self.sent_block(x, feats_sent.reshape(b * d, -1))  # (b*d, m)\n        docs = sents_enc.reshape(b, d, m)\n        docs_enc = self.doc_block(docs, feats_doc)\n        out = self.head(docs_enc)\n        return out.squeeze(1)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T09:38:58.825455Z","iopub.execute_input":"2021-07-22T09:38:58.826125Z","iopub.status.idle":"2021-07-22T09:38:58.862703Z","shell.execute_reply.started":"2021-07-22T09:38:58.826067Z","shell.execute_reply":"2021-07-22T09:38:58.861749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GloveTokenizer:\n    def __init__(self, num):\n        words = pd.read_csv(\n            root_dir / '../input/glove6b200d/glove.6B.200d.txt', header=None, sep=\" \", quoting=csv.QUOTE_NONE, usecols=[0]\n        ).values\n        words = [word[0] for word in words]\n        self.word2idx = {w: i for i, w in enumerate(words)}\n\n    def __call__(self, sent):\n        toks = [self.word2idx.get(w.lower()) for w in word_tokenize(sent)]\n        return [self.unk_token if t is None else t for t in toks]\n\n    @property\n    def unk_token(self):\n        return 400_000  # We appended this to the end of the embedding to return all zeros\n\n    @property\n    def pad_token(self):\n        return self.unk_token  # Seems that this is the best option for GLOVE\n\n\ndef prepare_txts(txts, tokenizer):\n    # Input: (bs,) str, Output: (bs, max_doc_len, max_sent_len)\n    # We choose to elongate all docs and sentences to the max rather than truncate some of them\n    # TODO: Do this better later:\n    # (1) Truncate smartly (if there is one very long outlier sentence or doc)\n    # (2) Group together docs of similar lengths (in terms of num_sents)\n    docs = [[tokenizer(sent) for sent in sent_tokenize(txt)] for txt in txts]\n    # pkl_save(root_dir/\"doc_lens\", pd.Series([len(doc) for doc in docs]))\n    max_doc_len = max([len(doc) for doc in docs])\n    docs = [doc + [[]] * (max_doc_len - len(doc)) for doc in docs]\n    # pkl_save(root_dir/\"sent_lens\", pd.Series([len(sent) for doc in docs for sent in doc]))\n    max_sent_len = max([len(sent) for doc in docs for sent in doc])\n    docs = [[s + [tokenizer.pad_token] * (max_sent_len - len(s)) for s in doc] for doc in docs]\n    return Tensor(docs)\n\n\ndef prepare_txts_cut(txts, tokenizer, max_doc_len=18, max_sent_len=49):\n    docs = [[tokenizer(sent)[:max_sent_len] for sent in sent_tokenize(txt)[:max_doc_len]] for txt in txts]\n    docs = [doc + [[]] * (max_doc_len - len(doc)) for doc in docs]\n    docs = [[s + [tokenizer.pad_token] * (max_sent_len - len(s)) for s in doc] for doc in docs]\n    return Tensor(docs)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-22T09:38:58.864782Z","iopub.execute_input":"2021-07-22T09:38:58.865196Z","iopub.status.idle":"2021-07-22T09:38:58.879349Z","shell.execute_reply.started":"2021-07-22T09:38:58.865154Z","shell.execute_reply":"2021-07-22T09:38:58.878632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize, word_tokenize\nimport tqdm\nimport numpy as np\n\ntest_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\ntokenizer = torch.load('../input/readnet/tokenizer')\n\n\ntxts = test_df.excerpt.tolist()\nx = prepare_txts_cut(txts, tokenizer)\n\nmodel = torch.load('../input/readnet/readnet.pth').cuda()\n\npreds = []\nmodel.eval()\n\nds = torch.utils.data.TensorDataset(x)\ndl = torch.utils.data.DataLoader(ds, shuffle=False, batch_size=16)\nfor batch in tqdm.tqdm(dl):\n    batch = torch.tensor(batch[0]).cuda()\n    pred = model(batch)\n    preds += pred.flatten().cpu().tolist()\n\npreds","metadata":{"execution":{"iopub.status.busy":"2021-07-22T09:38:58.881205Z","iopub.execute_input":"2021-07-22T09:38:58.88166Z","iopub.status.idle":"2021-07-22T09:38:59.751507Z","shell.execute_reply.started":"2021-07-22T09:38:58.881583Z","shell.execute_reply":"2021-07-22T09:38:59.750442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nresult_df = pd.DataFrame(\n    {\n        'id': test_df.id,\n        'target': preds\n    })\n\n\nresult_df.to_csv('submission.csv', index=False)\nresult_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-07-22T09:38:59.753051Z","iopub.execute_input":"2021-07-22T09:38:59.753395Z","iopub.status.idle":"2021-07-22T09:38:59.770418Z","shell.execute_reply.started":"2021-07-22T09:38:59.753358Z","shell.execute_reply":"2021-07-22T09:38:59.769415Z"},"trusted":true},"execution_count":null,"outputs":[]}]}