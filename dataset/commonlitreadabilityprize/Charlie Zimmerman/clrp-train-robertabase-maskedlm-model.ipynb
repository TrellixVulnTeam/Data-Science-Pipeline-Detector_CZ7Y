{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Solution Overview:**\n\nTrain Roberta-Base and RobertaLarge models on the contest data along with supplmemental sources similar to that data.  Fine tune the models using cross-validation folds. Inference weights all 10 models (two trained models * five fine-tuned models [five folds] per model) equally.\n\n**Notebook Sequence:**\n* [Train Roberta Base Model -- **This Notebook**](https://www.kaggle.com/charliezimmerman/clrp-train-robertabase-maskedlm-model)\n* [Train Roberta Large Model](https://www.kaggle.com/charliezimmerman/clrp-train-robertalarge-masked-lm-model/)\n* [Fine Tune Trained Roberta-Base Model](https://www.kaggle.com/charliezimmerman/clrp-finetune-trained-robertabase)\n* [Fine Tune Trained Roberta Large Model](https://www.kaggle.com/charliezimmerman/clrp-finetune-trained-robertalarge)\n* [Inference Notebook](https://www.kaggle.com/charliezimmerman/clrp-inference-robertabase-robertalarge-ensemble)\n\n**This Notebook influenced by:**\n\nhttps://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n\nand by examples/documentation at https://huggingface.co/\n\nNote that due to copyright concerns I am not making the data in the additional-clrp-input folder public. CRLP_Input.csv contains excerpts I manually downloaded from various places, including the site of the contest sponsor [CommonLit](https://www.commonlit.org/)  and [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page). Books.csv was auto-generated using the code at https://www.kaggle.com/charliezimmerman/fetch-clrp-data-from-web/ \n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport transformers\nfrom transformers import (AutoModel,AutoModelForMaskedLM, \n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)\n\nimport torch\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\ngc.enable()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T00:02:53.476899Z","iopub.execute_input":"2021-08-09T00:02:53.477243Z","iopub.status.idle":"2021-08-09T00:02:53.48625Z","shell.execute_reply.started":"2021-08-09T00:02:53.477209Z","shell.execute_reply":"2021-08-09T00:02:53.485427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINED_ROBERTA_FOLDER=\"./robertabase_clrp_model\"\nTRAIN_FILE_IN=\"../input/commonlitreadabilityprize/test.csv\"\nVAL_FILE_IN=\"../input/commonlitreadabilityprize/test.csv\"\nBOOK_DATA=\"../input/additional-clrp-input/books.csv\" #from gutenberg project\nADDL_CLRP_DATA = \"../input/additional-clrp-input/CRLP_Input.csv\" #additional passages from\n                                                                #common.lit.org\nTRAIN_FILE_OUT= \"./clrp_corpus.csv\"\nMODEL_PATH  = '../input/roberta-base'\nEPOCHS=5","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:02:53.488074Z","iopub.execute_input":"2021-08-09T00:02:53.488495Z","iopub.status.idle":"2021-08-09T00:02:53.493917Z","shell.execute_reply.started":"2021-08-09T00:02:53.488427Z","shell.execute_reply":"2021-08-09T00:02:53.493137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set up gpu\nscaler = torch.cuda.amp.GradScaler() \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:02:53.495804Z","iopub.execute_input":"2021-08-09T00:02:53.49629Z","iopub.status.idle":"2021-08-09T00:02:53.507939Z","shell.execute_reply.started":"2021-08-09T00:02:53.496137Z","shell.execute_reply":"2021-08-09T00:02:53.507219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(TRAIN_FILE_IN)\ntest = pd.read_csv(VAL_FILE_IN)\nlit = pd.read_csv(ADDL_CLRP_DATA)\nbooks=pd.read_csv(BOOK_DATA)\ntrain2=train[[\"excerpt\"]]\ntest2=test[[\"excerpt\"]]\nlit2=lit[[\"excerpt\"]]\nbooks2=books[[\"excerpt\"]]\n\n#use everything for training\ntrain=pd.concat([train2,test2, lit2, books2])\n\ntrain['excerpt']=train['excerpt'].apply(lambda x: x if len(x)<= 512 else x[:512])\ntrain['excerpt'] = train['excerpt'].apply(lambda x: x.replace('\\n',''))\n\n\ntrain.to_csv(TRAIN_FILE_OUT, index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:02:53.511037Z","iopub.execute_input":"2021-08-09T00:02:53.511289Z","iopub.status.idle":"2021-08-09T00:02:54.372237Z","shell.execute_reply.started":"2021-08-09T00:02:53.511265Z","shell.execute_reply":"2021-08-09T00:02:54.371367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForMaskedLM.from_pretrained(MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:02:54.375458Z","iopub.execute_input":"2021-08-09T00:02:54.375717Z","iopub.status.idle":"2021-08-09T00:03:01.759206Z","shell.execute_reply.started":"2021-08-09T00:02:54.375691Z","shell.execute_reply":"2021-08-09T00:03:01.758342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To train model using all data for training and evaluation\n# due to limited data size\ntrain_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=TRAIN_FILE_OUT,\n    block_size=256)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=TRAIN_FILE_OUT, \n    block_size=256)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\", \n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy= 'steps',\n    save_total_limit=2,\n    eval_steps=500,\n    save_steps=1000,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end =True,\n    prediction_loss_only=True,\n    report_to = \"none\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:03:01.762539Z","iopub.execute_input":"2021-08-09T00:03:01.762811Z","iopub.status.idle":"2021-08-09T00:03:21.581324Z","shell.execute_reply.started":"2021-08-09T00:03:01.762784Z","shell.execute_reply":"2021-08-09T00:03:21.580442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#~45 minutes\ntrainer.train()\ntrainer.save_model(TRAINED_ROBERTA_FOLDER)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:03:21.584542Z","iopub.execute_input":"2021-08-09T00:03:21.5848Z","iopub.status.idle":"2021-08-09T01:10:11.861306Z","shell.execute_reply.started":"2021-08-09T00:03:21.584775Z","shell.execute_reply":"2021-08-09T01:10:11.86015Z"},"trusted":true},"execution_count":null,"outputs":[]}]}