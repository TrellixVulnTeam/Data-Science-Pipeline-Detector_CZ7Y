{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Solution Overview:**\n\nTrain Roberta-Base and RobertaLarge models on the contest data along with supplmemental sources similar to that data.  Fine tune the models using cross-validation folds. Inference weights all 10 models (two trained models * five fine-tuned models [five folds] per model) equally.\n\n**Notebook Sequence:**\n* [Train Roberta Base Model](https://www.kaggle.com/charliezimmerman/clrp-train-robertabase-maskedlm-model)\n* [Train Roberta Large Model -- **This Notebook**](https://www.kaggle.com/charliezimmerman/clrp-train-robertalarge-masked-lm-model/)\n* [Fine Tune Trained Roberta-Base Model](https://www.kaggle.com/charliezimmerman/clrp-finetune-trained-robertabase)\n* [Fine Tune Trained Roberta Large Model](https://www.kaggle.com/charliezimmerman/clrp-finetune-trained-robertalarge)\n* [Inference Notebook](https://www.kaggle.com/charliezimmerman/clrp-inference-robertabase-robertalarge-ensemble)\n\n**This Notebook influenced by:**\n\nhttps://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n\nand by examples/documentation at https://huggingface.co/\n\nNote that due to copyright concerns I am not making the data in the additional-clrp-input folder public. CRLP_Input.csv contains excerpts I manually downloaded from various places, including the site of the contest sponsor [CommonLit](https://www.commonlit.org/)  and [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page). Books.csv was auto-generated using the code at https://www.kaggle.com/charliezimmerman/fetch-clrp-data-from-web/ ","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport transformers\nfrom transformers import (AutoModel,AutoModelForMaskedLM, \n                          AutoTokenizer, LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer, TrainingArguments)\n\nimport torch\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\ngc.enable()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T05:52:16.927543Z","iopub.execute_input":"2021-08-09T05:52:16.927953Z","iopub.status.idle":"2021-08-09T05:52:23.446698Z","shell.execute_reply.started":"2021-08-09T05:52:16.927868Z","shell.execute_reply":"2021-08-09T05:52:23.44586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINED_ROBERTALARGE_FOLDER=\"./robertalarge_clrp_model\"\nTRAIN_FILE_IN=\"../input/commonlitreadabilityprize/test.csv\"\nVAL_FILE_IN=\"../input/commonlitreadabilityprize/test.csv\"\nBOOK_DATA=\"../input/additional-clrp-input/books.csv\" #from gutenberg project\nADDL_CLRP_DATA = \"../input/additional-clrp-input/CRLP_Input.csv\" #additional passages from\n                                                                #common.lit.org\nTRAIN_FILE_OUT= \"./clrp_corpus.csv\"\nMODEL_PATH  = '../input/robertalarge'\nEPOCHS=5","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:52:26.584994Z","iopub.execute_input":"2021-08-09T05:52:26.585337Z","iopub.status.idle":"2021-08-09T05:52:26.589885Z","shell.execute_reply.started":"2021-08-09T05:52:26.585303Z","shell.execute_reply":"2021-08-09T05:52:26.588942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#set up gpu\nscaler = torch.cuda.amp.GradScaler() \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:52:30.421105Z","iopub.execute_input":"2021-08-09T05:52:30.421464Z","iopub.status.idle":"2021-08-09T05:52:30.476264Z","shell.execute_reply.started":"2021-08-09T05:52:30.421433Z","shell.execute_reply":"2021-08-09T05:52:30.475525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(TRAIN_FILE_IN)\ntest = pd.read_csv(VAL_FILE_IN)\nlit = pd.read_csv(ADDL_CLRP_DATA)\nbooks=pd.read_csv(BOOK_DATA)\ntrain2=train[[\"excerpt\"]]\ntest2=test[[\"excerpt\"]]\nlit2=lit[[\"excerpt\"]]\nbooks2=books[[\"excerpt\"]]\n\n#use everything for training\ntrain=pd.concat([train2,test2, lit2, books2])\n\ntrain['excerpt']=train['excerpt'].apply(lambda x: x if len(x)<= 512 else x[:512])\ntrain['excerpt'] = train['excerpt'].apply(lambda x: x.replace('\\n',''))\n\n\ntrain.to_csv(TRAIN_FILE_OUT, index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:52:32.473226Z","iopub.execute_input":"2021-08-09T05:52:32.473669Z","iopub.status.idle":"2021-08-09T05:52:33.743689Z","shell.execute_reply.started":"2021-08-09T05:52:32.473627Z","shell.execute_reply":"2021-08-09T05:52:33.742816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForMaskedLM.from_pretrained(MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:52:37.109006Z","iopub.execute_input":"2021-08-09T05:52:37.109367Z","iopub.status.idle":"2021-08-09T05:53:00.395215Z","shell.execute_reply.started":"2021-08-09T05:52:37.109332Z","shell.execute_reply":"2021-08-09T05:53:00.394362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To train model using all data for training and evaluation\n# due to limited data size\ntrain_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=TRAIN_FILE_OUT,\n    block_size=128)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=TRAIN_FILE_OUT, \n    block_size=128)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./checkpoints\", \n    overwrite_output_dir=True,\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    evaluation_strategy= 'steps',\n    save_total_limit=1,\n    eval_steps=1000,\n    save_steps=2000,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end =True,\n    prediction_loss_only=True,\n    report_to = \"none\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:54:48.948909Z","iopub.execute_input":"2021-08-09T05:54:48.949317Z","iopub.status.idle":"2021-08-09T05:55:05.324499Z","shell.execute_reply.started":"2021-08-09T05:54:48.949205Z","shell.execute_reply":"2021-08-09T05:55:05.323375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#~45 minutes\ntrainer.train()\ntrainer.save_model(TRAINED_ROBERTALARGE_FOLDER)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:53:28.379205Z","iopub.execute_input":"2021-08-09T05:53:28.379559Z","iopub.status.idle":"2021-08-09T05:54:18.774618Z","shell.execute_reply.started":"2021-08-09T05:53:28.379527Z","shell.execute_reply":"2021-08-09T05:54:18.773003Z"},"trusted":true},"execution_count":null,"outputs":[]}]}