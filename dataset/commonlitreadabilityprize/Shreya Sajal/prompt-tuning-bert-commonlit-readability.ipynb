{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" <h1 style=\"font-family:verdana;\"> <center>CommonLit Readability:Prompt Tuning BERT</center> </h1>","metadata":{}},{"cell_type":"markdown","source":"üìåGPT-2 Fine Tuning:https://www.kaggle.com/shreyasajal/pytorch-openai-gpt2-commonlit-readability","metadata":{}},{"cell_type":"markdown","source":"<h4 style=\"font-family:verdana\">\n    What is Prompt Tuning?<br><br>\nPrompt-tuning is a simple yet effective mechanism for learning ‚Äúsoft prompts‚Äù to condition frozen language models to perform specific downstream tasks.Soft prompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Finally, we show that conditioning,a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.<br>\nInstead of modeling classification as the probability of an output class given some input, p(y|X),where X is a series of tokens and y is a single class label, we now model it as conditional generation,where Y is a sequence of tokens that represent a class label.<br>\nPrompting is the approach of adding extra information for the model to condition on during its generation of Y . Normally, prompting is done by prepending a series of tokens, P, to the input X,such that the model maximizes the likelihood of the\ncorrect Y , pŒ∏(Y |[P; X]), while keeping the model parameters, Œ∏, fixed.<br>\nGiven a series of n tokens, {x0, x1, . . . , xn}, the first thing is embedding the tokens, forming a matrix Xe ‚àà Rn√óe where e is the dimension ofthe embedding space. Our soft-prompts are represented as a parameter Pe ‚àà Rp√óe\n, where p is the length of the prompt. Our prompt is then concatenated to the embedded input forming a single matrix [Pe; Xe] ‚àà R(p+n)√óe\n    \n","metadata":{}},{"cell_type":"markdown","source":"üìå[The Paper:The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691v1.pdf)","metadata":{}},{"cell_type":"markdown","source":"**NOTE:This notebook mainly illustrates the use of prompt embeddings in tuning your model.I didn't implement freezing because it wasn't giving good results in this case,just using the prompts embeddings worked good .Feel free to fork the notebook and experiment freezing or other things with it.**","metadata":{}},{"cell_type":"markdown","source":"# Let's start","metadata":{}},{"cell_type":"markdown","source":"\n<p style=\"color:#159364; font-family:cursive;\">INSTALL THE TRANSFORMERS PACKAGE FROM THE HUGGING FACE LIBRARY</center></p>\n","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-05-22T16:49:57.126257Z","iopub.execute_input":"2021-05-22T16:49:57.126565Z","iopub.status.idle":"2021-05-22T16:50:03.945327Z","shell.execute_reply.started":"2021-05-22T16:49:57.126537Z","shell.execute_reply":"2021-05-22T16:50:03.944387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">IMPORT THE LIBRARIES</center></p>","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport copy\nimport datetime\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport transformers\nfrom transformers import BertTokenizer,BertForSequenceClassification, BertModel, BertConfig\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport plotly.graph_objects as go\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:03.947016Z","iopub.execute_input":"2021-05-22T16:50:03.947377Z","iopub.status.idle":"2021-05-22T16:50:10.809653Z","shell.execute_reply.started":"2021-05-22T16:50:03.947338Z","shell.execute_reply":"2021-05-22T16:50:10.808768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE PROMPT EMBEDDINGS CLASS</center></p>","metadata":{}},{"cell_type":"markdown","source":"Reference:https://github.com/kipgparker/","metadata":{}},{"cell_type":"code","source":"class PROMPTEmbedding(nn.Module):\n    def __init__(self, \n                wte: nn.Embedding,\n                n_tokens: int = 10, \n                random_range: float = 0.5,\n                initialize_from_vocab: bool = True):\n        super(PROMPTEmbedding, self).__init__()\n        self.wte = wte\n        self.n_tokens = n_tokens\n        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n                                                                               n_tokens, \n                                                                               random_range, \n                                                                               initialize_from_vocab))\n            \n    def initialize_embedding(self, \n                             wte: nn.Embedding,\n                             n_tokens: int = 10, \n                             random_range: float = 0.5, \n                             initialize_from_vocab: bool = True):\n        if initialize_from_vocab:\n            return self.wte.weight[:n_tokens].clone().detach()\n        return torch.FloatTensor(wte.weight.size(1), n_tokens).uniform_(-random_range, random_range)\n            \n    def forward(self, tokens):\n        input_embedding = self.wte(tokens[:, self.n_tokens:])\n        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n        return torch.cat([learned_embedding, input_embedding], 1)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:10.811378Z","iopub.execute_input":"2021-05-22T16:50:10.811688Z","iopub.status.idle":"2021-05-22T16:50:10.82073Z","shell.execute_reply.started":"2021-05-22T16:50:10.811662Z","shell.execute_reply":"2021-05-22T16:50:10.819875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">LOOK AT THE DATA</center></p>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint('Number of training sentences: {:,}\\n'.format(df.shape[0]))\ndf.sample(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:10.822316Z","iopub.execute_input":"2021-05-22T16:50:10.822854Z","iopub.status.idle":"2021-05-22T16:50:10.944014Z","shell.execute_reply.started":"2021-05-22T16:50:10.822818Z","shell.execute_reply":"2021-05-22T16:50:10.943074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">A BIT OF PREPROCESSING</center></p>","metadata":{}},{"cell_type":"code","source":"def prep_text(text_df):\n    text_df = text_df.str.replace(\"\\n\",\"\",regex=False) \n    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\ndf[\"excerpt\"] = prep_text(df[\"excerpt\"])\ntest_df[\"excerpt\"] = prep_text(test_df[\"excerpt\"])","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:10.945517Z","iopub.execute_input":"2021-05-22T16:50:10.945918Z","iopub.status.idle":"2021-05-22T16:50:10.967233Z","shell.execute_reply.started":"2021-05-22T16:50:10.945875Z","shell.execute_reply":"2021-05-22T16:50:10.966063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">CREATE FOLDS</center></p>","metadata":{}},{"cell_type":"markdown","source":"Code taken from:https://www.kaggle.com/abhishek/step-1-create-folds","metadata":{}},{"cell_type":"code","source":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\n\n# create folds\ndf = create_folds(df, num_splits=5)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:10.969015Z","iopub.execute_input":"2021-05-22T16:50:10.969447Z","iopub.status.idle":"2021-05-22T16:50:10.992465Z","shell.execute_reply.started":"2021-05-22T16:50:10.96939Z","shell.execute_reply":"2021-05-22T16:50:10.991575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">TRAINING CONFIGURATION</center></p>","metadata":{}},{"cell_type":"code","source":"class CONFIG:\n    seed = 42\n    max_len = 331\n    train_batch = 16\n    valid_batch = 32\n    epochs = 10\n    n_tokens=20\n    learning_rate = 2e-5\n    splits = 5\n    scaler = amp.GradScaler()\n    model='bert-base-cased'\n    tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=True)\n    tokenizer.save_pretrained('./tokenizer')\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:10.994851Z","iopub.execute_input":"2021-05-22T16:50:10.995423Z","iopub.status.idle":"2021-05-22T16:50:14.837561Z","shell.execute_reply.started":"2021-05-22T16:50:10.995382Z","shell.execute_reply":"2021-05-22T16:50:14.836709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">REPRODUCIBILITY</center></p>","metadata":{}},{"cell_type":"code","source":"def set_seed(seed = CONFIG.seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed(CONFIG.seed)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:14.840478Z","iopub.execute_input":"2021-05-22T16:50:14.840845Z","iopub.status.idle":"2021-05-22T16:50:14.84852Z","shell.execute_reply.started":"2021-05-22T16:50:14.840806Z","shell.execute_reply":"2021-05-22T16:50:14.847706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE THE DATASET CLASS</center></p>","metadata":{}},{"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self,df):\n        self.text = df['excerpt'].values\n        self.target = df['target'].values\n        self.max_len = CONFIG.max_len\n        self.tokenizer = CONFIG.tokenizer\n        self.n_tokens=CONFIG.n_tokens\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        text = ' '.join(text.split())\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True\n        )\n        inputs['input_ids']=torch.cat((torch.full((1,self.n_tokens), 500).resize(CONFIG.n_tokens),torch.tensor(inputs['input_ids'], dtype=torch.long)))\n        inputs['attention_mask'] = torch.cat((torch.full((1,self.n_tokens), 1).resize(CONFIG.n_tokens), torch.tensor(inputs['attention_mask'], dtype=torch.long)))\n\n        return {\n            'ids': inputs['input_ids'],\n            'mask': inputs['attention_mask'],\n    \n            'target': torch.tensor(self.target[index], dtype=torch.float)\n        }\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:14.850199Z","iopub.execute_input":"2021-05-22T16:50:14.850557Z","iopub.status.idle":"2021-05-22T16:50:14.860018Z","shell.execute_reply.started":"2021-05-22T16:50:14.85052Z","shell.execute_reply":"2021-05-22T16:50:14.859017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">MODEL:BERT FOR SEQUENCE CLASSIFICATION from ü§ó </center></p>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"color:#159364; font-family:cursive;\">With prompt embeddings in the input,and all the layers have requires_grad True,you can try layer freezing as well\n</center></p>\n","metadata":{}},{"cell_type":"code","source":"model = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels = 1,\n    output_attentions = False,\n    output_hidden_states = False, \n)\nprompt_emb = PROMPTEmbedding(model.get_input_embeddings(), \n                      n_tokens=20, \n                      initialize_from_vocab=True)\nmodel.set_input_embeddings(prompt_emb)\nmodel.cuda()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-05-22T16:50:14.861527Z","iopub.execute_input":"2021-05-22T16:50:14.862125Z","iopub.status.idle":"2021-05-22T16:50:36.577927Z","shell.execute_reply.started":"2021-05-22T16:50:14.862085Z","shell.execute_reply":"2021-05-22T16:50:36.577122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">GET THE PREPARED DATA</center></p>","metadata":{}},{"cell_type":"code","source":"def get_data(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = BERTDataset(df_train)\n    valid_dataset = BERTDataset(df_valid)\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG.valid_batch, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:36.579129Z","iopub.execute_input":"2021-05-22T16:50:36.579451Z","iopub.status.idle":"2021-05-22T16:50:36.585436Z","shell.execute_reply.started":"2021-05-22T16:50:36.579421Z","shell.execute_reply":"2021-05-22T16:50:36.584658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">FOLD:0</center></p>","metadata":{}},{"cell_type":"code","source":"train_dataloader,validation_dataloader=get_data(0)\nlen(train_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:36.586748Z","iopub.execute_input":"2021-05-22T16:50:36.587425Z","iopub.status.idle":"2021-05-22T16:50:36.61847Z","shell.execute_reply.started":"2021-05-22T16:50:36.587384Z","shell.execute_reply":"2021-05-22T16:50:36.617777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">OPTIMIZER</center></p>","metadata":{}},{"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0}\n    ]  \n\noptimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:36.619667Z","iopub.execute_input":"2021-05-22T16:50:36.620027Z","iopub.status.idle":"2021-05-22T16:50:36.631474Z","shell.execute_reply.started":"2021-05-22T16:50:36.619993Z","shell.execute_reply":"2021-05-22T16:50:36.630627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">LEARNING RATE SCHEDULER</center></p>","metadata":{}},{"cell_type":"code","source":"# Defining LR Scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=len(train_dataloader)*CONFIG.epochs\n)\n\nlrs = []\nfor epoch in range(1, CONFIG.epochs + 1):\n    if scheduler is not None:\n        scheduler.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(CONFIG.epochs)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:36.632764Z","iopub.execute_input":"2021-05-22T16:50:36.633396Z","iopub.status.idle":"2021-05-22T16:50:37.39326Z","shell.execute_reply.started":"2021-05-22T16:50:36.633358Z","shell.execute_reply":"2021-05-22T16:50:37.392405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE LOSS AND TIME FUNCTIONS</center></p>","metadata":{}},{"cell_type":"code","source":"def loss_fn(output,target):\n     return torch.sqrt(nn.MSELoss()(output,target))\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:37.394488Z","iopub.execute_input":"2021-05-22T16:50:37.394831Z","iopub.status.idle":"2021-05-22T16:50:37.399429Z","shell.execute_reply.started":"2021-05-22T16:50:37.394793Z","shell.execute_reply":"2021-05-22T16:50:37.398621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE THE FUNCTION FOR TRAINING,VALIDATION AND RUNNING</center></p>","metadata":{}},{"cell_type":"code","source":"def run(model,optimizer,scheduler):\n    set_seed(40)\n    scaler=CONFIG.scaler\n    training_stats = []\n    total_t0 = time.time()\n    best_rmse = np.inf\n    epochs=CONFIG.epochs\n    for epoch_i in range(0, epochs):\n        print(\"\")\n        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n        t0 = time.time()\n        total_train_loss = 0\n        data_size=0\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            tr_loss=[]\n            b_input_ids = batch['ids'].to(CONFIG.device)\n            b_input_mask = batch['mask'].to(CONFIG.device)\n            b_labels = batch['target'].to(CONFIG.device)\n            batch_size = b_input_ids.size(0)\n            model.zero_grad() \n            with amp.autocast(enabled=True):\n                output= model(b_input_ids,attention_mask=b_input_mask)          \n                output=output[\"logits\"].squeeze(-1)\n                loss = loss_fn(output,b_labels)\n                tr_loss.append(loss.item()/len(output))\n            scheduler.step()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        avg_train_loss = np.mean(tr_loss)    \n        training_time = format_time(time.time() - t0)\n        gc.collect()\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n        print(\"\")\n        print(\"Running Validation...\")\n\n        t0 = time.time()\n        model.eval()\n        val_loss = 0\n        allpreds = []\n        alltargets = []\n        for batch in validation_dataloader:\n            losses = []\n            with torch.no_grad():\n                device=CONFIG.device\n                ids = batch[\"ids\"].to(device)\n                mask = batch[\"mask\"].to(device)\n                output = model(ids,mask)\n                output = output[\"logits\"].squeeze(-1)\n                target = batch[\"target\"].to(device)\n                loss = loss_fn(output,target)\n                losses.append(loss.item()/len(output))\n                allpreds.append(output.detach().cpu().numpy())\n                alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n                \n        allpreds = np.concatenate(allpreds)\n        alltargets = np.concatenate(alltargets)\n        val_rmse=mean_squared_error(alltargets, allpreds, squared=False)\n        losses = np.mean(losses)\n        gc.collect() \n        validation_time = format_time(time.time() - t0)\n        print(\"  Validation Loss: {0:.2f}\".format(losses))\n        print(\"  Validation took: {:}\".format(validation_time))\n        \n        if val_rmse <= best_rmse:\n            print(f\"Validation RMSE Improved ({best_rmse} -> {val_rmse})\")\n            best_rmse = val_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"rmse{:.4f}_epoch{:.0f}.bin\".format(best_rmse, epoch_i)\n            torch.save(model.state_dict(), PATH)\n            print(\"Model Saved\")\n            \n        training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': losses,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    ) \n    print(\"\")\n    print(\"Training complete!\")\n    return training_stats  ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:37.400712Z","iopub.execute_input":"2021-05-22T16:50:37.401321Z","iopub.status.idle":"2021-05-22T16:50:37.42087Z","shell.execute_reply.started":"2021-05-22T16:50:37.401278Z","shell.execute_reply":"2021-05-22T16:50:37.420177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <p style=\"color:#159364; font-family:cursive;\">VISUALIZATION FUNCTION </center></p>","metadata":{}},{"cell_type":"code","source":"def Visualizations(training_stats):\n    pd.set_option('precision', 2)\n    df_stats = pd.DataFrame(data=training_stats)\n    df_stats = df_stats.set_index('epoch')\n    layout = go.Layout(template= \"plotly_dark\")\n    fig = go.Figure(layout=layout)\n    fig.add_trace(go.Scatter(x=df_stats.index, y=df_stats['Training Loss'],\n                    mode='lines+markers',\n                    name='Training Loss'))\n    fig.add_trace(go.Scatter(x=df_stats.index, y=df_stats['Valid. Loss'],\n                    mode='lines+markers',\n                    name='Validation Loss'))\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:37.421868Z","iopub.execute_input":"2021-05-22T16:50:37.42232Z","iopub.status.idle":"2021-05-22T16:50:37.434498Z","shell.execute_reply.started":"2021-05-22T16:50:37.422283Z","shell.execute_reply":"2021-05-22T16:50:37.433763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n <p style=\"color:#159364; font-family:cursive;\">RUN THE MODEL WITH PROMPT EMBEDDINGS ON FOLD 0 </center></p>","metadata":{}},{"cell_type":"code","source":"df=run(model,optimizer,scheduler)\nVisualizations(df)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:50:37.435511Z","iopub.execute_input":"2021-05-22T16:50:37.438784Z","iopub.status.idle":"2021-05-22T17:06:55.074606Z","shell.execute_reply.started":"2021-05-22T16:50:37.438745Z","shell.execute_reply":"2021-05-22T17:06:55.07367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n![Upvote!](https://img.shields.io/badge/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)\n","metadata":{}}]}