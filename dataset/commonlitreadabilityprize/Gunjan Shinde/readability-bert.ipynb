{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import codecs\nimport copy\nimport csv\nimport gc\nfrom itertools import chain\nimport os\nimport pickle\nimport random\nimport time\nfrom typing import Dict, List, Tuple, Union\nimport warnings","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:49:20.315603Z","iopub.execute_input":"2021-07-31T17:49:20.315958Z","iopub.status.idle":"2021-07-31T17:49:20.323511Z","shell.execute_reply.started":"2021-07-31T17:49:20.315927Z","shell.execute_reply":"2021-07-31T17:49:20.322327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nimport nltk\nfrom nltk.corpus import wordnet\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops, tensor_util\nfrom tensorflow.python.keras.utils import losses_utils, tf_utils\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops.losses import util as tf_losses_util\nimport tensorflow_addons as tfa\nfrom transformers import AutoConfig, AutoTokenizer, TFAutoModel","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:24:57.756125Z","iopub.execute_input":"2021-07-31T17:24:57.756676Z","iopub.status.idle":"2021-07-31T17:25:01.346318Z","shell.execute_reply.started":"2021-07-31T17:24:57.75663Z","shell.execute_reply":"2021-07-31T17:25:01.345256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.348512Z","iopub.execute_input":"2021-07-31T17:25:01.348976Z","iopub.status.idle":"2021-07-31T17:25:01.355721Z","shell.execute_reply.started":"2021-07-31T17:25:01.348926Z","shell.execute_reply":"2021-07-31T17:25:01.354522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Copyright (C) 2020\n# Author: Joseph Sefara\n# URL: <https://github.com/dsfsi/textaugment/>\n\nclass Wordnet:\n    \"\"\"\n    A set of functions used to augment data.\n    Typical usage: ::\n        >>> import nltk\n        >>> nltk.download('punkt')\n        >>> nltk.download('wordnet')\n        >>> nltk.download('averaged_perceptron_tagger')\n        >>> from textaugment import Wordnet\n        >>> t = Wordnet(v=True,n=True,p=0.5)\n        >>> t.augment('I love school')\n        i adore school\n    \"\"\"\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        A method to initialize parameters\n        :type random_state: int\n        :param random_state: seed\n        :type v: bool\n        :param v: Verb, default is True\n        :type n: bool\n        :param n: Noun\n        :type runs: int\n        :param runs: Number of repetition on single text\n        :type p: float, optional\n        :param p: The probability of success of an individual trial. (0.1<p<1.0), default is 0.5\n        :rtype:   None\n        :return:  Constructer do not return.\n        \"\"\"\n\n        # Set random state\n        if 'random_state' in kwargs:\n            self.random_state = kwargs['random_state']\n            if isinstance(self.random_state, int):\n                np.random.seed(self.random_state)\n            else:\n                raise TypeError(\"random_state must have type int, float, str, bytes,\" \\\n                                \" or bytearray\")\n\n        # Set verb to be default if no values given\n        try:\n            if \"v\" not in kwargs and \"n\" not in kwargs:\n                kwargs['v'] = True\n                kwargs['n'] = False\n            elif \"v\" in kwargs and \"n\" not in kwargs:\n                kwargs['v'] = True\n                kwargs['n'] = False\n            elif \"v\" not in kwargs and \"n\" in kwargs:\n                kwargs['n'] = True\n                kwargs['v'] = False\n            if \"runs\" not in kwargs:\n                kwargs['runs']=1\n\n        except KeyError:\n            raise\n\n        try:\n            if \"p\" in kwargs:\n                if type(kwargs['p']) is not float:\n                    raise TypeError(\"p represent probability of success and \" \\\n                                    \"must be a float from 0.1 to 0.9. E.g p=0.5\")\n                elif type(kwargs['p']) is float:\n                    self.p = kwargs['p']\n            else:\n                kwargs['p'] = 0.5\n        except KeyError:\n            raise\n\n        self.p = kwargs['p']\n        self.v = kwargs['v']\n        self.n = kwargs['n']\n        self.runs = kwargs['runs']\n\n    def geometric(self, data):\n        \"\"\"\n        Used to generate Geometric distribution.\n        \n        :type data: list\n        :param data: Input data\n        :rtype:   ndarray or scalar\n        :return:  Drawn samples from the parameterized Geometric distribution.\n        \"\"\"\n\n        data = np.array(data)\n        first_trial = np.random.geometric(p=self.p, size=data.shape[0]) == 1\n        return data[first_trial]\n\n    def replace(self, data):\n        \"\"\"\n        The method to replace words with synonyms\n        \n        :type data: str\n        :param data: sentence used for data augmentation\n        :rtype:   str\n        :return:  The augmented data\n        \"\"\"\n        data = data.lower().split()\n        data_tokens = [[i, x, y] for i, (x, y) in enumerate(nltk.pos_tag(data))]\n        if self.v:\n            for loop in range(self.runs):\n                words = [[i, x] for i, x, y in data_tokens if y[0] == 'V']\n                words = [i for i in self.geometric(data=words)]\n                if len(words) >= 1:  # There are synonyms\n                    for word in words:\n                        synonyms1 = wordnet.synsets(word[1], wordnet.VERB)\n                        synonyms = list(set(\n                            chain.from_iterable([syn.lemma_names() for syn in synonyms1])\n                        ))\n                        synonyms_ = []  # Synonyms with no underscores goes here\n                        for w in synonyms:\n                            if '_' not in w:\n                                synonyms_.append(w)\n                        if len(synonyms_) >= 1:\n                            synonym = self.geometric(data=synonyms_).tolist()\n                            if synonym:  # There is a synonym\n                                data[int(word[0])] = synonym[0].lower()\n\n        if self.n:\n            for loop in range(self.runs):\n                words = [[i, x] for i, x, y in data_tokens if y[0] == 'N']\n                words = [i for i in self.geometric(data=words)]\n                if len(words) >= 1:  # There are synonyms\n                    for word in words:\n                        synonyms1 = wordnet.synsets(word[1], wordnet.NOUN)\n                        synonyms = list(set(\n                            chain.from_iterable([syn.lemma_names() for syn in synonyms1])\n                        ))\n                        synonyms_ = []\n                        for w in synonyms:\n                            if '_' not in w:\n                                synonyms_.append(w)\n                        if len(synonyms_) >= 1:\n                            synonym = self.geometric(data=synonyms_).tolist()\n                            if synonym:\n                                data[int(word[0])] = synonym[0].lower()\n\n        return \" \".join(data)\n\n    def augment(self, data):\n        \"\"\"\n        Data augmentation for text. Generate new dataset based on verb/nouns synonyms.\n        \n        :type data: str\n        :param data: sentence used for data augmentation \n        :rtype:   str\n        :return:  The augmented data\n        \"\"\"\n        # Error handling\n        if type(data) is not str:\n            raise TypeError(\"Only strings are supported\")\n        data = self.replace(data)\n        return data ","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.357815Z","iopub.execute_input":"2021-07-31T17:25:01.358503Z","iopub.status.idle":"2021-07-31T17:25:01.387701Z","shell.execute_reply.started":"2021-07-31T17:25:01.358455Z","shell.execute_reply":"2021-07-31T17:25:01.386431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LossFunctionWrapper(tf.keras.losses.Loss):\n    def __init__(self,\n                 fn,\n                 reduction=losses_utils.ReductionV2.AUTO,\n                 name=None,\n                 **kwargs):\n        super(LossFunctionWrapper, self).__init__(reduction=reduction, name=name)\n        self.fn = fn\n        self._fn_kwargs = kwargs\n\n    def call(self, y_true, y_pred):\n        if tensor_util.is_tensor(y_pred) and tensor_util.is_tensor(y_true):\n            y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(y_pred, y_true)\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n\n    def get_config(self):\n        config = {}\n        for k, v in six.iteritems(self._fn_kwargs):\n            config[k] = tf.keras.backend.eval(v) if tf_utils.is_tensor_or_variable(v) \\\n                else v\n        base_config = super(LossFunctionWrapper, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.391717Z","iopub.execute_input":"2021-07-31T17:25:01.392136Z","iopub.status.idle":"2021-07-31T17:25:01.403688Z","shell.execute_reply.started":"2021-07-31T17:25:01.392081Z","shell.execute_reply":"2021-07-31T17:25:01.402215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distance_based_log_loss(y_true, y_pred):\n    y_pred = ops.convert_to_tensor(y_pred)\n    y_true = math_ops.cast(y_true, y_pred.dtype)\n    margin = 1.0\n    p = (1.0 + tf.math.exp(-margin)) / (1.0 + tf.math.exp(y_pred - margin))\n    return tf.keras.losses.binary_crossentropy(y_true, p, from_logits=False,\n                                               label_smoothing=0.05)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.405722Z","iopub.execute_input":"2021-07-31T17:25:01.406553Z","iopub.status.idle":"2021-07-31T17:25:01.417066Z","shell.execute_reply.started":"2021-07-31T17:25:01.406501Z","shell.execute_reply":"2021-07-31T17:25:01.415863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DBLLogLoss(LossFunctionWrapper):\n    def __init__(self, reduction=losses_utils.ReductionV2.AUTO,\n                 name='distance_based_log_loss'):\n        super(DBLLogLoss, self).__init__(distance_based_log_loss, name=name,\n                                         reduction=reduction)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.418734Z","iopub.execute_input":"2021-07-31T17:25:01.419191Z","iopub.status.idle":"2021-07-31T17:25:01.427958Z","shell.execute_reply.started":"2021-07-31T17:25:01.419143Z","shell.execute_reply":"2021-07-31T17:25:01.426917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskCalculator(tf.keras.layers.Layer):\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(MaskCalculator, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        super(MaskCalculator, self).build(input_shape)\n\n    def call(self, inputs, **kwargs):\n        return tf.keras.backend.permute_dimensions(\n            x=tf.keras.backend.repeat(\n                x=tf.keras.backend.cast(\n                    x=tf.keras.backend.greater(\n                        x=inputs,\n                        y=0\n                    ),\n                    dtype='float32'\n                ),\n                n=self.output_dim\n            ),\n            pattern=(0, 2, 1)\n        )\n\n    def compute_output_shape(self, input_shape):\n        assert len(input_shape) == 1\n        shape = list(input_shape)\n        shape.append(self.output_dim)\n        return tuple(shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.432301Z","iopub.execute_input":"2021-07-31T17:25:01.432891Z","iopub.status.idle":"2021-07-31T17:25:01.444436Z","shell.execute_reply.started":"2021-07-31T17:25:01.432852Z","shell.execute_reply":"2021-07-31T17:25:01.441045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DatasetGen(tf.keras.utils.Sequence):\n    def __init__(self, data: Dict[str, Tuple[List[int], float, float, np.ndarray]],\n                 data_IDs: List[str],\n                 feature_scaler: Pipeline,\n                 output_scaler: StandardScaler,\n                 token_indices: np.ndarray, pad_token_id: int,\n                 median_distance_between_pairs: float,\n                 batch_size: int, batches_per_epoch: Union[int, None] = None):\n        self.data = copy.deepcopy(data)\n        self.token_indices = token_indices\n        self.pad_token_id = pad_token_id\n        self.batch_size = batch_size\n        self.median_distance_between_pairs = median_distance_between_pairs\n        self.batches_per_epoch = batches_per_epoch\n        self.output_scaler = output_scaler\n        self.feature_scaler = feature_scaler\n        self.pairs = set()\n        for key1 in data_IDs:\n            for key2 in data_IDs:\n                if key1 == key2:\n                    continue\n                if ((key1, key2) not in self.pairs) and ((key2, key1) not in self.pairs):\n                    self.pairs.add((key1, key2))\n        self.pairs = list(self.pairs)\n        random.shuffle(self.pairs)\n        self.n_samples = len(data_IDs)\n    \n    def __len__(self):\n        if self.batches_per_epoch is None:\n            return int(np.ceil(min(len(self.pairs), 5 * self.n_samples) / float(self.batch_size)))\n        return self.batches_per_epoch\n\n    def __getitem__(self, idx):\n        x_left = np.zeros(\n            shape=(self.batch_size, self.token_indices.shape[1]),\n            dtype=np.int32\n        )\n        left_features = []\n        x_right = np.zeros(\n            shape=(self.batch_size, self.token_indices.shape[1]),\n            dtype=np.int32\n        )\n        right_features = []\n        batch_y = [\n            np.zeros(\n                (self.batch_size, 1),\n                dtype=np.int32\n            ),\n            np.zeros(\n                (self.batch_size, 1),\n                dtype=np.float32\n            ),\n            np.zeros(\n                (self.batch_size, 1),\n                dtype=np.float32\n            )\n        ]\n        if self.batches_per_epoch is None:\n            batch_start = idx * self.batch_size\n            batch_end = min(len(self.pairs), batch_start + self.batch_size)\n            for sample_idx in range(batch_end - batch_start):\n                left_key, right_key = self.pairs[sample_idx + batch_start]\n                left_idx = self.data[left_key][0][0]\n                left_features.append(self.data[left_key][3][0:1])\n                left_target = self.data[left_key][1]\n                right_idx = self.data[right_key][0][0]\n                right_target = self.data[right_key][1]\n                right_features.append(self.data[right_key][3][0:1])\n                x_left[sample_idx] = self.token_indices[left_idx]\n                x_right[sample_idx] = self.token_indices[right_idx]\n                if abs(left_target - right_target) < self.median_distance_between_pairs:\n                    batch_y[0][sample_idx, 0] = 1\n                else:\n                    batch_y[0][sample_idx, 0] = 0\n                batch_y[1][sample_idx, 0] = left_target\n                batch_y[2][sample_idx, 0] = right_target\n            n_pad = self.batch_size - (batch_end - batch_start)\n            if n_pad > 0:\n                for sample_idx in range(batch_end - batch_start, self.batch_size):\n                    x_left[sample_idx] = x_left[sample_idx - 1]\n                    x_right[sample_idx] = x_right[sample_idx - 1]\n                    left_features.append(left_features[-1])\n                    right_features.append(right_features[-1])\n                    batch_y[0][sample_idx, 0] = batch_y[0][sample_idx - 1, 0]\n                    batch_y[1][sample_idx, 0] = batch_y[1][sample_idx - 1, 0]\n                    batch_y[2][sample_idx, 0] = batch_y[2][sample_idx - 1, 0]\n        else:\n            for sample_idx in range(self.batch_size):\n                left_key, right_key = random.choice(self.pairs)\n                p = np.ones((len(self.data[left_key][0]),),\n                            dtype=np.float64)\n                p[0] = max(2.0, p.shape[0] - 1.0)\n                p /= p.sum()\n                left_idx_ = np.random.choice(list(range(len(self.data[left_key][0]))), p=p)\n                left_idx = self.data[left_key][0][left_idx_]\n                left_target = np.random.normal(\n                    loc=self.data[left_key][1],\n                    scale=self.data[left_key][2]\n                )\n                left_features.append(self.data[left_key][3][left_idx_:(left_idx_ + 1)])\n                p = np.ones((len(self.data[right_key][0]),),\n                            dtype=np.float64)\n                p[0] = max(2.0, p.shape[0] - 1.0)\n                p /= p.sum()\n                right_idx_ = np.random.choice(list(range(len(self.data[right_key][0]))), p=p)\n                right_idx = self.data[right_key][0][right_idx_]\n                right_target = np.random.normal(\n                    loc=self.data[right_key][1],\n                    scale=self.data[right_key][2]\n                )\n                right_features.append(self.data[right_key][3][right_idx_:(right_idx_ + 1)])\n                x_left[sample_idx] = self.token_indices[left_idx]\n                x_right[sample_idx] = self.token_indices[right_idx]\n                if abs(left_target - right_target) < self.median_distance_between_pairs:\n                    batch_y[0][sample_idx, 0] = 1\n                else:\n                    batch_y[0][sample_idx, 0] = 0\n                batch_y[1][sample_idx, 0] = left_target\n                batch_y[2][sample_idx, 0] = right_target\n        batch_x = [\n            x_left,\n            generate_attention_mask(x_left, self.pad_token_id),\n            self.feature_scaler.transform(np.vstack(left_features)),\n            x_right,\n            generate_attention_mask(x_right, self.pad_token_id), \n            self.feature_scaler.transform(np.vstack(right_features))\n        ]\n        del x_left, x_right\n        batch_y[1] = self.output_scaler.transform(batch_y[1])\n        batch_y[2] = self.output_scaler.transform(batch_y[2]) \n        return batch_x, batch_y, None","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.447313Z","iopub.execute_input":"2021-07-31T17:25:01.44779Z","iopub.status.idle":"2021-07-31T17:25:01.486089Z","shell.execute_reply.started":"2021-07-31T17:25:01.447738Z","shell.execute_reply":"2021-07-31T17:25:01.484856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_attention_mask(token_indices: np.ndarray, padding_id: int) -> np.ndarray:\n    attention = np.zeros(token_indices.shape, dtype=np.int32)\n    for sample_idx in range(token_indices.shape[0]):\n        for token_idx in range(token_indices.shape[1]):\n            if token_indices[sample_idx, token_idx] == padding_id:\n                break\n            attention[sample_idx, token_idx] = 1\n    return attention","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.487624Z","iopub.execute_input":"2021-07-31T17:25:01.48878Z","iopub.status.idle":"2021-07-31T17:25:01.499815Z","shell.execute_reply.started":"2021-07-31T17:25:01.488728Z","shell.execute_reply":"2021-07-31T17:25:01.49876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_text_features(texts: List[List[str]], tok: AutoTokenizer) -> np.ndarray:\n    f = np.zeros((len(texts), 9), dtype=np.float32)\n    for idx, sentences in enumerate(texts):\n        f[idx, 0] = len(sentences)\n        words = []\n        pure_words = []\n        for cur_sent in sentences:\n            words_in_sentence = nltk.word_tokenize(cur_sent)\n            words += words_in_sentence\n            pure_words += list(filter(lambda it: it.isalpha(), words_in_sentence))\n        f[idx, 1] = len(words) / f[idx, 0]\n        f[idx, 2] = len(pure_words) / f[idx, 0]\n        f[idx, 3] = len(' '.join(sentences))\n        f[idx, 4] = len(pure_words)\n        f[idx, 5] = np.mean([len(w) for w in pure_words])\n        for w in pure_words:\n            syllables = tok.tokenize(w.lower())\n            f[idx, 6] += len(syllables)\n            f[idx, 7] += sum(map(lambda it: len(it), syllables))\n        f[idx, 7] /= f[idx, 6]\n        f[idx, 8] = f[idx, 6] / f[idx, 4]\n    return f","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.501601Z","iopub.execute_input":"2021-07-31T17:25:01.502035Z","iopub.status.idle":"2021-07-31T17:25:01.517075Z","shell.execute_reply.started":"2021-07-31T17:25:01.501987Z","shell.execute_reply":"2021-07-31T17:25:01.515993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data_for_training(\n    fname: str,\n    tok: AutoTokenizer\n) -> List[Dict[str, Tuple[List[str], float, float, np.ndarray]]]:\n    loaded_header = []\n    id_col_idx = -1\n    text_col_idx = -1\n    target_col_idx = -1\n    std_col_idx = -1\n    line_idx = 1\n    data = dict()\n    set_of_texts = set()\n    t = Wordnet(v=True, n=True, p=0.5)\n    with codecs.open(fname, mode='r', encoding='utf-8') as fp:\n        data_reader = csv.reader(fp, quotechar='\"', delimiter=',')\n        for row in data_reader:\n            if len(row) > 0:\n                err_msg = f'File {fname}: line {line_idx} is wrong!'\n                if len(loaded_header) == 0:\n                    loaded_header = copy.copy(row)\n                    try:\n                        text_col_idx = loaded_header.index('excerpt')\n                    except:\n                        text_col_idx = -1\n                    if text_col_idx <= 0:\n                        raise ValueError(err_msg + ' Field \"excerpt\" is not found!')\n                    try:\n                        id_col_idx = loaded_header.index('id')\n                    except:\n                        id_col_idx = -1\n                    if id_col_idx < 0:\n                        raise ValueError(err_msg + ' Field \"id\" is not found!')\n                    try:\n                        target_col_idx = loaded_header.index('target')\n                    except:\n                        target_col_idx = -1\n                    if target_col_idx < 0:\n                        raise ValueError(err_msg + ' Field \"target\" is not found!')\n                    try:\n                        std_col_idx = loaded_header.index('standard_error')\n                    except:\n                        std_col_idx = -1\n                    if std_col_idx < 0:\n                        err_msg2 = f'{err_msg} Field \"standard_error\" is not found!'\n                        raise ValueError(err_msg2)\n                else:\n                    sample_id = row[id_col_idx]\n                    if sample_id != sample_id.strip():\n                        raise ValueError(err_msg + f' {sample_id} is wrong sample ID!')\n                    if sample_id in data:\n                        err_msg2 = f'{err_msg} {sample_id} is not unique sample ID!'\n                        raise ValueError(err_msg2)\n                    text = row[text_col_idx].replace('\\r', '\\n')\n                    if len(text) == 0:\n                        raise ValueError(err_msg + f' Text {sample_id} is empty!')\n                    sentences = []\n                    for paragraph in map(lambda it: it.strip(), text.split('\\n')):\n                        if len(paragraph) > 0:\n                            sentences += nltk.sent_tokenize(paragraph)\n                    if len(sentences) == 0:\n                        raise ValueError(err_msg + f' Text {sample_id} is empty!')\n                    text = ' '.join([cur_sent.lower() for cur_sent in sentences])\n                    if text in set_of_texts:\n                        raise ValueError(err_msg + f' Text {sample_id} is not unique!')\n                    set_of_texts.add(text.lower())\n                    added_texts = [[cur_sent.lower() for cur_sent in sentences]]\n                    try:\n                        target_val = float(row[target_col_idx])\n                        ok = True\n                    except:\n                        target_val = 0.0\n                        ok = False\n                    if not ok:\n                        err_msg2 = err_msg\n                        err_msg2 += f' {row[target_col_idx]} is wrong target for ' \\\n                                    f'text {sample_id}.'\n                        raise ValueError(err_msg2)\n                    try:\n                        std_val = float(row[std_col_idx])\n                        ok = (std_val > 0.0)\n                    except:\n                        std_val = 0.0\n                        ok = False\n                    if not ok:\n                        err_msg2 = err_msg\n                        err_msg2 += f' {row[std_col_idx]} is wrong standard error' \\\n                                    f' for text {sample_id}.'\n                        warnings.warn(err_msg2)\n                    else:\n                        for _ in range(3):\n                            new_augmented_text = []\n                            for cur_sent in sentences:\n                                new_sent = t.augment(cur_sent.lower()).strip().lower()\n                                if len(new_sent) > 0:\n                                    new_augmented_text.append(new_sent)\n                            assert len(new_augmented_text) > 0\n                            random.shuffle(new_augmented_text)\n                            new_augmented_text_ = ' '.join(new_augmented_text)\n                            if (len(new_augmented_text_) > 0) and \\\n                                    (new_augmented_text_ not in set_of_texts):\n                                set_of_texts.add(new_augmented_text_)\n                                added_texts.append(new_augmented_text)\n                            del new_augmented_text, new_augmented_text_\n                        data[sample_id] = (\n                            list(map(lambda it: ' '.join(it), added_texts)),\n                            target_val, std_val,\n                            calc_text_features(added_texts, tok)\n                        )\n            line_idx += 1\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.518531Z","iopub.execute_input":"2021-07-31T17:25:01.519078Z","iopub.status.idle":"2021-07-31T17:25:01.546531Z","shell.execute_reply.started":"2021-07-31T17:25:01.519011Z","shell.execute_reply":"2021-07-31T17:25:01.545332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data_for_testing(fname: str, tok: AutoTokenizer, batch_size: int):\n    loaded_header = []\n    id_col_idx = -1\n    text_col_idx = -1\n    target_col_idx = -1\n    std_col_idx = -1\n    line_idx = 1\n    data = dict()\n    with codecs.open(fname, mode='r', encoding='utf-8') as fp:\n        data_reader = csv.reader(fp, quotechar='\"', delimiter=',')\n        for row in data_reader:\n            if len(row) > 0:\n                err_msg = f'File {fname}: line {line_idx} is wrong!'\n                if len(loaded_header) == 0:\n                    loaded_header = copy.copy(row)\n                    try:\n                        text_col_idx = loaded_header.index('excerpt')\n                    except:\n                        text_col_idx = -1\n                    if text_col_idx <= 0:\n                        raise ValueError(err_msg + ' Field \"excerpt\" is not found!')\n                    try:\n                        id_col_idx = loaded_header.index('id')\n                    except:\n                        id_col_idx = -1\n                    if id_col_idx < 0:\n                        raise ValueError(err_msg + ' Field \"id\" is not found!')\n                else:\n                    sample_id = row[id_col_idx]\n                    if sample_id != sample_id.strip():\n                        raise ValueError(err_msg + f' {sample_id} is wrong sample ID!')\n                    if sample_id in data:\n                        err_msg2 = f'{err_msg} {sample_id} is not unique sample ID!'\n                        raise ValueError(err_msg2)\n                    text = row[text_col_idx].replace('\\n', ' ').replace('\\r', ' ')\n                    text = ' '.join(text.split()).strip()\n                    if len(text) == 0:\n                        raise ValueError(err_msg + f' Text {sample_id} is empty!')\n                    features = calc_text_features([nltk.sent_tokenize(text)], tok) \n                    data[sample_id] = (text, features)\n                    if len(data) >= batch_size:\n                        yield data\n                        del data\n                        data = dict()\n            line_idx += 1\n    if len(data) > 0:\n        yield data","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.548267Z","iopub.execute_input":"2021-07-31T17:25:01.548744Z","iopub.status.idle":"2021-07-31T17:25:01.564979Z","shell.execute_reply.started":"2021-07-31T17:25:01.548695Z","shell.execute_reply":"2021-07-31T17:25:01.563696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_output_scaler(data: Dict[str, Tuple[List[int], float, float,\n                                              np.ndarray]]) -> StandardScaler:\n    outputs_for_training = np.empty((len(data), 1), dtype=np.float64)\n    for idx, sample_id in enumerate(list(data.keys())):\n        outputs_for_training[idx, 0] = data[sample_id][1]\n    return StandardScaler().fit(outputs_for_training)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.566833Z","iopub.execute_input":"2021-07-31T17:25:01.567484Z","iopub.status.idle":"2021-07-31T17:25:01.576272Z","shell.execute_reply.started":"2021-07-31T17:25:01.567435Z","shell.execute_reply":"2021-07-31T17:25:01.575191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_feature_scaler(data: Dict[str, Tuple[List[int], float, float,\n                                               np.ndarray]]) -> Pipeline:\n    features_for_training = []\n    for sample_id in data:\n        features_for_training.append(data[sample_id][3])\n    features_for_training = np.vstack(features_for_training)\n    scaler = Pipeline(steps=[\n        ('scaler', StandardScaler()),\n        ('transformer', PowerTransformer())\n    ])\n    return scaler.fit(features_for_training)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.578294Z","iopub.execute_input":"2021-07-31T17:25:01.57912Z","iopub.status.idle":"2021-07-31T17:25:01.587493Z","shell.execute_reply.started":"2021-07-31T17:25:01.579071Z","shell.execute_reply":"2021-07-31T17:25:01.586419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_data(\n    data: Union[List[Dict[str, Tuple[str, np.ndarray]]],\n                List[Dict[str, Tuple[List[str], float, float, np.ndarray]]]],\n    tokenizer: AutoTokenizer, max_seq_len: int\n) -> Tuple[Union[Dict[str, Tuple[int, np.ndarray]],\n                 Dict[str, Tuple[List[int], float, float, np.ndarray]]],\n           np.ndarray]:\n    tokenized_data = dict()\n    all_tokens_matrix = []\n    for sample_idx, cur_ID in enumerate(sorted(list(data.keys()))):\n        if len(data[cur_ID]) == 2:\n            tokens = tokenizer.tokenize(data[cur_ID][0].lower())\n            tokenized_data[cur_ID] = (len(all_tokens_matrix), data[cur_ID][1])\n            token_ids = tokenizer.convert_tokens_to_ids(\n                [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n            )\n            ndiff = max_seq_len - len(token_ids)\n            if ndiff > 0:\n                token_ids += [tokenizer.pad_token_id for _ in range(ndiff)]\n            elif ndiff < 0:\n                token_ids = token_ids[:max_seq_len]\n            all_tokens_matrix.append(token_ids)\n        else:\n            text_idx_list = []\n            for cur_text in data[cur_ID][0]:\n                tokens = tokenizer.tokenize(cur_text.lower())\n                token_ids = tokenizer.convert_tokens_to_ids(\n                    [tokenizer.cls_token] + tokens + [tokenizer.sep_token]\n                )\n                ndiff = max_seq_len - len(token_ids)\n                if ndiff > 0:\n                    token_ids += [tokenizer.pad_token_id for _ in range(ndiff)]\n                elif ndiff < 0:\n                    token_ids = token_ids[:max_seq_len]\n                text_idx_list.append(len(all_tokens_matrix))\n                all_tokens_matrix.append(token_ids)\n            tokenized_data[cur_ID] = (text_idx_list, data[cur_ID][1], data[cur_ID][2],\n                                      data[cur_ID][3])\n    return tokenized_data, np.array(all_tokens_matrix, dtype=np.int32)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.589304Z","iopub.execute_input":"2021-07-31T17:25:01.589876Z","iopub.status.idle":"2021-07-31T17:25:01.60713Z","shell.execute_reply.started":"2021-07-31T17:25:01.589828Z","shell.execute_reply":"2021-07-31T17:25:01.605864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_info_about_data(\n    data: Union[List[Dict[str, Tuple[str, np.ndarray]]],\n                List[Dict[str, Tuple[List[str], float, float, np.ndarray]]]],\n    identifiers: List[str]\n):\n    for_training = (len(data[identifiers[0]]) == 4)\n    if for_training:\n        print(f'Number of samples for training is {len(data)}.')\n    else:\n        print(f'Number of samples for submission is {len(data)}.')\n    print('')\n    print(f'{len(identifiers)} random samples:')\n    for cur_id in identifiers:\n        print('')\n        print(f'  Sample {cur_id}')\n        if for_training:\n            print('  Text:')\n            print(f'    {data[cur_id][0][0]}')\n            print(f'  Number of augmented texts is {len(data[cur_id][0]) - 1}.')\n            if (len(data[cur_id][0]) - 1) > 0:\n                if (len(data[cur_id][0]) - 1) > 1:\n                    print('  2 augmented texts:')\n                    for augmented in data[cur_id][0][1:3]:\n                        print(f'    {augmented}')\n                else:\n                    print('  Augmented text:')\n                    for augmented in data[cur_id][0][1:2]:\n                        print(f'    {augmented}')\n            print('  Target:')\n            print(f'    {data[cur_id][1]} +- {data[cur_id][2]}')\n            print('  Features:')\n            for it in data[cur_id][3].tolist(): print(f'    {it}') \n        else:\n            print(' Text:')\n            print(f'    {data[cur_id][0]}')\n            print(' Features:')\n            print(f'    {data[cur_id][1].tolist()[0]}')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.608891Z","iopub.execute_input":"2021-07-31T17:25:01.609779Z","iopub.status.idle":"2021-07-31T17:25:01.624176Z","shell.execute_reply.started":"2021-07-31T17:25:01.609726Z","shell.execute_reply":"2021-07-31T17:25:01.623033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def print_info_about_tokenized_data(\n    data: Union[Dict[str, Tuple[int, np.ndarray]],\n                Dict[str, Tuple[List[int], float, float, np.ndarray]]],\n    matrix: np.ndarray,\n    identifiers: List[str]\n):\n    for_training = (len(data[identifiers[0]]) == 4)\n    if for_training:\n        print(f'Number of tokenized samples for training is {len(data)}.')\n    else:\n        print(f'Number of tokenized samples for submission is {len(data)}.')\n    print('')\n    print(f'{len(identifiers)} random samples:')\n    for cur_id in identifiers:\n        print('')\n        print(f'Sample {cur_id}')\n        print('')\n        sample_idx = data[cur_id][0][0]\n        print(matrix[sample_idx].tolist())\n        print('')\n        print(data[cur_id][-1][0].tolist())\n        print('')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.625833Z","iopub.execute_input":"2021-07-31T17:25:01.626451Z","iopub.status.idle":"2021-07-31T17:25:01.637597Z","shell.execute_reply.started":"2021-07-31T17:25:01.626399Z","shell.execute_reply":"2021-07-31T17:25:01.636525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_median_distance_between_pairs(data: Dict[str, Tuple[List[int], float, float,\n                                                             np.ndarray]],\n                                       identifiers: List[str]) -> float:\n    distances = []\n    assert len(identifiers) == len(set(identifiers))\n    for idx, first_id in enumerate(identifiers):\n        first_target = data[first_id][1]\n        for second_id in identifiers[(idx + 1):]:\n            second_target = data[second_id][1]\n            distances.append(abs(first_target - second_target))\n    distances.sort()\n    distances = np.array(distances, dtype=np.float32)\n    n = distances.shape[0]\n    print('Mean distance between training pairs is {0:.5f}.'.format(\n        np.mean(distances)\n    ))\n    print('Minimal distance between training pairs is {0:.5f}.'.format(\n        np.min(distances)\n    ))\n    print('Maximal distance between training pairs is {0:.5f}.'.format(\n        np.max(distances)\n    ))\n    print('Median distance between training pairs is {0:.5f}.'.format(\n        distances[(n - 1) // 2]\n    ))\n    return distances[(n - 1) // 2]","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.639325Z","iopub.execute_input":"2021-07-31T17:25:01.639906Z","iopub.status.idle":"2021-07-31T17:25:01.65308Z","shell.execute_reply.started":"2021-07-31T17:25:01.639857Z","shell.execute_reply":"2021-07-31T17:25:01.651709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tf_euclidean_distance(vects):\n    x, y = vects\n    sum_square = tf.keras.backend.sum(tf.keras.backend.square(x - y),\n                                      axis=1, keepdims=True)\n    return tf.keras.backend.sqrt(\n        tf.keras.backend.maximum(sum_square, tf.keras.backend.epsilon())\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.654838Z","iopub.execute_input":"2021-07-31T17:25:01.655673Z","iopub.status.idle":"2021-07-31T17:25:01.66296Z","shell.execute_reply.started":"2021-07-31T17:25:01.655622Z","shell.execute_reply":"2021-07-31T17:25:01.662036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tf_eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.666151Z","iopub.execute_input":"2021-07-31T17:25:01.667668Z","iopub.status.idle":"2021-07-31T17:25:01.675854Z","shell.execute_reply.started":"2021-07-31T17:25:01.667606Z","shell.execute_reply":"2021-07-31T17:25:01.674778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_neural_network(bert_name: str, max_seq_len: int, feature_vector_size: int,\n                         batch_size: int) -> Tuple[tf.keras.Model, tf.keras.Model]:\n    transformer_model = TFAutoModel.from_pretrained(\n        pretrained_model_name_or_path=bert_name,\n        name='BaseTransformer'\n    )\n    united_embedding_size = 512\n    transformer_config = AutoConfig.from_pretrained(bert_name)\n    united_emb_layer = tf.keras.layers.Dense(\n        units=united_embedding_size, input_dim=transformer_config.hidden_size,\n        activation='tanh',\n        kernel_initializer=tf.keras.initializers.GlorotNormal(seed=42),\n        bias_initializer='zeros',\n        name='UnitedEmbeddingLayer'\n    )\n    print('Transformer Configuration')\n    print('=========================')\n    print(transformer_config)\n    left_tokens = tf.keras.layers.Input(shape=(max_seq_len,), batch_size=batch_size,\n                                        dtype=tf.int32, name='word_ids')\n    left_attention = tf.keras.layers.Input(shape=(max_seq_len,), batch_size=batch_size,\n                                           dtype=tf.int32, name='attention_mask')\n    left_features = tf.keras.layers.Input(shape=(feature_vector_size,), dtype=tf.float32,\n                                          batch_size=batch_size, name='features')\n    right_tokens = tf.keras.layers.Input(shape=(max_seq_len,), batch_size=batch_size,\n                                         dtype=tf.int32, name='right_word_ids')\n    right_attention = tf.keras.layers.Input(shape=(max_seq_len,), batch_size=batch_size,\n                                            dtype=tf.int32, name='right_attention_mask')\n    right_features = tf.keras.layers.Input(shape=(feature_vector_size,), dtype=tf.float32,\n                                           batch_size=batch_size, name='right_features')\n    left_sequence_output = transformer_model([left_tokens, left_attention])[0]\n    left_output_mask = MaskCalculator(\n        output_dim=transformer_config.hidden_size, trainable=False,\n        name='OutMaskCalculator'\n    )(left_attention)\n    left_masked_output = tf.keras.layers.Multiply(\n        name='OutMaskMultiplicator'\n    )([left_output_mask, left_sequence_output])\n    left_masked_output = tf.keras.layers.Masking(\n        name='OutMasking'\n    )(left_masked_output)\n    left_output = tf.keras.layers.GlobalAvgPool1D(name='AvePool')(left_masked_output)\n    left_output = tf.keras.layers.LayerNormalization(\n        name='Emdedding'\n    )(left_output)\n    left_output = tf.keras.layers.Concatenate(\n        name='Concat'\n    )([left_output, left_features])\n    left_output = tf.keras.layers.Dropout(\n        rate=0.3, seed=42, name='Dropout1' \n    )(left_output)\n    left_output = united_emb_layer(left_output)\n    right_sequence_output = transformer_model([right_tokens, right_attention])[0]\n    right_output_mask = MaskCalculator(\n        output_dim=transformer_config.hidden_size, trainable=False,\n        name='OutMaskCalculator_right'\n    )(right_attention)\n    right_masked_output = tf.keras.layers.Multiply(\n        name='OutMaskMultiplicator_right'\n    )([right_output_mask, right_sequence_output])\n    right_masked_output = tf.keras.layers.Masking(\n        name='OutMasking_right'\n    )(right_masked_output)\n    right_output = tf.keras.layers.GlobalAvgPool1D(\n        name='AvePool_right'\n    )(right_masked_output)\n    right_output = tf.keras.layers.LayerNormalization(\n       name='Emdedding_right'\n    )(right_output)\n    right_output = tf.keras.layers.Concatenate(\n        name='Concat_right'\n    )([right_output, right_features])\n    right_output = tf.keras.layers.Dropout(\n        rate=0.3, seed=42, name='Dropout1_right'\n    )(right_output)\n    right_output = united_emb_layer(right_output)\n    distance_output = tf.keras.layers.Lambda(\n        function=tf_euclidean_distance,\n        output_shape=tf_eucl_dist_output_shape,\n        name='L2DistLayer'\n    )([left_output, right_output])\n    regression_layer = tf.keras.layers.Dense(\n        units=1, input_dim=united_embedding_size, activation=None,\n        kernel_initializer=tf.keras.initializers.GlorotNormal(seed=42),\n        bias_initializer='zeros',\n        name='RegressionLayer'\n    )\n    left_regression_output = tf.keras.layers.Dropout(\n        rate=0.3, seed=42, name='Dropout2'\n    )(left_output)\n    left_regression_output = regression_layer(left_regression_output)\n    right_regression_output = tf.keras.layers.Dropout(\n        rate=0.3, seed=42, name='Dropout2_right'\n    )(right_output)\n    right_regression_output = regression_layer(right_regression_output)\n    regression_model = tf.keras.Model(\n        inputs=[left_tokens, left_attention, left_features],\n        outputs=[left_regression_output, left_output],\n        name='RegressionModel'\n    )\n    regression_model.build(input_shape=[(batch_size, max_seq_len),\n                                        (batch_size, max_seq_len),\n                                        (batch_size, feature_vector_size)])\n    siamese_model = tf.keras.Model(\n        inputs=[left_tokens, left_attention, left_features,\n                right_tokens, right_attention, right_features],\n        outputs=[distance_output, left_regression_output, right_regression_output],\n        name='SiameseModel'\n    )\n    radam = tfa.optimizers.RectifiedAdam(learning_rate=1e-4)\n    ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n    losses = [\n        DBLLogLoss(),\n        tf.keras.losses.MeanSquaredError(),\n        tf.keras.losses.MeanSquaredError()\n    ]\n    loss_weights = [2.0, 1.0, 1.0]\n    siamese_model.compile(optimizer=ranger, loss=losses,\n                          loss_weights=loss_weights)\n    return siamese_model, regression_model","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.678225Z","iopub.execute_input":"2021-07-31T17:25:01.678742Z","iopub.status.idle":"2021-07-31T17:25:01.708733Z","shell.execute_reply.started":"2021-07-31T17:25:01.678693Z","shell.execute_reply":"2021-07-31T17:25:01.707656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_minibatch(X: List[np.ndarray], y: List[np.ndarray]):\n    assert len(X) == 6\n    assert len(y) == 3\n    print('')\n    print('X1')\n    for it in X[0].tolist(): print(it)\n    print('')\n    print('X2')\n    for it in X[1].tolist(): print(it)\n    print('')\n    print('X3')\n    for it in X[2].tolist(): print(it)\n    print('')\n    print('X4')\n    for it in X[3].tolist(): print(it)\n    print('')\n    print('X5')\n    for it in X[4].tolist(): print(it)\n    print('X6')\n    for it in X[5].tolist(): print(it) \n    print('')\n    print('y1')\n    for it in y[0].tolist(): print(it)\n    print('')\n    print('y2')\n    for it in y[1].tolist(): print(it)\n    print('')\n    print('y3')\n    for it in y[2].tolist(): print(it)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.712336Z","iopub.execute_input":"2021-07-31T17:25:01.713171Z","iopub.status.idle":"2021-07-31T17:25:01.725738Z","shell.execute_reply.started":"2021-07-31T17:25:01.713117Z","shell.execute_reply":"2021-07-31T17:25:01.724584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_tsne(regressor: tf.keras.Model, batch_size: int,\n              data: Dict[str, Tuple[List[int], float, float, np.ndarray]],\n              feature_scaler: Pipeline,\n              token_matrix: np.ndarray,\n              identifiers: List[str], pad_id: int, title: str, figure_id: int):\n    indices = list(map(lambda it: data[it][0][0], identifiers))\n    colors = np.array(\n        list(map(lambda it: data[it][1], identifiers)),\n        dtype=np.float64\n    )\n    area = np.array(\n        list(map(lambda it: data[it][2], identifiers)),\n        dtype=np.float64\n    )\n    area /= np.max(area)\n    area *= 10.0\n    area = np.power(area, 2)\n    texts = token_matrix[indices]\n    src_features = np.vstack(\n        list(map(\n            lambda it: data[it][3][0:1], \n            identifiers\n        ))\n    )\n    assert src_features.shape[0] == texts.shape[0]\n    ndiff = texts.shape[0] % batch_size\n    if ndiff > 0:\n        last_text_idx = texts.shape[0] - 1\n        texts = np.vstack(\n            [texts] + \n            [texts[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n        src_features = np.vstack(\n            [src_features] +\n            [src_features[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n    attentions = generate_attention_mask(texts, pad_id)\n    assert texts.shape[0] % batch_size == 0, f'{texts.shape[0] % batch_size}'\n    _, features = regressor.predict(\n        [texts, attentions, feature_scaler.transform(src_features)],\n        batch_size=batch_size\n    )\n    features = features[:len(indices)]\n    projected_features = TSNE(n_components=2, n_jobs=-1).fit_transform(features)\n    fig = plt.figure(figure_id, figsize=(11, 11))\n    plt.scatter(x=projected_features[:, 0], y=projected_features[:, 1],\n                marker='o', cmap=plt.cm.get_cmap(\"jet\"), s=area,\n                c=colors, norm=Normalize(vmin=np.min(colors), vmax=np.max(colors)))\n    plt.title('t-SNE projections of texts ' + title)\n    plt.colorbar()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.729925Z","iopub.execute_input":"2021-07-31T17:25:01.73029Z","iopub.status.idle":"2021-07-31T17:25:01.750005Z","shell.execute_reply.started":"2021-07-31T17:25:01.730253Z","shell.execute_reply":"2021-07-31T17:25:01.747998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_training_process(history: tf.keras.callbacks.History, metric_name: str,\n                          figure_id: int):\n    val_metric_name = 'val_' + metric_name\n    possible_metrics = list(history.history.keys())\n    if metric_name not in history.history:\n        err_msg = f'The metric \"{metric_name}\" is not found!'\n        err_msg += f' Available metrics are: {possible_metrics}.'\n        raise ValueError(err_msg)\n    fig = plt.figure(figure_id, figsize=(7, 7))\n    metric_values = history.history[metric_name]\n    plt.plot(list(range(len(metric_values))), metric_values,\n             label='Training {0}'.format(metric_name))\n    if val_metric_name in history.history:\n        val_metric_values = history.history['val_' + metric_name]\n        assert len(metric_values) == len(val_metric_values)\n        plt.plot(list(range(len(val_metric_values))), val_metric_values,\n                 label='Validation {0}'.format(metric_name))\n    plt.xlabel('Epochs')\n    plt.ylabel(metric_name)\n    plt.title('Training process')\n    plt.legend(loc='best')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.754664Z","iopub.execute_input":"2021-07-31T17:25:01.757666Z","iopub.status.idle":"2021-07-31T17:25:01.775457Z","shell.execute_reply.started":"2021-07-31T17:25:01.757594Z","shell.execute_reply":"2021-07-31T17:25:01.774205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_new_trainset(regressor: tf.keras.Model, feature_scaler: Pipeline,\n                          output_scaler: StandardScaler, batch_size: int,\n                          data: Dict[str, Tuple[List[int], float, float]],\n                          token_matrix: np.ndarray, pad_id: int,\n                          identifiers: List[str]) -> Tuple[np.ndarray, np.ndarray,\n                                                           np.ndarray]:\n    indices = list(map(lambda it: data[it][0][0], identifiers))\n    texts = token_matrix[indices]\n    src_features = np.vstack(list(map(lambda it: data[it][3][0:1], identifiers)))\n    targets = np.array(list(map(lambda it: data[it][1], identifiers)),\n                       dtype=np.float64)\n    assert texts.shape[0] == src_features.shape[0]\n    ndiff = texts.shape[0] % batch_size\n    if ndiff > 0:\n        last_text_idx = texts.shape[0] - 1\n        texts = np.vstack(\n            [texts] + \n            [texts[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n        src_features = np.vstack(\n            [src_features] +\n            [src_features[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n    attentions = generate_attention_mask(texts, pad_id)\n    assert texts.shape[0] % batch_size == 0, f'{texts.shape[0] % batch_size}'\n    predictions, target_features = regressor.predict(\n        [texts, attentions, feature_scaler.transform(src_features)],\n        batch_size=batch_size\n    )\n    assert predictions.shape[0] == target_features.shape[0]\n    assert target_features.shape[1] > 1\n    target_features = target_features[:len(identifiers)]\n    predictions = output_scaler.inverse_transform(\n        predictions[:len(identifiers)]\n    ).reshape((len(identifiers),))\n    return target_features, targets, predictions","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.78433Z","iopub.execute_input":"2021-07-31T17:25:01.784698Z","iopub.status.idle":"2021-07-31T17:25:01.798654Z","shell.execute_reply.started":"2021-07-31T17:25:01.784662Z","shell.execute_reply":"2021-07-31T17:25:01.797519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def do_predictions(regressor: tf.keras.Model, feature_scaler: Pipeline,\n                   output_scaler: StandardScaler, \n                   batch_size: int,\n                   data: Union[Dict[str, int], Dict[str, Tuple[List[int], float, float]]],\n                   token_matrix: np.ndarray, pad_id: int,\n                   identifiers: List[str]=None) -> Dict[str, Tuple[float, np.ndarray]]:\n    if identifiers is None:\n        identifiers_ = sorted(list(data.keys()))\n    else:\n        identifiers_ = sorted(identifiers)\n    indices = list(map(\n        lambda it: data[it][0] if len(data[it]) == 2 else data[it][0][0],\n        identifiers_\n    ))\n    texts = token_matrix[indices]\n    src_features = np.vstack(\n        list(map(\n            lambda it: data[it][1] if len(data[it]) == 2 else data[it][3][0:1],\n            identifiers_\n        ))\n    )\n    assert texts.shape[0] == src_features.shape[0]\n    ndiff = texts.shape[0] % batch_size\n    if ndiff > 0:\n        last_text_idx = texts.shape[0] - 1\n        texts = np.vstack(\n            [texts] + \n            [texts[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n        src_features = np.vstack(\n            [src_features] +\n            [src_features[last_text_idx:(last_text_idx + 1)]\n             for _ in range(batch_size - ndiff)]\n        )\n    attentions = generate_attention_mask(texts, pad_id)\n    assert texts.shape[0] % batch_size == 0, f'{texts.shape[0] % batch_size}'\n    predictions, target_features = regressor.predict(\n        [texts, attentions, feature_scaler.transform(src_features)],\n        batch_size=batch_size\n    )\n    assert predictions.shape[0] == target_features.shape[0]\n    assert target_features.shape[1] > 1\n    predictions = np.reshape(predictions, newshape=(predictions.shape[0], 1))\n    predictions = output_scaler.inverse_transform(predictions)\n    return dict(map(\n        lambda idx: (\n            identifiers_[idx],\n            (predictions[idx, 0], target_features[idx:(idx + 1)])\n        ),\n        range(len(indices))\n    ))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.802998Z","iopub.execute_input":"2021-07-31T17:25:01.80407Z","iopub.status.idle":"2021-07-31T17:25:01.821401Z","shell.execute_reply.started":"2021-07-31T17:25:01.804002Z","shell.execute_reply":"2021-07-31T17:25:01.82027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mixup(X: np.ndarray, y: np.ndarray, mixup_coeff: float,\n          n_samples: int) -> Tuple[np.ndarray, np.ndarray]:\n    assert (mixup_coeff > 0.0) and (mixup_coeff < 1.0)\n    assert len(X.shape) == 2\n    assert len(y.shape) == 1\n    assert X.shape[0] == y.shape[0]\n    X_new = np.empty((n_samples, X.shape[1]), dtype=np.float64)\n    y_new = np.empty((n_samples,), dtype=np.float64)\n    for sample_idx in range(n_samples):\n        idx1 = random.randint(0, X.shape[0] - 1)\n        idx2 = random.randint(0, X.shape[0] - 1)\n        X_new[sample_idx] = (1.0 - mixup_coeff) * X[idx1] + mixup_coeff * X[idx2]\n        y_new[sample_idx] = (1.0 - mixup_coeff) * y[idx1] + mixup_coeff * y[idx2]\n    return X_new, y_new","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.823677Z","iopub.execute_input":"2021-07-31T17:25:01.824449Z","iopub.status.idle":"2021-07-31T17:25:01.835701Z","shell.execute_reply.started":"2021-07-31T17:25:01.824384Z","shell.execute_reply":"2021-07-31T17:25:01.834649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_regressor(trainset: Tuple[np.ndarray, np.ndarray],\n                    validset: Tuple[np.ndarray, np.ndarray],\n                    batch_size: int, ensemble_idx: int) -> tf.keras.Model:\n    regressor = tf.keras.Sequential(\n        layers=[\n            tf.keras.layers.InputLayer(\n                input_shape=(trainset[0].shape[1],),\n                dtype=tf.float32,\n                name=f'input_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.AlphaDropout(\n                rate=0.1, seed=ensemble_idx * 10,\n                name=f'dropout1_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.Dense(\n                units=400, activation='selu',\n                kernel_initializer=tf.keras.initializers.LecunNormal(\n                    seed=(ensemble_idx + 1) * 10\n                ),\n                name=f'dense1_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.AlphaDropout(\n                rate=0.1, seed=ensemble_idx * 20,\n                name=f'dropout2_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.Dense(\n                units=400, activation='selu',\n                kernel_initializer=tf.keras.initializers.LecunNormal(\n                    seed=(ensemble_idx + 1) * 20\n                ),\n                name=f'dense2_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.AlphaDropout(\n                rate=0.1, seed=ensemble_idx * 30,\n                name=f'dropout3_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.Dense(\n                units=300, activation='selu',\n                kernel_initializer=tf.keras.initializers.LecunNormal(\n                    seed=(ensemble_idx + 1) * 30\n                ),\n                name=f'dense3_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.AlphaDropout(\n                rate=0.1, seed=ensemble_idx * 40,\n                name=f'dropout4_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.Dense(\n                units=300, activation='selu',\n                kernel_initializer=tf.keras.initializers.LecunNormal(\n                    seed=(ensemble_idx + 1) * 40\n                ),\n                name=f'dense4_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.AlphaDropout(\n                rate=0.1, seed=ensemble_idx * 50,\n                name=f'dropout5_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.Dense(\n                units=200, activation='selu',\n                kernel_initializer=tf.keras.initializers.LecunNormal(\n                    seed=(ensemble_idx + 1) * 50\n                ),\n                name=f'dense5_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.AlphaDropout(\n                rate=0.1, seed=ensemble_idx * 60,\n                name=f'dropout6_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.Dense(\n                units=100, activation='selu',\n                kernel_initializer=tf.keras.initializers.LecunNormal(\n                    seed=(ensemble_idx + 1) * 60\n                ),\n                name=f'dense6_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.AlphaDropout(\n                rate=0.1, seed=ensemble_idx * 70,\n                name=f'dropout7_nn{ensemble_idx}'\n            ),\n            tf.keras.layers.Dense(\n                units=1, activation=None,\n                kernel_initializer=tf.keras.initializers.LecunNormal(\n                    seed=(ensemble_idx + 1) * 70\n                ),\n                name=f'dense7_nn{ensemble_idx}'\n            )\n        ],\n        name=f'FinalRegressor{ensemble_idx}'\n    )\n    radam = tfa.optimizers.RectifiedAdam(learning_rate=1e-3)\n    ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n    regressor.compile(optimizer=ranger, loss=tf.keras.losses.MeanSquaredError(),\n                      metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    regressor.summary()\n    callbacks = [\n        tf.keras.callbacks.EarlyStopping(\n            monitor=\"val_root_mean_squared_error\",\n            patience=10,\n            verbose=True,\n            restore_best_weights=True\n        )\n    ]\n    steps_per_epoch = (3 * validset[0].shape[0]) // batch_size\n    tf_trainset = tf.data.Dataset.from_tensor_slices(\n        trainset\n    ).repeat().shuffle(trainset[0].shape[0]).batch(batch_size)\n    tf_validset = tf.data.Dataset.from_tensor_slices(\n        validset\n    ).batch(batch_size) \n    regressor.fit(tf_trainset, validation_data=tf_validset,\n                  callbacks=callbacks, epochs=1000,\n                  steps_per_epoch=steps_per_epoch)\n    return regressor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.858492Z","iopub.execute_input":"2021-07-31T17:25:01.85902Z","iopub.status.idle":"2021-07-31T17:25:01.866255Z","shell.execute_reply.started":"2021-07-31T17:25:01.858971Z","shell.execute_reply":"2021-07-31T17:25:01.864816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_TEXT_LEN = 256\nPRETRAINED_BERT = '/kaggle/input/tfdistilbertbaseuncased'\nMINIBATCH_SIZE = 32","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.868479Z","iopub.execute_input":"2021-07-31T17:25:01.869038Z","iopub.status.idle":"2021-07-31T17:25:01.875299Z","shell.execute_reply.started":"2021-07-31T17:25:01.86899Z","shell.execute_reply":"2021-07-31T17:25:01.873963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/commonlitreadabilityprize'\nMODEL_DIR = '/kaggle/working'\nprint(f'{DATA_DIR} {os.path.isdir(DATA_DIR)}')\nprint(f'{MODEL_DIR} {os.path.isdir(MODEL_DIR)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.878042Z","iopub.execute_input":"2021-07-31T17:25:01.878391Z","iopub.status.idle":"2021-07-31T17:25:01.888392Z","shell.execute_reply.started":"2021-07-31T17:25:01.878347Z","shell.execute_reply":"2021-07-31T17:25:01.886966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainset_name = os.path.join(DATA_DIR, 'train.csv')\nprint(f'{trainset_name} {os.path.isfile(trainset_name)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.890267Z","iopub.execute_input":"2021-07-31T17:25:01.890795Z","iopub.status.idle":"2021-07-31T17:25:01.901138Z","shell.execute_reply.started":"2021-07-31T17:25:01.890746Z","shell.execute_reply":"2021-07-31T17:25:01.899888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testset_name = os.path.join(DATA_DIR, 'test.csv')\nprint(f'{testset_name} {os.path.isfile(testset_name)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.902962Z","iopub.execute_input":"2021-07-31T17:25:01.90377Z","iopub.status.idle":"2021-07-31T17:25:01.911414Z","shell.execute_reply.started":"2021-07-31T17:25:01.903719Z","shell.execute_reply":"2021-07-31T17:25:01.909752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_name = os.path.join(MODEL_DIR, 'submission.csv')\nprint(f'{submission_name} {os.path.isfile(submission_name)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.913136Z","iopub.execute_input":"2021-07-31T17:25:01.913896Z","iopub.status.idle":"2021-07-31T17:25:01.920963Z","shell.execute_reply.started":"2021-07-31T17:25:01.913848Z","shell.execute_reply":"2021-07-31T17:25:01.919463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regression_model_name = os.path.join(MODEL_DIR, 'regression_nn.h5')\nensemble_name = os.path.join(MODEL_DIR, 'ensemble')\nscaler_name = os.path.join(MODEL_DIR, 'output_scaler.pkl')\nfigure_identifier = 1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.922881Z","iopub.execute_input":"2021-07-31T17:25:01.923839Z","iopub.status.idle":"2021-07-31T17:25:01.929864Z","shell.execute_reply.started":"2021-07-31T17:25:01.923789Z","shell.execute_reply":"2021-07-31T17:25:01.928372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_BERT)\nprint(f'Vocabulary size is {pretrained_tokenizer.vocab_size}.')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:01.931701Z","iopub.execute_input":"2021-07-31T17:25:01.932493Z","iopub.status.idle":"2021-07-31T17:25:02.163601Z","shell.execute_reply.started":"2021-07-31T17:25:01.932443Z","shell.execute_reply":"2021-07-31T17:25:02.162344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_for_training = load_data_for_training(trainset_name,\n                                           pretrained_tokenizer)\nassert len(data_for_training) > 100","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:25:02.165087Z","iopub.execute_input":"2021-07-31T17:25:02.165484Z","iopub.status.idle":"2021-07-31T17:30:18.750257Z","shell.execute_reply.started":"2021-07-31T17:25:02.16544Z","shell.execute_reply":"2021-07-31T17:30:18.749138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_IDs = sorted(list(data_for_training.keys()))\nselected_IDs_for_training = random.sample(\n    population=all_IDs,\n    k=3\n)\nprint_info_about_data(data_for_training, selected_IDs_for_training)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:30:18.75354Z","iopub.execute_input":"2021-07-31T17:30:18.753895Z","iopub.status.idle":"2021-07-31T17:30:18.769811Z","shell.execute_reply.started":"2021-07-31T17:30:18.753855Z","shell.execute_reply":"2021-07-31T17:30:18.768545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_for_training, tokens_for_training = tokenize_data(\n    data=data_for_training,\n    tokenizer=pretrained_tokenizer,\n    max_seq_len=MAX_TEXT_LEN\n)\nprint_info_about_tokenized_data(\n    data=labels_for_training,\n    matrix=tokens_for_training,\n    identifiers=selected_IDs_for_training\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:30:18.771786Z","iopub.execute_input":"2021-07-31T17:30:18.772224Z","iopub.status.idle":"2021-07-31T17:30:45.21243Z","shell.execute_reply.started":"2021-07-31T17:30:18.772189Z","shell.execute_reply":"2021-07-31T17:30:45.21148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_feature_scaler = train_feature_scaler(labels_for_training) \nlabel_scaler = train_output_scaler(labels_for_training)\nwith open(scaler_name, 'wb') as scaler_fp:\n    pickle.dump((text_feature_scaler, label_scaler), scaler_fp)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:30:45.213938Z","iopub.execute_input":"2021-07-31T17:30:45.214329Z","iopub.status.idle":"2021-07-31T17:30:45.458945Z","shell.execute_reply.started":"2021-07-31T17:30:45.214287Z","shell.execute_reply":"2021-07-31T17:30:45.457768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.shuffle(all_IDs)\nn_train_size = int(round(len(all_IDs) * 0.8))\nn_val_size = int(round(len(all_IDs) * 0.1))\nIDs_for_training = all_IDs[:n_train_size]\nIDs_for_validation = all_IDs[n_train_size:(n_train_size + n_val_size)]\nIDs_for_final_testing = all_IDs[(n_train_size + n_val_size):]","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:30:45.460756Z","iopub.execute_input":"2021-07-31T17:30:45.461231Z","iopub.status.idle":"2021-07-31T17:30:45.477047Z","shell.execute_reply.started":"2021-07-31T17:30:45.461183Z","shell.execute_reply":"2021-07-31T17:30:45.475839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"median_dist = find_median_distance_between_pairs(\n    data=labels_for_training,\n    identifiers=IDs_for_training\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:30:45.478595Z","iopub.execute_input":"2021-07-31T17:30:45.479185Z","iopub.status.idle":"2021-07-31T17:30:48.542137Z","shell.execute_reply.started":"2021-07-31T17:30:45.479139Z","shell.execute_reply":"2021-07-31T17:30:48.541096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen_for_validation = DatasetGen(\n    data=labels_for_training,\n    data_IDs=IDs_for_validation,\n    token_indices=tokens_for_training,\n    median_distance_between_pairs=median_dist,\n    pad_token_id=pretrained_tokenizer.pad_token_id,\n    batch_size=MINIBATCH_SIZE,\n    output_scaler=label_scaler,\n    feature_scaler=text_feature_scaler\n)\nn_batches_per_validset = len(datagen_for_validation)\nprint(f'Mini-batches per validation set is {n_batches_per_validset}.')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:30:48.543637Z","iopub.execute_input":"2021-07-31T17:30:48.544238Z","iopub.status.idle":"2021-07-31T17:30:48.62413Z","shell.execute_reply.started":"2021-07-31T17:30:48.544191Z","shell.execute_reply":"2021-07-31T17:30:48.62309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_, y_, _ = datagen_for_validation[0]\nshow_minibatch(X_, y_)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:30:48.625532Z","iopub.execute_input":"2021-07-31T17:30:48.626142Z","iopub.status.idle":"2021-07-31T17:30:48.652352Z","shell.execute_reply.started":"2021-07-31T17:30:48.626094Z","shell.execute_reply":"2021-07-31T17:30:48.651384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_batches_per_epoch = 10 * n_batches_per_validset\ndatagen_for_training = DatasetGen(\n    data=labels_for_training,\n    data_IDs=IDs_for_training,\n    token_indices=tokens_for_training,\n    median_distance_between_pairs=median_dist,\n    pad_token_id=pretrained_tokenizer.pad_token_id,\n    batch_size=MINIBATCH_SIZE,\n    batches_per_epoch=n_batches_per_epoch,\n    output_scaler=label_scaler, \n    feature_scaler=text_feature_scaler\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:30:48.65391Z","iopub.execute_input":"2021-07-31T17:30:48.654302Z","iopub.status.idle":"2021-07-31T17:31:03.768366Z","shell.execute_reply.started":"2021-07-31T17:30:48.654259Z","shell.execute_reply":"2021-07-31T17:31:03.767304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_, y_, _ = datagen_for_training[0] \nshow_minibatch(X_, y_)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:03.769974Z","iopub.execute_input":"2021-07-31T17:31:03.77039Z","iopub.status.idle":"2021-07-31T17:31:03.921375Z","shell.execute_reply.started":"2021-07-31T17:31:03.770344Z","shell.execute_reply":"2021-07-31T17:31:03.920325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_for_training, model_for_inference = build_neural_network(\n    bert_name=PRETRAINED_BERT,\n    max_seq_len=MAX_TEXT_LEN,\n    feature_vector_size=text_feature_scaler.named_steps['scaler'].scale_.shape[0],\n    batch_size=MINIBATCH_SIZE\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:03.922897Z","iopub.execute_input":"2021-07-31T17:31:03.923336Z","iopub.status.idle":"2021-07-31T17:31:23.837671Z","shell.execute_reply.started":"2021-07-31T17:31:03.923289Z","shell.execute_reply":"2021-07-31T17:31:23.836642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_for_training.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:23.839328Z","iopub.execute_input":"2021-07-31T17:31:23.83977Z","iopub.status.idle":"2021-07-31T17:31:23.870459Z","shell.execute_reply.started":"2021-07-31T17:31:23.839722Z","shell.execute_reply":"2021-07-31T17:31:23.869348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_for_inference.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:23.872003Z","iopub.execute_input":"2021-07-31T17:31:23.872425Z","iopub.status.idle":"2021-07-31T17:31:23.891704Z","shell.execute_reply.started":"2021-07-31T17:31:23.872379Z","shell.execute_reply":"2021-07-31T17:31:23.890432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_tsne(regressor=model_for_inference, batch_size=MINIBATCH_SIZE,\n          feature_scaler=text_feature_scaler,\n          data=labels_for_training, token_matrix=tokens_for_training,\n          identifiers=IDs_for_validation + IDs_for_final_testing,\n          pad_id=pretrained_tokenizer.pad_token_id,\n          title='before training', figure_id=figure_identifier)\nfigure_identifier += 1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:23.893226Z","iopub.execute_input":"2021-07-31T17:31:23.893721Z","iopub.status.idle":"2021-07-31T17:31:34.810282Z","shell.execute_reply.started":"2021-07-31T17:31:23.893663Z","shell.execute_reply":"2021-07-31T17:31:34.809251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_for_validation = do_predictions(\n    regressor=model_for_inference, feature_scaler=text_feature_scaler,\n    output_scaler=label_scaler, \n    batch_size=MINIBATCH_SIZE,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_validation\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:34.811512Z","iopub.execute_input":"2021-07-31T17:31:34.811872Z","iopub.status.idle":"2021-07-31T17:31:35.63996Z","shell.execute_reply.started":"2021-07-31T17:31:34.811835Z","shell.execute_reply":"2021-07-31T17:31:35.638396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error = 0.0\nfor cur_id in IDs_for_validation:\n    difference = predictions_for_validation[cur_id][0] - labels_for_training[cur_id][1]\n    error += (difference * difference)\nerror /= float(len(IDs_for_validation))\nerror = np.sqrt(error)\nprint(f'RMSE on validation set before training = {error}')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:35.641448Z","iopub.execute_input":"2021-07-31T17:31:35.641859Z","iopub.status.idle":"2021-07-31T17:31:35.655105Z","shell.execute_reply.started":"2021-07-31T17:31:35.641817Z","shell.execute_reply":"2021-07-31T17:31:35.653916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del predictions_for_validation, error","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:35.657097Z","iopub.execute_input":"2021-07-31T17:31:35.658395Z","iopub.status.idle":"2021-07-31T17:31:35.672788Z","shell.execute_reply.started":"2021-07-31T17:31:35.658345Z","shell.execute_reply":"2021-07-31T17:31:35.671723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_for_testing = do_predictions(\n    regressor=model_for_inference, feature_scaler=text_feature_scaler,\n    output_scaler=label_scaler, \n    batch_size=MINIBATCH_SIZE,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_final_testing\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:35.675809Z","iopub.execute_input":"2021-07-31T17:31:35.680244Z","iopub.status.idle":"2021-07-31T17:31:37.425549Z","shell.execute_reply.started":"2021-07-31T17:31:35.679907Z","shell.execute_reply":"2021-07-31T17:31:37.424469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error = 0.0\nfor cur_id in IDs_for_final_testing:\n    difference = predictions_for_testing[cur_id][0] - labels_for_training[cur_id][1]\n    error += (difference * difference)\nerror /= float(len(IDs_for_final_testing))\nerror = np.sqrt(error)\nprint(f'RMSE on test set before training = {error}')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:37.427146Z","iopub.execute_input":"2021-07-31T17:31:37.427554Z","iopub.status.idle":"2021-07-31T17:31:37.439939Z","shell.execute_reply.started":"2021-07-31T17:31:37.42751Z","shell.execute_reply":"2021-07-31T17:31:37.438697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del predictions_for_testing, error","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:37.441983Z","iopub.execute_input":"2021-07-31T17:31:37.442745Z","iopub.status.idle":"2021-07-31T17:31:37.449136Z","shell.execute_reply.started":"2021-07-31T17:31:37.442693Z","shell.execute_reply":"2021-07-31T17:31:37.447913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [\n    tfa.callbacks.TimeStopping(seconds=int(round(3600 * 1.7)), verbose=True),\n    tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        patience=7,\n        verbose=True,\n        restore_best_weights=True\n    )\n]","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:37.450911Z","iopub.execute_input":"2021-07-31T17:31:37.451595Z","iopub.status.idle":"2021-07-31T17:31:37.459677Z","shell.execute_reply.started":"2021-07-31T17:31:37.451515Z","shell.execute_reply":"2021-07-31T17:31:37.458619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model_for_training.fit(datagen_for_training,\n                                 validation_data=datagen_for_validation,\n                                 epochs=1000, callbacks=callbacks)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:31:37.462794Z","iopub.execute_input":"2021-07-31T17:31:37.463171Z","iopub.status.idle":"2021-07-31T17:47:25.886107Z","shell.execute_reply.started":"2021-07-31T17:31:37.463136Z","shell.execute_reply":"2021-07-31T17:47:25.884702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_for_inference.save_weights(regression_model_name)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:25.888074Z","iopub.execute_input":"2021-07-31T17:47:25.888533Z","iopub.status.idle":"2021-07-31T17:47:26.535554Z","shell.execute_reply.started":"2021-07-31T17:47:25.888486Z","shell.execute_reply":"2021-07-31T17:47:26.5346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_training_process(history, \"loss\", figure_identifier)\nfigure_identifier += 1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:26.537003Z","iopub.execute_input":"2021-07-31T17:47:26.537639Z","iopub.status.idle":"2021-07-31T17:47:26.717292Z","shell.execute_reply.started":"2021-07-31T17:47:26.537576Z","shell.execute_reply":"2021-07-31T17:47:26.716111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_tsne(regressor=model_for_inference, batch_size=MINIBATCH_SIZE,\n          feature_scaler=text_feature_scaler,\n          data=labels_for_training, token_matrix=tokens_for_training,\n          identifiers=IDs_for_validation + IDs_for_final_testing,\n          pad_id=pretrained_tokenizer.pad_token_id,\n          title='after training', figure_id=figure_identifier)\nfigure_identifier += 1","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:26.719168Z","iopub.execute_input":"2021-07-31T17:47:26.71964Z","iopub.status.idle":"2021-07-31T17:47:32.121795Z","shell.execute_reply.started":"2021-07-31T17:47:26.719592Z","shell.execute_reply":"2021-07-31T17:47:32.118428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_for_validation = do_predictions(\n    regressor=model_for_inference, feature_scaler=text_feature_scaler,\n    output_scaler=label_scaler, \n    batch_size=MINIBATCH_SIZE,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_validation\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:32.123799Z","iopub.execute_input":"2021-07-31T17:47:32.12426Z","iopub.status.idle":"2021-07-31T17:47:32.944368Z","shell.execute_reply.started":"2021-07-31T17:47:32.124213Z","shell.execute_reply":"2021-07-31T17:47:32.943478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error = 0.0\nfor cur_id in IDs_for_validation:\n    difference = predictions_for_validation[cur_id][0] - labels_for_training[cur_id][1]\n    error += (difference * difference)\nerror /= float(len(IDs_for_validation))\nerror = np.sqrt(error)\nprint(f'RMSE on validation set after training = {error}')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:32.945857Z","iopub.execute_input":"2021-07-31T17:47:32.946466Z","iopub.status.idle":"2021-07-31T17:47:32.954545Z","shell.execute_reply.started":"2021-07-31T17:47:32.94642Z","shell.execute_reply":"2021-07-31T17:47:32.953676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del predictions_for_validation, error","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:32.955888Z","iopub.execute_input":"2021-07-31T17:47:32.956476Z","iopub.status.idle":"2021-07-31T17:47:32.967614Z","shell.execute_reply.started":"2021-07-31T17:47:32.956435Z","shell.execute_reply":"2021-07-31T17:47:32.966576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_for_testing = do_predictions(\n    regressor=model_for_inference, feature_scaler=text_feature_scaler,\n    output_scaler=label_scaler, \n    batch_size=MINIBATCH_SIZE,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_final_testing\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:32.969031Z","iopub.execute_input":"2021-07-31T17:47:32.969619Z","iopub.status.idle":"2021-07-31T17:47:34.723191Z","shell.execute_reply.started":"2021-07-31T17:47:32.969576Z","shell.execute_reply":"2021-07-31T17:47:34.722106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error = 0.0\nfor cur_id in IDs_for_final_testing:\n    difference = predictions_for_testing[cur_id][0] - labels_for_training[cur_id][1]\n    error += (difference * difference)\nerror /= float(len(IDs_for_final_testing))\nerror = np.sqrt(error)\nprint(f'RMSE on test set after training = {error}')","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:34.724665Z","iopub.execute_input":"2021-07-31T17:47:34.725056Z","iopub.status.idle":"2021-07-31T17:47:34.733521Z","shell.execute_reply.started":"2021-07-31T17:47:34.725014Z","shell.execute_reply":"2021-07-31T17:47:34.731377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del predictions_for_testing, error","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:34.735473Z","iopub.execute_input":"2021-07-31T17:47:34.735927Z","iopub.status.idle":"2021-07-31T17:47:34.746796Z","shell.execute_reply.started":"2021-07-31T17:47:34.735883Z","shell.execute_reply":"2021-07-31T17:47:34.745505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y_train, _ = generate_new_trainset(\n    regressor=model_for_inference, feature_scaler=text_feature_scaler,\n    output_scaler=label_scaler, batch_size=MINIBATCH_SIZE,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_training\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:34.748715Z","iopub.execute_input":"2021-07-31T17:47:34.749216Z","iopub.status.idle":"2021-07-31T17:47:54.859765Z","shell.execute_reply.started":"2021-07-31T17:47:34.749167Z","shell.execute_reply":"2021-07-31T17:47:54.858734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val, y_val, _ = generate_new_trainset(\n    regressor=model_for_inference, feature_scaler=text_feature_scaler,\n    output_scaler=label_scaler, batch_size=MINIBATCH_SIZE,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_validation\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:54.861198Z","iopub.execute_input":"2021-07-31T17:47:54.861624Z","iopub.status.idle":"2021-07-31T17:47:55.477001Z","shell.execute_reply.started":"2021-07-31T17:47:54.86154Z","shell.execute_reply":"2021-07-31T17:47:55.476009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test, y_test, ave_test_pred = generate_new_trainset(\n    regressor=model_for_inference, feature_scaler=text_feature_scaler,\n    output_scaler=label_scaler, batch_size=MINIBATCH_SIZE,\n    data=labels_for_training, token_matrix=tokens_for_training,\n    pad_id=pretrained_tokenizer.pad_token_id,\n    identifiers=IDs_for_final_testing\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:55.478559Z","iopub.execute_input":"2021-07-31T17:47:55.479Z","iopub.status.idle":"2021-07-31T17:47:57.186864Z","shell.execute_reply.started":"2021-07-31T17:47:55.478958Z","shell.execute_reply":"2021-07-31T17:47:57.185846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del datagen_for_training, datagen_for_validation\ndel labels_for_training, tokens_for_training\ndel data_for_training\ndel IDs_for_training, IDs_for_validation, IDs_for_final_testing\ndel model_for_training\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:47:57.188295Z","iopub.execute_input":"2021-07-31T17:47:57.188724Z","iopub.status.idle":"2021-07-31T17:47:57.826932Z","shell.execute_reply.started":"2021-07-31T17:47:57.188683Z","shell.execute_reply":"2021-07-31T17:47:57.825952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble = []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.append(\n    build_regressor(\n        trainset=mixup(\n            X=X_train,\n            y=label_scaler.transform(\n                y_train.reshape((y_train.shape[0], 1))\n            ).reshape((y_train.shape[0],)),\n            mixup_coeff=0.1,\n            n_samples=40000\n        ),\n        validset=(\n            X_val,\n            label_scaler.transform(\n                y_val.reshape((y_val.shape[0], 1))\n            ).reshape((y_val.shape[0],))\n        ),\n        batch_size=MINIBATCH_SIZE,\n        ensemble_idx=1\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.append(\n    build_regressor(\n        trainset=mixup(\n            X=X_train,\n            y=label_scaler.transform(\n                y_train.reshape((y_train.shape[0], 1))\n            ).reshape((y_train.shape[0],)),\n            mixup_coeff=0.1,\n            n_samples=40000\n        ),\n        validset=(\n            X_val,\n            label_scaler.transform(\n                y_val.reshape((y_val.shape[0], 1))\n            ).reshape((y_val.shape[0],))\n        ),\n        batch_size=MINIBATCH_SIZE,\n        ensemble_idx=2\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.append(\n    build_regressor(\n        trainset=mixup(\n            X=X_train,\n            y=label_scaler.transform(\n                y_train.reshape((y_train.shape[0], 1))\n            ).reshape((y_train.shape[0],)),\n            mixup_coeff=0.1,\n            n_samples=40000\n        ),\n        validset=(\n            X_val,\n            label_scaler.transform(\n                y_val.reshape((y_val.shape[0], 1))\n            ).reshape((y_val.shape[0],))\n        ),\n        batch_size=MINIBATCH_SIZE,\n        ensemble_idx=3\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.append(\n    build_regressor(\n        trainset=mixup(\n            X=X_train,\n            y=label_scaler.transform(\n                y_train.reshape((y_train.shape[0], 1))\n            ).reshape((y_train.shape[0],)),\n            mixup_coeff=0.1,\n            n_samples=40000\n        ),\n        validset=(\n            X_val,\n            label_scaler.transform(\n                y_val.reshape((y_val.shape[0], 1))\n            ).reshape((y_val.shape[0],))\n        ),\n        batch_size=MINIBATCH_SIZE,\n        ensemble_idx=4\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.append(\n    build_regressor(\n        trainset=mixup(\n            X=X_train,\n            y=label_scaler.transform(\n                y_train.reshape((y_train.shape[0], 1))\n            ).reshape((y_train.shape[0],)),\n            mixup_coeff=0.1,\n            n_samples=40000\n        ),\n        validset=(\n            X_val,\n            label_scaler.transform(\n                y_val.reshape((y_val.shape[0], 1))\n            ).reshape((y_val.shape[0],))\n        ),\n        batch_size=MINIBATCH_SIZE,\n        ensemble_idx=5\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.append(\n    build_regressor(\n        trainset=mixup(\n            X=X_train,\n            y=label_scaler.transform(\n                y_train.reshape((y_train.shape[0], 1))\n            ).reshape((y_train.shape[0],)),\n            mixup_coeff=0.1,\n            n_samples=40000\n        ),\n        validset=(\n            X_val,\n            label_scaler.transform(\n                y_val.reshape((y_val.shape[0], 1))\n            ).reshape((y_val.shape[0],))\n        ),\n        batch_size=MINIBATCH_SIZE,\n        ensemble_idx=6\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.append(\n    build_regressor(\n        trainset=mixup(\n            X=X_train,\n            y=label_scaler.transform(\n                y_train.reshape((y_train.shape[0], 1))\n            ).reshape((y_train.shape[0],)),\n            mixup_coeff=0.1,\n            n_samples=40000\n        ),\n        validset=(\n            X_val,\n            label_scaler.transform(\n                y_val.reshape((y_val.shape[0], 1))\n            ).reshape((y_val.shape[0],))\n        ),\n        batch_size=MINIBATCH_SIZE,\n        ensemble_idx=7\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.append(\n    build_regressor(\n        trainset=mixup(\n            X=X_train,\n            y=label_scaler.transform(\n                y_train.reshape((y_train.shape[0], 1))\n            ).reshape((y_train.shape[0],)),\n            mixup_coeff=0.1,\n            n_samples=40000\n        ),\n        validset=(\n            X_val,\n            label_scaler.transform(\n                y_val.reshape((y_val.shape[0], 1))\n            ).reshape((y_val.shape[0],))\n        ),\n        batch_size=MINIBATCH_SIZE,\n        ensemble_idx=8\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.append(\n    build_regressor(\n        trainset=mixup(\n            X=X_train,\n            y=label_scaler.transform(\n                y_train.reshape((y_train.shape[0], 1))\n            ).reshape((y_train.shape[0],)),\n            mixup_coeff=0.1,\n            n_samples=40000\n        ),\n        validset=(\n            X_val,\n            label_scaler.transform(\n                y_val.reshape((y_val.shape[0], 1))\n            ).reshape((y_val.shape[0],))\n        ),\n        batch_size=MINIBATCH_SIZE,\n        ensemble_idx=9\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ensemble.append(\n    build_regressor(\n        trainset=mixup(\n            X=X_train,\n            y=label_scaler.transform(\n                y_train.reshape((y_train.shape[0], 1))\n            ).reshape((y_train.shape[0],)),\n            mixup_coeff=0.1,\n            n_samples=40000\n        ),\n        validset=(\n            X_val,\n            label_scaler.transform(\n                y_val.reshape((y_val.shape[0], 1))\n            ).reshape((y_val.shape[0],))\n        ),\n        batch_size=MINIBATCH_SIZE,\n        ensemble_idx=10\n    )\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx, cur in enumerate(ensemble):\n    cur.save_weights(ensemble_name + f'{idx + 1}.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with codecs.open(submission_name, mode='w', encoding='utf-8') as fp:\n    data_writer = csv.writer(fp, quotechar='\"', delimiter=',')\n    data_writer.writerow(['id', 'target'])\n    for data_part in load_data_for_testing(testset_name, pretrained_tokenizer,\n                                           MINIBATCH_SIZE * 8):\n        labels_for_submission, tokens_for_submission = tokenize_data(\n            data=data_part,\n            tokenizer=pretrained_tokenizer,\n            max_seq_len=MAX_TEXT_LEN\n        )\n        del data_part\n        predictions_for_submission = do_predictions(\n            regressor=model_for_inference,\n            feature_scaler=text_feature_scaler,\n            output_scaler=label_scaler, \n            batch_size=MINIBATCH_SIZE,\n            data=labels_for_submission, token_matrix=tokens_for_submission,\n            pad_id=pretrained_tokenizer.pad_token_id\n        )\n        features_for_regressor = []\n        final_predictions = []\n        identifiers = []\n        for cur_id in predictions_for_submission:\n            identifiers.append(cur_id)\n            predicted, nn_features = predictions_for_submission[cur_id]\n            final_predictions.append(predicted)\n            features_for_regressor.append(nn_features)\n        final_predictions = np.array(final_predictions, dtype=np.float32)\n        features_for_regressor = np.vstack(features_for_regressor)\n        for cur in ensemble:\n            final_predictions += label_scaler.inverse_transform(\n                cur.predict(\n                    features_for_regressor,\n                    batch_size=MINIBATCH_SIZE\n                )\n            ).reshape((final_predictions.shape[0],))\n        final_predictions /= float(len(ensemble) + 1.0)\n        assert final_predictions.shape[0] == len(identifiers)\n        for cur_id, predicted in zip(identifiers, final_predictions.tolist()):\n            data_writer.writerow([cur_id, f'{predicted}'])\n        del predictions_for_submission\n        del labels_for_submission, tokens_for_submission\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-31T18:00:19.911021Z","iopub.status.idle":"2021-07-31T18:00:19.911804Z"},"trusted":true},"execution_count":null,"outputs":[]}]}