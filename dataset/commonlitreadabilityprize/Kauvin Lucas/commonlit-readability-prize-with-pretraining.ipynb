{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%html\n\n<div style=\"background: linear-gradient(320deg, rgb(62, 0, 42), rgb(2, 2, 106), rgb(36, 0, 181)); \ncolor: #fff; border-radius: 10px;\">\n<div style=\"color: #fff; padding-top: 20px; padding-left: 10px; padding-bottom: 5px; font-size: 24px; line-height: 25px\">CommonLit Readability Prize (with pretraining)</div>\n<div style=\"padding: 10px; color: #fff;\">By <strong>Kauvin Lucas</strong> in Kaggle</div></div>","metadata":{"execution":{"iopub.status.busy":"2021-11-23T16:35:00.201112Z","iopub.execute_input":"2021-11-23T16:35:00.201431Z","iopub.status.idle":"2021-11-23T16:35:00.215607Z","shell.execute_reply.started":"2021-11-23T16:35:00.201357Z","shell.execute_reply":"2021-11-23T16:35:00.214569Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Background\nI've used this notebook to generate a submission for the CommonLit Readability Prize competition. I joined this competition without having any idea of attention-based models or Transformer. Any feedback or question will be appreciated.\n\n### The purpose of this notebook\nWith this notebook I've tried to build a simple Transformer model for the competition. It consists of pretraining, fine-tuning and inference steps, and it takes around 40min on GPU to run everything. Unlike many other submissions, I did not attempt to build (and ensemble) more than one model (although I should to get a better score).\n\n### The problem\nA readability score measures reading effort and speed of written text, and may take into account several different metrics, each one with pros and drawbacks. This is a problem when choosing between formulas, as many of these readability tests may lack construct and theoretical validity. In addition, commercially available formulas may lack transparency and can be cost-prohibitive.\n\nFor that reason, it's required to build a NLP model to rate the complexity of passages of text for grade 3-12 students that is trained on the pairwise comparisons given by the teachers.","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom transformers import (AutoTokenizer, AutoModel, AutoConfig, AutoModelForMaskedLM, \n                          Trainer, TrainingArguments, DataCollatorForLanguageModeling, \n                          RobertaForSequenceClassification, AdamW,\n                          get_linear_schedule_with_warmup)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import (dataset, TensorDataset, DataLoader, RandomSampler, \n                                      SequentialSampler, random_split)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom typing import Dict\nfrom transformers.tokenization_utils import PreTrainedTokenizer\nfrom tqdm.notebook import tqdm\nimport os\nimport random\nimport time\nimport datetime\n\n# Set verbosity to the error level\ntransformers.logging.set_verbosity_error()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:28:05.496587Z","iopub.execute_input":"2021-08-09T14:28:05.496917Z","iopub.status.idle":"2021-08-09T14:28:05.505734Z","shell.execute_reply.started":"2021-08-09T14:28:05.496888Z","shell.execute_reply":"2021-08-09T14:28:05.504607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1 - Load datasets and base models","metadata":{}},{"cell_type":"code","source":"class LineByLineTextDataset(dataset.Dataset):\n    def __init__(self, data, tokenizer:PreTrainedTokenizer, block_size: int):\n        data = data[\"excerpt\"]\n        lines = [line for line in data if (len(line) > 0 and not line.isspace())]\n        batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)\n        self.examples = batch_encoding[\"input_ids\"]\n        self.examples = [{\"input_ids\": torch.tensor(e, dtype=torch.long)} for e in self.examples]\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, i) -> Dict[str, torch.tensor]:\n        return self.examples[i]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:28:59.907299Z","iopub.execute_input":"2021-08-09T14:28:59.907708Z","iopub.status.idle":"2021-08-09T14:28:59.916575Z","shell.execute_reply.started":"2021-08-09T14:28:59.907679Z","shell.execute_reply":"2021-08-09T14:28:59.915503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROBERTA_MODEL = \"../input/roberta-base\"\nTRAINING_FILE = \"../input/commonlitreadabilityprize/train.csv\"\nTEST_FILE = \"../input/commonlitreadabilityprize/test.csv\"\nSAMPLE_FILE = \"../input/commonlitreadabilityprize/sample_submission.csv\"\nMODEL_PATH = 'Models/clrp_roberta-pretrained'\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(ROBERTA_MODEL, do_lower_case=True)\nMODEL = transformers.AutoModelForMaskedLM.from_pretrained(ROBERTA_MODEL)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:29:00.379367Z","iopub.execute_input":"2021-08-09T14:29:00.379735Z","iopub.status.idle":"2021-08-09T14:29:08.915208Z","shell.execute_reply.started":"2021-08-09T14:29:00.379706Z","shell.execute_reply":"2021-08-09T14:29:08.914279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(TRAINING_FILE)\ntest_data = pd.read_csv(TEST_FILE)\ntrain_dataset = LineByLineTextDataset(\n    train_data,\n    tokenizer=TOKENIZER,\n    block_size=256)\nvalid_dataset = LineByLineTextDataset(\n    train_data,\n    tokenizer=TOKENIZER,\n    block_size=256)\ntest_dataset = LineByLineTextDataset(\n    test_data,\n    tokenizer=TOKENIZER,\n    block_size=256)\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=TOKENIZER, mlm=True, mlm_probability=0.15)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:29:08.91673Z","iopub.execute_input":"2021-08-09T14:29:08.917087Z","iopub.status.idle":"2021-08-09T14:29:12.669507Z","shell.execute_reply.started":"2021-08-09T14:29:08.917051Z","shell.execute_reply":"2021-08-09T14:29:12.668519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1 - Pretaining","metadata":{}},{"cell_type":"markdown","source":"#### 1.1 - Pretraining arguments","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:18:06.528693Z","iopub.execute_input":"2021-08-09T14:18:06.529108Z","iopub.status.idle":"2021-08-09T14:18:06.534384Z","shell.execute_reply.started":"2021-08-09T14:18:06.529019Z","shell.execute_reply":"2021-08-09T14:18:06.533318Z"}}},{"cell_type":"code","source":"WARMUP_STEPS = 0\nLEARNING_RATE = 5e-5\nWEIGHT_DECAY = 0\nEVAL_STEPS = 200\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nEPOCHS = 5","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:29:12.67148Z","iopub.execute_input":"2021-08-09T14:29:12.671892Z","iopub.status.idle":"2021-08-09T14:29:12.677488Z","shell.execute_reply.started":"2021-08-09T14:29:12.67185Z","shell.execute_reply":"2021-08-09T14:29:12.676371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    overwrite_output_dir=True,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n    per_device_eval_batch_size=VALID_BATCH_SIZE,\n    evaluation_strategy= 'steps',\n    save_total_limit=2,\n    eval_steps=EVAL_STEPS,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end =True,\n    prediction_loss_only=True,\n    warmup_steps=WARMUP_STEPS,\n    weight_decay=WEIGHT_DECAY,\n    report_to = \"none\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:29:12.679402Z","iopub.execute_input":"2021-08-09T14:29:12.679931Z","iopub.status.idle":"2021-08-09T14:29:12.735612Z","shell.execute_reply.started":"2021-08-09T14:29:12.679887Z","shell.execute_reply":"2021-08-09T14:29:12.734521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.2 - Trainer configuration","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:22:15.716472Z","iopub.execute_input":"2021-08-09T14:22:15.71687Z","iopub.status.idle":"2021-08-09T14:22:15.720747Z","shell.execute_reply.started":"2021-08-09T14:22:15.716833Z","shell.execute_reply":"2021-08-09T14:22:15.719725Z"}}},{"cell_type":"code","source":"trainer = Trainer(\n    model=MODEL,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:29:13.132011Z","iopub.execute_input":"2021-08-09T14:29:13.132387Z","iopub.status.idle":"2021-08-09T14:29:21.09694Z","shell.execute_reply.started":"2021-08-09T14:29:13.13235Z","shell.execute_reply":"2021-08-09T14:29:21.096023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.3 - Train and save pretrained model","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T14:29:21.09867Z","iopub.execute_input":"2021-08-09T14:29:21.099058Z","iopub.status.idle":"2021-08-09T14:42:12.614143Z","shell.execute_reply.started":"2021-08-09T14:29:21.099007Z","shell.execute_reply":"2021-08-09T14:42:12.613103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(MODEL_PATH)\ndel trainer\ntorch.cuda.empty_cache()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2 - Fine-tuning","metadata":{}},{"cell_type":"markdown","source":"#### 2.1 - Define fine-tuning parameters","metadata":{}},{"cell_type":"code","source":"epochs = 6\nlr = 5e-5\nfolds = 5\nmax_length = 160\nbatch_size = 32\ntrain_size = 0.9","metadata":{"execution":{"iopub.status.busy":"2021-08-09T16:44:35.943128Z","iopub.execute_input":"2021-08-09T16:44:35.943553Z","iopub.status.idle":"2021-08-09T16:44:35.94878Z","shell.execute_reply.started":"2021-08-09T16:44:35.943519Z","shell.execute_reply":"2021-08-09T16:44:35.947509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2 - Define CLRP model","metadata":{"execution":{"iopub.status.busy":"2021-07-31T22:00:42.533399Z","iopub.execute_input":"2021-07-31T22:00:42.533697Z","iopub.status.idle":"2021-07-31T22:00:42.538868Z","shell.execute_reply.started":"2021-07-31T22:00:42.533668Z","shell.execute_reply":"2021-07-31T22:00:42.537583Z"}}},{"cell_type":"code","source":"class CLRP_Model(nn.Module):\n    def __init__(self,path):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(path)\n        self.config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7}) \n        self.roberta = RobertaForSequenceClassification.from_pretrained(path, config = self.config)\n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.config.hidden_size,1)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        x = self.roberta(input_ids=input_ids, \n                         attention_mask=attention_mask, \n                         token_type_ids = token_type_ids)\n        weights = self.attention(x.hidden_states[-1])\n        x = torch.sum(weights * x.hidden_states[-1], dim=1)\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:59:27.153145Z","iopub.execute_input":"2021-08-09T15:59:27.153555Z","iopub.status.idle":"2021-08-09T15:59:27.163609Z","shell.execute_reply.started":"2021-08-09T15:59:27.153519Z","shell.execute_reply":"2021-08-09T15:59:27.162306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.2 - Define Finetune class","metadata":{}},{"cell_type":"code","source":"class Finetune():\n    def __init__(self, sentences, labels, model, base_path, seed = 42, batch_size = 32, epochs = 4, train_size = 0.9):\n        self.model = model\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(base_path, do_lower_case=True)\n        self.device = torch.device(\"cuda\")\n        self.sentences = sentences\n        self.labels = labels\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.train_size = train_size\n        self.seed = seed\n    def load_data(self, max_len):\n        input_ids = []\n        attention_masks = []\n        token_type_ids = []\n        for sent in self.sentences:\n            encoded_dict = self.tokenizer.encode_plus(\n                                sent,\n                                add_special_tokens = True,\n                                max_length = max_len,\n                                padding = 'max_length',\n                                return_attention_mask = True,\n                                return_token_type_ids=True,\n                                return_tensors = 'pt',\n                                truncation=True\n                           )   \n            input_ids.append(encoded_dict['input_ids'])\n            attention_masks.append(encoded_dict['attention_mask'])\n            token_type_ids.append(encoded_dict['token_type_ids'])\n        input_ids = torch.cat(input_ids, dim=0)\n        attention_masks = torch.cat(attention_masks, dim=0)\n        token_type_ids = torch.cat(token_type_ids, dim=0)\n        labels = torch.tensor(self.labels, dtype=torch.float)\n        dataset = TensorDataset(input_ids, attention_masks, token_type_ids, labels)\n        train_size = int(self.train_size * len(dataset))\n        val_size = len(dataset) - train_size\n        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n        train_dataloader = DataLoader(\n                    train_dataset,\n                    sampler = RandomSampler(train_dataset),\n                    batch_size = self.batch_size\n                )\n        validation_dataloader = DataLoader(\n                    val_dataset,\n                    sampler = SequentialSampler(val_dataset),\n                    batch_size = self.batch_size\n                )\n        return train_dataloader, validation_dataloader\n    def optimizer(self, train_dataloader, lr = 2e-5, eps = 1e-8, wd = 1e-2):\n        total_steps = len(train_dataloader) * self.epochs\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n             'weight_decay_rate': 0.1},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n             'weight_decay_rate': 0.0}\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters,\n                          lr = lr,\n                          eps = eps,\n                          weight_decay = wd\n                        )\n        scheduler = get_linear_schedule_with_warmup(optimizer, \n                                                    num_warmup_steps = 0,\n                                                    num_training_steps = total_steps)\n        return optimizer, scheduler\n    def train_and_evaluate(self, train_dataloader, validation_dataloader, optimizer, scheduler, tqdm):\n        self.model.cuda()\n        random.seed(self.seed)\n        np.random.seed(self.seed)\n        torch.manual_seed(self.seed)\n        torch.cuda.manual_seed_all(self.seed)\n        training_stats = []\n        total_t0 = time.time()\n        for epoch_i in range(0, self.epochs):\n            t0 = time.time()\n            total_train_loss = 0\n            self.model.train()\n            for step, batch in enumerate(train_dataloader):\n                b_input_ids = batch[0].to(self.device)\n                b_input_mask = batch[1].to(self.device)\n                b_input_token_type_ids = batch[2].to(self.device)\n                b_labels = batch[3].to(self.device)\n                self.model.zero_grad()\n                optimizer.zero_grad()\n                result = self.model(b_input_ids, \n                                    attention_mask=b_input_mask, \n                                    token_type_ids = b_input_token_type_ids)\n                loss = torch.sqrt(nn.MSELoss()(result.flatten(),b_labels.view(-1)))\n                total_train_loss += loss.item()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                tqdm.update(1)\n            avg_train_loss = total_train_loss / len(train_dataloader)\n            training_time = str(datetime.timedelta(seconds=int(round((time.time() - t0)))))\n            t0 = time.time()\n            self.model.eval()\n            total_eval_score = 0\n            total_eval_loss = 0\n            nb_eval_steps = 0\n            for batch in validation_dataloader:\n                b_input_ids = batch[0].to(self.device)\n                b_input_mask = batch[1].to(self.device)\n                b_input_token_type_ids = batch[2].to(self.device)\n                b_labels = batch[3].to(self.device)\n                with torch.no_grad():        \n                    result = self.model(b_input_ids, \n                                        attention_mask=b_input_mask, \n                                        token_type_ids = b_input_token_type_ids)\n                loss = torch.sqrt(nn.MSELoss()(result.flatten(),b_labels.view(-1)))\n                total_eval_loss += loss.item()\n                logits = result.detach().cpu().numpy()\n            avg_val_loss = total_eval_loss / len(validation_dataloader)\n            validation_time = str(datetime.timedelta(seconds=int(round((time.time() - t0)))))\n            training_stats.append(\n                {\n                    'epoch': epoch_i + 1,\n                    'Training Loss': avg_train_loss,\n                    'Valid. Loss': avg_val_loss,\n                    'Training Time': training_time,\n                    'Validation Time': validation_time\n                }\n            )\n        return self.model, training_stats","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:59:28.057819Z","iopub.execute_input":"2021-08-09T15:59:28.060631Z","iopub.status.idle":"2021-08-09T15:59:28.108728Z","shell.execute_reply.started":"2021-08-09T15:59:28.060569Z","shell.execute_reply":"2021-08-09T15:59:28.10755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3 - Train and save folds","metadata":{"execution":{"iopub.status.busy":"2021-08-01T00:27:51.961283Z","iopub.execute_input":"2021-08-01T00:27:51.961586Z","iopub.status.idle":"2021-08-01T00:27:51.965396Z","shell.execute_reply.started":"2021-08-01T00:27:51.961513Z","shell.execute_reply":"2021-08-01T00:27:51.96461Z"}}},{"cell_type":"code","source":"sentences = train_data.excerpt.values\nlabels = train_data.target.values\nprogress_bar = tqdm(total = int(epochs * folds * np.ceil(len(sentences)/batch_size * train_size)))\nprogress_bar.set_description(\"Finetuning progress over {} folds\".format(folds))\nfor fold in range(folds):\n    fold = fold + 1\n    model = CLRP_Model(MODEL_PATH)\n    finetune = Finetune(sentences, \n                        labels, \n                        model,\n                        ROBERTA_MODEL, \n                        epochs = epochs)\n    train_dataloader, validation_dataloader = finetune.load_data(max_length)\n    optimizer, scheduler = finetune.optimizer(train_dataloader, lr = lr)\n    model, training_stats = finetune.train_and_evaluate(train_dataloader, \n                                                        validation_dataloader, \n                                                        optimizer, \n                                                        scheduler,\n                                                        progress_bar)\n    if fold == 1:\n        df = pd.DataFrame(data=training_stats)\n        df[\"Fold\"] = fold\n    else:\n        df1 = pd.DataFrame(data=training_stats)\n        df1[\"Fold\"] = fold\n        df = df1.append(df)\n    output_dir = './Models/'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    torch.save(model.state_dict(), f\"./Models/model{fold}.bin\")\n    df.groupby(df.index).mean()\n    del model\n\n# Delete finetune class\ndel finetune\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T15:59:50.237581Z","iopub.execute_input":"2021-08-09T15:59:50.237966Z","iopub.status.idle":"2021-08-09T16:16:08.652443Z","shell.execute_reply.started":"2021-08-09T15:59:50.237935Z","shell.execute_reply":"2021-08-09T16:16:08.651513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display loss history.\npd.set_option('precision', 2)\ndf_stats = df.set_index('Fold')\ndf_stats","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3 - Inference","metadata":{}},{"cell_type":"code","source":"def base_model(base_path, num_labels=1):\n    config = AutoConfig.from_pretrained(base_path)\n    config.update({'num_labels': num_labels})\n    model = CLRP_Model(base_path)\n    return model\nclass Inference():\n    def __init__(self, sentences, model_path, base_path, seed = 42, batch_size = 16):\n        self.model = model\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(base_path, do_lower_case=True)\n        self.device = torch.device(\"cuda\")\n        self.sentences = sentences\n        self.batch_size = batch_size\n    def load_data(self, max_len):\n        input_ids = []\n        attention_masks = []\n        token_type_ids = []\n        for sent in self.sentences:\n            encoded_dict = self.tokenizer.encode_plus(\n                                sent,\n                                add_special_tokens = True,\n                                max_length = max_len,\n                                padding = 'max_length',\n                                return_attention_mask = True, \n                                return_token_type_ids=True,\n                                return_tensors = 'pt',\n                                truncation=True\n                           )   \n            input_ids.append(encoded_dict['input_ids'])\n            attention_masks.append(encoded_dict['attention_mask'])\n            token_type_ids.append(encoded_dict['token_type_ids'])\n        input_ids = torch.cat(input_ids, dim=0)\n        attention_masks = torch.cat(attention_masks, dim=0)\n        token_type_ids = torch.cat(token_type_ids, dim=0)\n        test_dataset = TensorDataset(input_ids, attention_masks, token_type_ids)\n        test_dataloader = DataLoader(\n                    test_dataset,\n                    sampler = SequentialSampler(test_dataset),\n                    batch_size = self.batch_size,\n                    pin_memory=False, \n                    drop_last=False, \n                    num_workers=0\n                )\n        return test_dataloader\n    def predict(self, test_dataloader):\n        self.model.to(self.device)\n        self.model.eval()\n        result = np.zeros(len(test_dataloader.dataset))    \n        index = 0\n        with torch.no_grad():\n            for batch_num, batch_data in enumerate(test_dataloader):\n                input_ids, attention_mask, token_type_ids = batch_data[0], \\\n                    batch_data[1], batch_data[2]\n                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda()\n                pred = self.model(input_ids, attention_mask, token_type_ids)                        \n                result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n                index += pred.shape[0]\n        return result","metadata":{"execution":{"iopub.status.busy":"2021-08-02T01:56:46.774427Z","iopub.execute_input":"2021-08-02T01:56:46.774793Z","iopub.status.idle":"2021-08-02T01:56:46.792849Z","shell.execute_reply.started":"2021-08-02T01:56:46.77476Z","shell.execute_reply":"2021-08-02T01:56:46.791907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_predictions = np.zeros((folds, len(test_data)))\nsentences = test_data.excerpt.values\nsubmission_df = pd.read_csv(SAMPLE_FILE)\nfor fold in range(folds):\n    model_path = f\"./Models/model{fold+1}.bin\"\n    model = base_model(ROBERTA_MODEL)\n    model.load_state_dict(torch.load(model_path))\n    inference = Inference(sentences, model, ROBERTA_MODEL)\n    test_dataloader = inference.load_data(144)\n    result = inference.predict(test_dataloader)\n    all_predictions[fold] = result\npredictions = all_predictions.mean(axis=0)\nsubmission_df.target = predictions\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2021-08-02T01:56:53.4637Z","iopub.execute_input":"2021-08-02T01:56:53.464064Z","iopub.status.idle":"2021-08-02T01:56:55.832852Z","shell.execute_reply.started":"2021-08-02T01:56:53.464032Z","shell.execute_reply":"2021-08-02T01:56:55.831852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}