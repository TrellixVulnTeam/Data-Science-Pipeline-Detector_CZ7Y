{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preparation","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimport pandas as pd\nfrom pandas import DataFrame\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\n\nimport re\n\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport sys\nsys.path = [\n    '../input/readability-package',\n] + sys.path\nimport readability\nimport spacy\n\nfrom sklearn import model_selection\n\nimport transformers\nimport torch\nimport pytorch_lightning as pl\nfrom transformers import BertModel, BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import DataLoader, Dataset\n\nimport random\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\nfrom fastprogress.fastprogress import  progress_bar","metadata":{"_uuid":"0bba5c7e-9921-4079-b4cc-72d3f86f848b","_cell_guid":"b547acd6-e998-4cbb-9596-4a256748f902","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T12:56:33.155679Z","iopub.execute_input":"2021-07-26T12:56:33.156088Z","iopub.status.idle":"2021-07-26T12:56:39.884384Z","shell.execute_reply.started":"2021-07-26T12:56:33.156008Z","shell.execute_reply":"2021-07-26T12:56:39.883552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n\ntrain_df['excerpt'] = train_df['excerpt'].apply(lambda e: e.replace('\\n', ''))\ntest_df['excerpt'] = test_df['excerpt'].apply(lambda e: e.replace('\\n', ''))","metadata":{"_uuid":"6642bb9b-dbf2-4b43-9eb5-b66bf5c8a239","_cell_guid":"d24622d2-072f-43e1-b2c4-48f8e5f55a5f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T12:56:39.885714Z","iopub.execute_input":"2021-07-26T12:56:39.886085Z","iopub.status.idle":"2021-07-26T12:56:39.986699Z","shell.execute_reply.started":"2021-07-26T12:56:39.886048Z","shell.execute_reply":"2021-07-26T12:56:39.985816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning Texts Function","metadata":{"_uuid":"94f5cc66-486b-4dc5-ae19-2d9ad6f9f219","_cell_guid":"777bc61a-30ca-4cd6-9001-dc52e4fe0c58","trusted":true}},{"cell_type":"code","source":"def preprocess(df):\n    excerpt_processed=[]\n    for e in progress_bar(df['excerpt']):\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed","metadata":{"_uuid":"626ee9a3-0960-4452-a78a-be1e9d3caaf9","_cell_guid":"bf4889d6-efd6-40b4-90f1-77ab6dc3d50d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T12:56:39.988604Z","iopub.execute_input":"2021-07-26T12:56:39.989009Z","iopub.status.idle":"2021-07-26T12:56:39.995468Z","shell.execute_reply.started":"2021-07-26T12:56:39.988968Z","shell.execute_reply":"2021-07-26T12:56:39.9945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['excerpt_preprocessed'] = preprocess(train_df)\ntest_df[\"excerpt_preprocessed\"] = preprocess(test_df)","metadata":{"_uuid":"b4878df3-4f21-4e39-9f0c-69004f01f6ff","_cell_guid":"0e0e268b-84a1-446f-bdc3-560dc2c8ecf2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T12:56:39.997358Z","iopub.execute_input":"2021-07-26T12:56:39.997767Z","iopub.status.idle":"2021-07-26T12:57:49.467775Z","shell.execute_reply.started":"2021-07-26T12:56:39.997705Z","shell.execute_reply":"2021-07-26T12:57:49.466845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fetch some features","metadata":{"_kg_hide-input":true}},{"cell_type":"code","source":"#source: https://www.kaggle.com/ravishah1/readability-feature-engineering-non-nn-baseline/data\n\ndef readability_measurements(passage: str):\n    \"\"\"\n    This function uses the readability library for feature engineering.\n    It includes textual statistics, readability scales and metric, and some pos stats\n    \"\"\"\n    results = readability.getmeasures(passage, lang='en')\n    \n    chars_per_word = results['sentence info']['characters_per_word']\n    syll_per_word = results['sentence info']['syll_per_word']\n    words_per_sent = results['sentence info']['words_per_sentence']\n    \n    kincaid = results['readability grades']['Kincaid']\n    ari = results['readability grades']['ARI']\n    coleman_liau = results['readability grades']['Coleman-Liau']\n    flesch = results['readability grades']['FleschReadingEase']\n    gunning_fog = results['readability grades']['GunningFogIndex']\n    lix = results['readability grades']['LIX']\n    smog = results['readability grades']['SMOGIndex']\n    rix = results['readability grades']['RIX']\n    dale_chall = results['readability grades']['DaleChallIndex']\n    \n    tobeverb = results['word usage']['tobeverb']\n    auxverb = results['word usage']['auxverb']\n    conjunction = results['word usage']['conjunction']\n    pronoun = results['word usage']['pronoun']\n    preposition = results['word usage']['preposition']\n    nominalization = results['word usage']['nominalization']\n    \n    pronoun_b = results['sentence beginnings']['pronoun']\n    interrogative = results['sentence beginnings']['interrogative']\n    article = results['sentence beginnings']['article']\n    subordination = results['sentence beginnings']['subordination']\n    conjunction_b = results['sentence beginnings']['conjunction']\n    preposition_b = results['sentence beginnings']['preposition']\n\n    \n    return [chars_per_word, syll_per_word, words_per_sent,\n            kincaid, ari, coleman_liau, flesch, gunning_fog, lix, smog, rix, dale_chall,\n            tobeverb, auxverb, conjunction, pronoun, preposition, nominalization,\n            pronoun_b, interrogative, article, subordination, conjunction_b, preposition_b]","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-26T12:57:49.469158Z","iopub.execute_input":"2021-07-26T12:57:49.469516Z","iopub.status.idle":"2021-07-26T12:57:49.479905Z","shell.execute_reply.started":"2021-07-26T12:57:49.469464Z","shell.execute_reply":"2021-07-26T12:57:49.478974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def spacy_features(df: pd.DataFrame):\n    \"\"\"\n    This function generates features using spacy en_core_wb_lg\n    I learned about this from these resources:\n    https://www.kaggle.com/konradb/linear-baseline-with-cv\n    https://www.kaggle.com/anaverageengineer/comlrp-baseline-for-complete-beginners\n    \"\"\"\n    \n    nlp = spacy.load('en_core_web_lg')\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in df.excerpt])\n        \n    return vectors\n\ndef get_spacy_col_names():\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n        \n    return names","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-26T12:57:49.481277Z","iopub.execute_input":"2021-07-26T12:57:49.481669Z","iopub.status.idle":"2021-07-26T12:57:49.493085Z","shell.execute_reply.started":"2021-07-26T12:57:49.481631Z","shell.execute_reply":"2021-07-26T12:57:49.492148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pos_tag_features(passage: str):\n    \"\"\"\n    This function counts the number of times different parts of speech occur in an excerpt\n    \"\"\"\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    \n    tags = pos_tag(word_tokenize(passage))\n    tag_list= list()\n    \n    for tag in pos_tags:\n        tag_list.append(len([i[0] for i in tags if i[1] == tag]))\n    \n    return tag_list","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-26T12:57:49.494505Z","iopub.execute_input":"2021-07-26T12:57:49.494954Z","iopub.status.idle":"2021-07-26T12:57:49.503427Z","shell.execute_reply.started":"2021-07-26T12:57:49.494916Z","shell.execute_reply":"2021-07-26T12:57:49.502644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_other_features(passage: str):\n    \"\"\"\n    This function is where I test miscellaneous features\n    This is experimental\n    \"\"\"\n    # punctuation count\n    periods = passage.count(\".\")\n    commas = passage.count(\",\")\n    semis = passage.count(\";\")\n    exclaims = passage.count(\"!\")\n    questions = passage.count(\"?\")\n    \n    # Some other stats\n    num_char = len(passage)\n    num_words = len(passage.split(\" \"))\n    unique_words = len(set(passage.split(\" \") ))\n    word_diversity = unique_words/num_words\n    \n    word_len = [len(w) for w in passage.split(\" \")]\n    longest_word = np.max(word_len)\n    avg_len_word = np.mean(word_len)\n    \n    return [periods, commas, semis, exclaims, questions,\n            num_char, num_words, unique_words, word_diversity,\n            longest_word, avg_len_word]","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-26T12:57:49.506574Z","iopub.execute_input":"2021-07-26T12:57:49.506979Z","iopub.status.idle":"2021-07-26T12:57:49.514649Z","shell.execute_reply.started":"2021-07-26T12:57:49.506944Z","shell.execute_reply":"2021-07-26T12:57:49.513587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_folds(data: pd.DataFrame, num_splits: int):\n    \"\"\" \n    This function creates a kfold cross validation system based on this reference: \n    https://www.kaggle.com/abhishek/step-1-create-folds\n    \"\"\"\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-26T12:57:49.516845Z","iopub.execute_input":"2021-07-26T12:57:49.517225Z","iopub.status.idle":"2021-07-26T12:57:49.527313Z","shell.execute_reply.started":"2021-07-26T12:57:49.517188Z","shell.execute_reply":"2021-07-26T12:57:49.52659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRDataset:\n    \"\"\"\n    This is my CommonLit Readability Dataset.\n    By calling the get_df method on an object of this class,\n    you will have a fully feature engineered dataframe\n    \"\"\"\n    def __init__(self, df: pd.DataFrame, train: bool, n_folds=2):\n        self.df = df\n        self.excerpts = df[\"excerpt_preprocessed\"]\n        \n        self._extract_features()\n        \n        if train:\n            self.df = create_folds(self.df, n_folds)\n        \n    def _extract_features(self):\n        scores_df = pd.DataFrame(self.df[\"excerpt_preprocessed\"].apply(lambda p : readability_measurements(p)).tolist(), \n                                 columns=[\"chars_per_word\", \"syll_per_word\", \"words_per_sent\",\n                                          \"kincaid\", \"ari\", \"coleman_liau\", \"flesch\", \"gunning_fog\", \"lix\", \"smog\", \"rix\", \"dale_chall\",\n                                          \"tobeverb\", \"auxverb\", \"conjunction\", \"pronoun\", \"preposition\", \"nominalization\",\n                                          \"pronoun_b\", \"interrogative\", \"article\", \"subordination\", \"conjunction_b\", \"preposition_b\"])\n        self.df = pd.merge(self.df, scores_df, left_index=True, right_index=True)\n        \n        spacy_df = pd.DataFrame(spacy_features(self.df), columns=get_spacy_col_names())\n        self.df = pd.merge(self.df, spacy_df, left_index=True, right_index=True)\n        \n        pos_df = pd.DataFrame(self.df[\"excerpt_preprocessed\"].apply(lambda p : pos_tag_features(p)).tolist(),\n                              columns=[\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \n                                       \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\",\n                                       \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"])\n        self.df = pd.merge(self.df, pos_df, left_index=True, right_index=True)\n        \n        other_df = pd.DataFrame(self.df[\"excerpt_preprocessed\"].apply(lambda p : generate_other_features(p)).tolist(),\n                                columns=[\"periods\", \"commas\", \"semis\", \"exclaims\", \"questions\",\n                                         \"num_char\", \"num_words\", \"unique_words\", \"word_diversity\",\n                                         \"longest_word\", \"avg_len_word\"])\n        self.df = pd.merge(self.df, other_df, left_index=True, right_index=True)\n        \n    def get_df(self):\n        return self.df\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        pass","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-26T12:57:49.528892Z","iopub.execute_input":"2021-07-26T12:57:49.529333Z","iopub.status.idle":"2021-07-26T12:57:49.544611Z","shell.execute_reply.started":"2021-07-26T12:57:49.529295Z","shell.execute_reply":"2021-07-26T12:57:49.543802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CLRDataset(train_df, train=True)\ntrain_df = dataset.get_df()\n\ntrain_df","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-26T12:57:49.545991Z","iopub.execute_input":"2021-07-26T12:57:49.546359Z","iopub.status.idle":"2021-07-26T12:59:47.767248Z","shell.execute_reply.started":"2021-07-26T12:57:49.546321Z","shell.execute_reply":"2021-07-26T12:59:47.76623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = CLRDataset(test_df, train=False)\ntest_df = test_dataset.get_df()\n\ntest_df","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-07-26T12:59:47.768687Z","iopub.execute_input":"2021-07-26T12:59:47.769159Z","iopub.status.idle":"2021-07-26T12:59:54.071086Z","shell.execute_reply.started":"2021-07-26T12:59:47.769115Z","shell.execute_reply":"2021-07-26T12:59:54.069958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorize By BERT Function","metadata":{"_uuid":"65e2949c-190c-4485-a017-d6873cb4d702","_cell_guid":"0ee90223-7c1e-4353-af95-fd2af3ff709e","trusted":true}},{"cell_type":"markdown","source":"## Fine Tuning","metadata":{}},{"cell_type":"code","source":"MODEL_PATH = '../input/huggingface-bert/bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:59:54.072762Z","iopub.execute_input":"2021-07-26T12:59:54.073195Z","iopub.status.idle":"2021-07-26T12:59:54.137731Z","shell.execute_reply.started":"2021-07-26T12:59:54.073148Z","shell.execute_reply":"2021-07-26T12:59:54.136882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertForSequenceClassification_pl(pl.LightningModule):\n    def __init__(self, model_name, num_labels, lr):\n        super().__init__()\n        \n        self.save_hyperparameters()\n        \n        self.bert_sc = BertForSequenceClassification.from_pretrained(\n            model_name,\n            num_labels=num_labels\n        )\n        \n    def training_step(self, batch, batch_idx):\n        output = self.bert_sc(**batch)\n        loss = output.loss\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        output = self.bert_sc(**batch)\n        val_loss = output.loss\n        self.log('val_loss', val_loss)\n        \n    def test_step(self, batch, batch_idx):\n        labels = batch.pop('labels')\n        output = self.bert_sc(**batch)\n        labels_predicted = output.logits.argmax(-1)\n        num_correct = (labels_predicted == labels).sum().item()\n        accuracy = num_correct / labels.size(0)\n        self.log('accuracy', accuracy)\n        \n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:59:54.139069Z","iopub.execute_input":"2021-07-26T12:59:54.139395Z","iopub.status.idle":"2021-07-26T12:59:54.148472Z","shell.execute_reply.started":"2021-07-26T12:59:54.13936Z","shell.execute_reply":"2021-07-26T12:59:54.147274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = pl.callbacks.ModelCheckpoint(\n    monitor='val_loss',\n    mode='min',\n    save_top_k=1,\n    save_weights_only=True,\n    dirpath='model/'\n)\n\ntrainer = pl.Trainer(\n    gpus=1,\n    max_epochs=10,\n    callbacks=[checkpoint]\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:59:54.149928Z","iopub.execute_input":"2021-07-26T12:59:54.150451Z","iopub.status.idle":"2021-07-26T12:59:54.231749Z","shell.execute_reply.started":"2021-07-26T12:59:54.150374Z","shell.execute_reply":"2021-07-26T12:59:54.230827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createBertFineDataSet(excerpts, targets):\n    data = []    \n    for excerpt, target in zip(excerpts, targets):\n        encoding = tokenizer.encode_plus(\n            excerpt,\n            max_length = 314,\n            padding='max_length',\n            truncation=True\n        )\n\n        encoding['labels'] = target\n        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n\n        data.append(encoding)\n\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:59:54.234915Z","iopub.execute_input":"2021-07-26T12:59:54.235201Z","iopub.status.idle":"2021-07-26T12:59:54.241066Z","shell.execute_reply.started":"2021-07-26T12:59:54.235174Z","shell.execute_reply":"2021-07-26T12:59:54.240166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# update kfold values for fine tune\nkfolds = []\n\nfor i in progress_bar(train_df.index):\n    kfolds.append(i % 5)\n    \ntrain_df['kfold'] = kfolds","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:59:54.242877Z","iopub.execute_input":"2021-07-26T12:59:54.243558Z","iopub.status.idle":"2021-07-26T12:59:54.272714Z","shell.execute_reply.started":"2021-07-26T12:59:54.243518Z","shell.execute_reply":"2021-07-26T12:59:54.271872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertForSequenceClassification_pl(\n    MODEL_PATH,\n    num_labels=1,\n    lr=1e-5\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T12:59:54.273848Z","iopub.execute_input":"2021-07-26T12:59:54.274366Z","iopub.status.idle":"2021-07-26T13:00:03.438926Z","shell.execute_reply.started":"2021-07-26T12:59:54.274331Z","shell.execute_reply":"2021-07-26T13:00:03.438146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in progress_bar(train_df['kfold'].unique()):\n    train_df_for_fine_tune = train_df[train_df['kfold'] != i]\n    test_df_for_fine_tune = train_df[train_df['kfold'] == i]\n    \n    dataset_train = createBertFineDataSet(\n        train_df_for_fine_tune['excerpt'],\n        train_df_for_fine_tune['target']\n    )\n    \n    dataset_val = createBertFineDataSet(\n        test_df_for_fine_tune['excerpt'],\n        test_df_for_fine_tune['target']\n    )\n\n    train_dataloader = DataLoader(\n        dataset_train,\n        batch_size=32,\n        shuffle=True\n    )\n    val_dataloader = DataLoader(\n        dataset_val, \n        batch_size=256\n    )\n\n    trainer.fit(model, train_dataloader, val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:00:03.440593Z","iopub.execute_input":"2021-07-26T13:00:03.441105Z","iopub.status.idle":"2021-07-26T13:19:27.736449Z","shell.execute_reply.started":"2021-07-26T13:00:03.441064Z","shell.execute_reply":"2021-07-26T13:19:27.735626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_path = checkpoint.best_model_path\n\nmodel = BertForSequenceClassification_pl.load_from_checkpoint(\n    best_model_path\n)\n\nFINE_TUNED_MODEL_PATH = '/kaggle/working/model_transformers'\n\nmodel.bert_sc.save_pretrained(FINE_TUNED_MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:19:27.737807Z","iopub.execute_input":"2021-07-26T13:19:27.738139Z","iopub.status.idle":"2021-07-26T13:19:32.006066Z","shell.execute_reply.started":"2021-07-26T13:19:27.738109Z","shell.execute_reply":"2021-07-26T13:19:32.005092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bert interface","metadata":{}},{"cell_type":"code","source":"class BertDataset(nn.Module):\n    def __init__(self, df, tokenizer, max_len=128):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer.encode_plus(\n            self.excerpt[idx],\n            return_tensors='pt',\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True\n        )\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \n\ndef get_embeddings(df, path, plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    MODEL_PATH = path\n    model = BertModel.from_pretrained(MODEL_PATH, num_labels=1)\n    model.to(device)\n    model.eval()\n    \n    ds = BertDataset(df, tokenizer, config['max_len'])\n    dl = DataLoader(\n        ds,\n        batch_size=config[\"batch_size\"],\n        shuffle=False,\n        num_workers = 4,\n        pin_memory=True,\n        drop_last=False\n    )\n\n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in progress_bar(list(enumerate(dl))):\n            inputs = {key:val.reshape(val.shape[0], -1).to(device) for key, val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs[0][:, 0].detach().cpu().numpy()\n            embeddings.extend(outputs)\n            \n    return np.array(embeddings)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:19:32.007588Z","iopub.execute_input":"2021-07-26T13:19:32.007989Z","iopub.status.idle":"2021-07-26T13:19:32.019602Z","shell.execute_reply.started":"2021-07-26T13:19:32.007949Z","shell.execute_reply":"2021-07-26T13:19:32.01849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:19:32.021118Z","iopub.execute_input":"2021-07-26T13:19:32.021561Z","iopub.status.idle":"2021-07-26T13:19:32.034964Z","shell.execute_reply.started":"2021-07-26T13:19:32.021521Z","shell.execute_reply":"2021-07-26T13:19:32.034051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'batch_size': 128,\n    'max_len': 256,\n    'seed': 42,\n}\nseed_everything(seed=config['seed'])\n\ntrain_embeddings =  get_embeddings(train_df, FINE_TUNED_MODEL_PATH)\ntest_embeddings = get_embeddings(test_df, FINE_TUNED_MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:19:32.03898Z","iopub.execute_input":"2021-07-26T13:19:32.039262Z","iopub.status.idle":"2021-07-26T13:20:12.991886Z","shell.execute_reply.started":"2021-07-26T13:19:32.039233Z","shell.execute_reply":"2021-07-26T13:20:12.990992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare train and test data","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_rows', 500)\n\ntrain_df.filter(regex='^(?!.*spacy_).*$').corr().query('target < -0.5 | 0.5 < target')['target']","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:20:12.997028Z","iopub.execute_input":"2021-07-26T13:20:12.999034Z","iopub.status.idle":"2021-07-26T13:20:13.100419Z","shell.execute_reply.started":"2021-07-26T13:20:12.99899Z","shell.execute_reply":"2021-07-26T13:20:13.099634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [\n    'chars_per_word', \n    'syll_per_word',\n    'coleman_liau',\n    'smog', \n    'rix',\n    'dale_chall',\n    'avg_len_word'\n]","metadata":{"execution":{"iopub.status.busy":"2021-07-26T13:20:13.104745Z","iopub.execute_input":"2021-07-26T13:20:13.107215Z","iopub.status.idle":"2021-07-26T13:20:13.113116Z","shell.execute_reply.started":"2021-07-26T13:20:13.107176Z","shell.execute_reply":"2021-07-26T13:20:13.112008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.DataFrame(train_embeddings)\nX_train = pd.concat([X_train, train_df[columns]], axis=1)\n\nX_test = pd.DataFrame(test_embeddings)\nX_test = pd.concat([X_test, test_df[columns]], axis=1)","metadata":{"_uuid":"80b92578-7842-4fa3-b5f6-72a3f7cf5800","_cell_guid":"b4e32d23-e46a-4025-9494-9f28f24aefcc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T13:20:13.11852Z","iopub.execute_input":"2021-07-26T13:20:13.121819Z","iopub.status.idle":"2021-07-26T13:20:13.159125Z","shell.execute_reply.started":"2021-07-26T13:20:13.12177Z","shell.execute_reply":"2021-07-26T13:20:13.158241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = train_df[['target']]","metadata":{"_uuid":"3b997aa7-4278-4135-b7b4-2740d88c85e0","_cell_guid":"cb4805aa-4d11-48b1-bc61-8ce69ff831c6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T13:20:13.163076Z","iopub.execute_input":"2021-07-26T13:20:13.165139Z","iopub.status.idle":"2021-07-26T13:20:13.172242Z","shell.execute_reply.started":"2021-07-26T13:20:13.165102Z","shell.execute_reply":"2021-07-26T13:20:13.171333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=71)\n\ncv = list(kf.split(X_train, y_train))","metadata":{"_uuid":"ccacfdb6-0a4c-4a00-9adb-c250ee2700d8","_cell_guid":"55b2b7c9-e6c5-4ce9-a44b-c08bdf0e3d7b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T13:20:13.177348Z","iopub.execute_input":"2021-07-26T13:20:13.180363Z","iopub.status.idle":"2021-07-26T13:20:13.19086Z","shell.execute_reply.started":"2021-07-26T13:20:13.180316Z","shell.execute_reply":"2021-07-26T13:20:13.189092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Light GBM","metadata":{"_uuid":"6b9d9e08-7e85-46a6-a05a-8cefae7a2176","_cell_guid":"141efa19-cc0e-44b8-8ddc-dc9e9d438db6","trusted":true}},{"cell_type":"code","source":"params = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'verbose': -1,\n    'learning_rate': 0.5,\n    'max_depth': 3,\n    'feature_pre_filter': False,\n    'lambda_l1': 2.215942517163985,\n    'lambda_l2': 0.0015606472088872934,\n    'num_leaves': 2,\n    'feature_fraction': 0.8999999999999999,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'min_child_samples': 20,\n}\n\npred = np.zeros(X_test.shape[0])\nrmses = []\n\nfor tr_idx, val_idx in progress_bar(cv):\n    x_tr, x_va = X_train.iloc[tr_idx], X_train.iloc[val_idx]\n    y_tr, y_va = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n\n    train_set = lgb.Dataset(x_tr, y_tr)\n    val_set = lgb.Dataset(x_va, y_va, reference=train_set)\n\n    model = lgb.train(\n        params,\n        train_set, \n        num_boost_round=10000,\n        early_stopping_rounds=100,\n        valid_sets=[train_set, val_set], \n        verbose_eval=-1\n    )\n\n    y_pred = model.predict(x_va)\n    rmse = np.sqrt(mse(y_va, y_pred))\n    rmses.append(rmse)\n    \n    tmp_pred = model.predict(X_test)\n    pred += tmp_pred / 5\n    \nprint(\"\\n\", \"Mean Fold RMSE:\", np.mean(rmses))","metadata":{"_uuid":"db191234-a015-4f4e-a23d-55fd2a7e4170","_cell_guid":"dab4ecc7-457b-4122-bef3-f524e3838258","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T13:20:13.195963Z","iopub.execute_input":"2021-07-26T13:20:13.196289Z","iopub.status.idle":"2021-07-26T13:20:25.86387Z","shell.execute_reply.started":"2021-07-26T13:20:13.196262Z","shell.execute_reply":"2021-07-26T13:20:25.862913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = pred\npredictions.to_csv(\"submission.csv\", index=False)\n\npredictions","metadata":{"_uuid":"f002712c-1d9f-4828-9292-811e60004207","_cell_guid":"04fac150-c0fd-4bbc-8c5e-dd97dfe7b7e5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-07-26T13:20:25.865242Z","iopub.execute_input":"2021-07-26T13:20:25.8656Z","iopub.status.idle":"2021-07-26T13:20:25.892952Z","shell.execute_reply.started":"2021-07-26T13:20:25.865561Z","shell.execute_reply":"2021-07-26T13:20:25.892061Z"},"trusted":true},"execution_count":null,"outputs":[]}]}