{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Roberta large model which was trained on some pseudo label dataset(created by 0.448 lb blending models)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-02T11:20:32.154779Z","iopub.execute_input":"2021-07-02T11:20:32.155148Z","iopub.status.idle":"2021-07-02T11:20:32.166448Z","shell.execute_reply.started":"2021-07-02T11:20:32.155072Z","shell.execute_reply":"2021-07-02T11:20:32.165518Z"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom sklearn.model_selection import KFold\n\nimport gc\ngc.enable()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 32\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"../input/robertalarge/\"\nTOKENIZER_PATH = \"../input/robertalarge/\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n\n# Remove incomplete entries if any.\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\nsubmission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")\n\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:20:32.167996Z","iopub.execute_input":"2021-07-02T11:20:32.168343Z","iopub.status.idle":"2021-07-02T11:20:32.255064Z","shell.execute_reply.started":"2021-07-02T11:20:32.16831Z","shell.execute_reply":"2021-07-02T11:20:32.254291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df\n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        # self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n\n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)\n\n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding='max_length',\n            max_length=MAX_LEN,\n            truncation=True,\n            return_attention_mask=True\n        )\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n\n        if self.inference_only:\n            return (input_ids, attention_mask)\n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)\n\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\": True,\n                       \"hidden_dropout_prob\": 0.0,\n                       'num_hidden_layers': 25,\n                       \"layer_norm_eps\": 1e-09})\n\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)\n\n        self.layer_norm = nn.LayerNorm(1024)\n\n\n        self.regressor = nn.Sequential(\n            nn.Linear(1024, 1)\n        )\n        # self._init_weights(self.regressor)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask\n                                      ,output_hidden_states=True\n                                      )\n\n        last_hidden_state = roberta_output.hidden_states[-1]\n\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        norm_mean_embeddings = self.layer_norm(mean_embeddings)\n        logits = self.regressor(norm_mean_embeddings)\n\n        logits = logits.squeeze(-1).squeeze(-1)\n\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:20:32.256747Z","iopub.execute_input":"2021-07-02T11:20:32.257076Z","iopub.status.idle":"2021-07-02T11:20:39.497043Z","shell.execute_reply.started":"2021-07-02T11:20:32.257043Z","shell.execute_reply":"2021-07-02T11:20:39.496172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:20:39.498698Z","iopub.execute_input":"2021-07-02T11:20:39.499032Z","iopub.status.idle":"2021-07-02T11:20:39.535163Z","shell.execute_reply.started":"2021-07-02T11:20:39.498999Z","shell.execute_reply":"2021-07-02T11:20:39.534071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = LitDataset(test_df, inference_only=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_predictions = np.zeros((NUM_FOLDS, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\nmodel_dir='../input/test-2021-final-1/roberta_large_pseudo/'\nfor index in range(NUM_FOLDS):            \n    model_path = f\"{model_dir}model_{index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path))    \n    model.to(DEVICE)\n    \n    all_predictions[index] = predict(model, test_loader)\n    \n    del model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:20:39.537303Z","iopub.execute_input":"2021-07-02T11:20:39.537686Z","iopub.status.idle":"2021-07-02T11:20:39.549948Z","shell.execute_reply.started":"2021-07-02T11:20:39.537648Z","shell.execute_reply":"2021-07-02T11:20:39.549142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = all_predictions.mean(axis=0)\nsubmission_df.target = predictions\nprint(submission_df)\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T11:20:39.551731Z","iopub.execute_input":"2021-07-02T11:20:39.552176Z","iopub.status.idle":"2021-07-02T11:20:39.562575Z","shell.execute_reply.started":"2021-07-02T11:20:39.552099Z","shell.execute_reply":"2021-07-02T11:20:39.561444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}