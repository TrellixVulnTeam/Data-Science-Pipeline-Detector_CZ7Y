{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Simple Baseline","metadata":{}},{"cell_type":"code","source":"import os\nimport pathlib\n\nif os.path.exists(\"/kaggle\"):\n    input_dir = pathlib.Path(\"/kaggle/input/commonlitreadabilityprize\")\n    \n    !pip install ../input/textstat/Pyphen-0.10.0-py3-none-any.whl\n    !pip install ../input/textstat/textstat-0.7.0-py3-none-any.whl\n    !pip install ../input/pandarallel151whl/pandarallel-1.5.1-py3-none-any.whl\nelse:\n    input_dir = pathlib.Path(\"../data/raw\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport time\nimport pathlib\nimport textstat\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nfrom pandarallel import pandarallel\nfrom pandas_profiling import ProfileReport\n\nfrom typing import AnyStr\n\nimport nltk\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\n\ntqdm.pandas()\n\npandarallel.initialize(progress_bar=True)\n\nplt.style.use('seaborn-darkgrid')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(input_dir / 'train.csv')\ntest = pd.read_csv(input_dir / 'test.csv')\nsmpl_sub = pd.read_csv(input_dir / 'sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_excerpt(text: AnyStr):\n    text = re.sub(\"[^a-zA-Z]\", \" \", text).lower()\n    text = nltk.word_tokenize(text)  # NOTE: 英文を単語分割する\n    text = [word for word in text if not word in set(stopwords.words(\"english\"))]\n    \n    lemma = nltk.WordNetLemmatizer()  # NOTE: 複数形の単語を単数形に変換する\n    text =  \" \".join([lemma.lemmatize(word) for word in text])\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntrain['excerpt_preprocessed'] = train['excerpt'].parallel_apply(preprocess_excerpt)\ntest['excerpt_preprocessed'] = test['excerpt'].parallel_apply(preprocess_excerpt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ref: https://nigimitama.hatenablog.jp/entry/2020/11/09/080000\n\nclass BaseTransformer(BaseEstimator, TransformerMixin):\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return self\n\nclass TextstatProcessing(BaseTransformer):\n    \n    def transform(self, X):\n        X['excerpt_len'] = X['excerpt_preprocessed'].str.len()\n        X['avg_word_len'] = X['excerpt_preprocessed'].apply(lambda x: [len(s) for s in x.split()]).map(np.mean)\n        X['char_count'] = X['excerpt'].map(textstat.char_count)\n        X['word_count'] = X['excerpt_preprocessed'].map(textstat.lexicon_count)\n        X['sentence_count'] = X['excerpt'].map(textstat.sentence_count)\n        X['syllable_count'] = X['excerpt'].apply(textstat.syllable_count)\n        X['smog_index'] = X['excerpt'].apply(textstat.smog_index)\n        X['automated_readability_index'] = X['excerpt'].apply(textstat.automated_readability_index)\n        X['coleman_liau_index'] = X['excerpt'].apply(textstat.coleman_liau_index)\n        X['linsear_write_formula'] = X['excerpt'].apply(textstat.linsear_write_formula)\n        return X\n    \n\nclass TfidfTransformer(BaseTransformer):\n    \n    def transform(self, X):\n        tfidf_vec = TfidfVectorizer(binary=True, ngram_range=(1,1))\n        vector = tfidf_vec.fit_transform(X['excerpt'])\n        vector = pd.DataFrame(vector.toarray(), columns=tfidf_vec.get_feature_names())\n        pca = PCA(n_components=5)\n        pca_vector = pd.DataFrame(pca.fit_transform(vector))\n        X = pd.concat([X, pca_vector], axis=1)\n        return X\n    \n\nclass DropFeature(BaseTransformer):\n    \n    def transform(self, X):\n        X.drop(['excerpt', 'excerpt_preprocessed'], axis=1, inplace=True)\n        return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsrc_X = train[['excerpt', 'excerpt_preprocessed']].copy()\n\npipe = Pipeline(steps=[\n    ('textstat_processing', TextstatProcessing()),\n    ('tfidf', TfidfTransformer()),\n    ('drop_feature', DropFeature()),\n])\n\npipe.fit(src_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dst_X = pipe.transform(src_X)\ndst_X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Model","metadata":{}},{"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.model_selection import cross_validate, cross_val_score\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_cv(n_splits: int, target: pd.Series, shuffle: bool = True):\n    target = target.to_frame()\n    num_bins = int(np.floor(1 + np.log2(len(target))))\n    target.loc[:, \"bins\"] = pd.cut(\n        target[\"target\"], bins=num_bins, labels=False\n    )\n\n    kf = model_selection.StratifiedKFold(n_splits=n_splits)\n    for train_idx, test_idx in kf.split(X=target, y=target['bins']):\n        yield train_idx, test_idx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pipe.transform(src_X)\ny = train['target']\ncv = custom_cv(5, y)\n\nmodel = make_pipeline(\n    StandardScaler(),\n    LinearRegression(normalize=True, n_jobs=-1),\n)\n\nresult = cross_validate(\n    model, \n    X, \n    y, \n    cv=5, \n    scoring={'rmse': make_scorer(rmse)}, \n    return_train_score=True,\n    return_estimator=True,\n    n_jobs=-1,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"fit_time   : {result['fit_time'].mean():.5f}\")\nprint(f\"score_time : {result['score_time'].mean():.5f}\")\nprint(f\"train_rmse : {result['train_rmse'].mean():.5f}\") \nprint(f\"test_rmse  : {result['test_rmse'].mean():.5f}\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"X_test = test[['excerpt', 'excerpt_preprocessed']].copy()\nX_test = pipe.transform(X_test)\npred = np.zeros(X_test.shape[0])\n\nfor estimator in result['estimator']:\n    pred += estimator.predict(X_test) / len(result['estimator'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.title('Target Distribution')\nplt.hist(pred, bins=50)\nplt.axvline(pred.mean(), color='tab:orange', linewidth=2, linestyle='--', label='mean')\nplt.xlabel('target')\nplt.ylabel('count')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smpl_sub['target'] = pred\nsmpl_sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}