{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Experiments\n\n<!-- |  |  |  | -->\n    \n| Experiment Name | CV | LB |\n| :--- | ---: | ---: |\n| Baselien |  |  |\n","metadata":{}},{"cell_type":"markdown","source":"## Import Library","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport pathlib\nfrom typing import Optional\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers\n\nfrom sklearn.metrics import mean_squared_error\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-29T01:52:23.466535Z","iopub.execute_input":"2021-07-29T01:52:23.466885Z","iopub.status.idle":"2021-07-29T01:52:30.079598Z","shell.execute_reply.started":"2021-07-29T01:52:23.46676Z","shell.execute_reply":"2021-07-29T01:52:30.078808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer,\n)\n\nMAX_LEN = 256  # 248\n\nclass LitDataset(Dataset):\n    def __init__(self, df, model_name_or_path=\"roberta-base\", inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n\n        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:52:30.081121Z","iopub.execute_input":"2021-07-29T01:52:30.081472Z","iopub.status.idle":"2021-07-29T01:52:30.094636Z","shell.execute_reply.started":"2021-07-29T01:52:30.081433Z","shell.execute_reply":"2021-07-29T01:52:30.093697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataloader(model_name):\n    test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\", usecols=[\"id\", \"excerpt\"])\n    dataset = LitDataset(test, model_name, inference_only=True)\n    dataloader = DataLoader(dataset, batch_size=32, drop_last=False, shuffle=False, num_workers=4)\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:52:30.098863Z","iopub.execute_input":"2021-07-29T01:52:30.099173Z","iopub.status.idle":"2021-07-29T01:52:30.106911Z","shell.execute_reply.started":"2021-07-29T01:52:30.099144Z","shell.execute_reply":"2021-07-29T01:52:30.105971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def load_data():\n    data = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\", usecols=[\"target\", \"standard_error\"])\n    data.drop(data[(data.target == 0) & (data.standard_error == 0)].index, inplace=True)\n    data.reset_index(drop=True, inplace=True)\n    return data\n\ntarget = load_data()[\"target\"].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:52:30.109728Z","iopub.execute_input":"2021-07-29T01:52:30.11006Z","iopub.status.idle":"2021-07-29T01:52:30.212664Z","shell.execute_reply.started":"2021-07-29T01:52:30.110022Z","shell.execute_reply":"2021-07-29T01:52:30.211825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_oof = []\nall_pred = []\nall_pred_for_stack = []","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:52:30.215911Z","iopub.execute_input":"2021-07-29T01:52:30.216188Z","iopub.status.idle":"2021-07-29T01:52:30.222826Z","shell.execute_reply.started":"2021-07-29T01:52:30.216159Z","shell.execute_reply":"2021-07-29T01:52:30.221763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_by_roberta(\n    model: nn.Module,\n    model_name_or_path: str,\n    model_dir: str,\n    num_fold: int = 5,\n):\n    model_dir = pathlib.Path(model_dir)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    dataloader = get_dataloader(model_name_or_path)\n    \n    pred = []\n    for i in range(num_fold):\n        model = model.to(device)\n        model.load_state_dict(torch.load(str(model_dir / f\"model_{i}.pth\")))\n        model.eval()  # Ignore dropout and bn layers.\n\n        pred_by_fold = []\n        with torch.no_grad():  # Skip gradient calculation\n            for batch in dataloader:\n                batch[0] = batch[0].to(device)\n                batch[1] = batch[1].to(device)\n\n                z = model(*batch)\n                pred_by_fold.append(z)\n\n        pred_by_fold = torch.cat(pred_by_fold, dim=0).detach().cpu().numpy().copy()\n        pred.append(pred_by_fold)\n\n    return np.mean(pred, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:52:30.226049Z","iopub.execute_input":"2021-07-29T01:52:30.226335Z","iopub.status.idle":"2021-07-29T01:52:30.237703Z","shell.execute_reply.started":"2021-07-29T01:52:30.226311Z","shell.execute_reply":"2021-07-29T01:52:30.236676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEEDS = [42, 422, 12, 123, 7]","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:52:30.240095Z","iopub.execute_input":"2021-07-29T01:52:30.240597Z","iopub.status.idle":"2021-07-29T01:52:30.246692Z","shell.execute_reply.started":"2021-07-29T01:52:30.240561Z","shell.execute_reply":"2021-07-29T01:52:30.245857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### [commonlit-finetuned-roberta-base](https://www.kaggle.com/konumaru/commonlit-finetuned-roberta-base)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n        \n        hidden_size = self.config.hidden_size\n        self.attention = nn.Sequential(            \n            nn.Linear(hidden_size, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(    \n            nn.LayerNorm(hidden_size),                    \n            nn.Linear(hidden_size, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-29T01:52:30.249983Z","iopub.execute_input":"2021-07-29T01:52:30.250279Z","iopub.status.idle":"2021-07-29T01:52:30.266413Z","shell.execute_reply.started":"2021-07-29T01:52:30.250248Z","shell.execute_reply":"2021-07-29T01:52:30.265604Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"commonlit-finetuned-roberta-base\"\nmodel_name_or_path = \"../input/roberta-transformers-pytorch/roberta-base\"\n\nfor seed in SEEDS:\n    oof = np.load(f\"../input/{data_dir}/seed{seed}/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path), \n        model_name_or_path=model_name_or_path,\n        model_dir=f\"../input/{data_dir}/seed{seed}/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)\n    \nall_pred_for_stack.append(np.mean(all_pred[-5:], axis=0))\nprint(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-5:], axis=0), squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:52:30.26847Z","iopub.execute_input":"2021-07-29T01:52:30.268892Z","iopub.status.idle":"2021-07-29T01:55:03.851408Z","shell.execute_reply.started":"2021-07-29T01:52:30.268851Z","shell.execute_reply":"2021-07-29T01:55:03.850408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### [commonlit-finetuned-roberta-base-init-4layers](https://www.kaggle.com/konumaru/commonlit-finetuned-roberta-base-init-4layers)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-29T01:55:03.853124Z","iopub.execute_input":"2021-07-29T01:55:03.853697Z","iopub.status.idle":"2021-07-29T01:55:03.868802Z","shell.execute_reply.started":"2021-07-29T01:55:03.85365Z","shell.execute_reply":"2021-07-29T01:55:03.867825Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"commonlit-finetuned-roberta-base-init-4layers\"\nmodel_name_or_path = \"../input/roberta-transformers-pytorch/roberta-base\"\n\nfor seed in SEEDS:\n    oof = np.load(f\"../input/{data_dir}/seed{seed}/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path),\n        model_name_or_path=model_name_or_path,\n        model_dir=f\"../input/{data_dir}/seed{seed}/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:55:03.869814Z","iopub.execute_input":"2021-07-29T01:55:03.870362Z","iopub.status.idle":"2021-07-29T01:56:58.896808Z","shell.execute_reply.started":"2021-07-29T01:55:03.870233Z","shell.execute_reply":"2021-07-29T01:56:58.895753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_pred_for_stack.append(np.mean(all_pred[-5:], axis=0))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:56:58.898607Z","iopub.execute_input":"2021-07-29T01:56:58.898967Z","iopub.status.idle":"2021-07-29T01:56:58.905475Z","shell.execute_reply.started":"2021-07-29T01:56:58.898928Z","shell.execute_reply":"2021-07-29T01:56:58.904153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-5:], axis=0), squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:56:58.906558Z","iopub.execute_input":"2021-07-29T01:56:58.907001Z","iopub.status.idle":"2021-07-29T01:56:58.917187Z","shell.execute_reply.started":"2021-07-29T01:56:58.906963Z","shell.execute_reply":"2021-07-29T01:56:58.916206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### [commonlit-finetuned-roberta-base-squad2](https://www.kaggle.com/konumaru/commonlit-finetuned-roberta-base-squad2)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n        \n        hidden_size = self.config.hidden_size\n        self.attention = nn.Sequential(            \n            nn.Linear(hidden_size, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(hidden_size, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-29T01:56:58.918714Z","iopub.execute_input":"2021-07-29T01:56:58.919138Z","iopub.status.idle":"2021-07-29T01:56:58.933882Z","shell.execute_reply.started":"2021-07-29T01:56:58.919091Z","shell.execute_reply":"2021-07-29T01:56:58.932876Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"commonlit-finetuned-roberta-base-squad2\"\nmodel_name_or_path = \"../input/roberta-transformers-pytorch/roberta-base\"\n\nfor seed in SEEDS:\n    oof = np.load(f\"../input/{data_dir}/seed{seed}/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path), \n        model_name_or_path=model_name_or_path,\n        model_dir=f\"../input/{data_dir}/seed{seed}/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:56:58.935267Z","iopub.execute_input":"2021-07-29T01:56:58.935639Z","iopub.status.idle":"2021-07-29T01:59:29.480619Z","shell.execute_reply.started":"2021-07-29T01:56:58.935603Z","shell.execute_reply":"2021-07-29T01:59:29.479385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_pred_for_stack.append(np.mean(all_pred[-5:], axis=0))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:59:29.482473Z","iopub.execute_input":"2021-07-29T01:59:29.482842Z","iopub.status.idle":"2021-07-29T01:59:29.491133Z","shell.execute_reply.started":"2021-07-29T01:59:29.48281Z","shell.execute_reply":"2021-07-29T01:59:29.490147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-5:], axis=0), squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:59:29.492559Z","iopub.execute_input":"2021-07-29T01:59:29.493009Z","iopub.status.idle":"2021-07-29T01:59:29.505565Z","shell.execute_reply.started":"2021-07-29T01:59:29.492958Z","shell.execute_reply":"2021-07-29T01:59:29.504658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### [commonlit-finetuned-roberta-large](https://www.kaggle.com/konumaru/commonlit-finetuned-roberta-large)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n        \n        hidden_size = self.config.hidden_size\n        self.attention = nn.Sequential(            \n            nn.Linear(hidden_size, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.LayerNorm(hidden_size),\n            nn.Linear(hidden_size, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-29T01:59:29.507033Z","iopub.execute_input":"2021-07-29T01:59:29.507438Z","iopub.status.idle":"2021-07-29T01:59:29.525266Z","shell.execute_reply.started":"2021-07-29T01:59:29.507397Z","shell.execute_reply":"2021-07-29T01:59:29.524147Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"commonlit-finetuned-roberta-large\"\nmodel_name_or_path = \"../input/roberta-transformers-pytorch/roberta-large\"\n\nfor seed in [42, 422, 12]:\n    oof = np.load(f\"../input/{data_dir}/seed{seed}/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path), \n        model_name_or_path=model_name_or_path,\n        model_dir=f\"../input/{data_dir}/seed{seed}/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T01:59:29.526739Z","iopub.execute_input":"2021-07-29T01:59:29.527177Z","iopub.status.idle":"2021-07-29T02:03:01.315039Z","shell.execute_reply.started":"2021-07-29T01:59:29.527132Z","shell.execute_reply":"2021-07-29T02:03:01.312788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_pred_for_stack.append(np.mean(all_pred[-3:], axis=0))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:03:01.318832Z","iopub.execute_input":"2021-07-29T02:03:01.31926Z","iopub.status.idle":"2021-07-29T02:03:01.324495Z","shell.execute_reply.started":"2021-07-29T02:03:01.319213Z","shell.execute_reply":"2021-07-29T02:03:01.323577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-3:], axis=0), squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:03:01.325784Z","iopub.execute_input":"2021-07-29T02:03:01.326451Z","iopub.status.idle":"2021-07-29T02:03:01.339937Z","shell.execute_reply.started":"2021-07-29T02:03:01.326411Z","shell.execute_reply":"2021-07-29T02:03:01.338588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### [commonlit-finetuned-roberta-large-squad2](https://www.kaggle.com/konumaru/commonlit-finetuned-roberta-large-squad2)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n        \n        hidden_size = self.config.hidden_size\n        self.attention = nn.Sequential(            \n            nn.Linear(hidden_size, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.LayerNorm(hidden_size),\n            nn.Linear(hidden_size, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n        weights = self.attention(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-29T02:03:01.341688Z","iopub.execute_input":"2021-07-29T02:03:01.342268Z","iopub.status.idle":"2021-07-29T02:03:01.359585Z","shell.execute_reply.started":"2021-07-29T02:03:01.342155Z","shell.execute_reply":"2021-07-29T02:03:01.358456Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"commonlit-finetuned-roberta-large-squad2\"\nmodel_name_or_path = \"../input/roberta-transformers-pytorch/roberta-large\"\n\nfor seed in [42, 422, 12]:\n    oof = np.load(f\"../input/{data_dir}/seed{seed}/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path), \n        model_name_or_path=model_name_or_path,\n        model_dir=f\"../input/{data_dir}/seed{seed}/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:03:01.364731Z","iopub.execute_input":"2021-07-29T02:03:01.365211Z","iopub.status.idle":"2021-07-29T02:06:15.800677Z","shell.execute_reply.started":"2021-07-29T02:03:01.365149Z","shell.execute_reply":"2021-07-29T02:06:15.799456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_pred_for_stack.append(np.mean(all_pred[-3:], axis=0))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:06:15.804124Z","iopub.execute_input":"2021-07-29T02:06:15.804475Z","iopub.status.idle":"2021-07-29T02:06:15.811264Z","shell.execute_reply.started":"2021-07-29T02:06:15.804437Z","shell.execute_reply":"2021-07-29T02:06:15.81042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-3:], axis=0), squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:06:15.81252Z","iopub.execute_input":"2021-07-29T02:06:15.813119Z","iopub.status.idle":"2021-07-29T02:06:15.825543Z","shell.execute_reply.started":"2021-07-29T02:06:15.813078Z","shell.execute_reply":"2021-07-29T02:06:15.824721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### [commonlit-finetuned-roberta-large-meanpool](https://www.kaggle.com/konumaru/commonlit-finetuned-roberta-large-meanpool)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModel,\n)\n\nclass LitModel(nn.Module):\n    def __init__(self, model_name_or_path=\"roberta-base\"):\n        super().__init__()\n\n        self.config = AutoConfig.from_pretrained(model_name_or_path)\n        self.config.update({\n            \"output_hidden_states\":True, \n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n        })                       \n        \n        self.roberta = AutoModel.from_pretrained(model_name_or_path, config=self.config)  \n        \n        hidden_size = self.config.hidden_size\n        self.regressor = nn.Sequential(                        \n            nn.Linear(hidden_size, 1)                        \n        )\n\n        self._init_embed_layers(reinit_layers=4)\n\n    def _init_embed_layers(self, reinit_layers: int = 4):\n        if reinit_layers > 0:\n            for layer in self.roberta.encoder.layer[-reinit_layers:]:\n                for module in layer.modules():\n                    if isinstance(module, nn.Linear):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.bias is not None:\n                            module.bias.data.zero_()\n                    elif isinstance(module, nn.Embedding):\n                        module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n                        if module.padding_idx is not None:\n                            module.weight.data[module.padding_idx].zero_()\n                    elif isinstance(module, nn.LayerNorm):\n                        module.bias.data.zero_()\n                        module.weight.data.fill_(1.0)\n        \n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(mean_embeddings)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-29T02:06:15.826871Z","iopub.execute_input":"2021-07-29T02:06:15.82722Z","iopub.status.idle":"2021-07-29T02:06:15.842379Z","shell.execute_reply.started":"2021-07-29T02:06:15.827185Z","shell.execute_reply":"2021-07-29T02:06:15.841416Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = \"commonlit-finetuned-roberta-large-meanpool\"\nmodel_name_or_path = \"../input/roberta-transformers-pytorch/roberta-large\"\n\nfor seed in [42, 422, 12]:\n    oof = np.load(f\"../input/{data_dir}/seed{seed}/oof.npy\").reshape(-1, 1)\n    pred = predict_by_roberta(\n        model=LitModel(model_name_or_path), \n        model_name_or_path=model_name_or_path,\n        model_dir=f\"../input/{data_dir}/seed{seed}/models\",\n    )\n    \n    all_oof.append(oof)\n    all_pred.append(pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:06:15.843676Z","iopub.execute_input":"2021-07-29T02:06:15.844104Z","iopub.status.idle":"2021-07-29T02:09:32.572664Z","shell.execute_reply.started":"2021-07-29T02:06:15.84407Z","shell.execute_reply":"2021-07-29T02:09:32.571422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_pred_for_stack.append(np.mean(all_pred[-3:], axis=0))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:09:32.575553Z","iopub.execute_input":"2021-07-29T02:09:32.575998Z","iopub.status.idle":"2021-07-29T02:09:32.581006Z","shell.execute_reply.started":"2021-07-29T02:09:32.575953Z","shell.execute_reply":"2021-07-29T02:09:32.580082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"RMSE: \", mean_squared_error(target,np.mean(all_oof[-3:], axis=0), squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:09:32.582254Z","iopub.execute_input":"2021-07-29T02:09:32.582758Z","iopub.status.idle":"2021-07-29T02:09:32.596979Z","shell.execute_reply.started":"2021-07-29T02:09:32.58272Z","shell.execute_reply":"2021-07-29T02:09:32.595699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Averaging","metadata":{}},{"cell_type":"code","source":"all_oof = np.concatenate(all_oof, axis=1)\nprint(\"RMSE: \", mean_squared_error(target, np.mean(all_oof, axis=1), squared=False))","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:09:32.599226Z","iopub.execute_input":"2021-07-29T02:09:32.5998Z","iopub.status.idle":"2021-07-29T02:09:32.609696Z","shell.execute_reply.started":"2021-07-29T02:09:32.59973Z","shell.execute_reply":"2021-07-29T02:09:32.608403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering for stacking model","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/textfeatmodule\")\nsys.path.append(\"../input/textfeatmodule/readability-package\")\n\nfrom textfeat import create_text_feat","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:09:32.612009Z","iopub.execute_input":"2021-07-29T02:09:32.612552Z","iopub.status.idle":"2021-07-29T02:09:36.244724Z","shell.execute_reply.started":"2021-07-29T02:09:32.612508Z","shell.execute_reply":"2021-07-29T02:09:36.243931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\ntext_feat = create_text_feat(test)\n\nfeat_cols = [\n    'chars_per_word',\n    'syll_per_word',\n#     'words_per_sent',\n#     'kincaid',\n#     'ari',\n#     'coleman_liau',\n#     'flesch',\n#     'gunning_fog',\n#     'lix',\n#     'smog',\n#     'rix',\n#     'dale_chall',\n#     'tobeverb',\n#     'auxverb',\n#     'conjunction',\n#     'pronoun',\n#     'preposition',\n#     'nominalization',\n#     'pronoun_b',\n    'interrogative',\n#     'article',\n#     'subordination',\n#     'conjunction_b',\n#     'preposition_b',\n]\n\n# feat_cols += [f\"spacy_{i}\" for i in range(300)]\n\nfeat_cols += [\n#     'CC',\n#     'CD',\n#     'DT',\n#     'EX',\n#     'FW',\n#     'IN',\n#     'JJ',\n#     'JJR',\n#     'JJS',\n#     'LS',\n#     'MD',\n#     'NN',\n#     'NNS',\n#     'NNP',\n#     'NNPS',\n#     'PDT',\n#     'POS',\n#     'PRP',\n#     'RB',\n#     'RBR',\n#     'RBS',\n#     'RP',\n#     'TO',\n#     'UH',\n#     'VB',\n#     'VBD',\n#     'VBG',\n#     'VBZ',\n#     'WDT',\n#     'WP',\n#     'WRB',\n#     'periods',\n    'commas',\n#     'semis',\n#     'exclaims',\n#     'questions',\n#     'num_char',\n#     'num_words',\n#     'unique_words',\n#     'word_diversity',\n#     'longest_word',\n#     'avg_len_word',\n]","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:09:36.24794Z","iopub.execute_input":"2021-07-29T02:09:36.2482Z","iopub.status.idle":"2021-07-29T02:09:45.588242Z","shell.execute_reply.started":"2021-07-29T02:09:36.248173Z","shell.execute_reply":"2021-07-29T02:09:45.587318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_stack = np.concatenate(all_pred_for_stack, axis=1)\nX_stack = np.concatenate([X_stack, text_feat[feat_cols].to_numpy()], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:09:45.589562Z","iopub.execute_input":"2021-07-29T02:09:45.589921Z","iopub.status.idle":"2021-07-29T02:09:45.601966Z","shell.execute_reply.started":"2021-07-29T02:09:45.589885Z","shell.execute_reply":"2021-07-29T02:09:45.600821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# Predict function for stacking.\ndef predict(\n    data: pd.DataFrame, \n    model_dir: str,\n    seed: int = 42,\n    n_splits: int = 5,\n    num_seed: int = 5,\n) -> np.ndarray:\n    all_pred = []\n    for i in range(num_seed):\n        _seed = seed + i\n        pred = np.zeros(data.shape[0])\n        for n_fold in range(n_splits):\n            with open(os.path.join(model_dir, f\"seed{_seed}/{n_fold}-fold.pkl\"), mode=\"rb\") as file:\n                model = pickle.load(file)\n            pred += model.predict(data) / n_splits\n        all_pred.append(pred.reshape(-1, 1))\n    return np.mean(all_pred, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:09:45.603553Z","iopub.execute_input":"2021-07-29T02:09:45.603944Z","iopub.status.idle":"2021-07-29T02:09:45.612322Z","shell.execute_reply.started":"2021-07-29T02:09:45.603908Z","shell.execute_reply":"2021-07-29T02:09:45.611057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_names = (\"bayesian_ridge\", \"ridge\", \"mlp\", \"svr\", \"xgb\")\n\npred_stacked = []\nfor model_name in model_names:\n    pred = predict(\n        X_stack, \n        f\"../input/k/konumaru/train-stack-models-roberta-base-rsa/{model_name}\",\n        num_seed=7\n    )\n    pred_stacked.append(pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:09:45.61394Z","iopub.execute_input":"2021-07-29T02:09:45.614345Z","iopub.status.idle":"2021-07-29T02:09:48.571193Z","shell.execute_reply.started":"2021-07-29T02:09:45.614282Z","shell.execute_reply":"2021-07-29T02:09:48.570373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n# # Averaging for submission\n# submission[\"target\"] = (\n#     0.7 * np.mean(np.concatenate(all_pred, axis=1), axis=1).reshape(-1, 1)\n#     + 0.3 * np.mean(np.concatenate(pred_stacked, axis=1), axis=1).reshape(-1, 1)\n# )\n\n# submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:09:48.572359Z","iopub.execute_input":"2021-07-29T02:09:48.572694Z","iopub.status.idle":"2021-07-29T02:09:48.598164Z","shell.execute_reply.started":"2021-07-29T02:09:48.572661Z","shell.execute_reply":"2021-07-29T02:09:48.597111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n# Averaging for submission\nsubmission[\"target\"] = (\n    0.7 * np.mean(np.concatenate(all_pred, axis=1), axis=1).reshape(-1, 1)\n    + 0.3 * np.mean(np.concatenate(pred_stacked, axis=1), axis=1).reshape(-1, 1)\n) * 1.01\n\nsubmission.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-29T02:09:48.599495Z","iopub.execute_input":"2021-07-29T02:09:48.599859Z","iopub.status.idle":"2021-07-29T02:09:48.609701Z","shell.execute_reply.started":"2021-07-29T02:09:48.599821Z","shell.execute_reply":"2021-07-29T02:09:48.608826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}