{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load library","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false}},{"cell_type":"code","source":"# warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# for NLP\nimport spacy\nfrom spacy.lang.en import English\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport en_core_web_sm\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.corpora import Dictionary\n\n# for data processing\nimport pandas as pd\nimport numpy as np\n\n# for statistics\nimport statistics\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport collections\n\n# for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nimport umap\n\n# for network analysis\nimport networkx as nx\n\n# for modeling\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn import metrics","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ntrain['flag'] = 'train'\ntest = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\ntest['flag'] = 'test'\nalldata = pd.concat([train,test], axis=0, ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explore URL and License\n* This dataset has source URL and License in about 800 observations.\n* First of all, I explored effects of URL and License on target.","metadata":{}},{"cell_type":"code","source":"alldata['url_YN'] = [0 if pd.isna(i) else 1 for i in alldata['url_legal']]\nalldata['license_YN'] = [0 if pd.isna(i) else 1 for i in alldata['license']]\nalldata['url_domain'] = [\"\" if pd.isna(s) else s.split('/')[2] for s in alldata['url_legal']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## License","metadata":{}},{"cell_type":"code","source":"CC_3 = ['CC BY 3.0', 'CC BY-NC 3.0', 'CC BY-NC-SA 3.0 ', 'CC BY-SA 3.0', 'CC BY-SA 3.0 and GFD']\nCC_4 = ['CC BY 4.0', 'CC BY-NC-ND 4.0', 'CC BY-NC-SA 4.0']\n\nlicense_cat = []\n\nfor cat in alldata['license']:\n    if cat in CC_3:\n        license_cat.append('CC_3')\n    elif cat in CC_4:\n        license_cat.append('CC_4')\n    elif pd.isna(cat):\n        license_cat.append('nan')\n    else:\n        license_cat.append('others')\n\nalldata['license_cat'] = license_cat\n        \nprint(alldata.groupby('license').count()['id'])\nprint(alldata.groupby('license_cat').count()['id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## URL","metadata":{}},{"cell_type":"code","source":"wiki = ['en.wikibooks.org', 'en.wikipedia.org']\nsimple = ['simple.wikipedia.org']\nafrican = ['www.africanstorybook.org']\nkids = ['kids.frontiersin.org']\nlit = ['www.commonlit.org']\n\nurl_cat = []\n\nfor cat in alldata['url_domain']:\n    if cat in wiki:\n        url_cat.append('wiki')\n    elif cat in simple:\n        url_cat.append('simple')\n    elif cat in african:\n        url_cat.append('african')\n    elif cat in kids:\n        url_cat.append('kids')\n    elif cat in lit:\n        url_cat.append('lit')\n    elif cat=='':\n        url_cat.append('nan')\n    else:\n        url_cat.append('others')\n        \nalldata['url_cat'] = url_cat\n\nprint(alldata.groupby('url_domain').count()['id'])\nprint(alldata.groupby('url_cat').count()['id'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\nsns.boxplot(x=alldata['license_cat'],y=alldata['target'], ax=ax1)\nsns.boxplot(x=alldata['url_cat'],y=alldata['target'], ax=ax2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The distributions of target score in different license/URL differ. This may effect on target score, but observation which has URL and License is very few.**","metadata":{}},{"cell_type":"markdown","source":"# Basic statistics\n* Explore the effect of sentense length and its statistics.","metadata":{}},{"cell_type":"code","source":"# sentence count and words in sentence count\nnlp = English()\nsbd = nlp.create_pipe('sentencizer')\nnlp.add_pipe(sbd)\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_list = []\nnum_sent_list = []\nnum_word_list = []\nnum_word_stats_list = []\n\nremove_list = [',', '.', ':', ';', '!', '?', '\\n', '-', '=', '/', '#', '$', '(', ')']\n\nfor doc in alldata['excerpt']:\n    doc = nlp(doc)\n    sent_temp = [s for s in doc.sents]\n    sent_list.append(sent_temp)\n    num_sent_list.append(len(sent_temp))\n    \n    num_word_temp = []\n    word_list = []\n    for s in sent_temp:\n        text_temp = [token.text for token in s]\n        word_list = [w for w in text_temp if w not in remove_list]\n        num_word_temp.append(len(word_list))\n        \n    num_word_stats_list.append([min(num_word_temp), max(num_word_temp), statistics.mean(num_word_temp), statistics.median(num_word_temp), statistics.stdev(num_word_temp)])\n    num_word_list.append(num_word_temp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_statistics = pd.DataFrame({'num_sentence':num_sent_list,\n                              'num_word_min':[x[0] for x in num_word_stats_list],\n                              'num_word_max':[x[1] for x in num_word_stats_list],\n                              'num_word_mean':[x[2] for x in num_word_stats_list],\n                              'num_word_median':[x[3] for x in num_word_stats_list],\n                              'num_word_stdev':[x[4] for x in num_word_stats_list]})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"statistics_col = df_statistics.columns\nfig,axes = plt.subplots(nrows=3,ncols=2,figsize=(14,14))\n\nfor i in range(3):\n    for j in range(2):\n        axes[i,j].scatter(df_statistics[statistics_col[j+(i*2)]],alldata['target'])\n        axes[i,j].set_title(statistics_col[j+(i*2)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Number of words in sentense may affect on readability. Intuitively, it is reasonabole because students cannot understand the meaning clearly if the sentence is too long.","metadata":{}},{"cell_type":"markdown","source":"# Word type and dependency type\n* Dose the ratio of word type (e.g. ADJ, NOUN, VERB etc) in the sentence affect on readability?\n* Dose the ratio of dependency type (e.g. prep, proj, det etc) in the sentence affect on readability?","metadata":{}},{"cell_type":"markdown","source":"## word type","metadata":{}},{"cell_type":"code","source":"for i in range(len(alldata)):\n    doc_id = alldata['id'][i]\n    test_doc = nlp(alldata['excerpt'][i])\n    token_pos = [w.pos_ for w in test_doc]\n    c = collections.Counter(token_pos)\n    token_df_temp = pd.DataFrame.from_dict(c, orient='index', columns=[doc_id])\n    token_df_temp[doc_id] = token_df_temp[doc_id]/sum(token_df_temp[doc_id])\n    \n    if i == 0:\n        token_df = token_df_temp\n    else:\n        token_df = token_df.merge(token_df_temp,left_index=True, right_index=True, how='outer')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_df = token_df.fillna(0).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes = plt.subplots(nrows=6,ncols=3,figsize=(16,24))\npos_name_list = token_df.columns\n\nfor i in range(6):\n    for j in range(3):\n        axes[i,j].scatter(token_df[pos_name_list[j+(i*3)]],alldata['target'])\n        axes[i,j].set_title(pos_name_list[j+(i*3)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Wow, VERV ratio and NOUN ratio may affect on readability!**","metadata":{}},{"cell_type":"markdown","source":"## dependency type","metadata":{}},{"cell_type":"code","source":"for i in range(len(alldata)):\n    doc_id = alldata['id'][i]\n    test_doc = nlp(alldata['excerpt'][i])\n    token_pos = [w.dep_ for w in test_doc]\n    c = collections.Counter(token_pos)\n    dep_df_temp = pd.DataFrame.from_dict(c, orient='index', columns=[doc_id])\n    dep_df_temp[doc_id] = dep_df_temp[doc_id]/sum(dep_df_temp[doc_id])\n    \n    if i == 0:\n        dep_df = dep_df_temp\n    else:\n        dep_df = dep_df.merge(dep_df_temp,left_index=True, right_index=True, how='outer')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dep_df = dep_df.fillna(0).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes = plt.subplots(nrows=9,ncols=5,figsize=(16,24))\ndep_name_list = dep_df.columns\n\nfor i in range(9):\n    for j in range(5):\n        #print(complexity_col[j+(i*2)])\n        axes[i,j].scatter(dep_df[dep_name_list[j+(i*5)]],alldata['target'])\n        axes[i,j].set_title(dep_name_list[j+(i*5)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Oh, some depencency types have effects on readability!**","metadata":{}},{"cell_type":"markdown","source":"# Dependency network analysis","metadata":{}},{"cell_type":"markdown","source":"## sentense as a network\n* Using depencensy analysis result, we can analysis the sentense as a network. So we can extract some network indice (e.g. degree, density, diameter etc) as features.","metadata":{}},{"cell_type":"code","source":"test_doc = nlp(alldata['excerpt'][0])\ntest_sent = [s for s in test_doc.sents]\n\nsource_list = []\ntarget_list = []\n\nfor token in test_sent[0]:\n    if token.pos_=='PUNCT' or token.dep_=='ROOT':\n        continue\n    else:\n        source_list.append(token.head.i)\n        target_list.append(token.i)\n        \ndf_dep = pd.DataFrame({'source':source_list,\n                       'target':target_list})\nprint(df_dep)\ndep_g = nx.from_pandas_edgelist(df_dep)\nnx.draw_networkx(dep_g)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It's super interesting!**","metadata":{}},{"cell_type":"markdown","source":"## Extract features from network analysis","metadata":{}},{"cell_type":"code","source":"def create_edge_df(doc):\n    doc = nlp(doc)\n    source_list = []\n    target_list = []\n\n    for token in doc:\n        if token.pos_=='PUNCT' or token.dep_=='ROOT':\n            continue\n        else:\n            source_list.append(token.head.i)\n            target_list.append(token.i)\n\n    df_dep = pd.DataFrame({'source':source_list,\n                           'target':target_list})\n    \n    return(df_dep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dep_network(doc):\n    edge_df = create_edge_df(doc)\n    g = nx.from_pandas_edgelist(edge_df)\n    return(g)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_index(doc):\n    g = create_dep_network(doc)\n    index_dict = {}\n    # degree\n    degree_df_temp = pd.DataFrame.from_dict(dict(nx.degree(g)), orient='index')\n    # density\n    density_list = [nx.density(g.subgraph(c)) for c in nx.connected_components(g)]\n    # diameter\n    diameter_list = [nx.diameter(g.subgraph(c)) for c in nx.connected_components(g)]\n    \n    index_dict = {'degree_min':degree_df_temp[0].min(),\n                  'degree_max':degree_df_temp[0].max(),\n                  'degree_mean':degree_df_temp[0].mean(),\n                  'degree_median':degree_df_temp[0].median(),\n                  'degree_std':degree_df_temp[0].std(),\n                  'density_min':min(density_list),\n                  'density_max':max(density_list),\n                  'density_mean':statistics.mean(density_list),\n                  'density_median':statistics.median(density_list),\n                  'density_std':statistics.stdev(density_list),\n                  'diameter_min':min(diameter_list),\n                  'diameter_max':max(diameter_list),\n                  'diameter_mean':statistics.mean(diameter_list),\n                  'diameter_median':statistics.median(diameter_list),\n                  'diameter_std':statistics.stdev(diameter_list)}\n    return(index_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(alldata)):\n    doc_id = alldata['id'][i]\n    doc = alldata['excerpt'][i]\n    index_dict = calculate_index(doc)\n\n    index_df_temp = pd.DataFrame.from_dict(index_dict, orient='index', columns=[doc_id])\n    \n    if i == 0:\n        index_df = index_df_temp\n    else:\n        index_df = index_df.merge(index_df_temp,left_index=True, right_index=True, how='outer')\n        \nindex_df = index_df.T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes = plt.subplots(nrows=5,ncols=3,figsize=(16,24))\nindex_name_list = index_df.columns\n\nfor i in range(5):\n    for j in range(3):\n        #print(complexity_col[j+(i*2)])\n        axes[i,j].scatter(index_df[index_name_list[j+(i*3)]],alldata['target'])\n        axes[i,j].set_title(index_name_list[j+(i*3)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word vector from SpaCy","metadata":{}},{"cell_type":"markdown","source":"## extract word vector\n* We can extract word vector from SpaCy default.\n* If it is document, we can extract mean word vector.","metadata":{}},{"cell_type":"code","source":"test_doc = nlp(alldata['excerpt'][0])\ntest_sent = [s for s in test_doc.sents]\nprint('word vector for sentence : ', test_sent[0].vector)\nprint('word vector length : ', len(test_sent[0].vector))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## extract word vector from all document\n* But before extraction, we need to some word type because those cause noise.\n* Please imagine, the word 'it' has a vector, the punctuation ',' also has a vectore. Do these words/punctuation contribute to total meaning?","metadata":{}},{"cell_type":"code","source":"POS = ['PRON', 'PROPN', 'NOUN', 'VERB', 'ADJ', 'ADV','ADP']\nnew_docs = []\ndocs = alldata['excerpt']\nfor doc in docs:\n    doc = nlp(doc)\n    new_docs.append(\" \".join([token.lemma_ for token in doc if token.pos_ in POS or len(POS) ==0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_wv_list = []\n\nfor doc in new_docs:\n    doc_temp = nlp(doc)\n    new_wv_list.append(doc_temp.vector)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_wv_new = pd.DataFrame(new_wv_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## PCA for word vector\n* Word vector in Spacy has 96 dimensions. Plotting them directly is difficult to understand.\n* So I plotted some principle components vs target score.","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components = 10)\nres_pca_10 = pca.fit_transform(df_wv_new)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes = plt.subplots(nrows=5,ncols=2,figsize=(16,32))\n\nfor i in range(5):\n    for j in range(2):\n        axes[i,j].scatter(res_pca_10[:,j+(i*2)],alldata['target'])\n        axes[i,j].set_title('PCA'+str(j+(i*2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Yes, 1st principal component related to target score!**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}