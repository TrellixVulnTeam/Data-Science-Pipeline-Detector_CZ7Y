{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About this implementation\n- Submission for this competition is limitted for three hours. If you include the training phase that takes long time, your submission will not be accepted.\n- So my other strategy is\n    1. Model training on kaggle notebook\n    1. Download the models to my local machine\n    1. Upload the models to kaggle\n    1. Add the models to my notebook\n    1. Run all codes except for the training phase\n- If this strategy is accepted, code execution time limit is not problem.","metadata":{}},{"cell_type":"markdown","source":"# Try it!","metadata":{}},{"cell_type":"code","source":"import torch \nfrom torch import nn \nimport torch.nn.functional as F\nimport numpy as np \nimport pandas as pd \nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport pytorch_lightning as pl \nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nimport transformers\nfrom transformers import get_linear_schedule_with_warmup, AdamW\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-12T01:16:59.051312Z","iopub.execute_input":"2021-06-12T01:16:59.051673Z","iopub.status.idle":"2021-06-12T01:17:06.251599Z","shell.execute_reply.started":"2021-06-12T01:16:59.051597Z","shell.execute_reply":"2021-06-12T01:17:06.250679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data and Process","metadata":{}},{"cell_type":"code","source":"#taking only the id,excerpt,target,standard_error\ndf = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\",usecols=[\"id\",\"excerpt\",\"target\",\"standard_error\"])\ntest_df = pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint(\"train shape\",df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:08.648345Z","iopub.execute_input":"2021-06-12T01:17:08.648696Z","iopub.status.idle":"2021-06-12T01:17:08.738605Z","shell.execute_reply.started":"2021-06-12T01:17:08.64866Z","shell.execute_reply":"2021-06-12T01:17:08.737338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(df['target'], df['standard_error'])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:10.022892Z","iopub.execute_input":"2021-06-12T01:17:10.023224Z","iopub.status.idle":"2021-06-12T01:17:10.184609Z","shell.execute_reply.started":"2021-06-12T01:17:10.023192Z","shell.execute_reply":"2021-06-12T01:17:10.183839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove outlier\ndf = df[df['standard_error']!=0]\nplt.scatter(df['target'], df['standard_error'])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:11.548674Z","iopub.execute_input":"2021-06-12T01:17:11.548985Z","iopub.status.idle":"2021-06-12T01:17:11.679134Z","shell.execute_reply.started":"2021-06-12T01:17:11.548954Z","shell.execute_reply":"2021-06-12T01:17:11.678374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#any null rows\nprint(\"TRAIN NULLS: \\n\",df.isnull().sum())\nprint(\"TEST NULLS: \\n\",df.isnull().sum()) ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:13.299587Z","iopub.execute_input":"2021-06-12T01:17:13.299914Z","iopub.status.idle":"2021-06-12T01:17:13.31018Z","shell.execute_reply.started":"2021-06-12T01:17:13.299882Z","shell.execute_reply":"2021-06-12T01:17:13.309047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove \\n and replace \\'s with 'sfrom the text\ndef prep_text(text_df):\n    text_df = text_df.str.replace(\"\\n\",\"\",regex=False) \n    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\ndf[\"excerpt\"] = prep_text(df[\"excerpt\"])\ntest_df[\"excerpt\"] = prep_text(test_df[\"excerpt\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:14.805414Z","iopub.execute_input":"2021-06-12T01:17:14.805767Z","iopub.status.idle":"2021-06-12T01:17:14.822819Z","shell.execute_reply.started":"2021-06-12T01:17:14.805737Z","shell.execute_reply":"2021-06-12T01:17:14.821858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_words = df[\"excerpt\"].apply(lambda x: len(x.split())).max()\nprint(\"maximum words in instance:\",max_words)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:16.448402Z","iopub.execute_input":"2021-06-12T01:17:16.448722Z","iopub.status.idle":"2021-06-12T01:17:16.483743Z","shell.execute_reply.started":"2021-06-12T01:17:16.448691Z","shell.execute_reply":"2021-06-12T01:17:16.482994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Folds ","metadata":{}},{"cell_type":"code","source":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\n# read training data\ndf = pd.read_csv(\"../input/commonlitreadabilityprize/train.csv\")\ndf = df[df['standard_error']!=0]\n\n# create folds\ndf = create_folds(df, num_splits=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:18.779897Z","iopub.execute_input":"2021-06-12T01:17:18.780245Z","iopub.status.idle":"2021-06-12T01:17:18.832423Z","shell.execute_reply.started":"2021-06-12T01:17:18.780209Z","shell.execute_reply":"2021-06-12T01:17:18.831646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bert Model and Training Module","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS = 200\nNUM_TRAIN_STEPS = int((df.shape[0]/BATCH_SIZE)*EPOCHS)\nNUM_WARMUP_STEPS = 0\nFOLDS = df.kfold.unique()\nNUM_FOLDS = df.kfold.nunique() ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:20.988446Z","iopub.execute_input":"2021-06-12T01:17:20.988778Z","iopub.status.idle":"2021-06-12T01:17:20.996227Z","shell.execute_reply.started":"2021-06-12T01:17:20.988749Z","shell.execute_reply":"2021-06-12T01:17:20.995457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.eps = 1e-8\n        \n    def forward(self,output,target):\n        return torch.sqrt(F.mse_loss(output,target)+self.eps)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:22.571704Z","iopub.execute_input":"2021-06-12T01:17:22.572031Z","iopub.status.idle":"2021-06-12T01:17:22.577046Z","shell.execute_reply.started":"2021-06-12T01:17:22.572Z","shell.execute_reply":"2021-06-12T01:17:22.576094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = transformers.AutoModel.from_pretrained(\"../input/bert-base-uncased\")\n        #self.model = transformers.AutoModel.from_pretrained(\"../input/huggingface-bert/bert-large-uncased\")\n        #self.model = transformers.AutoModel.from_pretrained(\"../input/roberta-transformers-pytorch/roberta-base\")\n        self.drop = nn.Dropout(0.3)\n        self.fc = nn.Linear(768,2)  # output to 2 dimensions, targets and errors\n        \n        # convolutional layer\n        self.conv1 = nn.Conv1d(205, 128, kernel_size=3, stride=1, padding=3)\n        self.conv2 = nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=3)\n        self.conv3 = nn.Conv1d(64, 1, kernel_size=3, stride=1, padding=3)\n        self.ReLU = nn.ReLU()\n        self.pool = nn.MaxPool1d(3)\n        #self.fc_conv = nn.Linear(257,2)\n        self.fc_conv = nn.Linear(30,2)\n        \n        #self.fc = nn.Linear(1024,2)\n        \n        # batch normalization\n        self.bn1 = nn.BatchNorm1d(128)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.bn3 = nn.BatchNorm1d(1)\n        \n    \n    def forward(self,inputs):\n        out = self.model(**inputs) # output from BERT model\n        last_hiddens = out[0]\n        #print(last_hiddens.size())\n        #out = self.drop(last_hiddens[:,0,:].squeeze(1))\n        out = self.conv1(last_hiddens)\n        out = self.bn1(out)  # add\n        out = self.ReLU(out)\n        out = self.pool(out)\n        out = self.conv2(out)\n        out = self.bn2(out)  # add\n        out = self.ReLU(out)\n        out = self.pool(out)\n        out = self.conv3(out)\n        out = self.bn3(out)  # add\n        out = self.ReLU(out)\n        out = self.pool(out)\n        #print(out.size())\n        return self.fc_conv(out)\n    \n    def configure_optimizers(self):\n        no_decay = ['bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01}, # original : 0.01\n            {'params': [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5) # original : 5e-5\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=NUM_WARMUP_STEPS, num_training_steps=NUM_TRAIN_STEPS)\n        return [optimizer],[scheduler] \n    \n    def loss_fn(self,output,target):\n        return RMSELoss()(output.view(-1,2),target.view(-1,2))\n    \n    def training_step(self,batch,batch_idx):\n        inputs = batch[\"inputs\"]\n        labels = batch[\"label\"]\n        output = self(inputs)\n        loss = self.loss_fn(output,labels)\n        return loss\n    \n    def validation_step(self,batch,batch_idx):\n        inputs = batch[\"inputs\"]\n        labels = batch[\"label\"]\n        output = self(inputs)\n        loss = self.loss_fn(output,labels)\n        self.log(\"val_loss\",loss,prog_bar=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:24.153643Z","iopub.execute_input":"2021-06-12T01:17:24.153968Z","iopub.status.idle":"2021-06-12T01:17:24.171858Z","shell.execute_reply.started":"2021-06-12T01:17:24.153934Z","shell.execute_reply":"2021-06-12T01:17:24.171032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize Dataset and Dataloader","metadata":{}},{"cell_type":"code","source":"class BertDataset(Dataset):\n    def __init__(self,texts,labels,max_len):\n        super().__init__()\n        self.texts = texts\n        self.max_len = max_len\n        self.labels = labels\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/bert-base-uncased\")\n        #self.tokenizer = transformers.AutoTokenizer.from_pretrained(\"../input/huggingface-bert/bert-large-uncased\")\n    \n    def __len__(self):\n        return self.labels.shape[0]\n    \n    def __getitem__(self,idx):\n        text = \" \".join(self.texts[idx].split())\n        label = self.labels[idx]\n        inputs = self.tokenizer(text,return_tensors=\"pt\",max_length = self.max_len, padding=\"max_length\",truncation=True)\n        return {\n            \"inputs\":{\"input_ids\":inputs[\"input_ids\"][0],\n                      \"token_type_ids\":inputs[\"token_type_ids\"][0],\n                      \"attention_mask\":inputs[\"attention_mask\"][0],},\n            \"label\":torch.tensor(label,dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:26.077568Z","iopub.execute_input":"2021-06-12T01:17:26.077888Z","iopub.status.idle":"2021-06-12T01:17:26.085323Z","shell.execute_reply.started":"2021-06-12T01:17:26.077855Z","shell.execute_reply":"2021-06-12T01:17:26.08422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainer\n- I already trained the models and incluce this notebook.\n- So skip this section when commit or submission","metadata":{}},{"cell_type":"code","source":"# fold = 0\n# print(\"Fold :\",fold)\n# train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n# train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n# valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n# train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n# bert_model = BertModel() \n# trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n# trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n# trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:28.340747Z","iopub.execute_input":"2021-06-12T01:17:28.341074Z","iopub.status.idle":"2021-06-12T01:17:28.34473Z","shell.execute_reply.started":"2021-06-12T01:17:28.341043Z","shell.execute_reply":"2021-06-12T01:17:28.343728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./checkpoint_0fold.ckpt\"> checkpoint_0fold.ckpt </a>","metadata":{}},{"cell_type":"code","source":"# fold = 1\n# print(\"Fold :\",fold)\n# train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n# train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n# valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n# train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n# bert_model = BertModel() \n# trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n# trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n# trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","metadata":{"execution":{"iopub.status.busy":"2021-06-11T22:39:56.760774Z","iopub.execute_input":"2021-06-11T22:39:56.761133Z","iopub.status.idle":"2021-06-11T23:12:13.701509Z","shell.execute_reply.started":"2021-06-11T22:39:56.761102Z","shell.execute_reply":"2021-06-11T23:12:13.70033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./checkpoint_1fold.ckpt\"> checkpoint_1fold.ckpt </a>","metadata":{}},{"cell_type":"code","source":"# fold = 2\n# print(\"Fold :\",fold)\n# train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n# train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n# valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n# train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n# bert_model = BertModel() \n# trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n# trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n# trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","metadata":{"execution":{"iopub.status.busy":"2021-06-11T23:13:09.803408Z","iopub.execute_input":"2021-06-11T23:13:09.803767Z","iopub.status.idle":"2021-06-11T23:39:56.815003Z","shell.execute_reply.started":"2021-06-11T23:13:09.80373Z","shell.execute_reply":"2021-06-11T23:39:56.813774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./checkpoint_2fold.ckpt\"> checkpoint_2fold.ckpt </a>","metadata":{}},{"cell_type":"code","source":"# fold = 3\n# print(\"Fold :\",fold)\n# train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n# train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n# valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n# train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n# bert_model = BertModel() \n# trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n# trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n# trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","metadata":{"execution":{"iopub.status.busy":"2021-06-11T23:40:11.630938Z","iopub.execute_input":"2021-06-11T23:40:11.631277Z","iopub.status.idle":"2021-06-12T00:10:33.490151Z","shell.execute_reply.started":"2021-06-11T23:40:11.631242Z","shell.execute_reply":"2021-06-12T00:10:33.489124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./checkpoint_3fold.ckpt\"> checkpoint_3fold.ckpt </a>","metadata":{}},{"cell_type":"code","source":"# fold = 4\n# print(\"Fold :\",fold)\n# train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n# train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n# valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n# train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n# valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n# bert_model = BertModel() \n# trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n# trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n# trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T00:11:10.521228Z","iopub.execute_input":"2021-06-12T00:11:10.521561Z","iopub.status.idle":"2021-06-12T00:39:50.985104Z","shell.execute_reply.started":"2021-06-12T00:11:10.521527Z","shell.execute_reply":"2021-06-12T00:39:50.984126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./checkpoint_4fold.ckpt\"> checkpoint_4fold.ckpt </a>","metadata":{}},{"cell_type":"code","source":"# for fold in FOLDS:\n#     print(\"Fold :\",fold)\n#     train_df, valid_df = df[df.kfold!=fold], df[df.kfold==fold]\n#     train_dataset = BertDataset(train_df.excerpt.values,(np.array([train_df.target.values,train_df.standard_error.values]).T),max_len=max_words)\n#     valid_dataset = BertDataset(valid_df.excerpt.values,(np.array([valid_df.target.values,valid_df.standard_error.values]).T),max_len=max_words)\n#     train_dloader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=4)\n#     valid_dloader = DataLoader(valid_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n#     bert_model = BertModel() \n#     trainer = pl.Trainer(gpus=-1,max_epochs=EPOCHS,callbacks=[EarlyStopping(monitor=\"val_loss\",mode=\"min\",patience=20)],checkpoint_callback=False)\n#     trainer.fit(model = bert_model,train_dataloader = train_dloader,val_dataloaders = valid_dloader)\n#     trainer.save_checkpoint(f\"checkpoint_{fold}fold.ckpt\") ","metadata":{"execution":{"iopub.status.busy":"2021-05-19T19:49:35.6274Z","iopub.execute_input":"2021-05-19T19:49:35.627728Z","iopub.status.idle":"2021-05-19T21:00:18.562793Z","shell.execute_reply.started":"2021-05-19T19:49:35.627697Z","shell.execute_reply":"2021-05-19T21:00:18.561747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# check with training data","metadata":{}},{"cell_type":"code","source":"target_prediction = np.zeros(df.shape[0]) \nerror_prediction = np.zeros(df.shape[0]) \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfor fold in FOLDS:\n    print(\"Fold:\",fold)\n    loaded_model = BertModel.load_from_checkpoint(f\"../input/bert-trained-models/checkpoint_{fold}fold.ckpt\",map_location=device)\n    loaded_model.to(device)\n    loaded_model.eval() \n    #using the same BertDataset module of train, here dummy labels are provided\n    check_dataset = BertDataset(df.excerpt.values,\n                                labels = (np.array([df.target.values,df.standard_error.values]).T),\n                                max_len=max_words)\n    check_dataloader = DataLoader(check_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n    out_target = []\n    out_error = []\n    for batch in check_dataloader:\n        x  = batch[\"inputs\"]\n        labels = batch[\"label\"]\n        for key in x.keys():\n            x[key] = x[key].to(device)\n        assert x[\"input_ids\"].is_cuda, f\"data is not in model device({loaded_model.device.type})\"\n        out = loaded_model(x)\n        out = torch.squeeze(out, dim=1)\n        #print(out.size())\n        out_target_t = out[:,0]\n        out_error_t = out[:,1]\n        out_target.extend(out_target_t.cpu().detach().numpy())\n        out_error.extend(out_error_t.cpu().detach().numpy())\n        label_target = labels[:,0]\n        label_error = labels[:,0]\n        #print(out,labels)\n    target_prediction += np.hstack(out_target)\n    error_prediction += np.hstack(out_error)\n    #target_label += np.hstack(label_target)\n    #error_label += np.hstack(label_error)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:17:35.007983Z","iopub.execute_input":"2021-06-12T01:17:35.008342Z","iopub.status.idle":"2021-06-12T01:20:36.496407Z","shell.execute_reply.started":"2021-06-12T01:17:35.008309Z","shell.execute_reply":"2021-06-12T01:20:36.495411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(target_prediction/5, df['target'])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:20:41.11109Z","iopub.execute_input":"2021-06-12T01:20:41.111454Z","iopub.status.idle":"2021-06-12T01:20:41.236773Z","shell.execute_reply.started":"2021-06-12T01:20:41.111417Z","shell.execute_reply":"2021-06-12T01:20:41.236029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(target_prediction/5, error_prediction/5)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:20:49.628706Z","iopub.execute_input":"2021-06-12T01:20:49.629031Z","iopub.status.idle":"2021-06-12T01:20:49.975462Z","shell.execute_reply.started":"2021-06-12T01:20:49.628999Z","shell.execute_reply":"2021-06-12T01:20:49.974668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Weights and Inference ","metadata":{}},{"cell_type":"code","source":"np.ones([test_df.shape[0],2])","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:20:57.440868Z","iopub.execute_input":"2021-06-12T01:20:57.441213Z","iopub.status.idle":"2021-06-12T01:20:57.44765Z","shell.execute_reply.started":"2021-06-12T01:20:57.441179Z","shell.execute_reply":"2021-06-12T01:20:57.446846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = BertDataset(test_df.excerpt.values,labels = np.ones([test_df.shape[0],2]),max_len=max_words)\ntest_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:20:59.986563Z","iopub.execute_input":"2021-06-12T01:20:59.986877Z","iopub.status.idle":"2021-06-12T01:21:00.040928Z","shell.execute_reply.started":"2021-06-12T01:20:59.986847Z","shell.execute_reply":"2021-06-12T01:21:00.039934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_prediction = np.zeros(test_df.shape[0]) \nerror_prediction = np.zeros(test_df.shape[0]) \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfor fold in FOLDS:\n    print(\"Fold:\",fold)\n    loaded_model = BertModel.load_from_checkpoint(f\"../input/bert-trained-models/checkpoint_{fold}fold.ckpt\",map_location=device)\n    loaded_model.to(device)\n    loaded_model.eval() \n    #using the same BertDataset module of train, here dummy labels are provided\n    test_dataset = BertDataset(test_df.excerpt.values,labels = np.ones([test_df.shape[0],2]),max_len=max_words)\n    test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False,num_workers=4)\n    out_target = []\n    out_error = []\n    for batch in test_dataloader:\n        x  = batch[\"inputs\"]\n        for key in x.keys():\n            x[key] = x[key].to(device)\n        assert x[\"input_ids\"].is_cuda, f\"data is not in model device({loaded_model.device.type})\"\n        out = loaded_model(x)\n        out = torch.squeeze(out, dim=1)\n        out_target_t = out[:,0]\n        out_error_t = out[:,1]\n        out_target.extend(out_target_t.cpu().detach().numpy())\n        out_error.extend(out_error_t.cpu().detach().numpy())\n    target_prediction += np.hstack(out_target)\n    error_prediction += np.hstack(out_error)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:21:03.130483Z","iopub.execute_input":"2021-06-12T01:21:03.130797Z","iopub.status.idle":"2021-06-12T01:21:22.458253Z","shell.execute_reply.started":"2021-06-12T01:21:03.130766Z","shell.execute_reply":"2021-06-12T01:21:22.457266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_prediction","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:21:25.891649Z","iopub.execute_input":"2021-06-12T01:21:25.891984Z","iopub.status.idle":"2021-06-12T01:21:25.899655Z","shell.execute_reply.started":"2021-06-12T01:21:25.891949Z","shell.execute_reply":"2021-06-12T01:21:25.898718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"target\"] = target_prediction/NUM_FOLDS\nsub = test_df.drop(\"excerpt\",axis=1) \nsub.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:21:29.797912Z","iopub.execute_input":"2021-06-12T01:21:29.798254Z","iopub.status.idle":"2021-06-12T01:21:29.815955Z","shell.execute_reply.started":"2021-06-12T01:21:29.79822Z","shell.execute_reply":"2021-06-12T01:21:29.815194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2021-06-12T01:21:32.993911Z","iopub.execute_input":"2021-06-12T01:21:32.994247Z","iopub.status.idle":"2021-06-12T01:21:33.009902Z","shell.execute_reply.started":"2021-06-12T01:21:32.994214Z","shell.execute_reply":"2021-06-12T01:21:33.008898Z"},"trusted":true},"execution_count":null,"outputs":[]}]}